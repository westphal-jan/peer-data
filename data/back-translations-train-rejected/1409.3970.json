{"id": "1409.3970", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2014", "title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data", "abstract": "Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set.", "histories": [["v1", "Sat, 13 Sep 2014 17:17:05 GMT  (4528kb,D)", "https://arxiv.org/abs/1409.3970v1", "22 pages, 9 figures. A version was submitted to TPAMI on May 29, 2014. arXiv admin note: substantial text overlap witharXiv:1305.5306"], ["v2", "Fri, 7 Aug 2015 02:44:29 GMT  (5698kb,D)", "http://arxiv.org/abs/1409.3970v2", "24 pages, 10 figures. A version has been accepted by TPAMI on Aug 4th, 2015. arXiv admin note: substantial text overlap witharXiv:1305.5306"], ["v3", "Thu, 31 Dec 2015 16:12:31 GMT  (5698kb,D)", "http://arxiv.org/abs/1409.3970v3", "24 pages, 10 figures. A version has been accepted by TPAMI on Aug 4th, 2015. Add footnote about how to train the model in practice in Section 5.1. arXiv admin note: substantial text overlap witharXiv:1305.5306"]], "COMMENTS": "22 pages, 9 figures. A version was submitted to TPAMI on May 29, 2014. arXiv admin note: substantial text overlap witharXiv:1305.5306", "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG cs.NE", "authors": ["yin zheng", "yu-jin zhang", "hugo larochelle"], "accepted": false, "id": "1409.3970"}, "pdf": {"name": "1409.3970.pdf", "metadata": {"source": "CRF", "title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data", "authors": ["Yin Zheng", "Yu-Jin Zhang"], "emails": ["y-zheng09@mails.tsinghua.edu.cn", "zhang-yj@tsinghua.edu.cn", "hugo.larochelle@usherbrooke.ca"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Related Work", "text": "As already mentioned, multimodal data are often modeled using extensions of the basic LDA theme model, such as Corr-LDA [2], Multimodal LDA [5], and MDRF [4]. In this paper, we focus on learning a common representation of three different modalities: visual words, annotations, and class names. The class label describes the image globally with a single descriptive label (such as coast, outdoor, city center, etc.), while the annotations focus on highlighting the local content within the image. Wang et al. [9] proposed a supervised LDA formulation to address this issue. Wang et al. [17] instead opted for maximum margin formulation of LDA (MLDA). Our work is also part of this line of work that extends theme models to a supervised variant of NDDRE: our first contribution in this paper is the extension of another theme model, Docmodal, multiauthority."}, {"heading": "3 Document NADE", "text": "In this section we describe the original DocNADE model (v) = 1 (i)."}, {"heading": "4 SupDocNADE for Multimodal Data", "text": "In this section, we will describe the approach of this paper, inspired by DocNADE, to learn together from multimodal data. At this point, we will focus on the single-layer version of our model and discuss later, in Section 5.First, we will describe a supervised extension of DocNADE (SupDocNADE) that incorporates the modality of class labeling into the training to learn more discriminatory hidden features for classification, then we will describe how we use this patient positioning information of visual words, and finally, we will describe how we model the markup model together with SupDocNADE."}, {"heading": "4.1 Supervised DocNADE", "text": "It has been shown that the way in which we have behaved in recent years, is not only the way in which we have behaved, but also the way in which we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have behaved, how we have, how we have behaved, how we have, how we have behaved, how we have, how we have behaved, how we have, how we have behaved, how we have, how we have behaved, how we have, how we, how we have behaved, how we have, how we have, how we, how we have, how we"}, {"heading": "4.2 Dealing with Multiple Regions", "text": "Spatial information plays an important role in understanding an image. For example, the sky often appears in the upper part of the image, while a car usually appears at the bottom of the image. A lot of previous work has used this intuition successfully. For example, the basic work on spatial pyramids [20] shows that extracting different visual word histograms across different regions instead of a single image-wide histogram can achieve significant performance gains. We take a similar approach, modelling both the presence of the visual words and the identity of the region in which they occur. Suppose the image is divided into different regions: R = {R1, R2,.., RM}, where M is the number of regions. The image can now be represented as asvR = [vR1, v R 2,.)."}, {"heading": "4.3 Dealing with Annotations", "text": "In this section, we will now describe how we also model the annotation word modality with SupDocNADE. Let A be the predefined vocabulary of all annotation words. Thus, we can represent the annotation of a given image as a mixture of visual and annotation words: vA = [a1,..., v A Dv, v A Dv + 1,..., v A Dv + L] (16) = [vR1,.., v R Dv, a1,.,., aL]. To embed the annotation words in the framework of SupDocNADE, we can treat each annotation word in the way we handle larger words that we use together."}, {"heading": "5 Deep Extension of SupDocNADE", "text": "Although SupDocNADE has achieved better results in our previous work than the other topic models [24], the lack of an efficient deep formulation of SupDocNADE reduces its ability to model multimodal data, especially compared to other models based on a deep neural network [13, 12]. Recently, Uria et al. [25] proposed an efficient extension of the original NADE model [26] for binary vector observations from which DocNADE was derived. We take Uria et al. [25] and propose SupDeepDocNADE, i.e. a clear, authoregressive neural topic model for multimodal data modeling. In this section, we present the in-depth extension of DocNADE (DeepDocNADE) and then describe how to incorporate verified information into your training. We also discuss how to begin with the imbalance between the number of visual discussion and the results before we proceed to good annotations."}, {"heading": "5.1 DocNADE revisited", "text": "We will focus on the unattended version of DocNADE for now and discuss the supervised case later.In Section 4.1, we mentioned that words are randomly permutated before each stochastic gradient update to make DocNADE a good inference model for each individual arrangement of words. [25] We can imagine the use of many arrangements as the instantiation of many different DocNADE models, one for each different arrangement. From this point of view, by training a single set of parameters (linking matrices and biases) on all these arrangements, which we can effectively interpret a parameter-sharing strategy through these models and the training process, a factorial number of DocNADE models can be interpreted simultaneously.We will now make the presentation of the arrangement more explicit in our notation. [25]"}, {"heading": "5.2 Deep Document NADE", "text": "As shown in paragraph 5.1, the training of DocNADE (1) (1) (1) (< 1) (3) (3) (3) (3) (3) (3) (4)) (4) (4)) (4)) (4) (4)) (4) (4) (4) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5 (5) (5) (5) (5 (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5) (5 (5) (5) (5 (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) ("}, {"heading": "5.3 Supervised Deep Document NADE", "text": "Deep Document NADE can also be extended to a monitored variant, which is called SupDeepDocNADE according to the formulation in Section 4.1. To add the monitored information in DeepDocNADE, the negative log probability function in Eq.17 could be extended as follows: L (v, y; \u03b8) = E o O \u2212 log p (v, y; \u03b8, o) (25) = E o O \u2212 log p (y | v, \u03b8) \u2212 log p (v | \u03b8, o) (26) Since p (y | v, \u03b8) is independent of o, Eq.26 can be rewritten as follows: L (v, y; \u03b8) = \u2212 log p (y | v, \u03b8) \u2212 E o O log p (v | O) (27) Then L (v, y; ltvilar) can be rewritten by scanning v, d and o & lt.d as follows: L (v, v; v; v, v, v, v, v, v) \u2212 v, v \u2212 log (v, v, v, v, v, v, v \u2212 log, v, v, v \u2212, v \u2212, p \u2212, p \u2212, p \u2212 log, p \u2212, p \u2212, p \u2212, p \u2212 log, p \u2212, p \u2212, p \u2212, v \u2212 log, v \u2212 v \u2212 v, v, v \u2212 v, v \u2212 log, v \u2212 v \u2212, v \u2212 log, v \u2212, v \u2212 v \u2212 v \u2212 log, v \u2212 v \u2212, v \u2212 log, v \u2212 v \u2212 v \u2212 v, v, v \u2212 log, v \u2212 v \u2212 v \u2212 log, v \u2212 v \u2212 log, and v \u2212 v \u2212 v \u2212 log."}, {"heading": "5.4 Weighting the Annotation Words", "text": "As mentioned in Section 4.3, annotation words can be embedded within the framework of SupDocNADE < vo = Q components to be weighted specifically by treating them in the same way we handle visual words. However, in practice, the number of visual words could be much greater than the number of annotation words. For example, the average number of annotation words for an image is much greater than the average number of annotation words for an image (5,15). The imbalance of visual words and annotation words could cause some problems. For example, the contribution to the hidden representation from the annotation words is so small that it could be ignored compared to the contribution from the huge mountain of visual words, and the gradients come from the annotation words."}, {"heading": "5.5 Exploiting Global Image Features", "text": "In addition to the spatial information and annotations embedded in the DocNADE framework in Section 4.2 and Section 4.3, global bottom-up characteristics, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12]. Global characteristics can supplement, among other things, local information extracted from patch-based visual words. In this section, we describe how such characteristics can be embedded in the framework of our model. Specifically, f-RNf should be the global feature vector extracted from an image, where Nf is the length of the global feature vector. One way to embed f in the model could be to condition the hidden representation of the global feature f as follows: h (1) = g (c (1) + W (1) x (vo < d) + Pf) (33)."}, {"heading": "6 Experiments and Results", "text": "In this section, we will compare the performance of our model with other models of multimodal data modeling. First, we will test the ability of each SupDocNADE hidden layer to learn from multimodal data from two real-world datasets that are widely used in research on other subject models. Then, we will test the performance of SupDeepDocNADE on the large-scale Multimedia Information Retrieval (MIR) Flickr and show that SupDeepDocNADE achieves state-of-the-art performance. Code for downloading the datasets and for SupDocNADE and SupDeepDocNADE is available at https: / / sites.google.com / site / zhengyin1126 / home / supdeepdocnade."}, {"heading": "6.1 Experiments for SupDocNADE", "text": "To test the ability of each SupDocNADE hidden layer to learn from multimodal data, we measured its performance using simultaneous image classification and annotation tasks. We tested our model against two real data sets: a subset of the LabelMe dataset [29] and the UIUC Sports dataset [30]. LabelMe and UIUC Sports contain annotations and are popular classification and annotation benchmarks. We performed extensive quantitative comparisons of SupDocNADE with the original DocNADE model and monitored LDA (sLDA) 4 [10, 9]. We also offer some comparisons with MLDA [17] and a spatial pyramid matching (SPM) approach [20]."}, {"heading": "6.1.1 Data sets Description", "text": "Following Wang et al. [9], we created our LabelMe dataset using the online tool to obtain 256 x 256 pixel images from the following 8 classes: highway, downtown, coast, forest, skyscraper, road, open country and mountain. For each class, 200 images were randomly selected and evenly divided into training and test kits for a total of 1600 frames. The UIUC sports dataset contains 1792 images, divided into 8 classes: badminton (313 frames), bocce (137 frames), croquet (330 frames), polo (183 frames), climbing (194 frames), rowing (255 frames), sailing (190 frames), snowboarding (190 frames). After previous work, the maximum side of each image was reduced to 400 pixels while maintaining aspect ratio."}, {"heading": "6.1.2 Experimental Setup for SupDocNADE", "text": "Following Wang et al. [9], 128 dimensional, densely extracted SIFT features were used for an image to extract the visual words. Step and patch size of the dense SIFT extraction was reduced to 8 and 16, respectively. The dense SIFT features from the training set were quantified into 240 clusters to construct our visual word vocabulary using K-Means. We divided each image into a 2 x 2 grid to extract the spatial position information, as described in Section 4.2. This resulted in 2 x 2 x 240 = 960 different visual word / region pairings. We use classification accuracy to evaluate the performance of the image classification and the average F measure of the 5 predicted annotations to evaluate the annotation performance, as in previous work. The F measurement of an image is defined as F measurement - measurement = 2 x precision precision of the image classification and the average F measure of the 5 predicted annotations to evaluate the annotation performance, as in previous work."}, {"heading": "6.1.3 Quantitative Comparison", "text": "This year we have the opportunity to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "6.1.4 Visualization of Learned Representations", "text": "Since topic models are often used to interpret and explore the semantic structure of image data, we investigated how we can observe the structure learned from SupDocNADE. We extracted the visual / annotation words most associated with specific class names within SupDocNADE as follows: Given a class caption street corresponding to a column U:, i in Matrix U, we selected the top 3 topics (hidden units) with the largest connecting weight in U:, i. We then determined the columns of the matrix W corresponding to these 3 hidden topics and selected the visual / annotation words with the largest average weight connection. The results of this method for classes street, sail, forest and highway are illustrated in Figure 5. To visualize the visual words, we show 16 image fields that belong to each visual word as they are extracted by K-means."}, {"heading": "6.2 Experiments for SupDeepDocNADE", "text": "We are now testing the performance of SupDeepDocNADE, the profound extension of SupDocNADE, on the large-scale MIR Flickr dataset [14]. MIR Flickr is a challenging benchmark for multimodal data modeling tasks. In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance with the MIR Flickr dataset over strong baselines: the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al. [31]."}, {"heading": "6.2.1 MIR Flickr Data Set", "text": "The MIR Flickr dataset contains 1 million real images collected by the image hosting site Flickr. The social tags of each image are also collected and used as annotations in our experiments. Among the 1 million images are 25,000 images labeled in 38 classes, such as sky, bird, humans, animals, car, etc., giving us a subset of labeled images. Each image in the labeled subset may have several class names. In our experiments we used 15,000 images for training and 10,000 images for the test. The remaining 975,000 images have no labels and were therefore used for the unattended pre-training of SupDeepDocNADE (see next section). The most common 2000 tags are collected for the annotation vocabulary, after previous work [12, 13]. The average number of annotations for animations is 5.15. In the entire dataset, 128 501 images have no annotations, of which are 51 images labeled in the subset of 4551."}, {"heading": "6.2.2 Experimental Setup for SupDeepDocNADE", "text": "To compare directly with the DBM approaches of Srivastava and Salakhutdinov, it is not useful to use the data we used. [13] To compare directly with the DBM approaches of Srivastava and Salakhutdinov, we use the same experimental configuration. [13] The images in ME Flickr are first used to hide the maximum side of each image with 480 pixels to maintain the aspect ratio. [13] Then, 128 dimensional SIFT features are densely sampled on these images to extract the visual words. [13] Following Srivastava and Salakhutdinov, the 4, 6, 8, 10 pixels, or the patch step is fixed to 3 pixels. SIFT features from the unlabeled images have been quantified in 2000 clusters that are used as visual vocabulary vocabulary."}, {"heading": "6.2.3 Comparison with other baselines", "text": "rrf\u00fc ide rrf\u00fc ide eeisrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "6.2.4 The Impact of the Annotation Weight", "text": "In Section 6.2.4, we suggested weighting the annotation words differently to solve the problem of imbalance in the number of visual and annotation words. In this part, we examine the influence of the annotation weight on performance. Specifically, we set the annotation weight to {1, 4000, 8000, 12,000, 16,000} and show the performance for each of the annotation weight values. Note that if the annotation weight is equal to 1, there is no compensation for the imbalance of visual words and annotation words. The other experimental configurations are the same as in Section 6.2.2.2 Figure 8 shows the performance comparison between the various annotation weights. As expected, SupDeepDocNADE performs extremely poorly if the annotation weights are equal to 1, if the annotation weight is increased, performance improves. Of all the chosen annotation weights, 12,000 performs best, which a MAP of 0.671 achieves, or 669 achieves performance in comparison to 6.000, respectively."}, {"heading": "6.2.5 Visualization of the Retrieval Results", "text": "Since SupDeepDocNADE is used for multimodal data modeling, we illustrate some results for multimodal data recovery tasks. Specifically, we show some qualitative results in two multimodal data recovery scenarios: multimodal data retrieval and text generation from images. Multimodal data retrieval: In this task, cosmic similarity is used as similarity metric. In this experiment, each query corresponds to an individual test example and the collection corresponds to the rest of the test set. Figure 10 illustrates the query results for multimodal data retrieval, showing the 4 most similar images of query input in the test set."}, {"heading": "7 Conclusion and Discussion", "text": "In this paper, we proposed SupDocNADE, a supervised extension of DocNADE that can be learned together from visual words, annotations, and class names. In addition, we proposed a profound extension of SupDocNADE that surpasses its shallow version and can be trained efficiently. Although both SupDocNADE and SupDeepDocNADE are inherently the same, SupDeepDocNADE differs in its training process from the single-layered version. Specifically, the training process of SupDeepDocNADE is performed over a subset of words, summing the progressions over multiple arrangements with the same permutation up to a randomly selected position d, while the single-layered version does the opposite, utilizing a single randomly selected arrangement of SupDeepDocNADE, but updating all the conditions on the words. Like all theme models, our model is designed to represent the distribution of the SupDeepDocNADE image as opposed to most models, and to a benchmark."}], "references": [{"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth"], "venue": "JMLR, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling annotated data", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "ACM SIGIR, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "CVPR, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia"], "venue": "ICCV, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Topic regression multi-modal latent dirichlet allocation for image annotation", "author": ["D. Putthividhy"], "venue": "CVPR, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal semi-supervised learning for image classification", "author": ["M. Guillaumin"], "venue": "CVPR, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia"], "venue": "ACM-MM, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["D. Blei"], "venue": "JMLR, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Simultaneous image classification and annotation", "author": ["C. Wang", "D. Blei", "F.-F. Li"], "venue": "CVPR, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Replicated softmax: an undirected topic model", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "NIPS, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimodal learning with deep boltzmann machines.", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "in NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "NIPS, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The mir flickr retrieval evaluation", "author": ["M.J. Huiskes", "M.S. Lew"], "venue": "ACM MIR, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "H. Lee"], "venue": "NIPS, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "NIPS 25, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Max-margin latent dirichlet allocation for image classification and annotation", "author": ["Y. Wang"], "venue": "BMVC, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio"], "venue": "arXiv preprint arXiv:1206.5538, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik"], "venue": "CVPR, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "The tradeoff between generative and discriminative classifiers", "author": ["G. Bouchard", "B. Triggs"], "venue": "COMPSTAT, 2004.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Topic modeling of multimodal data: an autoregressive approach", "author": ["Y. Zheng", "Y.-J. Zhang", "H. Larochelle"], "venue": "CVPR, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep and tractable density estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "ICML, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Journal of Machine Learning Research, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, 2001.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Color and texture descriptors", "author": ["B.S. Manjunath"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on, 2001.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Labelme: a database and web-based tool for image annotation", "author": ["B.C. Russell"], "venue": "IJCV, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "What, where and who? classifying events by scene and object recognition", "author": ["L.-J. Li", "L. Fei-Fei"], "venue": "ICCV, 2007. 23", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Image annotation with tagprop on the mirflickr set", "author": ["J. Verbeek"], "venue": "ACM MIR, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan"], "venue": "The Journal of Machine Learning Research, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "AISTATS, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "A tutorial on stochastic approximation algorithms for training restricted boltzmann machines and deep belief nets", "author": ["K. Swersky"], "venue": "Information Theory and Applications Workshop. IEEE, 2010. 24", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].", "startOffset": 133, "endOffset": 154}, {"referenceID": 1, "context": "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].", "startOffset": 133, "endOffset": 154}, {"referenceID": 2, "context": "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].", "startOffset": 133, "endOffset": 154}, {"referenceID": 3, "context": "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].", "startOffset": 133, "endOffset": 154}, {"referenceID": 4, "context": "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].", "startOffset": 133, "endOffset": 154}, {"referenceID": 5, "context": "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].", "startOffset": 133, "endOffset": 154}, {"referenceID": 6, "context": "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].", "startOffset": 133, "endOffset": 154}, {"referenceID": 7, "context": "One of the leading approaches is based on topic modelling, the most popular model being latent Dirichlet allocation or LDA [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "LDA is a generative model for documents that originates from the natural language processing community, but has had great success in computer vision [8, 9].", "startOffset": 149, "endOffset": 155}, {"referenceID": 8, "context": "LDA is a generative model for documents that originates from the natural language processing community, but has had great success in computer vision [8, 9].", "startOffset": 149, "endOffset": 155}, {"referenceID": 1, "context": "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].", "startOffset": 79, "endOffset": 91}, {"referenceID": 4, "context": "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].", "startOffset": 79, "endOffset": 91}, {"referenceID": 3, "context": "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].", "startOffset": 79, "endOffset": 91}, {"referenceID": 8, "context": "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].", "startOffset": 79, "endOffset": 91}, {"referenceID": 1, "context": "For instance, Correspondence LDA (Corr-LDA) [2] was proposed to discover the relationship between images and annotation modalities, by assuming each image topic must have a corresponding text topic.", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "Multimodal LDA [5] generalizes Corr-LDA by learning a regression module relating the topics from the different", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "Multimodal Document Random Field Model (MDRF) [4] was also proposed to deal with multimodal data, which learns cross-modality similarities from a document corpus containing multinomial data.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "Besides the annotation words, the class label modality can also be embedded into LDA, such as in supervised LDA (sLDA) [10, 9].", "startOffset": 119, "endOffset": 126}, {"referenceID": 9, "context": "In the realm of document modeling, Salakhutdinov and Hinton [11] proposed a so-called Replicated Softmax (RS) model for bags of words.", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "The RS model was later used for multimodal data modeling [12], where pairs of images and text annotations were modeled jointly within a deep Boltzmann machine (DBM) [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "The RS model was later used for multimodal data modeling [12], where pairs of images and text annotations were modeled jointly within a deep Boltzmann machine (DBM) [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 12, "context": "This deep learning approach to the generative modeling of multimodal data achieved state-of-the-art performance on the MIR Flickr data set [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "Another neural network based state-of-the-art multimodal data modeling approach is Multimodal Deep Recurrent Neural Network (MDRNN) [15] which aims at predicting missing data modalities through the rest of data modalities by minimizing the variation of information rather than maximizing likelihood.", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Recently, an alternative generative modeling approach for documents was proposed in Larochelle and Lauly [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "Larochelle and Lauly [16] also show that DocNADE is a better generative model of text documents than LDA and the RS model, and can extract a useful representation for text information retrieval.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "As previously mentioned, multimodal data is often modeled using extensions of the basic LDA topic model, such as Corr-LDA [2], Multimodal LDA [5] and MDRF [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "As previously mentioned, multimodal data is often modeled using extensions of the basic LDA topic model, such as Corr-LDA [2], Multimodal LDA [5] and MDRF [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "As previously mentioned, multimodal data is often modeled using extensions of the basic LDA topic model, such as Corr-LDA [2], Multimodal LDA [5] and MDRF [4].", "startOffset": 155, "endOffset": 158}, {"referenceID": 8, "context": "[9] proposed a supervised LDA formulation to tackle this problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[17] opted instead for a maximum margin formulation of LDA (MMLDA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Recently, deep neural networks are increasingly used for the probabilistic modeling of images and text (see [18] for a review).", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "The work of Srivastava and Salakhutdinov [12] on DBMs and Sohn et al.", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "[15] on MDRNN are good recent examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[19] also proposed deep autoencoder networks for multimodal learning, though this approach was recently shown to be outperformed by DBMs [13] and MDRNN [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "[19] also proposed deep autoencoder networks for multimodal learning, though this approach was recently shown to be outperformed by DBMs [13] and MDRNN [15].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Although DocNADE shows favorable performance over other topic models, the lack of an efficient deep formulation reduces its ability of modeling multimodal data, especially compared with the deep neural network based models [19, 12, 13].", "startOffset": 223, "endOffset": 235}, {"referenceID": 11, "context": "Although DocNADE shows favorable performance over other topic models, the lack of an efficient deep formulation reduces its ability of modeling multimodal data, especially compared with the deep neural network based models [19, 12, 13].", "startOffset": 223, "endOffset": 235}, {"referenceID": 11, "context": "As we\u2019ll see, the deep version of our DocNADE model will outperform the DBM approach of Srivastava and Salakhutdinov [13].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "In Larochelle and Lauly [16], DocNADE was used to model documents of real words, belonging to some predefined vocabulary.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "To address this issue, Larochelle and Lauly [16] propose to use a balanced binary tree to decompose the computation of the conditionals and obtain a complexity logarithmic inQ.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Larochelle and Lauly [16] model each left/right transition probabilities in the binary tree using a set of binary logistic regressors taking the hidden layer hi(v<i) as input.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "One could attempt to optimize the organization of the words within the tree, but a random assignment of the words to leaves works well in practice [16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "1 Supervised DocNADE It has been observed that learning image feature representations using unsupervised topic models such as LDA can perform worse than training a classifier directly on the visual words themselves, using an appropriate kernel such as a pyramid kernel [20].", "startOffset": 269, "endOffset": 273}, {"referenceID": 18, "context": "This is known as generative learning [21].", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "from left to right, top to bottom in the image), we follow Larochelle and Lauly [16] and randomly permute the words before every stochastic gradient update.", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "which often outperforms other activation functions [22] and has been shown to work well for image data [23].", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "which often outperforms other activation functions [22] and has been shown to work well for image data [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "For example, in the seminal work on spatial pyramids [20], it is shown that extracting different visual word histograms over distinct regions instead of a single image-wide histogram can yield substantial gains in performance.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Although SupDocNADE has achieved better performance than the other topic models in our previous work [24], the lack of an efficient deep formulation of SupDocNADE reduces its capability of modeling multimodal data, especially compared with other models based on deep neural network [13, 12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Although SupDocNADE has achieved better performance than the other topic models in our previous work [24], the lack of an efficient deep formulation of SupDocNADE reduces its capability of modeling multimodal data, especially compared with other models based on deep neural network [13, 12].", "startOffset": 282, "endOffset": 290}, {"referenceID": 10, "context": "Although SupDocNADE has achieved better performance than the other topic models in our previous work [24], the lack of an efficient deep formulation of SupDocNADE reduces its capability of modeling multimodal data, especially compared with other models based on deep neural network [13, 12].", "startOffset": 282, "endOffset": 290}, {"referenceID": 22, "context": "[25] proposed an efficient extension of the original NADE model [26] for binary vector observations, from which DocNADE was derived.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] proposed an efficient extension of the original NADE model [26] for binary vector observations, from which DocNADE was derived.", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "[25] and propose SupDeepDocNADE, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] notice, we can think of the use of many orderings as the instantiation of many different DocNADE models, one for each distinct ordering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25], we now denote p (v|\u03b8, o) as the joint distribution of the DocNADE model over the words of an image given the parameters \u03b8 and ordering o.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] will models binary vectors of fixed size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "For example, in the MIR Flickr data set, with the experimental setup of Srivastava and Salakhutdinov [12], the average number of visual words for an image is about 69 011, which is much larger than the average number of annotation words for an image (5.", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "3, bottom-up global features, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12].", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "3, bottom-up global features, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "3, bottom-up global features, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 26, "context": "We tested our model on 2 real-world data sets: a subset of the LabelMe data set [29] and the UIUC-Sports data set [30].", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "We tested our model on 2 real-world data sets: a subset of the LabelMe data set [29] and the UIUC-Sports data set [30].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "We performed extensive quantitative comparisons of SupDocNADE with the original DocNADE model and supervised LDA (sLDA)4 [10, 9].", "startOffset": 121, "endOffset": 128}, {"referenceID": 15, "context": "We also provide some comparisons with MMLDA [17] and a Spatial Pyramid Matching (SPM) approach [20].", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "We also provide some comparisons with MMLDA [17] and a Spatial Pyramid Matching (SPM) approach [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "[9], we constructed our LabelMe data set using the online tool to obtain images of size 256\u00d7256 pixels from the following 8 classes: highway, inside city, coast, forest, tall building, street, open country and mountain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9], 128 dimensional, densely extracted SIFT features were used to extract the visual words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9], the annotation words are not available at test time and all methods predict an image\u2019s class based solely on its bag of visual words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "4We mention that [9] has shown that sLDA performs better than Corr-LDA[2].", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "4We mention that [9] has shown that sLDA performs better than Corr-LDA[2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "Moreover, [4] found that Multimodal LDA [5] did not improve on the performance of Corr-LDA.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Moreover, [4] found that Multimodal LDA [5] did not improve on the performance of Corr-LDA.", "startOffset": 40, "endOffset": 43}, {"referenceID": 17, "context": "LabelMe UIUC-Sports Model Accuracy% F-measure% Accuracy% F-measure% SPM [20] 80.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "78 MMLDA [17] 81.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "51\u2020 sLDA [9] 81.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Since code for performing image annotation using sLDA is not publicly available, we compare directly with the results found in the corresponding paper [9].", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "[9] report F -measures of 38.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "We also compare with MMLDA [17], which has been applied to image classification and annotation separately.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "[17] is better than SupDocNADE on LabelMe but worse on UIUC-Sports.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] could also be adapted to perform both image classification and annotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] to generate two-layer SPM representations with a vocabulary size of 240, which is the same configuration as used by the other models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "2 Experiments for SupDeepDocNADE We now test the performance of SupDeepDocNADE, the deep extension of SupDocNADE, on the large-scale MIR Flickr data set [14].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set over strong baselines : the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al.", "startOffset": 188, "endOffset": 192}, {"referenceID": 13, "context": "In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set over strong baselines : the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al.", "startOffset": 200, "endOffset": 204}, {"referenceID": 5, "context": "In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set over strong baselines : the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al.", "startOffset": 214, "endOffset": 217}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The most frequent 2000 tags are collected for the annotation vocabulary, following previous work [12, 13].", "startOffset": 97, "endOffset": 105}, {"referenceID": 11, "context": "The most frequent 2000 tags are collected for the annotation vocabulary, following previous work [12, 13].", "startOffset": 97, "endOffset": 105}, {"referenceID": 11, "context": "2 Experimental Setup for SupDeepDocNADE In order to compare directly with the DBM approach of Srivastava and Salakhutdinov [13], we use the same experimental configuration.", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "Following Srivastava and Salakhutdinov [13], we used 4 different scales of patch size, which are 4, 6, 8, 10 pixels, respectively, and the patch step is fixed to 3 pixels.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "5), a combination of Gist [27] and MPEG-7 descriptors [28](EHD, HTD, CSD, CLD, SCD) is adopted in our experiments, as in Srivastava and Salakhutdinov [13].", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "5), a combination of Gist [27] and MPEG-7 descriptors [28](EHD, HTD, CSD, CLD, SCD) is adopted in our experiments, as in Srivastava and Salakhutdinov [13].", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "5), a combination of Gist [27] and MPEG-7 descriptors [28](EHD, HTD, CSD, CLD, SCD) is adopted in our experiments, as in Srivastava and Salakhutdinov [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "Note that the DBM [12, 13] also use 3 hidden layers with 2048 hidden units for each layer, thus our comparison with the DBM is fair.", "startOffset": 18, "endOffset": 26}, {"referenceID": 11, "context": "Note that the DBM [12, 13] also use 3 hidden layers with 2048 hidden units for each layer, thus our comparison with the DBM is fair.", "startOffset": 18, "endOffset": 26}, {"referenceID": 29, "context": "Once training is finalized, the hidden representation from the top hidden layer after observing all words (both visual words and annotation words) of an image is feed to a linear SVM [32] to compute confidences of an image belonging to each class.", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "We used the same 5 training/validation/test set splits on the labeled subset of MIR Flickr as Srivastava and Salakhutdinov [13] and report the average performance on the 5 splits.", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "To initialize the connection matrices, we followed the recommendation of Glorot and Bengio [33] used a uniform distribution: \u0398 \u223c U [ \u2212 \u221a 6 \u221a l\u0398 + w\u0398 , \u221a 6 \u221a l\u0398 + w\u0398 ] (36)", "startOffset": 91, "endOffset": 95}, {"referenceID": 31, "context": "To prevent overfitting, dropout [34] is adopted during training, with a dropout rate of 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "004 Multiple Kernel Learning SVMs [6] 0.", "startOffset": 34, "endOffset": 37}, {"referenceID": 28, "context": "623 TagProp [31] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "640 Multimodal DBM [13] 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "005 MDRNN [15] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 32, "context": "This corresponds to Polyak averaging [35], but where the linear average is replaced by a weighting that puts more emphasis on recent parameter values.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "3 Comparison with other baselines Table 2 presents a comparison of the performance of SupDeepDocNADE with the DBM approach of Srivastava and Salakhutdinov [13] and MDRNN of Sohn et al.", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "[15] as well as other strong baselines, in terms of MAP performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The other annotation weights also achieves good performance compared with the DBM model [13]: MAP of 0.", "startOffset": 88, "endOffset": 92}], "year": 2015, "abstractText": "Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-theart performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set.", "creator": "LaTeX with hyperref package"}}}