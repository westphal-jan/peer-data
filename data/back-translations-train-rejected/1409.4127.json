{"id": "1409.4127", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2014", "title": "Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network", "abstract": "Unconstrained video recognition and Deep Convolution Network (DCN) are two active topics in computer vision recently. In this work, we apply DCNs as frame-based recognizers for video recognition. Our preliminary studies, however, show that video corpora with complete ground truth are usually not large and diverse enough to learn a robust model. The networks trained directly on the video data set suffer from significant overfitting and have poor recognition rate on the test set. The same lack-of-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from images to videos to utilize the knowledge in the weakly labeled image corpus for video recognition. The image corpus help to learn important visual patterns for natural images, while these patterns are ignored by models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. We show that by means of transfer learning from image to video, we can learn a frame-based recognizer with only 4k videos. Because the image corpus is weakly labeled, the entire learning process requires only 4k annotated instances, which is far less than the million scale image data sets required by previous works. The same approach may be applied to other visual recognition tasks where only scarce training data is available, and it improves the applicability of DCNs in various computer vision problems. Our experiments also reveal the correlation between meta-parameters and the performance of DCNs, given the properties of the target problem and data. These results lead to a heuristic for meta-parameter selection for future researches, which does not rely on the time consuming meta-parameter search.", "histories": [["v1", "Mon, 15 Sep 2014 01:26:55 GMT  (2131kb,D)", "https://arxiv.org/abs/1409.4127v1", null], ["v2", "Mon, 15 Jun 2015 14:47:54 GMT  (7630kb,D)", "http://arxiv.org/abs/1409.4127v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yu-chuan su", "tzu-hsuan chiu", "chun-yen yeh", "hsin-fu huang", "winston h hsu"], "accepted": false, "id": "1409.4127"}, "pdf": {"name": "1409.4127.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network", "authors": ["Yu-Chuan Su", "Tzu-Hsuan Chiu", "Chun-Yen Yeh", "Hsin-Fu Huang", "Winston H. Hsu"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "II. DEEP CONVOLUTION NETWORK FOR VIDEO RECOGNITION", "text": "In this section, we review the basic concept of the neural folding network. We show that folding is a specialized regularization of the standard neural network that reduces the number of learnable parameters in the model based on prior knowledge in the field of vision. Afterwards, we discuss previous work on the evaluation of DCN architectures and the application of DCN for video detection."}, {"heading": "A. Convolution neural network", "text": "The main difficulty in using the Deep Neural Network is that it is difficult to learn due to the complexity of the network. (1) Local Response - Enforce sparse connection (2) Tied Weight - Enforce equal weight distribution within characteristics. (1) Local reaction forces sparse connections between layers with the local receptive field heuristic and bound weight restriction. (1) Convolution Network is essentially an MLP with two additional limitations: (1) Local reaction and (2) Tied weight. Local reaction forces sparse connections between layers with bound weight restriction. (2) The two limitations reduce the number of learnable parameters and avoid overfitting. Due to the depth and large number of learnable parameters. [19] suggested a greedy layerwise pre-training to overcome the problem."}, {"heading": "B. Network configuration", "text": "In order to specify the network architecture, we need to determine the depth of the network, the number of hidden units or folding cores in each level, the receptive field of each level, and many other details. However, meta-parameters are known to have a major impact on network performance [25] - [27]. However, due to the large number of possible configurations and computing costs, it is impossible to perform the network search for the optimal configuration in most situations. There are several popular network architectures in the research community [17], [26], [28] and many previous works use this configuration without further optimization [29], [30] although they are successful in many cases, most architectures are proposed without explanation or justification of choice, leading to the lack of knowledge about the system configuration that improves the configuration for specific task, which can be expected to improve the epigrams."}, {"heading": "C. Deep convolution network for video recognition", "text": "The question is to what extent people are able to decide for themselves what they want and what they want to do."}, {"heading": "III. TRANSFER LEARNING WITH DEEP CONVOLUTION NETWORK", "text": "In this section, we describe the learning approaches we use to use image information in video detection. Transfer learning helps to learn a generalizable DCN. This is important because DCNs tend to overadapt, especially when scarce training data is available. Although increasing training data helps solve the problem, there are cases where collecting new data with complete truth is difficult. Transfer learning solves the problem by using labeled data from other areas where a large amount of training data is available to improve the network. [20] Good representation does not necessarily optimize the loss during training, which occurs through supervised backpropagation in Deep Neural Network (SAE), in the sense that it improves generalizability by learning a better intermediate representation [20]."}, {"heading": "A. Augment training data", "text": "The first approach to transfer learning is to supplement video data with independent image data. Or we can form a DCN that simultaneously recognizes images from the image data sets and frames from the video data set, as shown in Fig. 3. This is possible because the interlayers Yahoo! -FlickrILSVRC 2012Fig. 4: First folding layer cores from Yahoo! Flickr- and ILSVRC2012-datasets were learned from non-overlapping datasets. Although they were learned from non-overlapping datasets, some cores are visually similar, supporting the assumption that some important image patterns are distributed across different datasets that provide sufficient training data. They are shared by all output units in neural networks and can benefit from the training data of other classes. If the lower layers can learn general visual patterns that are distributed across different datasets, the additional classes and training data can help to learn better features at the low and middle levels and avoid overadjustment."}, {"heading": "B. Transfer mid-level features", "text": "The second approach to transfer is to transfer the learned characteristics of images to video by using the learnable parameters of DCNs that already exist in the image domain, which can also be considered as a supervised pre-training, analogous to the unattended pre-training in DBN or SAE. The network is then refined with the target data set to optimize the characteristics in the target domain. This approach is motivated by the fact that the image characteristics are shared in all natural images so that they can be used to characterize the images outside the training set."}, {"heading": "IV. DATASET", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "V. NETWORK ARCHITECTURE", "text": "In this context, it is also worth mentioning the fact that the other two are two layers that move in the way they have developed in recent years. \"The first layer of the layer,\" according to the tenor, \"is the first layer of the layer.\" \"The second layer.\" \"The second layer.\" \"The second layer.\" \"\" The second layer. \"\" \"The second layer.\" \"\" The third layer. \"\" \"\" The third layer. \"\" \"\" The third layer. \"\" \"\" The third layer. \"\" \"\" \"The third layer.\" \"\" \"\" \"\" The third layer. \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "VI. EXPERIMENT \u2013 NETWORK CONFIGURATION SELECTION", "text": "Instead, an intensive search for the meta-parameters for each new problem is required, so that we can examine the correlation between the meta-parameters and network performance on a wide range of datasets. We continue to configure frame-based video sequences based on the target datasets. In particular, we study image resolution based on input images and the required depth of the network, which have the greatest impact on the computational costs of DCN."}, {"heading": "B. Depth of Architecture", "text": "In this section, we compare the performance of DCNs with different number of folding layers. Since adding layers to the network significantly increases computing costs, we want to make networks as flat as possible when the additional layers make no or even negative contribution to performance. To evaluate the impact of different depths on performance, we learn folding networks with 2 x 5 folding layers. The results are shown in Table II. Again, the two sets of data show different responses to the number of folding layers. In Yahoo! Flickr, the 3-folding layer network proves to be the most powerful, although the performance gain of the 3rd folding layer is relatively small. Note the superior performance of 5-folding networks compared to the 4-folding layer one benefits not only from the additional layer, but also from the additional folding core in every 9th layer. The performance gain of additional folding layers is more significant in ILSVRC2012, which is a relative improvement in terms of the top-1 graphics performance when the number of folding layers does not grow with the number of previous ones."}, {"heading": "C. Training Data Number and Diversity", "text": "We chose Yahoo! -Flickr because studying performance dependence on training size is part of the goal for which the dataset was designed. On the other hand, the ILSVRC2012 dataset is not suitable for the study because reducing the training set can either lead to very few samples for some classes or change the ratio of the different classes. We train 2-convolution-layer networks on Yahoo! -Flickr with 20k and 150k training samples for each class. We select 20k training samples for comparison because previous results show that the performance of the linear classifier is saturated with 20k training samples per class. The results are shown in Table III, which clearly shows that smaller training sets result in significant overmatch and therefore poor performance. CCVCCVVCCV + Yahoo! -FlickrFig. 6: Samples of the first convolution layer cores learned from the CCV dataset, and we learned from augmenting CCV- with only high training programs."}, {"heading": "VII. EXPERIMENT \u2013 VIDEO RECOGNITION WITH TRANSFER LEARNING", "text": "In this section, we examine the proposed transfer learning approach for video detection data sets. The overall workflow of video detection is as follows: We sample video images from the training set at a uniform sampling rate of 1 fps and train an image-wise DCN. The video label is applied to all images extracted from the video, although there may be irrelevant images. In test videos, we extract keyframes from the video and perform image-wise detection. The detection results of all key frames within a video are then aggregated using an average pooling to predict the category of the video."}, {"heading": "A. Augment training sets", "text": "In this section, we will explore the approach of augmenting video data sets with static image sets. In practice, we extract roughly the same number of images (0.4 million) from the image sets and mix these images with the video data sets to create a new training set. Results are shown in Table IV. Although the two data sets come from very different sources (static image versus video), expanding the video data sets actually improves performance. This can be explained by the first cores of the folding layer, as shown in Figure 6, which shows that by expanding the video data set, the network successfully learns the radio frequency signals that are ignored when training the network for CCV. These signals are known to be important, and ignoring them can affect generalizability."}, {"heading": "B. Transfer Mid-level Features", "text": "In this area, we are in a position to take all the necessary measures to defuse the situation."}, {"heading": "VIII. CONCLUSIONS", "text": "Our preliminary study shows that one of the most significant obstacles to training a DCN is the requirement of a large number of training samples. Without enough training samples, the networks are very prone to over-adjustment leading to poor performance. This problem is particularly important for video detection, as videos with the truth on the ground are rare and hard to obtain. To solve the problem, we train DCNs to transfer images to videos. The image corpus can be poorly labeled, which is widely used in various social media such as Flickr and Instagram. These rich image examples can help learn more robust detection mechanisms on the video frame, even though they come from different areas. The transfer learning process allows DCN to be trained with scarce training data, where we achieve reasonable performance by using only 4k videos for training."}], "references": [{"title": "Consumer video understanding: A benchmark database and an evaluation of human and machine performance", "author": ["Y.-G. Jiang", "G. Ye", "S.-F. Chang", "D. Ellis", "A.C. Loui"], "venue": "ICMR, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Recognizing complex events using large margin joint low-level event model", "author": ["H. Izadinia", "M. Shah"], "venue": "ECCV, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A discriminative cnn video representation for event detection", "author": ["Z. Xu", "Y. Yang", "A.G. Hauptmann"], "venue": "arXiv preprint arXiv:1411.4006, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting feature and class relationships in video categorization with regularized deep neural networks", "author": ["Y.-G. Jiang", "Z. Wu", "J. Wang", "X. Xue", "S.-F. Chang"], "venue": "arXiv preprint arXiv:1502.07209, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling spatialtemporal clues in a hybrid deep learning framework for video classification", "author": ["Z. Wu", "X. Wang", "Y.-G. Jiang", "H. Ye", "X. Xue"], "venue": "arXiv preprint arXiv:1504.01561, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating two-stream cnn for video classification", "author": ["H. Ye", "Z. Wu", "R.-W. Zhao", "X. Wang", "Y.-G. Jiang", "X. Xue"], "venue": "arXiv preprint arXiv:1504.01920, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1920}, {"title": "A survey of video datasets for human action and activity recognition", "author": ["J.M. Chaquet", "E.J. Carmona", "A. Fern\u00e1ndez-Caballero"], "venue": "Comput. Vis. Image Underst., vol. 117, no. 6, pp. 633\u2013659, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation campaigns and trecvid", "author": ["A.F. Smeaton", "P. Over", "W. Kraaij"], "venue": "MIR, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "CVPR, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Detecting activities of daily living in first-person camera views", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "CVPR, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A 3-dimensional sift descriptor and its application to action recognition", "author": ["P. Scovanner", "S. Ali", "M. Shah"], "venue": "Proceedings of the 15th International Conference on Multimedia, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "A spatio-temporal descriptor based on 3d-gradients", "author": ["A. Kl\u00e4ser", "M. Marsza\u0142ek", "C. Schmid"], "venue": "BMVC, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting image-trained cnn architectures for unconstrained video classification", "author": ["S. Zha", "F. Luisier", "W. Andrews", "N. Srivastava", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1503.04144, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep learning approach to machine transliteration", "author": ["T. Deselaers", "S. Hasan", "O. Bender", "H. Ney"], "venue": "Proceedings of the Fourth Workshop on Statistical Machine Translation, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "Trans. Audio, Speech and Lang. Proc., vol. 20, no. 1, pp. 14\u201322, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012.  12", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "ICML, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Why does unsupervised pre-training help deep learning?", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "JMLR, vol", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "ICML, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "NIPS, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Multidigit number recognition from street view imagery using deep convolutional neural networks", "author": ["I.J. Goodfellow", "Y. Bulatov", "J. Ibarz", "S. Arnoud", "V. Shet"], "venue": "arXiv preprint arXiv:1312.6082, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and transferring mid-level image representation using convolution neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3531, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "CVPR, 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 1, pp. 221\u2013231, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Action recognition by learning temporal slowness invariant features", "author": ["L. Pei", "M. Ye", "X. Zhao", "Y. Dou", "J. Bao"], "venue": "The Visual Computer, pp. 1\u201310, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "ICML, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Large scale visual recognition challenge 2012.", "author": ["J. Deng"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Flickrtag prediction using multi-modal fusion and meta information", "author": ["Y.-C. Su", "T.-H. Chiu", "G.-L. Wu", "C.-Y. Yeh", "F. Wu", "W. Hsu"], "venue": "ACM MM, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "2013. [Online]. Available: http://caffe.berkeleyvision.org/,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba"], "venue": "PAMI, vol. 30, no. 11, pp. 1958\u20131970, 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1958}], "referenceMentions": [{"referenceID": 0, "context": "in recent years [1]\u2013[6] due to both the applicability of technology and real application needs.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "in recent years [1]\u2013[6] due to both the applicability of technology and real application needs.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "Many previous works focus on recognition in very specific domains such as action recognition [7], and the videos usually come from limited sources such as news, movies or recorded in lab environment [8], [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Many previous works focus on recognition in very specific domains such as action recognition [7], and the videos usually come from limited sources such as news, movies or recorded in lab environment [8], [9].", "startOffset": 199, "endOffset": 202}, {"referenceID": 8, "context": "Many previous works focus on recognition in very specific domains such as action recognition [7], and the videos usually come from limited sources such as news, movies or recorded in lab environment [8], [9].", "startOffset": 204, "endOffset": 207}, {"referenceID": 0, "context": "specific pooling method [1], [3], [10].", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "specific pooling method [1], [3], [10].", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "specific pooling method [1], [3], [10].", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "In addition to imagebased features, effort has been made to develop video specific features that capture information encoded in the temporal dimension [11], [12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "In addition to imagebased features, effort has been made to develop video specific features that capture information encoded in the temporal dimension [11], [12].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "In fact, some empirical results show that image-based features may perform as well as or even more robust than spatio-temporal features, especially when detecting complex events or high level semantic concepts in the video [1], [10], [13], [14].", "startOffset": 223, "endOffset": 226}, {"referenceID": 9, "context": "In fact, some empirical results show that image-based features may perform as well as or even more robust than spatio-temporal features, especially when detecting complex events or high level semantic concepts in the video [1], [10], [13], [14].", "startOffset": 228, "endOffset": 232}, {"referenceID": 12, "context": "In fact, some empirical results show that image-based features may perform as well as or even more robust than spatio-temporal features, especially when detecting complex events or high level semantic concepts in the video [1], [10], [13], [14].", "startOffset": 234, "endOffset": 238}, {"referenceID": 13, "context": "In fact, some empirical results show that image-based features may perform as well as or even more robust than spatio-temporal features, especially when detecting complex events or high level semantic concepts in the video [1], [10], [13], [14].", "startOffset": 240, "endOffset": 244}, {"referenceID": 14, "context": "Deep Learning as a learning paradigm has been shown effective in various domains including natural language processing [15], speech recognition [16], etc.", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Deep Learning as a learning paradigm has been shown effective in various domains including natural language processing [15], speech recognition [16], etc.", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "In computer vision, the great success of DCNs on the ImageNet dataset [17], [18] has attracted extensive research interest, and many further efforts have made DCN the state-of-the-art algorithm for various visual recognition tasks in static images.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "In computer vision, the great success of DCNs on the ImageNet dataset [17], [18] has attracted extensive research interest, and many further efforts have made DCN the state-of-the-art algorithm for various visual recognition tasks in static images.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Although efforts have been made to collect million scale video recognition benchmarks [13], the label of the dataset is still in video level.", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "image feature in previous research [1], we apply DCN on frame basis and aggregate frame-wise result using late fusion.", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "While some recent works exploit various aggregation methods such as recurrent neural network (RNN) [3], [5] over DCN, their models are still based on frame-wise DCN and can benefit from better DCN models obtained by our transfer learning approach.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "While some recent works exploit various aggregation methods such as recurrent neural network (RNN) [3], [5] over DCN, their models are still based on frame-wise DCN and can benefit from better DCN models obtained by our transfer learning approach.", "startOffset": 104, "endOffset": 107}, {"referenceID": 18, "context": "[19] proposed a greedy layerwise pre-training to overcome the problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Pre-training is believed to learn a better initialization that captures important patterns in the data, which facilitates the following supervised learning and leads to better generalizability [20].", "startOffset": 193, "endOffset": 197}, {"referenceID": 20, "context": "The unsupervisedly learned networks can be used for transfer learning, as shown in [21], [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "The unsupervisedly learned networks can be used for transfer learning, as shown in [21], [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "In computer vision, the most popular Deep Learning architecture is DCN [23].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "To avoid overfitting, convolution was introduced as a regularization on W based on the prior knowledge on the human visual system and the image signal [23], [24].", "startOffset": 151, "endOffset": 155}, {"referenceID": 23, "context": "To avoid overfitting, convolution was introduced as a regularization on W based on the prior knowledge on the human visual system and the image signal [23], [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 16, "context": "The most impressive breakthrough of DCN on visual recognition comes from its superior performance on the ImageNet dataset, where [17] and [18]", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "The most impressive breakthrough of DCN on visual recognition comes from its superior performance on the ImageNet dataset, where [17] and [18]", "startOffset": 138, "endOffset": 142}, {"referenceID": 24, "context": "It is known that meta-parameters have a great impact on the network performance [25]\u2013[27].", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "It is known that meta-parameters have a great impact on the network performance [25]\u2013[27].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "There exist several popular network architectures in the research community [17], [26], [28], and many previous works use", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "There exist several popular network architectures in the research community [17], [26], [28], and many previous works use", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "There exist several popular network architectures in the research community [17], [26], [28], and many previous works use", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "these configuration without further optimization [29], [30], Although being successful in many cases, most of the architectures are proposed without explanation or justification of the choice.", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "these configuration without further optimization [29], [30], Although being successful in many cases, most of the architectures are proposed without explanation or justification of the choice.", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "[28] provides high level arguments for their motivation for increasing network depth and connection design, but they do not provide experiment evidences for the particular choice of architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25], [26] provide systematic experiment results to justify their choice of network", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[25], [26] provide systematic experiment results to justify their choice of network", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "[27] optimizes the step size and receptive field of the convolution kernel in first layer, based on the model proposed in [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[27] optimizes the step size and receptive field of the convolution kernel in first layer, based on the model proposed in [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "[31] evaluates various configurations of DCN on image recognition including network architecture, but they only test three different architectures which provide limited information on the effect of meta-", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "This extension has been shown successful on various action recognition benchmarks [32]\u2013[34], but it does not perform well on recognizing more complex events [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "This extension has been shown successful on various action recognition benchmarks [32]\u2013[34], but it does not perform well on recognizing more complex events [13].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "This extension has been shown successful on various action recognition benchmarks [32]\u2013[34], but it does not perform well on recognizing more complex events [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "[13] proposes a more general framework for fusing multiple frames, and they evaluate different fusion strategies on million scale video dataset for recognizing sports event.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3], [14] exploits different pooling methods to combine frame features extracted by static image DCN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[3], [14] exploits different pooling methods to combine frame features extracted by static image DCN.", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "[4] combines DCN features with other hand crafted features using another neural network, where the video DCN representation is the average of frame-wise features extracted by static image DCN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[35] proposes a late fusion approach to combine local motion and appearance information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5], [36] use Long Short Term Memory network on top of the frame-wise DCN feature to model the long-term dependency of high level semantic concepts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "[5], [36] use Long Short Term Memory network on top of the frame-wise DCN feature to model the long-term dependency of high level semantic concepts.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "The goal is similar to the pre-training process in Deep Belief Network (DBN) and Stacked Auto-Encoder (SAE) in the sense that it improves the generalizability by learning a better intermediate representation [20].", "startOffset": 208, "endOffset": 212}, {"referenceID": 17, "context": "Because the convolution kernels in DCN learn important low level visual patterns in natural images [18], these convolution kernels serve", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "This is especially important for the higher layers in DCN, such as the fully connected layer, because these layers capture more complex patterns [18] that may not generalize well to other domains.", "startOffset": 145, "endOffset": 149}, {"referenceID": 36, "context": "that good initialization will lead to better performance [37].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "While previous researches focus on unsupervised pretraining for better representation learning, several parallel works have utilize the supervised pre-training approaches to address the same lack-of-training-data problem [13], [29], [30], [38].", "startOffset": 221, "endOffset": 225}, {"referenceID": 28, "context": "While previous researches focus on unsupervised pretraining for better representation learning, several parallel works have utilize the supervised pre-training approaches to address the same lack-of-training-data problem [13], [29], [30], [38].", "startOffset": 227, "endOffset": 231}, {"referenceID": 29, "context": "While previous researches focus on unsupervised pretraining for better representation learning, several parallel works have utilize the supervised pre-training approaches to address the same lack-of-training-data problem [13], [29], [30], [38].", "startOffset": 233, "endOffset": 237}, {"referenceID": 37, "context": "While previous researches focus on unsupervised pretraining for better representation learning, several parallel works have utilize the supervised pre-training approaches to address the same lack-of-training-data problem [13], [29], [30], [38].", "startOffset": 239, "endOffset": 243}, {"referenceID": 12, "context": "Also, some of our discoveries and conclusions are consistent to those in [13], [29] as described below.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "Also, some of our discoveries and conclusions are consistent to those in [13], [29] as described below.", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "ILSVRC2012 [39] is an 1k classes subset of ImageNet used in ImageNet Large Scale Visual Recognition Challenge.", "startOffset": 11, "endOffset": 15}, {"referenceID": 39, "context": "Therefore, it contains only weak supervision, and the label is known to be noisy because of user tagging behavior [40].", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "Columbia Consumer Video (CCV) Database [1] is a video dataset that targets on unconstrained consumer video content analysis.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "We use all 15 event classes for training and use the videos with event label following [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": "For networks with two convolution layers, we adopt an architecture similar to that of LeNet5 [23].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "The 4-convolution-layer network is similar to the architecture used in [17] with some simplification.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Note that we do not perform response normalization and grouping on the convolution kernels as in [17], and our max pooling is performed over 2x2 regions without overlapping.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "The 5-convolution-layer network is based on the architecture proposed in [17] with the following modification.", "startOffset": 73, "endOffset": 77}, {"referenceID": 40, "context": "The Caffe package [41] is used for learning the networks.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "The learning process is similar to that in [17], where we", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "The same dropout ratio, initial learning rate and momentum as in [17] are used.", "startOffset": 65, "endOffset": 69}, {"referenceID": 41, "context": "While it is claimed that these thumbnail images are still human recognizable [42], it is obvious that images with higher resolution contain more detailed information that may be helpful for visual recognition.", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "These results are identical with that in [13], which shows that update only the fully connected layers lead to better recognition accuracy, although we use totally different data and different network structures.", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "It is also consistent with [29], which shows", "startOffset": 27, "endOffset": 31}], "year": 2015, "abstractText": "Recognizing high level semantic concepts and complex events in consumer videos has become an important research topic with great application needs in the past few years. In this work, we apply Deep Convolution Network (DCN), the state-ofthe-art static image recognition algorithm, to recognize semantic concepts in unconstrained consumer videos. Our preliminary studies show that the annotation of video corpora is not sufficient to learn robust DCN models. The networks trained directly on the video dataset suffer from significant overfitting. The same lackof-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from weakly labeled images corpus to videos. The image corpus helps to learn important visual patterns for natural images, while these patterns are ignored by the models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. By transfer learning from image to video, we can learn a video recognizer with only 4k videos. Because the image corpus is weakly labeled, the entire learning process requires only 4k annotated instances, which is far less than the million scale image and video datasets used in previous works. The same approach can be applied to other visual recognition tasks where only scarce training data is available. Our extensive experiments on network configurations also lead to better understanding about the effect of various metaparameters in DCN, which enable better network architecture design for future researches and applications.", "creator": "LaTeX with hyperref package"}}}