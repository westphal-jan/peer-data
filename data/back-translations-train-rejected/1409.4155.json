{"id": "1409.4155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2014", "title": "Active Metric Learning from Relative Comparisons", "abstract": "This work focuses on active learning of distance metrics from relative comparison information. A relative comparison specifies, for a data point triplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to $x_k$. Such constraints, when available, have been shown to be useful toward defining appropriate distance metrics. In real-world applications, acquiring constraints often require considerable human effort. This motivates us to study how to select and query the most useful relative comparisons to achieve effective metric learning with minimum user effort. Given an underlying class concept that is employed by the user to provide such constraints, we present an information-theoretic criterion that selects the triplet whose answer leads to the highest expected gain in information about the classes of a set of examples. Directly applying the proposed criterion requires examining $O(n^3)$ triplets with $n$ instances, which is prohibitive even for datasets of moderate size. We show that a randomized selection strategy can be used to reduce the selection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-size problems. Experiments show that the proposed method consistently outperforms two baseline policies.", "histories": [["v1", "Mon, 15 Sep 2014 04:37:46 GMT  (105kb,D)", "http://arxiv.org/abs/1409.4155v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sicheng xiong", "r\\'omer rosales", "yuanli pei", "xiaoli z fern"], "accepted": false, "id": "1409.4155"}, "pdf": {"name": "1409.4155.pdf", "metadata": {"source": "CRF", "title": "Active Metric Learning from Relative Comparisons", "authors": ["Sicheng Xiong", "R\u00f3mer Rosales", "Yuanli Pei", "Xiaoli Z. Fern"], "emails": ["xfern}@eecs.oregonstate.edu", "rrosales@linkedin.com"], "sections": [{"heading": null, "text": "Categories and Subject Descriptions []; [] General Terms Keywords Active Learning, Relative Comparisons"}, {"heading": "1. INTRODUCTION", "text": "In this context, it is worth mentioning that this is a very complex issue."}, {"heading": "2. RELATED WORK", "text": "This study looks at pairs of information indicating that two instances are similar or not. [12] A distance metric is learned by minimizing the distances between instances, while the distances between instances are defined by relative comparisons. [14] Schultz et al. has formulated a limited optimization problem in which the constraints are defined by relative comparisons, and the goal is to achieve a distance metric that remains as close as possible to an un-weighted euclidean metric."}, {"heading": "3. PROPOSED METHOD", "text": "The problem dealt with in this paper is how to efficiently select triplets to learn a suitable metric, where efficiency is defined by the complexity of the query. Specifically, the less queries / questions are asked to achieve a certain performance, the higher the efficiency. A query is defined as a request to a user / oracle to mark a triplet. We consider this an iterative process, which implies that the decision to select the query should depend on what has been learned from all previous queries."}, {"heading": "3.1 Problem Setup", "text": "Given the data D = {x1, \u00b7 \u00b7, xn}, we assume that there is an underlying unknown class structure that assigns each instance to one of the C classes. We designate the unknown labels of the instances by y = {y1, \u00b7 \u00b7 \u00b7, yn}, with each label yi-Y, {1, \u00b7 \u00b7, C}. In the environment mentioned, it is impossible to directly observe the labels of the classes. Instead, the information about labels can only be obtained by relative comparison queries. A relative comparison query, called triplet rijk = (xi, xj, xk), can be interpreted as a question: \"Is xj more similar to the query than xk?\" In the face of a query rijk, an oracle / user returns an answer indicated by lijk-A, {yes, no, dk} based on the classes to which the three instances belong."}, {"heading": "3.2 A Probabilistic Model for Query Answers", "text": "To answer this question, we model the relationship between data points, their class names and the query answers in a probable way. We designate the names for all triplets existing in D with l = {lijk} and let lijk be random variables. We explicitly refer to the instance class names y = {y1, \u00b7 \u00b7, yn} in our probability model and assume that the query answer is independent of the data points containing their class names. Formally, the conditional probability of triplets and class names is defined by p (l, y | D) = ijk: lijk, l} p (lijk, yi, yj, yk)."}, {"heading": "3.3 Active Learning Criterion", "text": "In an active learning environment we try to make a triplet rijk from a pool of unlabeled triplets Ru's and query its its as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as"}, {"heading": "3.4 Scaling to Large Datasets", "text": "Based on the formulation so far, active learning algorithm work is most likely by selecting a triplet from the list of all unfilled triplets (1). (1) At worst, there are only three triplets in Ru for the oracle. (2) This is largely an open research question that goes beyond the scope of this paper. (1) This is a set of three triplets Rl = the set of all triplets generated by D. 2: Generate the subset Rp. (3: repeat 4: Use the steps in Section 3.5 to estimate p (Rl) for i = 1, \u00b7 5: for each triplet. (2: Generate the subset Rp) Ru by random sampling. (3: repeat 4: Use the steps in Section 3.5 to estimate p. (Rl) \u00b7 5: for each triplep."}, {"heading": "3.5 Probability Estimates and Implementation", "text": "DetailsThe formula above requires the estimation of multiple probabilities. In particular, in order to evaluate the objective value achieved by a given triplet (i, j, k), we must estimate p (yh = c | Rl) for h, j, k) and c (1,..., C), the probability of each of the three data points belongs to each of the possible classes, and if we estimate p (yh = c | Rl) for h, j, k), the probability of different responses to the given triplet as analyzed in Section 3.3. we will describe how these probabilities are estimated in this section. First, we note that if we calculate p (yh = c | Rl) for h, j, k) and c, the probability of different responses to the given triplet as analyzed in Section 3.3. We will describe how these probabilities are estimated in this section."}, {"heading": "3.6 Complexity Analysis", "text": "In this section, we analyze the runtime of our proposed algorithm. Lines 1 and 2 of our algorithm perform the initialization and random sampling of the pool, which take up O (n) time when | Rp | is set to pn, where p is a constant (in our experiment, we use p as 100). In line 3, we perform metric learning, apply k means with the distance metrics learned, and create the Random Forest (RF) classifier to estimate probabilities. The complexity of metric learning varies depending on which distance metric learning method is used. In our experiments, we use the method in [14], which presents metric learning as a quadratic programming problem, which can be solved in polynomial time based on the number of dimensions and linearly on n."}, {"heading": "4. EXPERIMENTS", "text": "In this section, we will evaluate experimentally the proposed method, which we will call info. In the following, we will first describe the experimental setup for our evaluation and then present and discuss the experimental results."}, {"heading": "4.1 Datasets", "text": "We evaluate our proposed method using seven benchmark datasets from the UCI Machine Learning Repository [6] and an additional larger dataset. Benchmark datasets include Breast Tissue (referred to as breast), Parksinons [9], Statlog image segmentation (referred to as segment), Soybean Small, Waveform Database Generator (referred to as wave), Wine and Yeast. We also use an additional real-world dataset called HJA Birdsong. This dataset contains audio segments extracted from the spectra of a collection of 10-second birdsong recordings and uses bird species as class markers [3]."}, {"heading": "4.2 Baselines and Experimental Setup", "text": "To the best of our knowledge, there is no active learning algorithm for relative comparisons. To investigate the effectiveness of our method, we compare it with several randomized basic policies 2: \u2022 Random: In each iteration, the learner randomly selects an blank triplet to ask. \u2022 Non-verbal: In each iteration, the learner selects the blank triplet that has the least overlap with previously selected triplets. If several decisions exist, we randomly select one. We use the metric distance learning algorithm introduced by [14]. This algorithm formulates a limited optimization problem where the limitations are defined by the triplets and the goals are as close as possible to learning a distance metric."}, {"heading": "4.3 Evaluation Criteria", "text": "The first is triplet accuracy, which evaluates how accurately the learned distance metric from the test data can predict the answer to a blank yes / no triplet. This is a common criterion used in previous studies of metric learning with relative comparisons [12, 14]. To create the triplet test set, we generate all triplets with yes / no labels from the test data for small datasets such as breast, Parkinson's, soybean, wine. For larger datasets, we randomly stitch 200K yes / no triplets from the test set to estimate the triplet accuracy. For the second measurement, we record the classification accuracy of a 1-nearest neighbor (1NN) classifier [4] based on the test data based on the learned distance metric. The purpose of this measurement is to investigate how much distance metrics can improve the classification accuracy."}, {"heading": "4.4 Results", "text": "The results are averaged over 50 random runs. Table 2 shows the 1NN classification accuracy, with 10, 20, 40, 60, 80 and 100 queries. We also list the 1NN accuracy without queries as the original performance. In each case, the best result (s) in obesity is / are highlighted based on paired t-tests at p = 0.05. The win / tie / loss result (from Table 2) for information against each method is summarized separately in Table 3.First, we observe that randomly and nonredundantly have decent performance in most datasets and the two baselines do not differ significantly from each other. These results are consistent with what has been reported in previous metric study studies that improve metric tendencies as we have more and more randomly selected yes / no triplets for the two major datasets that we generally do not know."}, {"heading": "4.5 Further Investigation", "text": "The following are the most important factors contributing to the success of \"Yes.\""}, {"heading": "5. CONCLUSION", "text": "This paper examined active metric learning from relative comparisons. Specifically, it addressed questions of form: Is instance i more similar to instance j than to instance k. the approach took advantage of the existence of an underlying (but unknown) class structure, which is implicitly applied by the user / oracle to answer such questions. In formulating active learning in this context, we found it interesting that some query answers cannot be used by existing metric learning algorithms: the answer does not know any useful limitations for them. The approach presented addressed this in a natural, reasonable manner. We proposed an information theory goal that explicitly measures how much information about class names can be obtained from the response to a query. In order to reduce the complexity of selecting a query, we also demonstrated that a simple sample scheme can provide excellent performance guarantees. Experimental results showed that the method selected by the researchers did not only contribute to improving the drill lines, but also to improving the accuracy of the spacing lines."}, {"heading": "6. REFERENCES", "text": "[1] S. Basu, A. Banerjee, and R. Mooney. Activesemi-supervision on Information theory for pairwise confined clustering. InProceedings of the SIAM International Conference on Data mining, pp. 333-344, 2004. [2] L. Breiman. Random forests. Machine learning, 45 (1): 5-32, 2001. [3] F. Briggs, X. Z. Fern, and R. Raich. Rank-loss support instance machines for miml instance annotation. In Proceedings of the 18th ACM SIGKDD international conference on knowledge discovery and data mining, KDD '12, pp. 534-542, New York, NY, USA, 2012. ACM. T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information theory, 13 (1): 21-27, 1967. [5] T. Cover and J. Thomas Elements of Theory. Wiley, 1991."}], "references": [{"title": "Active semi-supervision for pairwise constrained clustering", "author": ["S. Basu", "A. Banerjee", "R. Mooney"], "venue": " Proceedings of the SIAM International Conference on Data mining, pages 333\u2013344", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, 45(1):5\u201332", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Rank-loss support instance machines for miml instance annotation", "author": ["F. Briggs", "X.Z. Fern", "R. Raich"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201912, pages 534\u2013542, New York, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Transactions on Information theory, 13(1):21\u201327", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1967}, {"title": "Elements of Information Theory", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Constraint selection by committee: An ensemble approach to identifying informative constraints for semi-supervised clustering", "author": ["D. Greene", "P. Cunningham"], "venue": "pages 140\u2013151", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A sequential algorithm for training text classifiers", "author": ["D. Lewis", "W. Gale"], "venue": "Proceedings of ACM SIGIR International Conference on Research and development in information retrieval, pages 3\u201312", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection", "author": ["M. Little", "P. McSharry", "S. Roberts", "D. Costello", "I. Moroz"], "venue": "BioMedical Engineering OnLine, 6(1):23", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Active query selection for semi-supervised clustering", "author": ["P. Mallapragada", "R. Jin", "A. Jain"], "venue": "Proceedings of International Conference on Pattern Recognition, pages 1\u20134", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Psychometric Theory", "author": ["J. Nunnally", "I. Bernstein"], "venue": "McGraw Hill, Inc.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning sparse metrics via linear programming", "author": ["R. Rosales", "G. Fung"], "venue": "Proceedings of ACM SIGKDD International Conference on Knowledge discovery and data mining, pages 367\u2013373", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Toward optimal active learning through sampling estimation of error reduction", "author": ["N. Roy", "A. Mccallum"], "venue": "Proceedings of International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning a distance metric from relative comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "Advances in neural information processing systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Query by committee", "author": ["H. Seung", "M. Opper", "H. Sompolinsky"], "venue": "Proceedings of the annual workshop on Computational learning theory, pages 287\u2013294", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "The Journal of Machine Learning Research, 2:45\u201366", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"], "venue": "Advances in neural information processing systems, pages 521\u2013528", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Active constrained clustering by examining spectral eigenvectors", "author": ["Q. Xu", "M. Desjardins", "K. Wagstaff"], "venue": "Discovery Science, pages 294\u2013307", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Bayesian active distance metric learning", "author": ["L. Yang", "R. Jin", "R. Sukthankar"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "One category of distance learning algorithms [12, 14, 18] uses userprovided side information to introduce constraints or hints that are approximately consistent with the underlying distance.", "startOffset": 45, "endOffset": 57}, {"referenceID": 12, "context": "One category of distance learning algorithms [12, 14, 18] uses userprovided side information to introduce constraints or hints that are approximately consistent with the underlying distance.", "startOffset": 45, "endOffset": 57}, {"referenceID": 16, "context": "One category of distance learning algorithms [12, 14, 18] uses userprovided side information to introduce constraints or hints that are approximately consistent with the underlying distance.", "startOffset": 45, "endOffset": 57}, {"referenceID": 16, "context": "[18] used pairwise information specifying whether", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Alternatively, other studies such as [12, 14] consider constraints introduced by relative comparisons described by triplets (xi, xj , xk), specifying that instance or data point xi is more similar to instance xj than to instance xk.", "startOffset": 37, "endOffset": 45}, {"referenceID": 12, "context": "Alternatively, other studies such as [12, 14] consider constraints introduced by relative comparisons described by triplets (xi, xj , xk), specifying that instance or data point xi is more similar to instance xj than to instance xk.", "startOffset": 37, "endOffset": 45}, {"referenceID": 9, "context": "Research in psychology has revealed that people are often inaccurate in making absolute judgments, but are much more reliable when judging comparatively [11].", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": ", using user click-through data in information retrieval tasks [14]), for many real applications, acquiring such comparisons requires manual inspection of the instances.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "proposed one of the first formal approaches for distance metric learning with side information [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "Distance metric learning with relative comparisons has also been studied in different contexts [12, 14].", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "Distance metric learning with relative comparisons has also been studied in different contexts [12, 14].", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "formulated a constrained optimization problem where the constraints are defined by relative comparisons and the objective is to learn a distance metric that remains as close to an un-weighted Euclidean metric as possible [14].", "startOffset": 221, "endOffset": 225}, {"referenceID": 10, "context": "[12] proposed to learn a projection matrix from relative comparisons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "There is a large body of literature on active learning for supervised classification [15].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "One common strategy for selecting a data instance to label is uncertainty sampling [8] where the instance with the highest label uncertainty is selected to be labeled.", "startOffset": 83, "endOffset": 86}, {"referenceID": 14, "context": "In Query-bycommittee [16], multiple models (committee) are trained on different versions of the labeled data, and the unlabeled instance with the largest disagreement among the committee members is queried for labeling.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Other representative techniques include selecting the instance that is closet to the decision boundary (min margin) [17], and selecting the instance that leads to the largest expected error reduction [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "Other representative techniques include selecting the instance that is closet to the decision boundary (min margin) [17], and selecting the instance that leads to the largest expected error reduction [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "In relation to our work, most previous approaches concentrate on active selection of pairwise constraints [1, 7, 10, 19].", "startOffset": 106, "endOffset": 120}, {"referenceID": 5, "context": "In relation to our work, most previous approaches concentrate on active selection of pairwise constraints [1, 7, 10, 19].", "startOffset": 106, "endOffset": 120}, {"referenceID": 8, "context": "In relation to our work, most previous approaches concentrate on active selection of pairwise constraints [1, 7, 10, 19].", "startOffset": 106, "endOffset": 120}, {"referenceID": 17, "context": "In relation to our work, most previous approaches concentrate on active selection of pairwise constraints [1, 7, 10, 19].", "startOffset": 106, "endOffset": 120}, {"referenceID": 18, "context": "In the context of distance metric learning, an active learning strategy was proposed in [20] whithin the larger context of a Bayesian metric learning formulation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "This oracle is consistent with that used in prior work [12, 14], except for one difference.", "startOffset": 55, "endOffset": 63}, {"referenceID": 12, "context": "This oracle is consistent with that used in prior work [12, 14], except for one difference.", "startOffset": 55, "endOffset": 63}, {"referenceID": 4, "context": "More specifically, we employ the mutual information (MI) function [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 12, "context": "In this paper, we use an existing metric learning algorithm by Schultz and Joachims [14] for the first step.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "In our experiments we use the method in [14], which casts", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "Building the RF takes O(NTn logn), where NT is the number of decision trees in RF and n is the number of instances [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "The benchmark datasets include Breast Tissue (referred to as Breast), Parksinons [9], Statlog image segmentation (referred to as Segment ), Soybean small, Waveform Database Generator (V.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "This dataset contains audio segments extracted from the spectrums of a collection of 10-second bird song recordings and uses the bird species as class labels[3].", "startOffset": 157, "endOffset": 160}, {"referenceID": 12, "context": "We use the distance metric learning algorithm introduced by [14].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "We have also considered an alternative metric learning method introduced by [12] obtaining consistent results.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "This is a common criterion used in previous studies on metric learning with relative comparisons [12, 14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 12, "context": "This is a common criterion used in previous studies on metric learning with relative comparisons [12, 14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 3, "context": "For the second measure, we record the classification accuracy of a 1-nearest neighbor (1NN) classifier [4] on the test data based on the learned distance metric.", "startOffset": 103, "endOffset": 106}], "year": 2014, "abstractText": "This work focuses on active learning of distance metrics from relative comparison information. A relative comparison specifies, for a data point triplet (xi, xj , xk), that instance xi is more similar to xj than to xk. Such constraints, when available, have been shown to be useful toward defining appropriate distance metrics. In real-world applications, acquiring constraints often require considerable human effort. This motivates us to study how to select and query the most useful relative comparisons to achieve effective metric learning with minimum user effort. Given an underlying class concept that is employed by the user to provide such constraints, we present an information-theoretic criterion that selects the triplet whose answer leads to the highest expected gain in information about the classes of a set of examples. Directly applying the proposed criterion requires examining O(n) triplets with n instances, which is prohibitive even for datasets of moderate size. We show that a randomized selection strategy can be used to reduce the selection pool from O(n) to O(n), allowing us to scale up to larger-size problems. Experiments show that the proposed method consistently outperforms two baseline policies.", "creator": "LaTeX with hyperref package"}}}