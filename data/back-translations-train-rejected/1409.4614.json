{"id": "1409.4614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2014", "title": "Lexical Normalisation of Twitter Data", "abstract": "Twitter with over 500 million users globally, generates over 100,000 tweets per minute. The 140 character limit per tweet has, perhaps unintentionally, encourages users to use shorthand notations and to strip spellings to their bare minimum \"syllables\" or elisions e.g. \"srsly\". The analysis of twitter messages which typically contain misspellings, elisions, and grammatical errors, poses a challenge to established Natural Language Processing (NLP) tools which are generally designed with the assumption that the data conforms to the basic grammatical structure commonly used in English language. In order to make sense of Twitter messages it is necessary to first transform them into a canonical form, consistent with the dictionary or grammar. This process, performed at the level of individual tokens (\"words\"), is called lexical normalisation. This paper investigates various techniques for lexical normalisation of Tweeter data and presents the findings as the techniques are applied to process raw data from Tweeter.", "histories": [["v1", "Tue, 16 Sep 2014 12:59:07 GMT  (495kb)", "http://arxiv.org/abs/1409.4614v1", "4 pages"], ["v2", "Wed, 17 Sep 2014 02:48:53 GMT  (480kb)", "http://arxiv.org/abs/1409.4614v2", "4 pages; removed typos"], ["v3", "Sun, 13 Sep 2015 04:56:13 GMT  (466kb)", "http://arxiv.org/abs/1409.4614v3", "4 pages; removed typos"], ["v4", "Sun, 20 Sep 2015 01:11:53 GMT  (466kb)", "http://arxiv.org/abs/1409.4614v4", "Removed typos"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bilal ahmed"], "accepted": false, "id": "1409.4614"}, "pdf": {"name": "1409.4614.pdf", "metadata": {"source": "CRF", "title": "Lexical Normalisation of Twitter Data", "authors": ["Bilal Ahmed"], "emails": ["bahmad@student.unimelb.edu.au"], "sections": [{"heading": null, "text": "Twitter, with over 500 million users worldwide, generates over 100,000 tweets per minutes1. The 140-character limit per tweet has encouraged users, perhaps unintentionally, to use shorthand notations and reduce spellings to their minimal \"syllables\" or omissions, such as \"srsly.\" Analyzing Twitter messages, which typically contain spelling errors, omissions, and grammatical errors, poses a challenge to established tools for natural language processing (NLP), which are generally designed on the assumption that the data matches the basic grammatical structure commonly used in the English language. To make Twitter messages meaningful, it is necessary to first transform them into a canonical form consistent with the dictionary or grammar. This process, carried out at the level of individual tokens (\"words\"), is referred to as lexical normalization."}, {"heading": "1. Introduction", "text": "A tweeter message or \"tweet\" consists of 140 characters or less and generally contains hash tags and @ symbols. To lexically analyze a tweeter message, each token must be identified on a case-by-case basis before normalization techniques are used to correct spelling errors and understand the various abbreviations and omissions commonly used in tweeter messages. The following sections describe the different techniques applied to identity: \"known\" or \"in-vocabulary\" words, punctuation and special symbols (both general and tweeter-specific), and candidates for normalization. We then apply various normalization techniques to correct tokens from the vocabulary (\"OOV\")."}, {"heading": "2. In Vocabulary Tokens", "text": "The first step is to identify tokens or words that match 1 Twitter in numbers, The Telegraph, March 2013. A token is marked as \"in the vocabulary (\" IV \") if an exact match is found. To this end, we have used a lexicon of 115,326 words (words.txt) to identify words\" in the vocabulary. \"Tokens that are outside this vocabulary are then considered candidates for normalization and are further edited or marked as non-candidates if they are deemed unsuitable for normalization."}, {"heading": "3. Non-Candidate Tokens", "text": "In addition to the usual punctuation marks, a Twitter message or tweet typically contains hash tags, the \"#\" symbol to identify keywords or topics in a tweet, and the \"@\" symbol followed by a user's Twitter username to refer to a user in replies or comments, and the tokens are analyzed using regular expressions to identify special characters, punctuations, and tweet-specific symbols, which are marked as non-candidates (\"NO\") and not processed for normalization."}, {"heading": "4. Normalisation of Out of Vocabulary Tokens", "text": "In fact, it is such that it is a matter of a way in which people in the world put themselves in a different world, in which they put themselves in another world, in which they put themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they live in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they find themselves in another world, in which they live in another world, in which they live in another world, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5. Conclusions", "text": "Normalizing high-accuracy tokens can be quite a challenge, given the number of possible variations on a given token. To make matters worse, social media tools like Twitter are increasingly using omissions and acronyms. It is important to consider the various normalization techniques available and select those that best suit the purpose. A mix of techniques such as editing distance and Soundex or Refined Soundex usually leads to better accuracy compared to their sole application. Techniques based on context such as Peter Norvig's algorithm increase the accuracy of normalization. Likewise, N-gram matching, though exhaustive, can be optimized to achieve accurate results based on context."}], "references": [{"title": "Using the Web for Language Independent Spellchecking and Autocorrection Google Inc., Pyrmont NSW 2009, Australia", "author": ["G. Ellis"], "venue": null, "citeRegEx": "Ellis,? \\Q2013\\E", "shortCiteRegEx": "Ellis", "year": 2013}], "referenceMentions": [], "year": 2013, "abstractText": "Twitter with over 500 million users globally, generates over 100,000 tweets per minute. The 140 character limit per tweet has, perhaps unintentionally, encourages users to use shorthand notations and to strip spellings to their bare minimum \u201csyllables\u201d or elisions e.g. \u201csrsly\u201d. The analysis of twitter messages which typically contain misspellings, elisions, and grammatical errors, poses a challenge to established Natural Language Processing (NLP) tools which are generally designed with the assumption that the data conforms to the basic grammatical structure commonly used in English language. In order to make sense of Twitter messages it is necessary to first transform them into a canonical form, consistent with the dictionary or grammar. This process, performed at the level of individual tokens (\u201cwords\u201d), is called lexical normalisation. This paper investigates various techniques for lexical normalisation of Tweeter data and presents the findings as the techniques are applied to process raw data from Tweeter.", "creator": "Microsoft\u00ae Word 2010"}}}