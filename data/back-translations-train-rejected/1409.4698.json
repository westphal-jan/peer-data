{"id": "1409.4698", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2014", "title": "A Mixtures-of-Experts Framework for Multi-Label Classification", "abstract": "We develop a novel probabilistic approach for multi-label classification that is based on the mixtures-of-experts architecture combined with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relations from multi-label data using the efficient tree-structured classifiers, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and build a more accurate model. We develop and present algorithms for learning the model from data and for performing multi-label predictions on future data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods.", "histories": [["v1", "Tue, 16 Sep 2014 16:52:14 GMT  (223kb,D)", "http://arxiv.org/abs/1409.4698v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["charmgil hong", "iyad batal", "milos hauskrecht"], "accepted": false, "id": "1409.4698"}, "pdf": {"name": "1409.4698.pdf", "metadata": {"source": "CRF", "title": "A Mixtures-of-Experts Framework for Multi-Label Classification", "authors": ["Charmgil Hong"], "emails": [], "sections": [{"heading": null, "text": "We are developing a new probabilistic approach to multi-label classification based on the mixtures-of-experts architecture in combination with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relationships of multi-label data using efficient tree-structured classifiers, while the blend-of-experts architecture aims to balance the tree-structured constraints and build a more precise model. We are developing and presenting algorithms to learn the model from data and to make multi-label predictions about future data instances. Experiments with multiple benchmark data sets show that our approach delivers highly competitive results and outperforms existing state-of-the-art multi-label classification methods."}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want. (...) It's not that they want it. (...) It's not that they want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. (...) It's that they don't want it. \""}, {"heading": "2 PROBLEM DEFINITION", "text": "Multi-Label Classification (MLC) is a classification problem in which each data instance is associated with a subset of labels from a range of possible labels. Let's leave d = | L |. We can define d binary class variables Y1,..., Yd, where the value of Yi in instance x indicates whether the i-th label in L is present in x or not. We get labeled training data D = {x (n), y (n)} Nn = 1,..., where x (n) = (n) 1,..., x (n) m) the m-dimensional characteristic vector of the n-th instance (the instance) andy (n)} n,..., y (n) d is its d -dimensional class vector (the output) yyyy.We want to learn a function h that matches x and assigns each instance d, represented by its characteristic vector, class}, a class \u2192 yh: 1."}, {"heading": "3 RELATED RESEARCH", "text": "In this section, we review the related research in MLC and outline the main differences between our approaches. Previous MLC methods ignore the relationships between classes and learn to predict each class individually (Clare and King, 2001; Boutell et al., 2004). Zhang and Zhou (2007) presented the various methods that each classroom carries within it by combining KNN with Bayesian Inference. An approach that broadens the scope for action by incorporating an intermediate layer of classifiers was proposed by Godbole and Sarawagi, 2004) and later by Cheng and Hu-llermeier, 2009). The main drawback of these methods is that class dependencies are either not represented or are indirectly represented in a limited way."}, {"heading": "4 PRELIMINARY", "text": "The MLC solution we propose in this paper combines multiple base MLC classifiers with the mixtures of expert functions (ME) depending on ME (ME) (Jacobs et al., 1991) architecture. The base classifiers we use are based on the conditional treestructured Bayesian Networks (CTBN) (Batal et al., 2013).The model is defined by the following decomposition: P (y | x) = K-class is a blending model consisting of a number of experts who are combined via gating (or switching) to represent the conditional distribution P (y | x).The model is defined by the following definition: P (y | x) = K-class used by different experts."}, {"heading": "5 PROPOSED SOLUTION", "text": "In this section, we will develop a Multi-Label Mixtures-ofExperts (ML-ME) model that uses the ME framework in combination with the CTBN classifiers to improve the classification accuracy of MLC tasks and develop algorithms for learning and predicting them. Our main motivation is to take advantage of the divide-and-conquer principle, which states that a large, complex problem can be broken down and effectively solved by simpler sub-problems. Specifically, we want to precisely model the relationships between inputs X and class variables Y by learning multiple CTBN models and improving their prediction capability by combining their results. In Section 5.1, we will describe the mix defined by the ML-ME model. In Section 5.2 to 5.4, we will present the learning and prediction algorithms for the ML-ME model."}, {"heading": "5.1 REPRESENTATION", "text": "By following the definition of ME in Equation (2), MLME defines the multivariate posterior distribution of class vector y = (y1,..., yd) as: P (y | x) = K \u2211 k = 1 gk (x) P (y | x, Ek), (4) where P (y | x, Ek) is the common conditional distribution, defined by the k-th expert Ek and gk (x) = P (Ek | x) is the gating function that reflects how much the k-th expert should contribute to predicting classes for input x. In this work, we model the gating functions for the various experts using the softmax function, also called normalized exponentials: gk (x) = exp (Gkx)."}, {"heading": "5.2 PARAMETER LEARNING FOR FIXED STRUCTURES", "text": "In this section we describe how to learn the parameters of ML-ME when the structures of the individual CTBN experts are known and fixed. We return to the structural learning problem in Section 5.3.\u0432 = {1). (D; T) denotes the set of all parameters of the ML-ME model. (Our goal is to find the parameters that optimize the log probability of training data: l (D; E) = N \u00b2 n = 1 logP (y (n) | x (n)))) By replacing the common probability with the definition of ML-ME (Equation (6)), we obtain: l (D; E) = N \u00b2 n = 1 log K \u00b2 k = 1 gk (n)))) P (y (n) | x (n), Tk) we refer to the equation (7) as the observed logging."}, {"heading": "5.2.1 Complexity", "text": "E-step We calculate h (n) k for each instance on each CTBN expert. This requires O (md) multiplications. Consequently, the complexity of a single E-step is O (KNmd).M-step The calculation of the derivative in Equation (11) requires O (mN) multiplications, therefore the optimization of HG O (mNl) operations, where l is the number of L-BFGS steps. To learn HT, we optimize an instance-weighted logistic regression for each node of each of the K experts, which requires learning O (Kd) logistic regression models."}, {"heading": "5.3 STRUCTURE LEARNING", "text": "In the previous section, we described the parameter learning of ML-ME by assuming that we have repaired the individual CTBN structures. In this section, we will present how to automatically learn CTBN structures from data. In short, we will apply a sequential boosting-like heuristics; i.e., we will learn a structure that focuses on \"hard instances\" that tend to misclassify earlier CTBNs with each iteration. In the following, we will first describe how to learn a single CTBN structure from instance-weighted data, and then describe how to reweight the instances and gradually add new structures to the ML-ME model."}, {"heading": "5.3.1 Learning a Single CTBN Structure on Weighted Data", "text": "To learn the CTBN structure that best approximates the weighted data, we find the structure that maximizes the weighted conditional log probability (WCLL) (WCLL), where D = {x (n), y (n)} Nn = 1 is the data, and vice versa, Nn = 1 is the instance weight. Note that we continue to use D in training data Dtr and the held data Dh.Given a CTBN structure T, we train their parameters using Dtr, which is the instance-weighted logistic regression using Dtr and the corresponding instance weights. On the other hand, we use WCLL of Dh to define the score that measures the quality of T. Score (T) = n, logP (n) logP (n), which is the balance of Vtr and the corresponding instance weights of p."}, {"heading": "5.3.2 Learning Multiple CTBN Structures", "text": "In order to obtain multiple, effective CTBN structures for the ML-ME model, we apply the algorithm described above several times with different instance weights. We assign weights in such a way that we assign higher weights to poorly predicted instances; and we assign lower weights to well predicted instances. We start by assigning all instances of uniform weights (i.e., all instances are a priori equally important).\u03c9 (n) = 1 / N: n = 1,..., NUsing this initial set of weights, we find the initial CTBN structure T1 (and its parameters successT1) and set the current model M to T1. We then estimate the prediction error margin \u03c9 (n) = 1 \u2212 P (y (n) | x (n), M) for each instance and renormalize the initial CTBN structure T1 (and its parameters successT1) = 1. We then repeat our structural algorithm \u03c9 (y (n), after repeating the T2 and the current structure of the TN), we learn to repeat the {T2 and T2} according to the current structure of the BN."}, {"heading": "5.3.3 Complexity", "text": "In order to learn a single CTBN structure, we have to calculate the edge weights for the complete graph G, for which P (Yi | X, Yj) must be estimated for all d2 class pairs. The maximum branching in G can be determined in O (d2) by means of (Tarjan, 1977). To learn K-CTBN structures for the mixture, we repeat these steps K times. Therefore, the total complexity O (Kd2) is times as high as the complexity of learning logistic regression."}, {"heading": "5.4 PREDICTION", "text": "To make a prediction for a new instance x, we want to find the MAP assignment of the class variables (see Equation (1)). In general, this requires evaluating all possible assignments of values to d class variables, which is exponentially in.An important advantage of the CTBN model is that the MAP inference can be performed more efficiently by avoiding a blind enumeration of all possible assignments. Specifically, the MAP inference to a CTBN is linear in the number of classes (O (d)) when implemented with a variant of the max sum algorithm on a tree structure (Batal et al., 2013).However, our ML-ME model consists of several CTBNs, and the MAP solution may ultimately require enumerating the exponentially many class assignments."}, {"heading": "6 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 DATA", "text": "We use ten publicly available MLC datasets from different areas, including music detection, semantic caption, biology, and text classification. Table 2 summarizes the properties of the datasets, showing the number of instances (N), the number of attribute variables (m), and the number of class variables (d). In addition, it shows two statistics: 1) label cardinality (LC), i.e. the average number of labels per instance, and 2) distinct label sets (DLS), i.e. the number of different class configurations that occur in the data. Note that we used the 10 most common labels for RCV1 datasets."}, {"heading": "6.2 METHODS", "text": "We compare the performance of our proposed method, which we refer to as ML-ME, with the following MLC methods: \u2022 Binary Relevance (BR) (Boutell et al., 2004; Clareand King, 2001) \u2022 Classification with Heterogeneous Features (CHF) (Godbole and Sarawagi, 2004) \u2022 Multi-Label k-Nearest Neighbor (MLKNN) (Zhang and Zhou, 2007) \u2022 Instance-Based Learning by Logistic Regression (IBLR) (Cheng and Hu \u00fcllermeier, 2009) \u2022 Classifier Chains (CC) (Read et al., 2009) \u2022 Ensemble of Classifier Chains (ECC) (Read et al., 2009) \u2022 Probabilistic Classifier Chains (PCC) (Dembczynski et al., 2010) \u2022 Maximum Margin Output Coding (MMOC) (Zhang and Schneider, 2012) \u2022 CME components (BN)."}, {"heading": "6.3 EVALUATION MEASURES", "text": "Evaluating the performance of MLC methods is more difficult than traditional classification methods, and the most appropriate metric is the Exact Accuracy of Match (EMA), which calculates the percentage of cases whose predicted output vectors are exactly the same as their true class vectors (i.e., all classes are correctly predicted).This metric is appropriate for MLC because it evaluates the success of the method in finding the mode of P (X | Y) (see Section 2). However, the EMA may be too hard, especially if the output dimensionality is high. An alternative metric is the conditional log probability loss (CLL loss), which calculates the negative conditional log probability of the test cases: CLL loss = N = 1 \u2212 log (P (y (n) | x (n))))) CLL loss evaluates how much probability mass is given to the true label vectors (the higher the probability, the lower the loss)."}, {"heading": "6.4 RESULTS", "text": "Tables 3, 4, 5 and 6 show the performance of all methods in terms of EMA, CLL loss, micro-F1 and macro-F1, respectively. All results are obtained with tenfold cross-validation. To evaluate the statistical significance in the performance measurement differences, we use paired test at 0.05 significance level. We use markers * / ~ to indicate whether ML-ME is statistically superior / inferior to compare the compared methodology. Note that we only show the results of the MMOC on four data sets (emotions, image, scene and yeast) because it is not ready on the rest of the data sets (MMOC has not completed a round of learning within 24 hours). Also, PCC has failed to evaluate the enron dataset that it has all 253 possible class assignments that are clearly impracticable."}, {"heading": "7 CONCLUSION", "text": "In this paper, we have proposed a new likely approach to the multi-label classification problem. Our approach models different input-output relations using conditional tree-structured Bayesian networks, while the mixtures-of-experts architecture aims to compensate for the tree-structured constraints and as a result achieve a more accurate model. We have formulated and developed the algorithms for learning the model of data, and for performing multi-label predictions of future data instances. Our experiments on a wide range of datasets showed that our approach exceeds several state-of-the-art methods and produces more reliable probability estimates."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "We develop a novel probabilistic approach for multi-label classification that is based on the mixtures-of-experts architecture combined with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relations from multi-label data using the efficient tree-structured classifiers, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and build a more accurate model. We develop and present algorithms for learning the model from data and for performing multi-label predictions on future data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods.", "creator": "TeX"}}}