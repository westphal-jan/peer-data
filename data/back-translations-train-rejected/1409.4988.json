{"id": "1409.4988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2014", "title": "An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for Clusters Mining and Knowledge Discovery", "abstract": "We propose a multi-agent algorithm able to automatically discover relevant regularities in a given dataset, determining at the same time the set of configurations of the adopted parametric dissimilarity measure yielding compact and separated clusters. Each agent operates independently by performing a Markovian random walk on a suitable weighted graph representation of the input dataset. Such a weighted graph representation is induced by the specific parameter configuration of the dissimilarity measure adopted by the agent, which searches and takes decisions autonomously for one cluster at a time. Results show that the algorithm is able to discover parameter configurations that yield a consistent and interpretable collection of clusters. Moreover, we demonstrate that our algorithm shows comparable performances with other similar state-of-the-art algorithms when facing specific clustering problems.", "histories": [["v1", "Wed, 17 Sep 2014 14:39:37 GMT  (1014kb)", "http://arxiv.org/abs/1409.4988v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.MA", "authors": ["filippo maria bianchi", "enrico maiorino", "lorenzo livi", "antonello rizzi", "alireza sadeghian"], "accepted": false, "id": "1409.4988"}, "pdf": {"name": "1409.4988.pdf", "metadata": {"source": "CRF", "title": "An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for Clusters Mining and Knowledge Discovery", "authors": ["Filippo Maria Bianchi", "Enrico Maiorino", "Lorenzo Livi", "Antonello Rizzi", "Alireza Sadeghian"], "emails": ["filippo.binachi@ryerson.ca", "enrico.maiorino@uniroma1.it", "llivi@scs.ryerson.ca", "antonello.rizzi@uniroma1.it", "asadeghi@ryerson.ca"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.49 88v1 [cs.LWe propose a multi-agent algorithm that is capable of automatically detecting relevant regularities in a given dataset and, at the same time, defining the set of configurations of the assumed parametric measure of dissimilarity, resulting in compact and separate clusters. Each agent works independently by conducting a Markovian random walk on a suitable, weighted diagram. Results show that the algorithm is able to detect parameter configurations caused by the specific parameter configuration of the dissimilarity measure assumed by the agent, each looking for a cluster and making decisions. Results show that the algorithm is able to detect parameter configurations corresponding to the author by e-mail: filippo.binachi @ ryerson.ca (Filippo Maria Bianchi), riencoiorco.maior1.ino @ unirouniviino."}, {"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "1.1. Related Works", "text": "The work we present in this paper relates to various topics, in particular graph clustering, conductivity evaluation, metric learning and agent-based computing. At best, it has not been possible to identify other papers that address the problem of clustering and knowledge-gathering with approaches similar to those we propose. However, the aim of this section is to help the reader to put our work into context and correctly identify the concepts to which our work relates. In particular, LD-ABCD identifies clusters on a data set represented by a labeled graph: graph clustering is a known problem and has been addressed in many other papers [10, 46, 18, 20, 21, 38]. Such clusters are discovered by various agents operating according to a paradigm inspired by the multiagent systems that can be found in literature: graph clustering is a known problem and cluster formation is not identical."}, {"heading": "2. The Proposed LD-ABCD Algorithm", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2.1. Graph Construction", "text": "Suppose an agent ai is equipped with the PC m (i) j, and let S be the dataset under Analysis, with n = | S |. The corresponding weighted graph Gj = (V, E, w) is described by the vertices V, each of which represents a pattern in S, and by the edges E, which are weighted by the implementation of w (\u00b7) as an exponential kernel: w (elk; m (i) j) = exp (\u2212 \u03c4exp \u00b7 d (xl, xk; m (i) j)))))). (1) Setting the parameter \u03c4exp \u2265 0 is an important problem and will be discussed later in Sec 2.2. A weighted graph can be described by the n \u00b7 n weighted adjacence matrix Aj, defined as: Aj (l, k) = w (elk; m (i) j)."}, {"heading": "2.2. Random Walk for Cluster Search", "text": "To perform a RW on Gj, we must define the so-called transition matrix [32], Mj, which is used by an agent to navigate between the vertices. Mj is defined as follows: Mj = D \u2212 1 j Aj, (3) where Dj is the (diagonal) degree matrix: Dj (l, l) = \u2211 | V | k = 1Aj (l, k). An RW can effectively be characterized by exploiting the stationary distribution (SD) of the Markov process on which the RW is based. SD can be interpreted as the left eigenvector of Mj, which is associated with the largest eigenvalue, i.e., 1. Every complete and non-bifurcated graph has a stationary distribution [32], which can be conveniently defined by exploiting the so-called degree distribution (vl) = D (l) = D (l, l)."}, {"heading": "2.3. Cluster Quality Evaluation", "text": "An agent ai generates a cluster chj during a RW performed on Gj with the PC m (i) j, which consists directly of the series of wells of the subgraph ghj visited during the RW (see figure 2). In the following, we will refer equally to ghj and chj. Once a cluster chj is returned by an agent ai, the cluster can either be accepted or rejected, depending on its quality. Intuitively, a cluster is considered good if it contains several elements that are also very similar to each other, according to the current PC. A well-established measure for evaluating the quality of a cluster associated with a subgraph of a larger graph is conductivity [27], which quantifies how well the subgraph kneels internally and how many edges (with their associated weights) are cut."}, {"heading": "2.4. Energy Update", "text": "Setting an appropriate value for the (maximum) length of an RW is another important issue that needs to be taken into account, since it is exclusively related to the typical size of the returned cluster / subgraph. A set called Energy ei determines how many steps an agent ai can perform during a RW. Energy is initialized to a value einit and it is modified successively at each step of the RW. As an agent, the agent visits the graph, it builds a subgraph ghj that is visited, increases its size and modifies its current conductivity accordingly. In particular, when a vertex vl is inserted in ghj, the conductivity of the subgraph increases when vl is far away (i.e., very different in our setting) from the other entries of ghj; otherwise, we will decrease the conductivity, since the graph is complete, we insert a subgraph vl to a subgraph ghj."}, {"heading": "2.5. Selection of New PCs", "text": "A RW is terminated when the energy ei reaches a value lower or equal to zero. The subset of corners visited forms the resulting cluster, chj, the quality of which is evaluated according to Equation 7. If CQ (chj) is greater or equal to \u03c4CQ, the cluster is added to the collection of good clusters discovered by the agents, along with the PC m (i) j used by the agent to detect such a cluster. Since it is likely that a dataset contains more than one cluster of elements similar to w.r.t. of the same PC, it is reasonable to assume that if a PC has led m (i) j to the identification of a good cluster, it can be further exploited to detect additional good clusters within the same dataset. Then, if a cluster is accepted, we restore the original energy quantity of the agent, i.e. we set ei = einit and start a new RW variant that implies the agent in the same weighted diagram."}, {"heading": "2.6. Aggregation of Clusters/PCs", "text": "It is therefore necessary to define a dissimilarity between the metrics that we present in a single meta-cluster group, the intersection of which is sufficiently high."}, {"heading": "2.7. Convergence Criterion", "text": "In order to determine the convergence criterion of LD-ABCD, we have decided to analyze how the meta-clusters evolve, rather than looking at the individual clusters returned by the agents. In fact, a single cluster returned by an agent may differ from the existing by very few elements, making it difficult to determine whether it is an effectively new cluster. Agents terminate the search if for a certain period of time, defined by the integer value threshold, no new meta-clusters are generated and the average cluster quality of the existing meta-clusters does not increase. The luster quality of a meta-cluster is evaluated as an average of the cluster qualities of each associated cluster. In particular, if an agent successively returns at peak times, a cluster associated with an existing meta-cluster can reach the entire meta-cluster and it does not improve its average CQ, or the cluster is too low due to its CQ."}, {"heading": "2.8. Analysis of Computational Complexity", "text": "In this section we will examine the temporal and spatial complexity of the LD-ABCD. Concerning space occupation, the upper limit in the storage of the weighted adjacency matrix, A, which each agent must use to represent the graph, is O (n2), where this quantity is modified (influenced by the experimental setting of the algorithms and by the intrinsic random nature of the RWs).The length of a typical RW refers to the energy of the agent and how this quantity is modified (influenced by the experimental setting of the algorithms)."}, {"heading": "3. LD-ABCD with Exploration\u2013Exploitation Agents", "text": "In this section, we propose an alternative approach to exploring the PC space, which we have referred to as exploration and exploitation. An agent operating according to the well-known Metropolis-Hastings algorithms is referred to as an \"Explorer,\" commonly used in statistical physics; the exploration strategy is consistent with the unified search described in Sec; and it is intended to perform a wide-area exploratory search in the PC space; an explorer operating according to the exploration strategy is referred to as an \"Explorer.\" Every time an RPC is identified by an explorer, it is stored in a common data structure to allow for successive improvements in exploitation; and an agent randomly evaluating the strategy evaluates several different PCs."}, {"heading": "4. Experiments", "text": "In this section, we will discuss the experiments being conducted to evaluate the performance of (both variants of) \u03b2-ABCD, with the results primarily discussed in Sec. 4.1 to evaluate the quality of clusters found by LD-ABCD on some known benchmarking datasets. However, we will offer a comparison with state-of-the-art graph-based and RW-based algorithms via a specific setting of clusters where patterns are labeled with ground-level class names for performance evaluation. Then, in Sec. 4.2, we will present some experiments that underscore the ability of our system to detect relevant information in noisy datasets. It is noteworthy that the identification of relevant clusters, along with the PCs used to detect such clusters, provide semantic characterization and a high-level description of the data. In Sec. 4.3, we will demonstrate the ability of LD-ABCD to detect multiple PCs that characterize individual clusters, and then define a relationship between the three containing them."}, {"heading": "4.1. Evaluating the Purity of the RWs", "text": "We have looked at four different real data sets from the UCI Machine Learning Repository (7), which we consider to be two different types of data processing: Wine, Breast Cancer, Iris, and E-Coli. We have decided to use the above data patterns because they are very well known, easy to obtain, and for some of them it has been possible to compare the results of other algorithms that perform clustering with an RW [2]. All datasets contain described patterns organized in different classes. As we have described in Sec, the LD-ABCD algorithm uses the CQ (7) - a criterion based solely on conductivity evaluation - for the acceptance or rejection of clusters that are identified during the RWs. In the following experiments we demonstrate the reliability of our (unattended) cluster acceptance using the superordinate information of the class labels."}, {"heading": "4.2. Discovering Relevant PCs", "text": "In this context, it should also be mentioned that the forms of investment mentioned in the study are the investment instruments mentioned by the ECB, which are able to realize the investment instruments mentioned by the ECB. (...) In this context, it should also be mentioned that the investment instruments are able to reconstruct the investment instruments. (...) The investment instruments of banks are able to reconstruct the investment instruments. (...) The investment instruments are able to reconstruct the investment motive power of the investment motive power of banks. (...) The investment motive power of banks is able to reconstruct the investment motive power of banks. (...) The investment power of banks is able to reconstruct the capital motive power of capital capital. (...) The investment power of banks is able to reconstruct the capital motive power of the banks. (...) The investment power of banks is able to reconstruct the financial motive power of the banks."}, {"heading": "4.3. Identification of Equivalent PCs", "text": "In this section, we will evaluate the ability of LD-ABCD to detect the PCs that contain two related points that can each characterize a portion of the data. In this section, we have shown the concept of equivalent PCs that are located in each meta-cluster. In this case, it is the way in which the PCs are captured in the individual meta-clusters. In this case, it is the way in which the PCs are separated from the individual meta-clusters that are characterized by the high and similar CQ values that we interpret as equivalent, in the sense that they can be used interactively to identify and characterize the meta-clusters. Furthermore, we can see relationships between the parameters w.r.t. The Dataset group shows this process and makes it easy to understand, we have a synthetic datasp system."}, {"heading": "4.4. Tests using Exploration\u2013Exploitation Strategy", "text": "Here we evaluate the performance improvement we get when using the exploration and exploitation strategy, which in Sec. 3 w.r.t. is the original PC search of Sec. 2.5. We assume that the two approaches will be tested on a high-dimensional synthetic dataset. A good estimate of the search efficiency is the mean CQ (MCQ) over all accepted clusters as a function of time (i.e., algorithm iterations). Of course, after a short initial phase, a higher MCQ value, in each given time step, indicates a faster identification of the RPCs. By definition, the MCQ extends to 1, and the maximum execution time (measured in the number of iterations) is a user-defined dataset located in a 30-dimensional space, and it is characterized by the definition of the RPCs."}, {"heading": "5. Conclusions", "text": "With this study, we presented a dissimilarity-based multi-agent system, LD-ABCD, capable of detecting relevant clusters in a dataset whose elements are grouped according to different and possibly equivalent configurations. Multiple parameter configurations highlight the characteristics of the clusters that are considered discriminatory and represent the key to interpreting and characterizing the rules contained in the datasets."}, {"heading": "Appendix A Graph Conductance and Related Approximation", "text": "In view of a Graphen G = (V, E), with n = V (S), the conductivity of a subset S (S) conditioned property (S) as:????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "Acknowledgements", "text": "The work presented in this paper was partly financed by Telecom Italia S.p.a. The authors would like to thank Corrado Moiso, software systems architect at Telecom Italia - Future Centre, for the value he has brought to the present project through valuable comments, ideas and assistance in the drafting and execution of the research summarized here."}], "references": [{"title": "Efficient agent-based cluster ensembles", "author": ["A. Agogino", "K. Tumer"], "venue": "Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 1079\u20131086. ACM,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-agent Random Walks for Local Clustering on Graphs", "author": ["M. Alamgir", "U. von Luxburg"], "venue": "IEEE 10th International Conference on Data Mining (ICDM),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Local graph partitioning using pagerank vectors", "author": ["R. Andersen", "F. Chung", "K. Lang"], "venue": "Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 475\u2013486. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometry, flows, and graph-partitioning algorithms", "author": ["S. Arora", "S. Rao", "U. Vazirani"], "venue": "Communications of the ACM, 51(10):96\u2013105,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["S. Arora", "S. Rao", "U. Vazirani"], "venue": "Journal of the ACM (JACM), 56(2):5,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral methods for automatic multiscale data clustering", "author": ["A. Azran", "Z. Ghahramani"], "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 1, pages 190\u2013197. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Local descriptors and similarity measures for frontal face recognition: A comparative analysis", "author": ["M. Bereta", "W. Pedrycz", "M. Reformat"], "venue": "Journal of Visual Communication and Image Representation, 24(8):1213\u20131231,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Two Density-based k -means Initialization Algorithms for Non-Metric Data Clustering", "author": ["F.M. Bianchi", "L. Livi", "A. Rizzi"], "venue": "Pattern Analysis and Applications,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A Game-Theoretic Approach to Hypergraph Clustering", "author": ["S.R. Bul\u00f2", "M. Pelillo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(6):1312\u20131327,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards information-theoretic K-means clustering for image indexing", "author": ["J. Cao", "Z. Wu", "J. Wu", "W. Liu"], "venue": "Signal Processing, 93(7):2026\u20132037, July", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A framework for Multi-Agent Based Clustering", "author": ["S. Chaimontree", "K. Atkinson", "F. Coenen"], "venue": "Autonomous Agents and Multi-Agent Systems, 25(3):425\u2013446,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent trends in Ant Colony Optimization and data clustering: A brief survey", "author": ["U. Chandrasekhar", "P. Naga"], "venue": "2nd International Conference on Intelligent Agent and Multi-Agent Systems (IAMA), pages 32\u201336,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "A boosting approach for supervised mahalanobis distance metric learning", "author": ["C.-C. Chang"], "venue": "Pattern Recognition, 45(2):844\u2013862,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral Graph Theory", "author": ["F. Chung"], "venue": "AMS, June", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Cluster transitions in a multi-agent clustering model", "author": ["F. De Smet", "D. Aeyels"], "venue": "Proceedings of the 48th IEEE Conference on Decision and Control, 2009 held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009., pages 4778\u20134784,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "The dissimilarity space: Bridging structural and statistical pattern recognition", "author": ["R.P.W. Duin", "E. P\u0229kalska"], "venue": "Pattern Recognition Letters, 33(7):826\u2013832,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Graph-Based k-Means Clustering: A Comparison of the Set Median versus the Generalized Median Graph", "author": ["M. Ferrer", "E. Valveny", "F. Serratosa", "I. Bardaj\u0301\u0131", "H. Bunke"], "venue": "In Proceedings of the 13th International Conference on Computer Analysis of Images and Patterns,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "A note on spider walks", "author": ["C. Gallesco", "S. Mueller", "S. Popov"], "venue": "ESAIM: Probability and Statistics, 15:390\u2013401,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph based k-means clustering", "author": ["L. Galluccio", "O. Michel", "P. Comon", "A.O. Hero III"], "venue": "Signal Processing, 92(9):1970\u20131984, Sept.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustering with a new distance measure based on a dual-rooted tree", "author": ["L. Galluccio", "O. Michel", "P. Comon", "M. Kliger", "A.O. Hero III"], "venue": "Information Sciences, 251(0): 96\u2013113,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-agent systems and distributed data mining", "author": ["C. Giannella", "R. Bhargava", "H. Kargupta"], "venue": "Cooperative Information Agents VIII, pages 1\u201315. Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Conductance and congestion in power law graphs", "author": ["C. Gkantsidis", "M. Mihail", "A. Saberi"], "venue": "ACM SIGMETRICS Performance Evaluation Review, volume 31, pages 148\u2013159. ACM,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple kernel learning algorithms", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "The Journal of Machine Learning Research, 12:2211\u20132268,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Expander graphs and their applications", "author": ["S. Hoory", "N. Linial", "A. Wigderson"], "venue": "Bulletin of the American Mathematical Society, 43(4):439\u2013561,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering Spatio-Temporal Data: an Augmented Fuzzy C-Means", "author": ["H. Izakian", "W. Pedrycz", "I. Jamal"], "venue": "IEEE Transactions on Fuzzy Systems, 21(5):855\u2013868,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "On clusterings: Good, bad and spectral", "author": ["R. Kannan", "S. Vempala", "A. Vetta"], "venue": "Journal of the ACM (JACM), 51(3):497\u2013515,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A Combine-Correct-Combine Scheme for Optimizing Dissimilarity-Based Classifiers", "author": ["S.-W. Kim", "R.P.W. Duin"], "venue": "E. Bayro-Corrochano and J.-O. Eklundh, editors, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, volume 5856 of LNCS, pages 425\u2013432. Springer Berlin Heidelberg,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms", "author": ["T. Leighton", "S. Rao"], "venue": "Journal of ACM, 46(6):787\u2013832, Nov.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimized dissimilarity space embedding for labeled graphs", "author": ["L. Livi", "A. Rizzi", "A. Sadeghian"], "venue": "Information Sciences, 266:47\u201364,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Granular Modeling and Computing Approaches for Intelligent Analysis of Non-Geometric Data", "author": ["L. Livi", "A. Rizzi", "A. Sadeghian"], "venue": "Applied Soft Computing,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Random Walks on Graphs: A Survey", "author": ["L. Lov\u00e1sz"], "venue": "D. Mikl\u00f3s, V. T. S\u00f3s, and T. Sz\u0151nyi, editors, Combinatorics, Paul Erd\u0151s is Eighty, volume 2, pages 353\u2013398. J\u00e1nos Bolyai Mathematical Society, Budapest,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Fast approximation algorithms for cut-based problems in undirected graphs", "author": ["A. Madry"], "venue": "2010 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 245\u2013254. IEEE,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The Journal of Chemical Physics, 21(6):1087\u20131092,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1953}, {"title": "Local discriminative distance metrics ensemble learning", "author": ["Y. Mu", "W. Ding", "D. Tao"], "venue": "Pattern Recognition, 46(8):2337\u20132349,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "A Novel Coordination Strategy for Multi-Agent Control using Overlapping Subnetworks with Application to Power Systems", "author": ["R.R. Negenborn", "G. Hug-Glanzmann", "B. De Schutter", "G. Andersson"], "venue": "J. Mohammadpour and K. M. Grigoriadis, editors, Efficient Modeling and Control of Large-Scale Systems, pages 251\u2013278. Springer, Norwell, Massachusetts,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic fuzzy clustering and its application in motion segmentation", "author": ["T.M. Nguyen", "Q.M.J. Wu"], "venue": "IEEE Transactions on Fuzzy Systems, 21(6):1019\u20131031, Dec", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "A theoretical formalism for analyzing agent-based models", "author": ["M.J. North"], "venue": "Complex Adaptive Systems Modeling, 2(1):3,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge-based clustering: from data to information granules", "author": ["W. Pedrycz"], "venue": "John Wiley & Sons,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Proximity-Based Clustering: A Search for Structural Consistency in Data With Semantic Blocks of Features", "author": ["W. Pedrycz"], "venue": "IEEE Transactions on Fuzzy Systems, 21 (5):978\u2013982,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear multicriteria clustering based on multiple dissimilarity matrices", "author": ["S. Queiroz", "F. d. A.T. de Carvalho", "Y. Lechevallier"], "venue": "Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Estimating pagerank on graph streams", "author": ["A.D. Sarma", "S. Gollapudi", "R. Panigrahy"], "venue": "Journal of the ACM (JACM), 58(3):13,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Data Analysis of (Non-)Metric Proximities at Linear Costs", "author": ["F.-M. Schleif", "A. Gisbrecht"], "venue": "E. Hancock and M. Pelillo, editors, Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages 59\u201374. Springer Berlin Heidelberg,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient dual approach to distance metric learning", "author": ["C. Shen", "J. Kim", "F. Liu", "L. Wang", "A. van den Hengel"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A local clustering algorithm for massive graphs and its application to nearly-linear time graph partitioning", "author": ["D.A. Spielman", "S.-H. Teng"], "venue": "CoRR, abs/0809.3232,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Personalized PageRank Clustering: A graph clustering algorithm based on random walks", "author": ["S.A. Tabrizi", "A. Shakery", "M. Asadpour", "M. Abbasi", "M.A. Tavallaie"], "venue": "Physica A: Statistical Mechanics and its Applications, 392(22):5772\u20135785,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Numerical linear algebra, volume 50", "author": ["L.N. Trefethen", "D. Bau III"], "venue": "Siam,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval", "author": ["L. Yang", "R. Jin", "L. Mummert", "R. Sukthankar", "A. Goode", "B. Zheng", "S.C.H. Hoi", "M. Satyanarayanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(1):30\u201344, Jan", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised fuzzy clustering with metric learning and entropy regularization", "author": ["X. Yin", "T. Shu", "Q. Huang"], "venue": "Knowledge-Based Systems, 35:304\u2013311,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised distance metric learning based on local linear regression for data clustering", "author": ["H. Zhang", "J. Yu", "M. Wang", "Y. Liu"], "venue": "Neurocomputing, 93:100\u2013105,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "An interval weighed fuzzy c-means clustering by genetically guided alternating optimization", "author": ["L. Zhang", "W. Pedrycz", "W. Lu", "X. Liu", "L. Zhang"], "venue": "Expert Systems with Applications, 41(13):5960\u20135971,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 35, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 9, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 37, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 7, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 24, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 49, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 8, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 44, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 16, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 18, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 19, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 5, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 36, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 0, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 20, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 1, "context": "Typical settings involving the interplay of both approaches include random walk (RW) based algorithms [2, 19], in which agents move and interact on the graph via specific (probabilistic) mechanisms.", "startOffset": 102, "endOffset": 109}, {"referenceID": 17, "context": "Typical settings involving the interplay of both approaches include random walk (RW) based algorithms [2, 19], in which agents move and interact on the graph via specific (probabilistic) mechanisms.", "startOffset": 102, "endOffset": 109}, {"referenceID": 28, "context": "fundamental issue is the definition of the dissimilarity among the input patterns [30, 43, 17], since the specific dissimilarity measure adopted by the data mining procedure affects the possibility of discovering meaningful regularities.", "startOffset": 82, "endOffset": 94}, {"referenceID": 41, "context": "fundamental issue is the definition of the dissimilarity among the input patterns [30, 43, 17], since the specific dissimilarity measure adopted by the data mining procedure affects the possibility of discovering meaningful regularities.", "startOffset": 82, "endOffset": 94}, {"referenceID": 15, "context": "fundamental issue is the definition of the dissimilarity among the input patterns [30, 43, 17], since the specific dissimilarity measure adopted by the data mining procedure affects the possibility of discovering meaningful regularities.", "startOffset": 82, "endOffset": 94}, {"referenceID": 29, "context": "Depending on the application at hand, data can be collected and represented relying on several different formalisms [31].", "startOffset": 116, "endOffset": 120}, {"referenceID": 26, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 38, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 39, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 6, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 22, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 42, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 46, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 47, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 48, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 12, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 33, "context": "For a given dissimilarity measure, it is possible to distinguish two main approaches [35]: those trying to determine a partition of data, and those that focus on searching for isolated clusters surrounded by uncategorized data.", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Local description of data is of particular interest, since it allows to characterize the input data by means of a heterogeneous collection of descriptions [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 25, "context": "To this end, we heavily exploit the graph conductance concept [27].", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 44, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 16, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 18, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 19, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 36, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 5, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 34, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 11, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 10, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 14, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 1, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 36, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 1, "context": "Each agent examines the patterns by performing a RW [2, 19] on the graph that represents the dataset and tries to group them in different clusters.", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "Each agent examines the patterns by performing a RW [2, 19] on the graph that represents the dataset and tries to group them in different clusters.", "startOffset": 52, "endOffset": 59}, {"referenceID": 25, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 23, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 45, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 27, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 4, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 31, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 21, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 40, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 42, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 46, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 47, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 48, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 12, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 0, "context": "Without loss of generality, we also assume that M = [0, 1], where D is the number of parameters/weights characterizing d(\u00b7, \u00b7;m).", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "The dataset is initially represented as a weighted complete undirected graph, Gj = (V, E , w), where each edge ekl \u2208 E is characterized by a weight, w(ekl;m (i) j ) \u2208 [0, 1], which depends on the dissimilarity d(xk, xl;m (i) j ) evaluated with the specific m (i) j .", "startOffset": 167, "endOffset": 173}, {"referenceID": 30, "context": "Each agent performs a Markovian RW [32] on the graph Gj , visiting a number of vertices (nodes) until a quantity called \u201cenergy\u201d is not depleted.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "To perform a RW on Gj we need to define the so-called transition matrix [32], Mj , which is used by an agent to navigate among the vertices.", "startOffset": 72, "endOffset": 76}, {"referenceID": 30, "context": "Every complete and non bipartite graph has a stationary distribution [32], which can be conveniently defined by exploiting the so-called degree distribution,", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "A well-established measure used for evaluating the quality of a cluster associated to a subgraph of a larger graph is the conductance [27], \u03c6(chj), which quantifies how well knit is the subgraph internally and how many edges (with their associated weights) connected to vertices outside the cluster are cut.", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "The exact computation of \u03a6(Gj) is a NP-Hard problem [27], and hence it is not computationally feasible.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "sampled in the parameters space [0, 1]).", "startOffset": 32, "endOffset": 38}, {"referenceID": 32, "context": "The search method is inspired to the well-known Metropolis-Hastings algorithm [34], often employed in statistical physics.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "The adopted dissimilarity measure is the weighted Euclidean distance; each m (i) j is a vector in [0, 1] , where D is the dimensionality of the data at hand.", "startOffset": 98, "endOffset": 104}, {"referenceID": 1, "context": "We decided to use the aforementioned datasets since they are very well-known, easy to obtain and for some of them it was possible to provide a comparison with the results obtained by other algorithms which perform clustering using a RW [2].", "startOffset": 236, "endOffset": 239}, {"referenceID": 1, "context": "As mentioned before, we provide a comparison with the MARW algorithm [2] and two other algorithms therein considered, which are Nibble [45] and Apr.", "startOffset": 69, "endOffset": 72}, {"referenceID": 43, "context": "As mentioned before, we provide a comparison with the MARW algorithm [2] and two other algorithms therein considered, which are Nibble [45] and Apr.", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "PageRank [3] (in the following denoted as N and APR), relatively to the first two dataset treated (Wine and Breast Cancer).", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "To make results comparable, we adopted the same performance measure described in [2] for evaluating the purity of a cluster.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "According to MARW [2], we stopped the RWs as soon as a given number z of different vertices are visited.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 148, "endOffset": 151}, {"referenceID": 1, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 222, "endOffset": 225}, {"referenceID": 3, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 266, "endOffset": 269}, {"referenceID": 3, "context": "8 we show the first three components of the considered patterns, omitting the 4-th component, x[4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 300, "endOffset": 303}, {"referenceID": 3, "context": "Note that the values of x[4] for the blue clusters are drawn from a Gaussian distribution instead.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "Figure 8: Plot of the first three dimensions of the dataset characterized by four clusters in [0, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "To show this process and make it easily understandable, we have used a synthetic dataset in [0, 1], which contains four different clusters.", "startOffset": 92, "endOffset": 98}, {"referenceID": 0, "context": "Since we wanted to keep the data in each cluster sufficiently isolated from the others, we drew the noise values by a random sampling considering a domain obtained by subtracting from [0, 1] a suitable neighborhood of all the clusters.", "startOffset": 184, "endOffset": 190}, {"referenceID": 0, "context": "Figure 9: (Color version online) A snapshot of the dataset of vectors in [0, 1], containing four different clusters.", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "Such a dataset contains 300 vectors in [0, 1], whose components are real values extracted from eight different Gaussian distributions GA, GB, .", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "1 for the neighborhoods of the clusters that we subtract from [0, 1].", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "\u0109i c2(100%) [1, 0, 0, 0] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "895 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "895 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "876 [0, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109ii c1(100%) [1, 0, 0, 0] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "877 [0, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "856 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "856 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "902 [1, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "902 [1, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "890 [0, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "890 [0, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "882 [1, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "882 [1, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "882 [1, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "853 [0, 1, 0, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "853 [0, 1, 0, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109v c3(100%) [0, 0, 0, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "916 [0, 0, 1, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "916 [0, 0, 1, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "859 [0, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109vi c4(100%) [0, 0, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109vi c4(100%) [0, 0, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "934 [0, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "925 [0, 0, 0, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 13, "context": "While computing the conductance (11) of any subset S \u2282 V is simple, computing the conductance of the graph \u03a6(G) consists in solving the following NP-Hard problem [15]:", "startOffset": 162, "endOffset": 166}, {"referenceID": 27, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 4, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 31, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 21, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 40, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 13, "context": "Among the many techniques, spectral techniques [15] provide a very powerful approach.", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "M and N have the same eigenvalues and the eigenvectors are linearly correlated [32, 15].", "startOffset": 79, "endOffset": 87}, {"referenceID": 13, "context": "M and N have the same eigenvalues and the eigenvectors are linearly correlated [32, 15].", "startOffset": 79, "endOffset": 87}, {"referenceID": 30, "context": "The celebrated Cheeger inequality [32] establishes an important relation among the conductance of G (12) with \u03bb2: \u03a6(G) 8 \u2264 1\u2212 \u03bb2 \u2264 \u03a6(G), (17)", "startOffset": 34, "endOffset": 38}, {"referenceID": 45, "context": "The QR-decomposition [47] is the most straightforward numerical technique for this purpose, which is however characterized by a cubic computational complexity.", "startOffset": 21, "endOffset": 25}, {"referenceID": 45, "context": "To overcome this drawback, we can use the power method described in [47], a fast algorithm that is able to compute in pseudolinear time the largest eigenvalue and related eigenvector of a positive semi definite (PSD) matrix.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "The following theorem is an important result for the convergence of the power method [4, 25].", "startOffset": 85, "endOffset": 92}, {"referenceID": 23, "context": "The following theorem is an important result for the convergence of the power method [4, 25].", "startOffset": 85, "endOffset": 92}], "year": 2014, "abstractText": "We propose a multi-agent algorithm able to automatically discover relevant regularities in a given dataset, determining at the same time the set of configurations of the adopted parametric dissimilarity measure yielding compact and separated clusters. Each agent operates independently by performing a Markovian random walk on a suitable weighted graph representation of the input dataset. Such a weighted graph representation is induced by the specific parameter configuration of the dissimilarity measure adopted by the agent, which searches and takes decisions autonomously for one cluster at a time. Results show that the algorithm is able to discover parameter configurations Corresponding Author Email addresses: filippo.binachi@ryerson.ca (Filippo Maria Bianchi), enrico.maiorino@uniroma1.it (Enrico Maiorino), llivi@scs.ryerson.ca (Lorenzo Livi), antonello.rizzi@uniroma1.it (Antonello Rizzi), asadeghi@ryerson.ca (Alireza Sadeghian) URL: https://sites.google.com/site/lorenzlivi/ (Lorenzo Livi), http://infocom.uniroma1.it/~rizzi/ (Antonello Rizzi), http://www.scs.ryerson.ca/~asadeghi/ (Alireza Sadeghian) Preprint submitted to Information Sciences September 18, 2014 that yield a consistent and interpretable collection of clusters. Moreover, we demonstrate that our algorithm shows comparable performances with other similar state-of-the-art algorithms when facing specific clustering problems.", "creator": "LaTeX with hyperref package"}}}