{"id": "1409.5330", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2014", "title": "Learning and approximation capability of orthogonal super greedy algorithm", "abstract": "We consider the approximation capability of orthogonal super greedy algorithms (OSGA) and its applications in supervised learning. OSGA is concerned with selecting more than one atoms in each iteration step, which, of course, greatly reduces the computational burden when compared with the conventional orthogonal greedy algorithm (OGA). We prove that even for function classes that are not the convex hull of the dictionary, OSGA does not degrade the approximation capability of OGA provided the dictionary is incoherent. Based on this, we deduce a tight generalization error bound for OSGA learning. Our results show that in the realm of supervised learning, OSGA provides a possibility to further reduce the computational burden of OGA in the premise of maintaining its prominent generalization capability.", "histories": [["v1", "Thu, 18 Sep 2014 15:09:47 GMT  (49kb)", "http://arxiv.org/abs/1409.5330v1", "30 pages,14 figures"]], "COMMENTS": "30 pages,14 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jian fang", "shaobo lin", "zongben xu"], "accepted": false, "id": "1409.5330"}, "pdf": {"name": "1409.5330.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jian Fang", "Shaobo Lin", "Zongben Xu"], "emails": ["sblin1983@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.53 30v1 [cs.LG] 1 8Se pWe look at the approximation capability of orthogonal supergreedy algorithms (OSGA) and their application in supervised learning. OSGA deals with the selection of more than one atom in each iteration step, which of course significantly reduces the computational load compared to the traditional orthogonal greedy algorithm (OGA). We prove that OSGA does not affect the approximation capability of OGA, even for function classes that are not the convex shell of the dictionary, if the dictionary is incoherent. Building on this, we deduce a severe generalization error that is reserved for OSGA learning. Our results show that in the area of supervised learning, OSGA offers a way to further reduce the computing capability of OGA under the premise of maintaining its prominent generalization capability."}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Greedy-type algorithms", "text": "Let H be a Hilbert space equipped with standard and internal product. \u2212 Define L1 = \u2212 f = f = f = f = f = f = f = g = g =. The standard of L1 is defined by [f =] f = L1: = g =. The four most commonly used are the pure greedy, orthogonal greedy, relaxed and gradual projection algorithms, often indicated by their acronyms. \u2212 There are several types of greedy algorithms [25]. \u2212 The four most commonly used are the pure, orthogonal greedy and step-by-step projected algorithms, often indicated by their acronyms. \u2212 There are OGA, RGA and SPA. \u2212 k: In all of the greedy algorithms mentioned above, we start with the setting f0: = 0. The new approach fk (k) is defined on the basis of fk \u2212 k."}, {"heading": "3. Approximation and learning by OSGA", "text": "After presenting some basic concepts of statistical learning theory, in this section we derive a general approximation theorem about OSGA and follow its application in the field of supervised learning."}, {"heading": "3.1. Statistical learning theory", "text": "For most machine learning problems, the data is taken from two groups: the input space X Rd and the output space Y R. The relationship between the variable x X and the variable y Y is not deterministic and is described by a probability distribution on Z: = X \u00b7 Y, which allows the probability of decomposition on X. Let us leave z = (xi, yi) n = 1 a series of finite random samples of size n, n N, which are drawn identically, independently of each other on Z. The example set z is called the education theorem. Without loss of generality, we assume that | yi | yi) n = 1 is a set of finite random samples of size n \u2212 N drawn on Z."}, {"heading": "3.2. Approximation capability of OSGA", "text": "A consensus in the nonlinear approximation community is that greed-type algorithms can break the \"curse of dimensionality\" (24). Lemma 2.1 seems to confirm this assertion, since a dimensional independent convergence rate has been derived. However, we find that this is not exactly true, since the condition that the target functions belong to the convex shell of the dictionary is becoming increasingly stringent as the dimension of the variable estimate grows [1]. The similar phenomenon in terms of OGA approximation has been successfully addressed in [1] by using convergence results for a variety of function classes and not just those related to the convex shell of the dictionary."}, {"heading": "3.3. OSGA learning", "text": "It was noted that the OSGA learning scheme is shown in this subsection: \"The OSGA learning scheme is shown in the following sections: (1), (1), (1), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2)."}, {"heading": "4. Simulation Supports", "text": "In this section, we present several toy simulations to illustrate the feasibility, effectiveness and efficiency of OSGA learning. The main purpose can be divided into three aspects: The first is to show that there is a relationship between the level of \"dictionary selection\" and the level of \"greedy definition\" for greedy learning. As the incoherence assumption is too strict to describe and difficult to verify the dictionary's property, especially for supervised learning [21], we do not implement the dictionary simulation with such a pessimistic assumption. Instead, we use two widely used dictionaries, such as the trigonometric polynomic dictionary and the Gaussian function dictionary, to justify our point of view; the second is to analyze the advantages and disadvantages of OSGA learning; in particular, we compare both the training time and the test time between OSGA learning with different dictionaries for different regression functions."}, {"heading": "4.1. Experiment Setup", "text": "The data sets: we employ two regression functions f\u03c1 asf1 (x) = sinc (40x \u2212 10) + sinc (60x \u2212 30) + sinc (20x \u2212 1) + cos (10x), where sinc (t) = sin t, andf2 (x) = 1 / 3 \u2212 x, if 0 \u2264 x < 1 / 3, x2, if 1 / 3 \u2264 x \u2264 2 / 3; \u2212 1, 2 / 3 < x \u2264 1. It is easy to see that f1 is an infinitely differentiated function and f2 is a discontinuous function. We generated the training sample z = {(xi, yi)} the simulation of 5000i = 1 by independent and random sampling xi of U (0, 1), and the corresponding y function is to be yi = f\u043c (xi) +."}, {"heading": "4.2. The relationship between dictionary-selection and greedy-definition", "text": "In fact, most of them will be able to move to another world, in which they will be able to escape rather than to another world."}, {"heading": "4.3. The pros and cons of OSGA Learning", "text": "The reason for this is that most people are able to protect themselves without being able to bring themselves to safety, \"he told the Deutsche Presse-Agentur.\" I don't think we will be able to lull ourselves to safety, \"he told the Deutsche Presse-Agentur.\" I think we will be able to put ourselves in a position to put ourselves in a position to put ourselves in a position, \"he said.\" I think we will be able to put ourselves in a position. \""}, {"heading": "4.4. The generalization ability of OSGA Learning", "text": "Finally, we evaluate the generalization capacity of OSGA learning compared to some typical dictionary-based learning methods. Since the purpose of this work is not to track the best dictionary, we only use two fixed dictionaries such as GRD and TPD. Specifically, we list OSGA-1 (or OGA), OSGA-2, OSGA-5, OSGA-10, Lasso, Grating regression, half regression, and greedy aggregation on the same data and dictionaries. Here, OSGA-s indicates that there are s atoms selected in the \"greedy definition level\" of OSGA. The results are summarized in Tables 1 and 2. It can be found in Tables 1 and 2 that the testing error of OSGA-s increases with increasing s, while for TPD the testing error of OSGA-s is reduced monotonously with respect to the compression capability of OSGA-s, which confirms our assertion in section 4.2 further."}, {"heading": "5. Proofs", "text": "< g) and gi (s). < g) and gi (s). < g) and gi (s). < g) and gi (s). < g) and gi (s). < g) and gi (s). < g) and gi (s). < g) and gi (s). < g) and gi (s). < g) and gi (s). < g (s) and gi (s). < g (s) and gi (s). < g (s) and gi (s). < g (s) and gi (s). < g (s) and gi (s). < g (s) and gi (s)."}, {"heading": "6. Concluding Remarks", "text": "It is time for us to set out in search of new ways to travel the world."}], "references": [{"title": "Approximation and learning by greedy algorithms", "author": ["A.R. Barron", "A. Cohen", "W. Dahmen", "R. DeVore"], "venue": "Ann. Statist., 36 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Random sampling of multivariate trigonometric polynomials", "author": ["R.F. Bass", "K. Gr\u00f6chenig"], "venue": "SIAM J. Math. Anal., 36 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Interpolation of Operators", "author": ["C. Bennett", "R. Sharpley"], "venue": "Academic Press, Boston", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Learning rates of multi-kernel regression by orthogonal greedy algorithm", "author": ["H. Chen", "L. Li", "Z. Pan"], "venue": "J. Statist. Plan. & Infer., 143 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bull. Amer. Math. Soc., 39 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning Theory: An Approximation Theory Viewpoint", "author": ["F. Cucker", "D.X. Zhou"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Subspace pursuit for compressive sensing signal recontruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Inf. Theory, 55 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Commun. Pure Appl. Math., 57 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Iteratively re-weighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R. DeVore", "M. Fornasier", "C. G\u00fcnt\u00fcrk"], "venue": "Commun. Pure Appl. Math., 63 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Constructive Approximation", "author": ["R. DeVore", "G. Lorentz"], "venue": "Springer-Verlag, Berlin", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Some remarks on greedy algorithms", "author": ["R. DeVore", "V. Temlyakov"], "venue": "Adv. Comput. Math., 5 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "On Lebesgue-type inequalities for greedy approximation", "author": ["D. Donoho", "M. Elad", "V. Temlyakov"], "venue": "J. Approx. Theory, 147 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit", "author": ["D.L. Donoho", "Y. Tsaig", "O. Drori", "J.L. Starck"], "venue": "IEEE Trans. Inf. Theory, 58 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J. Friedman"], "venue": "Ann. Statis., 29 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "A Distribution-Free Theory of Nonparametric Regression", "author": ["L. Gy\u00f6rfy", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer, Berlin", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Random sampling of sparse trigonometric polynomials II- Orthogonal matching pursuit versus basis pursit", "author": ["S. Kunis", "H. Rauhut"], "venue": "Found. Comput. Math., 8 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning capability of relaxed greedy algorithms", "author": ["S.B. Lin", "Y.H. Rong", "X.P. Sun", "Z.B. Xu"], "venue": "IEEE Trans. Neural Netw. & Learn. Syst., 24 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "The orthogonal super greedy algorithm and applications in compressed sensing", "author": ["E. Liu", "V. Temlyakov"], "venue": "IEEE. Trans. Inf. Theory, 58 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Super greedy type algorithms", "author": ["E. Liu", "V. Temlyakov"], "venue": "Adv. Comput. Math., 37 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Some properties of Gaussian reproducing kernel Hilbert spaces and their implications for function approximation and learning theory", "author": ["H. Minh"], "venue": "Constr. Approx., 32 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning out of leaders", "author": ["M. Mougeot", "D. Picard", "K. Tribouley"], "venue": "J. Royal Statis. Soc. Series B, 74 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Numerical Analysis", "author": ["T. Sauer"], "venue": "Addison-Wesley Longman, London", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "D. Helmbold and B.Williamson, edited, Proceedings of the 14th Annual Conference on Computational Learning Theory, pp 416-426. Springer, New York", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear methods of approximation", "author": ["V. Temlyakov"], "venue": "Found. Comput. Math., 3, ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Greedy approximation", "author": ["V. Temlakov"], "venue": "Acta Numer., 17 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "On performance of greedy algorithms", "author": ["V. Temlyakov", "P. Zheltov"], "venue": "J. Approx. Theory, 163 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. ROY. Statist. Soc. Ser. B, 58 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "Greed is good: algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "IEEE Trans. Inf. Theory, 50 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Computational methods for sparse solution of linear inverse problems", "author": ["J.A. Tropp", "S. Wright"], "venue": "in: Proceedings of the IEEE, 98: 948-958", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "L1/2 regularization: a thresholding representation theory and a fast solver", "author": ["Z.B. Xu", "X.Y. Chang", "F.M. Xu", "H. Zhang"], "venue": "IEEE. Trans. Neural netw & Learn. system., 23 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient greedy learning for massive data", "author": ["C. Xu", "S.B. Lin", "J. Fan"], "venue": "Manuscript", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Greedy metrics in orthogonal greedy learning", "author": ["L. Xu", "S.B. Lin", "J.S. Zeng", "Z.B. Xu"], "venue": "Manuscript", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximation with polynomial kernels and SVM classifiers", "author": ["D.X. Zhou", "K. Jetter"], "venue": "Adv. Comput. Math., 25 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The use of greedy algorithms in the context of nonlinear approximation [1] is very appealing since it greatly reduces the computational burden when compared The research was supported by the National 973 Programming (2013CB329404), the Key Program of National Natural Science Foundation of China (Grant No.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 90, "endOffset": 101}, {"referenceID": 15, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 90, "endOffset": 101}, {"referenceID": 27, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 90, "endOffset": 101}, {"referenceID": 12, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 28, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 11, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 153, "endOffset": 161}, {"referenceID": 25, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 153, "endOffset": 161}, {"referenceID": 0, "context": "Greedy learning, or more specifically, applying greedy algorithms to tackle supervised learning problems, has been proved to possess charming generalization capability with lower computational burden than the widely used coefficient-based regularization methods [1].", "startOffset": 262, "endOffset": 265}, {"referenceID": 31, "context": "From approximation to learning, greedy learning can be usually formulated as a four-stage stepwise learning strategy [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Since greedy learning\u2019s inception in supervised learning [14], the aforementioned four stages were comprehensively studied for various purposes.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "[4] and Lin et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] proposed that the kernel based dictionary is a good choice for greedy learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] pointed out that the metric of greedy-definition is not uniquely the greediest one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] declared that both relaxed greedy iteration and orthogonal greedy iteration can achieve a fast learning rate for greedy learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] provided an l complexity regularization strategy and Chen et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] proposed an l complexity constraint strategy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "All these results showed that as a new learning scheme, greedy learning deserves avid studying due to its stepwise learning character [14].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 3, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 16, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 30, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 31, "context": "In the recent work [32], Xu et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "In this paper, we study the learning capability of orthogonal super greedy algorithm (OSGA) which was proposed by Liu and Temlyakov [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "For OSGA approximation, it was shown in [18] (see also [19]) that for incoherent dictionaries, OSGA reduces the computational burden when compared with OGA.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "For OSGA approximation, it was shown in [18] (see also [19]) that for incoherent dictionaries, OSGA reduces the computational burden when compared with OGA.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "However, such an assumption to the target functions is very stringent if the dimension of variable is large [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Interestingly, we find that, even for functions out of the convex hull of the dictionary, the approximation capability of OSGA is similar as that of OGA [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 24, "context": "There exist several types of greedy algorithms [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "Initially, set f s 0 = 0 and r 0 = f , then the OSGA proposed in [18] for each k \u2265 1 can be inductively define as the following.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "1 proved in [18] shows that OSGA can achieve the optimal approximation rate of ks term nonlinear approximation [24].", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "1 proved in [18] shows that OSGA can achieve the optimal approximation rate of ks term nonlinear approximation [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "Z (f(x)\u2212 y)d\u03c1, which is minimized by the regression function [6], defined by f\u03c1(x) := \u222b", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "With the assumption that f\u03c1 \u2208 L\u03c1 X , it is well known [5] that, for every f \u2208 L2\u03c1X , there holds E(f)\u2212 E(f\u03c1) = \u2016f \u2212 f\u03c1\u2016\u03c1.", "startOffset": 54, "endOffset": 57}, {"referenceID": 23, "context": "Approximation capability of OSGA A consensus in the nonlinear approximation community is that greedy-type algorithms can break the \u201ccurse of dimensionality\u201d [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "We find, however, this is not exactly true since, in practice, the condition that the target functions belong to the convex hull of the dictionary becomes more and more stringent as the dimension of variable grows [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 0, "context": "tackled in [1] by proving convergence results for a variety of function classes and not simply those are related to the convex hull of the dictionary.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Along the flavor of [1], we study the behavior of OSGA approximation when the target functions f \u2208 H are more general.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "We consider the real interpolation spaces [3] Rp = [H,L1]\u03b8,\u221e, 0 < \u03b8 < 1, with p defined by 1 p = \u03b8 + 1\u2212 \u03b8 2 = 1 + \u03b8 2 .", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "Recall that f \u2208 [X, Y ]\u03b8,\u221e if and only if for all t > 0, there holds [10] K(f, t) \u2264 Ct, (3.", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "OSGA learning It was pointed out in [18] that OSGA can be applied in compressed sensing very well.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "It is shown in Algorithm 1 that the only difference between OSGA and OGA learning [1] is that in OSGA there are s atoms selected in the \u201cgreedy-definition\u201d stage.", "startOffset": 82, "endOffset": 85}, {"referenceID": 32, "context": "Furthermore, as y \u2208 [\u2212L, L], it is easy to deduce [33] that \u2016\u03a0Lf s z,m \u2212 f\u03c1\u2016\u03c1 \u2264 \u2016f s z,m \u2212 f\u03c1\u2016\u03c1.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We further notice that up to the constant, the deduced oracle inequality is the same as that deduced in [1] with k in [1, Theorem 3.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "Along [1]\u2019s flavor, for r > 0, we define the space L1 as the set of all functions f such that, for all N , there exists h \u2208 span{DN} satisfying \u2016h\u2016L1 \u2264 B, and \u2016f \u2212 h\u2016\u03c1 \u2264 BN.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "Hence, we use the same l complexity regularization strategy as that in [1] to choose m.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "1 in [1] for the details.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "Since the incoherence assumption is too strict to describe the property of the dictionary and difficult to verify, especially for supervised learning [21], we do not implement the simulation for dictionaries with such a pessimistic assumption.", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 124, "endOffset": 127}, {"referenceID": 26, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 8, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 182, "endOffset": 185}, {"referenceID": 29, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 236, "endOffset": 240}, {"referenceID": 13, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 262, "endOffset": 266}, {"referenceID": 21, "context": "Methods: For OSGA and OGA learning, we applied QR decomposition to acquired the least squared estimates [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "For greedy boosting, we used the L2boost algorithm [14] with the step size 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "For L2 coefficient regularization (or ridge regression), we use its analytic regularized least square solution [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "For L1 (or lasso) and L1/2 coefficient regularization schemes, we utilize the iterative soft [8] and half [30] thresholding algorithms to obtain the corresponding estimators, respectively.", "startOffset": 93, "endOffset": 96}, {"referenceID": 29, "context": "For L1 (or lasso) and L1/2 coefficient regularization schemes, we utilize the iterative soft [8] and half [30] thresholding algorithms to obtain the corresponding estimators, respectively.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "we do not divide the training set into training data and validation data and use validation data to choose parameters as in [15], instead, we use the test set to fix parameters directly.", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "As is shown in [21], the incoherence assumption in the background of supervised learning is too strict to describe the property of dictionaries.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "It can be found in [2] that the TPD dictionary together with the random samples can develop a wellconditioned sampling matrix [2, Theorem 5.", "startOffset": 19, "endOffset": 22}, {"referenceID": 19, "context": "1], while the sampling matrix constructed by GRD is usually ill-conditioned [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "Thus, according to the known bias and variance trade-off principle [6], the bias decreases a little while the variance increases a little, which makes the final generalization error varies only a little.", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "The first and third lemmas can be found in [12] and [11], respectively.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "The first and third lemmas can be found in [12] and [11], respectively.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "In fact, we borrowed the idea of orthogonal super greedy algorithm (OSGA) for incoherent dictionaries from nonlinear approximation and compressive sensing [18] to the supervised 26", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "Precisely, our error estimate for OSGA learning yields a learning rate as (n/ logn), which is the same as that of OGA [1].", "startOffset": 118, "endOffset": 121}], "year": 2014, "abstractText": "We consider the approximation capability of orthogonal super greedy algorithms (OSGA) and its applications in supervised learning. OSGA is concerned with selecting more than one atoms in each iteration step, which, of course, greatly reduces the computational burden when compared with the conventional orthogonal greedy algorithm (OGA). We prove that even for function classes that are not the convex hull of the dictionary, OSGA does not degrade the approximation capability of OGA provided the dictionary is incoherent. Based on this, we deduce a tight generalization error bound for OSGA learning. Our results show that in the realm of supervised learning, OSGA provides a possibility to further reduce the computational burden of OGA in the premise of maintaining its prominent generalization capability.", "creator": "LaTeX with hyperref package"}}}