{"id": "1409.5495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "Efficient Feature Group Sequencing for Anytime Linear Prediction", "abstract": "We propose a regularized linear learning algorithm to sequence groups of features, where each group incurs test-time cost or computation. Specifically, we develop a simple extension to Orthogonal Matching Pursuit (OMP) that respects the structure of groups of features with variable costs, and we prove that it achieves near-optimal anytime linear prediction at each budget threshold where a new group is selected. Our algorithm and analysis extends to generalized linear models with multi-dimensional responses. We demonstrate the scalability of the resulting approach on large real-world data-sets with many feature groups associated with test-time computational costs. Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in timeliness, an anytime prediction performance metric, while providing rigorous performance guarantees.", "histories": [["v1", "Fri, 19 Sep 2014 00:37:42 GMT  (597kb,D)", "http://arxiv.org/abs/1409.5495v1", "work submitted to NIPS2014"], ["v2", "Fri, 23 Jan 2015 16:25:08 GMT  (645kb,D)", "http://arxiv.org/abs/1409.5495v2", "work submitted to AISTATS2015"], ["v3", "Mon, 23 May 2016 20:58:25 GMT  (688kb,D)", "http://arxiv.org/abs/1409.5495v3", "In proceedings of UAI 2016"], ["v4", "Mon, 5 Dec 2016 21:19:03 GMT  (620kb,D)", "http://arxiv.org/abs/1409.5495v4", "Published in UAI 2016, Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, UAI 2016"]], "COMMENTS": "work submitted to NIPS2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanzhang hu", "alexander grubb", "j", "rew bagnell", "martial hebert"], "accepted": false, "id": "1409.5495"}, "pdf": {"name": "1409.5495.pdf", "metadata": {"source": "CRF", "title": "Efficient Feature Group Sequencing for Anytime Linear Prediction", "authors": ["Hanzhang Hu", "Alexander Grubb", "J. Andrew Bagnell"], "emails": ["hanzhang@andrew.cmu.edu", "agrubb@cmu.edu", "dbagnell@ri.cmu.edu", "hebert@ri.cmu.edu"], "sections": [{"heading": null, "text": "Specifically, we are developing a simple extension of the Orthogonal Matching Pursuit (OMP) that respects the structure of variable-cost trait groups, and we demonstrate that it achieves near-optimal linear prediction at each budget threshold at which a new group is selected. Our algorithm and analysis extend to general linear models with multi-dimensional responses. We demonstrate the scalability of the resulting approach to large real-world datasets with many trait groups associated with test time calculation costs. Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in punctuality [7], a measure of performance at any time, while providing strict performance guarantees."}, {"heading": "1 Introduction", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "2 Method", "text": "In this section, we present our cost-sensitive Group Orthogonal Matching Pursuit algorithm for linear predictions with a single response dimension. In Appendix B, we extend this algorithm to generalized linear models and multidimensional results."}, {"heading": "2.1 Feature Group Sequencing for Linear Regression", "text": "Let the data be {(xi, yi): i = 1,..., n}, where xi-RD is the characteristic of the sample i and yi-R is its response. Let X-Rn \u00b7 D, Y-Rn be the data matrix and the response vector. We assume here that the answers have a mean zero, i.e., n = 1 yi = 0. Let G1,..., GJ be a group structure (division) of characteristics with costs c (G1),..., c (GJ) and the number of dimensions DG1,..., DGJ. Let S be a union of groups of characteristics, i.e. S-F, {1,..., J}. We define xiS as a characteristic of the sample limited to the dimensions in S, and we define the risk of S with the regularization constant as: R (S), min w | 12n x-R = a problem limited to the size in S."}, {"heading": "2.2 Cost Sensitive Group Orthogonal Matching Pursuit", "text": "8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8"}, {"heading": "3 Theoretical Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Main Result", "text": "Let us explain G = g1, g2,..., gJ the order of the character groups that our CS-G-OMP has achieved = > results. Let S be any order of the character groups. < K \u2212 \u2212 s2,... to be any competing consequences of the costs K, that is, let G < B > be the first character groups of G, gj, g2,..., gL. Let us designate S < K > s2,... to be any competing consequences of the costs K, that is, any j = 1.. J: Gj S < K < K > c (Gj) = K. For two selected character groups F < s2, let us define bGS as the gradient of the objective F (G) in relation to the coefficient of group S, wS; components of the bGS can be calculated with b G, defined in Equation 2. Let us define the matrix to be X, which is explained by C + T.1nnX."}, {"heading": "3.2 Data-sets and Parameter Set-up", "text": "We are experimenting with CS-G-OMP for a linear prediction at any time on two real datasets, each of which has a significant number of character groups with associated costs. \u2022 Yahoo! Learning to Rank Challenge [1] contains 883k web documents, each of which has a relevance value in {0, 1, 2, 3, 4}. Each of the 501 document features has associated computational costs in {1, 5, 20, 50, 100, 150, 200}, and the total feature cost is approximately 17K. The original dataset has no character group structures, so we have generated random group structures by grouping features of the same cost into groups of a given size.1 \u2022 Agriculture is a proprietary dataset containing 510k data samples, 328 characters and 57 character groups. Each sample has a binary designation in {1, 2}. Each character group has random costs measured in its average value time.2"}, {"heading": "3.3 Evaluation Metric", "text": "Following the practice of [7] we use the range below the maximization target F (explained deviation) vs. cost curve, normalized by the total range, as the time measurement of anytime1We experiments on group sizes s = 5, 10, 15, 20}. We choose the regulator \u03bb = 10 \u2212 5 based on validation. We use s = 10 for qualitative results such as charts and figures, but we report quantitative results for all group sizes s, we report average test performance. The initial risk is R (\u2205) = 0.85,2 There are 6 groups of size 32, and the other groups have sizes between 1 and 6. The cost of each group is their expected calculation time in seconds, which ranges from 0.0005 to 0.0088; the total feature cost is 0.111. We choose regulators. The data is divided into five 100k sets, and the remaining 10k are used for validation."}, {"heading": "3.4 Feature Cost", "text": "Our proposed CS-G-OMP differs from Group Orthogonal Matching Pursuit (G-OMP) [10] in that G-OMP does not take into account feature costs when evaluating features. We show that this difference is crucial for a linear prediction at all times. In Figure 1b we compare the objective cost curves of CS-G-OMP and G-OMP, which are stopped at 0.97 stop costs on YAHOO! LTR. As expected, CS-G-OMP achieves a better overall prediction in each budget, showing the importance of including feature costs in qualitative terms. Table 1 and Table 2 quantify this effect and show that CS-G-OMP achieves a better time measurement than regular G-OMP."}, {"heading": "3.5 Group Whitening", "text": "We provide experimental evidence that group lightening, i.e. XTg Xg = IDg for each group g, is a key assumption both in this paper and in previous literature for selecting feature groups [10] and [11]. In Figure 2, we compare current prediction performance using group white data with those using the common normalization scheme, in which each feature dimension is individually normalized to have a deviation between mean and unit. The objective vs. cost curve qualitatively shows that group lightening consistently leads to better predictions. This behavior is expected from data sets whose feature groups contain correlated features, e.g. group lightening effectively prevents selection steps from overestimating the predictive power of feature groups of repeated good features."}, {"heading": "3.6 Variants on Selection Criterion", "text": "This year, there is less than a year to go before an agreement can be reached."}, {"heading": "A Proof Details of Theorem and Lemmas", "text": "These general results in turn lead to an extension of Theorem 3.1 to generalized linear models. We consider each attribute f as a function hf, that the coefficient of d in w (S) is w (S) d = fS (ed), where there is the best linear predictor with characteristics in S, i.e., fS (x), w (S) TxS. For each attribute f (D) the coefficient of d is in w (S) d = fS (ed), where there is the dth dimensional unit of measurement vector."}, {"heading": "B Appendix B: Generalized Linear Model with Multi-Dimensional Response", "text": "Here we describe the CS-G-OMP extension to generalized linear models with multidimensional results. (Suppose we have P-Dimensional answers, we define the best characteristic coefficient matrix to characteristic S such as W (S), RP \u00b7 DS. We assume that the model E [y | x] = is effective (Wx), in terms of current linear predictive force fS, which is defined as fS (x). Then it selects the group g, which represents the cost per unit of internal products between characteristic hf and gradient rp, for p = 1,..., P.input: The data matrix X = [f1, fD]."}], "references": [{"title": "Proceedings of the Yahoo! Learning to Rank Challenge, held at ICML 2010, Haifa, Israel, June 25, 2010, volume 14 of JMLR Proceedings", "author": ["Olivier Chapelle", "Yi Chang", "Tie-Yan Liu", "editors"], "venue": "JMLR.org,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["Minmin Chen", "Zhixiang Eddie Xu", "Kilian Q. Weinberger", "Olivier Chapelle", "Dor Kedem"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["Abhimanyu Das", "David Kempe"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Anytime algorithm development", "author": ["Joshua Grass", "Shlomo Zilberstein"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Speedboost: Anytime prediction with uniform nearoptimality", "author": ["Alexander Grubb", "J. Andrew Bagnell"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski", "Francis Bach"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Timely object recognition", "author": ["Sergey Karayev", "Tobias Baumgartner", "Mario Fritz", "Trevor Darrell"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Submodular function maximization", "author": ["Andreas Krause", "Daniel Golovin"], "venue": "In Tractability: Practical Approaches to Hard Problems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Joint cascade optimization using a product of boosted classifiers", "author": ["Leonidas Lefakis", "Francois Fleuret"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Grouped orthogonal matching pursuit for variable selection and prediction", "author": ["Aurelie C. Lozano", "Grzegorz Swirszcz", "Naoki Abe"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Group orthogonal matching pursuit for logistic regression", "author": ["Aurelie C. Lozano", "Grzegorz Swirszcz", "Naoki Abe"], "venue": "In AISTATS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Matching pursuit with time-frequency dictionaries", "author": ["Stphane Mallat", "Zhifeng Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Subset selection in regression. Monographs on statistics and applied probability", "author": ["Alan J. Miller"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Inverting Modified Matrices", "author": ["Max A. Woodbury"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1950}, {"title": "Anytime representation learning", "author": ["Zhixiang Xu", "Matt Kusner", "Gao Huang", "Kilian Q. Weinberger"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML- 13),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Zhixiang Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "venue": "In In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in timeliness[7], an anytime prediction performance metric, while providing rigorous performance guarantees.", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "We view these prediction problems with unknown test-time budgets as anytime predictions [4], which trade execution time for quality of results.", "startOffset": 88, "endOffset": 91}, {"referenceID": 13, "context": "One popular approach is to extend Lasso [14] to the group setting, e.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": ", Group Lasso[18] balances the trade-off by solving", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "Group Lasso can be further extended to incorporate feature group costs in the group regularization constants[2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "Selecting based on the exact gains in performance, also known as Forward Regression (FR) [13], often performs well.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "One computational feasible method to approximate this exact greedy selection is Orthogonal Matching Pursuit (OMP) [12], which chooses candidates that induce the steepest change in objective value.", "startOffset": 114, "endOffset": 118}, {"referenceID": 9, "context": "This result is then extended by [10] and [11] to handle feature groups.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "This result is then extended by [10] and [11] to handle feature groups.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "[17] and [16] propose methods to learn cost-sensitive non-linear transformation of features for linear classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] and [16] propose methods to learn cost-sensitive non-linear transformation of features for linear classification.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "[5] approaches the trade-off between anytime prediction quality and feature computation with gradient boosting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] treats feature selection procedure as a Markov Decision Process and learns a policy of applying intermediate learners and computing features through reinforcement learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Cascading classifiers like [2] and [9] approach anytime prediction by filtering out the dominating negative classes using a sequence classifiers of increasing complexity and feature costs.", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "Cascading classifiers like [2] and [9] approach anytime prediction by filtering out the dominating negative classes using a sequence classifiers of increasing complexity and feature costs.", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "In this work, we propose Cost-Sensitive Group Orthogonal Matching Pursuit (CSG-OMP), which extends Group Orthogonal Matching Pursuit (G-OMP) [10], for anytime linear predictions.", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "Theoretical analyses of [10] and [11] also rely on this assumption but did not explain the impact of not group whitening the data, which we will do in experiments and analysis.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "Theoretical analyses of [10] and [11] also rely on this assumption but did not explain the impact of not group whitening the data, which we will do in experiments and analysis.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "As mentioned in [11], if the data is not group whitened before training, one equivalent method of group whitening is to replace \u2016X g (Y \u2212XGj\u22121w)\u20162 in step (\u2217) with bj\u22121 g T (X g Xg) \u22121bGj\u22121 g = (Y \u2212XGj\u22121w)Xg(X g Xg)X g (Y \u2212XGj\u22121w).", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "2, which mimics a classical result in submodular maximization literature such as Equation 8 of [8], the proof of the theorem is a standard technique in approximate submodular maximization literature, which we defer to the appendix.", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "6 in [3], we can show that:", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "\u2022 Yahoo! Learning to Rank Challenge [1] contains 883k web documents, each of which has a relevance score in {0, 1, 2, 3, 4}.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "3 Evaluation Metric Following the practice of [7], we use the area under the maximization objective F (explained variance) vs.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "To account for this effect, we stop the curve when it reaches the plateau: we define an \u03b1-stopping cost for parameter \u03b1 in [0, 1] as the cost at which our CS-G-OMP achieves \u03b1 of the final objective value in training, and we ignore the objective vs.", "startOffset": 123, "endOffset": 129}, {"referenceID": 6, "context": "To approximate the best possible anytime performance, we follow the approach of [7] to create an Oracle objective vs.", "startOffset": 80, "endOffset": 83}, {"referenceID": 17, "context": "We extend Group Lasso[18] to be our baseline algorithm: we scale the regularization constant of each group with the cost of the group, to get the minimization problem, minw\u2208RD \u2016Y \u2212Xw\u20162 + \u03bb \u2211J j=1 c(Gj)\u2016wGj\u20162, where we use various value of regularization constant \u03bb to obtain lasso paths.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "4 Feature Cost Our proposed CS-G-OMP differs from Group Orthogonal Matching Pursuit (G-OMP) [10] in that G-OMP does not consider feature costs when evaluating features.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": ", X g Xg = IDg for each group g, is a key assumption of both this work and previous feature group selection literature [10] and [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": ", X g Xg = IDg for each group g, is a key assumption of both this work and previous feature group selection literature [10] and [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "[7] defines timeliness as the area under the average precision vs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "time curve We use an off-the-shelf software to solve the optimization: SPAMS (SPArse Modeling Software [6]).", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "The performance advantage of CS-G-OMP over CS-G-OMP-Single is much clearer in AGRICULTURAL than that in YAHOO! LTR, because the former data-set has a natural group structure which may contain correlated features in each group, whereas the latter data-set has a randomly generated group structure whose features have been filtered by feature selection before the publication of the data-set [1].", "startOffset": 390, "endOffset": 393}, {"referenceID": 14, "context": "In the context of linear regression, assuming the group sizes are bounded by a constant, when we are to select the number K feature group, we can compute a new model of K groups in O(KN) using matrix inversion lemma [15]), evaluate it in O(KN), and compute the gradients w.", "startOffset": 216, "endOffset": 220}], "year": 2017, "abstractText": "We propose a regularized linear learning algorithm to sequence groups of features, where each group incurs test-time cost or computation. Specifically, we develop a simple extension to Orthogonal Matching Pursuit (OMP) that respects the structure of groups of features with variable costs, and we prove that it achieves nearoptimal anytime linear prediction at each budget threshold where a new group is selected. Our algorithm and analysis extends to generalized linear models with multi-dimensional responses. We demonstrate the scalability of the resulting approach on large real-world data-sets with many feature groups associated with test-time computational costs. Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in timeliness[7], an anytime prediction performance metric, while providing rigorous performance guarantees.", "creator": "LaTeX with hyperref package"}}}