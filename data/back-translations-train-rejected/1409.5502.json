{"id": "1409.5502", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "Using crowdsourcing system for creating site-specific statistical machine translation engine", "abstract": "A crowdsourcing translation approach is an effective tool for globalization of site content, but it is also an important source of parallel linguistic data. For the given site, processed with a crowdsourcing system, a sentence-aligned corpus can be fetched, which covers a very narrow domain of terminology and language patterns - a site-specific domain. These data can be used for training and estimation of site-specific statistical machine translation engine", "histories": [["v1", "Fri, 19 Sep 2014 02:50:04 GMT  (181kb)", "http://arxiv.org/abs/1409.5502v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander kalinin", "george savchenko"], "accepted": false, "id": "1409.5502"}, "pdf": {"name": "1409.5502.pdf", "metadata": {"source": "CRF", "title": "Using crowdsourcing system for creating site-specific statistical machine translation engine", "authors": ["Alexander Kalinin", "George Savchenko"], "emails": ["verbalab@yandex.ru", "@langprism.com"], "sections": [{"heading": null, "text": "A good tool for globalizing the content of websites, but it is also a good source of parallel linguistic data. For the given page, which is processed using a crowdsourcing system, a sentence corpus can be retrieved that covers a very narrow range of terminology and language patterns - a site-specific domain. This data can be used to train and evaluate the site-specific SMT engine."}, {"heading": "1 Crowdsourcing in site-translation", "text": "The more the Internet develops and internationalizes, the more the question arises about the speed of translation of web contents. One possible solution to increase the speed to maintain the necessary level of quality is crowdsourcing translation. According to the crowdsourcing approach, the translation is not done by a team of hired professional translators, but by a voluntary community. A well-known example of crowdsourcing project is Wikipedia, a huge web encyclopedia in which articles are written and edited by anyone willing to contribute to the development. On the basis of this \"openness,\" Wikipedia develops its data and refreshes it to manage the project. The same idea is behind crowdsourcing translation: the task of translating the previously published content is distributed among the members of the community. One such translation community arose around the online game \"Pottermore\" (http: / / www.pottermore.com)."}, {"heading": "2 Problems in site-specific statistical machine translation (SMT)", "text": "From SMT theory, it is known that a translation machine works best when trained on examples close to the sentences it wants to translate. Thus, an SMT machine trained on corpora retrieved from a particular page does better translations for that page than the machine trained on any other body. It works because the content of a particular page has lexical and structural homogeneity. However, the body of a page cannot guarantee good performance simply because of the lack of data - the average volume of translated sentences per page is about 7,000 sentences (according to LangPrism [7] statistics). While the more or less acceptable automatic translation requires at least 100k sentences for translation models and 500k for language models [1]. One possible solution to dealing with body scarcity is to combine a site-specific body with a larger, unspecified (ordinary) body. Following this assumption online, a site-specific translation machine was created."}, {"heading": "3 Training translation site-specific translation model with combined corpus", "text": "The site-specific corpus contained 7k samples; it contained the names of characters, places, and terms of sorcerers used in games; the common 1M corpus was provided by Yandex [6], consisting of newspapers; the sentences were already \"sentence-to-sentence\" aligned so that no additional sentence alignment was carried out; an open source \"Moses\" [2] solution was used as the SMT system; the translation direction was from English to Russian, as the most popular within the LangPrism project; two corpses were combined to form a training set for Moses; and the 5th order language model was then trained on the Russian part of the corpus; and the translation model was then trained."}, {"heading": "4 Results", "text": "Three phases of testing were carried out 1) testing different site-specific motors using automated measurements 2) comparing the best site-specific motor with other site-specific measurements 3) human pairwise estimation between the best site-specific motor and the best MT web serviceThe test was performed over 700 samples extracted from the web body, which did not occur in the training or tuning samples. The highest results were calculated with maximum company sizes. Next, the best motor trained for 7k specific body parts and 1M common body parts was selected with tools from the \"Moses suitcase.\" As can be seen, the quality of the engine increases with increases in both the general body and the specific body. The highest results were achieved with maximum body sizes. Afterwards, the best motor matched to 7k specific body parts and 1M common body parts, the corpse was coached with the other human body parts."}, {"heading": "5. Conclusion and further work", "text": "As a result, it can be concluded that training custom statistical machine translation machines that use two corporations - one common and one specific - can increase quality compared to training with specific or shared corpus. Moreover, a bespoke machine trained on a rather modest corpus (1M + 7k) with open source solutions can compete with and even exceed popular translation systems from companies such as Google and MS. Moreover, this level of quality is achieved without sophisticated pre-processing and linguistic analysis by using only the data from crowdsourced translations. Of course, such quality can only be achieved for the page for which the system was designed, and it would show much worse outcomes elsewhere. The problem with the system was that it showed quite poor quality with long sentences while Google worked well with them."}], "references": [{"title": "Statistical Machine Translation", "author": ["P Koehn"], "venue": "CAMBRIDGE UNIVERSITY PRESS", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The Method of Paired Comparisons", "author": ["H.A. David"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "Technical Report RC22176(W0109-022),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "A Study of Translation Edit Rate with Targeted Human Annotation,", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": "Proceedings of Association for Machine Translation in the Americas,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "While more or less acceptable automatic translation requires at least 100k sentences for translation models and 500k for language models [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "The performance of SMT engine was estimated with BLEU [4] and TER [5] metrics using tools from \u2018Moses\u2019 suite.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "The performance of SMT engine was estimated with BLEU [4] and TER [5] metrics using tools from \u2018Moses\u2019 suite.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Moreover, in spite of correlation with human judgments the BLEU score is often under critics and is considered not very reliable by some MT expersts [4].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "So it was decided to perform pairwise human estimation using method described in [3].", "startOffset": 81, "endOffset": 84}], "year": 2013, "abstractText": "A crowdsourcing translation approach is a good tool for globalization of site content, but it is also a good source of parallel linguistic data. For the given site, processed with a crowdsourcing system, a sentence-aligned corpus can be fetched, which covers a very narrow domain of terminology and language patterns a sitespecific domain. These data can be used for training and estimation of site-specific SMT engine. 1 Crowdsourcing in site-translation The more internet develops and becomes more internationalized the more question of speed of translation of web-content arises. One possible solution for increasing the speed preserving necessary level of quality is crowdsourcing translation. According to crowdsourcing approach, the translation is performed not by a team of hired professional translators, but by a volunteering community. A well-known example of crowdsourcing project is Wikipedia, a huge web encyclopedia where articles are written and revised by anyone who is willing to contributed to the development of it. Because of this \u201copenness\u201d Wikipedia is developing and refreshes its data very quickly, and no maintenance from Wikipedia administrators is required to manage the project. The same idea is behind the crowdsourcing translation: the task of translation of the content published before is distributed among members of the community. One of such translation community has appeared around on-line game \u201cPottermore\u201d (http://www.pottermore.com). Users from Russian took part in translating content from the game using LangPrism tool [7], an extension for Internet browser that enables user to make translation of any web-page and to share the result of his translation. One of the main problems of translating large web-project is the fact that new web pages demanding translation always appear, and it takes some time before translators will notice them and translate. This crowdsourcing translation gap can be filled by high quality machine translation system that is tailored to the peculiar web site. For example, it can be trained using parallel texts fetched from crowdsourcing translation activity. 2 Problems in site-specific statistical machine translation (SMT) From SMT theory, it is known that a translation engine performs best if it has been trained on samples that are close to the sentences it aims to translate. Thus, SMT engine trained on corpora retrieved from a particular site will make better translations for this very site than the engine trained on any other corpus. It works because the content of a particular site has lexical and structural homogeneity. However, site\u2019s corpus alone can\u2019t guarantee a good performance because of data sparsity \u2014 the average volume of translated sentences per site is about 7000 sentences (according to LangPrism [7] statistics). While more or less acceptable automatic translation requires at least 100k sentences for translation models and 500k for language models [1]. One possible solution for dealing with the corpus sparsity is to combine site-specific corpus with a bigger non-specified (common) corpus. Following this assumption, a site-specific translation engine to translate an on-line game was trained. 3 Training translation site-specific translation model with combined corpus The site-specific corpus was obtained from www.pottermore.com on-line game translations performed by Russian community via LangPrism platform. The volume of site-specific corpus included 7k samples. It contained names of characters, places, terms of magician items used in games. The common 1M corpus was provided by Yandex [6] consisting of newspapers\u2019. The sentences had already been \u2018sentence-to-sentence\u2019 aligned, so no additional sentential alignment was performed. As an SMT system an open-source solution \u201cMoses\u201d[2] was used. The translation direction was from English to Russian, as the most popular one within the LangPrism project. Two corpuses were combined to shape a training set for Moses. Then, the 5-th order language model was trained on the Russian part of the corpus. After that, the translation model was trained. Phrase-based approach for training translation model was used. No other factored training was made. After the training, tuned the decoder\u2019s *.ini file using WERmetrics from MOSES was tuned. Samples for tuning were taken from the target site and did not overlap with training set. The workflow of the training is presented in Figure. 1. To see if the quality of SMT engine had increased after some additional work, done by the translation community (some more text from the site has been translated) four iterations were performed with different specific corpus sizes. Figure 1. Combined corpus training workflow", "creator": "Microsoft\u00ae Word 2013"}}}