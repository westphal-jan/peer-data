{"id": "1409.5616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "A Survey on Soft Subspace Clustering", "abstract": "Subspace clustering (SC) is a promising clustering technology to identify clusters based on their associations with subspaces in high dimensional spaces. SC can be classified into hard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have been extensively studied and well accepted by the scientific community, SSC algorithms are relatively new but gaining more attention in recent years due to better adaptability. In the paper, a comprehensive survey on existing SSC algorithms and the recent development are presented. The SSC algorithms are classified systematically into three main categories, namely, conventional SSC (CSSC), independent SSC (ISSC) and extended SSC (XSSC). The characteristics of these algorithms are highlighted and the potential future development of SSC is also discussed.", "histories": [["v1", "Fri, 19 Sep 2014 12:01:08 GMT  (372kb)", "http://arxiv.org/abs/1409.5616v1", null], ["v2", "Fri, 8 Apr 2016 02:08:55 GMT  (1234kb)", "http://arxiv.org/abs/1409.5616v2", "This paper has been published in Information Sciences Journal in 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhaohong deng", "kup-sze choi", "yizhang jiang", "jun wang", "shitong wang"], "accepted": false, "id": "1409.5616"}, "pdf": {"name": "1409.5616.pdf", "metadata": {"source": "CRF", "title": "A Survey on Soft Subspace Clustering", "authors": ["Zhaohong Deng", "Kup-Sze Choi", "Jun Wang", "Shitong Wang"], "emails": ["zhdeng@ucdavis.edu,", "dzh666828@aliyun.com"], "sections": [{"heading": null, "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}], "references": [{"title": "Survey of clustering algorithms,IEEE", "author": ["R. Xu", "D. Wunsch"], "venue": "Trans. Neural Networks", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Data clustering: 50 years beyond K-means, Pattern Recognition Letters", "author": ["A.K. Jain"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Partitive clustering (K-means family), Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["Y. Xiao", "J. Yu"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Subspace clustering for high dimensional data: a review, ACM SIGKDD Explorations Newsletter", "author": ["L. Parsons", "E. Haque", "H. Liu H"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A survey on enhanced subspace clustering, Data Mining and Knowledge Discovery", "author": ["K. Sim", "V. Gopalkrishnan", "A. Zimek A"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering, ACM Transactions on Knowledge Discovery from Data", "author": ["H.P. Kriegel", "P. Kr\u00f6ger", "A. Zimek"], "venue": "Article No", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Subspace clustering, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["H.P. Kriegel", "P. Kr\u00f6ger", "A. Zimek"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Automatic subspace clustering of high dimensional data for data mining applications", "author": ["R. Agrawal", "J. Gehrke", "D. Gunopulos", "P. Raghavan"], "venue": "Proceedings of ACM SIGMOD Int\u2019l Conf. Management of Data,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Entropy-based subspace clustering for mining numerical data", "author": ["C.H. Cheng", "A.W. Fu", "Y. Zhang"], "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "MAFIA: Efficient and scalable subspace clustering for very large data sets", "author": ["S. Goil", "H. Nagesh", "A. Choudhary"], "venue": "Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Finding generalized projected clusters in high dimensional spaces", "author": ["C.C. Aggarwal", "P.S. Yu"], "venue": "in: Proc. ACM SIGMOD Int\u2019l Conf. Management of Data,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "FINDIT: a fast and intelligent subspace clustering algorithm using dimension voting, Information and Software Technology", "author": ["K.G. Woo", "J.H. Lee", "M.H. Kim"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A Monte Carlo algorithm for fast projective clustering", "author": ["C.M. Procopiuc", "M. Jones", "P.K. Agarwal"], "venue": "Proceedings of the 2002 ACM SIGMOD international conference on Management of data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "\u03b4-clusters: Capturing subspace correlation in a large data set, Proceedings", "author": ["J. Yang", "W. Wang", "H. Wang"], "venue": "18th International Conference on Data Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Fast algorithms for projected clustering", "author": ["C.C. Aggarwal", "J.L. Wolf", "P.S. Yu"], "venue": "ACM SIGMOD Record,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Harp: A practical projected clustering algorithm", "author": ["K.Y. Yip", "D.W. Cheung", "M.K. Ng"], "venue": "IEEE Trans. Knowledge and Data Engineering", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Local dimensionality reduction: A new approach to indexing high dimensional spaces", "author": ["K. Chakrabarti", "S. Mehrotra"], "venue": "Proceedings of the 26th International Conference on Very Large Data Bases", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Optimal variable weighting for ultrametric and additive tree clustering, Quality and Quantity", "author": ["G. De Soete"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "Optimal variable weighting for ultrametric and additive trees and K-means partitioning: Methods and software, Journal of Classification", "author": ["V. Makarenkov", "P. Legendre"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Feature weighting in k-means clustering, Machine learning", "author": ["D.S. Modha", "W.S. Spangler"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Improving fuzzy c-means clustering based on feature-weight learning, Pattern Recognition Letters", "author": ["X. Wang", "Y. Wang", "L. Wang"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Synthesized clustering: A method for amalgamating alternative clustering bases with differential weighting of variables, Psychometrika", "author": ["W.S. DeSarbo", "J.D. Carroll", "L.A. Clark"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1984}, {"title": "A new feature weighted fuzzy clustering algorithm, In: Rough Sets, Fuzzy Sets, Data Mining, and Granular Computing", "author": ["J. Li", "X. Gao", "L. Jiao"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Automated variable weighting in k-means type clustering,IEEE", "author": ["J.Z. Huang", "M.K Ng", "H. Rong"], "venue": "Trans. Pattern Analysis and Machine Intelligence", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "A maximum weighted likelihood approach to simultaneous model selection and feature weighting in Gaussian mixture", "author": ["Y. Cheung", "H. Zeng"], "venue": "Artificial Neural Networks\u2013ICANN", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Developing a feature weight self-adjustment mechanism for a K-means clustering algorithm, Computational statistics & data analysis", "author": ["C.Y. Tsai", "C.C. Chiu"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A model-based approach for discrete data clustering and feature weighting using MAP and stochastic complexity,IEEE", "author": ["N. Bouguila"], "venue": "Trans. Knowledge and Data Engineering", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Fuzzy clustering with weighting of data variables, International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems", "author": ["A. Keller", "F. Klawonn"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Unsupervised learning of prototypes and attribute weights, Pattern recognition", "author": ["H. Frigui", "O. Nasraoui"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "An optimization algorithm for clustering using weighted dissimilarity measures, Pattern recognition", "author": ["E.Y. Chan", "W.K. Ching", "M.K. Ng"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Subspace clustering of text documents with feature weighting k-means algorithm, In: Advances in Knowledge Discovery and Data Mining", "author": ["L. Jing", "M.K. Ng", "J. Xu"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "A fuzzy subspace algorithm for clustering high dimensional data, In: Advanced Data Mining and Applications", "author": ["G. Gan", "J. Wu", "Z. Yang"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "A convergence theorem for the fuzzy subspace clustering (FSC) algorithm, Pattern Recognition", "author": ["G. Gan", "J. Wu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "An entropy weighting k-means algorithm for subspace clustering of high-dimensional sparse data,IEEE", "author": ["L. Jing", "M.K. Ng", "J.Z. Huang"], "venue": "Trans. Knowledge and Data Engineering", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Locally adaptive metrics for clustering high dimensional data, Data Mining and Knowledge Discovery", "author": ["C. Domeniconi", "D. Gunopulos", "S. Ma"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Subspace clustering of high dimensional data", "author": ["C. Domeniconi", "D. Papadopoulos", "D. Gunopulos"], "venue": "SIAM Int. Conf. on Data Mining,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "A probability model for projective clustering on high dimensional data", "author": ["L. Chen", "Q. Jiang", "S. Wang"], "venue": "Eighth IEEE International Conference on Data Mining,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Model-based method for projective clustering", "author": ["L. Chen", "Q. Jiang", "S. Wang"], "venue": "IEEE Trans. Knowledge and Data Engineering", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "An entropy weighting mixture model for subspace clustering of high-dimensional data, Pattern Recognition Letters", "author": ["L. Peng", "J. Zhang"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Meulman, Clustering objects on subsets of attributes (with discussion), Journal of the Royal Statistical Society: Series B (Statistical Methodology", "author": ["J.J.J.H. Friedman"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2004}, {"title": "Enhanced soft subspace clustering integrating within-cluster and  26  between-cluster information, Pattern Recognition", "author": ["Z. Deng", "K.S. Choi", "F.L. Chung"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "The study on enhanced soft subspace clustering techniques", "author": ["Q. Guan"], "venue": "Master Dissertation,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "An improved k-means algorithm for clustering using entropy weighting measures, the 7th World", "author": ["T. Li", "Y. Chen"], "venue": "Congress on Intelligent Control and Automation,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "TW-k-means: Automated Two-level Variable Weighting Clustering Algorithm for Multi-View", "author": ["X. Che", "X. Xu", "J. Huang"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "A feature group weighting method for subspace clustering of high-dimensional data[J", "author": ["X. Chen", "Y. Ye", "X. Xu"], "venue": "Pattern Recognition", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Comparison between two coevolutionary feature weighting algorithms in clustering, Pattern Recognition", "author": ["P. Gan\u00e7arski", "A. Blansche", "A. Wania"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Particle swarm optimizer for variable weighting in clustering high-dimensional data, Machine learning", "author": ["Y. Lu", "S. Wang", "S. Li"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Multiobjective evolutionary algorithm-based soft subspace clustering, 2012", "author": ["L. Zhu", "L. Cao", "J. Yang"], "venue": "IEEE Congress on Evolutionary Computation,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Attribute weighted mercer kernel based fuzzy clustering algorithm for general non-spherical datasets, Soft Computing", "author": ["H. Shen", "J. Yang", "S. Wang"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "Minkowski metric, feature weighting and anomalous cluster initializing in K-Means clustering", "author": ["R. Cordeiro de Amorim", "B. Mirkin"], "venue": "Pattern Recognition,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "A clustering ensemble framework based on elite selection of weighted clusters, Advances in Data Analysis and Classification", "author": ["H. Parvin", "B. Minaei-Bidgoli"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "A weight entropy k-means algorithm for clustering dataset with mixed numeric and categorical data", "author": ["T. Li", "Y. Chen"], "venue": "Fifth International Conference on Fuzzy Systems and Knowledge Discovery,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2008}, {"title": "A K-means type clustering algorithm for subspace clustering of mixed numeric and categorical datasets, Pattern Recognition Letters", "author": ["A. Ahmad", "L. Dey"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "A novel attribute weighting algorithm for clustering high-dimensional categorical data, Pattern Recognition", "author": ["L. Bai", "J. Liang", "C. Dang"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Weighted cluster ensembles: Methods and analysis, ACM Transactions on Knowledge Discovery from Data", "author": ["C. Domeniconi", "M. Al-Razgan"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Projective clustering ensembles, Data Mining and Knowledge Discovery", "author": ["F. Gullo", "C. Domeniconi", "A. Tagarelli"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Extending data reliability measure to a filter approach for soft subspace clustering", "author": ["T. Boongoen", "C. Shang", "N. Iam-On"], "venue": "IEEE Trans. Systems, Man, and Cybernetics, Part B: Cybernetics", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Soft subspace clustering with an improved feature weight self-adjustment mechanism, International Journal of Machine Learning and Cybernetics", "author": ["G. Guo", "S. Chen", "L. Chen"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Double indices induced FCM clustering and its integration with fuzzy subspace clustering,2012", "author": ["J Wang", "S. Wang S", "Z. Deng"], "venue": "IEEE International Conference on Fuzzy Systems,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowledge and Data Engineering", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Transfer latent variable model based on divergence analysis, Pattern Recognition", "author": ["X. Gao", "X. Wang", "X. Li"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2011}, {"title": "Robust visual domain adaptation with low-rank reconstruction,2012", "author": ["I.H. Jhuo", "D. Liu", "D.T. Lee"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2012}, {"title": "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach, 2012", "author": ["L. Duan", "D. Xu", "S.F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approac", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2011}, {"title": "Fuzzy partition based soft subspace clustering and its applications in high dimensional data", "author": ["J. Wang", "S.T. Wang", "F.L. Chung", "Z.H. Deng"], "venue": "Information Sciences,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "Fuzzy c-means for very large data,IEEE", "author": ["T.C. Havens", "J.C. Bezdek", "C. Leckie", "L.O. Hall", "M. Palaniswami"], "venue": "Trans. Fuzzy Systems", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "Core vector machines: fast SVM training on large data sets", "author": ["I.W. Tsang", "J.T. Kwo", "P.M. Cheung"], "venue": "Journal of Machine Learning Research", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Large-scale maximum margin discriminant analysis using core vector machines", "author": ["I.W. Tsang", "A. Kocsor", "J.T. Kwok"], "venue": "IEEE Trans. Neural Networks", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2008}, {"title": "FRSDE: fast reduced set density estimator using minimal enclosing ball approximation, Pattern Recognition", "author": ["Z.H. Deng", "F.L. Chung", "S.T. Wang"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2008}, {"title": "From minimum enclosing ball to fast fuzzy inference system training on large datasets", "author": ["F.L. Chung", "Z.H. Deng", "S.T. Wang"], "venue": "IEEE Trans. Fuzzy Systems", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2009}, {"title": "Scalable TSK fuzzy modeling for very large datasets using minimal enclosing ball approximation", "author": ["Z.H. Deng", "K.S.Choi", "F.L. Chung", "S.T. Wang"], "venue": "IEEE. Trans. Fuzzy systems", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 149, "endOffset": 154}, {"referenceID": 1, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 149, "endOffset": 154}, {"referenceID": 3, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 262, "endOffset": 267}, {"referenceID": 4, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 262, "endOffset": 267}, {"referenceID": 5, "context": "Despite extensive studies of clustering techniques over the past decades in various application areas like statistics, machine learning and database [1-2], the conventional clustering techniques fall short when clustering is performed in high dimensional spaces [4-6].", "startOffset": 262, "endOffset": 267}, {"referenceID": 3, "context": "This category of SC algorithms attempts to identify the exact subspaces for different clusters, which can be further divided into bottom-up and top-down subspace search methods [4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 7, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 12, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "Examples of the former include CLIQUE [8], ENCLUS [9] and MAFIA [10]; and that of the latter are ORCLUS [11], FINDIT [12], DOC [13], \uf064 -Clusters [14], and PROCLUS [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "Other common HSC algorithms also include HARP [16] and LDR [17].", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "Other common HSC algorithms also include HARP [16] and LDR [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 4, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 5, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 6, "context": "A detailed review of HSC algorithms can be found in [4-7].", "startOffset": 52, "endOffset": 57}, {"referenceID": 17, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 18, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 19, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 20, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 21, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 22, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 23, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 24, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 25, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 26, "context": "SSC can be considered as an extension of the conventional feature weighting clustering [18-27].", "startOffset": 87, "endOffset": 94}, {"referenceID": 17, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 0, "endOffset": 7}, {"referenceID": 21, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 22, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 23, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 24, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 25, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 26, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 8, "endOffset": 15}, {"referenceID": 27, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 28, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 29, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 30, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 31, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 32, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 16, "endOffset": 23}, {"referenceID": 33, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 24, "endOffset": 28}, {"referenceID": 34, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 30, "endOffset": 34}, {"referenceID": 35, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 40, "endOffset": 44}, {"referenceID": 37, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 45, "endOffset": 49}, {"referenceID": 38, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 50, "endOffset": 54}, {"referenceID": 39, "context": "[18-21] [22-27] [28-33] [34], [35] [36] [37],[38] [39] [40] XSSC Between-class separation Multi-view learning Evolutionary learning New metrics Unbalanced clusters Others", "startOffset": 55, "endOffset": 59}, {"referenceID": 40, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 0, "endOffset": 7}, {"referenceID": 41, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 0, "endOffset": 7}, {"referenceID": 42, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 0, "endOffset": 7}, {"referenceID": 43, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 8, "endOffset": 12}, {"referenceID": 44, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 14, "endOffset": 18}, {"referenceID": 45, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 19, "endOffset": 26}, {"referenceID": 46, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 19, "endOffset": 26}, {"referenceID": 47, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 19, "endOffset": 26}, {"referenceID": 48, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 27, "endOffset": 31}, {"referenceID": 49, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 33, "endOffset": 37}, {"referenceID": 50, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 38, "endOffset": 42}, {"referenceID": 51, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 52, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 53, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 54, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 55, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 56, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 57, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 58, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 64, "context": "[41-43] [44], [45] [46-48] [49], [50] [51] [52-59, 65]", "startOffset": 43, "endOffset": 54}, {"referenceID": 17, "context": "1 Separated Feature Weighting Algorithms (1) OVW-UAT The Optimal Variable Weighting for Ultrametric and Additive Tree (OVW-UAT) is a feature weighting strategy developed for hierarchical clustering methods to solve the variable weighting problems [18].", "startOffset": 247, "endOffset": 251}, {"referenceID": 18, "context": "Makarenkov and Legendre extended the OVW-UAT approach to optimal variable weighting for the k-means clustering [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "(2) C-K-means The Convex K-means (C-K-means) is a method proposed specifically for variable weighting in k-means clustering [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "(3) WFCM The Weighted FCM (WFCM) algorithm is another clustering method grouped under the category of separated feature weighting [21].", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "2 Coupled Feature Weighting Algorithms (1) SYNCLUS The Synthesized Clustering (SYNCLUS) algorithm is developed to deal with variable weighting in k-means clustering [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "The algorithm is computationally intensive and very time-consuming [3], making it not suitable for handling large data sets.", "startOffset": 67, "endOffset": 70}, {"referenceID": 22, "context": "7 The Feature Weighted Fuzzy K-means (FWFKM) algorithm performs clustering through an iterative procedure based on the fuzzy k-means algorithm and the supervised ReliefF algorithm [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 23, "context": "proposed the automated variable weighting in k-means type clustering algorithm (W-k-means) by using the following objective function [24],", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "The W-k-means algorithm has received increasing attention, based on which modified algorithms are proposed, such as the fuzzy subspace clustering algorithm [32, 33] to be reviewed in Subsection 4.", "startOffset": 156, "endOffset": 164}, {"referenceID": 32, "context": "The W-k-means algorithm has received increasing attention, based on which modified algorithms are proposed, such as the fuzzy subspace clustering algorithm [32, 33] to be reviewed in Subsection 4.", "startOffset": 156, "endOffset": 164}, {"referenceID": 24, "context": "(4) MWLA Cheung and Zeng proposed the Maximum Weighted Likelihood (MWL) learning framework in the context of Gaussian mixture model to identify the clustering structure and the relevant features automatically and simultaneously [25].", "startOffset": 228, "endOffset": 232}, {"referenceID": 25, "context": "8 (5) FWSA Tsai and Chiu proposed the Feature Weight Self-Adjustment Algorithm (FWSA) based on the k-means clustering model [26].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "(6) MA-DDC-FW For the problems of unsupervised discrete feature selection/weighting, the Model-based Approach for Discrete Data Clustering and Feature Weighting (MA-DDC-FW) [27] is proposed that makes use of a probabilistic approach to assign relevance weights to the discrete features.", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "(1) AWFCM Keller and Klawonn proposed the Attribute Weighting Fuzzy Clustering (AWFCM) based on FCM model with the objective function below [28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "[0,1] ij u \uf0ce , 1 1 \uf03d \uf0e5 \uf03d C", "startOffset": 0, "endOffset": 5}, {"referenceID": 40, "context": "Based on AWFCM, improved versions of the algorithm have been proposed, such as EFWSSC algorithm [41].", "startOffset": 96, "endOffset": 100}, {"referenceID": 28, "context": "(2) SCAD Frigui and Nasraoui proposed the Simultaneous Clustering and Attribute Discrimination (SCAD) method [29].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "[30] employed an objective function similar to that of the W-k-means algorithm [24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[30] employed an objective function similar to that of the W-k-means algorithm [24].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "10 k w \uf074 of the kth feature of all the clusters [24] were replaced by the weights ik w \uf074 of the k-th feature of each cluster.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "In fact, it is also clear from the objective function of AWA and that of AWFCM as discussed above [28] that AWA is a hard clustering version of the AWFCM algorithm.", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "The Fuzzy Weighting K-Means (FWKM) algorithm [31] is proposed to overcome this problem.", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "proposed the Fuzzy Subspace Clustering (FSC) algorithm [32, 33] using an objective similar to that of the FWKM algorithm [31] discussed previously.", "startOffset": 55, "endOffset": 63}, {"referenceID": 32, "context": "proposed the Fuzzy Subspace Clustering (FSC) algorithm [32, 33] using an objective similar to that of the FWKM algorithm [31] discussed previously.", "startOffset": 55, "endOffset": 63}, {"referenceID": 30, "context": "proposed the Fuzzy Subspace Clustering (FSC) algorithm [32, 33] using an objective similar to that of the FWKM algorithm [31] discussed previously.", "startOffset": 121, "endOffset": 125}, {"referenceID": 32, "context": "A detailed analysis of the properties of FSC can be found in [33].", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 28, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 29, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 30, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 31, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 32, "context": "The fuzzy index \uf074 is usually set such that 1 \uf03e \uf074 in order to ensure the convergence of the algorithms [28-33].", "startOffset": 102, "endOffset": 109}, {"referenceID": 33, "context": "proposed the Entropy Weighting K-Means (EWKM) clustering algorithm [34] using the following objective function,", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "EWKM has become a benchmarking ISSC algorithm and is further extended to develop various XSSC algorithms, such as ESSC algorithm [34].", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "(2) LAC The weighting in the Local Adaptive Clustering (LAC) algorithm [35] is also controllable by entropy.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "In addition to the entropy weighting LAC in [35], Domeniconi proposed an alternative LAC algorithm with a different objective function [36] as follows", "startOffset": 44, "endOffset": 48}, {"referenceID": 35, "context": "In addition to the entropy weighting LAC in [35], Domeniconi proposed an alternative LAC algorithm with a different objective function [36] as follows", "startOffset": 135, "endOffset": 139}, {"referenceID": 34, "context": "Note that the objective function is maximized to solve the solution variables, which is distinct from the entropy-based LAC approach [35] discussed earlier.", "startOffset": 133, "endOffset": 137}, {"referenceID": 36, "context": "proposed the Fuzzy/Model Projective Clustering (FPC/MPC) algorithm based on the mixture model [37, 38].", "startOffset": 94, "endOffset": 102}, {"referenceID": 37, "context": "proposed the Fuzzy/Model Projective Clustering (FPC/MPC) algorithm based on the mixture model [37, 38].", "startOffset": 94, "endOffset": 102}, {"referenceID": 38, "context": "(2) EWMM Peng and Zhang proposed the entropy weighting mixture model (EWMM) algorithm [39].", "startOffset": 86, "endOffset": 90}, {"referenceID": 38, "context": "When compared with the classical FCM/K-means model based ISSC algorithms, the two mixture model based algorithms are expected to possess stronger adaptation abilities to the data distributions, as evident from that promising clustering results achieved on high dimensional data [39].", "startOffset": 278, "endOffset": 282}, {"referenceID": 39, "context": "Among them, the Clustering Objects on Subsets of Attributes (COSA) is a representative algorithm which was proposed by Friedman and Meulman [40] with the following objective function.", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "As discussed in [34], a shortcoming of COSA is that it may not be scalable to accommodate large data sets.", "startOffset": 16, "endOffset": 20}, {"referenceID": 33, "context": "(1) ESSC Based on the EWKM method [34], the Enhanced SSC (ESSC) algorithm was proposed to", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "It is noteworthy to point out that when 1 \uf0ae m and 0 \uf03d \uf068 , the ESSC algorithm is reduced to the EWKM algorithm [34].", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "(2) EFWSSC Using a similar strategy, Guan improved the FSC algorithm [32, 33] by making use of between-cluster separation and proposed the Enhanced Fuzzy Weighting Soft Subspace Clustering (EFWSSC) algorithm [42].", "startOffset": 69, "endOffset": 77}, {"referenceID": 32, "context": "(2) EFWSSC Using a similar strategy, Guan improved the FSC algorithm [32, 33] by making use of between-cluster separation and proposed the Enhanced Fuzzy Weighting Soft Subspace Clustering (EFWSSC) algorithm [42].", "startOffset": 69, "endOffset": 77}, {"referenceID": 41, "context": "(2) EFWSSC Using a similar strategy, Guan improved the FSC algorithm [32, 33] by making use of between-cluster separation and proposed the Enhanced Fuzzy Weighting Soft Subspace Clustering (EFWSSC) algorithm [42].", "startOffset": 208, "endOffset": 212}, {"referenceID": 42, "context": "proposed the Improved Entropy Weighting K-means (IEWKM) with the following objective function [43],", "startOffset": 94, "endOffset": 98}, {"referenceID": 40, "context": "Although the IEWKM algorithm here and the ESSC algorithm [41] are both based on EWKM and they both employ the between-cluster separation, the former is much more difficult to be optimized.", "startOffset": 57, "endOffset": 61}, {"referenceID": 42, "context": "Besides, from the viewpoint of optimization, the learning rules for the cluster centers in the IEWKM algorithm lack rigorous derivation [43].", "startOffset": 136, "endOffset": 140}, {"referenceID": 43, "context": "proposed the Two-level variable Weighting k-means (TW-k-means) by introducing the view weighting mechanism [44].", "startOffset": 107, "endOffset": 111}, {"referenceID": 44, "context": "(2) FG-k-means Another multi-view SSC algorithm to be introduced is the Feature Groups weighting k-means (FG-k-means) algorithm [45].", "startOffset": 128, "endOffset": 132}, {"referenceID": 45, "context": "proposed two SSC algorithms based on coevolution learning [46].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "The first algorithm was inspired by the Lamarck theory and used the distance-based cost function defined in the AWC algorithm [30] as the fitness function.", "startOffset": 126, "endOffset": 130}, {"referenceID": 45, "context": "The experimental results in [46] highlighted the benefits of using coevolutionary feature weighting methods to improve the knowledge discovery process.", "startOffset": 28, "endOffset": 32}, {"referenceID": 46, "context": "The following objective function is employed for variable optimization [47],", "startOffset": 71, "endOffset": 75}, {"referenceID": 47, "context": "that makes use of new encoding and operators [48].", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "By comparing MWK-Means with AWA [30], we can easily see that when 2 p \uf03d ,", "startOffset": 32, "endOffset": 36}, {"referenceID": 50, "context": "5 Imbalanced Clusters For data with imbalanced cluster, a XSSC algorithm, called the Weighted LAC (WLAC), is proposed based on elite selection of weighted clusters [51].", "startOffset": 164, "endOffset": 168}, {"referenceID": 51, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 52, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 53, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 54, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 55, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 56, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 57, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 58, "context": "6 Other XSSC Algorithms In addition to the five subcategories of XSSC algorithms discussed above, many other XSSC algorithms have also been reported recently [52-59] and are described briefly as follows.", "startOffset": 158, "endOffset": 165}, {"referenceID": 51, "context": "(1) Category Data While the IEWKM algorithm is first proposed for category and numeric mixture data [52], algorithms developed specifically for this type of data have also been made available.", "startOffset": 100, "endOffset": 104}, {"referenceID": 52, "context": "For example, a modified version of EWKM [53] was proposed by Ahmad and Dey for handling category and numeric mixture data.", "startOffset": 40, "endOffset": 44}, {"referenceID": 53, "context": "also proposed a modified SSC algorithm based on AWA and the use of an improved metric for category data [54].", "startOffset": 104, "endOffset": 108}, {"referenceID": 54, "context": "Domeniconi and Al-razgan proposed a graph-partitioning-based clustering ensemble approach called Weighted Similarity Partitioning Algorithm (WSPA) to overcome the problem of parameter sensitivity of LAC algorithm [55].", "startOffset": 213, "endOffset": 217}, {"referenceID": 55, "context": "proposed the Projective Clustering Ensembles (PCE) algorithms for ensemble learning of soft clustering [56].", "startOffset": 103, "endOffset": 107}, {"referenceID": 56, "context": "proposed a novel approach to SSC based on the measure of data reliability, named as reliability-based SSC algorithms [57].", "startOffset": 117, "endOffset": 121}, {"referenceID": 59, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 60, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 61, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 62, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 63, "context": "Transfer learning and domain adaptive learning [60-64] are very attractive approaches in the fields of machine learning for their applicability in the real-world modeling and learning tasks, such as classification, regression and clustering, particularly in situations where the data of the current scene (also commonly named as the target domain) are insufficient, or are difficult to be used for a certain task, while some information of a related scene (also commonly named as the source domain) is available for the task.", "startOffset": 47, "endOffset": 54}, {"referenceID": 43, "context": "The research presented in [44, 45] indicates that multi-view clustering is a very effective approach to improve clustering Target domain / Current scene Source domain / Reference scene", "startOffset": 26, "endOffset": 34}, {"referenceID": 44, "context": "The research presented in [44, 45] indicates that multi-view clustering is a very effective approach to improve clustering Target domain / Current scene Source domain / Reference scene", "startOffset": 26, "endOffset": 34}, {"referenceID": 65, "context": "Related work can be seen in [66].", "startOffset": 28, "endOffset": 32}, {"referenceID": 66, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 67, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 68, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 69, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}, {"referenceID": 70, "context": "Some new advancement for the learning on the large datasets, such as the minimal enclosing ball approximation technique [67-71], can also be taken into account to develop the scalable and fast SSC algorithms.", "startOffset": 120, "endOffset": 127}], "year": 2014, "abstractText": "Subspace clustering (SC) is a promising clustering technology to identify clusters based on their associations with subspaces in high dimensional spaces. SC can be classified into hard subspace clustering (HSC) and soft subspace clustering (SSC). While HSC algorithms have been extensively studied and well accepted by the scientific community, SSC algorithms are relatively new but gaining more attention in recent years due to better adaptability. In the paper, a comprehensive survey on existing SSC algorithms and the recent development are presented. The SSC algorithms are classified systematically into three main categories, namely, conventional SSC (CSSC), independent SSC (ISSC) and extended SSC (XSSC). The characteristics of these algorithms are highlighted and the potential future development of SSC is also discussed.", "creator": "\u00fe\u00ff"}}}