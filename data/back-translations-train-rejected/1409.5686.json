{"id": "1409.5686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "Transfer Prototype-based Fuzzy Clustering", "abstract": "The traditional prototype based clustering methods, such as the well-known fuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good clustering partition. If the available data is limited or scarce, most of the existing prototype based clustering algorithms will no longer be effective. While the data for the current clustering task may be scarce, there is usually some useful knowledge available in the related scenes/domains. In this study, the concept of transfer learning is applied to prototype based fuzzy clustering (PFC). Specifically, the idea of leveraging knowledge from the source domain is exploited to develop a set of transfer prototype based fuzzy clustering (TPFC) algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM, fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been chosen to incorporate with knowledge leveraging mechanism to develop the corresponding transfer clustering algorithms. Novel objective functions are proposed to integrate the knowledge of source domain with the data of target domain for clustering in the target domain. The proposed algorithms have been validated on different synthetic and real-world datasets and the results demonstrate their effectiveness when compared with both the original prototype based fuzzy clustering algorithms and the related clustering algorithms like multi-task clustering and co-clustering.", "histories": [["v1", "Fri, 19 Sep 2014 14:58:56 GMT  (970kb)", "http://arxiv.org/abs/1409.5686v1", "The manuscript has been submitted to IEEE Trans. Fuzzy Systmes in 2013"], ["v2", "Tue, 5 Apr 2016 09:43:45 GMT  (1670kb)", "http://arxiv.org/abs/1409.5686v2", "The manuscript has been accepted by IEEE Trans. Fuzzy Systmes in 2015"]], "COMMENTS": "The manuscript has been submitted to IEEE Trans. Fuzzy Systmes in 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhaohong deng", "yizhang jiang", "fu-lai chung", "hisao ishibuchi", "kup-sze choi", "shitong wang"], "accepted": false, "id": "1409.5686"}, "pdf": {"name": "1409.5686.pdf", "metadata": {"source": "CRF", "title": "Transfer Prototype-based Fuzzy Clustering", "authors": ["Zhaohong Deng", "Yizhang Jiang", "Fu-Lai Chung", "Kup-Sze Choi", "Shitong Wang"], "emails": [], "sections": [{"heading": null, "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "II. Prototype Based Clustering", "text": "Typically, it has the following form of objective function: J fU-U-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V (,), (1), where U is the partition matrix and vice versa the set of prototypical parameters. Its goal is to optimize the objective function in eq. (1) to obtain the optimal partition matrix U and the optimal prototype parameter set. First, consider the following three types of prototype-based cluster formation, namely cluster center prototype-based cluster formation [18-21], cluster direction-based cluster formation [23, 46-50] and subspace cluster-prototype-based cluster formation [22, 51-53].A. Cluster center-based cluster formation."}, {"heading": "B. Cluster Direction Prototype Based Clustering", "text": "Cluster Direction Prototype-based clustering is also a representative branch of prototype-based clustering. In this type of cluster method, the direction vector is used to characterize the prototypes [23, 46-50]. The most representative cluster direction prototype-based cluster methods are the k-level cluster algorithm (KPC) [23] and the blurred KPC (FKPC) algorithm [47, 48]. The objective functions of KPC and FKPC can be described as follows: KPC / FKPC: / 1 1 1 1,, min () K N m T KPC FKPC ij i ii j J u b x v (3-1).st [0,1] for FKPC {0,1} for KPCij iju u, 1 1K iji u, 10 N ijj u N, 1i ijj u N, T v v v v v v, where K is the number of clusters; CT v v v."}, {"heading": "C. Subspace Cluster Prototype Based Clustering", "text": "In this type of clustering, the prototype of each cluster is characterized by a cluster center and a weight vector representing the soft subspace for that cluster. A representative algorithm is the fuzzy subspace cluster algorithm (FSC) [51], whose objective function is defined as FSC: 21 1 1 1, min ik ikC N d C djk ikFSC iji j k i k J u w wx v U V W (4-1).st {0,1} iju u, 10 N ijj u N, [0,1] ikotC N, 1 d ikk k k w, 10 C idi idi w C, where 1 [,] T CW w w is the matrix of the weighting of vectors."}, {"heading": "III. Transfer Prototype Based Fuzzy Clustering", "text": "This section introduces for the first time a learning mechanism based on knowledge transfer, on the basis of which three TPFC algorithms are proposed."}, {"heading": "A. Knowledge Leverage Based TPFC", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1) Framework", "text": "One promising strategy for meeting this challenge is to learn by referring to the information in the source domain that may contain useful information for the target domain. Normally, two types of information are available from the source domain, namely the original data and the induced knowledge, as shown in Figure 2. If the data is used directly in the source domain, it may not always be appropriate. In fact, it is extremely difficult to control and balance the similarity and difference in data distributions between the two domains, as there may be a drift of data distribution between the domains. On the other hand, the influence of the induced knowledge from the source domain can be comfortably controlled, as the induced knowledge is much clearer and more concise than the data in the source domain. Therefore, we consider it more appropriate to use the knowledge from the source domain than the knowledge from the source domain that is based on the source domain."}, {"heading": "2) Objective Function", "text": "The following general objective function is proposed for TPFC based on the above-mentioned knowledge lever-based transfer learning strategy: min; TPFC t sJ f U, (,) + (,), (5), which consists of two different functional terms; the first functional term tf U, (,) is inherited from the classic PFC algorithms and used to learn from the data in the target domain; the second functional term, sf U, (,) is introduced to use knowledge from the source domain; this term can be used in various forms depending on the design of the prototype-based cluster algorithm; and with this general objective function, three TPFC algorithms, namely Transfer fuzzyc-mean (TFCM), Transfer fuzzy k-plane clustering (TFKPC) and Transfer fuzzy subspace clustering (TFSC) are proposed."}, {"heading": "B. TPFC Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1) Transfer FCM", "text": "Among cluster-center-prototype-based cluster methods, indistinct c-means [19] are used to learn the knowledge of the source centers, where the method of the source centers is very popular and is used here for the development of our first TPFC algorithm, i.e. Transfer FCM (TFCM). For this purpose, the following objective function is defined: 2 211 1 1 1 1 1.22 1 1min) C N C Nm j ji iTFCM ij ij ij ij ij ij ij ij u u U V vx vx (6).st [0,1] iju, 11 Ciji u, 10N ijj u N m, where ij ij ij ij ij ij ij ij ij ij ij ij ij ij ij ij ij ij ij ij ij ii ij ij ij ij (6)."}, {"heading": "2) Transfer FKPC", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "3) Transfer FSC", "text": "Under the existing sub-space cluster prototypes cluster methods are based on updating FSC [51] to research the corresponding transfer learning version, i.e., TFSC. Thus, the following objective function is proposed: 21 1 1 1 1 1, 2 21 1 1 1 1 1 1 () C N d C dm ij (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K) K (1) K (1) K (1) K) K (1) K (1) K) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1 K) K (1 K) K (1 K) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1) K (1 K) K (1) K (1 K) K (1 K (1) K (1) K (1) K (1) K (1 K) K (1) K (1 K (1) K (1) K (1 K) K (1 K) K (1 K) K (1 K) K (1 K) K (1 K (1 K) K (1 K) K (1 K) K (1 K (1 K) K) K (1 K) K (1 K) K (1 K (1 K) K) K"}, {"heading": "C. Computational Complexity and Convergence Analysis", "text": "The complexity of the proposed algorithms can be described as follows. For the TFCM and TFKPC algorithms, their computational complexities are either () O TNC TC, where T is the total number of iterations, N is the size of the dataset, and C is the number of clusters. In other words, they are of the same order as those of the classic FCM and FKPC algorithms. For the convergence of the three proposed TPFC algorithms, their computational complexity is () O TNC TC TCd, where d is the number of features that are of the same order as those of the classical FSC. For the convergence of the three proposed TPFC algorithms, Zangwill's convergence theorem can be adopted as in the convergence studies of many existing algorithms, e.g. [51]."}, {"heading": "IV. Experimental Results", "text": "The three proposed TPFC algorithms, i.e. TFCM, TFKPC and TFSC, have been extensively evaluated on synthetic and real data sets. In this section, the indices used for performance evaluation and experimental setup are described first, then the performance of the proposed algorithms on synthetic and real text data sets is reported and discussed, and a further comparison with other related algorithms is performed on multiple synthetic and real data sets. All experiments were performed using Matlab codes on a computer with 1.66 GHz CPU and 2GB RAM."}, {"heading": "A. Performance Indices and Experimental Setup", "text": "In fact, the number of data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the data sets mentioned in relation to the"}, {"heading": "B. Synthetic Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1) Evaluation of TFCM Algorithm", "text": "This year, the time has come for the EU member states to be able to use the EU accession perspective to claim the EU accession perspective for themselves."}, {"heading": "2) Evaluation of TFKPC Algorithm", "text": "Structures are used to evaluate the performance of the proposed TFKPC algorithm. Data were generated for the source domain and the target domain, respectively, where each domain contains three clusters in different hyperplanes with different cluster direction prototypes, as shown in Figure 5. The parameters used to generate the data are in Table VI. From Table VI, we can see that the three clusters in the two domains have similar but different hyperplane parameters. Specifically, Gaussian noise has been added to the synthetic dataset with the given noise parameter, i.e. the standard deviation as shown in Table VI. Figure 5 shows that the data in the target domain is scarce, while the data in the source domain is sufficient and helpful to improve the cluster effect in the target domain. Results such as those in previous subsections are presented in Tables VII-X. From the results, the following observations can be obtained: (1) Table II-VX."}, {"heading": "3) Evaluation of TFSC Algorithm", "text": "The results of the study show that most of them are able to identify themselves even though they are able to generate the data listed in the table. Figure 6 (a) and Figure 6 (b) each contain three sub-figures, each corresponding to a cluster in which the high-dimensional data in a cluster are viewed as sequences and shown in the corresponding sub-figure. Figure 6 (b) shows that the properties of characteristics such as the x-coordinate and the y-coordinate are different in each case. From these sub-figures we can see the corresponding sub-space of each cluster in which the characteristics are more important than other characteristics of the associated cluster. For example, from three sub-figures in Figure 6 (a) we can see that we see most characteristics of the characteristics of three clusters in Figure 6."}, {"heading": "D. Comparison with Related Clustering Algorithms", "text": "Related work, namely two transfer clustering algorithms, i.e., autodidact clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and Co-Clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] both on synthetic and on real datasets used in the subsections IV-B & C. For the proposed clustering algorithms, the parameter setting is the same as the parameter setting described in Section IV-B and for the LSSMTC algorithm, the regulation parameter [0,1] parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter parameter"}, {"heading": "E. Discussion", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "V. Conclusions", "text": "In this study, the concept of knowledge use is used to develop a set of transfer prototype-based fuzzy clustering methods for application scenarios in which data is limited or insufficiently available for effective clustering. Based on the established knowledge transfer leverage learning mechanism, several transfer prototype-based cluster algorithms are used to address the deficiency caused by the lack of data in the target domain. Proposed prototype-based cluster algorithms learn not only from the data of the target domain, but also from the knowledge of the source domain. Experimental results have shown the attractiveness and effectiveness of the proposed transfer prototype-based fuzzy clustering algorithms compared to existing classical prototype-based cluster algorithms without the transfer learning capabilities, and other related algorithms such as the multi-task clustering algorithms in the domain can be well treated."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Hong Kong Polytechnic University under grant number GrantG-UA68, the National Natural Science Foundation of China under grant number 61170122, 2012 NSFC, Natural Science Foundation of Jiangsu Province under grant number 2012JSNSFC and JiangSu 333Expert Engineering Grant (BRA2011142)."}], "references": [{"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowledge Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "On minimum distribution discrepancy support vector machine for domain adaptation", "author": ["J.W. Tao", "F.L. Chung", "S.T. Wang"], "venue": "Pattern Recognition, vol. 45, no. 11, pp. 3962-3984, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive localization through transfer learning in indoor Wi-Fi environment", "author": ["Z. Sun", "Y.Q. Chen", "J. Qi", "J.F. Liu"], "venue": "Proc. 7th International Conference on Machine Learning and Applications, pp. 331\u2013336, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative learning for differing training and test distributions", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "Proc. 24th Int. Conf. Machine Learning, pp. 81-88, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to learn with the informative vector machine", "author": ["N.D. Lawrence", "J.C. Platt"], "venue": "Proc. 21st Int. Conf. Machine Learning, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Knowledge transfer via multiple model local structure mapping", "author": ["J. Gao", "W. Fan", "J. Jiang", "J. Han"], "venue": "Proc. 14th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, pp. 283-291, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Transfer learning by mapping with minimal target data", "author": ["L. Mihalkova", "R.J. Mooney"], "venue": "Proc. Assoc. for the Advancement of Artificial Intelligence (AAAI \u201908) Workshop Transfer Learning for Complex Tasks, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep transfer via second-order Markov logic", "author": ["J. Davis", "P. Domingos"], "venue": "Proc. Assoc. for the Advancement of Artificial Intelligence (AAAI \u201908) Workshop Transfer Learning for Complex Tasks, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Trans. Neural Networks, vol. 22, no.2, pp.199-210, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Transferred dimensionality reduction", "author": ["Z. Wang", "Y. Song", "C. Zhang"], "venue": "Proc. European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD \u201908), pp. 550-565, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian task-level transfer learning for non-linear regression", "author": ["P. Yang", "Q. Tan", "Y. Ding"], "venue": "Proc. Int. Conf. on Computer Science and Software Engineering, pp. 62-65, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Application of transfer regression to TCP throughput prediction", "author": ["L. Borzemski", "G. Starczewski"], "venue": "Proc. First Asian Conference on Intelligent Information and Database Systems, pp.  28-33, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Transfer regression model for indoor 3D location estimation", "author": ["J. Liu", "Y. Chen", "Y. Zhang"], "venue": "Lecture Note on Computer Science 5916, pp. 603\u2013613, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge-Leverage based TSK fuzzy system modeling", "author": ["Z.H. Deng", "Y.Z. Jiang", "K.S. Choi", "F.L. Chung", "S.T. Wang"], "venue": "IEEE Trans. Neural Networks and Learning Systems, vol. 24, no. 8, pp. 1200-1212, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledge-Leverage based fuzzy system and its modeling", "author": ["Z.H. Deng", "Y.Z. Jiang", "F.L. Chung", "H. Ishibuchi", "S.T. Wang"], "venue": "IEEE Trans. Fuzzy systems, vol.21, no. 4, pp. 597-609, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-taught clustering", "author": ["W. Dai", "Q. Yang", "G. Xue", "Y. Yu"], "venue": "Proc. 25th Int. Conf. Machine Learning, pp. 200-207, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Transfer spectral clustering", "author": ["W.H. Jiang", "F.L. Chung"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 789-803, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A fuzzy relative of the ISODATA process and its use in detecting compact well separated clusters", "author": ["J. Dunn"], "venue": "Journal of Cybernetics, vol. 3, no. 3, pp.32\u201357, 1973.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1973}, {"title": "Pattern Recognition with Fuzzy Objective Function Algorithms", "author": ["J.C. Bezdek"], "venue": "New York Plenum,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1981}, {"title": "A possibilistic approach to clustering.", "author": ["R. Krishnapuram", "J.M. Keller"], "venue": "IEEE Trans. on Fuzzy Systems, vol.1,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "A maximum entropy approach to fuzzy clustering", "author": ["R.P. Li", "M.A. Mukaidono"], "venue": "Proc on IEEE Int Conf Fuzzy Syst. Yokohama, Japan, pp. 2227-2232, 1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Automated variable weighting in k-means type clustering", "author": ["J.Z. Huang", "M.K. Ng", "H. Rong", "Z. Li"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 27 (5) (2005) 657\u2013668.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "K-plane clustering", "author": ["P.S. Bradley", "Q.L. Mangasarian"], "venue": "Journal of Global Optimization, vol. 16, no. 1, pp. 23-32, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.P. Kriegel", "J. Sander", "X. Xu"], "venue": "Proc. 2nd International Conference on KDD, pp. 226-231, 1996.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "Density-based clustering in spatial databases: The algorithm GDBSCAN and its applications", "author": ["J. Sander", "M. Ester", "H.P. Kriegel", "X. Xu"], "venue": "Data Mining and Knowledge Discovery, vol. 2, no. 2, pp. 169-194, 1998.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "An efficient approach to clustering in large multimedia databases with noise", "author": ["A. Hinneburg", "D.A. Keim"], "venue": "Proc. 2nd International Conference on KDD, pp. 58\u201365, 1998.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "OPTICS: Ordering points to identify the clustering structure", "author": ["M. Ankerst", "M.M. Breunig", "H.P. Kriegel", "J. Sander"], "venue": "Proc. International Conference on Management of Data, pp. 49\u201360, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "DECODE: a new method for discovering clusters of different densities in spatial data", "author": ["T. Pei", "A. Jasra", "D.J. Hand", "A.X. Zhu", "C. Zhou"], "venue": "Data Mining and Knowledge Discovery, vol. 18, no. 3, pp. 337\u2013369, 2009.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine. Intelligence, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "A clustering algorithm based on graph connectivity", "author": ["E. Hartuv", "R. Shamir"], "venue": "Information Processing Letter, vol. 76, no. 4, pp. 175\u2013181, 2000.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems, vol. 2, pp. 849-856, 2002.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Fast graph-based relaxed clustering for large data sets using minimal enclosing ball", "author": ["P.J. Qian", "F.L. Chung", "S.T. Wang", "Z.H. Deng"], "venue": "IEEE Trans. Systems, Man, and Cybernetics, Part B- Cybernetics, vol. 42, no. 3, pp. 672-687, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector clustering", "author": ["A. Ben-Hur", "D. Horn", "H. Siegelmann", "V. Vapnik"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 125\u2013137, 2001.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "A genetic approach to the automatic clustering problem", "author": ["L. Tseng", "S. Yang"], "venue": "Pattern Recognition, vol. 34, no. 2, pp. 415\u2013424, 2001.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "The self-organizing map", "author": ["T. Kohonen"], "venue": "Proc. IEEE, vol. 78, no. 9, pp.1464\u20131480, 1990.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Co-clustering on manifolds", "author": ["Q.Q. Gu", "J. Zhou"], "venue": "Proc. 15th International Conference on KDD, pp. 359-368, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative fuzzy clustering", "author": ["W. Pedrycz"], "venue": "Pattern Recognition Letters, vol. 23, no. 14, pp. 1675-1686, 2002.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Collaborative clustering with the use of Fuzzy C-Means and its quantification", "author": ["W. Pedrycz", "P. Rai"], "venue": "Fuzzy Sets and Systems, vol. 159, no. 18, pp. 2399-2427, 2008.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning the shared subspace for multi-task clustering and transductive transfer classification", "author": ["Q.Q. Gu", "J. Zhou"], "venue": "Proc. 9th IEEE International Conference on Data Mining, pp. 159-168,  2009.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-task clustering via domain adaptation", "author": ["Z.H. Zhang", "J. Zhou"], "venue": "Pattern Recognition, vol. 45, no. 1, pp. 465-473, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Integrating constraints and metric learning in semi-supervised clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "Proc. 21th International Conference on Machine learning, pp.81 -88, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Semantic web content analysis: A study in proximity-based collaborative clustering", "author": ["V. Loia", "W. Pedrycz", "S. Senatore"], "venue": "IEEE Trans. Fuzzy Systems, vol. 15, no. 6, pp. 1294\u20131312, 2007.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "Fuzzy clustering with a knowledge-based guidance", "author": ["W. Pedrycz"], "venue": "Pattern Recognition Letters, vol. 25, no. 4, pp. 469\u2013480, 2004.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "P-FCM: A proximity based fuzzy clustering", "author": ["W. Pedrycz", "V. Loia", "S. Senatore"], "venue": "Fuzzy Sets and Systems, vol. 148, no. 1, pp. 21\u201341, 2004.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering analysis of gene expression data based on semi-supervised visual clustering algorithm", "author": ["F.L. Chung", "S.T. Wang", "Z.H. Deng", "S.Chen", "D.W. Hu"], "venue": "Soft Comput. vol. 10, no. 11, pp. 981-993, 2006.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "A modified k-plane clustering algorithm for identification of hybrid systems", "author": ["M. Tabatabaei-Pour", "K. Salahshoor", "B. Moshiri"], "venue": "Proc. 6 World Congress on Intelligent Control and Automation, pp. 1333-1337, 2006.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Fuzzy k-Plane clustering algorithm", "author": ["Y. Wang", "S.C Chen", "D.Q. Zhang", "X.B. Yang"], "venue": "Pattern Recognition and Artificial Intelligence, vol. 20, no. 5, pp. 704-710, 2007.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "A spatially constrained fuzzy hyper-prototype clustering algorithm", "author": ["J. Liu", "T.D. Pham"], "venue": "Pattern Recognition, vol. 45, no. 4, pp. 1759-1771, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative model based clustering of directional data", "author": ["A Banerjee", "I.S. Dhillon", "J. Ghosh"], "venue": "Conf erence on Knowledge Discovery in Data, Washington, DC, 2003.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2003}, {"title": "Concept decompositions for large sparse text data using clustering", "author": ["I.S. Dhillon", "D.S.Modha1"], "venue": "Machine Learning, vol. 42 no.1, pp.143- 175, 2001.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2001}, {"title": "A convergence theorem for the fuzzy subspace clustering (FSC) algorithm", "author": ["G.J. Gan", "J.H. Wu"], "venue": "Pattern Recognition, vol. 41, no. 6, pp. 1939\u20131947, 2008.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1939}, {"title": "An entropy weighting k-means algorithm for subspace  clustering of high-dimensional sparse data", "author": ["L.P. Jing", "M.K. Ng", "Z.X. Huang"], "venue": "IEEE Trans. Knowledge and Data Engineering, vol. 19, no. 8, pp. 1026\u20131041, 2007.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhanced soft subspace clustering integrating within-cluster and between-cluster information", "author": ["Z.H. Deng", "K.S. Choi", "F.L. Chung", "S.T. Wang"], "venue": "Pattern Recognition, vol. 43, no. 3, pp. 767-781, 2010.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "An improved convergence theorem for the fuzzy c-means clustering algorithms", "author": ["R. Hathaway", "J. Bezdek", "W. Tucker"], "venue": "in: J. Bezdek (Ed.), Analysis of Fuzzy Information, vol. III, CRC Press, Boca Raton, pp. 123\u2013131, 1987.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1987}, {"title": "Non-linear Programming: A Unified Approach", "author": ["W. Zangwill"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1969}, {"title": "Distance-based clustering of CGH data", "author": ["J. Liu", "J. Mohammed", "J. Carter"], "venue": "Bioinformatics, vol. 22, no. 16, pp. 1971\u20131978, 2006.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1971}, {"title": "Co-clustering based classification for out-of-domain documents", "author": ["W.Y. Dai", "G.R. Xue", "Q. Yang", "Y. Yu"], "venue": "Proc. 13th International Conference on KDD, pp. 210-219, 2007.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}, {"title": "Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering", "author": ["A.K. McCallum"], "venue": "http://www-2.cs.cmu.edu/~mccallum/bow/.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 0}, {"title": "Weighted cluster ensembles: Methods and analysis", "author": ["C. Domeniconi", "M. Al-Razgan"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), vo. 2, no. 4, pp.1-40, 2009.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "Projective clustering ensembles", "author": ["G. Francesco", "C. Domeniconi", "A. Tagarelli"], "venue": "Data Mining and Knowledge Discovery, vol. 26, no. 3, pp. 452-511, 2013.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Transfer learning [1] is receiving more and more attentions in the fields of machine learning and data mining.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "In the past decade or so, transfer learning has been studied extensively for different applications, such as text classification [2] and indoor WiFi location estimation [3].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "In the past decade or so, transfer learning has been studied extensively for different applications, such as text classification [2] and indoor WiFi location estimation [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 1, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 106, "endOffset": 111}, {"referenceID": 2, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 106, "endOffset": 111}, {"referenceID": 3, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 106, "endOffset": 111}, {"referenceID": 4, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 106, "endOffset": 111}, {"referenceID": 5, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 106, "endOffset": 111}, {"referenceID": 6, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 106, "endOffset": 111}, {"referenceID": 7, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 106, "endOffset": 111}, {"referenceID": 8, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 157, "endOffset": 164}, {"referenceID": 9, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 157, "endOffset": 164}, {"referenceID": 10, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 202, "endOffset": 209}, {"referenceID": 11, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 202, "endOffset": 209}, {"referenceID": 12, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 202, "endOffset": 209}, {"referenceID": 13, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 202, "endOffset": 209}, {"referenceID": 14, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 202, "endOffset": 209}, {"referenceID": 15, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 250, "endOffset": 258}, {"referenceID": 16, "context": "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].", "startOffset": 250, "endOffset": 258}, {"referenceID": 17, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 102, "endOffset": 109}, {"referenceID": 18, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 102, "endOffset": 109}, {"referenceID": 19, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 102, "endOffset": 109}, {"referenceID": 20, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 102, "endOffset": 109}, {"referenceID": 21, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 102, "endOffset": 109}, {"referenceID": 22, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 102, "endOffset": 109}, {"referenceID": 23, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 150, "endOffset": 157}, {"referenceID": 24, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 150, "endOffset": 157}, {"referenceID": 25, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 150, "endOffset": 157}, {"referenceID": 26, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 150, "endOffset": 157}, {"referenceID": 27, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 150, "endOffset": 157}, {"referenceID": 28, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 196, "endOffset": 203}, {"referenceID": 29, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 196, "endOffset": 203}, {"referenceID": 30, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 196, "endOffset": 203}, {"referenceID": 31, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 196, "endOffset": 203}, {"referenceID": 32, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 274, "endOffset": 281}, {"referenceID": 33, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 274, "endOffset": 281}, {"referenceID": 34, "context": "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].", "startOffset": 274, "endOffset": 281}, {"referenceID": 15, "context": "In [16], the self-taught clustering (STC) has been proposed as a very first transfer clustering algorithm based on mutual information.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17], a transfer learning version of spectral clustering is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "For the self-taught clustering, it is based on mutual information (MI) and therefore enough data in both of source and target domains are assumed available for estimating the probability densities properly and compute the MI accordingly [16].", "startOffset": 237, "endOffset": 241}, {"referenceID": 16, "context": "For the transfer spectral clustering algorithm [17], it is specifically designed for the spectral clustering while our work focuses on the prototype based fuzzy clustering methods.", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.", "startOffset": 84, "endOffset": 92}, {"referenceID": 37, "context": "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.", "startOffset": 84, "endOffset": 92}, {"referenceID": 38, "context": "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.", "startOffset": 119, "endOffset": 127}, {"referenceID": 39, "context": "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.", "startOffset": 119, "endOffset": 127}, {"referenceID": 40, "context": "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].", "startOffset": 187, "endOffset": 194}, {"referenceID": 41, "context": "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].", "startOffset": 187, "endOffset": 194}, {"referenceID": 42, "context": "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].", "startOffset": 187, "endOffset": 194}, {"referenceID": 43, "context": "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].", "startOffset": 187, "endOffset": 194}, {"referenceID": 44, "context": "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].", "startOffset": 187, "endOffset": 194}, {"referenceID": 40, "context": "For example, the most commonly used information is \u201cmust_link\u201d or \u201cshould_not_link\u201d of the data pairs [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 129, "endOffset": 136}, {"referenceID": 18, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 129, "endOffset": 136}, {"referenceID": 19, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 129, "endOffset": 136}, {"referenceID": 20, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 129, "endOffset": 136}, {"referenceID": 22, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 183, "endOffset": 194}, {"referenceID": 45, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 183, "endOffset": 194}, {"referenceID": 46, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 183, "endOffset": 194}, {"referenceID": 47, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 183, "endOffset": 194}, {"referenceID": 48, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 183, "endOffset": 194}, {"referenceID": 49, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 183, "endOffset": 194}, {"referenceID": 21, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 244, "endOffset": 255}, {"referenceID": 50, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 244, "endOffset": 255}, {"referenceID": 51, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 244, "endOffset": 255}, {"referenceID": 52, "context": "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].", "startOffset": 244, "endOffset": 255}, {"referenceID": 18, "context": "Among the cluster center prototype based clustering methods, two well-known fuzzy algorithms are the fuzzy c-means (FCM) clustering algorithm [19] and the possibilistic c-means (PCM) clustering algorithm [20] with their objective functions defined respectively as: FCM: 2 1 1 , min C N m j i FCM ij i j J u \uf03d \uf03d \uf03d \uf02d \uf0e5 \uf0e5 U V v x (2-1)", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Among the cluster center prototype based clustering methods, two well-known fuzzy algorithms are the fuzzy c-means (FCM) clustering algorithm [19] and the possibilistic c-means (PCM) clustering algorithm [20] with their objective functions defined respectively as: FCM: 2 1 1 , min C N m j i FCM ij i j J u \uf03d \uf03d \uf03d \uf02d \uf0e5 \uf0e5 U V v x (2-1)", "startOffset": 204, "endOffset": 208}, {"referenceID": 0, "context": "st [0,1] ij u \uf0ce , 1 1 C ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , PCM: 2 1 1 1 1 , min (1 ) C N C N m m j i PCM ij i ij i j i j J u u \uf068 \uf03d \uf03d \uf03d \uf03d \uf03d \uf02d \uf02b \uf02d \uf0e5 \uf0e5 \uf0e5 \uf0e5 U V v x (2-2)", "startOffset": 3, "endOffset": 8}, {"referenceID": 0, "context": "st [0,1] ij u \uf0ce , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , where C is the number of clusters; d j R \uf0ce x is the jth data sample; \uf05b \uf05d , , T i C \uf03d V v v \uf04b is the", "startOffset": 3, "endOffset": 8}, {"referenceID": 20, "context": "Another representative cluster center prototype based clustering method is the maximal entropy clustering (MEC) algorithm [21] whose objective function is defined as", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "st [0,1] ij u \uf0ce , 1 1 C ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 ,, where [ ] ij CxN u \uf03d U is the probabilistic partition matrix whose element denotes the probability of the jth data sample belonging to the ith class, and 1 1 ln( ) C N ij ij i j u u \uf03d \uf03d \uf0e5 \uf0e5 denotes the negative Shannon entropy.", "startOffset": 3, "endOffset": 8}, {"referenceID": 22, "context": "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].", "startOffset": 98, "endOffset": 109}, {"referenceID": 45, "context": "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].", "startOffset": 98, "endOffset": 109}, {"referenceID": 46, "context": "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].", "startOffset": 98, "endOffset": 109}, {"referenceID": 47, "context": "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].", "startOffset": 98, "endOffset": 109}, {"referenceID": 48, "context": "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].", "startOffset": 98, "endOffset": 109}, {"referenceID": 49, "context": "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].", "startOffset": 98, "endOffset": 109}, {"referenceID": 22, "context": "The most representative cluster direction prototype based clustering methods are the k-plane clustering (KPC) algorithm [23] and the fuzzy KPC (FKPC) algorithm [47, 48].", "startOffset": 120, "endOffset": 124}, {"referenceID": 46, "context": "The most representative cluster direction prototype based clustering methods are the k-plane clustering (KPC) algorithm [23] and the fuzzy KPC (FKPC) algorithm [47, 48].", "startOffset": 160, "endOffset": 168}, {"referenceID": 47, "context": "The most representative cluster direction prototype based clustering methods are the k-plane clustering (KPC) algorithm [23] and the fuzzy KPC (FKPC) algorithm [47, 48].", "startOffset": 160, "endOffset": 168}, {"referenceID": 0, "context": "st [0,1] for FKPC {0,1} for KPC ij", "startOffset": 3, "endOffset": 8}, {"referenceID": 48, "context": "Another representative cluster direction prototype based clustering algorithm is the sphere k-means clustering (SKM) algorithm [49, 50] with the following objective function: SKM: 1 1 , min K N T SKM ij j i i j J u \uf03d \uf03d \uf03d \uf0e5 \uf0e5 U V x v (3-2)", "startOffset": 127, "endOffset": 135}, {"referenceID": 49, "context": "Another representative cluster direction prototype based clustering algorithm is the sphere k-means clustering (SKM) algorithm [49, 50] with the following objective function: SKM: 1 1 , min K N T SKM ij j i i j J u \uf03d \uf03d \uf03d \uf0e5 \uf0e5 U V x v (3-2)", "startOffset": 127, "endOffset": 135}, {"referenceID": 21, "context": "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].", "startOffset": 158, "endOffset": 169}, {"referenceID": 50, "context": "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].", "startOffset": 158, "endOffset": 169}, {"referenceID": 51, "context": "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].", "startOffset": 158, "endOffset": 169}, {"referenceID": 52, "context": "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].", "startOffset": 158, "endOffset": 169}, {"referenceID": 50, "context": "One representative algorithm is the fuzzy subspace clustering (FSC) algorithm [51], whose objective function is defined as FSC: \uf028 \uf029 2 1 1 1 1 1 , , min ik ik C N d C d jk ik FSC ij i j k i k J u w w x v \uf03d \uf03d \uf03d \uf03d \uf03d \uf03d \uf02d \uf02b \uf0e5 \uf0e5 \uf0e5 \uf0e5 \uf0e5 U V W \uf074 \uf074 \uf073 (4-1) .", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "st {0,1} ij u \uf0ce , 1 1 C ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , [0,1] ik w \uf0ce , 1 1 d ik k w \uf03d \uf03d \uf0e5 , 1 0 C id i w C \uf03d \uf03c \uf03c \uf0e5 , where 1 [ , , ] T C \uf03d W w w \uf04b is the matrix of weighting vectors and \uf074 is the fuzzy index of the fuzzy weighting; [ ] ij CxN u \uf03d U is the crisp partition matrix, with other parameters defined as in Eq.", "startOffset": 64, "endOffset": 69}, {"referenceID": 51, "context": "Another representative subspace cluster prototype based clustering algorithm is the entropy weighting k-means (EWKM) algorithm [52], whose objective function is defined as", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "st {0,1} ij u \uf0ce , 1 1 C ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , [0,1] ik w \uf0ce , 1 1 d ik k w \uf03d \uf03d \uf0e5 , 1 0 C id i w C \uf03d \uf03c \uf03c \uf0e5 ,", "startOffset": 64, "endOffset": 69}, {"referenceID": 18, "context": "TPFC Algorithms 1) Transfer FCM Among the cluster center prototype based clustering methods, fuzzy c-means [19] is of high popularity and is adopted here to develop our first TPFC algorithm, i.", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "st [0,1] ij u \uf0ce , 1 1 C ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , where i v\uf025 denotes the ith cluster center obtained in the source domain.", "startOffset": 3, "endOffset": 8}, {"referenceID": 19, "context": "For other popular center-prototype based clustering algorithms, such as PCM [20], one can develop the corresponding transfer learning versions in a similar manner.", "startOffset": 76, "endOffset": 80}, {"referenceID": 46, "context": "2) Transfer FKPC Among the cluster direction prototype based clustering methods, FKPC [47] is adopted here to develop the corresponding transfer clustering algorithm, i.", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "st [0,1] ij u \uf0ce , 1 1 K ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , 1 Ti i \uf03d v v where the first term is directly inherited from the original FKPC algorithm and the second term is designed to learn from the knowledge of source domain.", "startOffset": 3, "endOffset": 8}, {"referenceID": 9, "context": "[10]-[12]) for TFKPC: ( ) 2 T i i i i \uf06c \uf02d \uf03d X D S X v v (10)", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[10]-[12]) for TFKPC: ( ) 2 T i i i i \uf06c \uf02d \uf03d X D S X v v (10)", "startOffset": 5, "endOffset": 9}, {"referenceID": 50, "context": "3) Transfer FSC Among the existing subspace cluster prototype based clustering methods, the FSC [51] is adopted to study the corresponding transfer learning version, i.", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "st [0,1] ij u \uf0ce , 1 1 C ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , [0,1] ik w \uf0ce , 1 1 d i k w \uf03d \uf03d \uf0e5 .", "startOffset": 3, "endOffset": 8}, {"referenceID": 0, "context": "st [0,1] ij u \uf0ce , 1 1 C ij i u \uf03d \uf03d \uf0e5 , 1 0 N ij j u N \uf03d \uf03c \uf03c \uf0e5 , [0,1] ik w \uf0ce , 1 1 d i k w \uf03d \uf03d \uf0e5 .", "startOffset": 64, "endOffset": 69}, {"referenceID": 50, "context": "[51].", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Let\u2019s recall Zangwill\u2019s convergence theorem 1 [54, 55].", "startOffset": 46, "endOffset": 54}, {"referenceID": 54, "context": "Let\u2019s recall Zangwill\u2019s convergence theorem 1 [54, 55].", "startOffset": 46, "endOffset": 54}, {"referenceID": 50, "context": "With similar strategy in [51], we can prove the corresponding theorems 2a-2c with the auxiliary definitions 1-4.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": ", \uf07b \uf07d satisfies the constraint A C CN M B \uf03d \uf0ce U U , where CN B denotes the vector space of all real C N \uf0b4 matrices and constraint A is defined as [0, 1] ij u \uf0ce , 1 i C \uf0a3 \uf0a3 , 1 j N \uf0a3 \uf0a3 ,", "startOffset": 146, "endOffset": 152}, {"referenceID": 55, "context": ", the rand index (RI) and the normalized mutual information (NMI) [56], are used to evaluate the performance of the clustering algorithms.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "Both RI and NMI take a value within the interval [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 198, "endOffset": 206}, {"referenceID": 7, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 198, "endOffset": 206}, {"referenceID": 14, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 209, "endOffset": 217}, {"referenceID": 7, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 209, "endOffset": 217}, {"referenceID": 8, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 218, "endOffset": 226}, {"referenceID": 26, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 218, "endOffset": 226}, {"referenceID": 0, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 227, "endOffset": 236}, {"referenceID": 11, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 227, "endOffset": 236}, {"referenceID": 14, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 239, "endOffset": 248}, {"referenceID": 12, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 239, "endOffset": 248}, {"referenceID": 8, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 249, "endOffset": 257}, {"referenceID": 24, "context": "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8] \uf02d [15 , 8] [9 , 27] [ 1 , 12] \uf02d [15 , 13] [9 , 25]", "startOffset": 249, "endOffset": 257}, {"referenceID": 0, "context": "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.", "startOffset": 136, "endOffset": 143}, {"referenceID": 30, "context": "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.", "startOffset": 136, "endOffset": 143}, {"referenceID": 9, "context": "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.", "startOffset": 145, "endOffset": 153}, {"referenceID": 39, "context": "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.", "startOffset": 145, "endOffset": 153}, {"referenceID": 19, "context": "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.", "startOffset": 158, "endOffset": 166}, {"referenceID": 54, "context": "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.", "startOffset": 158, "endOffset": 166}, {"referenceID": 56, "context": "The 20 newsgroups (20NG) text datasets [57] were adopted.", "startOffset": 39, "endOffset": 43}, {"referenceID": 57, "context": "For the adopted data, dimensionality reduction has been applied by using the BOW toolkit [58] to effectively preprocess the high dimensional data into the final data containing 800 effective features used for clustering.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.", "startOffset": 75, "endOffset": 79}, {"referenceID": 38, "context": ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.", "startOffset": 153, "endOffset": 157}, {"referenceID": 39, "context": ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.", "startOffset": 203, "endOffset": 207}, {"referenceID": 0, "context": "For the proposed clustering algorithms, the parameter setting is the same as that described in subsection IV-B, and for the LSSMTC algorithm, the regularization parameter [0,1] \uf06c \uf0ce has been tried with", "startOffset": 171, "endOffset": 176}, {"referenceID": 16, "context": "The parameter setting about TSC and STC is referred to [17].", "startOffset": 55, "endOffset": 59}, {"referenceID": 58, "context": "In addition, in order to weaken the influence of the parameters, some other investigations can also be done as suggested in [59, 60].", "startOffset": 124, "endOffset": 132}, {"referenceID": 59, "context": "In addition, in order to weaken the influence of the parameters, some other investigations can also be done as suggested in [59, 60].", "startOffset": 124, "endOffset": 132}, {"referenceID": 58, "context": "By integrating the clustering results obtained under different parameter settings, ensemble clustering will provide comparable result with the case under the optimal parameter setting [59, 60].", "startOffset": 184, "endOffset": 192}, {"referenceID": 59, "context": "By integrating the clustering results obtained under different parameter settings, ensemble clustering will provide comparable result with the case under the optimal parameter setting [59, 60].", "startOffset": 184, "endOffset": 192}], "year": 2014, "abstractText": "The traditional prototype based clustering methods, such as the well-known fuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good clustering partition. If the available data is limited or scarce, most of the existing prototype based clustering algorithms will no longer be effective. While the data for the current clustering task may be scarce, there is usually some useful knowledge available in the related scenes/domains. In this study, the concept of transfer learning is applied to prototype based fuzzy clustering (PFC). Specifically, the idea of leveraging knowledge from the source domain is exploited to develop a set of transfer prototype based fuzzy clustering (TPFC) algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM, fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been chosen to incorporate with knowledge leveraging mechanism to develop the corresponding transfer clustering algorithms. Novel objective functions are proposed to integrate the knowledge of source domain with the data of target domain for clustering in the target domain. The proposed algorithms have been validated on different synthetic and real-world datasets and the results demonstrate their effectiveness when compared with both the original prototype based fuzzy clustering algorithms and the related clustering algorithms like multi-task clustering and co-clustering.", "creator": "\u00fe\u00ff"}}}