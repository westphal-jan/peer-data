{"id": "1409.5705", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "abstract": "Multiclass logistic regression (MLR) is a fundamental machine learning model to do multiclass classification. However, it is very challenging to perform MLR on large scale data where the feature dimension is high, the number of classes is large and the number of data samples is numerous. In this paper, we build a distributed framework to support large scale multiclass logistic regression. Using stochastic gradient descent to optimize MLR, we find that the gradient matrix is computed as the outer product of two vectors. This grants us an opportunity to greatly reduce communication cost: instead of communicating the gradient matrix among machines, we can only communicate the two vectors and use them to reconstruct the gradient matrix after communication. We design a Sufficient Vector Broadcaster (SVB) to support this communication pattern. SVB synchronizes the parameter matrix of MLR by broadcasting the sufficient vectors among machines and migrates gradient matrix computation on the receiver side.SVB can reduce the communication cost from quadratic to linear without incurring any loss of correctness. We evaluate the system on the ImageNet dataset and demonstrate the efficiency and effectiveness of our distributed framework.", "histories": [["v1", "Fri, 19 Sep 2014 15:42:28 GMT  (210kb,D)", "https://arxiv.org/abs/1409.5705v1", null], ["v2", "Mon, 7 Sep 2015 12:14:30 GMT  (7540kb,D)", "http://arxiv.org/abs/1409.5705v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["pengtao xie", "jin kyu kim", "yi zhou", "qirong ho", "abhimanu kumar", "yaoliang yu", "eric xing"], "accepted": false, "id": "1409.5705"}, "pdf": {"name": "1409.5705.pdf", "metadata": {"source": "CRF", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "authors": ["Pengtao Xie", "Jin Kyu Kim", "Yi Zhou", "Qirong Ho", "Abhimanu Kumar", "Yaoliang Yu", "Eric Xing"], "emails": ["jkspruce@gmail.com", "hoqirong@gmail.com", "abhimanyu.kumar@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world, in which they are able, in which they are able to move, and in which they are able, in which they are able to move."}, {"heading": "2 Sufficient Factor Property of Matrix-Parametrized Models", "text": "The core objective of Sufficient Factor Broadcasting (SFB) is to reduce network communication costs for matrix-parameterized models, especially those that follow an optimization formula (P) min W 1 N \u2211 i = 1 fi (Wai) + h (W) (1), where the model is parameterized by a matrix W (RJ) \u00b7 D. The loss function fi (\u00b7) is typically defined by a series of training samples {(ai, bi)} Ni = 1, suppressing the dependence on bi. We allow fi (\u00b7) to be either convex or nonconvex, smooth or non-smooth (with subgradients everywhere); examples include \"2 loss and multi-class logistic loss, among others. Regularizer h (W) assumes that an efficient operator allows proxh (\u00b7) [3] [S] values."}, {"heading": "2.1 Optimization via proximal SGD and SDCA", "text": "SGD. (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD. (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SGD). (SG"}, {"heading": "4 Sufficient Factor Broadcaster: An Implementation", "text": "In this section, we introduce Sufficient Factor Broadcasters (SFBcasters) - an implementation of SFBs - including consistency models, programming interfaces, and implementation details. We emphasize that SFBs do not require a specific system; they can be implemented on existing distributed frameworks, with any suitable communication topology - such as star3, ring, tree, full-connected, and 2For convenience, we assume that each worker has enough memory to keep a full copy of the parameter matrix W. If W is too large, it can be split between several machines [10, 22, 20] or local disk space (i.e., outside the core), and we plan to examine these strategies as future work."}, {"heading": "4.1 Flexible Consistency Models", "text": "Our CRC implementation supports three consistency models: Bulk Synchronous Parallel (BSP-SFB) > Asynchronous Parallel (ASP-SFB) and Stale Synchronous Parallel (SSP-SFB), and we offer theoretical convergence guarantees for BSP-SFB and SSP-SFB in the next section. BSP-SFB: Under BSP [11, 23, 38], a global final convergence ensures that all workers have completed their work, and synchronizes their parameter copies before moving on to the next iteration. BSP is a strong consistency model that guarantees the same calculation result (and thus the convergence of algorithms). ASP-SFB: BSP can react sensitively to strains (slow workers) by limiting the distributed system to the speed of the slowest worker."}, {"heading": "4.2 Programming Interface", "text": "To send SF pairs (u, v), the user adds them to a buffer object sv list signaled by commit (). Finally, to choose between BSP, ASP, and SSP consistency, the user simply sets the durability to an appropriate value (0 for BSP, for ASP, all other values for SSP). SFBcaster automatically updates the local parameter matrix of each worker using all SF pairs - including both locally compressed SF pairs added to the sv list and SF pairs received by other workers. Figure 2 shows that SFBcaster pseudoxi code for multiclogic regression."}, {"heading": "5 Implementation Details", "text": "Figure 4 shows the implementation details for each worker in SFBcaster. Each worker maintains three threads: SF Computing Thread, Parameter Update Thread, and Communication Thread. Each worker keeps a local copy of the parameter matrix and a partition of the training data. In addition, he maintains an SF Input Queue, in which the sufficient factors are calculated locally and received remotely, and an SF Queue Output, in which SFs are stored that are sent to other workers. In each iteration, the SF Computing Thread checks the consistency policy detailed in Section 4 of the main paper. If allowed, this thread randomly selects a minibatch of samples from the training data, computes the SFs, and pushes them into the input and output SF Queue. The Update Thread parameter retrieves SFs from the input SF Queue and uses them to update the parameter matrix."}, {"heading": "6 Cost Analysis and Theory", "text": "We examine the costs and convergence of communication costs in FMS O (PJD) in a linear way. We examine the costs and convergence of CRCs under synchronous and bounded-async-async costs (e.g. SSP [5, 15, 8]). We examine the costs and convergence of the behaviour of CRCs. (CRCs) We compare communication, space and time (to apply updates to W) with the costs of peer-to-peer CRCs (u, v) to other workers, i.e. O (P 2K + D) values are sent by iteration - linear in matrix dimensions J, D, and square in P. Since SF pairs cannot be aggregated before transmission, the costs are dependent on FMS K. In contrast, the communication costs in FMS O (PJD) are linear in matrix dimensions J, quadratic in P, and adratic in P."}, {"heading": "7 Experiments", "text": "(1) Multiclass Logistic Regression (MLR), (2), (3), (3), (3), (3), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (6), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (7), (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8, (8), (8), (8), (8), (8, (8), (8), (8), (8, (8), (8), (8, (8), (8, (8), (8, (8), (8), (8, (8), (8, (8), (8), (8), (8, (8), (8), (8, (8), (8, (8), (8), (8, (8), (8), (8), (8, (8), (8, (8), (8), (8, (8), (8), (, (8), (, (8), (8), (8), (, (8), (, (8), (8), (8), (, (, (8), (8), (, (8), (8), (8), (, (, (8), (8), (8),"}, {"heading": "8 Related Works and Discussion", "text": "A number of system and algorithmic solutions have been proposed to reduce communication costs in distributed ML > R updates. On the system side, [10] it has been proposed to reduce communication overhead by reducing the frequency of exchange of parameters / gradients between workers and the central server. [22] used filters to select a portion of the \"important\" parameters / updates for transmission to reduce the number of data entries to be communicated. On the algorithm side, [32] and [36] explored the target conflicts between communication and computation in the distributed dual mean or distributed dual coordinate ascent. [28] proposed an approximate Newton-like method for achieving communication efficiency in distributed optimizations. SFB is orthogonal to these existing approaches and could be combined with them to further reduce communication costs. Peer-to-peer, decentralized modal support for the system, however, has been fully investigated in other large ML architectures [21]."}, {"heading": "A Proof of Convergence", "text": "Proof of Theorem 1: Proof Fc. (=) Proof Fc. (=) Proof Fc. (=) Proof Fc. (=) Proof Fc. (=) (=) Proof Fc. (=) (=) Proof Fc. (=) Proof Fc. (=) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (P.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (. (.) (.) (.) (.) (.) (.) (.) (. (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (.) (. (.) (.) (.) (. (.) (.) (. (.) (. (. (.) (.) (.) (. (. (.) (. (. (.) (. (.) (.) (. (. (.) (. (.) (. (. (.) (. (.) (. (. (.) (. (.) (. (. (. (.) (.) (.) (. (. (.) (. (. (.) (. (.) (. (. (.) (. (. (. (.) (. (.) (.) (."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Scalable inference in latent variable models", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A.J. Smola"], "venue": "In WSDM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Nonlinear programming", "author": ["D.P. Bertsekas"], "venue": "Athena scientific Belmont,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Distributed decision-tree induction in peer-to-peer systems. Statistical Analysis and Data Mining: The ASA Data", "author": ["K. Bhaduri", "R. Wolff", "C. Giannella", "H. Kargupta"], "venue": "Science Journal,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Project adam: building an efficient and scalable deep learning training system", "author": ["T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman"], "venue": "In OSDI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "High-performance distributed ml at scale through parameter server consistency models", "author": ["W. Dai", "A. Kumar", "J. Wei", "Q. Ho", "G. Gibson", "E.P. Xing"], "venue": "In AAAI", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A local asynchronous distributed privacy preserving feature selection algorithm for large peer-to-peer networks", "author": ["K. Das", "K. Bhaduri", "H. Kargupta"], "venue": "Knowledge and information systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Powergraph: distributed graph-parallel computation on natural graphs", "author": ["J.E. Gonzalez", "Y. Low", "H. Gu", "D. Bickson", "C. Guestrin"], "venue": "In OSDI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Distributed training of large-scale logistic models", "author": ["S. Gopal", "Y. Yang"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J.K. Kim", "P.B. Gibbons", "G.A. Gibson", "G. Ganger", "E. Xing"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Passcode: Parallel asynchronous stochastic dual co-ordinate descent", "author": ["C.-J. Hsieh", "H.-F. Yu", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Communicationefficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1c", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "On model parallelization and scheduling strategies for distributed machine learning", "author": ["S. Lee", "J.K. Kim", "X. Zheng", "Q. Ho", "G.A. Gibson", "E.P. Xing"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Malt: distributed data-parallelism for existing ml applications", "author": ["H. Li", "A. Kadav", "E. Kruus", "C. Ungureanu"], "venue": "In Proceedings of the Tenth European Conference on Computer Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B.-Y. Su"], "venue": "In OSDI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "In SIGMOD,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by v1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Gossip learning with linear models on fully distributed data", "author": ["R. Orm\u00e1ndi", "I. Heged\u0171s", "M. Jelasity"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Lshtc: A benchmark for large-scale text classification", "author": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari"], "venue": "[cs.IR],", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "author": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Large-scale distributed non-negative sparse coding and sparse dictionary learning", "author": ["V. Sindhwani", "A. Ghoting"], "venue": "In KDD,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1986}, {"title": "Replicated data consistency explained through baseball", "author": ["D. Terry"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Communication/computation tradeoffs in consensus-based distributed optimization", "author": ["K. Tsianos", "S. Lawlor", "M.G. Rabbat"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "In CVPR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In NIPS,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2002}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "In NSDI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix W.", "startOffset": 129, "endOffset": 132}, {"referenceID": 34, "context": "1 Introduction For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix W.", "startOffset": 165, "endOffset": 169}, {"referenceID": 23, "context": "1 Introduction For many popular machine learning (ML) models, such as multiclass logistic regression (MLR), neural networks (NN) [7], distance metric learning (DML) [35] and sparse coding [24], their parameters can be represented by a matrix W.", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "Learning MPMs in large scale ML problems is challenging: ML application scales have risen dramatically, a good example being the ImageNet [12] compendium with millions of images grouped into tens of thousands of classes.", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 9, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 20, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 6, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 28, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 13, "context": "Consider a data-parallel algorithm, in which every worker uses a subset of the data to update the parameters \u2014 a common paradigm is to synchronize the full parameter matrix and update matrices amongst all workers [11, 10, 21, 7, 29, 14].", "startOffset": 213, "endOffset": 236}, {"referenceID": 25, "context": "In one application of MLR to Wikipedia [26], J = 325k and D > 10, 000, thus W contains several billion entries (tens of GBs of memory).", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 199, "endOffset": 203}, {"referenceID": 1, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 264, "endOffset": 267}, {"referenceID": 9, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 279, "endOffset": 283}, {"referenceID": 14, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 295, "endOffset": 299}, {"referenceID": 6, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 314, "endOffset": 317}, {"referenceID": 21, "context": "have been developed for large scale machine learning, including Bulk Synchronous Parallel (BSP) systems such as Hadoop [11] and Spark [38], graph computation frameworks such as Pregel [23], GraphLab [13], and bounded-asynchronous key-value stores such as Yahoo LDA[2], DistBelief[10], Petuum-PS [15], Project Adam [7] and [22].", "startOffset": 322, "endOffset": 326}, {"referenceID": 10, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 9, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 28, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 13, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 6, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 20, "context": "When using these systems to learn MPMs, it is common to transmit the full parameter matrices W and/or matrix updates \u2206W between machines, usually in a server-client style [11, 10, 29, 14, 7, 21].", "startOffset": 171, "endOffset": 194}, {"referenceID": 9, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 139, "endOffset": 150}, {"referenceID": 14, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 139, "endOffset": 150}, {"referenceID": 6, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 139, "endOffset": 150}, {"referenceID": 15, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 26, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 35, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 17, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 16, "context": "We focus on models with a common property: when the parameter matrix W of these models is optimized with stochastic gradient descent (SGD) [10, 15, 7] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], the update4W computed over one (or a few) data sample(s) is of low-rank, e.", "startOffset": 195, "endOffset": 215}, {"referenceID": 23, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 18, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 34, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 36, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 6, "context": "A rich set of models [24, 19, 35, 37, 7] fall into this family: for instance, when solving an MLR problem using SGD, the stochastic gradient is 4W = uv>, where u is the prediction probability vector and v is the feature vector.", "startOffset": 21, "endOffset": 40}, {"referenceID": 6, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 37, "endOffset": 40}, {"referenceID": 34, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 127, "endOffset": 131}, {"referenceID": 36, "context": "Other models include neural networks [7], distance metric learning [35], sparse coding [24], non-negative matrix factorization [19], principal component analysis, and group Lasso [37].", "startOffset": 179, "endOffset": 183}, {"referenceID": 15, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 9, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 14, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 26, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 35, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 17, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 6, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 16, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 20, "context": "SFB efficiently learns parameter matrices using the SGD or SDCA algorithms, which are widely-used in distributed ML [16, 10, 15, 27, 36, 18, 7, 17, 21].", "startOffset": 116, "endOffset": 151}, {"referenceID": 21, "context": "GraphLab, Petuum-PS and [22]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "SFs have been used to speed up some (but not all) network communication in deep learning [7]; our work differs primarily in that we always transmit SFs, never full matrices.", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 100, "endOffset": 112}, {"referenceID": 22, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 100, "endOffset": 112}, {"referenceID": 37, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 127, "endOffset": 138}, {"referenceID": 1, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 127, "endOffset": 138}, {"referenceID": 9, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 127, "endOffset": 138}, {"referenceID": 4, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 184, "endOffset": 195}, {"referenceID": 14, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 184, "endOffset": 195}, {"referenceID": 30, "context": "SFB does not impose strong requirements on the distributed system \u2014 it can be used with synchronous [11, 23, 38], asynchronous [13, 2, 10], and bounded-asynchronous consistency models [5, 15, 31], in order to trade off between system efficiency and algorithmic accuracy.", "startOffset": 184, "endOffset": 195}, {"referenceID": 34, "context": "We provide theoretical analysis of SFB under synchronous and bounded-async consistency, and demonstrate that SFB learning of matrix-parametrized models significantly outperforms strategies that communicate the full parameter/update matrix, on a variety of applications including distance metric learning [35], sparse coding [24] and unregularized/`2-regularized multiclass logistic regression.", "startOffset": 304, "endOffset": 308}, {"referenceID": 23, "context": "We provide theoretical analysis of SFB under synchronous and bounded-async consistency, and demonstrate that SFB learning of matrix-parametrized models significantly outperforms strategies that communicate the full parameter/update matrix, on a variety of applications including distance metric learning [35], sparse coding [24] and unregularized/`2-regularized multiclass logistic regression.", "startOffset": 324, "endOffset": 328}, {"referenceID": 2, "context": "assumed to admit an efficient proximal operator proxh(\u00b7) [3].", "startOffset": 57, "endOffset": 60}, {"referenceID": 23, "context": ", class labels in classification, response values in regression), or even unobserved auxiliary information (such as sparse codes in sparse coding [24]) associated with data sample i.", "startOffset": 146, "endOffset": 150}, {"referenceID": 23, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 18, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 34, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 36, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 6, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 79, "endOffset": 98}, {"referenceID": 34, "context": "This optimization problem (P) can be used to represent a rich set of ML models [24, 19, 35, 37, 7], such as the following: Distance metric learning (DML) [35] improves the performance of other ML algorithms, by learning a new distance function that correctly represents similar and dissimilar pairs of data samples; this distance function is a matrix W that can have billions of parameters or more, depending on the data sample dimensionality.", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "Sparse coding (SC) [24] learns a dictionary of basis from data, so that the data can be re-represented sparsely (and thus efficiently) in terms of the dictionary.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "In SC, W is the dictionary matrix, ai are the sparse codes, bi is the input feature vector and fi(\u00b7) is a quadratic function [24].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 14, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 6, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 20, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 155, "endOffset": 170}, {"referenceID": 15, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 26, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 35, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 17, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 16, "context": "1 Optimization via proximal SGD and SDCA To solve the optimization problem (P), it is common to employ either (proximal) stochastic gradient descent (SGD) [10, 15, 7, 21] or stochastic dual coordinate ascent (SDCA) [16, 27, 36, 18, 17], both of which are popular and well-established parallel optimization techniques.", "startOffset": 215, "endOffset": 235}, {"referenceID": 35, "context": "Stochastic DCA: SDCA applies to problems (P) where fi(\u00b7) is convex and h(\u00b7) is strongly convex [36] (e.", "startOffset": 95, "endOffset": 99}, {"referenceID": 29, "context": "most K; in Restricted Boltzmann Machines [30], the update of the weight matrix is computed from four vectors u1,v1,u2,v2 as u1v 1 \u2212 u2v 2 , i.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "rank-2; for the BFGS algorithm [4], the update of the inverse Hessian is computed from two vectors u,v as \u03b1uu> \u2212 \u03b2(uv> + vu>), i.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "as used by Project Adam [7] to learn neural networks: there, a centralized server maintains the global parameter matrix, and each client keeps a local copy.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.", "startOffset": 72, "endOffset": 84}, {"referenceID": 21, "context": "If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.", "startOffset": 72, "endOffset": 84}, {"referenceID": 19, "context": "If W is too large, one can either partition it across multiple machines [10, 22, 20], or use local disk storage (i.", "startOffset": 72, "endOffset": 84}, {"referenceID": 20, "context": "Halton-sequence [21].", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "BSP-SFB: Under BSP [11, 23, 38], an end-ofiteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration.", "startOffset": 19, "endOffset": 31}, {"referenceID": 22, "context": "BSP-SFB: Under BSP [11, 23, 38], an end-ofiteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration.", "startOffset": 19, "endOffset": 31}, {"referenceID": 37, "context": "BSP-SFB: Under BSP [11, 23, 38], an end-ofiteration global barrier ensures all workers have completed their work, and synchronized their parameter copies, before proceeding to the next iteration.", "startOffset": 19, "endOffset": 31}, {"referenceID": 14, "context": "ASP-SFB: BSP can be sensitive to stragglers (slow workers) [15, 31], limiting the distributed system to the speed of the slowest worker.", "startOffset": 59, "endOffset": 67}, {"referenceID": 30, "context": "ASP-SFB: BSP can be sensitive to stragglers (slow workers) [15, 31], limiting the distributed system to the speed of the slowest worker.", "startOffset": 59, "endOffset": 67}, {"referenceID": 12, "context": "The Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others.", "startOffset": 32, "endOffset": 43}, {"referenceID": 1, "context": "The Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others.", "startOffset": 32, "endOffset": 43}, {"referenceID": 9, "context": "The Asynchronous Parallel (ASP) [13, 2, 10] communication model addresses this issue, by allowing workers to proceed without waiting for others.", "startOffset": 32, "endOffset": 43}, {"referenceID": 14, "context": "ASP is efficient in terms of iteration throughput, but carries the risk that worker parameter copies can end up greatly out of synchronization, which can lead to algorithm divergence [15].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "Figure 3: Sample code of sparse coding in SFB SSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a boundedasynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s.", "startOffset": 88, "endOffset": 99}, {"referenceID": 14, "context": "Figure 3: Sample code of sparse coding in SFB SSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a boundedasynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s.", "startOffset": 88, "endOffset": 99}, {"referenceID": 30, "context": "Figure 3: Sample code of sparse coding in SFB SSP-SFB: Stale Synchronous Parallel (SSP) [5, 15, 31] is a boundedasynchronous consistency model that serves as a middle ground between BSP and ASP; it allows workers to advance at different rates, provided that the difference in iteration number between the slowest and fastest workers is no more than a user-provided staleness s.", "startOffset": 88, "endOffset": 99}, {"referenceID": 14, "context": "SSP alleviates the straggler issue while guaranteeing algorithm convergence [15, 31].", "startOffset": 76, "endOffset": 84}, {"referenceID": 30, "context": "SSP alleviates the straggler issue while guaranteeing algorithm convergence [15, 31].", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "When s = 0, SSP-SFB reduces to BSP-SFB [11, 38], and when s =\u221e, SSP-SFB becomes ASP-SFB.", "startOffset": 39, "endOffset": 47}, {"referenceID": 37, "context": "When s = 0, SSP-SFB reduces to BSP-SFB [11, 38], and when s =\u221e, SSP-SFB becomes ASP-SFB.", "startOffset": 39, "endOffset": 47}, {"referenceID": 6, "context": "Computational Model Total comms, per iter W storage per machine W update time, per iter SFB (peer-to-peer) O(P K(J +D)) O(JD) O(P KJD) FMS (client-server [7]) O(PJD) O(JD) O(PJD) at server, O(PKJD) at clients Figure 5: Cost of using SFB versus FMS, where K is minibatch size, J,D are dimensions of W, and P is the number of workers.", "startOffset": 154, "endOffset": 157}, {"referenceID": 21, "context": "In addition, SFBcaster possesses high elasticity [22]: new workers can be added and existing workers can be taken offline, without restarting the running framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes.", "startOffset": 4, "endOffset": 14}, {"referenceID": 14, "context": "SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes.", "startOffset": 4, "endOffset": 14}, {"referenceID": 7, "context": "SSP [5, 15, 8]) consistency, and show that SFB can be preferable to full-matrix synchronization/communication schemes.", "startOffset": 4, "endOffset": 14}, {"referenceID": 6, "context": "Cost Analysis: Table 5 compares the communications, space and time (to apply updates to W) costs of peer-to-peer SFB, against full matrix synchronization (FMS) under a client-server architecture [7].", "startOffset": 195, "endOffset": 198}, {"referenceID": 9, "context": "In order to make SFB applicable to data centers with 1000s of machines [10, 22], we note that SFB costs can be further reduced via efficient broadcast strategies: e.", "startOffset": 71, "endOffset": 79}, {"referenceID": 21, "context": "In order to make SFB applicable to data centers with 1000s of machines [10, 22], we note that SFB costs can be further reduced via efficient broadcast strategies: e.", "startOffset": 71, "endOffset": 79}, {"referenceID": 20, "context": "in the Halton-sequence strategy [21], each machine connects with and broadcasts messages to Q = O(log(P )) machines, rather than all P machines.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Our analysis differs from [5] in two ways: (1) [5] explicitly maintains a consensus model which would require transmitting the parameter matrix among worker machines \u2014 a communication bottleneck that we were able to avoid; (2) we allow subsampling in each worker machine.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "Our analysis differs from [5] in two ways: (1) [5] explicitly maintains a consensus model which would require transmitting the parameter matrix among worker machines \u2014 a communication bottleneck that we were able to avoid; (2) we allow subsampling in each worker machine.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Accordingly, our theoretical guarantee is probabilistic, instead of the deterministic one in [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 14, "context": "Note that our analysis also covers worker parameters {W p}, which improves upon analyses that only show convergence of a central server parameter [15, 8, 1].", "startOffset": 146, "endOffset": 156}, {"referenceID": 7, "context": "Note that our analysis also covers worker parameters {W p}, which improves upon analyses that only show convergence of a central server parameter [15, 8, 1].", "startOffset": 146, "endOffset": 156}, {"referenceID": 0, "context": "Note that our analysis also covers worker parameters {W p}, which improves upon analyses that only show convergence of a central server parameter [15, 8, 1].", "startOffset": 146, "endOffset": 156}, {"referenceID": 37, "context": "For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models.", "startOffset": 153, "endOffset": 161}, {"referenceID": 21, "context": "For baselines, we compare with (a) Spark [38] for MLR and L2-MLR, and (b) full matrix synchronization (FMS) implemented on open-source parameter servers [15, 22] for all four models.", "startOffset": 153, "endOffset": 161}, {"referenceID": 33, "context": "Due to data sparsity, both the update matrices and sufficient factors are sparse; we For DML, we use the parametrization proposed in [34], which learns a linear projection matrix L \u2208 Rd\u00d7k, where d is the feature dimension and k is the latent dimension.", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "This has the same communication complexity as [7], which sends SFs from clients to servers, but sends full matrices from servers to clients (which dominates the overall cost).", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "Datasets and Experimental Setup We used two datasets for our experiments: (1) ImageNet [12] ILSFRC2012 dataset, which contains 1.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "2 million images from 1000 categories; the images are represented with LLC features [33], whose dimensionality is 172k.", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "(2) Wikipedia [26] dataset, which contains 2.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "Computation time for SFB is slightly longer than FMS because (1) update matrices must be Spark is about 2x slower than PS [15, 22] based C++ implementation of FMS, due to JVM and RDD overheads.", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "Computation time for SFB is slightly longer than FMS because (1) update matrices must be Spark is about 2x slower than PS [15, 22] based C++ implementation of FMS, due to JVM and RDD overheads.", "startOffset": 122, "endOffset": 130}, {"referenceID": 9, "context": "On the system side, [10] proposed to reduce communication overhead by reducing the frequency of parameter/gradient exchanges between workers and the central server.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "[22] used filters to select part of \u201cimportant\u201d parameters/updates for transmission to reduce the number of data entries to be communicated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "On the algorithm side, [32] and [36] studied the tradeoffs between communication and computation in distributed dual averaging and distributed stochastic dual coordinate ascent respectively.", "startOffset": 23, "endOffset": 27}, {"referenceID": 35, "context": "On the algorithm side, [32] and [36] studied the tradeoffs between communication and computation in distributed dual averaging and distributed stochastic dual coordinate ascent respectively.", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "[28] proposed an approximate Newton-type method to achieve communication efficiency in distributed optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 8, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 24, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 20, "context": "Peer-to-peer, decentralized architectures have been investigated in other distributed ML frameworks [6, 9, 25, 21].", "startOffset": 100, "endOffset": 114}, {"referenceID": 4, "context": "p=1 E \uf8f0\u2016|Sp|\u2211 j\u2208Ic p \u2207fj(W p)\u2212\u2207Fp(W p)\u20162 | Fc\u22121 \uf8f9\uf8fb } {{ } \u03c3\u03022P \u2264 \u03b7 c \u03c3\u0302P, Now use the update rule W p = W c p + \u2211P p=1 Up(W c p, I c p) and the descent lemma [5], we have F (W)\u2212 F (W) \u2264 \u3008W \u2212W,\u2207F (W)\u3009+ LF 2 \u2016W \u2212W\u20162 (9)", "startOffset": 158, "endOffset": 161}], "year": 2015, "abstractText": "Matrix-parametrized models, including multiclass logistic regression and sparse coding, are used in machine learning (ML) applications ranging from computer vision to computational biology. When these models are applied to large-scale ML problems starting at millions of samples and tens of thousands of classes, their parameter matrix can grow at an unexpected rate, resulting in high parameter synchronization costs that greatly slow down distributed learning. To address this issue, we propose a Sufficient Factor Broadcasting (SFB) computation model for efficient distributed learning of a large family of matrix-parameterized models, which share the following property: the parameter update computed on each data sample is a rank-1 matrix, i.e. the outer product of two \u201csufficient factors\u201d (SFs). By broadcasting the SFs among worker machines and reconstructing the update matrices locally at each worker, SFB improves communication efficiency \u2014 communication costs are linear in the parameter matrix\u2019s dimensions, rather than quadratic \u2014 without affecting computational correctness. We present a theoretical convergence analysis of SFB, and empirically corroborate its efficiency on four different matrix-parametrized ML models.", "creator": "LaTeX with hyperref package"}}}