{"id": "1409.6075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2014", "title": "The Information Theoretically Efficient Model (ITEM): A model for computerized analysis of large datasets", "abstract": "This document discusses the Information Theoretically Efficient Model (ITEM), a computerized system to generate an information theoretically efficient multinomial logistic regression from a general dataset. More specifically, this model is designed to succeed even where the logit transform of the dependent variable is not necessarily linear in the independent variables. This research shows that for large datasets, the resulting models can be produced on modern computers in a tractable amount of time. These models are also resistant to overfitting, and as such they tend to produce interpretable models with only a limited number of features, all of which are designed to be well behaved.", "histories": [["v1", "Mon, 22 Sep 2014 03:39:23 GMT  (492kb,D)", "https://arxiv.org/abs/1409.6075v1", null], ["v2", "Mon, 6 Oct 2014 11:12:07 GMT  (996kb,D)", "http://arxiv.org/abs/1409.6075v2", null], ["v3", "Tue, 4 Nov 2014 05:41:04 GMT  (1622kb,D)", "http://arxiv.org/abs/1409.6075v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tyler ward"], "accepted": false, "id": "1409.6075"}, "pdf": {"name": "1409.6075.pdf", "metadata": {"source": "CRF", "title": "The Information Theoretically Efficient Model (ITEM): A model for computerized analysis of large datasets", "authors": ["Tyler Ward"], "emails": ["ward.tyler@gmail.com"], "sections": [{"heading": null, "text": "The Information Theoretically Efficient Model (ITEM): A modelfor computerized analysis of large datasetsTyler Ward ward.tyler @ gmail.com November 5, 2014ar Xiv: 140 9.60 75v3 [cs.LG] 4 November 201 4Contents"}, {"heading": "1 Introduction 4", "text": "1.1 Problem description................................................. 41.1.1 The analogy............................. 51.1.2. The example problem.................................. 61.1.3. The example data................."}, {"heading": "2 Efficiency Considerations 10", "text": "2.1 Computational efficiency..........................................................................................................."}, {"heading": "3 Model Choice 12", "text": "3.1 Parametric models.................................................................................................... 133.1.2 Multinomic regression.............................................................."}, {"heading": "4 An Introduction to ITEM 19", "text": "4.1 ITEM Curve Families.........................................................................................................................................................................................................................................................."}, {"heading": "5 Implementation of ITEM 24", "text": "5.1 Adjustment of ITEM curves...................................................................................................................."}, {"heading": "6 Results 32", "text": "6.1 Initial adjustment curve Convergence.............................................................................................................................................................................................................................................................................................."}, {"heading": "7 Advanced Fitting Topics 43", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Conclusion 49", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Reference Implementation 49", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 Future Work 50", "text": "References 51I Appendix: Information-Theoretic Efficiency 52AbstractThis document deals with the Information Thetically Efficient Model (ITEM), a computer-based system for generating an information-theoretically efficient multinomial logistic regression from a general dataset. Specifically, this model is successful even where the Logit transformation of the dependent variables in the independent variables is not necessarily linear. This research shows that the resulting models for large datasets can be produced in a traceable period of time on modern computers. These models are also resistant to overhaul, and as such they tend to produce interpretable models with only a limited number of features, all designed to behave well."}, {"heading": "1 Introduction", "text": "A good overview of the state of the art is provided by the article [6], which presents the general problem that ITEM should solve and briefly describes the various methods in the literature that can solve this problem."}, {"heading": "1.1 Statement of the Problem", "text": "Consider the usual regression situation: We have data (~ xi, ~ yi) for i = 1, 2,..., N. Here, ~ xi = (xi, 1, xi, 2,...., xi, K) T and ~ yi = (yi, 1, yi, 2,... yi, W) T are the regressor and response variables. Let's also assume that xi, k, R and ~ yi is a probability vector. In short, X = {~ xi} and Y = {~ yi}. A model is a function AXY (xw) whose output is a prediction for yw. In most common situations, the ModelAXY model must be derived from the data: X = {~ x1, ~ x2,....., ~ xN} and Y = {~ y1, ~ y2,..., ~ yN}, this process is considered appropriate."}, {"heading": "1.1.1 The Analogy", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "1.1.2 The Example Problem", "text": "In the United States, a mortgage is a loan made by a lender (usually a bank) to a borrower (usually an individual). To do this, the lender can examine a large dataset of historical behavior and try to build a model based on that historical picture, it is important for the lender to project the likely future behavior of the borrower. [7] This data is used to make about 600 million observations, so that they represent an example of a real model. Other similar datasets are also available, most of which contain several billion observations."}, {"heading": "1.1.3 The Example Data", "text": "A full mortgage model would include dozens of securities, but it is not helpful to enumerate them all here. \u2022 loanId: A unique ID that identifies the loan. \u2022 Month: The month of observation. \u2022 FICO: The borrower's FICO score in the making. \u2022 MTMLTV: The LTV of the loan was modified for the house price, repayment and reduction. \u2022 endStatus: The age of the loan in months. \u2022 Incentive: The interest rate improvement that the borrower would expect from the refinancing. \u2022 isOwner: Is this home occupied by its owner. \u2022 startStatus: What was the status of the loan at the beginning of the month. \u2022 endStatus: What was the status of the loan at the end of the month. Each line in the above table represents an observation. A given loan would be represented by many observations. Some of the columns (such as FICO) that do not change with time will be dependent only on the other high interest rates in the 3rd."}, {"heading": "2 Efficiency Considerations", "text": "Efficiency will be very important for the space of the models in question. When discussing here, two types of efficiency are considered. \u2022 Computational Efficiency \u2022 Information Theoretic EfficiencyA good model in this space is one that comes very close to the actual phenomenon under consideration with minimal human intervention and an appropriate amount of computing resources."}, {"heading": "2.1 Computational Efficiency", "text": "There are about 60 million mortgages in the country. Mortgage models typically require path-dependent effects. As just one example, one of the best predictors of whether a borrower will miss a payment or not is the number of months since the last missed payment. Regressors like these ensure that during each credit month transition probability can have a closed form, there is no (known) closed form for the distribution of credit status at any time more than 1 month in the future. So, if the accuracy of the credit level is desired, then numerous (e.g. 1000 +) simulations will be needed. A typical loan (on a typical path) requires an average of about 100 months of projection prior to liquidation, as the typical mortgage is refinanced approximately every 5-10 years. Moreover, the financial companies that typically use these models are to examine a significant number (e.g. 20) of interest rate and interest rate scenarios."}, {"heading": "2.2 Information Theoretic Efficiency", "text": "Therefore, it is important that the functional family of which the model consists is able to approximate the physical phenomenon in question very accurately. Appendix II contains some definitions and discussions about these factors. The important point to take out of Appendix II is that most models will not converge with the exact distribution of the data. In this problem area, the data set is large, so if the model that adapts itself is efficient in the sense that the variance of the parameters is small, then the error in the model will be dominated by the incongruity between the model form itself and the distribution of the data. The primary purpose of ITEM is to eliminate some of these inconsistencies and thus produce a more precise model.If a distribution is the limit of a sequence of models, then it is clear that in realistic cases it is the limit of such a sequence, since the number of parameters goes into infinity."}, {"heading": "3 Model Choice", "text": "There are several large families of models that could be applied to this space. < A sample is examined below >. 1. Linear Regression2. Nonlinear Regression3. Kernel Smoothing4. Neural Network5. Random ForestIn this problem space, the predicted amount in question is a probability. Therefore, linear regression is totally inappropriate as it can lead to probabilities greater than 1.0 or less than 0.0. Even if this were limited, it does not have the expected saturation behavior. We would expect that any function we choose would steadily approach a certain degree of probability as the regressors become more or less favorable. Computer costs and technical considerations eliminate core smoothing and neural networks from consideration. Neural networks themselves would go well beyond the 10,000 CPU cycle compression budgets that are provided for this model. In addition, there are problems with explanability."}, {"heading": "3.1 Parametric Models", "text": "Within mortgage modeling there are two widely used families of parametric models. 1. Structural models 2. Multinomial regressionThere are advantages and disadvantages of each of these models, which are discussed in turn."}, {"heading": "3.1.1 Structural Models", "text": "For a good overview of a structural model, see [8]. When modelers speak of structural models, they usually mean a model that is not the result of maximum probability optimization. Occasionally (as in [8]), a structural model is defined more explicitly than a hedonic model (see [9]), driven by known (e.g. economic) factors. Usually, a true structural model is both and will take the form of a collection of unrelated terms estimated by manual verification of the data set, or by drawing an analogy between the data and relevant policies or the law. The problem with this approach is that it is enormously expensive in terms of work hours, tends to have explanability problems (i.e., why has this been added rather than that), and almost always produces results in parameters that are not very optimal. It does not help that only a handful of effects can be discovered by this manual process, and that higher dimensions are not good for humans."}, {"heading": "3.1.2 Multinomial Regression", "text": "A broad class of functions that fit this description is the class of multinomial cumulative distribution functions. This family of functions takes a vector of values (often referred to as \"performance values\" or \"inclinations\") as input and produces a probability vector as output. Common examples of this family are logistical (related to the Logit function) and Gaussian CDF (related to the Probit function). If there is a strong belief in these models that the data actually follows one of these distributions, then this should definitely be taken into account. For most data sets, and especially for mortgages, there is no reason to believe that one of these distributions is a more natural fit than the other. Therefore, the Logit model is typically used because it is mathematically cheaper than the Probit model."}, {"heading": "3.2 Multinomial Logistic Regression", "text": "Since modern mortgage modeling is typically based on multinomial logistic regression, it is helpful to verify the technique yourself (\u03b2 \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3.3 Optimality of Logistic Regression", "text": "The logistic regression model is highly efficient for several reasons. First, it is crucial that only analytical functions are automatically considered. Analytical functions are always C \u221e, and therefore it is always possible for an automatic nonlinear optimizer to solve their parameters. Furthermore, analytical functions are closed under composition, so it is easy to build analytical functions using simpler analytical functions than building blocks. It should be possible to build a good model using non-analytical C \u221e functions, but such functions are rare and often difficult to construct. As a practical matter, all reasonable functions that are C \u221e are also analytical, so this is a distinction without any meaningful difference. Multinomial logistic regression is analytical, and it also has in a sense optimal compression efficiency. See Appendix I for more details. In short, each analytical CDF will be based on one or more analytical functions that are not polynomial."}, {"heading": "3.4 Splined Multinomial Logistic Regression", "text": "This year it is more than ever before."}, {"heading": "4 An Introduction to ITEM", "text": "This year is the highest in the history of the country."}, {"heading": "4.1 ITEM Curve Families", "text": "Note that for each cumulative distribution function CDF (x), which is analytical in x, the function CDF (a2 (x-b) will meet all requirements, provided it is efficient to calculate. The logistic function is an effective CDF, and it has closed form derivatives, so it is a natural choice. The formula is such that this CDF is always inclined upwards, therefore the associated \u03b2 has the expected character, positive for positive correlation, negative for negative correlation. This function also has closed form derivatives, which make it much easier to handle within the optimizer. The formula is thenCa, b (x) = 11 + e \u2212 a2 (x \u2212 b) derivatives aredda Ca, b (x) = 2a (x), b (x) which are within the optimist."}, {"heading": "4.2 Comparison to Generalized Additive Models", "text": "ITEM can be considered a special case of a generalized additive model [1], albeit one that is parametric. As the nature of the problem has eliminated all non-parametric models from consideration, no proper non-parametric GAM could be considered for this problem area. The problem is that a real GAM would require the entire dataset in memory to generate predictions, which is not realistic for multi-TB datasets. A slight modification of a GAM could be used, since the data could be captured and averaged for each parameter, then these averages would be associated with (for example) a cubic spline, which would have the advantage of fitting comparatively quickly and also being relatively efficient to use. The disadvantage is that if the bucket size is too small, there will be a lot of oscillation, but a bucket size that is too large to lose a lot of detail."}, {"heading": "4.3 Information Theoretical Considerations", "text": "As soon as an automated process for curve selection is released, it is of crucial importance to treat two information-theoretical considerations."}, {"heading": "4.3.1 Efficient Convergence", "text": "The functions selected above have been selected in such a way that most typical data distributions are well approximated by a small number of functions that can be selected individually. \u2022 The choice of functions changes the assumptions associated with our model from the classic logistic model pair to a new set of highly relaxed assumptions. \u2022 The function L \u2212 1 (~ y) is not pathological (* see below). \u2022 The terms of ~ x interact by summing up at log spacings. Note the first condition. By \"non-pathological\" we actually only mean that it is efficiently approximated by a limited number of logistic and gaussian functions. For example, the condition suffices that their Fourier transformation disintegrates quickly in frequency. Instead of requiring linearity, we only require that the function is not pathological, a much weaker condition. The second condition remains unchanged. If there are complicated interaction dates between the variables, it will still be left to a human operator to discover that the individual terms are sufficiently similar to the individual ones, as the number of interactions may be sufficiently covered."}, {"heading": "4.3.2 Resistance to Overfitting", "text": "In order to achieve optimum accuracy, the model must not contain curves that are not well supported by the dataset. This inclusion of foreign characteristics is called overfit. This requirement has two parts. 1. The model should stop calculating if it does not make sense, into further workranges2. The model must not contain characteristics that are not well supported by the dataset. The key to this is the use of an information criterion. In the case of ITEM, this is done using the AIC / BIC. These criteria differ only by a constant of (2 \u2212 ln (N)) k, with the BIC being the stricter criterion. In the results section, the choice of the stop state is discussed, as it does not usually matter that an AIC criterion can contain slightly more curves than a minor argument, where the BIC is the stricter criterion. In the results section, the choice of the stop state on the BIC curve is discussed, as it is easier on the BIC curve because it is contested on the BIC side of the IC, as it is easier on the BIC curve to differ."}, {"heading": "5 Implementation of ITEM", "text": "There are many subtle points to the actual adjustment of the ITEM model, so this process is described here."}, {"heading": "5.1 Fitting ITEM curves", "text": "This year, it is more than ever before that it will be able to put itself at the forefront in order to find its way into the future."}, {"heading": "5.2 Annealing ITEM curves", "text": "rrf\u00fc ide eeirlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5.3 Adaptive Optimization", "text": "For the ITEM model, all optimizations are taken beyond the expected value of the log probability of the dataset. (\u03b2\u03b21) These functions are as follows: (\u03b2) = 1N N \u2211 k = 0 ~ yk \u00b7 ln (~ L (\u03b2 \u00b7 F (~ xk)))) (18) In this equation, N is the number of observations, and L (\u03b2 \u00b7 F (~ xk)) is the estimated value of the transition probabilities. Considering only the subtotals of this function.fM (\u03b2) = 1M + k = 0 ~ yk \u00b7 ln (\u03b2 \u00b7 F (~ xk))) (19) It is clear that for large M, fM (\u03b2) \u2212 f (\u03b2) is distributed as N, where it is simply the standard deviation of the average element from the dataset. Optimizers are driven not by the value of f (\u03b2), but by the differences between these values."}, {"heading": "5.4 Hybrid Projection", "text": "Once a model is appropriate, it must be used to project future behavior. Although this process is not discussed in detail in this paper, an efficient means of performing this projection is described."}, {"heading": "5.4.1 Markov Matrix Multplication", "text": "If the model m contains potential states, then in any period of time the m \u00b7 m Markov matrix can be calculated at a time t (here referred to as M (t)). Then the next state ~ s (t) from ~ s (t \u2212 1) can be calculated by matrix multiplication. ~ s (t) = M (t \u2212 1) ~ s (t \u2212 1) (24) This method has two primary disadvantages. In general, a regressor such as \"number of months since the last delinquent\" is incredibly important, but non-Markovian regressors. In addition, some states waste resources on compressions for extremely rare conditions. In mortgage modeling, both disadvantages are extremely rare. If the matrix multiplication method is applied, then the majority of compilations in each month are anyway such that these loan rates are the most expensive since they are not even used for simulations."}, {"heading": "5.4.2 Simulation", "text": "An alternative to matrix multiplication is to select a random transition each month, weighted by the probability of the transition. Instead of ~ s (t), the equation now includes ~ s (t), where ~ s (t) consists of a single 1 and all other elements are zero, which means that only one column of M (t) has to be calculated. If we call this selected matrix M (t), it is composed of all zeros except a single 1 element in the randomly selected (weighted according to probability) position along the column corresponding to the 1 element in ~ s (t \u2212 1), then ~ s (t) is particularly easy to calculate now."}, {"heading": "5.4.3 Hybrid Simulation", "text": "In addition, this method should allow the use of non-Markovian regressors. If we do, then the non-Markovian regressors are not a problem, as the status of most loans is completely clear at all times. Comparing this with the simulation approach, we can assume that they always remain in C. If we do, then the non-Markovian regressors are not a problem, as the status of most loans is completely clear at all times. Comparing this with the simulation approach, we can assume that there will be a miscalculation in one of two ways. The credit transitions to P2 are over in the first case, as P is an absorbing state that does no harm. Although the non-Markovian regressors are correct, they are no longer needed."}, {"heading": "6 Results", "text": "The ITEM model was implemented as described above and applied to the Freddie Mac loan-level dataset. For the purposes of this demonstration, only a limited number of regressors was used; in a real production environment, a much larger dataset would be used, but the results would otherwise be very similar; the inclusion of several of the most important regressors will be enough to prove the point; in addition, it should be noted that only the first few million observations from the database were used; the number of observations is limited by the test machine's memory; in a live production environment, the observations used would be randomly selected and mixed; without the muffling, it is known that the observations used correspond to the oldest loans; and, in addition, the adaptive optimization is somewhat undermined by the fact that the first few blocks of calculations represent only a few (about 10k) loans that were made at approximately the same time; therefore, no curve is achievable that would not improve the adjustment of these loans, or the other order of 5 or two order of magnitude."}, {"heading": "6.1 Initial Curve Fitting Convergence", "text": "Dre eeisrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "6.2 Fitting Computational Cost", "text": "This year is the highest in the history of the country."}, {"heading": "6.3 Fitting Results", "text": "With the above example (using 1 million data points), the results are already greatly improved by the curves that have been drawn. Note, however, that all of these adjustments were calculated entirely without human intervention, except for one person, to define the regressors, and give the model the list of regressors that it can use. A typical mortgage model built within the industry takes at least one man-month of data analysis, fitting, and validation. So the ITEM model automates most of these tasks and accomplishes in minutes what a person would typically do in a few days in a more traditional environment.In these charts, the curve that was previously labeled with all the flags, but before any of the curves were drawn, does not include the effect of age or incentive. The curve that is labeled with the curves after the model is added. The graphs clearly show that even a few curves have dramatically improved the adjustment of these regressors, but before any curves have been sorted."}, {"heading": "6.4 Model Curves", "text": "A complete and complete examination of all the curves drawn by this model is outside the scope of this work. However, for illustrative reasons, the curve that refers to incentives for the 5 million point dataset is shown below. As can be seen in Figure 5, the model's response to incentives is an extremely smooth curve that quickly saturates in both positive and negative directions; this curve was presented here as a multiple. Assuming that the C \u2212 > P probability is not too high, this probability is then multiplied by the value of that curve at a given incentive value. Only relative multiplications matter here, so it does not matter whether the curve 1 happens or not. This shows that a credit with strong negative incentive is about one-third as likely to pre-pay a credit with strong positive incentive, a probability that fits with intuition and is in fact a close approximation to the historical data that can be seen in Figure 3."}, {"heading": "7 Advanced Fitting Topics", "text": "This year, more than ever before in the history of the city, in which it is more far than ever before, it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in a place, in a place, in a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is about a place, in which it is a place, in which it is about a place, in which it is a place, in which it is a place, in which it is, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is, in which it is a place, in which it is a place, in which it is a place, in which it is a place, in which it is, in which it is, in which it is a place, in which it is a place, in which it is, in which it is a place, in which it is a place, in which it is, in which it is a place, in which it is, in which it is a place is a place, in which it is a place, in which it is a place, in which it is a place, in which it is a place is a place is a place, in which it is a"}, {"heading": "8 Conclusion", "text": "The ITEM model is designed to automate repetitive tasks and enable the automatic creation of an efficient model. In the examples above, all adjustments were made automatically, without manual intervention beyond the initial definition of regressors. In a more typical mortgage model, a modeler would typically spend months analyzing data, selecting regressors, and examining model residuals. Any iteration of this process would take several days, and many dead ends would be examined, largely due to inefficient regressor and parameter selections. Generally, another process would be required to remove insignificant parameters, followed by further rounds of adjustment and analysis. The ITEM model automates this entire cycle. It performs thousands of iterations of fit, expansion, residual analysis, and subsequent re-fit, all within minutes, not days. The model does not add any insignificant job parameters, so that there is no need to pre-select or pre-select the optimal way of each of the ITEM model."}, {"heading": "9 Reference Implementation", "text": "Attached to this template is a Java reference implementation of the ITEM model core. This reference implementation provides only the core modeling functionality. To execute this model, the user must define enumerations (see edu.columbia.tjw.item.base.StandardCurveType as an example) that describe the status of the modeled phenomenon and also describe the available regressors. In addition, the user must provide a class to generate data grids that implement edu.columbia.tjw.item.ItemFittingGrid and related interfaces. Once these interfaces are implemented, the model can be used with the ItemFitter. The resulting model is suitable for projection, but no general projection code is provided in this reference implementation. Likewise, the glow code is not provided at this time, but could be provided if interest exists."}, {"heading": "10 Future Work", "text": "Estimated future work includes some results for the annealing of runs and the effects of noise initiation during assembly. In addition, the model could be applied to a larger number of regressors to show a more comprehensive mortgage model. Applying the model to large regressor sets may reveal previously unknown behavior that may be worth investigating in itself. The hybrid simulation approach could also be explored in more detail."}, {"heading": "I Appendix: Information-Theoretic Efficiency", "text": "This term can have several meanings, so this appendix actually defines the meaning for the purposes of this paper. (The first thing to note about modeling in general is that it is closely related to the concept of relative entropy, i.e. entropy against a party that has access to some models or data. In fact, a model is little more than a compression function, with all the limitations that this entails. Let's say, for example, that the datasets X and Y are available. If you want to transfer this data to another person, it might be more efficient to adjust a model first AXY (~ x) = the compression algorithms that need to shorten at least one dataset. Ideally, the entropy of the model and the residuality together would be smaller than the entropy of the answer ~ y. However, we know that any compression algorithms that shorten at least one dataset."}], "references": [{"title": "Generalized Additive Models", "author": ["Hastie", "Tibshirani"], "venue": "Statistical Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Sparse Multinomial Logistic Regression: Fast Algorithms and Generalization Bounds", "author": ["B. Krishnapuram et"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE IN- TELLIGENCE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Regression Shrinkage and Selection via the Lasso Journal of the Royal Statistical Society", "author": ["R. Tibshirani"], "venue": "Series B Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "2 Comparison to Generalized Additive Models ITEM may be considered to be a special case of a Generalized Additive Model [1], albeit one that is parametric.", "startOffset": 120, "endOffset": 123}], "year": 2014, "abstractText": "This document discusses the Information Theoretically Efficient Model (ITEM), a computerized system to generate an information theoretically efficient multinomial logistic regression from a general dataset. More specifically, this model is designed to succeed even where the logit transform of the dependent variable is not necessarily linear in the independent variables. This research shows that for large datasets, the resulting models can be produced on modern computers in a tractable amount of time. These models are also resistant to overfitting, and as such they tend to produce interpretable models with only a limited number of features, all of which are designed to be well behaved.", "creator": "LaTeX with hyperref package"}}}