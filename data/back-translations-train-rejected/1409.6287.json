{"id": "1409.6287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2014", "title": "On tensor rank of conditional probability tables in Bayesian networks", "abstract": "A difficult task in modeling with Bayesian networks is the elicitation of numerical parameters of Bayesian networks. A large number of parameters is needed to specify a conditional probability table (CPT) that has a larger parent set. In this paper we show that, most CPTs from real applications of Bayesian networks can actually be very well approximated by tables that require substantially less parameters. This observation has practical consequence not only for model elicitation but also for efficient probabilistic reasoning with these networks.", "histories": [["v1", "Mon, 22 Sep 2014 19:32:15 GMT  (19kb,D)", "http://arxiv.org/abs/1409.6287v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ji\\v{r}\\'i vomlel", "petr tichavsk\\'y"], "accepted": false, "id": "1409.6287"}, "pdf": {"name": "1409.6287.pdf", "metadata": {"source": "CRF", "title": "On tensor rank of conditional probability tables in Bayesian networks", "authors": ["Ji\u0159\u0131\u0301 Vomlel", "Petr Tichavsk\u00fd"], "emails": ["vomlel@utia.cas.cz", "tichavsk@utia.cas.cz"], "sections": [{"heading": "1 INTRODUCTION", "text": "In most cases, this is an unforeseen situation."}, {"heading": "2 CP TENSOR DECOMPOSITION", "text": "Any CPT can be considered a tensor. A tensor is simply a figure A: I \u2192 R, where I = I1 \u00b7.. \u00b7 Ik is a natural number called a tensor A, and Ij, j = 1,..., k are index records. Typically, Ij are sets of integers of cardinality nj. Then we can say that tensor A has the dimensions n1,..., nk. Tensor A rank first if it can be written as an outer product of vectors: A = a1. accords with the outer product defined for all (i1,.. \u2212 ik). \u00b7 Ik as Ai1,..., ik = a1, i1 \u00b7. \u00b7 ak, ik, where aj = (aj, i) i-tenens1,."}, {"heading": "3 NUMERICAL EXPERIMENTS", "text": "For the aforementioned rf the aforementioned rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the"}, {"heading": "4 CONCLUSIONS", "text": "We presented numerical experiments on 15 Bayesian network models from real-world applications, and the results suggest that most of the conditional probability tables can be approximated very well by tables that rank lower than one would expect from a general table of the same dimensions. Our results are consistent with the results presented in [9], where the authors experimentally verified that noisy-max provides a surprisingly good fit for up to 50% of the CPTs in two of the three Bayesian networks they tested. [4] The tensor box is freely available at http: / / www.bsp.brain.riken. jp / \u02dc phan / tensorbox.php.php.The low rank approximation should not only provide model elucidation by using a compact parameterization of the internal structure of a CPT, but also during the probable inference. We suspect that some low rank approximation CPT might provide a better reason for one of the two parent CPT specialists, but rather that one of the other CPT specialists might actually want to do it better in the case that one of two CPT."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the Czech Science Foundation through projects No. 13-20012S and No. 14-13713S."}], "references": [{"title": "Analysis of individual differences in multidimensional scaling via an n-way generalization of Eckart-Young decomposition", "author": ["J.D. Carroll", "J.J. Chang"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1970}, {"title": "Canonical probabilistic models for knowledge engineering", "author": ["F.J. D\u0131\u0301ez", "M.J. Druzdzel"], "venue": "Technical Report CISIAD-06-01,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "An efficient factorization for the noisy MAX", "author": ["F.J. D\u0131\u0301ez", "S.F. Gal\u00e1n"], "venue": "International Journal of Intelligent Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Foundations of the PARAFAC procedure: Models and conditions for an \u201dexplanatory\u201d multi-mode factor analysis", "author": ["R.A. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1970}, {"title": "A MUNIN network for the median nerve \u2014 a case study on loops", "author": ["K.G. Olesen", "U. Kj\u00e6rulff", "F. Jensen", "F.V. Jensen", "B. Falck", "S. Andreassen", "S.K. Andersen"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Low complexity damped Gauss-Newton algorithms for CANDECOMP/PARAFAC", "author": ["A.-H. Phan", "P. Tichavsk\u00fd", "A. Cichocki"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Exploiting tensor rank-one decomposition in probabilistic inference", "author": ["P. Savicky", "J. Vomlel"], "venue": "Kybernetika, 43(5),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Probabilistic inference with noisy-threshold models based on a CP tensor decomposition", "author": ["J. Vomlel", "P. Tichavsk\u00fd"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Knowledge engineering for Bayesian networks: How common are noisy-MAX distributions in practice?", "author": ["A. Zagorecki", "M.J. Druzdzel"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "They include, so called, canonical models [2] whose typical examples are noisy-or, noisy-max and noisy-threshold, etc.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 2, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 6, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 7, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 3, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 42, "endOffset": 48}, {"referenceID": 6, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 58, "endOffset": 64}, {"referenceID": 7, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "CP-tensor decomposition [4, 1] is decomposition of a tensor into a sum of r tensors of rank one.", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "CP-tensor decomposition [4, 1] is decomposition of a tensor into a sum of r tensors of rank one.", "startOffset": 24, "endOffset": 30}, {"referenceID": 6, "context": "In [7] the CP tensor decomposition (called tensor rank-one decomposition there) was applied to CPTs of canonical models.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "See [7] or [8] for a more detailed explanation, which is out of scope of this paper.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "See [7] or [8] for a more detailed explanation, which is out of scope of this paper.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "composition we used the Fast Damped Gauss-Newton (LevenbergMarquard) algorithm [6] implemented in the Matlab package TENSORBOX .", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Our results are in line with results presented in [9] where the authors experimentally verified that noisy-max provides a surprisingly good fit for as many as 50% of CPTs in two of the three Bayesian networks they tested.", "startOffset": 50, "endOffset": 53}], "year": 2016, "abstractText": "A difficult task in modeling with Bayesian networks is the elicitation of numerical parameters of Bayesian networks. A large number of parameters is needed to specify a conditional probability table (CPT) that has a larger parent set. In this paper we show that, most CPTs from real applications of Bayesian networks can actually be very well approximated by tables that require substantially less parameters. This observation has practical consequence not only for model elicitation but also for efficient probabilistic reasoning with these networks.", "creator": "LaTeX with hyperref package"}}}