{"id": "1409.6448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2014", "title": "HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection", "abstract": "In this paper, we propose a novel method for fast face recognition called L1/2 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features can be extracted at multiple scales and orientations to form Gabor-feature based image, which in turn improves the recognition rate. Besides, in the presence of occluded face image, the scale of Gabor-feature based global dictionary can be compressed accordingly because redundancies exist in Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of Gabor-feature based global dictionary can be compressed because high-dimensional face images can be rapidly represented by low-dimensional feature. By introducing L1/2 regularization, our approach can produce sparser and more robust representation compared to regularized Sparse Representation based Classification (SRC), which also contributes to the decrease of the computational cost in sparse representation. In comparison with related work such as SRC and Gabor-feature based SRC (GSRC), experimental results on a variety of face databases demonstrate the great advantage of our method for computational cost. Moreover, we also achieve approximate or even better recognition rate.", "histories": [["v1", "Tue, 23 Sep 2014 08:36:05 GMT  (899kb)", "http://arxiv.org/abs/1409.6448v1", "Submitted to IEEE Computational Intelligence Magazine in 09/2014"]], "COMMENTS": "Submitted to IEEE Computational Intelligence Magazine in 09/2014", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["bo han", "bo he", "tingting sun", "mengmeng ma", "amaury lendasse"], "accepted": false, "id": "1409.6448"}, "pdf": {"name": "1409.6448.pdf", "metadata": {"source": "CRF", "title": "HSR: \u2044 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection", "authors": ["Bo Han", "Bo He", "Tingting Sun", "Mengmeng Ma", "Amaury Lendasse"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose a novel method of fast face recognition called Regularized Sparse Representation. By using Hierarchical Feature Selection (HSR), we can compress the scope and dimension of the global dictionary, which directly contributes to reducing computing costs in the sparse representation in which our approach is strongly rooted. It consists of Gabor Waves and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor Wavelets, local features can be extracted in multiple scales and orientations to form a Gabor feature-based image, which in turn improves the detection rate. Furthermore, the scale of the Gabor feature-based global dictionary can be compressed accordingly because redundancies exist in Gabor feature-based selections."}, {"heading": "1 Introduction", "text": "Facial recognition technology plays an important role in people's lives, ranging from commercial to law enforcement applications, such as real-time surveillance, biometric identification and information security. It is one of the most difficult topics at the interface of computer vision and cognitive science. However, in recent years, extensive facial recognition research has been conducted by many psychologists, neuroscientists and engineers to increase the recognition rate. Generally, the definition of face recognition can be sufficiently formulated in terms of features such as facial expression and expression. Generally speaking, if facial recognition qualities are sufficiently provided, the features of facial recognition will be mainly related to extraction and recognition; specifically, there are two types of popular facial features, including holistic features and local features."}, {"heading": "2 Previous works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The structure of the original ELM", "text": "The Extreme Learning Machine (ELM) was proposed by Huang et.al for a faster learning speed and higher generalization performance [14] [15]. The essence of ELM is that the parameters of the hidden nodes can be randomly generated without manual tuning [16]. Specifically, the input data is mapped to an L-dimensional hidden layer and the network output is given by Eq (1). () \u2211 () (1) Where [] the output weights are between the hidden nodes and the output nodes, () if the output of the hidden layer is the input weight, the input distortion and () the activation function, they correspond to the output of the hidden node. The ELM algorithm can be summarized as follows: In given training samples {} where input data [] and the target labels [] are stable, the input data [] are mapped to a three-dimensional hidden layer []."}, {"heading": "2.2 Sparse representation based on \ud835\udc73\ud835\udfcf regularization", "text": "Intuitively, a new matrix can be defined to concatenate the total training samples of all classes: [] (6) Then, of course, the linear representation of the training samples can be written as Eq (7). (7) According to the sparse coding over-norm minimization, the sparse coefficients can be calculated as Eq (8). (8) In the case of closed data, we should express the test sample as the sum of sparse representation and errors. Then, the previous model [7] can be modified as Eq (9). (9) Where (), and the term \"energy efficiency\" can be used as an EQ class with the help of sparse coding (10)."}, {"heading": "3 \ud835\udc73\ud835\udfcf \ud835\udfd0\u2044 Regularized Sparse Representation using Hierarchical Feature Selection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Framework", "text": "\"I think it's going to take a lot of time to get to the bottom of it, but I think it's going to take a lot of time to get to the bottom of it,\" he said, adding that he had no idea if he would be able to get to the bottom of it."}, {"heading": "3.2 Hierarchical Feature Selection", "text": "For sparse representation, the compression of the global dictionary usually results from the reduction of dimension and scale (the number of elements), which directly contributes to the reduction of computational costs. We use Gabor waves and ELM-AE hierarchically for the hierarchical selection of features. By using Gabor waves, we can initially present original images by Gabor features, which can improve the detection rate. Furthermore, if the facial images are partially obscured, we can compress the scale of the Gabor feature-based occlusion dictionary by sparse encoding, which allows the scale of the Gabor feature-based global dictionary to be compressed accordingly. By using ELM-AE, high-dimensional images can quickly be represented by low-dimensional features, allowing us to compress the dimension of the Gabor feature-based global dictionary and test images."}, {"heading": "3.2.1 Gabor-Feature based Image Representation and Occlusion Dictionary", "text": "The motivation we choose is primarily due to its biological relevance and its arithmetical properties. In this section, we will first of all ask the question how we can represent the image of Gabor (Gabor-characteristic) in its entirety. () Second, we will examine the situation of Gabor (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor-characteristic) in its entirety (Gabor) in its entirety (Gabor-characteristic) in its entirety (Gabor) in its entirety (Gabor-characteristic) in its entirety (Gabor) in its entirety (Gabor-characteristic) in its entirety (Gabor) in its entirety (Gabor) in its entirety (Gabor)"}, {"heading": "3.2.2 ELM-AE for High-Dimensional Images Representation", "text": "In fact, it is so that we will be able to exercise in the reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary reactionary Jnn."}, {"heading": "3.3 \ud835\udc73\ud835\udfcf \ud835\udfd0\u2044 Regularized Sparse Representation", "text": "For detection modeling, our approach is strongly rooted in the sparse representation due to the good detection rate and robustness of covered faces. In this section, we first introduce a generic framework of sparse representation, and then compare various regulated parameters such as \u2044, and. Finally, we opt for \u2015 because it can produce a sparser and more robust representation that speeds up facial recognition."}, {"heading": "3.3.1 Generic Framework of Sparse Representation for Face Recognition", "text": "A classical assumption is that a new image of the class can well be presented as a linear combination of all training samples. (22) However, where the identity of the test image is unknown, the problem becomes more complex because we will present the test image using the training dictionary. Thus, the linear representation of the test image can be written as an equation (22). (22) Where is a vector of sparse coefficients. In practical situations, the coefficient vector is often complicated due to the presence of partially closed faces. The linear model should be modified as an equation (23). [] (23) Where is a vector of errors, [] and [] where is a vector of sparse coefficients, so that face recognition in the presence of occlusion parameters can be complicated."}, {"heading": "3.3.2 Regularized Parameters: \ud835\udc73 , \ud835\udc73\ud835\udfcf \ud835\udfd0\u2044 and \ud835\udc73\ud835\udfcf", "text": "The motivation for choosing regulation stems from two aspects: First, although the sparsest coefficients can be achieved through -norm minimization, the solution of -norm minimization is a NP-hard problem. Therefore, they first proved that regulation is the same as regulation based on certain constraints. In practice, however, we find that -norm minimization does not usually produce the sparsest solution, so that a new regulated parameter can be introduced that offers a more sparse solution than regulation. Fortunately, Xus \"experimental results have shown that regulation can produce a more sparse representation compared to regulation, which is also proven by its geometry."}, {"heading": "4 Experimental results", "text": "In this section, we present some experimental results on available benchmark databases to compare the performance of the proposed HSR algorithm with GSRC and SRC. [24] The reason why HSR is not compared with deep learning algorithms is due to the general fact that their computing costs are very expensive. To comprehensively evaluate the performance of HSR, this section is divided into two detailed sections. In Section 4.1, we first tested our method on non-occlusion face data sets, and then in Section 4.2, we tested the new method on face data sets against occlusion using two different frameworks (no partition and partition). All simulations for the HSR, GSRC and SRC algorithms are performed in Matlab 7.8 environment, which is run in an Intel Xeon E5-1650 CPU. In the experiments of gabor wavesystems, the parameters are set as urgency, urgency, algorithm and orientation algorithms."}, {"heading": "4.1 Face recognition without occlusion", "text": "In fact, we will be able to put ourselves first without having to manoeuvre ourselves into an impasse."}, {"heading": "4.2 Face recognition with occlusion", "text": "In this section, we will also compare the performance of the SRC, GSRC on a subset of AR Dataset that includes locked images. The selected subset consists of images of individuals (male and female) on which all subjects wear their sunglasses. The second series of tests also includes images of individuals who have opted for training, and the remaining data is divided into two separate groups. The first series of tests contains images of all subjects wearing their sunglasses."}, {"heading": "5 Conclusions", "text": "In this paper, we propose a new method for fast face recognition, called \u2044 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By using hierarchical feature selection, we can extract the local features from the image, which improves detection rate as local features are less sensitive to facial variation. More importantly, the global dictionary can be easily compressed into dimensions and scales by hierarchical feature selection, speeding up the calculation of sparse representation. To be more precise, it is possible to compress the scale of Gabor feature-based occlusion caricature caricature caricature caricature space using ELM-AE. By introducing a more regulated representation, our method can produce a sparer representation than a sparer representation, and high-dimensional images and global dictionaries can quickly be compressed into the low-dimensional feature space using ELM-AE."}, {"heading": "Acknowledgements", "text": "The authors thank Dr. Jun Zhou at Griffith University for helpful and excellent discussions and comments. This work is supported in part by the Natural Science Foundation of China (41176076, 31202036, 51075377).Reference [1] W.Y. Zhao, R. Chellppa, P.J. Phillips, A. Rosenfeld, Face recognition: A liter-ature survey. ACM Computing Survey 35 (2003) 399-459. [2] M. Turk, A. Pentland for recognition. J. Cognitive Neuroscience 3 (1991) 71-86. [3] P.N. Belhumeur, J.P. Hespanha, D.J. Kriengman, Eigenfaces vs. fisherfaces using class specific linear projection. IEEE PAMI 19 (1997) 711-720. [4] Barkan, Oren, et al."}], "references": [{"title": "Face recognition: A literature survey", "author": ["W.Y. Zhao", "R. Chellppa", "P.J. Phillips", "A. Rosenfeld"], "venue": "ACM Computing Survey 35 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "J. Cognitive Neuroscience 3 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Eigenfaces vs", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriengman"], "venue": "fisherfaces: Recognition using class specific linear projection. IEEE PAMI 19 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Fast high dimensional vector multiplication face recognition", "author": ["Barkan", "Oren"], "venue": "Computer Vision (ICCV),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Linear subspace learning-based dimensionality reduction", "author": ["Jiang", "Xudong"], "venue": "Signal Processing Magazine, IEEE", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Lie bodies: A manifold representation of 3D human shape", "author": ["O. Freifeld", "M.J. Black"], "venue": "Computer Vision\u2013ECCV 2012. Springer Berlin Heidelberg ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["J. Wright", "A.Y. Yang"], "venue": "Ganesh, et al., Robust face recognition via sparse representation, Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(2) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Gabor feature based sparse representation for face recognition with gabor occlusion dictionary", "author": ["M. Yang", "L. Zhang"], "venue": "Computer Vision\u2013ECCV 2010. Springer Berlin Heidelberg, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Face Recognition Using Gabor Wavelet for Image Processing Applications", "author": ["Sharma", "Buddhi Prakash", "Rajesh Rana", "Rajesh Mehra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Extreme Learning Machines", "author": ["Cambria", "Erik"], "venue": "IEEE Intelligent Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "The effect of non-linear dimension reduction on Gabor filter bank feature space", "author": ["Gupta", "Hitesh A", "Anirudh Raju", "Abeer Alwan"], "venue": "The Journal of the Acoustical Society of America", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition", "author": ["C. Liu", "H. Wechsler"], "venue": "IEEE IP 11 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Data modeling: Visual psychology approach and L1/2 regularization theory", "author": ["Z B. Xu"], "venue": "Proc. Int. Congress of Mathmaticians. ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Extreme learning machine: theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing 70(1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on. IEEE, 2 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.B. Huang", "H. Zhou", "X. Ding"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 42(2) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Methods for computing the Moore-Penrose generalized inverse", "author": ["B. Noble"], "venue": "and related matters. Generalized Inverses and Applications, ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1976}, {"title": "Autoencoders, Unsupervised Learning, and Deep Architectures", "author": ["Baldi", "Pierre"], "venue": "ICML Unsupervised and Transfer Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Convex incremental extreme learning machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing, 70(16) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Factor analysis and AIC", "author": ["H. Akaike"], "venue": "Psychometrika, 52(3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1987}, {"title": "Multimodel inference understanding AIC and BIC in model selection", "author": ["K.P. Burnham", "D.R. Anderson"], "venue": "Sociological methods & research, 33(2) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "The Journal of Machine Learning Research, 7 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Genomic selection using regularized linear regression models: ridge regression, lasso, elastic net and their extensions", "author": ["Ogutu", "Joseph O", "Torben Schulz-Streeck", "Hans-Peter Piepho"], "venue": "BMC proceedings. Vol. 6. No. Suppl 2. BioMed Central Ltd,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning deep architectures for AI. Foundations and trends\u00ae in Machine Learning", "author": ["Bengio", "Yoshua"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Acquring linear subspaces for face recognition under variable lighting", "author": ["K. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE PAMI 27 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "R", "author": ["A. Martinez"], "venue": "benavente, The AR face database ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "The FERET evaluation methodology for face recognition algorithms", "author": ["P.J. Phillips", "H. Moon", "S.A. Rizvi", "P. Rauss"], "venue": "IEEE PAMI 22 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The technique of face recognition plays an important role in people\u2019s life ranging from commercial to law enforcement applications, such as real-time surveillance, biometric personal identification, and information security[1].", "startOffset": 238, "endOffset": 241}, {"referenceID": 1, "context": "However, the classical methods using holistic features such as Eigenface[2], Fisherface[3] and Randomface are hardly to reveal the essential structures of high-dimensional faces[4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "However, the classical methods using holistic features such as Eigenface[2], Fisherface[3] and Randomface are hardly to reveal the essential structures of high-dimensional faces[4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "However, the classical methods using holistic features such as Eigenface[2], Fisherface[3] and Randomface are hardly to reveal the essential structures of high-dimensional faces[4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "Therefore, researchers recently prefer local-feature based methods like subspace learning[5] or manifold representation[6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Therefore, researchers recently prefer local-feature based methods like subspace learning[5] or manifold representation[6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 6, "context": "Recently, Wright and Ma[7] reported their work called the sparse representation based classification (SRC).", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Recently, Yang and Zhang\u2019s work (Gabor-feature based SRC (GSRC)) [8] claimed that if Gabor wavelets[9] can be employed in feature extraction, it is possible to obtain a much more compact occlusion dictionary in the presence of occluded faces, which not only speeds up the computation but also improves the recognition rate.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "Recently, Yang and Zhang\u2019s work (Gabor-feature based SRC (GSRC)) [8] claimed that if Gabor wavelets[9] can be employed in feature extraction, it is possible to obtain a much more compact occlusion dictionary in the presence of occluded faces, which not only speeds up the computation but also improves the recognition rate.", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE)[10] hierarchically.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "To be more specific, Gabor wavelets could effectively extract local features at multiple scales and orientations[11] forming Gabor-feature based images, which can greatly improve the recognition rate.", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Finally, the Gabor-feature based methods have been applied into face recognition leading to state-of-the-art recognition rate[12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "In the recognition modeling, the main difference between our method HSR and SRC is that -norm minimization is replaced by \u2044 -norm minimization[13] because \u2044 -norm minimization can produce sparser representation, which directly decreases the computational cost of sparse representation.", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "In comparison with related work such as SRC[7] and GSRC[8], experimental results demonstrated that our method is slightly complicated in structure, but it shows the great advantage for computational cost.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "In comparison with related work such as SRC[7] and GSRC[8], experimental results demonstrated that our method is slightly complicated in structure, but it shows the great advantage for computational cost.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "al for faster learning speed and higher generalization performance[14][15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "al for faster learning speed and higher generalization performance[14][15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The essence of ELM is that the parameters of the hidden nodes can be generated randomly without manually tuning[16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "Where [15][17] denotes the Moore-Penrose generalized inverse of matrix .", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "Where [15][17] denotes the Moore-Penrose generalized inverse of matrix .", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "To make the resultant solution more stable and have better generalization performance[10], a positive value \u2044 as a regularization term can be added to the diagonal of shown Eq(4).", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Then the previous model[7] can be modified as Eq(9).", "startOffset": 23, "endOffset": 26}, {"referenceID": 12, "context": "Second, Xu\u2019s experiments[13] demonstrated that the performance of sparse representation using \u2044 regularization is stronger than that using other regularization (0<p<1/2 or 1/2<p<1).", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "More specifically, Gabor wavelets[12] usually demonstrate good characteristics of spatial locality and orientation selectivity.", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "The occlusion dictionary in SRC is normally selected as the identity matrix [7], so SRC has a large number of elements in occlusion dictionary, which definitely increases the computational cost of optimization.", "startOffset": 76, "endOffset": 79}, {"referenceID": 17, "context": "For Auto-Encoder[18], the output data  \u0302 is similar to the input data .", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "al[10], and the main objective of ELM-AE is to represent the input data meaningfully and rapidly.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "To be more specific, we first modify the basic ELM[15][19] to conduct unsupervised learning ( ), and random weights and biases of the hidden nodes are chosen to be orthogonal because orthogonalization will make the generalization of ELM-AE better.", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "To be more specific, we first modify the basic ELM[15][19] to conduct unsupervised learning ( ), and random weights and biases of the hidden nodes are chosen to be orthogonal because orthogonalization will make the generalization of ELM-AE better.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "For example, if general loss function can be denoted as square loss, then the framework can be converted into AIC[20] and BIC[21] criteria when , the framework can be converted into Lasso algorithm[22] when , the framework can be converted into ridge regression[23] when .", "startOffset": 261, "endOffset": 265}, {"referenceID": 12, "context": "Second, although we initially want to explore other possibilities like -norm minimization (0<p<1/2 or 1/2<p<1), Xu\u2019s experiments[13] clearly demonstrated that the performance of sparse representation using \u2044 regularization is stronger than that using other regularization (0<p<1/2 or 1/2<p<1).", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "The reason why HSR does not compare with deep learning algorithms[24] is due to the common fact that their computational cost is very expensive.", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "We compared the performance of the HSR with two classical algorithms SRC and GSRC on three typical facial image databases: Extended Yale B[25], AR[26] and FERET[27].", "startOffset": 138, "endOffset": 142}, {"referenceID": 25, "context": "We compared the performance of the HSR with two classical algorithms SRC and GSRC on three typical facial image databases: Extended Yale B[25], AR[26] and FERET[27].", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "We compared the performance of the HSR with two classical algorithms SRC and GSRC on three typical facial image databases: Extended Yale B[25], AR[26] and FERET[27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 6, "context": "In our experiments, we set [7] in HSR, GSRC and SRC by our experience.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "We quoted the approach in [Wright][7] to partition the whole image into blocks and processed each block independently, assuming the occlusion part is contiguous.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "[1] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Barkan, Oren, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jiang, Xudong.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] O.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Sharma, Buddhi Prakash, Rajesh Rana, and Rajesh Mehra.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Cambria, Erik, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Gupta, Hitesh A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Z B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Baldi, Pierre.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Ogutu, Joseph O.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Bengio, Yoshua.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] P.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "In this paper, we propose a novel method for fast face recognition called \u2044 Regularized Sparse Representation using Hierarchical Feature Selection (HSR). By employing hierarchical feature selection, we can compress the scale and dimension of global dictionary, which directly contributes to the decrease of computational cost in sparse representation that our approach is strongly rooted in. It consists of Gabor wavelets and Extreme Learning Machine Auto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features can be extracted at multiple scales and orientations to form Gabor-feature based image, which in turn improves the recognition rate. Besides, in the presence of occluded face image, the scale of Gabor-feature based global dictionary can be compressed accordingly because redundancies exist in Gabor-feature based occlusion dictionary. For ELM-AE part, the dimension of Gabor-feature based global dictionary can be compressed because high-dimensional face images can be rapidly represented by low-dimensional feature. By introducing \u2044 regularization, our approach can produce sparser and more robust representation compared to regularized Sparse Representation based Classification (SRC), which also contributes to the decrease of the computational cost in sparse representation. In comparison with related work such as SRC and Gaborfeature based SRC (GSRC), experimental results on a variety of face databases demonstrate the great advantage of our method for computational cost. Moreover, we also achieve approximate or even better recognition rate.", "creator": "Microsoft\u00ae Word 2010"}}}