{"id": "1409.7384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2014", "title": "A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure", "abstract": "Feature subset selection, as a special case of the general subset selection problem, has been the topic of a considerable number of studies due to the growing importance of data-mining applications. In the feature subset selection problem there are two main issues that need to be addressed: (i) Finding an appropriate measure function than can be fairly fast and robustly computed for high-dimensional data. (ii) A search strategy to optimize the measure over the subset space in a reasonable amount of time. In this article mutual information between features and class labels is considered to be the measure function. Two series expansions for mutual information are proposed, and it is shown that most heuristic criteria suggested in the literature are truncated approximations of these expansions. It is well-known that searching the whole subset space is an NP-hard problem. Here, instead of the conventional sequential search algorithms, we suggest a parallel search strategy based on semidefinite programming (SDP) that can search through the subset space in polynomial time. By exploiting the similarities between the proposed algorithm and an instance of the maximum-cut problem in graph theory, the approximation ratio of this algorithm is derived and is compared with the approximation ratio of the backward elimination method. The experiments show that it can be misleading to judge the quality of a measure solely based on the classification accuracy, without taking the effect of the non-optimum search strategy into account.", "histories": [["v1", "Thu, 25 Sep 2014 11:57:48 GMT  (2712kb)", "https://arxiv.org/abs/1409.7384v1", "Submitted to IEEETrans On Pattern Analysis and Machine Intelligence"], ["v2", "Wed, 12 Nov 2014 13:56:54 GMT  (2712kb)", "http://arxiv.org/abs/1409.7384v2", "IEEETrans On Pattern Analysis and Machine Intelligence"]], "COMMENTS": "Submitted to IEEETrans On Pattern Analysis and Machine Intelligence", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tofigh naghibi", "sarah hoffmann", "beat pfister"], "accepted": false, "id": "1409.7384"}, "pdf": {"name": "1409.7384.pdf", "metadata": {"source": "CRF", "title": "A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure", "authors": ["Tofigh Naghibi"], "emails": ["naghibi@tik.ee.ethz.ch", "hoffmann@tik.ee.ethz.ch", "pfister@tik.ee.ethz.ch"], "sections": [{"heading": null, "text": "ar Xiv: 140 9.73 84v2 [cs.LG] 1 2N ovIndex Terms - Feature Selection, Mutual information, Convex objective, Approximation ratio."}, {"heading": "1 INTRODUCTION", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2 MUTUAL INFORMATION PROS AND CONS", "text": "Consider a N-dimensional feature vector X = [X1, X2, XN] and a dependent variable C, which can be either a class name in the case of classification or a target variable in the case of regression. Mutual information function is defined as a distance from the independence between X and C, measured by the Kullback-Leibler divergence [11]. However, mutual information measures the amount of information shared between X and C by measuring their level of dependence. Denote the common property of X and C and their marginal distributions by Pr (X, C), Pr (X) and Pr (C), or the mutual information between the feature vector and the class name can be defined as follows: I (X1, X2,., XN; C) = I (X) = I (X) = Pr (X) = Pr (C) Pr (C) Pr (X), C) Pr (C), C (X) dr (X) (X)."}, {"heading": "2.1 First Expansion: Multi-way Mutual Information Expansion", "text": "The first expansion of mutual information used here is based on the natural extension of mutual information to more than two random variables proposed by McGill (30) and Abramson [1]. According to her proposal, the triple reciprocal information between random variables Yi is defined by: I (Y1; Y2; Y3) = I (Y1; Y3) = I (Y1; Y3) \u2212 I (Y1, Y2; Y3) \u2212 I (Y1; Y2) (Y3) (7) where \"between variables the common variables are referred to. Note that similar to the double reciprocal information is symmetrical with respect to Yi variables, i.e., I (Y1; Y3) = I (Y2; Y1). Generalization of N variables: I (Y1; Y2; YN) = I (Y2)."}, {"heading": "2.2 Second Expansion: Chain Rule of Information", "text": "The second expansion for mutual information is based on the chain rule of information [11]: I (X; C) = N \u2211 i = 1I (Xi; C | Xi \u2212 1,.., X1) (11) The chain rule of information allows the selection of the order quite flexibly. For example, the right side can be written in the order (X1,., X2,.., XN) or (XN, XN \u2212 1,., X1). In general, it can be extended via N! different permutations of the feature set {X1,.,., XN}. If one considers the sum of all possible expansions, one can conclude that (N!) I (X; C) = (Xi \u2212 1)! N \u2211 i = (Xi; C) (12) + (N \u2212 2)! N \u2211 i1 = 1 (X1,."}, {"heading": "2.3 Truncation of the Expansions", "text": "In the two proposed expansions (9) and (13), mutual information dates with more than two characteristics represent interaction properties of higher order. Neglecting the terms of higher order results in the so-called truncated approximation of the mutual information function. If we ignore the constant coefficient in (13), the truncated forms of proposed expansions can be written as follows: D1 = N \u2211 i = 1I (Xi; C) \u2212 N \u2212 1 \u2211 i + 1I (Xi; Xj; C) (14), where D1 is the truncated approximation of (9) and D2 for (13). Interestingly, despite the very similar structure of the expressions in (14), the difference is intrinsically different."}, {"heading": "2.3.1 JMI, mRMR & MIFS Criteria", "text": "Several known criteria, including Joint Mutual Information (JMI) [31], Minimum Redundancy Maximum Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can be derived directly from D1 and D2. Using the identity: I (Xi; Xj; C) = I (Xi; C) + I (Xj; C) \u2212 I (Xi, Xj; C) in D2 shows that D2 is equivalent to JMI.JMI = D2 = N \u2212 1 \u2211 i = 1N \u2211 j = i + 1I (Xi, Xj; C) (15) Using I (Xi; Xj; C) = I (Xi; Xj | C) \u2212 I (Xi; Xj | C) \u2212 I (Xi; Xj | C) \u2212 I (Xi) \u2212 I (Xj) \u2212 I (1I = 1I = 1H = 1H = 1H (Xj) = 1H = 1H = 1H = 1H."}, {"heading": "2.3.2 Implicitly Assumed Distribution", "text": "A natural question that arises in this context with respect to the proposed conditional and conditional independence between random variables is: under what probable assumptions will the proposed approaches become mutual information functions? That is, what structure should a common pdf add to generate mutual information in the form of D1 or D2? However, if we assume that the features differ from each other and in the form of conditional information, then it is easy to verify that mutual information has the form of MaxRelevance introduced in the form of MaxRelevance. These two assumptions define the adjusted independence map of Pr (C), where the independence map is defined by a common probability distribution. Definition 1: An independence map (i-map) is a view stable table or a set of rules that all have conditional and conditional independence between random variables."}, {"heading": "2.3.3 MIFS Derivation from Kirkwood Approximation", "text": "It is shown in [26] that the abbreviation of the common entropy H (X) in the first order (1st order) is equivalent to the approximation of the full-dimensional pdf Pr (X) by means of common pdfs to the dimensionality of r or less. However, the second order Kirkwood approximation of Pr (X) can be described as follows [26]: The P-r (X) -r (X) -1 i = 1% N \u2212 j = i + 1 Pr (Xi, Xj) [... N \u2212 i = 1 Pr (Xi)]] n \u2212 2 (18) Now we assume that the following two assumptions apply: Characteristics are conditionally independent, that is: Pr (X | C) = 1 Pr \u2212 n \u2212 probable."}, {"heading": "2.4 D2 Approximation", "text": "Of our experiments, which we omit due to space constraints, D2 tends to underestimate the mutual information, while D1 shows a great overestimation of independent features and a great underestimation (even negativity) in the presence of dependent features. Generally, D2 shows more robustness than D1. However, the same results can be observed for mRMR derived from D2 and MIFS. Previous work also came to the same results and reported that mRMR works better and more robust than MIFS, especially when the group of features is large. Therefore, we use D2 as a truncated approximation in the following sections. For simplicity, the subscript is dropped and it is pared as follows: D ({X1,., XN}) = N-Score i = 1I (Xi; C) (20) \u2212 1N \u2212 1N \u2212 i = i + 1I (Xi; Xj; C), although a D-Score is not suitable for a larger measurement."}, {"heading": "3 SEARCH STRATEGIES", "text": "In view of a measurement function 1D, the subset of P-N characteristics (SSP) can be defined as follows: Definition 2: In view of the N characteristics Xi and a dependent variable C, a subset of P-N characteristics can be selected that maximizes the measurement function. It is assumed that the cardinality P is known to the optimal feature subset. In practice, the exact value of P can be obtained by evaluating subsets for different values of cardinality P with the final induction algorithm. Note that it is fundamentally different from the wrapper methods. While the wrapper methods require testing 2N subsets, here at most N runs of the learning algorithm are required to evaluate all possible values of P. A search strategy is an algorithm that attempts to find a feature subset in the feature subset space with 2N characteristics 2 that optimizes the measurement function."}, {"heading": "3.1 Convex Based Search", "text": "The forward algorithm (FS) iteratively selects a set S of size P as follows: 1) Initializes S0 = \u2205. 2) In each iteration i, select the attribute Xm maximizingD (Si \u2212 1) and set Si = Si \u2212 1 - Xm. 3) Similarly, the reverse elimination (BE) can be called: 1) Start with the complete set of attribute SN. 2) Iteratively remove a variable Xm maximizingD (Si \u2212 1), and set Si \u2212 1 = Si\\ Xm, where removing X from S\\ X.3) Output SP. An experimental comparative evaluation of several variants of these two algorithms was performed in [2]. From an information theory standpoint, the disadvantage of the forward selection method is that it can only evaluate the benefit of a single attribute in the limited context of the previously selected artificial features."}, {"heading": "3.2 Approximation Analysis", "text": "In order to gain more insight into the quality of a measurement function, it is essential to be able to examine problems directly. However, since estimating the exact mutual information value in real data is not practicable, it is not possible to evaluate the measurement function directly. Its quality can only be examined indirectly by the final classification performance (or other measurable criteria). However, the quality of a measurement function is not the only factor contributing to the classification rate. Since SSP is a NP-hard problem, the search strategy can only find a local optimal solution. That is, in addition to the quality of a measurement function, the inaccuracy of the search strategy also contributes to the final classification error. Thus, in order to draw a conclusion about the quality of a measurement function, it is essential to have an insight into the accuracy of the search strategy in use. In this section, we compare the accuracy of the proposed method with the traditional backward-looking elimination approach.A standard approach to investigating the accuracy of an optimization algorithm is as close to the problem as it comes to the optimal analysis."}, {"heading": "4 EXPERIMENTS", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5 CONCLUSION", "text": "In this paper, a convex-based parallel search strategy for character selection, COBRA, was proposed, whose approximation ratio was derived and compared with the approximation ratio of the reverse elimination method. It was experimentally demonstrated that COBRA outperforms sequential search methods, especially for sparse data. Furthermore, we presented two series extensions for mutual information and demonstrated that most of the mutual information-based score functions in literature, including mRMR and MIFS, are truncated approximations of these extensions. Furthermore, the underlying link between MIFS and the Kirwood approximation was investigated, and it was shown that the assumption of conditional class independence and Kirkwood approximation for Pr (X) reduces mutual information to the MIFS criterion."}, {"heading": "6 ACKNOWLEDGMENTS", "text": "This work was partially supported by the Swiss National Science Foundation (SNSF)."}], "references": [{"title": "Information theory and coding", "author": ["N. Abramson"], "venue": "McGraw-Hill, New York", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1963}, {"title": "A comparative evaluation of sequential feature selection algorithms", "author": ["D.W. Aha", "R.L. Bankert"], "venue": "Learning from Data: Artificial Intelligence and Statistics V. Springer-Verlag", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Greedily finding a dense subgraph", "author": ["Y. Asahiro", "K. Iwama", "H. Tamaki", "T. Tokuyama"], "venue": "Journal of Algorithms", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "On the feature selection criterion based on an approximation of multidimensional mutual information", "author": ["K.S. Balagani", "V.V. Phoha"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Trans. on Neural Networks, 5:537\u2013550", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Measure Theory", "author": ["V.I. Bogachev"], "venue": "Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A new perspective for information theoretic feature selection", "author": ["G. Brown"], "venue": "Proceedings of Artificial Intelligence and Statistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection", "author": ["G. Brown", "A. Pocock", "M. Zhao", "M. Luj\u00e1n"], "venue": "Journal of Machine Learning", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "An evolving system based on probabilistic neural network", "author": ["P.M. Ciarelli", "E.O.T. Salles", "E. Oliveira"], "venue": "Proceedings of BSNN", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "An evaluation of feature selection methods and their application to computer security", "author": ["J. Doak"], "venue": "Technical Report CSE-92-18, University of California at Davis", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Transmission of Information: A Statistical Theory of Communications", "author": ["R. Fano"], "venue": "The MIT Press, Cambridge, MA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1961}, {"title": "A projection pursuit algorithm for exploratory data analysis", "author": ["J.H. Friedman", "J.W. Tukey"], "venue": "IEEE Trans. on Computers", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1974}, {"title": "On the quadratic assignment problem", "author": ["A.M. Frieze", "J. Yadegar"], "venue": "Discrete Applied Mathematics", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "On the potential inadequacy of mutual information for feature selection", "author": ["B. Fr\u00e9nay", "G. Doquire", "M. Verleysen"], "venue": "Proceedings of ESANN", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "SpeeDP: an algorithm to compute SDP bounds for very large maxcut instances", "author": ["L. Grippo", "L. Palagi", "M. Piacentini", "V. Piccialli", "G. Rinaldi"], "venue": "Mathematical Programming", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal feature extraction and fusion for audiovisual speech recognition", "author": ["M. Gurban"], "venue": "PhD thesis, 4292, STI, EPF Lausanne", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiple mutual informations and multiple interactions in frequency data", "author": ["T.S. Han"], "venue": "Information and Control", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1980}, {"title": "Probability of error", "author": ["M. Hellman", "J. Raviv"], "venue": "equivocation and the Chernoff bound. IEEE Trans. on Information Theory", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1970}, {"title": "Nonparametric Statistical Methods", "author": ["M. Hollander", "D.A. Wolfe"], "venue": "2nd Edition. Wiley-Interscience", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "What are the differences between Bayesian classifiers and mutual-information classifiers? IEEE Trans", "author": ["B.G. Hu"], "venue": "on Neural Networks and Learning Systems", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Markov networks: Maximum bounded tree-width graphs", "author": ["D. Karger", "N. Srebro"], "venue": "Proceedings of the 12th Annual Symposium on Discrete Algorithms, pages 392\u2013401", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Extraction of configurational entropy from molecular simulations via an expansion approximation", "author": ["B.J. Killian", "J.Y. Kravitz", "M.K. Gilson"], "venue": "Journal of Chemical Physics", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Wrappers for performance enhancement and oblivious decision graphs", "author": ["R. Kohavi"], "venue": "PhD thesis, Stanford, CA, USA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Input feature selection for classification problems", "author": ["N. Kwak", "C. Choi"], "venue": "IEEE Trans. on Neural Networks", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Yalmip: a toolbox for modeling and optimization in matlab", "author": ["J. Lofberg"], "venue": "Proceedings of Computer Aided Control Systems Design", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Multivariate information transmission", "author": ["W. McGill"], "venue": "IRE Professional Group on Information Theory", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1954}, {"title": "On the Use of Variable Complementarity for Feature Selection in Cancer Classification", "author": ["P. Meyer", "G. Bontempi"], "venue": "Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex approximation of the NP-hard search problem in feature subset selection", "author": ["T. Naghibi", "S. Hoffmann", "B. Pfister"], "venue": "Proceedings of ICASSP", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A branch and bound algorithm for feature subset selection", "author": ["P.M. Narendra", "K. Fukunaga"], "venue": "IEEE Trans. on Computers", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1977}, {"title": "SVM-based feature selection by direct objective minimisation", "author": ["J. Neumann", "C. Schn\u00f6rr", "G. Steidl"], "venue": "Proceedings of DAGM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Feature selection based on mutual information: criteria of max-dependency", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "max-relevance, and min-redundancy. IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "and H", "author": ["S. Poljak", "F. Rendl"], "venue": "Wolkowicz. A recipe for semidefinite relaxation for (0,1)-quadratic programming. Journal of Global Optimization", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1995}, {"title": "Probabilistic construction of deterministic algorithms: approximating packing integer programs", "author": ["P. Raghavan"], "venue": "Journal of Computer and System. Sciences", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1988}, {"title": "An Introduction to Information Theory", "author": ["F.M. Reza"], "venue": "Dover Publications, Inc., New York", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1961}, {"title": "Ch", "author": ["I. Rodriguez-Lujan", "R. Huerta"], "venue": "Elkan, and C. S. Cruz. Quadratic programming feature selection. Journal of Machine Learning Research", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding dense subgraphs with semidefinite programming", "author": ["A. Srivastav", "K. Wolf"], "venue": "Approximation Algorithms for Combinatiorial Optimization. Springer", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Robust feature selection algorithms", "author": ["H. Vafaie", "K. De Jong"], "venue": "Proceedings of TAI", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1993}, {"title": "A new outlook on Shannon\u2019s information measures", "author": ["R.W. Yeung"], "venue": "IEEE Trans. on Information Theory", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1991}, {"title": "A Newton-CG augmented Lagrangian method for semidefinite programming", "author": ["X. Zhao", "D. Sun", "K. Toh"], "venue": "SIAM J. on Optimization", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Wrapper methods [27] use the performance of an induction algorithm (for instance a classifier) as the measure function.", "startOffset": 16, "endOffset": 20}, {"referenceID": 32, "context": "The second group of feature selection methods are called embedded methods [34] and are based on some internal parameters of the ML algorithm.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "The third group of the search algorithms are based on targeted projection pursuit which is a linear mapping algorithm to pursue an optimum projection of data onto a low dimensional manifold that scores highly with respect to a measure function [15].", "startOffset": 244, "endOffset": 248}, {"referenceID": 37, "context": "Recently, two convex quadratic programing based methods, QPFS in [39] and SOSS in [32] have been suggested to address the search problem.", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "Recently, two convex quadratic programing based methods, QPFS in [39] and SOSS in [32] have been suggested to address the search problem.", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "We reformulate the feature selection problem as a (0-1)-quadratic integer programming and will show that it can be relaxed to an SDP problem, which is convex and hence can be solved with efficient algorithms [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 10, "context": "The mutual information function is defined as a distance from independence between X and C measured by the Kullback-Leibler divergence [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 36, "context": "Mutual information can also be considered a measure of set intersection [38].", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": "satisfies all three properties of a formal measure over sets [42] [6], i.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "satisfies all three properties of a formal measure over sets [42] [6], i.", "startOffset": 66, "endOffset": 69}, {"referenceID": 12, "context": "More specifically, Fano\u2019s weak lower bound [13] on Pe,", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "1 + Pelog2(ny\u22121) \u2265 H(C)\u2212 I(X;C) (3) where ny is the number of classes and the Hellman-Raviv [22] upper bound,", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "A more detailed discussion can be found in [17].", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "Because to calculate mutual information, estimating the high-dimensional joint probability Pr(X, C) is inevitable which is, in turn, known to be an NP hard problem [25].", "startOffset": 164, "endOffset": 168}, {"referenceID": 4, "context": "As mutual information is hard to evaluate, several alternatives have been suggested [5], [35], [28].", "startOffset": 84, "endOffset": 87}, {"referenceID": 33, "context": "As mutual information is hard to evaluate, several alternatives have been suggested [5], [35], [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 26, "context": "As mutual information is hard to evaluate, several alternatives have been suggested [5], [35], [28].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "The first expansion of mutual information that is used here, relies on the natural extension of mutual information to more than two random variables proposed by McGill [30] and Abramson [1].", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "The first expansion of mutual information that is used here, relies on the natural extension of mutual information to more than two random variables proposed by McGill [30] and Abramson [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 19, "context": "Unlike 2-way mutual information, the generalized mutual information is not necessarily nonnegative and hence, can be interpreted as a signed measure of set intersection [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "This means, the information contained in the interactions of the variables is greater than the sum of the information of the individual variables [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "The second expansion for mutual information is based on the chain rule of information [11]:", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "Several known criteria including Joint Mutual Information (JMI) [31], minimal Redundancy Maximal Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can immediately be derived from D1 and D2.", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "Several known criteria including Joint Mutual Information (JMI) [31], minimal Redundancy Maximal Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can immediately be derived from D1 and D2.", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "Several known criteria including Joint Mutual Information (JMI) [31], minimal Redundancy Maximal Relevance (mRMR) [35] and Mutual Information Feature Selection (MIFS) [5] can immediately be derived from D1 and D2.", "startOffset": 167, "endOffset": 170}, {"referenceID": 33, "context": "in [35].", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "That is, mRMR is a truncated approximation of mutual information and not a heuristic approximation as suggested in [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "The question regarding the implicit consistent i-map that MIFS adopts has been investigated in [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 24, "context": "It is shown in [26] that truncation of the joint entropy H(X) at the rth-order is equivalent to approximating the full-dimensional pdf Pr(X) using joint pdfs with dimensionality of r or smaller.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "The 2nd order Kirkwood approximation of Pr(X), can be denoted as follows [26]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "The first assumption has already appeared in previous works [9] [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "The first assumption has already appeared in previous works [9] [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 31, "context": "Therefore, as explained in [33] the branch and bound based search strategies can not be applied to them.", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "A very similar approach has been applied [8] (by using D1 approximation) to derive several known criteria like MIFS [5] and mRMR [35].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "A very similar approach has been applied [8] (by using D1 approximation) to derive several known criteria like MIFS [5] and mRMR [35].", "startOffset": 116, "endOffset": 119}, {"referenceID": 33, "context": "A very similar approach has been applied [8] (by using D1 approximation) to derive several known criteria like MIFS [5] and mRMR [35].", "startOffset": 129, "endOffset": 133}, {"referenceID": 7, "context": "However, in [8] and most of other previous works, the set score function in (20) is immediately reduced to an individual-feature score function by fixing N\u22121 features in the feature set.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "In the following, we investigate a convex approximation of the binary objective function appearing in feature selection inspired by the Goemans-Williamson maximum cut approximation approach [18].", "startOffset": 190, "endOffset": 194}, {"referenceID": 25, "context": "The wide range of proposed search strategies in the literature can be divided into three categories: 1- Exponential complexity methods including exhaustive search [27], branch and bound based algorithms [33].", "startOffset": 163, "endOffset": 167}, {"referenceID": 31, "context": "The wide range of proposed search strategies in the literature can be divided into three categories: 1- Exponential complexity methods including exhaustive search [27], branch and bound based algorithms [33].", "startOffset": 203, "endOffset": 207}, {"referenceID": 39, "context": "3- Stochastic methods like simulated annealing and genetic algorithms [41], [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "3- Stochastic methods like simulated annealing and genetic algorithms [41], [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "An experimentally comparative evaluation of several variants of these two algorithms has been conducted in [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 16, "context": "The optimization problem in (25) can be seen as an instance of the maximum-cut problem [18] with an additional cardinality constraint, also known as the k-heaviest subgraph or maximum partitioning graph problem.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "The two main approaches to solve this combinatorial problem are either to use the linear programming relaxation by linearizing the product of two binary variables [16], or the semidefinite programming (SDP) relaxation suggested in [18].", "startOffset": 163, "endOffset": 167}, {"referenceID": 16, "context": "The two main approaches to solve this combinatorial problem are either to use the linear programming relaxation by linearizing the product of two binary variables [16], or the semidefinite programming (SDP) relaxation suggested in [18].", "startOffset": 231, "endOffset": 235}, {"referenceID": 6, "context": "The \u3008SDP\u3009problem can be solved within an additive error \u03b3 of the optimum by for example interior point methods [7] whose computational complexity are polynomial in the size of the input and log( 1 \u03b3 ).", "startOffset": 111, "endOffset": 114}, {"referenceID": 35, "context": "The randomized rounding step is a standard procedure to produce a binary solution from the real-valued solution of \u3008SDP\u3009and is widely used for designing and analyzing approximation algorithms [37].", "startOffset": 192, "endOffset": 196}, {"referenceID": 41, "context": "We use the SDP-NAL solver [43] with the Yalmip interface [29] to implement this algorithm in Matlab.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "We use the SDP-NAL solver [43] with the Yalmip interface [29] to implement this algorithm in Matlab.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Even more efficient algorithms for low-rank SDP have been suggested claiming that they can solve problems with the size up to N=30000 in a reasonable amount of time [19].", "startOffset": 165, "endOffset": 169}, {"referenceID": 38, "context": "that the weight of the subgraph induced by S is maximized [40].", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "An SDP based algorithm for k-HSP has been suggested in [40] and its approximation ratio has been analyzed.", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "The approximation ratio of BE for k-HSP has been investigated in [3].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "The approximation ratios of both algorithms for different values of P , as a function of N (total number of features), have been listed in Table 1 (values are calculated from the formulas in [3]).", "startOffset": 191, "endOffset": 194}, {"referenceID": 33, "context": "[35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "These datasets have been widely used in previous feature selection studies [35], [10].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "These datasets have been widely used in previous feature selection studies [35], [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Wilcoxon-Nemenyi post-hoc analysis, as suggested in [23], on the average accuracies (the last column of Table 4).", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "In the next experiment COBRA is compared with two other convex programming based feature selection algorithms, SOSS [32] and QPFS [39].", "startOffset": 116, "endOffset": 120}, {"referenceID": 37, "context": "In the next experiment COBRA is compared with two other convex programming based feature selection algorithms, SOSS [32] and QPFS [39].", "startOffset": 130, "endOffset": 134}], "year": 2014, "abstractText": "Feature subset selection, as a special case of the general subset selection problem, has been the topic of a considerable number of studies due to the growing importance of data-mining applications. In the feature subset selection problem there are two main issues that need to be addressed: (i) Finding an appropriate measure function than can be fairly fast and robustly computed for high-dimensional data. (ii) A search strategy to optimize the measure over the subset space in a reasonable amount of time. In this article mutual information between features and class labels is considered to be the measure function. Two series expansions for mutual information are proposed, and it is shown that most heuristic criteria suggested in the literature are truncated approximations of these expansions. It is well-known that searching the whole subset space is an NP-hard problem. Here, instead of the conventional sequential search algorithms, we suggest a parallel search strategy based on semidefinite programming (SDP) that can search through the subset space in polynomial time. By exploiting the similarities between the proposed algorithm and an instance of the maximumcut problem in graph theory, the approximation ratio of this algorithm is derived and is compared with the approximation ratio of the backward elimination method. The experiments show that it can be misleading to judge the quality of a measure solely based on the classification accuracy, without taking the effect of the non-optimum search strategy into account.", "creator": "LaTeX with hyperref package"}}}