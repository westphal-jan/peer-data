{"id": "1409.7612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2014", "title": "Semi-supervised Classification for Natural Language Processing", "abstract": "Semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data. It has several advantages over supervised classification in natural language processing domain. For instance, supervised classification exploits only labeled data that are expensive, often difficult to get, inadequate in quantity, and require human experts for annotation. On the other hand, unlabeled data are inexpensive and abundant. Despite the fact that many factors limit the wide-spread use of semi-supervised classification, it has become popular since its level of performance is empirically as good as supervised classification. This study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing, biomedical information processing, text classification, and summarization.", "histories": [["v1", "Thu, 25 Sep 2014 15:18:44 GMT  (549kb)", "http://arxiv.org/abs/1409.7612v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rushdi shams"], "accepted": false, "id": "1409.7612"}, "pdf": {"name": "1409.7612.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Classification for Natural Language Processing", "authors": ["Rushdi Shams"], "emails": ["rshams@csd.uwo.ca."], "sections": [{"heading": null, "text": "rE's rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "II. OVERVIEW OF SEMI-SUPERVISED LEARNING", "text": "Astonishingly, test results show that marginal marked data is sufficient to form models that are well suited for semi-supervised learning [3], and the models generated are then applied to unmarked data to mark them. Confidence in their marking is measured using a confidence threshold predetermined by users. Note that learning algorithms often have their own confidence measures, which generally depend on their operating principles. For example, the class probability values for each instance of data are considered confidence measures for Na\u00efve Bayes models [4]. In the case of unmarked data, when the models reach the specified confidence threshold, the newly marked data is added to the pool of originally marked data. This process continues unless (i) the confidence of the models in the described models does not reach the threshold, or (ii) the models are not labelled confidentially and all remaining data is not labelled confidentially."}, {"heading": "A. Learning Problems", "text": "Semi-supervised learning can be roughly categorised into two groups: (i) transductive learning and (ii) inductive learning. Transductive learning is like a take-away exam. This group of semi-supervised learning attempts to evaluate the goodness of a model assumption on unlabelled data after training a classifier with the available labelled data. Inductive learning, on the other hand, is often considered a class exam - it evaluates the goodness of a model assumption on unseen, unlabelled test data after training a classifier with both labelled and unlabelled data. Figure 1 shows the boundaries between these two types of semi-supervised learning. While the entire cycle in the figure illustrates inductive learning, steps 1-3 describe transductive learning."}, {"heading": "B. Working Principle", "text": "Figure 2 shows how semi-supervised learning works with very few marked but abundantly unlabeled data. Figure 2a shows that based on the position of positive (x = 1) and negative (x = \u2212 1) labeled data, a supervised decision boundary is drawn at x = 0 based on the average of the data points. However, for only these two labeled data and 100 unlabeled data (represented by green dots in Figure 2b), this boundary would have shifted more to the right than the supervised decision boundary. This shift is based on the distribution of unlabeled data points taking into account the position of the positive and negative examples. In this particular case, the semi-supervised classifier assumes that the green dots near the red cross point form a kind of didatable shape, while the green dots near the unlabeled cases are not distinguished in the opaque form."}, {"heading": "C. Types of Algorithms", "text": "There are several semi-monitored algorithms, and most of them can be divided into two groups based on their characteristics: (i) generative algorithms and (ii) discriminatory algorithms. Therefore, the models generated by these two types of algorithms are called generative and discriminatory models. However, the following examples can explain the key difference between the two types of models: First, the learner learns each language and then tries to classify the speeches according to their learning. Second, the learner learns the differences between the speeches according to different attributes or traits that are present in them, and then he tries to classify the speeches according to their learning, and then we try to classify the speeches according to their learning."}, {"heading": "D. Types of Learning", "text": "The half-hearted learning processes can be divided into three areas: (i) self-training, (ii) co-training and (iii) active learning. (i) self-training, from a set of initially designated data L, a classifier is generated. (i) This classifier is then applied to a set of initially blank data U. (i) Classification of blank data is observed. (i) The reclassified instances are linked to L to produce a set of terms produced by Unew. (i) A second classifier is generated by Lnew, and then the classifiers of unnamed data are applied. (i) This cycle continues until the reclassified instances are associated with L."}, {"heading": "III. SEMI-SUPERVISED CLASSIFICATION FOR NATURAL LANGUAGE PROCESSING", "text": "This section discusses various applications of natural language processing in the field of semi-supervised classification, based mainly on the results of several classical and state-of-the-art literature in the fields of parsing, text classification, text summary and biomedical information mining."}, {"heading": "A. Parsing", "text": "Steedman et al. [5] found that self-training has very little effect on parser improvements. Similar results are reported by Clark et al. [6], who applied self-training to the marking of parts of the language (POS), but the only work that reported a successful performance of self-training to improve parsers are very few [7]. This paper focuses on both the work of McClosky et al., because they do not adapt the parser in use, because the adaptation has some drastic effects on self-training. Instead of using an adaptive parser, the charniak parser used both labeled and unlabeled data from the same source range. Using a re-Ranker next to the parser makes their work different from many contemporary works. The parser uses third-order grammar and five probability distributions that have more than five linguistic attributes attributes attributes attributes attributes attributes."}, {"heading": "B. Text Classification", "text": "They mentioned the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\" for the \"for the\""}, {"heading": "C. Extractive Text Summarization", "text": "Wong et al. [16] conducted a comparative study in which they produced extractive summaries using both monitored and semi-monitored classifiers; the authors used four traditional sets of attributes to train their classifiers: (1) surface (2) relevance (3) event and (4) content attributes; they tried different combinations of attributes and found that classifiers had produced better summaries when the surface, relevance and content attributes were combined; the novelty of their work is that they used monitored SVM as well as their semi-monitored version called probabilistic SVM or PSVM to generate classifiers and compare their performance; as a measure of performance, they considered ROUGE scores and found that the ROUGE I scores of their SVM classifier are 0.396, while the human ROUGE I scores are 0.42 compared to the gold summaries."}, {"heading": "D. Biomedical Information Mining", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "IV. CONCLUSIONS", "text": "This year, it has come to the point that it has never come as far as it has this year."}], "references": [{"title": "Text bundling: Statistics based data-reduction.", "author": ["L. Shih", "J.D. Rennie", "Y.-H. Chang", "D.R. Karger"], "venue": "AAAI Press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Introduction to Semi-Supervised Learning", "author": ["X. Zhu", "A.B. Goldberg", "R. Brachman", "T. Dietterich"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Semi-Supervised Learning, 1st ed", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Estimating continuous distributions in bayesian classifiers", "author": ["G.H. John", "P. Langley"], "venue": "Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, ser. UAI\u201995. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1995, pp. 338\u2013345.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Bootstrapping statistical parsers from small datasets", "author": ["M. Steedman", "M. Osborne", "A. Sarkar", "S. Clark", "R. Hwa", "J. Hockenmaier", "P. Ruhlen", "S. Baker", "J. Crim"], "venue": "Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 1, ser. EACL \u201903. Stroudsburg, PA, USA: Association for Computational Linguistics, 2003, pp. 331\u2013338. [Online]. Available: http://dx.doi.org/10.3115/1067807.1067851", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Bootstrapping pos taggers using unlabelled data", "author": ["S. Clark", "J.R. Curran", "M. Osborne"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4, ser. CONLL \u201903. Stroudsburg, PA, USA: Association for Computational Linguistics, 2003, pp. 49\u201355. [Online]. Available: http://dx.doi.org/10.3115/1119176.1119183", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Effective self-training for parsing", "author": ["D. Mcclosky", "E. Charniak", "M. Johnson"], "venue": "In Proc. N. American ACL (NAACL, 2006, pp. 152\u2013159.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Map adaptation of stochastic grammars", "author": ["M. Bacchiani", "M. Riley", "B. Roark", "R. Sproat"], "venue": "Comput. Speech Lang., vol. 20, no. 1, pp. 41\u201368, Jan. 2006. [Online]. Available: http://dx.doi.org/10.1016/j.csl.2004.12.001", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Computer Sciences, University of Wisconsin-Madison, Tech. Rep. 1530, 2005. [Online]. Available: http://pages.cs.wisc.edu/\u223cjerryzhu/pub/ssl survey.pdf", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Ecml-pkdd discovery challenge 2006 overview", "author": ["S. Bickel"], "venue": "Proceedings of the ECML-PKDD Discovery Challenge Workshop, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised spam filtering: does it work?", "author": ["M. Mojdeh", "G.V. Cormack"], "venue": "Eds. ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Advances in kernel methods", "author": ["T. Joachims"], "venue": "B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, Eds. Cambridge, MA, USA: MIT Press, 1999, ch. Making Large-scale Support Vector Machine Learning Practical, pp. 169\u2013184. [Online]. Available: http://dl.acm.org/citation.cfm?id=299094.299104", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Spam filtering using statistical data compression models", "author": ["A. Bratko", "G.V. Cormack", "D. R", "B. Filipic", "P. Chan", "T.R. Lynam", "T.R. Lynam"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 2673\u20132698, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Harnessing unlabeled examples through iterative application of dynamic markov modeling", "author": ["G.V. Cormack"], "venue": "In Proceedings of the ECML-PKDD Discovery Challenge Workshop, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Trec 2006 spam track overview", "author": ["G. Cormack"], "venue": "Proceedings of TREC 2006, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Extractive summarization using  supervised and semi-supervised learning", "author": ["K.-F. Wong", "M. Wu", "W. Li"], "venue": "Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, ser. COLING \u201908. Stroudsburg, PA, USA: Association for Computational Linguistics, 2008, pp. 985\u2013992. [Online]. Available: http://dl.acm.org/citation.cfm?id=1599081.1599205", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Extracting protein-protein interaction information from biomedical text with svm.", "author": ["T. Mitsumori", "M. Murata", "Y. Fukuda", "K. Doi", "H. Doi"], "venue": "IEICE Transactions, vol. 89-D,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Extracting information on protein-protein interactions from biological literature based on machine learning approaches", "author": ["K. Sugiyama", "K. Hatano", "M. Yoshikawa", "S. Uemura"], "venue": "Genome Informatics Series, pp. 699\u2013700, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised classification for extracting protein interaction sentences using dependency parsing", "author": ["G. Erkan", "A. \u00d6zg\u00fcr", "D. Radev"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007, pp. 228\u2013237.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Biomedical information extraction with predicate-argument structure patterns", "author": ["A. Yakushiji", "Y. Miyao", "Y. Tateisi", "J. Tsujii"], "venue": "Proceedings of the 11th Annual Meeting of the Association for Natural Language Processing, 2005, pp. 60\u201369.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Self-taught learning: Transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ser. ICML \u201907. New York, NY, USA: ACM, 2007, pp. 759\u2013766. [Online]. Available: http://doi.acm.org/10.1145/1273496.1273592", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "These difficulties have serious effects on supervised learning since a good fit of a classifier model requires as much labeled data as possible for its training [1].", "startOffset": 161, "endOffset": 164}, {"referenceID": 1, "context": "On many tasks, they are as good as supervised models and in most cases, they are better than cluster-based, unsupervised models [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "Surprisingly, test results show that marginal labeled data are sufficient to train models with good fit for semi-supervised learning [3].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "2: Supervised and semi-supervised decision boundaries drawn by a random classifier for two labeled and 100 unlabeled data [2].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "For instance, class probability values for each data instance are considered as confidence measures for Na\u0131\u0308ve Bayes models [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "[5] found that self-training has very small effects on parser improvements.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] who applied self-training to part-of-speech (POS) tagging.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The only works that reported successful execution of self-training to improve parsers are very few [7] [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "The only works that reported successful execution of self-training to improve parsers are very few [7] [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Zhu [9], however, asserted that in semi-supervised classification, unlabeled sentences for which the parser accuracy is unusually better than normal should be restricted to be included in the pool of labeled data.", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "[7], however, stated that they did not followed this approach particularly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The results in 2006 ECML/PKDD spam discovery challenge [10] indicated that spam filters based on semi-supervised classification outperformed supervised filters.", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Interestingly, Mojdeh and Cormack [11] found completely different results when they re-designed the challenge with different collections of email datasets.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "The best performing filters in the challenge were all semi-supervised filters and based on support vector machines SVM and TSVM [12], Dynamic Markov Compression (DMC) [13], and Logistic regression with self-training (LR) [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "The best performing filters in the challenge were all semi-supervised filters and based on support vector machines SVM and TSVM [12], Dynamic Markov Compression (DMC) [13], and Logistic regression with self-training (LR) [14].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "The best performing filters in the challenge were all semi-supervised filters and based on support vector machines SVM and TSVM [12], Dynamic Markov Compression (DMC) [13], and Logistic regression with self-training (LR) [14].", "startOffset": 221, "endOffset": 225}, {"referenceID": 14, "context": "On the other hand, in 2007 TREC Spam Track Challenge [15], the participating spam filters were trained with publicly available emails and their model accuracy was tested on emails collected from user inboxes (i.", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "In an attempt to see whether semi-supervised filters perform as good as it was reported in [10], Mojdeh and Cormack [11] reproduced the work by replacing the datasets of ECML/PKDD challenge with TREC challenge datasets.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "In an attempt to see whether semi-supervised filters perform as good as it was reported in [10], Mojdeh and Cormack [11] reproduced the work by replacing the datasets of ECML/PKDD challenge with TREC challenge datasets.", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "[16] have conducted a comparative study where they produced extractive summaries by using both supervised and semi-supervised classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "A number of supervised tools are developed to classify candidate sentences from biomedical articles (see for example [17], [18], and [19]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "A number of supervised tools are developed to classify candidate sentences from biomedical articles (see for example [17], [18], and [19]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Comparisons showed that the F-score with TSVM was significantly better than those reported by two contemporary work [18] [21].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "Comparisons showed that the F-score with TSVM was significantly better than those reported by two contemporary work [18] [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "Light gray dots mean older papers while dark gray dots mean newer papers [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 20, "context": "If the labeled and unlabeled data are collected from completely different sources and their properties differ, then rather than using semisupervised classification, transfer learning and selftaught classification are encouraged to use [22].", "startOffset": 235, "endOffset": 239}], "year": 2014, "abstractText": "Semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data. It has several advantages over supervised classification in natural language processing domain. For instance, supervised classification exploits only labeled data that are expensive, often difficult to get, inadequate in quantity, and require human experts for annotation. On the other hand, unlabeled data are inexpensive and abundant. Despite the fact that many factors limit the wide-spread use of semi-supervised classification, it has become popular since its level of performance is empirically as good as supervised classification. This study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing, biomedical information processing, text classification, and summarization.", "creator": "LaTeX with hyperref package"}}}