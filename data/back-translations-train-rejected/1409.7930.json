{"id": "1409.7930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2014", "title": "Cognitive Learning of Statistical Primary Patterns via Bayesian Network", "abstract": "In cognitive radio (CR) technology, the trend of sensing is no longer to only detect the presence of active primary users. A large number of applications demand for the relationship of user behaviour in spatial, temporal, and frequency domains. To satisfy such requirements, we study the statistical relationship of primary users by introducing a Bayesian network (BN) based structure learning framework in this paper. How to learn the BN structure is a long standing issue, not fully understood even in the machining learning field. Besides, another key problem in the learning scenario is that the CR has to identify how many variables in the BN, which is usually considered as prior knowledge in machine learning applications. To solve such two issues simultaneously, this paper proposes a BN structure learning scheme consisting of an efficient structure learning algorithm and a blind variable identification algorithm. The proposed scheme bears significantly lower computational complexity compared with previous ones, and is capable of determining the structure without assuming any prior knowledge about variables. With this scheme, cognitive users could understand the statistical pattern of primary networks efficiently, such that more efficient cognitive protocols could be designed across different network layers.", "histories": [["v1", "Sun, 28 Sep 2014 16:36:06 GMT  (842kb)", "https://arxiv.org/abs/1409.7930v1", "draft"], ["v2", "Tue, 30 Sep 2014 17:24:30 GMT  (843kb)", "http://arxiv.org/abs/1409.7930v2", "draft"], ["v3", "Fri, 10 Oct 2014 23:39:38 GMT  (0kb,I)", "http://arxiv.org/abs/1409.7930v3", "This paper has been withdrawn by the author (Weijia Han), since some proofs need to be reviewed"], ["v4", "Fri, 19 Dec 2014 02:57:49 GMT  (846kb)", "http://arxiv.org/abs/1409.7930v4", "This paper has been refreshed with a new version"], ["v5", "Mon, 9 Feb 2015 13:01:07 GMT  (781kb)", "http://arxiv.org/abs/1409.7930v5", "This paper has been refreshed with a new version"]], "COMMENTS": "draft", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weijia han", "huiyan sang", "min sheng", "jiandong li", "shuguang cui"], "accepted": false, "id": "1409.7930"}, "pdf": {"name": "1409.7930.pdf", "metadata": {"source": "CRF", "title": "Cognitive Learning of Statistical Primary Patterns via Bayesian Network", "authors": ["Weijia Han", "Huiyan Sang", "Min Sheng", "Jiandong Li", "Shuguang Cui"], "emails": ["alfret@gmail.com,", "msheng@mail.xidian.edu.cn,", "jdli@pcn.xidian.edu.cn).", "huiyan@stat.tamu.edu).", "cui@ece.tamu.edu)."], "sections": [{"heading": null, "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "II. SYSTEM MODEL", "text": "Our system model consists of a primary cellular network and several secondary sensors over a primary observation area, of which the detailed specifications are given as follows. The primary cellular network consists of several primary base stations and several mobile primary users. The observation area consists of the cells and a road, as shown in Fig. 1, where we look at the mobile primary users who for our study case move only along a one-way street (from right to left). The arrival of users at the entrance of the road follows a Poisson process. The incoming mobile users pass the road at a speed generated by a uniform distribution. As soon as the primary base station of mobile primary users secondary sensor # 1 # 2 # 3Fig. 1. System diagram mobile users move on the road, they are no longer observed. Additionally, the primary network obeys the following setup: 1) The primary base stations are located on a pre-designed network plan (i.e. the network plan)."}, {"heading": "III. COGNITIVE BAYESIAN NETWORK", "text": "In the Czech Republic, as we have previously argued, it is valuable to know the statistical behavior of the primary network. By studying this knowledge, the secondary network could more efficiently and comprehensively utilize the resources of the unused spectrum. In our setup, each secondary sensor was able to obtain a number of observations (or samples) on the on / off status of the observed primary base station, which is the key element of the primary network. Existing work in relation to CR acquisition focused mainly on the busy / unused status of a certain spectrum hole through the observation of a particular primary base station. In contrast, our goal is to learn the statistical pattern of the spatial and temporal behavior of several networked primary base stations by evaluating the obtained observations across the entire network over different time periods. To achieve our goal, learning the BN structure is used as a key methodology. The BN framework is used in artificial intelligence and to evaluate the complex consequences of the network, in order to quantify the effects of artificial intelligence and in different systems."}, {"heading": "A. Bayesian Network Approach", "text": "In the case of BN, the spatial and temporal interactions between primary base stations are expressed by a directed graph G = (V, E), where V is a finite, non-empty set, the elements of which are referred to as nodes denoting the variable fi, t, and E is a set of directed lines linking the pairs of different elements in V. If there is a directed edge from fi1, t1 to fi2, t2, where fi1, t1, fi2, t2, t1 is denoted, this means that fi2, t2 of fi1, i.e. fi2, t2 is affected by fi1, t1. In the BN analysis, such a directional relationship is generally expressed by a conditional probability P (fi2, t2, t1). For the graphical BN, it has a distinguishing feature that: For an arbitrary fi, t, tV, it is conditionally independent of the set of all other indirectly connected nodes, since the set of all directly connected nodes is apparently available when the pattern is complete (E)."}, {"heading": "B. BN Structure Learning in CR", "text": "In our CR scenario, since the observations are collected by the sensors used, it is easy to identify M for the range of i in fi, t. However, due to the randomness of the primary number of users, speed and traffic, the temporal information about T, which defines the range for t of fi, t, BN learning must not only efficiently design the relationship and interaction between the variables, but must also identify the temporal scale of T."}, {"heading": "C. Cognitive Bayesian Network Learning", "text": "Considering the unknown T and the high computational complexity problems, we propose a BN model for our CR sensing case, calling it a cognitive BN (CBN). The CBN has four properties: 1) It is a first-order BN and its nodes are ordered in the temporal domain; 2) its structure is completely interconnected, meaning that there is an edge between each fi, t and fi, t \u2212 1; 3) the observation of each variable is binary; 4) it has an unknown operating period. Here, \"ordered in the temporal domain\" means that for a particular i, fi, fi, t + 1 there is an edge, t \u2212 1. Characteristics 3 and 4 are two direct results from the system model. Thus, we explain only the features 1 and 2 below. In our system model, the user movement and the data service behavior of the primary users at the present time depend on the system that we could determine the system by multiple reduction of time alone."}, {"heading": "IV. EFFICIENT LEARNING IN CBN", "text": "In this section, we propose an efficient learning algorithm that could function correctly under an arbitrary period T, while the problem of an unknown T is examined in the next section. As explained earlier, high computational complexity is a critical problem in learning the BN structure. It was shown [12] that the mutual information check approach leads to the desired efficiency, but cannot handle a dynamic number of variables. In other words, we proposed an efficient learning algorithm that has the same complexity as the I (X; Y; Z) method and can handle the different number of variables. When using conditional mutual information, each possible edge is checked one after the other. In other words, the number of possible edges affects the learning effort. Let's mention that our CBN is a first-order BN and is arranged in the temporal domain. It means that in CBN the direction of the edges represents an edge as a sub-cliff."}, {"heading": "A. Learning a C-clique by Conventional Methods", "text": "In fact, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "B. Efficient Learning Algorithm for Conditional Probability Table", "text": "As already explained, the efficiency of CBN learning based on mutual information is determined by two factors: the method of measuring dependence and the adjustment to the number of variables. Among the two factors, the latter plays a 2For the clarity of expression, empirical probability and true probability are determined by the same notation system.3When implementing this calculation in the code, 2 (3 + 1) for-loops.5 plays a leading role. In this section, we show our efforts to handle the second factor by using feature 2 of our CBN."}, {"heading": "1) Completely Connected Structure and Its Benefit:", "text": "In a completely contiguous BN we must P (fi, t), p (fi, t), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p (t), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p, p), p), p), p), p), p), p), p), p), p), p"}, {"heading": "C. Efficient Dependence Measure", "text": "As discrete random variables X and Y with support Y = subjects Y = subjects Y = subjects Y = subjects Y = subjects Y = subjects Y = subjects Y = subjects Y = subjects Y = subjects Y = subjects Y) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) P (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X (subjects X) (subjects X) (subjects X (subjects X) (subjects X) (subjects X) (subjects X (subjects X) (subjects X) (subjects X) (subjects X) (subjects X (subjects X) (subjects X) (subjects X) (subjects X (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) (subjects X (subjects X) (subjects X) (subjects X) (subjects X) (subjects X) X (subjects X (subjects X) (subjects X) (subjects X) X (subjects X (subjects X) X (subjects X) X (subjects X) X (subjects X - X) (subjects X (subjects X) X (subjects X) X (subjects X (subjects X) X (subjects X) X (subjects X (subjects X - X - X - X - X - - - - - - - - - - - - - - (subjects - - (- - - - - - - - (- - - - - -) - (P) P) (subjects P) (P) (subjects P) (subjects P) (subjects P) (subjects P) (subjects X (X - - (X - - - - - - - - - - (- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"}, {"heading": "D. Efficient C-clique Learning Procedure", "text": "Based on the previously discussed BbCPT and CPbD, the procedure for learning a C clique is summarized as follows: Initialization of C, L, and S; 1) Obtain the conditional probability table according to (22) - (25); 2) Obtain the CPbD of all edges according to (44) - (47).8 As we have already emphasized, C and L only need to be calculated and stored once according to the number of variables in a C clique, which means that they could be primarily calculated and stored. If we consider C and L as foreknowledge, the above procedure shows that our learning algorithm is merely a sequence of closed-shaped function evaluations and has a simpler dependency check function than that in [12]. Consequently, the proposed learning method exhibits lower computational complexity and is able to adapt to various variables. Note that the learned CPPT will be a variable to differ from the next measure of the existence of the clique in the section B12."}, {"heading": "V. LEARNING CBN WITH UNKNOWN T", "text": "The previous section introduces the CBN learning algorithm with a given T. Sometimes the value of T is not known a priori. For example, we do not know the exact value of T in our CR system model. T's uncertainty implies that the number of variables in the temporal domain in the CBN model is unknown. Consequently, after a number of observations, we do not know exactly what time variables the collected observations have generated. In the structural learning literature, observations are normally already correctly associated with various variables. Therefore, T's uncertainty is a difficult problem that is less studied in the BN community. We propose a heuristic but efficient solution to this problem as follows. To estimate T, we first formulate this uncertainty problem into a mathematical problem. Let us leave Dp (x) = a minimum i, k and MDp (fi, t + x; fk, t: k1 / fk, t), which is the sum of the dependencies between the two observation periods respectively."}, {"heading": "A. Learning T \u2217s", "text": "With respect to our system, the temporal correlation between two observations decreases with increasing intervals. Obviously, it is better to set a sufficiently long period of time Ts to ensure the statistically independent state. However, since the number of available observations N is limited in practice, we opt for an empirical estimate of Ts and Tp. From both a statistical and a technical point of view, the empirical probability is valuable and useful as long as it comes close to the true one. Furthermore, it is impossible to find a practicable value of Ts to ensure that Tp (Ts) = 0 using empirical probabilities. Therefore, the empirical probability is valuable and useful as long as it comes close to the true one. Furthermore, it is impossible to find a practicable value of Ts (Ts) that yields Tp (Ts) = 0 using empirical probabilities."}, {"heading": "B. Learning Tp", "text": "As discussed above, the CPbD has the ability to reflect the dependence between the variables = 1,4 = 1,4 = 1,4 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2.0: 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2.0: 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2.0: 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0: 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0 = 2,0"}, {"heading": "C. Overall CBN Learning Scheme", "text": "Based on (48) and the algorithms Ts and Tp, the complete CBN learning scheme is shown in Table IV, which enabled us to efficiently obtain the CBN with measured edges and the associated conditional probability table."}, {"heading": "VI. SIMULATION RESULTS", "text": "In this section, we first compare the computational complexity between the proposed CPbD algorithm and the conventional [12] and then present the learning results."}, {"heading": "A. Comparison of Computational Complexity", "text": "The computational complexity is evaluated by running the Matlab codes of the proposed algorithm and the conventional algorithm on the same desktop, and the corresponding runtime is recorded to reflect the real computational costs. In this subsection, the learning complexity is evaluated when considering the CBN structure, which consists only of a C-clique, since the learning time of the entire CBN structure is linear to that of a C-clique. In addition, the effects of observation numbers and variable numbers are taken into account for a comprehensive comparison. In our simulations, the observations are generated on the basis of binomial M-distributions 5. The simulated results are presented in Fig. 3 and Fig. 4, where the computational costs are the total running time for adhering to the conditional probability table and measuring each edge. It is obvious that the computational costs of the proposed algorithm are much lower than the conventional one, especially as the numbers of the base stations and observations increase."}, {"heading": "B. Learning Outcomes", "text": "RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR"}, {"heading": "VII. CONCLUSION", "text": "RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR"}], "references": [{"title": "Cognitive radio: making software radios more personal,", "author": ["J. Mitola", "G.Q.J. Maguire"], "venue": "Personal Communications, IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Practical issues for spectrum management with cognitive radios,", "author": ["S. Dudley", "W. Headley", "M. Lichtman", "E. Imana", "X. Ma", "M. Abdelbar", "A. Padaki", "A. Ullah", "M. Sohul", "T. Yang", "J. Reed"], "venue": "Proceedings of the IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Signal processing in cognitive radio,", "author": ["J. Ma", "G. Li", "B.-H. Juang"], "venue": "Proceedings of the IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Spatial false alarm in cognitive radio network,", "author": ["W. Han", "J. Li", "Z. Li", "J. Si", "Y. Zhang"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Optimal linear cooperation for spectrum sensing in cognitive radio networks,", "author": ["Z. Quan", "S. Cui", "A. Sayed"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "S", "author": ["K. Murphy"], "venue": "Mian et al., \u201cModelling gene expression data using dynamic bayesian networks,\u201d Technical report, Computer Science Division, University of California, Berkeley, CA, Tech. Rep.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Review: Learning bayesian networks: Approaches and issues,", "author": ["R. Daly", "Q. Shen", "S. Aitken"], "venue": "Knowl. Eng. Rev.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Dynamic bayesian networks: A state of the art,\u201d University of Twente, Centre for Telematics and Information", "author": ["V. Mihajlovic", "M. Petkovic"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating the causal explanatory value of bayesian network structure learning algorithms,", "author": ["P. Shaughnessy", "G. Livingston"], "venue": "Research Paper, Department of Computer Science, University of Massachusetts Lowell.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Large-sample learning of bayesian networks is np-hard,", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Learning bayesian networks from data: An information-theory based approach,", "author": ["J. Cheng", "R. Greiner", "J. Kelly", "D. Bell", "W. Liu"], "venue": "Artif. Intell.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Data networks", "author": ["D.P. Bertsekas", "R.G. Gallager", "P. Humblet"], "venue": "2nd ed. Prentice-Hall, Englewood Cliffs, NJ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1987}, {"title": "Efficient soft decision fusion rule in cooperative spectrum sensing,", "author": ["W. Han", "J. Li", "Z. Li", "J. Si", "Y. Zhang"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Since the terminology was coined in 1999 [1], cognitive radio (CR) has been developed for more than fifteen years, which has drawn attention from both academic and industrial communities since it is intended to enable smart use of the scarce spectrum resource with the initial objective of maximizing spectrum utilization.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "To achieve such new objectives, the statistical knowledge on the primary network status becomes necessary [2] for resource management and system control, which gets us", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "This will go beyond most of the existing CR sensing literature, which usually focus on detecting the presence of primary users only [3]\u2013[5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "This will go beyond most of the existing CR sensing literature, which usually focus on detecting the presence of primary users only [3]\u2013[5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "In this paper, we introduce the Bayesian network (BN) [6] structure learning method to obtain the statistical primary networking pattern, via observing the on/off status of primary base stations.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "The Bayesian model has been well known in the field of artificial intelligence (AI) [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "When considering the probability and uncertainty, BN is a distinct technique for modeling the complex interaction among real world facts [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "However, the associated computational complexity is high since it needs to evaluate the dependence between each pair of variables in a target system and compute the corresponding conditional probability table1 [7], which becomes the major drawback in applying BN structure learning.", "startOffset": 210, "endOffset": 213}, {"referenceID": 9, "context": "One is to use heuristic searching to construct a probable model and then evaluate it by a scoring function [10].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "The score-based approach of learning BNs has been proven as a NP-hard problem [11].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "In [12], the authors show a much more efficient approach to learn the ordered BN by using mutual information to check the dependence of any possible pairs of nodes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "1In statistics, the conditional probability table is defined for a set of discrete random variables to quantify the marginal probability of a single variable with respect to the others [9].", "startOffset": 185, "endOffset": 188}, {"referenceID": 11, "context": "With this, our learning algorithm not only has the same computational overhead as that in [12], but can also dynamically adapt to different numbers of variables.", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "The score-based approach for learning Bayesian networks has been shown NP-hard [11], which is a key challenge in the learning community.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "Recently, a relatively efficient way to learn an ordered BN is given in [12] by using the conditional mutual information to check the dependence between any possible pair of nodes.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "Moreover, in [12], the conditional mutual information checking is performed for every combination of all possible parent nodes of X .", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "In other words, our system model has the first-order Markov property, which has been adopted previously [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "In [13], it is shown that a wireless communication network could be represented by a Markov state transition system.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "dependence [14].", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "It has been shown [12] that the mutual information check based approach leads to the desired efficiency; but it cannot handle a dynamic number of variables.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Accordingly, for a given x, we could use (12) defined below to generate a corresponding vector cx leading to c = cx[1] \u00d7 cx[2] \u00d7\u22ef\u00d7 cx[M], with cx[i] = \u230a x 2i\u22121 \u230b /2 (12)", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "Accordingly, for a given x, we could use (12) defined below to generate a corresponding vector cx leading to c = cx[1] \u00d7 cx[2] \u00d7\u22ef\u00d7 cx[M], with cx[i] = \u230a x 2i\u22121 \u230b /2 (12)", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "For example, when M = 2 and x = 2, c2 = [1 0], which means c = 10.", "startOffset": 40, "endOffset": 45}, {"referenceID": 1, "context": "1) for m = 1 \u2236 1 \u2236 M 2) e1 = 02m\u00d71, e1[2,1] = \u22121, e1[2/2,1] = 1, 3) generate a circulant matrix e based on e 1 , each row is a backward shifting of e 1 , 4) l1 = e[1 \u2236 m, \u2236] , 5) for n = 2 \u2236 1 \u2236 m 6) ln = I2\u00d72 \u2297Ln\u22121, Ln\u22121 = ln\u22121, 7) end 8) end 9) L = [L1,L2,\u22ef,LM ].", "startOffset": 38, "endOffset": 43}, {"referenceID": 0, "context": "1) for m = 1 \u2236 1 \u2236 M 2) e1 = 02m\u00d71, e1[2,1] = \u22121, e1[2/2,1] = 1, 3) generate a circulant matrix e based on e 1 , each row is a backward shifting of e 1 , 4) l1 = e[1 \u2236 m, \u2236] , 5) for n = 2 \u2236 1 \u2236 m 6) ln = I2\u00d72 \u2297Ln\u22121, Ln\u22121 = ln\u22121, 7) end 8) end 9) L = [L1,L2,\u22ef,LM ].", "startOffset": 38, "endOffset": 43}, {"referenceID": 11, "context": "When regarding C and L as prior knowledge, the above procedure shows that our learning algorithm is just a sequence of closed-form function evaluations and has a simpler dependence check function compared with that in [12].", "startOffset": 218, "endOffset": 222}, {"referenceID": 11, "context": "We could also now apply simple binary threshold over CPbD to decide the existence of the edges in the learned clique, as in [12].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "In this section, we first give the comparison of computational complexity between the proposed CPbD algorithm and the conventional one [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "\u201cProp\u201d and \u201cConv\u201d are abbreviations for our proposed scheme and the conventional one proposed in [12], respectively.", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "Note that if we add a binary thresholding to make decisions on the existence of edges, which is used for binary testing of each edge in [12], the learning outcomes will be similar to those in [12].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "Note that if we add a binary thresholding to make decisions on the existence of edges, which is used for binary testing of each edge in [12], the learning outcomes will be similar to those in [12].", "startOffset": 192, "endOffset": 196}], "year": 2015, "abstractText": "In cognitive radio (CR) technology, the trend of sensing is no longer to only detect the presence of active primary users. A large number of applications demand for more comprehensive knowledge on primary user behaviors in spatial, temporal, and frequency domains. To satisfy such requirements, we study the statistical relationship among primary users by introducing a Bayesian network (BN) based framework. How to learn such a BN structure is a long standing issue, not fully understood even in the statistical learning community. Besides, another key problem in this learning scenario is that the CR has to identify how many variables are in the BN, which is usually considered as prior knowledge in statistical learning applications. To solve such two issues simultaneously, this paper proposes a BN structure learning scheme consisting of an efficient structure learning algorithm and a blind variable identification scheme. The proposed approach incurs significantly lower computational complexity compared with previous ones, and is capable of determining the structure without assuming much prior knowledge about variables. With this result, cognitive users could efficiently understand the statistical pattern of primary networks, such that more efficient cognitive protocols could be designed across different network layers.", "creator": "LaTeX with hyperref package"}}}