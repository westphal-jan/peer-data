{"id": "1409.7963", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2014", "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation", "abstract": "In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.", "histories": [["v1", "Sun, 28 Sep 2014 21:32:15 GMT  (7934kb,D)", "http://arxiv.org/abs/1409.7963v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["arjun jain", "jonathan tompson", "yann lecun", "christoph bregler"], "accepted": false, "id": "1409.7963"}, "pdf": {"name": "1409.7963.pdf", "metadata": {"source": "CRF", "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation", "authors": ["Arjun Jain", "Jonathan Tompson", "Yann LeCun", "Christoph Bregler"], "emails": ["bregler}@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Posture detection in video is a long-standing problem in computer vision with a wide range of applications. However, posture detection remains a challenging problem due to the high dimensionality of the input data and the high variability of possible body positions. Traditionally, computer vision-based approaches have relied on manifestations such as texture fields, edges, color histograms, foreground silhouettes, or handmade local features (such as histograms of gradients (HoG) [2]) rather than motion-based features. Alternatively, psychophysical experiments [3] have shown that movement is a powerful visual feature that can be used alone to extract high-level information, including articulated positions. Previous work [4, 5] has reported that the use of motion features to support inference has had little or no effect on performance. Simply adding high-order connectivity to conventional models would lead to unsolvable conclusions."}, {"heading": "2 Prior Work", "text": "Most of the models used in these systems are explicitly 2D or 3D-based geometric models. Most systems had to be initialized by hand (except [12]), and they focused on gradually updating parameters from one frame to the next. More complex examples come from the HumanEva competitions [17], which use video or higher resolution models, such as SCAPE [18] and extensions. We refer the reader to a complete overview of this era. Most of these techniques have been shown to produce very high-resolution animations of detailed bodies and deformations."}, {"heading": "3 Body-Part Detection Model", "text": "We propose an architecture of the Convolutional Network (ConvNet) to estimate the 2D position of human joints in the video (Section 3.2). The input into the network consists of an RGB image and a series of motion characteristics. We examine a variety of formulations for motion characteristics (Section 3.1). Finally, we will present a simple spatial model to solve a specific partial problem associated with the evaluation of our model on the FLIC motion dataset (Section 3.3)."}, {"heading": "3.1 Motion Features", "text": "The aim of this section is to integrate features that are representative of the true motion field (the perspective projection of the 3D velocity field of moving surfaces) as input into our recognition network, so that it can use motion as a keyword for locating body parts. To this end, we evaluate and analyze four motion characteristics that fall under two broad categories: those that use simple derivatives of the RGB video frame and those that use optical flow functions. For each RGB image pair fi and fi + \u03b4, we propose the following characteristics: - RGB image pair - {fi, fi + \u03b4} - RGB image and an RGB differential image - {fi, fi + \u03b4 \u2212 fi} - optical Flow2 vectors - {fi, FLOW (fi + precision authors) {fi, which we are the optical flow magnitude - {fi, fi + \u03b4} - we are."}, {"heading": "3.2 Convolutional Network", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "3.3 Simple Spatial Model", "text": "Our model is evaluated on our new FLIC motion dataset (Section 3.1). As shown on the original FLIC dataset, the test images in FLIC motion may contain multiple persons, but only one actor per frame is labeled in the test set. As such, a rough torso position of the described person is provided at test time to locate the \"right\" person. We incorporate this information using a simple and efficient spatial model. Including this level has two key advantages. First, the correct functional activation of the part detector is selected for the person for whom a floor truth label has been recorded, an example of which is shown in Figure 5. Second, the common positions of each part are positioned near the single floor truth torso that connectivity between the joints is then (indirectly) limited, while the enforcement of the poses is anatomical."}, {"heading": "4 Results", "text": "The training time for our model on the FLIC motion dataset (3957 images of the training set, 1016 images of the test set) is about 12 hours, and the FPROP of a single image takes about 50ms4. For our models that use the optical flow as a motion characteristic, the most expensive part of our pipeline is the calculation of the optical flow, which takes about 1.89 seconds per image pair. (We plan to examine future real-time flow estimates).4 The analysis of our system was done on a 12-core workstation using an NVIDIA Titan GPUSection 4.1 compares the performance of the motion characteristics in Section 3.1. Section 4.2 compares our architecture with other techniques and shows that our system significantly exceeds existing modern techniques. Note that for all experiments in Section 4.1, we use a smaller model with 16 convolutionary characteristics in the first 3 layers. A model with 128 instead of 16 characteristics for the first 3 evolutionary layers is used for the results in Section 4.2."}, {"heading": "4.1 Comparison and Analysis of Proposed Motion Features", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "4.2 Comparison with Other Techniques", "text": "Figure 9 (a) and 9 (b) compares the performance of our system with other state-of-the-art models on the FLIC dataset for the elbow and wrist joints respectively. Our detector is able to significantly exceed all previous techniques on this sophisticated dataset. Note that the use of only motion characteristics is already better [6, 7, 8]. Note also that the use of only motion characteristics is less accurate than the use of a combination of motion characteristics and RGB images, especially in the high-precision range. This is because fine details such as eyes and noses are missing in motion characteristics. Toshev et al. [49] suffers from imprecision in the high-precision region, which we attribute to an inefficient direct regression of pose vectors from images. MODEC [1], Eichner et al. [6] and Sapp et al. [8] rely on hand-made HoG characteristics, which we attribute to an inefficient direct regression of pose vectors from images."}, {"heading": "5 Conclusion", "text": "We have shown that in integrating both RGB and motion characteristics into our deep ConvNet architecture, our network is capable of surpassing existing state-of-the-art techniques for recognizing body positions in video. We have also shown that the use of motion characteristics alone can surpass some traditional algorithms [6, 7, 8]. Our results suggest that even very simple time cues can significantly improve performance with a very small increase in model complexity. Therefore, we suggest that future work should place more emphasis on the correct use of motion characteristics. We also want to further explore higher time characteristics, potentially through learned spatio-temporal convolution states, and we hope that the use of a more expressive time-spatial model (using movement constraints) will help to significantly improve performance."}, {"heading": "6 Acknowledgments", "text": "The authors thank Tyler Zhu for his help in creating datasets. This research was partially funded by the Office of Naval Research ONR Award N000141210327."}], "references": [{"title": "Modec: Multimodal decomposable models for human pose estimation", "author": ["B. Sapp", "B. Taskar"], "venue": "CVPR.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Visual perception of biological motion and a model for its analysis", "author": ["G. Johansson"], "venue": "Perception and Psychophysics", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1973}, {"title": "Progressive search space reduction for human pose estimation", "author": ["V. Ferrari", "M. Marin-Jimenez", "A. Zisserman"], "venue": "CVPR.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Sidestepping intractable inference with structured ensemble cascades", "author": ["D. Weiss", "B. Sapp", "B. Taskar"], "venue": "NIPS.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Better appearance models for pictorial structures", "author": ["M. Eichner", "V. Ferrari"], "venue": "BMVC.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Articulated pose estimation with flexible mixtures-ofparts", "author": ["Y. Yang", "D. Ramanan"], "venue": "CVPR.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Cascaded models for articulated pose estimation", "author": ["B. Sapp", "A. Toshev", "B. Taskar"], "venue": "ECCV.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Model-based vision: a program to see a walking person", "author": ["D. Hogg"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Model-based tracking of self-occluding articulated objects", "author": ["J.M. Rehg", "T. Kanade"], "venue": "Computer Vision.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Model-based estimation of 3d human motion with occlusion based on active multi-viewpoint selection", "author": ["I.A. Kakadiaris", "D. Metaxas"], "venue": "CVPR.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Tracking people with twists and exponential maps", "author": ["C. Bregler", "J. Malik"], "venue": "CVPR.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Articulated body motion capture by annealed particle filtering", "author": ["J. Deutscher", "A. Blake", "I. Reid"], "venue": "CVPR.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Stochastic tracking of 3d human figures using 2d image motion", "author": ["H. Sidenbladh", "M.J. Black", "D.J. Fleet"], "venue": "ECCV.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Covariance scaled sampling for monocular 3d body tracking", "author": ["C. Sminchisescu", "B. Triggs"], "venue": "CVPR.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion", "author": ["L. Sigal", "A. Balan", "B.M. J"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Scape: shape completion and animation of people", "author": ["D. Anguelov", "P. Srinivasan", "D. Koller", "S. Thrun", "J. Rodgers", "J. Davis"], "venue": "TOG.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Vision-based human motion analysis: An overview", "author": ["R. Poppe"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Performance capture from sparse multi-view video", "author": ["E. De Aguiar", "C. Stoll", "C. Theobalt", "N. Ahmed", "H.P. Seidel", "S. Thrun"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Moviereshape: Tracking and reshaping of humans in videos", "author": ["A. Jain", "T. Thorm\u00e4hlen", "H.P. Seidel", "C. Theobalt"], "venue": "TOG.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast articulated motion tracking using a sums of gaussians body model", "author": ["C. Stoll", "N. Hasler", "J. Gall", "H. Seidel", "C. Theobalt"], "venue": "ICCV.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Orientation histograms for hand gesture recognition", "author": ["W.T. Freeman", "M. Roth"], "venue": "International Workshop on Automatic Face and Gesture Recognition.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "On space-time interest points", "author": ["I. Laptev"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Human detection using oriented histograms of flow and appearance", "author": ["N. Dalal", "B. Triggs", "C. Schmid"], "venue": "ECCV.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimating human body configurations using shape context matching", "author": ["G. Mori", "J. Malik"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Recovering 3D human pose from monocular images", "author": ["A. Agarwal", "B. Triggs", "I. Rhone-Alpes", "F. Montbonnot"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Inferring 3d structure with a statistical image-based shape model", "author": ["K. Grauman", "G. Shakhnarovich", "T. Darrell"], "venue": "ICCV.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Fast pose estimation with parametersensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "ICCV.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Strike a pose: Tracking people by finding stylized poses", "author": ["D. Ramanan", "D. Forsyth", "A. Zisserman"], "venue": "CVPR.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning sign language by watching TV (using weakly aligned subtitles)", "author": ["P. Buehler", "A. Zisserman", "M. Everingham"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "The representation and matching of pictorial structures", "author": ["M.A. Fischler", "R. Elschlager"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1973}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "venue": "CVPR.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Pictorial structures revisited: People detection and articulated pose estimation", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "CVPR.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Human pose estimation using body parts dependent joint regressors", "author": ["M. Dantone", "J. Gall", "C. Leistner", "L.V. Gool."], "venue": "CVPR.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning Effective Human Pose Estimation from Inaccurate Annotation", "author": ["S. Johnson", "M. Everingham"], "venue": "CVPR.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Poselet conditioned pictorial structures", "author": ["L. Pishchulin", "M. Andriluka", "P. Gehler", "B. Schiele"], "venue": "CVPR.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Poselets: Body part detectors trained using 3d human pose annotations", "author": ["L. Bourdev", "J. Malik"], "venue": "ICCV.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Articulated pose estimation using discriminative armlet classifiers", "author": ["G. Gkioxari", "P. Arbelaez", "L. Bourdev", "J. Malik"], "venue": "CVPR.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M. Zeiler", "F.R."], "venue": "arXiv preprint arXiv:1311.2901.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Yaniv Taigman", "M.R. Ming Yang", "L. Wolf"], "venue": "CVPR.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion", "author": ["L. Deng", "O. Abdel-Hamid", "D. Yu"], "venue": "ICASSP.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["P. Sermanet", "K. Kavukcuoglu", "S. Chintala", "Y. LeCun"], "venue": "CVPR.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "ICCV.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "CVPR.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning human pose estimation features with convolutional networks", "author": ["A. Jain", "J. Tompson", "M. Andriluka", "G. Taylor", "C. Bregler"], "venue": "ICLR.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time continuous pose recovery of human hands using convolutional networks", "author": ["J. Tompson", "M. Stein", "Y. LeCun", "K. Perlin"], "venue": "TOG.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustered pose and nonlinear appearance models for human pose estimation", "author": ["S. Johnson", "M. Everingham"], "venue": "BMVC.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast image scanning with deep max-pooling convolutional neural networks", "author": ["A. Giusti", "D.C. Ciresan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "CoRR.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "ICLR.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "ICML.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset [1] with additional motion features.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Traditionally, computer vision-based approaches tend to rely on appearance cues such as texture patches, edges, color histograms, foreground silhouettes or hand-crafted local features (such as histogram of gradients (HoG) [2]) rather than motion-based features.", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "Alternatively, psychophysical experiments [3] have shown that motion is a powerful visual cue that alone can be used to extract high-level information, including articulated pose.", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "Previous work [4, 5] has reported that using motion features to aid pose inference has had little or no impact on performance.", "startOffset": 14, "endOffset": 20}, {"referenceID": 4, "context": "Previous work [4, 5] has reported that using motion features to aid pose inference has had little or no impact on performance.", "startOffset": 14, "endOffset": 20}, {"referenceID": 5, "context": "Further, we show that by using motion features alone our method outperforms [6, 7, 8] (see Fig 9(a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available.", "startOffset": 76, "endOffset": 85}, {"referenceID": 6, "context": "Further, we show that by using motion features alone our method outperforms [6, 7, 8] (see Fig 9(a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available.", "startOffset": 76, "endOffset": 85}, {"referenceID": 7, "context": "Further, we show that by using motion features alone our method outperforms [6, 7, 8] (see Fig 9(a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available.", "startOffset": 76, "endOffset": 85}, {"referenceID": 0, "context": "\u2013 A new dataset called FLIC-motion, which is the FLIC dataset [1] augmented with \u2018motion-features\u2019 for each of the 5003 images collected from Hollywood movies.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "Geometric Model Based Tracking: One of the earliest works on articulated tracking in video was Hogg [9] in 1983 using edge features and a simple cylinder based body model.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 10, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 11, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 12, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 13, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 14, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 15, "context": "More complex examples come from the HumanEva dataset competitions [17] that use video or higher-resolution shape models such as SCAPE [18] and extensions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "More complex examples come from the HumanEva dataset competitions [17] that use video or higher-resolution shape models such as SCAPE [18] and extensions.", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "We refer the reader to [19] for a complete survey of this era.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "Most recently such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations [20, 21, 22].", "startOffset": 128, "endOffset": 140}, {"referenceID": 19, "context": "Most recently such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations [20, 21, 22].", "startOffset": 128, "endOffset": 140}, {"referenceID": 20, "context": "Most recently such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations [20, 21, 22].", "startOffset": 128, "endOffset": 140}, {"referenceID": 21, "context": "in 1995 [23] using oriented angle histograms to recognize hand configurations.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 207, "endOffset": 211}, {"referenceID": 26, "context": "Different architectures have since been proposed, including \u201cshape-context\u201d edge-based histograms from the human body [28, 29] or just silhouette features [30].", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "Different architectures have since been proposed, including \u201cshape-context\u201d edge-based histograms from the human body [28, 29] or just silhouette features [30].", "startOffset": 118, "endOffset": 126}, {"referenceID": 28, "context": "Different architectures have since been proposed, including \u201cshape-context\u201d edge-based histograms from the human body [28, 29] or just silhouette features [30].", "startOffset": 155, "endOffset": 159}, {"referenceID": 29, "context": "[31] learn a parameter sensitive hash function to perform example-based pose estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Many techniques have been proposed that extract, learn, or reason over entire body features, using a combination of local detectors and structural reasoning (see [32] for coarse tracking and [33] for person-dependent tracking).", "startOffset": 162, "endOffset": 166}, {"referenceID": 31, "context": "Many techniques have been proposed that extract, learn, or reason over entire body features, using a combination of local detectors and structural reasoning (see [32] for coarse tracking and [33] for person-dependent tracking).", "startOffset": 191, "endOffset": 195}, {"referenceID": 32, "context": "Though the idea of using \u201cPictorial Structures\u201d by Fischler and Elschlager [34] has been around since the 1970s, matching them efficiently to images has only been possible since the famous work on \u2018Deformable Part Models\u2019 (DPM) by Felzenszwalb et al.", "startOffset": 75, "endOffset": 79}, {"referenceID": 33, "context": "[35] in 2008.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 5, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 6, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 35, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 36, "context": "Johnson and Everingham [38], who also proposed the \u2018Leeds Sports Database\u2019, employ a cascade of body part detectors to obtain more discriminative templates.", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "Pishchulin [39] proposes a model that augments the DPM unaries with Poselet conditioned [40] priors.", "startOffset": 11, "endOffset": 15}, {"referenceID": 38, "context": "Pishchulin [39] proposes a model that augments the DPM unaries with Poselet conditioned [40] priors.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "Sapp and Taskar [1] propose a model where they cluster images in the posespace and then find the mode which best describes the input image.", "startOffset": 16, "endOffset": 19}, {"referenceID": 38, "context": "Following the Poselets approach [40], the Armlets approach by Gkioxari et al.", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": "[41] incorporates edges, contours, and color histograms in addition to the HoG features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] use an ensemble of random trees to perform per-pixel labeling of body parts in depth images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 42, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 43, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 44, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 45, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 46, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 47, "context": "[49, 50, 51] also apply neural networks for pose recognition, specifically Toshev et al.", "startOffset": 0, "endOffset": 12}, {"referenceID": 48, "context": "[49, 50, 51] also apply neural networks for pose recognition, specifically Toshev et al.", "startOffset": 0, "endOffset": 12}, {"referenceID": 49, "context": "[49, 50, 51] also apply neural networks for pose recognition, specifically Toshev et al.", "startOffset": 0, "endOffset": 12}, {"referenceID": 47, "context": "[49] show better than state-of-the-art performance on the \u2018FLIC\u2019 and \u2018LSP\u2019 [52] datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[49] show better than state-of-the-art performance on the \u2018FLIC\u2019 and \u2018LSP\u2019 [52] datasets.", "startOffset": 75, "endOffset": 79}, {"referenceID": 46, "context": "[48] to compute optical-flow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] could not provide us with the exact version of the movie that was used for creating the original dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 47, "context": "Recent work [49, 50] has shown ConvNet architectures are well suited for the task of human body pose detection, and due to the availability of modern Graphics Processing Units (GPUs), we can perform Forward Propagation (FPROP) of deep ConvNet architectures at interactive frame-rates.", "startOffset": 12, "endOffset": 20}, {"referenceID": 48, "context": "Recent work [49, 50] has shown ConvNet architectures are well suited for the task of human body pose detection, and due to the availability of modern Graphics Processing Units (GPUs), we can perform Forward Propagation (FPROP) of deep ConvNet architectures at interactive frame-rates.", "startOffset": 12, "endOffset": 20}, {"referenceID": 51, "context": "The input patches are first normalized using Local Contrast Normalization (LCN [53]) for the RGB channels and a new normalization method for the motion features we call Local Motion Normalization (LMN).", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "Recent work [54, 55] eliminates this redundancy and thus yields a dramatic speed up.", "startOffset": 12, "endOffset": 20}, {"referenceID": 53, "context": "Recent work [54, 55] eliminates this redundancy and thus yields a dramatic speed up.", "startOffset": 12, "endOffset": 20}, {"referenceID": 49, "context": "[51]) would replace the last 3 convolutional layers with a fully-connected neural network whose input context is the feature activations for the entire input image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "We use Nesterov momentum to reduce training time [56] and we randomly perturb the input images each epoch by randomly flipping and scaling the images to prevent network overtraining and improve generalization performance.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Note that using only motion features already outperforms [6, 7, 8].", "startOffset": 57, "endOffset": 66}, {"referenceID": 6, "context": "Note that using only motion features already outperforms [6, 7, 8].", "startOffset": 57, "endOffset": 66}, {"referenceID": 7, "context": "Note that using only motion features already outperforms [6, 7, 8].", "startOffset": 57, "endOffset": 66}, {"referenceID": 47, "context": "[49] suffers from inaccuracy in the high-precision region, which we attribute to inefficient direct regression of pose vectors from images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "MODEC [1], Eichner et al.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "[6] and Sapp et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] build on hand crafted HoG features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 48, "context": "[50] do not use multi-scale information and evaluate their model in a sliding window fashion, whereas we use the \u2018one-shot\u2019 approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49], Jain et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[50], MODEC [1], Eichner et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[50], MODEC [1], Eichner et al.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "[6], Yang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] and Sapp et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "also shown that using motion features alone can outperform some traditional algorithms [6, 7, 8].", "startOffset": 87, "endOffset": 96}, {"referenceID": 6, "context": "also shown that using motion features alone can outperform some traditional algorithms [6, 7, 8].", "startOffset": 87, "endOffset": 96}, {"referenceID": 7, "context": "also shown that using motion features alone can outperform some traditional algorithms [6, 7, 8].", "startOffset": 87, "endOffset": 96}], "year": 2014, "abstractText": "In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset [1] with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.", "creator": "LaTeX with hyperref package"}}}