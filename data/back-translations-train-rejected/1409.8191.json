{"id": "1409.8191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2014", "title": "A Neural Networks Committee for the Contextual Bandit Problem", "abstract": "This paper presents a new contextual bandit algorithm, NeuralBandit, which does not need hypothesis on stationarity of contexts and rewards. Several neural networks are trained to modelize the value of rewards knowing the context. Two variants, based on multi-experts approach, are proposed to choose online the parameters of multi-layer perceptrons. The proposed algorithms are successfully tested on a large dataset with and without stationarity of rewards.", "histories": [["v1", "Mon, 29 Sep 2014 17:08:21 GMT  (36kb,D)", "http://arxiv.org/abs/1409.8191v1", "21st International Conference on Neural Information Processing"]], "COMMENTS": "21st International Conference on Neural Information Processing", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["robin allesiardo", "raphael feraud", "djallel bouneffouf"], "accepted": false, "id": "1409.8191"}, "pdf": {"name": "1409.8191.pdf", "metadata": {"source": "CRF", "title": "A Neural Networks Committee for the Contextual Bandit Problem", "authors": ["Robin Allesiardo", "Rapha\u00ebl F\u00e9raud", "Djallel Bouneffouf"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "For online decision-making problems, such as online advertising or marketing optimization, a decision algorithm has to select several actions. Each of these options is associated with page information (profile or context) and the reward feedback is limited to the option selected. For example, when a visitor clicks on an ad online, a request with context (website address, cookies, customer profile, etc.) is sent to the server; the server sends an ad that appears on the page. If the visitor clicks on the ad, the server receives a reward. The server has to balance between researching new ads and exploiting known ads. In addition, in an actual application, both rewards and data distributions can change over time. For example, displaying a new ad can change the click probability of all ads, and the content of a website can change over time."}, {"heading": "2 Previous work", "text": "In fact, it is the case that the majority of people who live in the USA, live in the USA, in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "3 Our algorithm: NeuralBandit1", "text": "Definition 1 (Contextual bandit = argmaxht H yht) is the optimal repentance. (yt, 1, yt, K) Definition 1 (Contextual bandit) Definition 1 (Contextual bandit) Definition 2 (Cumulative repentance) is the optimal repentance. (yt, 1, yT, K) Definition 1 (K) Definition 1 (K) Definition 2 (Cumulative repentance) Definition H (K) is a set of hypotheses, ht (H) is a hypothesis of hypotheses, ht (H) is a hypothesis of hypotheses."}, {"heading": "4 Models selection with adversarial bandit", "text": "The performance of neural networks is influenced by several parameters, such as the learning step, the number of hidden layers, their size and the initialization of weights. A multi-layer perceptron, which corresponds to a number of parameters, is called a model. Selection of models is done by batch learning with a validation set. However, with the help of online learning, we propose to train the models in parallel and to use the opposing bandit algorithm EXP3 [5,6] to choose the best model. Choosing an opposing bandit algorithm is justified by the fact that the performance of each model changes overtime due to the learning itself or due to the non-stationary nature of the data.Exp3 Let the model be an exploration parameter, wt = (w1t, wMt) a weight vector in which the neural algorithms neural algorithms on 1 and M initialize the number of models."}, {"heading": "5 Experiments", "text": "It is not the way in which it is about the question of whether it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way"}, {"heading": "6 Conclusion", "text": "We introduced a new contextual bandit algorithm NeuralBandit1. Two variants of the model selection NeuralBandit2 and NeuralBandit3 used a contextual bandit algorithm to find the best parameters of neural networks. We confronted them with stationary and non-stationary data streams. They achieved a smaller cumulative regret than Banditron, LinUCB and CTS. This approach is successful and has the advantage that it can be trivially parallelized. Model differentiation shows a significant advantage over the Forest Covert Type dataset with non-stationary. We also demonstrated empirically that our two model selection algorithms are resilient to concept drift. These experimental results suggest that neural networks are serious candidates for solving the problem of contextual bandits. However, they are freed from linearity at the expense of regret, with the cost function not convex."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics 6(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning 47(2-3)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": "Biometrika 25", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1933}, {"title": "Thompson Sampling: An Asymptotically Optimal Finite Time Analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "Algorithmic Learning Theory, Proc. of the 23rd International Conference (ALT). Volume LNCS 7568., Lyon, France, Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "On-line learning with malicious noise and the closure algorithm", "author": ["P. Auer", "N. Cesa-Bianchi"], "venue": "Ann. Math. Artif. Intell. 23(1-2)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput. 32(1)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R.D. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma"], "venue": "COLT.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Mortal multi-armed bandits", "author": ["D. Chakrabarti", "R. Kumar", "F. Radlinski", "E. Upfal"], "venue": "NIPS.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A stochastic bandit algorithm for scratch games", "author": ["R. Feraud", "T. Urvoy"], "venue": "In Hoi, S.C.H., Buntine, W.L., eds.: ACML. Volume 25 of JMLR Proceedings., JMLR.org", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploration and exploitation of scratch games", "author": ["R. Feraud", "T. Urvoy"], "venue": "Machine Learning 92(2-3)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Online optimization in x-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "In Koller, D., Schuurmans, D., Bengio, Y., Bottou, L., eds.: NIPS, Curran Associates, Inc.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In F\u00fcrnkranz, J., Scheffer, T., Spiliopoulou, M., eds.: ECML. Volume 4212 of Lecture Notes in Computer Science., Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Feature selection as a one-player game", "author": ["R. Gaudel", "M. Sebag"], "venue": "In F\u00fcrnkranz, J., Joachims, T., eds.: ICML, Omnipress", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Pac-bayesian analysis of contextual bandits", "author": ["Y. Seldin", "P. Auer", "F. Laviolette", "J. Shawe-Taylor", "R. Ortner"], "venue": "In Shawe-Taylor, J., Zemel, R.S., Bartlett, P.L., Pereira, F.C.N., Weinberger, K.Q., eds.: NIPS.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient optimal learning for contextual bandits", "author": ["M. Dud\u0131\u0301k", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang"], "venue": "CoRR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In Platt, J.C., Koller, D., Singer, Y., Roweis, S.T., eds.: NIPS, Curran Associates, Inc.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "CoRR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Contextual bandits with linear payoff functions", "author": ["W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "In Gordon, G.J., Dunson, D.B., Dudk, M., eds.: AISTATS. Volume 15 of JMLR Proceedings., JMLR.org", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "CoRR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908, New York, NY, USA, ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Netw. 2(5)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Programming backgammon using self-teaching neural nets", "author": ["G. Tesauro"], "venue": "Artificial Intelligence 134", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "CoRR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On-line learning for very large datasets", "author": ["L. Bottou", "Y. LeCun"], "venue": "Applied Stochastic Models in Business and Industry 21(2)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "1. MIT Press, Cambridge, MA, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 0, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 68, "endOffset": 73}, {"referenceID": 1, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 68, "endOffset": 73}, {"referenceID": 2, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 98, "endOffset": 103}, {"referenceID": 3, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 98, "endOffset": 103}, {"referenceID": 4, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 141, "endOffset": 146}, {"referenceID": 5, "context": "Optimal solutions have been provided using a stochastic formulation [1,2], a Bayesian formulation [3,4], or using an adversarial formulation [5,6].", "startOffset": 141, "endOffset": 146}, {"referenceID": 6, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 143, "endOffset": 148}, {"referenceID": 7, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 143, "endOffset": 148}, {"referenceID": 8, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 189, "endOffset": 195}, {"referenceID": 9, "context": "Variants of the initial problem were introduced due to practical constraints (appearance of a new advertisment after the beginning of learning [7,8], fixed number of contractual page views [9,10]).", "startOffset": 189, "endOffset": 195}, {"referenceID": 10, "context": "A tree structured bandit such as X-armed bandits [11] or a UCT variant [12] can be used to explore and exploit a tree structure of contextual variables to ar X iv :1 40 9.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "A tree structured bandit such as X-armed bandits [11] or a UCT variant [12] can be used to explore and exploit a tree structure of contextual variables to ar X iv :1 40 9.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "find the leaves which provide the highest rewards [13].", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "Seldin et al [14] modelize the contexts by state sets, which are associated with bandit problems.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "Dud\u0131\u0301k et al [15] propose an algorithm of policies elimination.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "The epoch-greedy algorithm [16] alternates exploration then exploitation.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "In LINUCB [17,18] and in Contextual Thompson Sampling (CTS) [19], the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.", "startOffset": 10, "endOffset": 17}, {"referenceID": 17, "context": "In LINUCB [17,18] and in Contextual Thompson Sampling (CTS) [19], the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.", "startOffset": 10, "endOffset": 17}, {"referenceID": 18, "context": "In LINUCB [17,18] and in Contextual Thompson Sampling (CTS) [19], the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Banditron [20] uses a perceptron per action to recognize rewarded contexts.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "The EXP4 algorithm [6] selects the best arm from N experts advices (probability vectors).", "startOffset": 19, "endOffset": 22}, {"referenceID": 19, "context": "Inspired by Banditron [20] it estimates the probabilities of rewards by using neural networks in order to be free of the hypothesis of linear separability of the data.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "Neural networks are universal approximators [21].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "They are used in reinforcement learning [22,23] and can estimate accurately the probabilities of rewards within actual and complex problems.", "startOffset": 40, "endOffset": 47}, {"referenceID": 22, "context": "They are used in reinforcement learning [22,23] and can estimate accurately the probabilities of rewards within actual and complex problems.", "startOffset": 40, "endOffset": 47}, {"referenceID": 23, "context": "In addition, the stochastic gradient achieves good performances in terms of convergence to the point of best generalization [24] and has the advantage of learning online.", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "The backpropagation algorithm [25] allows calculating the gradient of the error for each neuron from the last to the first layer by minimizing a cost function.", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "Using online learning, we propose to train the models in parallel and to use the adversarial bandit algorithm EXP3 [5,6] to choose the best model.", "startOffset": 115, "endOffset": 120}, {"referenceID": 5, "context": "Using online learning, we propose to train the models in parallel and to use the adversarial bandit algorithm EXP3 [5,6] to choose the best model.", "startOffset": 115, "endOffset": 120}, {"referenceID": 0, "context": "Exp3 Let \u03b3model \u2208 [0, 1] be an exploration parameter, wt = (w t , .", "startOffset": 18, "endOffset": 24}], "year": 2014, "abstractText": "This paper presents a new contextual bandit algorithm, NeuralBandit, which does not need hypothesis on stationarity of contexts and rewards. Several neural networks are trained to modelize the value of rewards knowing the context. Two variants, based on multi-experts approach, are proposed to choose online the parameters of multi-layer perceptrons. The proposed algorithms are successfully tested on a large dataset with and without stationarity of rewards.", "creator": "LaTeX with hyperref package"}}}