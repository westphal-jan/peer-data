{"id": "1409.8498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2014", "title": "Non-Myopic Learning in Repeated Stochastic Games", "abstract": "This paper addresses learning in repeated stochastic games (RSGs) played against unknown associates. Learning in RSGs is extremely challenging due to their inherently large strategy spaces. Furthermore, these games typically have multiple (often infinite) equilibria, making attempts to solve them via equilibrium analysis and rationality assumptions wholly insufficient. As such, previous learning algorithms for RSGs either learn very slowly or make extremely limiting assumptions about the game structure or associates' behaviors. In this paper, we propose and evaluate the notion of game abstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces an RSG to a multi-armed bandit problem, which can then be solved using an expert algorithm. Gabe maintains many aspects of the original game, including security and Pareto optimal Nash equilibria. We demonstrate that Gabe substantially outperforms existing algorithms in many scenarios.", "histories": [["v1", "Tue, 30 Sep 2014 11:46:29 GMT  (91kb,D)", "http://arxiv.org/abs/1409.8498v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.LG", "authors": ["jacob w crandall"], "accepted": false, "id": "1409.8498"}, "pdf": {"name": "1409.8498.pdf", "metadata": {"source": "CRF", "title": "Non-Myopic Learning in Repeated Stochastic Games", "authors": ["Jacob W. Crandall"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it will be able to put itself at the top of the group, in the way it has done in the past and in the way it has done in the past."}, {"heading": "2 Notation", "text": "Let A (s) = Ai (s) \u00b7 A-i (s) be the common actions available in s, where Ai (s) and A-i (s) are the action groups of players iar Xiv: 140 9.84 98v1 [cs. GT] 3 0Se p20 14and \u2212 i. Each episode (or round) of an RSG begins in the initial state s-S and ends when a target state sg-G-S is reached. Once a target state is reached, a new episode begins in the state.When a joint action a = (ai, a-i) is played in the state, each player receives the finite rewards ri (s, a) and r-i (s, a)."}, {"heading": "3 Example RSG", "text": "In fact, most of them are able to survive on their own if they are not able to survive on their own."}, {"heading": "4 Game Abstraction By Experts (Gabe)", "text": "Gabe (summarized in Algorithm 1) reduces the strategy space of an RSG to a finite set of strategies or algorithms. Each RSG defines a policy for all states. In this way Gabe transforms an RSG into a multi-armed bandit problem in which each arm is an expert. Gabe, however, has similarities to the work of Elidrisi et al. (2014) in which the RSG is reduced to a normal game by identifying common action sequences (or pathways) for each player. These pathways are then used as actions of a normal game. Identifying these actions is based on effective exploration strategies and clusters and thresholds of algorithms. Gabe instead defines to reduce the RSG to a multi-armed problem."}, {"heading": "5 Experts", "text": "The first two genres, leader and follower strategies, were defined by Littman and Stone (2001). We introduce the third genre, calling for preventive strategies, in this paper. These three genres define appropriate responses to many algorithms that are likely to be dominated by this solution. Leading Strategies A leadership strategy tries to encourage its partner to follow a target solution (Littman and Stone 2005). The leader plays his own part of the target solution as long as his partner plays his role. However, if the partner deviates from this solution, the leader retaliates in subsequent steps to ensure that the partner does not benefit from the deviation. In order to derive an effective set of leadership strategies, we need to efficiently calculate three things. Second, we need to select desirable target solutions that correspond to any leadership strategy we place in the target solution. Third, we need to determine how deviations from the target solution are punished."}, {"heading": "6 Properties of the Abstraction", "text": "This abstraction limits the set of strategies available to a player, but preserves the following three attributes of the original RSG. Property 1 (\u03b5-Pareto optimal NEs) \u03a6 contains leadership strategies aimed at playing \u03b5-Pareto optimal NEs of the repetitive game. The same NEs exist in the original RSG. Property 2 (Security) Since \u03a6 contains the maximum strategy, it maintains the original level of security of the original RSG. Property 3 (Best Answer) \u0445 contains MBRL, which learns (in the original game) the best response to the game of stationary partners with the same state representation. Thus, it preserves important attributes of the original RSG, attributes that correspond to previously defined learning goals in games (Powers and Shoham 2005). In the next section, we show that this simplified strategy space allows an expert algorithm to learn non-myopic strategies in many scenarios, helping it to exceed existing algorithms."}, {"heading": "7 Empirical Performance", "text": "We combined the expert algorithms Exp3 (Auer et al. 1995) and S + + (Crandall 2014) with the sentence \u03a6 to form two new algorithms: Gabe-Exp3 and Gabe-S + +. Exp3 is a commonly used expert algorithm, while S + + is a recently developed algorithm that has demonstrated high empirical performance in repeated normal form games. We evaluated these algorithms against ten players in three different games. Partners We evaluated the strategies of Gabe-Exp3 and Gabe-S + when paired with both non-learning and learning algorithms. The non-learning algorithms were Coop, Bully, FolkEgal, FolkEgal (de Cote and Littman 2008), Maximin, Bouncer and CFRNE. Coop plays the strategy that maximizes the payouts of its partners. Bully is identical to FolkEgal, except that it tries to force the finish solution that maximizes the other players."}, {"heading": "8 Conclusion and Discussion", "text": "Gabe reduces an RSG to a multi-armed bandit problem, which can then be solved with the help of an expert algorithm. Gabe retains many important features of the original game, including security, best response and Pareto optimism. Furthermore, we have shown empirically that Gabe with an effective expert algorithm outperforms existing learning algorithms in a number of general sum RSGs. Gabe differs from earlier methods of game abstraction (Gilpin and Sandholm 2006; Schnizlein, Bowling and Szafron 2009; Ganzfried, Sandholm and Waugh 2012; Sandholm and Singh 2012). While earlier methods sought to reduce the number of states and actions in the game (to make equilibrium calculations feasible), Gabe abstracts the strategies at a high level of the game. This makes it possible to present these non-abstract strategies simultaneously with previous methods."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "This paper addresses learning in repeated stochastic games (RSGs) played against unknown associates. Learning in RSGs is extremely challenging due to their inherently large strategy spaces. Furthermore, these games typically have multiple (often infinite) equilibria, making attempts to solve them via equilibrium analysis and rationality assumptions wholly insufficient. As such, previous learning algorithms for RSGs either learn very slowly or make extremely limiting assumptions about the game structure or associates\u2019 behaviors. In this paper, we propose and evaluate the notion of game abstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces an RSG to a multiarmed bandit problem, which can then be solved using an expert algorithm. Gabe maintains many aspects of the original game, including security and Pareto optimal Nash equilibria. We demonstrate that Gabe substantially outperforms existing algorithms in many scenarios.", "creator": "LaTeX with hyperref package"}}}