{"id": "1409.8558", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2014", "title": "A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis", "abstract": "Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral coefficients as the vocal tract parameterization of the speech signal. Mel Cepstral coefficients were never intended to work in a parametric speech synthesis framework, but as yet, there has been little success in creating a better parameterization that is more suited to synthesis. In this paper, we use deep learning algorithms to investigate a data-driven parameterization technique that is designed for the specific requirements of synthesis. We create an invertible, low-dimensional, noise-robust encoding of the Mel Log Spectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is then unwrapped and used as the initialization for a Multi-Layer Perceptron (MLP). The MLP is fine-tuned by training it to reconstruct the input at the output layer. This MLP is then split down the middle to form encoding and decoding networks. These networks produce a parameterization of the Mel Log Spectrum that is intended to better fulfill the requirements of synthesis. Results are reported for experiments conducted using this resulting parameterization with the ClusterGen speech synthesizer.", "histories": [["v1", "Tue, 30 Sep 2014 14:20:29 GMT  (72kb,D)", "http://arxiv.org/abs/1409.8558v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["prasanna kumar muthukumar", "alan w black"], "accepted": false, "id": "1409.8558"}, "pdf": {"name": "1409.8558.pdf", "metadata": {"source": "CRF", "title": "A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis", "authors": ["Prasanna Kumar Muthukumar", "Alan W Black"], "emails": ["pmuthuku@cs.cmu.edu,", "awb@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "The speech encoder used in modern parametric speech synthesis [1] has been largely unchanged for several years, and the standard encoding technique is usually a variant of Mel-Cepstral analysis [2]. While many different parameterizations of the spectrum have been developed for synthesis [3] [5], few have managed to survive in the long run. The most obvious signs of this are the systems that are subject to the annual Blizzard challenge [7]. Very few statistical parametric systems that have been subject to the challenge since their inception use vocoders that do not use Cepstral coefficients. Even highly successful techniques such as the various flavors of STRAIGHT [8] are rarely used directly by the synthesizer, which are usually converted into Mel Cepstral coefficients before they are used by statistical parameters."}, {"heading": "2. Stacked Denoising Autoencoder", "text": "This is because the standard technique for forming a new-fangled network is capable of correcting this error."}, {"heading": "3. Building Encoding and Decoding networks", "text": "The \"pre-training\" process for our approach is identical to that for speech recognition. We build an SDA on our features by stacking multiple denoising autoencoders that have been built by learning to reconstruct corrupted versions of the input. Once the SDA is trained, we unpack the SDA, as shown in Figure 2. The unpacked SDA acts as the initialization for a multi-layer perceptron (MLP). An N-layer SDA creates an MLP with 2N \u2212 1 layers. Backpropagation is used to fine-tune the MLP so that the output layer can reconstruct the input available to the first layer through the bottleneck in the middle. Once this fine-tuning is complete, this network is split into two parts."}, {"heading": "3.1. Input features", "text": "In previous sections, we have discussed how a deep neural network will construct a low-dimensional noise-stable representation of the speech signal, but what should our deep neural network actually encode? To put it more clearly: What should be the input to our deep neural network that it can learn to reconstruct? Should it be the actual speech signal itself, the size spectrum, the complex spectrum, or any of the other representations that signal processing research has provided us with? Theoretically, input representation should not matter, since it has been proven that multilayered upstream networks are universal approximations [20]. However, this evidence does not limit the size or structure of the network, nor does it provide a training algorithm that achieves the global optimum. Therefore, it makes sense to train the network on a representation that is known to be strongly correlated to speech perception. Human hearing is known to be both amplitude and frequency logarial."}, {"heading": "4. Experiments and Results", "text": "The idea is that people who stand up for people's rights also stand up for the rights of people who stand up for people's rights, but not for the rights of people who stand up for people's rights, the idea is that the rights of people who stand up for people's rights, for people who don't stand up for people's rights, for people who don't stand up for people's rights, for people who don't stand up for people's rights, but for the rights of people who don't stand up for people's rights, but for the rights of people who violate their rights and duties. \""}, {"heading": "4.1. Synthesis tests", "text": "The next set of tests concerned the use of the 50-dimensional encoding of the deep neural network as parameterization for the statistical parametric synthesizer ClusterGene. MCD values for the three voices described above are given in Table 2.The Mel Cepstral Distortion is higher for the deep neural network encoding compared to the standard system. Furthermore, the base system was preferred in informal subjective tests. As we mentioned earlier in this section, we believe that the MCD of DNN systems were influenced by the fact that the deep neural network was not directly optimized for the score, as the standard system did. The lack of a good objective metric that would work with the DNN approach to parameterization exacerbated the problem by making it difficult to make design-related decisions. This reversal prevented us from taking full advantage of the deep neural network. This is probably the reason for the lower subjective quality."}, {"heading": "4.2. Network optimization", "text": "We tested the neural network by varying both its width and depth. Table 3 summarizes the results of these experiments, all of which took place on the SLT voice. The first column describes the structure of the stacked denoising autoencoder. The unrolled multi-layer perceptron corresponding to each SDA would be twice as deep. The network described in the first row of the table is the same network as the one in the SLT line in Table 2. As can be seen in the table, increasing the width of the layers of the neural network at constant depth improves the performance of the network. However, increasing the depth makes a much greater impact on performance. This is consistent with the theory of deep learning [29]. A deep close network also takes much less time to train compared to a flat network."}, {"heading": "5. Discussion", "text": "All of the experiments described in this paper were conducted on one of the following Nvidia GPUs: Tesla M2050, GRID GK104, GTX670, GTX660 and GTX580. Finding the right stable hardware combination that provides the most efficient training platform is also an exploratory task. Although we have tried to build on work in related areas (especially speech recognition and speech coding) to find reasonable topologies for our networks, the generative nature of speech synthesis naturally has other requirements, so we believe that significant improvements are possible within this core technology. Currently, this work focuses on finding a coding for modeling the voice tract of the speaker, as this allows the most direct comparison with MCEP parameterization."}, {"heading": "6. References", "text": "[1] H. Zen, K. Tokuda, and A. Black, \"Statistical Parameter Networks.\" [2] S. Zen, K. Zen, S. Zen, S. Zen, S. Zen, S. Zen, S. Zen, S. Zen, S. Zen, S. Zen, S. Zen, S. S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S.,.,.,., S., S., S., S., S., S., S., S.,., S.,.,., S., S., S., S., S.,.,.,., S., S., S., S.,., S., S.,., S., S., S., \""}], "references": [{"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A. Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u2013 1064, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Melgeneralized cepstral analysis-a unified approach to speech spectral estimation.", "author": ["K. Tokuda", "T. Kobayashi", "T. Masuko", "S. Imai"], "venue": "ICSLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "On the use of a hybrid harmonic/stochastic model for TTS synthesis-by-concatenation", "author": ["T. Dutoit", "B. Gosselin"], "venue": "Speech Communication, vol. 19, no. 2, pp. 119\u2013143, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Applying the harmonic plus noise model in concatenative speech synthesis", "author": ["Y. Stylianou"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 1, pp. 21\u201329, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "MBR-PSOLA text-to-speech synthesis based on an MBE re-synthesis of the segments database", "author": ["T. Dutoit", "H. Leich"], "venue": "Speech Communication, vol. 13, no. 3, pp. 435\u2013440, 1993.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones", "author": ["E. Moulines", "F. Charpentier"], "venue": "Speech communication, vol. 9, no. 5, pp. 453\u2013467, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "TANDEM-STRAIGHT: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, f0, and aperiodicity estimation", "author": ["H. Kawahara", "M. Morise", "T. Takahashi", "R. Nisimura", "T. Irino", "H. Banno"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 3933\u20133936.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Speech parameter generation from HMM using dynamic features", "author": ["K. Tokuda", "T. Kobayashi", "S. Imai"], "venue": "Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, vol. 1. IEEE, 1995, pp. 660\u2013663.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "A speech parameter generation algorithm considering global variance for HMM-based speech synthesis", "author": ["T. Tomoki", "K. Tokuda"], "venue": "IEICE TRANSACTIONS on Information and Systems, vol. 90, no. 5, pp. 816\u2013824, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1\u201340, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 1096\u20131103.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems, vol. 19, p. 153, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Extracting deep bottleneck features using stacked auto-encoders", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3377\u20133381.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder.", "author": ["L. Deng", "M.L. Seltzer", "D. Yu", "A. Acero", "A.-R. Mohamed", "G.E. Hinton"], "venue": "in Interspeech. ISCA,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks, vol. 2, no. 5, pp. 359\u2013366, 1989.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "A re-determination of the equal-loudness relations for pure tones", "author": ["D.W. Robinson", "R.S. Dadson"], "venue": "British Journal of Applied Physics, vol. 7, no. 5, p. 166, 1956.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1956}, {"title": "A scale for the measurement of the psychological magnitude pitch.", "author": ["S. Stevens", "J. Volkmann", "E. Newman"], "venue": "Journal of the Acoustical Society of America,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1937}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), Jun. 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "ClusterGen: a statistical parametric synthesizer using trajectory modeling.", "author": ["A.W. Black"], "venue": "INTERSPEECH,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "The CMU arctic speech databases", "author": ["J. Kominek", "A.W. Black"], "venue": "Fifth ISCA Workshop on Speech Synthesis, 2004.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Mel-cepstral distance measure for objective speech quality assessment", "author": ["R.F. Kubichek"], "venue": "Communications, Computers and Signal Processing, 1993., IEEE Pacific Rim Conference on, vol. 1. IEEE, 1993, pp. 125\u2013128.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1993}, {"title": "Line spectrum representation of linear predictor coefficients of speech signals", "author": ["F. Itakura"], "venue": "The Journal of the Acoustical Society of America, vol. 57, no. S1, pp. S35\u2013S35, 1975.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1975}, {"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": "Algorithmic Learning Theory. Springer, 2011, pp. 18\u201336.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The speech coder used in modern Statistical Parametric Speech Synthesis[1] has remained largely unchanged for a number of years.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "The standard coding technique is usually a variant of Mel Cepstral analysis[2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "While many different parameterizations of the spectrum have been developed for synthesis[3][4][5][6], few have yet managed to survive in the long run.", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "While many different parameterizations of the spectrum have been developed for synthesis[3][4][5][6], few have yet managed to survive in the long run.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "While many different parameterizations of the spectrum have been developed for synthesis[3][4][5][6], few have yet managed to survive in the long run.", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "While many different parameterizations of the spectrum have been developed for synthesis[3][4][5][6], few have yet managed to survive in the long run.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "Even highly successful techniques like the various flavors of STRAIGHT[8] are rarely used by the synthesizer directly.", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "Techniques such as [9] and [10] rectify some of the problems that occur with this representation but the Mel Cepstral representation still leaves plenty of room for improvement.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "Techniques such as [9] and [10] rectify some of the problems that occur with this representation but the Mel Cepstral representation still leaves plenty of room for improvement.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "Neural networks themselves have existed for many years but the training algorithms that had been used were incapable of effectively training networks that had a large number of hidden layers[11].", "startOffset": 190, "endOffset": 194}, {"referenceID": 10, "context": "This is because the standard technique used for training a neural network is the backpropagation algorithm[12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "One strategy that was developed in recent years was to start off by training the neural network one pair of layers at a time and then building the next pair on top of previous ones[13][14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "One strategy that was developed in recent years was to start off by training the neural network one pair of layers at a time and then building the next pair on top of previous ones[13][14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "Our search for a technique to create a purely data-driven parameterization led us to the Stacked Denoising Autoencoder (SDA) which was developed for pretraining deep neural networks[15].", "startOffset": 181, "endOffset": 185}, {"referenceID": 14, "context": "The SDA is trained in a manner more or less identical to the layer-wise pretraining procedure described in [16] and [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The SDA is trained in a manner more or less identical to the layer-wise pretraining procedure described in [16] and [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "An example of this is the Deep Bottleneck Features that are used in Speech Recognition[17][18].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "An example of this is the Deep Bottleneck Features that are used in Speech Recognition[17][18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "This approach is similar to the one proposed for efficient speech coding in [19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Apart from the fact that [19] proposes the use of the code for other applications, it is also different in that it specifically looks for a binary encoding.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "In previous sections, we have discussed how a deep neural network will build a low-dimensional noise-robust representation of the speech signal, but what should our deep neural network actually encode? To put it more explicitly, what should be the input to our deep neural network that it can learn to reconstruct? Should it be the actual speech signal itself, the magnitude spectrum, the complex spectrum, or any of the other representations that signal processing research has provided us? In theory, the input representation should not matter since it has been proven that multilayer feedforward networks are universal approximators[20].", "startOffset": 635, "endOffset": 639}, {"referenceID": 19, "context": "Human hearing is known to be logarithmic both in amplitude[21], and frequency[22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "Human hearing is known to be logarithmic both in amplitude[21], and frequency[22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 21, "context": "We built the SDAs and the MLPs using the Theano[23] python library, and the parametric Speech Synthesizer using ClusterGen[24].", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "We built the SDAs and the MLPs using the Theano[23] python library, and the parametric Speech Synthesizer using ClusterGen[24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "We ran experiments on 3 different voices: the RMS, and SLT voices from the CMU Arctic databases[25] and the Hindi corpus released as part of the 2014 Blizzard challenge[26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 24, "context": "The standard objective metric used in nearly all evaluations of parametric speech synthesis is Mel Cepstral Distortion[27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "This is important because synthesizers like ClusterGen form clusters of the data vectors at the leaves of the trees and represent the cluster by its mean[1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 25, "context": "Therefore, only representations like MCEPs or Line Spectral Pairs[28] have been found to be suitable.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "This is in line with deep learning theory[29].", "startOffset": 41, "endOffset": 45}], "year": 2014, "abstractText": "Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral coefficients as the vocal tract parameterization of the speech signal. Mel Cepstral coefficients were never intended to work in a parametric speech synthesis framework, but as yet, there has been little success in creating a better parameterization that is more suited to synthesis. In this paper, we use deep learning algorithms to investigate a data-driven parameterization technique that is designed for the specific requirements of synthesis. We create an invertible, low-dimensional, noiserobust encoding of the Mel Log Spectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is then unwrapped and used as the initialization for a Multi-Layer Perceptron (MLP). The MLP is fine-tuned by training it to reconstruct the input at the output layer. This MLP is then split down the middle to form encoding and decoding networks. These networks produce a parameterization of the Mel Log Spectrum that is intended to better fulfill the requirements of synthesis. Results are reported for experiments conducted using this resulting parameterization with the ClusterGen speech synthesizer.", "creator": "LaTeX with hyperref package"}}}