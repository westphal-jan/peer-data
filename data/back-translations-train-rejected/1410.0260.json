{"id": "1410.0260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2014", "title": "ASKIT: Approximate Skeletonization Kernel-Independent Treecode in High Dimensions", "abstract": "We present a fast algorithm for kernel summation problems in high-dimensions. These problems appear in computational physics, numerical approximation, non-parametric statistics, and machine learning. In our context, the sums depend on a kernel function that is a pair potential defined on a dataset of points in a high-dimensional Euclidean space. A direct evaluation of the sum scales quadratically with the number of points. Fast kernel summation methods can reduce this cost to linear complexity, but the constants involved do not scale well with the dimensionality of the dataset.", "histories": [["v1", "Wed, 1 Oct 2014 15:41:11 GMT  (105kb,D)", "http://arxiv.org/abs/1410.0260v1", "22 pages, 6 figures"], ["v2", "Fri, 23 Jan 2015 22:38:05 GMT  (112kb,D)", "http://arxiv.org/abs/1410.0260v2", "22 pages, 6 figures"], ["v3", "Fri, 13 Mar 2015 17:31:21 GMT  (112kb,D)", "http://arxiv.org/abs/1410.0260v3", "22 pages, 6 figures"]], "COMMENTS": "22 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["william b march", "bo xiao", "george biros"], "accepted": false, "id": "1410.0260"}, "pdf": {"name": "1410.0260.pdf", "metadata": {"source": "CRF", "title": "ASKIT: APPROXIMATE SKELETONIZATION KERNEL-INDEPENDENT TREECODE IN HIGH DIMENSIONS", "authors": ["WILLIAM B. MARCH", "BO XIAO"], "emails": [], "sections": [{"heading": null, "text": "The main algorithmic components of the rapid-response checksum algorithms are the separation of the checksum checktotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotalitytotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotaltotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotalitytotaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltototaltotaltototaltototaltototaltototaltototaltototaltototaltotaltototaltotaltotaltototaltotaltototal"}, {"heading": "4 64 4 1.00 97 7E-01 1E+00 5 4 <1 4 15% 3 %", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 64 32 1.00 97 6E-02 1E+00 5 7 <1 6 15% 3 %", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 256 128 1.00 88 3E-03 9E-01 7 8 1 7 5 % 1 %", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 64 4 5.00 97 8E-01 1E+00 5 5 <1 4 15% 3 %", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 64 32 5.00 97 1E-04 1E+00 5 7 <1 6 15% 3 %", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 256 128 5.00 88 4E-08 1E+00 7 8 1 7 5 % 1 %", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "19 64 32 1.76 35 7E-02 1E+00 57 564 6 559 83% 23%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "20 64 128 1.76 35 5E-02 1E+00 57 729 7 723 83% 23%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "21 256 128 1.76 25 2E-02 1E+00 63 1011 11 1000 55% 8 %", "text": "In fact, most of us don't know what to do, and they don't know what to do, and they don't know what to do, what to do, \"he told the German Press Agency.\" But I don't think they should do what they should do. \""}], "references": [{"title": "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "COMMUNICATIONS OF THE ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "On the compression of low rank matrices", "author": ["Hongwei Cheng", "Zydrunas Gimbutas", "Per-Gunnar Martinsson", "Vladimir Rokhlin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "A fast adaptive multipole algorithm in three dimensions", "author": ["H. Cheng", "Leslie Greengard", "Vladimir Rokhlin"], "venue": "Journal of Computational Physics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Random projection trees and low dimensional manifolds", "author": ["S. Dasgupta", "Y. Freund"], "venue": "Proceedings of the 40th annual ACM symposium on Theory of computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Fast directional multilevel algorithms for oscillatory kernels", "author": ["B. Engquist", "L. Ying"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "The black-box fast multipole method", "author": ["William Fong", "Eric Darve"], "venue": "Journal of Computational Physics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A generalized fast mulipole method for nonoscillatory kernels", "author": ["Zydrunas Gimbutas", "Vladimir Rokhlin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "N-body problems in statistical learning, Advances in neural information processing", "author": ["A.G. Gray", "A.W. Moore"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "A fast algorithm for particle simulations", "author": ["Leslie Greengard", "Vladimir Rokhlin"], "venue": "Journal of Computational Physics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1987}, {"title": "Fast approximation of the discrete Gauss transform in higher dimensions", "author": ["Michael Griebel", "Daniel Wissel"], "venue": "Journal of Scientific Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A high-performance, portable implementation of the MPI message passing interface standard", "author": ["W. Gropp", "E. Lusk", "N. Doss", "A. Skjellum"], "venue": "Parallel Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Dual-tree fast gauss transforms", "author": ["Dongryeol Lee", "Alexander Gray", "Andrew Moore"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Fast high-dimensional kernel summations using the monte carlo multipole method", "author": ["Dongryeol Lee", "Alexander G Gray"], "venue": "in NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Cur matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Far-field compression for fast kernel summation methods in high dimensions", "author": ["William B. March", "George Biros"], "venue": "arXiv preprint,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "An accelerated kernel-independent fast multipole method in one dimension", "author": ["Per-Gunnar Martinsson", "Vladimir Rokhlin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "in NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Density Estimation for Statistics and Data Analysis", "author": ["Bernard W. Silverman"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1986}, {"title": "Least squares support vector machine classifiers", "author": ["Johan AK Suykens", "Joos Vandewalle"], "venue": "Neural processing letters,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Improved fast gauss transform and efficient kernel density estimation, in Computer Vision, 2003. Proceedings", "author": ["Changjiang Yang", "Ramani Duraiswami", "Nail A Gumerov", "Larry Davis"], "venue": "Ninth IEEE International Conference on,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "A kernel-independent adaptive fast multipole method in two and three dimensions", "author": ["Lexing Ying", "George Biros", "Denis Zorin"], "venue": "Journal of Computational Physics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "1 in O(N logN) work (treecodes) or O(N) work (fast multipole methods) to arbitrary accuracy [5, 12].", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "In a companion paper [21], we review the main methods for constructing far-field approximation and", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "Although there has been extensive work on these methods for classical kernels like the Gaussian, other kernels are also used such as kernels with variable bandwidth that are not shift invariant [27].", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "Our low rank far field scheme uses an approximate interpolative decomposition (ID) (for the exact ID see [19, 15]) which is constructed using nearest-neighbor sampling augmented with randomized uniform sampling.", "startOffset": 105, "endOffset": 113}, {"referenceID": 16, "context": "The basic notions for the far field were introduced in [21].", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "The nearest neighbors are computed using random projection trees [6] with", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Examples of such difficult to compress kernels are the high frequency Helmholtz kernel [7] and in high intrinsic dimensions the Gaussian kernel (for certain bandwidths and point distributions) [21].", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Examples of such difficult to compress kernels are the high frequency Helmholtz kernel [7] and in high intrinsic dimensions the Gaussian kernel (for certain bandwidths and point distributions) [21].", "startOffset": 193, "endOffset": 197}, {"referenceID": 20, "context": "Linear inference methods such as support vector machines [28] and dimension reduction methods such as principal components analysis [23] can be generalized to kernel methods [3], which in turn require fast kernel summation.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "Linear inference methods such as support vector machines [28] and dimension reduction methods such as principal components analysis [23] can be generalized to kernel methods [3], which in turn require fast kernel summation.", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Seminal work for kernel summations in d \u2264 3 includes [11, 5] and [12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 3, "context": "Seminal work for kernel summations in d \u2264 3 includes [11, 5] and [12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 10, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 13, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "In higher dimensions related work includes [13, 10, 16, 24, 29].", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "One of the fastest schemes in high dimensions is the improved fast Gauss transform [29, 24].", "startOffset": 83, "endOffset": 91}, {"referenceID": 22, "context": "Examples include [30, 8, 9].", "startOffset": 17, "endOffset": 27}, {"referenceID": 6, "context": "Examples include [30, 8, 9].", "startOffset": 17, "endOffset": 27}, {"referenceID": 7, "context": "Examples include [30, 8, 9].", "startOffset": 17, "endOffset": 27}, {"referenceID": 16, "context": "However, these methods also scale as O(c) or worse [21].", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "We also mention methods which rely only on kernel evaluations and use spatial decompositions to scale with dintr in which the far-field approximation is computed on the fly with exact error guarantees [10] or approximately using Monte Carlo methods [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 14, "context": "We also mention methods which rely only on kernel evaluations and use spatial decompositions to scale with dintr in which the far-field approximation is computed on the fly with exact error guarantees [10] or approximately using Monte Carlo methods [17].", "startOffset": 249, "endOffset": 253}, {"referenceID": 18, "context": "Another scheme that only requires kernel evaluations (in the frequency domain) is [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "For a more extensive discussion on the far field approximation that we use here and a more detailed review of the related literature, we refer the reader to our work [21].", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": ") Representative works include [1] and [6].", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": ") Representative works include [1] and [6].", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "The Fast Multipole Method [11] extends this idea by also constructing an incoming representation which approximates the potentials due to a group of distant sources at a target point; it results in O(n) complexity.", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "As we mentioned, the far-field approximation is much more complicated, and there is a great variety of options which we discuss in [21].", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": ",[11]).", "startOffset": 1, "endOffset": 5}, {"referenceID": 22, "context": "The middle subfigure illustrates the kernel independent fast multipole method which is a hybrid of algebraic and analytic methods [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "The last figure shows a purely algebraic approach similar to [22].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Following [19, 4], we refer to the construction of this approximation for a matrix as skeletonization.", "startOffset": 10, "endOffset": 17}, {"referenceID": 12, "context": "It can be shown [15] that", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "1(b), we show a method based on placing equivalent sources and finding equivalent densities that can approximate the far field [30].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "One method [22] (Figure 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "Randomized linear algebra algorithms can achieve this by either random projections [15] or the construction of an importance sampling distribution [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "Randomized linear algebra algorithms can achieve this by either random projections [15] or the construction of an importance sampling distribution [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "Nearest neighbors and binary tree decomposition: To find nearest neighbors we use a greedy search using random projection trees [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 16, "context": "In [21], we developed a sampling scheme that is a hybrid between uniform sampling combined with nearest neighbor sampling (for distance decaying kernels).", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "\u00b6The equation of the curve is xi = f(i\u03c0t), t \u2208 [0, 1], i = 1, .", "startOffset": 47, "endOffset": 53}, {"referenceID": 15, "context": "It can be shown that a particular importance sampling distribution (based on what is known as statistical leverage scores) can provide a O(\u03c3s+1) reconstruction error if the sample size ` is proportional to s log s [20].", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "In [21], we show that the combined ID approximation and uniform sampling error in the far-field from a single source node to the rest of the points can be bounded as follows.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "7 in [21].", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "with constant and variable h; we select h based on kernel density estimation theory [27] (Eq.", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "(More discussion and results can be found in [21].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Runs (33-34) are done using 16 and 256 nodes respectively and the MPI library [14].", "startOffset": 78, "endOffset": 82}], "year": 2017, "abstractText": "We present a fast algorithm for kernel summation problems in high-dimensions. These problems appear in computational physics, numerical approximation, non-parametric statistics, and machine learning. In our context, the sums depend on a kernel function that is a pair potential defined on a dataset of points in a high-dimensional Euclidean space. A direct evaluation of the sum scales quadratically with the number of points. Fast kernel summation methods can reduce this cost to linear complexity, but the constants involved do not scale well with the dimensionality of the dataset. The main algorithmic components of fast kernel summation algorithms are the separation of the kernel sum between near and far field (which is the basis for pruning) and the efficient and accurate approximation of the far field. We introduce novel methods for pruning and for approximating the far field. Our far field approximation requires only kernel evaluations and does not use analytic expansions. Pruning is not done using bounding boxes but rather combinatorially using a sparsified nearest-neighbor graph of the input distribution. The time complexity of our algorithm depends linearly on the ambient dimension. The error in the algorithm depends on the low-rank approximability of the far field, which in turn depends on the kernel function and on the intrinsic dimensionality of the distribution of the points. The error of the far field approximation does not depend on the ambient dimension. We present the new algorithm along with experimental results that demonstrate its performance. As a highlight, we report results for Gaussian kernel sums for 100 million points in 64 dimensions, for one million points in 1000 dimensions, and for problems in which the Gaussian kernel has a variable bandwidth. To the best of our knowledge, all of these experiments are impossible or prohibitively expensive with existing fast kernel summation methods.", "creator": "LaTeX with hyperref package"}}}