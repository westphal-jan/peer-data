{"id": "1410.0471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2014", "title": "PinView: Implicit Feedback in Content-Based Image Retrieval", "abstract": "This paper describes PinView, a content-based image retrieval system that exploits implicit relevance feedback collected during a search session. PinView contains several novel methods to infer the intent of the user. From relevance feedback, such as eye movements or pointer clicks, and visual features of images, PinView learns a similarity metric between images which depends on the current interests of the user. It then retrieves images with a specialized online learning algorithm that balances the tradeoff between exploring new images and exploiting the already inferred interests of the user. We have integrated PinView to the content-based image retrieval system PicSOM, which enables applying PinView to real-world image databases. With the new algorithms PinView outperforms the original PicSOM, and in online experiments with real users the combination of implicit and explicit feedback gives the best results.", "histories": [["v1", "Thu, 2 Oct 2014 08:05:19 GMT  (117kb,D)", "http://arxiv.org/abs/1410.0471v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["zakria hussain", "arto klami", "jussi kujala", "alex p leung", "kitsuchart pasupa", "peter auer", "samuel kaski", "jorma laaksonen", "john shawe-taylor"], "accepted": false, "id": "1410.0471"}, "pdf": {"name": "1410.0471.pdf", "metadata": {"source": "CRF", "title": "PinView: Implicit Feedback in Content-Based Image Retrieval", "authors": ["Zakria Hussain", "Arto Klami", "Jussi Kujala", "Alex P. Leung", "Kitsuchart Pasupa", "Peter Auer", "Samuel Kaski", "Jorma Laaksonen", "John Shawe-Taylor"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Content-based Image Retrieval CBIR, ExplorationExploitation, Eye Tracking, Implicit Feedback, Multiple Kernel Learning"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 BACKGROUND AND RELATED WORK", "text": "This year, it has come to the point where you see yourself as being able to live in a country where you are able, where you are able to move, to move and to move, where you are able to move, and where you are able to move, to show that you are able to move, to show that you are able to move, to show that you are able to move, to show that you are able to move."}, {"heading": "3 SYSTEM COMPONENTS", "text": "In this section we describe the main components of the system. It consists of four main components, which are explained in more detail in the following sections. The first component predicts the relevance of the seen images by clicks and image attributes. Tensor decomposition and multiple learning modules for the user then derive a metric between images using known visual characteristics of the images (see Table 1 and [43] for more detailed descriptions of the features used) and relevance feedback on the seen images. The last component, a specialized exploration exploitation algorithm LINREL, suggests new images to the user to be shown to. Figure 1 summarizes the information flow and relationships between the different components. Input by the user, captured by mouse clicks and the Eytracker, is fed into the image relevance predictor, and the predicted relevance values are then assigned to the multiple kernel learning module together with the images extracted from the image attributes."}, {"heading": "3.1 Relevance Prediction from Eye Movements and Clicks", "text": "This year, it will be able to mention the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated csrcsrteeeaeVrln."}, {"heading": "3.2 Multiple Kernel Learning", "text": "Learning similarity measures or metrics of importance to our CBIR task is central to various weight combinations. Some image searches require a combination of image characteristics to quickly distinguish them from other, less relevant images. For example, color and texture characteristics may be important to find images of snowy landscapes, whereas color may be the only important feature needed to find images of blue skies. We would like to use a combination of metrics as a keyword to find relevant images quickly and efficiently, and then apply these learned metrics (kernels) to the LINREL algorithm of Section 3.4.Given the image characteristics vectors of blue skies (I), \u03c6 (J), let the inner product expect us to use the kernel function k (\u00b7, \u00b7) between images I and J, where we have a mapping function [50]. Multiple Kernel Learning (MKL) attempts to find a combination of kernel problems by combining 51 with a resonant or a regresonant."}, {"heading": "3.3 Tensor Decomposition", "text": "Since eye movements are only available for images that have already been presented to the user, eye movement characteristics cannot be used directly to predict the relevance of invisible images. To increase this problem, we refer the known image characteristics to the (as yet) unknown eye movement characteristics by learning a common representation that combines these two views. We learn this relationship by using an implicit representation that creates an implicit correlation space [54]. The tensor representation can be calculated by selecting points between each kernel matrix of each view [55], [56].Hence, let Knightt = [k \u03c6 (Iu, Iv)] 1 \u2264 t \u2212 be the kernel Gram matrix."}, {"heading": "4 EXPERIMENTS", "text": "In this section we will describe experimental evaluations of the PinView system. We will empirically examine the following two questions: (a) How close can we get to explicit feedback performance with less time-consuming implicit feedback? (b) Is it possible to improve performance still further by combining implicit and explicit feedback, especially if the explicit feedback is only partial (a single click on the most relevant image) and viewing patterns are expected to reveal more relevant images? In the experiments, we will use three variants of PinView: 1) PinView system with implicit feedback from viewing patterns. 2) PinView system with explicit feedback from clicks. 3) PinView system with explicit and implicit feedback, both from viewing patterns and clicks. For evaluation purposes, these variants will be compared with the baseline of surfing (i.e., showing randomly ordered images with explicit pictorial implicit feedback) and the SOM-IR-explicit component."}, {"heading": "4.1 Offline Experiments", "text": "The number of images that can be found in this database is the same as the number of associated images that are captured in each category. There are few that are able to find images that are able to identify themselves. The number of images that are contained in this database is the same as the number of associated images that are able to identify themselves. The number of images that are captured in the database is the same as the number of associated images that are retrieved in each category. The number of images that are retrieved in each category is the same as the number of associated images that are retrieved in each category. The number of images that are retrieved in each category is the same as the number of associated images that are divided into each category. The number of associated images that are retrieved in each group is divided into each group."}, {"heading": "4.2 Online Experiments", "text": "This year it is so far that it only takes a few days to get to the point where it is so far that in the first half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the first half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the first half of the second half of the year.ndU"}, {"heading": "5 DISCUSSION AND CONCLUSIONS", "text": "In this paper, we describe our PinView CBIR system, which records implicit relevance signals from the user and derives their image search intentions through the use of several novel machine learning methods. We show that the PinView variants work better than surfing (a series of randomly arranged images), suggesting that PinView would be useful at least in scenarios where tag-based evidence is not available or has already been used to limit the search to a subset of the original collection. Implicit feedback from the glance exceeded the baseline, meaning that pure implicit feedback is a viable option when it is difficult or too tedious to provide explicit feedback. Explicit feedback from clicks gave more precise results, and there was evidence that explicit and implicit feedback produced the best results. Summary, the combination of algorithms in PinView is a very promising approach to image repetition."}, {"heading": "ACKNOWLEDGMENTS", "text": "The research leading to these results was funded by the Seventh Framework Programme of the European Community (FP7 / 2007-2013) under Funding Agreement No. 216529, Personal Information Navigator Adapting Through Viewing, PinView, IST programmes of the European Community under the PASCAL2 Network of Excellence, IST-2007216886, and the Finnish Academy of the Finnish Computational Inference Research Competence Centre (COIN, 251170)."}], "references": [{"title": "Image retrieval: Ideas, influences, and trends of the new age", "author": ["R. Datta", "D. Joshi", "J. Li", "J.Z. Wang"], "venue": "ACM Computing Surveys, vol. 40, no. 2, pp. 1\u201360, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Vision-based image retrieval (vbir): A new eye-tracking based approach to efficient and intuitive image retrieval", "author": ["K. Essig"], "venue": "Ph.D. dissertation, Technischen Fakult\u00e4 der Universit\u00e4 Bielefeld, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Content based image retrieval: An attention monitoring approach", "author": ["H. Grecu"], "venue": "Ph.D. dissertation, Universitatea Politehnica din Bucuresti, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Implicit feedback for inferring user preference: a bibliography", "author": ["D. Kelly", "J. Teevan"], "venue": "SIGIR Forum, vol. 37, no. 2, pp. 18\u2013 28, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Can relevance of images be inferred from eye movements?", "author": ["A. Klami", "C. Saunders", "T.E. de Campos", "S. Kaski"], "venue": "Proceeding of the 1st ACM International Conference on Multimedia Information Retrieval", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Eye tracking: A perceptual interface for content based image retrieval", "author": ["O. Oyekoya"], "venue": "Ph.D. dissertation, University College London, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "It\u2019s in Your Eyes: Gaze Based Image Retrieval in Context", "author": ["L. Scherffig"], "venue": "ZKM Institute for Basic Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Pinview: Implicit feedback in content-based image retrieval", "author": ["P. Auer", "Z. Hussain", "S. Kaski", "A. Klami", "J. Kujala", "J. Laaksonen", "A.P. Leung", "K. Pasupa", "J. Shawe-Taylor"], "venue": "Proc. of Workshop on Applications of Pattern Analysis, vol. 11, 2010, pp. 51\u201357.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploration-exploitation of eye movement enriched multiple feature spaces for content-based image retrieval", "author": ["Z. Hussain", "A.P. Leung", "K. Pasupa", "D.R. Hardoon", "P. Auer", "J. Shawe-Taylor"], "venue": "Machine Learning and Knowledge Discovery in Databases European Conference, ECML PKDD 2010. Berlin Heidelberg, Germany: Springer, 2010, pp. 554\u2013569.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Image retrieval: Current techniques, promising directions, and open issues", "author": ["Y. Rui", "T.S. Huang", "S.-F. Chang"], "venue": "Journal of Visual Communication and Image Representation, vol. 10, no. 1, pp. 39\u201362, 1999.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Content-based image retrieval at the end of the early years", "author": ["A.W.M. Smeulders", "M. Worring", "S. Santini", "A. Gupta", "R. Jain"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 22, no. 12, pp. 1349\u20131380, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Content-based image retrieval systems: A survey", "author": ["R.C. Veltkamp", "M. Tanase"], "venue": "Utrecht University, Information and Computing Sciences, Utrecht, The Netherlands, Tech. Rep. 2000-34 (revised version), October 2002, available at: http://www.aalab.cs.uu.nl/cbirsurvey/.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Content-based multimedia information retrieval: State of the art and challenges", "author": ["M.S. Lew", "N. Sebe", "C. Djeraba", "R. Jain"], "venue": "ACM Trans. on Multimedia Computing, Communications and Applications, vol. 2, no. 1, pp. 1\u201319, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "The state of the art in image and video retrieval", "author": ["N. Sebe", "M.S. Lew", "X. Zhou", "T.S. Huang", "E.M. Bakker"], "venue": "CIVR\u201903: Proc. of the 2nd international conference on Image and video retrieval. Berlin, Heidelberg: Springer-Verlag, 2003, pp. 1\u20138.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling user subjectivity in image libraries", "author": ["R.W. Picard", "T.P. Minka", "M. Szummer"], "venue": "M.I.T Media Laboratory, Tech. Rep. #382, 1996.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Relevance feedback: A power tool in interactive content-based image retrieval", "author": ["Y. Rui", "T.S. Huang", "M. Ortega", "S. Mehrotra"], "venue": "IEEE Trans. on Circuits and Systems for Video Technology, vol. 8, no. 5, pp. 644\u2013655, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Relevance feedback for image retrieval: A comprehensive review", "author": ["X.S. Zhou", "T.S. Huang"], "venue": "Multimedia Systems, vol. 8, no. 6, pp. 536\u2013544, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "An optimized interaction strategy for Bayesian relevance feedback", "author": ["I.J. Cox", "M.L. Miller", "T.P. Minka", "P.N. Yianilos"], "venue": "CVPR98: IEEE Conference on Computer Vision and Pattern Recognition. Los Alamitos, CA, USA: IEEE Computer Society, 1998, pp. 553\u2013558.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Implicit interest indicators", "author": ["M. Claypool", "P. Le", "M. Wased", "D. Brown"], "venue": "IUI\u201901: Proc. of the 6th International Conference on Intelligent User Interfaces. New York, NY, USA: ACM, 2001, pp. 33\u201340.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Evaluating implicit measures to improve web search", "author": ["S. Fox", "K. Karnawat", "M. Mydland", "S. Dumais", "T. White"], "venue": "ACM Trans. on Information Systems, vol. 23, pp. 147\u2013168, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Accurately interpreting clickthrough data as implicit feedback", "author": ["T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "G. Gay"], "venue": "Proc. of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. New York, NY, USA: ACM, 2005, pp. 154\u2013161.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Cortically coupled computer vision for rapid image search", "author": ["A. Gerson", "L. Passa", "P. Sajda"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 14, no. 2, pp. 174\u2013179, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Human-aided computing: utilizing implicit human processing to classify images", "author": ["P. Shenoy", "D.S. Tan"], "venue": "Proceeding of the 26th Annual SIGCHI Conference on Human Factors in Computing Systems. New York, NY, USA: ACM, 2008, pp. 845\u2013854.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Identifying natural images from human brain activity", "author": ["K. Kay", "T. Naselaris", "R. Prenger", "J. Gallant"], "venue": "Nature, vol. 452, no. 7185, pp. 352\u2013355, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to decode cognitive states from brain images", "author": ["T. Mitchell", "R. Hutchinson", "R. Niculescu", "F. Pereira", "X. Wang", "M. Just", "S. Newman"], "venue": "Machine Learning, vol. 57, no. 1, pp. 145\u2013175, 2004.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Brain state decoding for rapid image retrieval", "author": ["J. Wang", "E. Pohlmeyer", "B. Hanna", "Y.-G. Jiang", "P. Sajda", "S.-F. Chang"], "venue": "Proc. of the 17th ACM International Conference on Multimedia. New York, NY, USA: ACM, 2009, pp. 945\u2013954.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Implicit human-centered tagging", "author": ["A. Vinciarelli", "N. Suditu", "M. Pantic"], "venue": "Proc. or IEEE International Conference on Multimedia and Expo, ICME 2009. Piscataway, NJ, USA: IEEE press, 2009, pp. 1428\u20131431.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Using facial expressions and peripheral physiological signals as implicit indicators of topical relevance", "author": ["I. Arapakis", "I. Konstas", "J. Jose"], "venue": "Proc. of the 17th ACM international conference on Multimedia. New York, NY, USA: ACM, 2009, pp. 461\u2013470.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Affective feedback: an investigation into the role of emotions in the information seeking process", "author": ["I. Arapakis", "J. Jose", "P. Gray"], "venue": "SIGIR\u201908: Proc. of the 31st Annual International ACM SIGIR conference on Research and Development in Information Retrieval. New York, NY, USA: ACM, 2008, pp. 395\u2013402.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Affective ranking of movie scenes using physiological signals and content analysis", "author": ["M. Soleymani", "G. Chanel", "J.J. Kierkels", "T. Pun"], "venue": "MS\u201908: Proc. of the 2th ACM Workshop on Multimedia Semantics. New York, NY, USA: ACM, 2008, pp. 32\u201339.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Suitor: an attentive information system", "author": ["P.P. Maglio", "R. Barrett", "C.S. Campbell", "T. Selker"], "venue": "IUI\u201900: Proc. of the 5th Interna-  12 tional Conference on Intelligent User Interfaces. New York, NY, USA: ACM, 2000, pp. 169\u2013176.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast hands-free writing by gaze direction", "author": ["D.J. Ward", "D.J. MacKay"], "venue": "Nature, vol. 418, p. 838, 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Eye movements as implicit relevance feedback", "author": ["G. Buscher", "A. Dengel", "L. van Elst"], "venue": "CHI \u201908 Extended Abstracts on Human Factors in Computing Systems. New York, NY, USA: ACM, 2008, pp. 2991\u20132996.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining eye movements and collaborative filtering for proactive information retrieval", "author": ["K. Puolam\u00e4ki", "J. Saloj\u00e4rvi", "E. Savia", "J. Simola", "S. Kaski"], "venue": "SIGIR \u201905: Proc. of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. New York, NY, USA: ACM press, 2005, pp. 146\u2013153.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Can eyes reveal interest?\u2014Implicit queries from gaze patterns", "author": ["A. Ajanki", "D.R. Hardoon", "S. Kaski", "K. Puolam\u00e4ki", "J. Shawe- Taylor"], "venue": "User Modeling and User-Adapted Interaction: The Journal of Personalization Research, vol. 19, pp. 307\u2013339, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Eye tracking as a new interface for image retrieval", "author": ["O. Oyekoya", "F. Stentiford"], "venue": "BT Technology Journal, vol. 22, no. 3, pp. 161\u2013169, 2004.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Towards gaze-based relevance feedback in image retrieval", "author": ["H. Grecu", "C. Cudalbu", "V. Buzuloiu"], "venue": "Proc. of International Workshop on Bioinspired Information Processing: Cognitive modeling and gaze-based communication (BIP 2005), 2005, poster abstract.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Perceptual image retrieval using eye movements", "author": ["O. Oyekoya", "F. Stentiford"], "venue": "Proc. of the International Workshop on Intelligence Computing in Pattern Analysis/Synthesis 2006, 2006, pp. 281\u2013 289.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "GaZIR: Gaze-based zooming interface for image retrieval", "author": ["L. Kozma", "A. Klami", "S. Kaski"], "venue": "Proc. of ICMI-MLMI 2009, The 11th International Conference on Multimodal Interfaces and The 6th Workshop on Machine Learning for Multimodal Interaction. New York, NY, USA: ACM, 2009, pp. 305\u2013312.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Content based image retrieval using a combination of visual features and eye tracking data", "author": ["Z. Liang", "H. Fu", "Y. Zhang", "Z. Chi", "D. Feng"], "venue": "Proc. of ETRA 2010: ACM Symposium on Eye- Tracking Research & Applications. New York, NY, USA: ACM, 2010, pp. 41\u201344.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Eye movement as an interaction mechanism for relevance feedback in a contentbased image retrieval system", "author": ["Y. Zhang", "H. Fu", "Z. Liang", "Z. Chi", "D. Feng"], "venue": "Proc. of ETRA 2010: ACM Symposium on Eye-Tracking Research & Applications. New York, NY, USA: ACM, 2010, pp. 37\u201340.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual attention for implicit relevance feedback in a content based image retrieval", "author": ["A. Faro", "D. Giordano", "C. Pino", "C. Spampinato"], "venue": "Proc. of ETRA 2010: ACM Symposium on Eye-Tracking Research & Applications. New York, NY, USA: ACM, 2010, pp. 73\u201376.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluating the performance in automatic image annotation: example case by adaptive fusion of global image features", "author": ["V. Viitaniemi", "J. Laaksonen"], "venue": "Signal Processing: Image Communications, vol. 22, no. 6, pp. 557\u2013568, July 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "PicSOM \u2014 self-organizing image retrieval with MPEG-7 content descriptions", "author": ["J. Laaksonen", "M. Koskela", "E. Oja"], "venue": "IEEE Trans. on Neural Network, vol. 13, pp. 841\u2013853, 2002.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "What are you looking for?: An eyetracking study of information usage in web search", "author": ["E. Cutrell", "Z. Guan"], "venue": "CHI \u201907:  Proc. of the SIGCHI conference on Human Factors in Computing Systems. New York, NY, USA: ACM, 2007, pp. 407\u2013416.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Eye-tracking analysis of user behavior in WWW search", "author": ["L.A. Granka", "T. Joachims", "G. Gay"], "venue": "SIGIR \u201904: Proc. of the 27th annual international ACM SIGIR conference on Research and Development in Information Retrieval. New York, NY, USA: ACM, 2004, pp. 478\u2013479.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Information retrieval by inferring implicit queries from eye movements", "author": ["D.R. Hardoon", "J. Shawe-Taylor", "A. Ajanki", "K. Puolam\u00e4ki", "S. Kaski"], "venue": "11th International Conference on Artificial Intelligence and Statistics, 2007.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Can relevance be inferred from eye movements in information retrieval?", "author": ["J. Saloj\u00e4rvi", "I. Kojo", "J. Simola", "S. Kaski"], "venue": "in Proc. of WSOM\u201903, Workshop on Self-Organizing Maps. Kitakyushu, Japan: Kyushu Institute of Technology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2003}, {"title": "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2004}, {"title": "Learning convex combinations of continuously parameterized basic kernels.", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil"], "venue": "Computational Learning Theory, ser. Lecture Notes in Computer Science,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2005}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "Proc. of the 21st International Conference on Machine Learning, ICML. New York, NY, USA: ACM, 2004, pp. 41\u201348.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 5, pp. 27\u201372, 2004.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2004}, {"title": "Image ranking with implicit feedback from eye movements", "author": ["D.R. Hardoon", "K. Pasupa"], "venue": "Proc. of ETRA 2010: ACM Symposium on Eye-Tracking Research & Applications. New York, NY, USA: ACM, 2010, pp. 291\u2013298.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensor products of Hilbert space effect algebras", "author": ["S. Pulmannov\u00e1"], "venue": "Reports on Mathematical Physics, vol. 53(2), pp. 301\u2013316, 2004.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning via linear operators: Maximum margin regression; multiclass and multiview learning at one-class complexity", "author": ["S. Szedmak", "J. Shawe-Taylor", "E. Parado-Hernandez"], "venue": "University of Southampton, Tech. Rep., 2005.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2005}, {"title": "Decomposing the tensor kernel support vector machine for neuroscience data with structure labels", "author": ["D.R. Hardoon", "J. Shawe-Taylor"], "venue": "Machine Learning Journal: Special Issue on Learning From Multiple Sources, vol. 79, no. 1-2, pp. 29\u201346, 2010.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Using confidence bounds for exploration-exploitation trade-offs", "author": ["P. Auer"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 397\u2013 422, 2003.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2003}, {"title": "A contextualbandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "WWW2010: Proc. of the 19th International Conference on the World Wide Web. New York, NY, USA: ACM, 2010, pp. 661\u2013670.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR09: IEEE Conference on Computer Vision and Pattern Recognition. Los Alamitos, CA, USA: IEEE, 2009, pp. 248\u2013255.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "A common approach to continue the search, after possible pruning by tags, is to ask the user for explicit relevance feedback on the shown images [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "Another approach is to obtain this feedback implicitly, by measuring indirect signals on attention patterns of the users and inferring the relevance of the seen images from these [2], [3], [4], [5], [6], [7].", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "Another approach is to obtain this feedback implicitly, by measuring indirect signals on attention patterns of the users and inferring the relevance of the seen images from these [2], [3], [4], [5], [6], [7].", "startOffset": 184, "endOffset": 187}, {"referenceID": 3, "context": "Another approach is to obtain this feedback implicitly, by measuring indirect signals on attention patterns of the users and inferring the relevance of the seen images from these [2], [3], [4], [5], [6], [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 4, "context": "Another approach is to obtain this feedback implicitly, by measuring indirect signals on attention patterns of the users and inferring the relevance of the seen images from these [2], [3], [4], [5], [6], [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "Another approach is to obtain this feedback implicitly, by measuring indirect signals on attention patterns of the users and inferring the relevance of the seen images from these [2], [3], [4], [5], [6], [7].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "Another approach is to obtain this feedback implicitly, by measuring indirect signals on attention patterns of the users and inferring the relevance of the seen images from these [2], [3], [4], [5], [6], [7].", "startOffset": 204, "endOffset": 207}, {"referenceID": 7, "context": "ference papers [8], [9].", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "ference papers [8], [9].", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "Content-based image retrieval (CBIR) is a well-researched topic, whose history can be followed and comprehensive introductions to which can be found in surveys such as [1], [10], [11], [12].", "startOffset": 168, "endOffset": 171}, {"referenceID": 9, "context": "Content-based image retrieval (CBIR) is a well-researched topic, whose history can be followed and comprehensive introductions to which can be found in surveys such as [1], [10], [11], [12].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "Content-based image retrieval (CBIR) is a well-researched topic, whose history can be followed and comprehensive introductions to which can be found in surveys such as [1], [10], [11], [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "Content-based image retrieval (CBIR) is a well-researched topic, whose history can be followed and comprehensive introductions to which can be found in surveys such as [1], [10], [11], [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 12, "context": ", reviews [13], [14].", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": ", reviews [13], [14].", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "The semantic gap [11], i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "In some types of searches it will be just the visual and not the semantic similarity between the searched and retrieved images that plays the primary role and, consequently, the problem of the semantic gap will be minimal [1].", "startOffset": 222, "endOffset": 225}, {"referenceID": 14, "context": "Since the mid-1990\u2019s, relevance feedback has been used for incorporating the user\u2019s preferences and his understanding of the semantic similarity of images in the retrieval process [15], [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "Since the mid-1990\u2019s, relevance feedback has been used for incorporating the user\u2019s preferences and his understanding of the semantic similarity of images in the retrieval process [15], [16].", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "Research on relevance feedback techniques constitutes a subfield of CBIR research in its own right and the early works on the topic have been summarized in [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 15, "context": "In retrieval systems with multiple feature representations of the images, a straightforward approach could be to ask the user to tune the relative weights of the features in order to be able to find more relevant images [16].", "startOffset": 220, "endOffset": 224}, {"referenceID": 16, "context": "In practical CBIR systems implementing relevance feedback, the standard setting is that after the user has been presented with a set of images, the system expects him to reliably assess the relevance of each retrieved image and to return this information back to the system [17].", "startOffset": 274, "endOffset": 278}, {"referenceID": 17, "context": "It is also possible that instead of assessing each image independently, the user is asked to rank the images on the page by their relevance in comparison searching [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "those cited in [17]), explicit interactive relevance feedback has been shown to provide a dramatic improvement in the accuracy of image retrieval.", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "It has become clear that implicit feedback can improve information retrieval accuracy (see the review [4]), but figuring out the most effective modalities for various search scenarios is still a subject of ongoing research and various alternatives are being proposed ranging from simple measures like number of clicks to brain computer interfaces that are not yet practically feasible for real search tools.", "startOffset": 102, "endOffset": 105}, {"referenceID": 18, "context": "[19] studied use of mouse and keyboard activity, as well as time spent on the page and scrolling, and [20] compared the amount of information between such implicit channels and explicit feedback.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[19] studied use of mouse and keyboard activity, as well as time spent on the page and scrolling, and [20] compared the amount of information between such implicit channels and explicit feedback.", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "More advanced works still using the regular control devices use click-through data, typically on the search result page [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 21, "context": "The C3Vision system [22] and a human-aided computing approach by [23] infer image categories or presence of distinct objects in images from EEG measurements, and [24], [25] use fMRI techniques for image categorization.", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "The C3Vision system [22] and a human-aided computing approach by [23] infer image categories or presence of distinct objects in images from EEG measurements, and [24], [25] use fMRI techniques for image categorization.", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "The C3Vision system [22] and a human-aided computing approach by [23] infer image categories or presence of distinct objects in images from EEG measurements, and [24], [25] use fMRI techniques for image categorization.", "startOffset": 162, "endOffset": 166}, {"referenceID": 24, "context": "The C3Vision system [22] and a human-aided computing approach by [23] infer image categories or presence of distinct objects in images from EEG measurements, and [24], [25] use fMRI techniques for image categorization.", "startOffset": 168, "endOffset": 172}, {"referenceID": 25, "context": "[26] built a prototype image annotation system using these ideas; relevance of images is inferred from EEG and visual pattern mining is used to retrieve similar images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "For example, [27] infers tags for images from implicit speech and [28] considers facial expressions as", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "For example, [27] infers tags for images from implicit speech and [28] considers facial expressions as", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "In addition, various physiological measurements are extensively used for inferring the affective state of the user, which can in turn be used as a feedback source [29], [30].", "startOffset": 163, "endOffset": 167}, {"referenceID": 29, "context": "In addition, various physiological measurements are extensively used for inferring the affective state of the user, which can in turn be used as a feedback source [29], [30].", "startOffset": 169, "endOffset": 173}, {"referenceID": 30, "context": "The primary feedback in this work is based on eye movements, which have become an increasingly popular feedback source in recent years, following the early concepts by [31].", "startOffset": 168, "endOffset": 172}, {"referenceID": 31, "context": "The approaches range from explicit control [32] and relevance estimation of text passages [33], [34] to inferring complete queries based on eye-movements on the results pages [35].", "startOffset": 43, "endOffset": 47}, {"referenceID": 32, "context": "The approaches range from explicit control [32] and relevance estimation of text passages [33], [34] to inferring complete queries based on eye-movements on the results pages [35].", "startOffset": 90, "endOffset": 94}, {"referenceID": 33, "context": "The approaches range from explicit control [32] and relevance estimation of text passages [33], [34] to inferring complete queries based on eye-movements on the results pages [35].", "startOffset": 96, "endOffset": 100}, {"referenceID": 34, "context": "The approaches range from explicit control [32] and relevance estimation of text passages [33], [34] to inferring complete queries based on eye-movements on the results pages [35].", "startOffset": 175, "endOffset": 179}, {"referenceID": 35, "context": "Based on the results of a comparison between a visual attention model and measured gaze fixations, it was suggested in [36] that eye tracking could be used as an interface for image retrieval, but no actual retrieval setup was yet investigated.", "startOffset": 119, "endOffset": 123}, {"referenceID": 6, "context": "The Eye-Vision-Bot system, presented in [7], integrated an eye tracker with the GIFT image retrieval system1 merely as a demonstration of the possibilities of gaze-based interaction without any experimental evaluations.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "In [3], [37], a CBIR system was implemented that used offline image saliency and online gaze fixations for extracting visual features from those image areas that were likely to be relevant when determining the relevancy of the image.", "startOffset": 3, "endOffset": 6}, {"referenceID": 36, "context": "In [3], [37], a CBIR system was implemented that used offline image saliency and online gaze fixations for extracting visual features from those image areas that were likely to be relevant when determining the relevancy of the image.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "First fully interactive and experimentally evaluated CBIR systems that made use of eye-tracking data were presented in [2], [6], [38].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "First fully interactive and experimentally evaluated CBIR systems that made use of eye-tracking data were presented in [2], [6], [38].", "startOffset": 124, "endOffset": 127}, {"referenceID": 37, "context": "First fully interactive and experimentally evaluated CBIR systems that made use of eye-tracking data were presented in [2], [6], [38].", "startOffset": 129, "endOffset": 133}, {"referenceID": 37, "context": "The selection of an image as relevant was in [38] solely dependent on the accumulated fixation time exceeding a preset threshold, whereas in [6] also a richer set of gaze parameters, including saccadic speeds and the number of images with fixations, were used.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "The selection of an image as relevant was in [38] solely dependent on the accumulated fixation time exceeding a preset threshold, whereas in [6] also a richer set of gaze parameters, including saccadic speeds and the number of images with fixations, were used.", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "Image similarity assessment was in [2] based on visual features extracted from non-overlapping tiles of the images.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "Clear performance improvements were obtained in the evaluations over random selection in [6], [38] and over simple image clicking without gaze-based distance weighting in [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 37, "context": "Clear performance improvements were obtained in the evaluations over random selection in [6], [38] and over simple image clicking without gaze-based distance weighting in [2].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "Clear performance improvements were obtained in the evaluations over random selection in [6], [38] and over simple image clicking without gaze-based distance weighting in [2].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Two decisive characteristics common to the setups of [2], [6], [38] should, however, be noticed.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "Two decisive characteristics common to the setups of [2], [6], [38] should, however, be noticed.", "startOffset": 58, "endOffset": 61}, {"referenceID": 37, "context": "Two decisive characteristics common to the setups of [2], [6], [38] should, however, be noticed.", "startOffset": 63, "endOffset": 67}, {"referenceID": 38, "context": "Later, also [39] and [40], [41] and [42] introduced their image retrieval systems using eye movements.", "startOffset": 12, "endOffset": 16}, {"referenceID": 39, "context": "Later, also [39] and [40], [41] and [42] introduced their image retrieval systems using eye movements.", "startOffset": 21, "endOffset": 25}, {"referenceID": 40, "context": "Later, also [39] and [40], [41] and [42] introduced their image retrieval systems using eye movements.", "startOffset": 27, "endOffset": 31}, {"referenceID": 41, "context": "Later, also [39] and [40], [41] and [42] introduced their image retrieval systems using eye movements.", "startOffset": 36, "endOffset": 40}, {"referenceID": 38, "context": "one [39] was based on a conceptual interface designed to be controlled completely by implicit gaze, providing a mix of a browsing and search tool.", "startOffset": 4, "endOffset": 8}, {"referenceID": 40, "context": "The second study mostly concentrated on the accuracy of inferring the relevance in [41] and on fixation-weighted region matching between the query and database images in [40].", "startOffset": 83, "endOffset": 87}, {"referenceID": 39, "context": "The second study mostly concentrated on the accuracy of inferring the relevance in [41] and on fixation-weighted region matching between the query and database images in [40].", "startOffset": 170, "endOffset": 174}, {"referenceID": 41, "context": "The last one [42] used gaze data for genuinely implicit relevance feedback by the means of reranking the results of Google Image Search.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": "Tensor decomposition and multiple kernel learning modules then infer a metric between images using known visual features of the images (see Table 1 and [43] for more detailed descriptions of the used features) and relevance feedback on the seen images.", "startOffset": 152, "endOffset": 156}, {"referenceID": 43, "context": "Finally, the system selects a new set of images with the LINREL algorithm based on the inferred relevance scores and the final metric given by the tensor decomposition, and the images are retrieved from a database and displayed through the PicSOM backend [44].", "startOffset": 255, "endOffset": 259}, {"referenceID": 38, "context": "As implicit feedback PinView uses eye movements of the user, building on the recent promising results on inferring image relevance from eye movements [39], [41].", "startOffset": 150, "endOffset": 154}, {"referenceID": 40, "context": "As implicit feedback PinView uses eye movements of the user, building on the recent promising results on inferring image relevance from eye movements [39], [41].", "startOffset": 156, "endOffset": 160}, {"referenceID": 44, "context": "Some examples include the human-computer interaction aspects of how users perform searches [45], analysis of user behavior in web search [46], and using eye movements as implicit relevance feedback in textual IR [47], [48].", "startOffset": 91, "endOffset": 95}, {"referenceID": 45, "context": "Some examples include the human-computer interaction aspects of how users perform searches [45], analysis of user behavior in web search [46], and using eye movements as implicit relevance feedback in textual IR [47], [48].", "startOffset": 137, "endOffset": 141}, {"referenceID": 46, "context": "Some examples include the human-computer interaction aspects of how users perform searches [45], analysis of user behavior in web search [46], and using eye movements as implicit relevance feedback in textual IR [47], [48].", "startOffset": 212, "endOffset": 216}, {"referenceID": 47, "context": "Some examples include the human-computer interaction aspects of how users perform searches [45], analysis of user behavior in web search [46], and using eye movements as implicit relevance feedback in textual IR [47], [48].", "startOffset": 218, "endOffset": 222}, {"referenceID": 48, "context": "of a given image category of the PASCAL Visual Object Classes Challenge 2007 (VOC2007) dataset [49].", "startOffset": 95, "endOffset": 99}, {"referenceID": 49, "context": "Given image feature vectors \u03c6(I), \u03c6(J), let the inner product \u03c6(I)\u03c6(J)> = k(I, J) denote the kernel function k(\u00b7, \u00b7) between images I and J , where \u03c6 is some feature mapping [50].", "startOffset": 174, "endOffset": 178}, {"referenceID": 50, "context": "Multiple kernel learning (MKL) attempts to find a combination of kernels by solving a classification (or regression) problem using a weighted combination of kernels [51], [52], [53].", "startOffset": 165, "endOffset": 169}, {"referenceID": 51, "context": "Multiple kernel learning (MKL) attempts to find a combination of kernels by solving a classification (or regression) problem using a weighted combination of kernels [51], [52], [53].", "startOffset": 171, "endOffset": 175}, {"referenceID": 52, "context": "Multiple kernel learning (MKL) attempts to find a combination of kernels by solving a classification (or regression) problem using a weighted combination of kernels [51], [52], [53].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "We follow an elastic-net formulation of ridge regression MKL, which uses a parameter \u03bb \u2208 [0, 1] in order to move between a 1-norm regularization (when \u03bb = 1) and a 2-norm regularization (when \u03bb = 0).", "startOffset": 89, "endOffset": 95}, {"referenceID": 53, "context": "We learn this relationship by using a tensor representation which creates an implicit correlation space [54].", "startOffset": 104, "endOffset": 108}, {"referenceID": 54, "context": "The tensor representation can be computed by taking dot products between each individual kernel matrix of each view [55], [56].", "startOffset": 116, "endOffset": 120}, {"referenceID": 55, "context": "The tensor representation can be computed by taking dot products between each individual kernel matrix of each view [55], [56].", "startOffset": 122, "endOffset": 126}, {"referenceID": 56, "context": "We then use the kernel matrix K\u03a6\u25e6\u03a8 to train a tensor kernel SVM [57] to generate a weight matrix which is composed of both views.", "startOffset": 64, "endOffset": 68}, {"referenceID": 56, "context": "This has been resolved by [57], who propose a novel singular value decomposition (SVD) like approach for decomposing the resulting tensor weight matrix into its two component parts, without needing to directly access the feature spaces \u03c6 and \u03c8.", "startOffset": 26, "endOffset": 30}, {"referenceID": 53, "context": "Given \u03b2 = [\u03b2u \u2208 R]1\u2264u\u2264t\u22121 we can project any of the MKL combined image features as follows [54]:", "startOffset": 91, "endOffset": 95}, {"referenceID": 57, "context": "The LINREL algorithm (originally devised and analysed in [58]) is an exploration-exploitation oriented online learning algorithm.", "startOffset": 57, "endOffset": 61}, {"referenceID": 57, "context": "It is shown in the analysis of LINREL [58], that selecting an image with high variance according to the above rule improves the accuracy of the estimated weight vector \u0175t.", "startOffset": 38, "endOffset": 42}, {"referenceID": 57, "context": "While the original LINREL algorithm in [58] explores the dimensions of the feature vector explicitly, more recent variations of LINREL (e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 58, "context": "LinUCB in [59]) use regularization to deal with large feature vectors.", "startOffset": 10, "endOffset": 14}, {"referenceID": 57, "context": "It has been shown in [58] that the variance of the estimate at(I) \u00b7 rt can essentially be bounded by \u03c3 t (I) = \u2016at(I)\u20162.", "startOffset": 21, "endOffset": 25}, {"referenceID": 43, "context": "For evaluation purposes these variants are compared with the baseline of browsing (that is, showing randomly ordered images) and the PicSOM [44] CBIR system sharing the same interface as PinView but lacking the novel machine learning components.", "startOffset": 140, "endOffset": 144}, {"referenceID": 48, "context": "The data set of images used in the offline experiments is the train subset of the PASCAL Visual Object Classes Challenge 2007 (VOC2007) dataset [49].", "startOffset": 144, "endOffset": 148}, {"referenceID": 59, "context": "The online experiments use a subset of the ImageNet dataset [60], created by the authors and called IMG2010 dataset.", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "Our final conclusions from the present work and other serious attempts [2], [6] to use and evaluate implicit relevance feedback from eye movements in iterative online content-based image retrieval are as follows: First, when used for purely implicit relevance feedback, eye movements perform better than random picking as was demonstrated in [6] and in this paper.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "Our final conclusions from the present work and other serious attempts [2], [6] to use and evaluate implicit relevance feedback from eye movements in iterative online content-based image retrieval are as follows: First, when used for purely implicit relevance feedback, eye movements perform better than random picking as was demonstrated in [6] and in this paper.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Our final conclusions from the present work and other serious attempts [2], [6] to use and evaluate implicit relevance feedback from eye movements in iterative online content-based image retrieval are as follows: First, when used for purely implicit relevance feedback, eye movements perform better than random picking as was demonstrated in [6] and in this paper.", "startOffset": 342, "endOffset": 345}, {"referenceID": 1, "context": "Third, when combining explicit click-based and implicit gaze-based relevance feedback together, the system performance will exceed the level of solely explicit relevance feedback as was proven in [2] and in our experiments.", "startOffset": 196, "endOffset": 199}, {"referenceID": 1, "context": "In [2], the image collection was arguably simpler than ours.", "startOffset": 3, "endOffset": 6}], "year": 2014, "abstractText": "This paper describes PinView, a content-based image retrieval system that exploits implicit relevance feedback collected during a search session. PinView contains several novel methods to infer the intent of the user. From relevance feedback, such as eye movements or pointer clicks, and visual features of images, PinView learns a similarity metric between images which depends on the current interests of the user. It then retrieves images with a specialized online learning algorithm that balances the tradeoff between exploring new images and exploiting the already inferred interests of the user. We have integrated PinView to the content-based image retrieval system PicSOM, which enables applying PinView to real-world image databases. With the new algorithms PinView outperforms the original PicSOM, and in online experiments with real users the combination of implicit and explicit feedback gives", "creator": "LaTeX with hyperref package"}}}