{"id": "1410.0510", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2014", "title": "Deep Sequential Neural Network", "abstract": "Neural Networks sequentially build high-level features through their successive layers. We propose here a new neural network model where each layer is associated with a set of candidate mappings. When an input is processed, at each layer, one mapping among these candidates is selected according to a sequential decision process. The resulting model is structured according to a DAG like architecture, so that a path from the root to a leaf node defines a sequence of transformations. Instead of considering global transformations, like in classical multilayer networks, this model allows us for learning a set of local transformations. It is thus able to process data with different characteristics through specific sequences of such local transformations, increasing the expression power of this model w.r.t a classical multilayered network. The learning algorithm is inspired from policy gradient techniques coming from the reinforcement learning domain and is used here instead of the classical back-propagation based gradient descent techniques. Experiments on different datasets show the relevance of this approach.", "histories": [["v1", "Thu, 2 Oct 2014 10:58:17 GMT  (448kb,D)", "http://arxiv.org/abs/1410.0510v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["ludovic denoyer", "patrick gallinari"], "accepted": false, "id": "1410.0510"}, "pdf": {"name": "1410.0510.pdf", "metadata": {"source": "CRF", "title": "Deep Sequential Neural Network", "authors": ["Ludovic Denoyer", "Patrick Gallinari"], "emails": ["ludovic.denoyer@lip6.fr", "patrick.gallinari@lip6.fr"], "sections": [{"heading": null, "text": "At this point, we propose a new neural network model, in which each layer is associated with a series of candidate mappings. As an input is processed on each layer, a mapping between these candidates is selected according to a sequential decision-making process. The resulting model is structured according to a DAG-like architecture, so that a path from the root to a leaf node defines a sequence of transformations. Instead of considering global transformations as in classic multi-layered networks, this model allows us to learn a series of local transformations, and is therefore able to process data with different properties through specific sequences of such local transformations, increasing the expressiveness of this model."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Deep Sequential Neural Networks", "text": "Consider X = RX the input space and Y = RY the output space, where X and Y are each the dimension of the input and output spaces. We designate {(x1, y1),..., (x, \"y\") the set of designated educational instances so that xi-X and yi-y. (x \"+ 1, y\" + 1),..., (xT, yT)} denote the set of test examples. The DSNN model has a DAG structure defined as follows: \u2022 Each node n is in {n1,..., nN} where N is the total number of nodes of the DAG \u2022 n root node n1, n1 has no parent node. \u2022 cn, i corresponds to the i-th child of the node n and # n is the number of children of n, in this case i is a value between 1 and # n."}, {"heading": "2.1 Inference in DSNN", "text": "In view of such a DAG structure G, the inference process is as follows: 1. First, an input x x x is presented at the root node n (1) = n1 of the DAG2. 2n (t) is used to denote the node selected in due course. (1) DSNN inference method 1: method INFERENCE (x). x is the input vector 2: z (1) x 3: n (1) \u2190 n1 4: t 1 5: while non-leaf (n (t)) inference method 1: method INFERENCE (t) (z (t)). Sampling with the distribution over children nodes 7: n (t + 1) \u2190 n1 5: while not leaf (t) 5: inference method (n) is terminated. The inference ended 6: a (t)."}, {"heading": "3 Learning DSNN with gradient-based approaches", "text": "The training method aims to simultaneously learn the two mapping functions fi, j and the selection functions pi (to minimize a given learning loss). (...) Our learning algorithm is based on an extension of the progression techniques inspired by the reinforcement literature. (...) Specifically, our learning method is close to the methods proposed in [18] and [12]. (...) The performance of our system is not referred to as a reward signal, which is common in reinforcement learning, but as a loss function. (...) Let us name the parameters of the f functions and the parameters of the p functions. (...) The performance of our system is referred to as J (...). (...)"}, {"heading": "4 Experiment", "text": "This year, it's gotten to the point that it will be able to retaliate, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said.\" But it's the way it is. \""}, {"heading": "5 Related Work", "text": "The idea of processing input data through various functions is not new and has been suggested, for example, in Neural Tree Networks [17, 15], with Hierarchical Mixture of Experts [8], where the idea is to calculate different transformations of data and aggregate these transformations; the difference to our approach lies both in the inference process and in the way the model is learned; they also share the idea of processing different inputs with different calculations, which is the main idea behind decision trees [13] and also newer classification techniques such as [3]. Finally, some links have already been established between classification and amplification learning algorithms [4, 2]. In particular, the use of recursive neural networks from the modeling of Markov decision processes learned through policy gradient techniques has been extensively investigated in [18] and in a recent paper suggesting the use of such models for image classification [12]."}, {"heading": "6 Conclusion and Perspectives", "text": "We have proposed a new family of models called Deep Sequential Neural Networks, which differ from neural networks because they do not always apply the same transformations, but can choose which transformation they want to apply depending on the input. The learning algorithm is based on the calculation of the gradient obtained by extending political gradient techniques. In its simplest form, DSNNs are equivalent to DNNs. Experiments with different data sets have shown the effectiveness of these models."}], "references": [{"title": "The dropout learning algorithm", "author": ["Pierre Baldi", "Peter J. Sadowski"], "venue": "Artif. Intell.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Fast classification using sparse decision dags", "author": ["R\u00f3bert Busa-Fekete", "Djalel Benbouzid", "Bal\u00e1zs K\u00e9gl"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Text classification: A sequential reading approach", "author": ["Gabriel Dulac-Arnold", "Ludovic Denoyer", "Patrick Gallinari"], "venue": "In Advances in Information Retrieval - 33rd European Conference on IR Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Sequential approaches for learning datum-wise sparse representations", "author": ["Gabriel Dulac-Arnold", "Ludovic Denoyer", "Philippe Preux", "Patrick Gallinari"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Sequentially generated instance-dependent image representations for classification", "author": ["Gabriel Dulac-Arnold", "Ludovic Denoyer", "Nicolas Thome", "Matthieu Cord", "Patrick Gallinari"], "venue": "Internation Conference on Learning Representations - ICLR 2014,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning to segment from a few well-selected training images", "author": ["Alireza Farhangfar", "Russell Greiner", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["Michael I. Jordan", "Robert A. Jacobs"], "venue": "Neural Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Timely object recognition", "author": ["Sergey Karayev", "Tobias Baumgartner", "Mario Fritz", "Trevor Darrell"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Neural Networks: Tricks of the Trade - Second Edition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1406.6247,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Induction of decision trees", "author": ["J. Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1986}, {"title": "Multi-column deep neural networks for image classification", "author": ["Jurgen Schmidhuber"], "venue": "In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Neural trees: a new tool for classification", "author": ["J A Sirat", "J-P Nadal"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Deep learning for NLP (without magic)", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Human Language Technologies,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Perceptron trees: A case study in hybrid concept representations", "author": ["Paul E. Utgoff"], "venue": "In Proceedings of the 7th National Conference on Artificial Intelligence. St. Paul,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1988}, {"title": "Solving deep memory pomdps with recurrent policy gradients", "author": ["Daan Wierstra", "Alexander F\u00f6rster", "Jan Peters", "J\u00fcrgen Schmidhuber"], "venue": "In Artificial Neural Networks - ICANN", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Reinforcement Learning (RL) techniques which are usually devoted to problems in dynamic environments have been recently used for classical machine learning tasks like classification [5, 2].", "startOffset": 182, "endOffset": 188}, {"referenceID": 1, "context": "Reinforcement Learning (RL) techniques which are usually devoted to problems in dynamic environments have been recently used for classical machine learning tasks like classification [5, 2].", "startOffset": 182, "endOffset": 188}, {"referenceID": 2, "context": "For example [3] and [12] consider that the sequential process is an acquisition process able to focus on relevant parts of the input data; [9] for example focuses on the sequential prediction process with a cascade approach.", "startOffset": 12, "endOffset": 15}, {"referenceID": 11, "context": "For example [3] and [12] consider that the sequential process is an acquisition process able to focus on relevant parts of the input data; [9] for example focuses on the sequential prediction process with a cascade approach.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "For example [3] and [12] consider that the sequential process is an acquisition process able to focus on relevant parts of the input data; [9] for example focuses on the sequential prediction process with a cascade approach.", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "RL opens now some interesting research directions for classical ML tasks and allows one to imagine solutions to complex problems like budgeted classification [4] or anytime prediction [6].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "RL opens now some interesting research directions for classical ML tasks and allows one to imagine solutions to complex problems like budgeted classification [4] or anytime prediction [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 9, "context": "The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].", "startOffset": 112, "endOffset": 120}, {"referenceID": 13, "context": "The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].", "startOffset": 112, "endOffset": 120}, {"referenceID": 6, "context": "The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].", "startOffset": 141, "endOffset": 144}, {"referenceID": 18, "context": "The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 15, "context": "The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "Many variants of learning algorithms have been proposed, from complex gradient computations [11], to dropout methods [1], but the baseline learning algorithm still consists in recursively computing the gradient by using the back-propagation algorithm and performing (stochastic) gradient descent.", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Many variants of learning algorithms have been proposed, from complex gradient computations [11], to dropout methods [1], but the baseline learning algorithm still consists in recursively computing the gradient by using the back-propagation algorithm and performing (stochastic) gradient descent.", "startOffset": 117, "endOffset": 120}, {"referenceID": 17, "context": "More precisely, our learning method is close to the methods proposed in [18] and [12] with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206 computing the quality of the system.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "More precisely, our learning method is close to the methods proposed in [18] and [12] with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206 computing the quality of the system.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Instead of using this estimate, we replace \u2206(F (xi, H), y) by \u2206(F (xi, H), y) \u2212 b where b = Ep(x,H,y)[\u2206(F (xi, H), y)] which can be easily estimated on the training set [18].", "startOffset": 169, "endOffset": 173}, {"referenceID": 16, "context": "The idea of processing input data by different functions is not new and have been proposed for example in Neural Tree Networks [17, 15], with Hierarchical Mixture of Experts [8] where the idea is to compute different transformations of data and to aggregate these transformations.", "startOffset": 127, "endOffset": 135}, {"referenceID": 14, "context": "The idea of processing input data by different functions is not new and have been proposed for example in Neural Tree Networks [17, 15], with Hierarchical Mixture of Experts [8] where the idea is to compute different transformations of data and to aggregate these transformations.", "startOffset": 127, "endOffset": 135}, {"referenceID": 7, "context": "The idea of processing input data by different functions is not new and have been proposed for example in Neural Tree Networks [17, 15], with Hierarchical Mixture of Experts [8] where the idea is to compute different transformations of data and to aggregate these transformations.", "startOffset": 174, "endOffset": 177}, {"referenceID": 12, "context": "They also share the idea of processing different inputs with different computations which is the a major idea underlying decision trees [13] and also more recent classification techniques like [3].", "startOffset": 136, "endOffset": 140}, {"referenceID": 2, "context": "They also share the idea of processing different inputs with different computations which is the a major idea underlying decision trees [13] and also more recent classification techniques like [3].", "startOffset": 193, "endOffset": 196}, {"referenceID": 3, "context": "At last, some links have already be done between classification and reinforcement learning algorithms [4, 2].", "startOffset": 102, "endOffset": 108}, {"referenceID": 1, "context": "At last, some links have already be done between classification and reinforcement learning algorithms [4, 2].", "startOffset": 102, "endOffset": 108}, {"referenceID": 17, "context": "Particularly, the use of recurrent neural networks from modelling Markov Decision Processes learned by Policy gradient techniques has been deeply explored in [18] and in a recent work that proposes the use of such models for image classification [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 11, "context": "Particularly, the use of recurrent neural networks from modelling Markov Decision Processes learned by Policy gradient techniques has been deeply explored in [18] and in a recent work that proposes the use of such models for image classification [12].", "startOffset": 246, "endOffset": 250}], "year": 2014, "abstractText": "Neural Networks sequentially build high-level features through their successive layers. We propose here a new neural network model where each layer is associated with a set of candidate mappings. When an input is processed, at each layer, one mapping among these candidates is selected according to a sequential decision process. The resulting model is structured according to a DAG like architecture, so that a path from the root to a leaf node defines a sequence of transformations. Instead of considering global transformations, like in classical multilayer networks, this model allows us for learning a set of local transformations. It is thus able to process data with different characteristics through specific sequences of such local transformations, increasing the expression power of this model w.r.t a classical multilayered network. The learning algorithm is inspired from policy gradient techniques coming from the reinforcement learning domain and is used here instead of the classical back-propagation based gradient descent techniques. Experiments on different datasets show the relevance of this approach.", "creator": "LaTeX with hyperref package"}}}