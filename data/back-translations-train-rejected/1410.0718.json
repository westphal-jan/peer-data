{"id": "1410.0718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2014", "title": "Not All Neural Embeddings are Born Equal", "abstract": "Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status.", "histories": [["v1", "Thu, 2 Oct 2014 21:35:35 GMT  (23kb,D)", "http://arxiv.org/abs/1410.0718v1", "4 pages plus 1 page of references"], ["v2", "Thu, 13 Nov 2014 15:58:35 GMT  (83kb,D)", "http://arxiv.org/abs/1410.0718v2", "4 pages plus 1 page of references"]], "COMMENTS": "4 pages plus 1 page of references", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix hill", "kyunghyun cho", "sebastien jean", "coline devin", "yoshua bengio"], "accepted": false, "id": "1410.0718"}, "pdf": {"name": "1410.0718.pdf", "metadata": {"source": "CRF", "title": "Not All Neural Embeddings are Born Equal", "authors": ["Felix Hill", "KyungHyun Cho"], "emails": [], "sections": [{"heading": null, "text": "It is known that word representations can be learned from the distribution patterns in corpora. Originally, such representations were constructed by counting word coincidences so that the characteristics in the representation of a word corresponded with other words [13, 19]. Neural language models, an alternative to learning word representations, use language data to optimize (latent) characteristics in relation to a language modeling goal. The goal may be to predict either the next word based on the initial words of a sentence [4, 16, 9] or simply a close word based on a single keyword [15, 17]. The representations learned through neural models (sometimes referred to as embedding) generally exceed those acquired through the coincidence of models when applied to NLP tasks [3]. Despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings."}, {"heading": "1 Learning embeddings from language data", "text": "Both neural language models and translation models learn real embedding (of a certain dimension) of words in a pre-defined vocabulary V that covers many or all words in their training corpus. At each training step, a \"score\" for the current training example (or stack) is calculated based on the embedding in its current state. This score is compared with the objective function of the model, and the error is propagated backwards to update both the model weights (which influence how the score is calculated from the embedding) and the embedding characteristics. At the end of this process, the embedding should encode information that will enable the model to optimally fulfill its goal."}, {"heading": "1.1 Monolingual models", "text": "In the original neural language model [4] and subsequent variants [9], each training example consists of n consecutive words from which the model is trained to predict the n-th word that gave the first n \u2212 ar Xiv: 141 0.07 18v1 [cs.CL] 2 October 21. Words. The model initially presents the input as an orderly sequence of embeddings, which it converts into a single fixed, \"hidden\" representation, e.g. by concatenation and non-linear projection. Based on this representation, a probability distribution is calculated above the vocabulary from which the model can try a guess on the next word. Model weights and embeddings are updated to maximize the probability of correct guesses for all sentences in the training corpus.Recent work has shown that high-quality word embeddings can be learned via the vocabulary if the vocabulary, without a non-linear layer [17]."}, {"heading": "1.2 Translation-based embeddings", "text": "Neural translation models generate a suitable sentence in their target language St by specifying a sentence Ss in their source language [see e.g. 18, 6], learning different sentences of embeddings for the words Vs and Vt in the source or target language, respectively. Such a model presents Ss as an ordered sequence of embeddings of words from Vs. The sequence for Ss is then encoded into a single representation RS. 2 Finally, by referring to the embeddings in Vt, RS and a representation of what has been generated so far, a sentence in the target language is decoded word by word. If the decoded word does not match the corresponding word in the training target St at any time, the error is recorded."}, {"heading": "2 Comparing Mono-lingual and Translation-based Embeddings", "text": "In order to understand the relationships between the different embeddings, we trained both the RNN encoder decoders [RNNenc, 7] and the RNN search architectures [2] on a 300m long word corpus of the English-French sentence pairs. We performed all experiments with the resulting (English) source genres from these models. By comparison, we developed a monolingual skipgram model [14] and its glove variant [17] for the same number of epochs on the English half of the bilingual corpus."}, {"heading": "2.1 Quantity of training data", "text": "Previous work has trained monolingual models on corpora that are many times larger than the English half of our parallel translation corpus. To verify whether these models simply require more training data to capture similarities as effectively as translation models, we trained them on ever larger subsets of Wikipedia.4 The results refute this possibility: the performance of monolingual embedding in similarity tasks converges significantly below the level of translation-based embedding (fig. 1)."}, {"heading": "2.2 Analogy questions", "text": "Dictionary analogy questions are an alternative method of evaluating word representations [15, 17]. In this task, models have to identify the correct answer (girl) if they ask questions such as \"man is for boy, woman is for...\" In case of embedding in skipgram style, it has been shown that if m, b and w are embedding for man, boy and woman, the correct answer is often the closest neighbor in the vocabulary (by cosmic distance) to the vector v = w + b \u2212 m [14]. [3] In order to limit the effective vocabulary of each model to the intersection of all model vocabularies, we have excluded all questions containing an answer outside this intersection from the vector v = w + b \u2212 m [14]."}, {"heading": "3 Conclusions", "text": "In fact, monolingual models include a general interpretation of the relationships between concepts (as found in the semantic analogy, for example), but also that they do not capture the similarity between the individual words either semantically or syntactically. Indeed, the fact that they also refer to the complete sentence model suggests that they problematize the similarity with the monolingual models is perhaps not surprising, since the words are generally neither semantic nor synchronous."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "[cs.CL],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder\u2013Decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Learning abstract concepts from multi-modal data: Since you probably can\u2019t see what i mean", "author": ["Felix Hill", "Anna Korhonen"], "venue": "In Proceedings of EMNLP 2014,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "[cs.CL],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1408.3456,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais"], "venue": "Psychological review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Anonymized. In Anonymized", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "Originally, such representations were constructed by counting word co-occurrences, so that the features in one word\u2019s representation corresponded to other words [13, 19].", "startOffset": 161, "endOffset": 169}, {"referenceID": 18, "context": "Originally, such representations were constructed by counting word co-occurrences, so that the features in one word\u2019s representation corresponded to other words [13, 19].", "startOffset": 161, "endOffset": 169}, {"referenceID": 3, "context": "The objective can be to predict either the next word given the initial words of a sentence [4, 16, 9], or simply a nearby word given a single cue word [15, 17].", "startOffset": 91, "endOffset": 101}, {"referenceID": 15, "context": "The objective can be to predict either the next word given the initial words of a sentence [4, 16, 9], or simply a nearby word given a single cue word [15, 17].", "startOffset": 91, "endOffset": 101}, {"referenceID": 8, "context": "The objective can be to predict either the next word given the initial words of a sentence [4, 16, 9], or simply a nearby word given a single cue word [15, 17].", "startOffset": 91, "endOffset": 101}, {"referenceID": 14, "context": "The objective can be to predict either the next word given the initial words of a sentence [4, 16, 9], or simply a nearby word given a single cue word [15, 17].", "startOffset": 151, "endOffset": 159}, {"referenceID": 16, "context": "The objective can be to predict either the next word given the initial words of a sentence [4, 16, 9], or simply a nearby word given a single cue word [15, 17].", "startOffset": 151, "endOffset": 159}, {"referenceID": 2, "context": "The representations learned by neural models (sometimes called embeddings) generally outperform those acquired by co-occurence counting models when applied to NLP tasks [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "In the original neural language model [4] and subsequent variants [9], each training example consists of n subsequent words, of which the model is trained to predict the n-th word given the first n \u2212", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "In the original neural language model [4] and subsequent variants [9], each training example consists of n subsequent words, of which the model is trained to predict the n-th word given the first n \u2212", "startOffset": 66, "endOffset": 69}, {"referenceID": 14, "context": "More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer [15, 17].", "startOffset": 118, "endOffset": 126}, {"referenceID": 16, "context": "More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer [15, 17].", "startOffset": 118, "endOffset": 126}, {"referenceID": 14, "context": "For instance, in the skipgram approach [15], for each \u2018cue word\u2019 w the \u2018context words\u2019 c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "To learn translation-based embeddings, we trained both the RNN encoder-decoder [RNNenc, 7] and the RNN Search architectures [2] on a 300m word corpus of English-French sentence pairs.", "startOffset": 124, "endOffset": 127}, {"referenceID": 13, "context": "For comparison, we trained a monolingual skipgram model [14] and its Glove variant [17] for the same number of epochs on the English half of the bilingual corpus.", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "For comparison, we trained a monolingual skipgram model [14] and its Glove variant [17] for the same number of epochs on the English half of the bilingual corpus.", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "As in previous studies [1, 5, 3], we evaluate embeddings by calculating pairwise (cosine) distances and correlating these distances with (gold-standard) human judgements.", "startOffset": 23, "endOffset": 32}, {"referenceID": 4, "context": "As in previous studies [1, 5, 3], we evaluate embeddings by calculating pairwise (cosine) distances and correlating these distances with (gold-standard) human judgements.", "startOffset": 23, "endOffset": 32}, {"referenceID": 2, "context": "As in previous studies [1, 5, 3], we evaluate embeddings by calculating pairwise (cosine) distances and correlating these distances with (gold-standard) human judgements.", "startOffset": 23, "endOffset": 32}, {"referenceID": 0, "context": "Table 1 shows the correlations of different model embeddings with three such gold-standard resources, WordSim-353 [1], MEN [5] and SimLex-999 [11].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Table 1 shows the correlations of different model embeddings with three such gold-standard resources, WordSim-353 [1], MEN [5] and SimLex-999 [11].", "startOffset": 123, "endOffset": 126}, {"referenceID": 10, "context": "Table 1 shows the correlations of different model embeddings with three such gold-standard resources, WordSim-353 [1], MEN [5] and SimLex-999 [11].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "To interpret these results, it should be noted that SimLex-999 evaluation quantifies conceptual similarity (dog - wolf ), whereas MEN and WordSim-353 (despite its name) quantify more general relatedness (dog - collar) [12].", "startOffset": 218, "endOffset": 222}, {"referenceID": 9, "context": "1 Subsequent variants apply different algorithms for selecting the set of (w, c) from the training corpus [10] Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence [2].", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "1 Subsequent variants apply different algorithms for selecting the set of (w, c) from the training corpus [10] Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence [2].", "startOffset": 215, "endOffset": 218}, {"referenceID": 12, "context": "The TOEFL synonym test contains 80 cue words, each with four possible answers, of which one is a correct synonym [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "Lexical analogy questions are an alternative way of evaluating word representations [15, 17].", "startOffset": 84, "endOffset": 92}, {"referenceID": 16, "context": "Lexical analogy questions are an alternative way of evaluating word representations [15, 17].", "startOffset": 84, "endOffset": 92}, {"referenceID": 13, "context": "For skipgram-style embeddings, it has been shown that if m,b and w are the embeddings for man, boy and woman respectively, the correct answer is often the nearest neighbour in the vocabulary (by cosine distance) to the vector v = w + b\u2212m [14].", "startOffset": 238, "endOffset": 242}, {"referenceID": 13, "context": "We evaluated the embeddings on this task using the same vector-algebra method as [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "To perform well on SimLex-999, embeddings must encode information approximating what concepts are (their function or ontology), even when this contradicts the signal conferred by co-occurrence (as can be the case for related-but-dissimilar concept pairs) [11].", "startOffset": 255, "endOffset": 259}], "year": 2017, "abstractText": "Neural language models learn word representations that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models. We show that translation-based embeddings outperform those learned by cutting-edge monolingual models at single-language tasks requiring knowledge of conceptual similarity and/or syntactic role. The findings suggest that, while monolingual models learn information about how concepts are related, neural-translation models better capture their true ontological status. It is well known that word representations can be learned from the distributional patterns in corpora. Originally, such representations were constructed by counting word co-occurrences, so that the features in one word\u2019s representation corresponded to other words [13, 19]. Neural language models, an alternative means to learn word representations, use language data to optimise (latent) features with respect to a language modelling objective. The objective can be to predict either the next word given the initial words of a sentence [4, 16, 9], or simply a nearby word given a single cue word [15, 17]. The representations learned by neural models (sometimes called embeddings) generally outperform those acquired by co-occurence counting models when applied to NLP tasks [3]. Despite these clear results, it is not well understood how the architecture of neural models affects the information encoded in their embeddings. Here, we explore this question by considering the embeddings learned by architectures with a very different objective function to monolingual language models: neural machine translation models. We show that translation-based embeddings outperform monolingual embeddings on two types of task: those that require knowledge of conceptual similarity (rather than simply association or relatedness), and those that require knowledge of syntactic role. We discuss what the findings indicate about the information content of different embeddings, and suggest how this content might emerge as a consequence of the translation objective. 1 Learning embeddings from language data Both neural language models and translation models learn real-valued embeddings (of specified dimension) for words in some pre-specified vocabulary, V , covering many or all words in their training corpus. At each training step, a \u2018score\u2019 for the current training example (or batch) is computed based on the embeddings in their current state. This score is compared to the model\u2019s objective function, and the error is backpropagated to update both the model weights (affecting how the score is computed from the embeddings) and the embedding features. At the end of this process, the embeddings should encode information that enables the model to optimally satisfy its objective. 1.1 Monolingual models In the original neural language model [4] and subsequent variants [9], each training example consists of n subsequent words, of which the model is trained to predict the n-th word given the first n \u2212 1 ar X iv :1 41 0. 07 18 v1 [ cs .C L ] 2 O ct 2 01 4 1 words. The model first represents the input as an ordered sequence of embeddings, which it transforms into a single fixed length \u2018hidden\u2019 representation by, e.g., concatenation and non-linear projection. Based on this representation, a probability distribution is computed over the vocabulary, from which the model can sample a guess at the next word. The model weights and embeddings are updated to maximise the probability of correct guesses for all sentences in the training corpus. More recent work has shown that high quality word embeddings can be learned via models with no nonlinear hidden layer [15, 17]. Given a single word in the corpus, these models simply predict which other words will occur nearby. For each word w in V , a list of training cases (w, c) : c \u2208 V is extracted from the training corpus. For instance, in the skipgram approach [15], for each \u2018cue word\u2019 w the \u2018context words\u2019 c are sampled from windows either side of tokens of w in the corpus (with c more likely to be sampled if it occurs closer to w).1 For each w in V , the model initialises both a cue-embedding, representing the w when it occurs as a cue-word, and a context-embedding, used when w occurs as a context-word. For a cue word w, the model can use the corresponding cueembedding and all context-embeddings to compute a probability distribution over V that reflects the probability of a word occurring in the context of w. When a training example (w, c) is observed, the model updates both the cue-word embedding of w and the context-word embeddings in order to increase the conditional probability of c. 1.2 Translation-based embeddings Neural translation models generate an appropriate sentence in their target language St given a sentence Ss in their source language [see, e.g., 18, 6]. In doing so, they learn distinct sets of embeddings for the vocabularies Vs and Vt in the source and target languages respectively. Observing a training case (Ss, St), such a model represents Ss as an ordered sequence of embeddings of words from Vs. The sequence for Ss is then encoded into a single representation RS .2 Finally, by referencing the embeddings in Vt, RS and a representation of what has been generated thus far, the model decodes a sentence in the target language word by word. If at any stage the decoded word does not match the corresponding word in the training target St, the error is recorded. The weights and embeddings in the model, which together parameterise the encoding and decoding process, are updated based on the accumulated error once the sentence decoding is complete. Despite differences in architecture, the translation objective results in similar pressures being exerted on the embeddings for both models. The source language embeddings must be such that the model can combine them to form single representations for ordered sequences of multiple words (which in turn must enable the decoding process). The target language embeddings must facilitate the process of decoding these representations into correct target-language sentences. 2 Comparing Mono-lingual and Translation-based Embeddings To learn translation-based embeddings, we trained both the RNN encoder-decoder [RNNenc, 7] and the RNN Search architectures [2] on a 300m word corpus of English-French sentence pairs. We conducted all experiments with the resulting (English) source embeddings from these models. For comparison, we trained a monolingual skipgram model [14] and its Glove variant [17] for the same number of epochs on the English half of the bilingual corpus. We also extracted embeddings from a full-sentence language model [CW, 8] trained for several months on a larger 1bn word corpus. As in previous studies [1, 5, 3], we evaluate embeddings by calculating pairwise (cosine) distances and correlating these distances with (gold-standard) human judgements. Table 1 shows the correlations of different model embeddings with three such gold-standard resources, WordSim-353 [1], MEN [5] and SimLex-999 [11]. Interestingly, translation embeddings perform best on SimLex-999, while the two sets of monolingual embeddings perform better on modelling the MEN and WordSim353. To interpret these results, it should be noted that SimLex-999 evaluation quantifies conceptual similarity (dog wolf ), whereas MEN and WordSim-353 (despite its name) quantify more general relatedness (dog collar) [12]. The results seem to indicate that translation-based embeddings better capture similarity, while monolingual embeddings better capture relatedness. 1 Subsequent variants apply different algorithms for selecting the set of (w, c) from the training corpus [10] Alternatively, subsequences (phrases) of Ss may be encoded at this stage in place of the whole sentence [2].", "creator": "LaTeX with hyperref package"}}}