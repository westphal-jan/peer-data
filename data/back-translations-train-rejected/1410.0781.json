{"id": "1410.0781", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2014", "title": "SimNets: A Generalization of Convolutional Networks", "abstract": "We present a deep layered architecture that generalizes classical convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators, one being a similarity function whose family contains the convolution operator used in ConvNets, and the other is a new 'soft max-min-mean' operator called MMECS that realizes classical operators like ReLU and max-pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. Two interesting properties that emerge from the architecture are: (i) the basic input to hidden-units to output-nodes machinery contains as special case a kernel machine, and (ii) initializing networks using unsupervised learning is natural. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are 1/8 the size of comparable ConvNets.", "histories": [["v1", "Fri, 3 Oct 2014 08:47:03 GMT  (628kb,D)", "https://arxiv.org/abs/1410.0781v1", null], ["v2", "Sat, 25 Oct 2014 09:47:07 GMT  (644kb,D)", "http://arxiv.org/abs/1410.0781v2", null], ["v3", "Sun, 7 Dec 2014 15:51:28 GMT  (648kb,D)", "http://arxiv.org/abs/1410.0781v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["nadav cohen", "amnon shashua"], "accepted": false, "id": "1410.0781"}, "pdf": {"name": "1410.0781.pdf", "metadata": {"source": "CRF", "title": "SimNets: A Generalization of Convolutional Networks", "authors": ["Nadav Cohen", "Amnon Shashua"], "emails": ["cohennadav@cs.huji.ac.il", "shashua@cs.huji.ac.il"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. The SimNet architecture", "text": "The SimNet architecture consists of two operators - a \"similarity\" operator defining a \"similarity,\" a \"similarity\" operator generalizing internal product integration in ConvNets, and a \"maximum\" (\"maximum\") operator defining a \"maximum\" (\"maximum\") operator, a \"maximum\" (\"maximum\") operator, a \"maximum\" (\"maximum\") operator, a \"maximum\" (\"maximum\") operator, a \"maximum\" (\"maximum\") operator, a \"maximum\" (\"maximum\" maximum \") operator, a\" maximum \"(\" maximum \"maximum\") operator, a \"maximum\" (\"maximum\" maximum \") operator,\" a \"maximum\" (\"maximum\") operator, \"a\" maximum \"(\" maximum \"),\" a \"maximum\" (\"maximum\"), \"a\" maximum \"(\" maximum \") operator,\" a \"maximum\" (\"maximum\" maximum \"),\" a \"d\" (\"maximum\"), \"a\" maximum \"(\" maximum \"),\" a \"d\" (\"maximum\"), \"a\" a \"maximum\" (\"maximum\" (\"maximum\"), \"a\" a \"d\" (\"maximum\"), \"a\" a \"(\" maximum \"(\" maximum \") operator,\" a \"a\" maximum \"(\" maximum \"maximum\"), \"a\" a \"(\" maximum \"(\" maximum \") operator,\" a \"a\" a \"(\" maximum \"maximum\", \"a\" maximum \"d\"), \"a\", \"a\" a \",\" a \"a\" a \"(\" maximum \"maximum\", a \"a\" maximum \"(\" maximum \"maximum\"), a \"a\", a \"(\" maximum \"maximum\", \"a\", \"a\" a \"maximum\" d \",\" a \", a\" (\"a\" maximum \"maximum\" a \"maximum\", a \"a\", \"a\" maximum \"maximum\" a \", a\", a \"maximum\", a \", a\", a \"(\" a \"maximum\" a \"maximum\" maximum \"), a\", a \"a\" a \"maximum\" a \"(\" a \"maximum\", a \"maximum\", a \"), a\", a \"(\""}, {"heading": "3. SimNets and kernel machines", "text": "So far, we have made the architectural decisions of SimNets to realize classic ConvNets, which are a rudimentary special case of the available possibilities. In particular, we have not used the lp similarity and the offsets {bts} in the MEX layer. In the following subsection, we will consider a \"Multi-Layer Perceptron\" (MLP) construction, which consists of a single hidden layer in addition to input and output layers. In the following subsection, we will examine the case in which the input layer is processed by patches, which includes hidden layer locality and sharing, and a pooling operation follows the hidden layer - a structure prevalent in classic ConvNets."}, {"heading": "3.1. MLP analogy: input\u2192 hidden layer\u2192 output", "text": "The similarity and MEX operators give simple generalizations of the folding and maximum / middle pooling layers in ConvNets. As we now show, they generate something of greater consequence when applied successively. To make the point as short as possible, we consider an MLP construction consisting of the input nodes (which form the input vector x-Rd), n hidden units and a single output unit. The value h (x) of the output unit is the result of a mapping unit Rd \u2192 R, defined by the two SimNet operators that are applied in succession (n similarity operators with different templates and shared mapping operators, followed by MEX with offsets): h (x) = MEXp > l unit (x, zl) + bl) l = 1, n (3) A direct analogy to the existing work is achieved by specifying a linear similarity."}, {"heading": "3.2. A basic 3-layer SimNet with locality, sharing", "text": "It is the first layer in which such a network, consisting of input cards (channels) - each layer for each layer of each individual feature card is then grouped together to reduce the dimensionality, and finally a classification output layer will predict the label of the input signal. We will show that such a network, consisting of input cards, a pooling sketch, which also corresponds to a kernel SVM output, with the kernels for a \"patch-based\" representation of the input signal. Locality, sharing and pooling are realized in the conventional way. Namely, the input sketch is divided into (possibly overlapping) patches, where d \u00b7 w \u00b7 D, h, the height and width of the mapping of a similar layer is realized."}, {"heading": "4. Other SimNet settings \u2013 global average pooling", "text": "In fact, we have the potential to provide a richer hypothesis than we do to double the number of parameters in the similarity layer. \"We have the potential to provide a richer hypothesis than we double the number of parameters in the similarity layer.\" \"We have the potential to provide a richer hypothesis.\" \"We have the potential to double the number of parameters in the similarity layer.\" \"We have the potential to double the number of parameters in the similarity layer.\" \"We have the potential to provide a richer hypothesis than we have to double the number of parameters in the similarity layer.\" \"We have the potential to double the number of parameters in the similarity layer.\" \"We have the potential to double the number of parameters in the similarity layer.\""}, {"heading": "5. Initializing parameters using unsupervised", "text": "In the past, however, these have been adopted by carefully selected random initializations that do not use any data at all (see, for example, [14, 29, 21]). An initialization scheme that is much larger than necessary to represent the true hypothetical space is sufficient to overcome the rigor of the training. Indeed, successful training of ConvNets typically requires the development of an over-specific network (i.e. a network that is much larger than necessary to represent the true hypothetical space), whereas the latter has been shown to produce good training results, a computational problem of over-adjustment also enables it. Expanded susceptibility to over-adjustment has led to various techniques and heuristic methods (dropout [11]) that represent a must-be-prominent form of art to work this scheme properly."}, {"heading": "6. Experiments", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "6.1. Benchmarking against the ConvNet", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "6.2. Benchmarking against the \u201csingle-layer\u201d network of Coates et al.", "text": "The \"single layer\" network of Coates et al. ([5]) is of interest to multiple accounts. First, with GMM coding, the network corresponds to the SimNet variant presented in eqn. 9. Second, their best result with \"triangle\" coding is one of the highest accuracies reported on CIFAR-10 for networks of this depth (absolute state of the art in 2011). Third, their observations in terms of the effect of beautification are relevant to the SimNet architecture, and indeed we found that for the evaluated SimNet with l1 and l2 similarities, whitening makes a difference."}, {"heading": "6.3. The importance of unsupervised initialization", "text": "In order to evaluate the importance of SimNet's unattended initialization scheme, which was presented in Section 5, we trained the evaluated SimNet (Fig. 3 (a) with a weighted l1 similarity, using no data for initialization. Specifically, we initialized the templates z1,..., zn randomly with a zero-mean variance of Gaussian distribution (in accordance with the fact that the input fields are white to have a zero-mean and unit variance), and the weights u1,..., un with constant. In addition to the difference in initialization, SimNet was trained exactly as described above. In conducting the experiment with 200 templates, the accuracy in cross-validation dropped from 76% to 74.1%. In 400 templates, the accuracy dropped from 76.8% to 74.4%. In 50 and 100 templates, there was no convergence of the learning algorithm. We conclude that the SimNet initialization scheme actually has a significant impact on the performance."}, {"heading": "7. Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Nitzan Guberman and Or Sharir for their dedicated contribution to the experiments carried out in this work, which is partly funded by the Intel grant ICRI-CI No. 9-2012-6133 and the ISF Center Scholarship 1790 / 12."}, {"heading": "A. Proof of theorem 1", "text": "To prove the theory, we need the following definition and the following problem: Definition 1 = > Let us be a Hilbert room and S-H be a collection of vectors. < V, V, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H"}, {"heading": "B. Patch-based kernel-SVM", "text": "In this appendix, we show how the classification described in eqn 10 is applied to all subordinate networks, which essentially corresponds to the basic \"localitysharing-pooling\" function. < < p (f), can be called a multiclass SVM ([6]) with reduced support vectors ([27]. In this formulation, the classified instances are not represented by integral vectors, but rather by blocks of multiple vectors. Furthermore, the support vectors are subject to limitations that can be interpreted as forced \"locality\" and \"sharing.\" In the context of SimNet, the vectors representing an input patch are simply the localization of the support vectors corresponds to the fact that the input vectors of local patches are processed in a spatially conscious manner, and the sharing constraint corresponds to the fact that the input vectors are displayed at a comparable level on all."}], "references": [{"title": "Image thresholding based on the em algorithm and the generalized gaussian distribution", "author": ["Yakoub Bazi", "Lorenzo Bruzzone", "Farid Melgani"], "venue": "Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence", "author": ["Joan Bruna", "St\u00e9phane Mallat"], "venue": "IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Kernel methods for deep learning", "author": ["Youngmin Cho", "Lawrence K Saul"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y Ng", "Honglak Lee"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "author": ["Caglar Gulcehre", "Kyunghyun Cho", "Razvan Pascanu", "Yoshua Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Yangqing Jia"], "venue": "http:// caffe.berkeleyvision.org/,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Sum-product networks: A new deep architecture", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Hierarchical models of object recognition in cortex", "author": ["Maximilian Riesenhuber", "Tomaso Poggio"], "venue": "Nature neuroscience,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A direct method for building sparse kernel learning algorithms", "author": ["Mingrui Wu", "Bernhard Sch\u00f6lkopf", "G\u00f6khan Bak\u0131r"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "An introduction to Hilbert space", "author": ["Nicholas Young"], "venue": "Cambridge university press,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}], "referenceMentions": [{"referenceID": 11, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 19, "context": "[14, 29, 21, 24, 23]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "The learning capacity is controlled using over-specified networks (networks that are larger than necessary in order to model the problem), followed by various forms of regularization techniques such as Dropout ([11]).", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "The first is that the ConvNet architecture has not changed much since its early introduction in the 1980s ([15]) \u2013 there were some attempts to create other types of deep-layered architectures (cf.", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "[19, 3, 18]), but these are not commonly used compared to ConvNets.", "startOffset": 0, "endOffset": 11}, {"referenceID": 2, "context": "[19, 3, 18]), but these are not commonly used compared to ConvNets.", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[19, 3, 18]), but these are not commonly used compared to ConvNets.", "startOffset": 0, "endOffset": 11}, {"referenceID": 7, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 21, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 17, "context": "[10, 2, 25]), it has since been observed that these schemes have little to no advantage over carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 3, "context": "These machine learning methods were well suited for \u201cflat\u201d architectures, and while attempts to apply them to deep layered architectures have been made ([4]), they did not keep up with the performance levels of the layered ConvNet architecture.", "startOffset": 153, "endOffset": 156}, {"referenceID": 13, "context": "The second, as special cases, plays the role of ReLU activation ([17]) and max pooling in ConvNets, but in addition, has capabilities that make SimNets much more than ConvNets.", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "In a set of limited experiments on CIFAR-10 dataset ([13]) using a small number of layers, we achieved better or comparable performance to state of the art ConvNets with the same number of layers, and the specialized network studied in [5], using 1/9 and 1/5, respectively, of the number of learned parameters.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "In a set of limited experiments on CIFAR-10 dataset ([13]) using a small number of layers, we achieved better or comparable performance to state of the art ConvNets with the same number of layers, and the specialized network studied in [5], using 1/9 and 1/5, respectively, of the number of learned parameters.", "startOffset": 236, "endOffset": 239}, {"referenceID": 13, "context": "The SimNet architecture consists of two operators \u2013 a \u201csimilarity\u201d operator that generalizes the inner-product operator found in ConvNets, and a soft max-average-min operator called MEX that replaces the ConvNet ReLU activation ([17]) and max/average pooling layers, and allows additional capabilities as will be described below.", "startOffset": 229, "endOffset": 233}, {"referenceID": 6, "context": "There were other attempts to generalize maxout, notably the recently proposed Lp unit [9], which is defined by ( 1 n \u2211n l=1|zl x + bl| )1/p .", "startOffset": 86, "endOffset": 89}, {"referenceID": 16, "context": "As shown in [20], Klin and Klp are kernels on R (note that for p > 2, the expression above for Klp is not a kernel).", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "This setting was studied in [27] in the context of binary (2class) classification.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "The extension to multiclass ([6]) is straightforward.", "startOffset": 29, "endOffset": 32}, {"referenceID": 19, "context": "This approach follows the line of the \u201cglobal average pooling\u201d structure recently suggested in the context of ConvNets, which has been shown to outperform the traditional \u201cdense classification\u201d paradigm ([16, 23]).", "startOffset": 204, "endOffset": 212}, {"referenceID": 7, "context": "[10, 2, 25]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[10, 2, 25]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 21, "context": "[10, 2, 25]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "Over time, however, these were taken over by carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 132, "endOffset": 144}, {"referenceID": 17, "context": "Over time, however, these were taken over by carefully selected random initializations that do not use data at all (see for example [14, 29, 21]).", "startOffset": 132, "endOffset": 144}, {"referenceID": 8, "context": "The enhanced susceptibility to overfitting has led to various regularization techniques and heuristics (Dropout ([11]) being the most", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "This observation suggests estimating the parameters (shapes \u03b2l, scales \u03b1l,i and means \u03bcl,i) of the Generalized Gaussian mixture using unlabeled input patches (via standard statistical estimation methods, such as that presented in [1]), and initializing the similarity layer accordingly.", "startOffset": 230, "endOffset": 233}, {"referenceID": 10, "context": "For the experiments reported here, we used the CIFAR-10 dataset ([13]), which consists of 60, 000 color images (50, 000 for training and 10, 000 for testing) of size 32 \u00d7 32 partitioned into 10 classes, with 6, 000 images per class (5, 000 for training, 1, 000 for testing).", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "(a) Patch labeling SimNet (b) Comparable ConvNet (c) Comparable \u201csingle-layer\u201d network studied in [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 18, "context": "cludes momentum and acceleration ([22]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "([5]), which has the same depth as the evaluated SimNet, and whose performance on CIFAR-10 is one of the best reported for networks of such depth (absolute state of the art in 2011).", "startOffset": 1, "endOffset": 4}, {"referenceID": 4, "context": "In [5], a number of unsupervised learning methods were devised for \u201ccoding\u201d the input image.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "The ConvNet was implemented using Caffe toolbox ([12]), with random initialization and SGD training.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "For the SimNet we also added patch \u201cwhitening\u201d, in accordance with the suggestion of [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "The positive effect of whitening for the SimNet (which has l1/l2 similarities) was verified experimentally, whereas for the ConvNet, we observed that whitening does not have a positive effect (complying with the observations of [5]).", "startOffset": 228, "endOffset": 231}, {"referenceID": 4, "context": "([5]) is of interest on several accounts.", "startOffset": 1, "endOffset": 4}, {"referenceID": 4, "context": "In [5], the network \u201ctemplates\u201d (i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "More specifically, we used SGD to jointly modify the network templates and SVM coefficients produced by [5], in an attempt to reach higher accuracy levels than those reported by the authors.", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "In [5] results are reported for up to 1, 600 templates.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Implementing deep SimNets with state of the art optimization techniques (including GPU acceleration) is an ongoing effort, but we were able to implement a basic SimNet and conduct benchmarks comparing it against two networks of the same depth \u2013 an analogous ConvNet and the \u201csinglelayer\u201d network of [5].", "startOffset": 299, "endOffset": 302}, {"referenceID": 4, "context": "The results demonstrate that a SimNet can achieve comparable and/or better accuracy, while requiring a significantly smaller network (in terms of the number of learned parameters) \u2013 around 1/9 the size of the ConvNet and 1/5 the size of the network in [5].", "startOffset": 252, "endOffset": 255}], "year": 2014, "abstractText": "We present a deep layered architecture that generalizes classical convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators, one being a similarity function whose family contains the convolution operator used in ConvNets, and the other is a new soft max-min-mean operator called MEX that realizes classical operators like ReLU and max pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. Three interesting properties emerge from the architecture: (i) the basic input to hidden layer to output machinery contains as special cases kernel machines with the Exponential and Generalized Gaussian kernels, the output units being \u201dneurons in feature space\u201d (ii) in its general form, the basic machinery has a higher abstraction level than kernel machines, and (iii) initializing networks using unsupervised learning is natural. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are an order of magnitude smaller than comparable ConvNets.", "creator": "LaTeX with hyperref package"}}}