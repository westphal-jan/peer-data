{"id": "1410.1090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2014", "title": "Explain Images with Multimodal Recurrent Neural Networks", "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.", "histories": [["v1", "Sat, 4 Oct 2014 20:24:34 GMT  (346kb,D)", "http://arxiv.org/abs/1410.1090v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["junhua mao", "wei xu", "yi yang", "jiang wang", "alan l yuille"], "accepted": false, "id": "1410.1090"}, "pdf": {"name": "1410.1090.pdf", "metadata": {"source": "CRF", "title": "Explain Images with Multimodal Recurrent Neural Networks", "authors": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L. Yuille"], "emails": ["mjhustc@ucla.edu,", "wei.xu@baidu.com,", "yangyi05@baidu.com,", "wangjiang03@baidu.com,", "yuille@stat.ucla.edu"], "sections": [{"heading": "1 Introduction", "text": "Obtaining sentence descriptions for images becomes an important task and has many applications, such as early childhood education, image restoration and navigation for the blind. Thanks to the rapid development of computer vision and natural language processing technologies, recent work has made significant progress in this task (see a brief review in Section 2). Many of these works treat it as a retrieval task, extracting features for both sentences and images and assigning them to the same semantic embedding space. These methods address the tasks of retrieving sentences in the face of query images or retrieving images in the face of query sets. However, they can only label the query image with sentence annotations of the images that already exist in the data sets, so the ability to describe new images that contain hitherto invisible combinations of objects and scenarios is lacking. In this work, we propose a multimodal retrieval model that reactivates neural networks (NRN)."}, {"heading": "2 Related Work", "text": "In recent years, the structure of deep neural networks has evolved rapidly in both computer vision and natural language. Krizhevsky et. al. [17] proposed a deep 8-layer Convolutionary Neural Network (referred to as AlexNet) for image classification tasks, far surpassing previous methods for computer vision. Recently, Girshick et. al. [7] proposed an object recognition system based on AlexNet. For natural language, the recursive neural network shows performance in many tasks, such as speech recognition and word embedding, learning ability [22, 23, 24]. Image-sentence recovery. Many works treat images as a retrieval task and formulate the problem as ranking or embedding learning problems."}, {"heading": "3 Model Architecture", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Simple recurrent neural network", "text": "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman Network [4], which is widely used for many natural language processing tasks such as speech recognition [22, 23]. Its architecture is shown in Figure 2 (a). It has three types of levels in each timeframe: the input word layer w, the recurring level r, and the output layer y. the activation of input, repetition, and output layers at time t is designated as w (t), r (t), and y (t) in each timeframe. w (t) is the uniform representation of the current word. This representation is binary and has the same dimension of vocabulary size with only one non-zero element. y (t) can be calculated as follows: x (t) = [w (t) r (t \u2212 1); r (t) = f1 (t); propa (V \u00b7 r (t) (t); the differentiation of time (1) and weighting (1)."}, {"heading": "3.2 Our m-RNN model", "text": "This year it is more than ever before."}, {"heading": "4 Training the m-RNN", "text": "For the formation of our m-RNN model, we use a cost function based on the perplexity of the sentences in the training set on the basis of their corresponding images. Perplexity for a word sequence (i.e. a sentence) w1: L is calculated as follows: log2 PPL (w1: L | I) = \u2212 1L L L L \u2211 n = 1 log2 P (wn | w1: n \u2212 1, I) (5), where L is the length of the word sequences, PPL (w1: L | I) represents the perplexity of the sentence w1: L in view of image I. P (wn | w1: n \u2212 1, I) is the probability of generating the word wn in view of I and previous words w1: n \u2212 1. It corresponds to the characteristic vector of the SoftMax layer of our model. The cost function of our model is the average logic probability of the words given in their context and the corresponding images w1: n 1. \u2212 It corresponds to the characteristic layer of our SoftMax model."}, {"heading": "5 Learning of Sentence and Image Features", "text": "The architecture of our model allows to reverse propagate the gradients of the loss function to both the part of speech modeling (i.e. the word embedding layers and the recurring layer) and the image part (e.g. AlexNet [17]). For the part of speech modeling, as mentioned above, we randomly initialize the voice modeling layers and learn their parameters. For the image part, we connect the seventh layer of a pre-trained Convolutionary Neural Network [17, 3] (AlexNet). The same features extracted from the seventh layer of AlexNet (also referred to as deaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30]. Recent multimodal retrieval work [15] showed that the use of the RCNN object recognition framework [7] in combination with the deaf features significantly improves performance."}, {"heading": "6 Sentence Generation, Image and Sentence Retrieval", "text": "We can use the trained m-RNN model for three tasks: 1) Sentence generation; 2) Sentence generation (retrieving the most relevant sentences for the given image); 3) Image recovery (retrieving the most relevant images for the given sentence); The process of sentence generation is simple. If we start with the beginning mark \"# # START # #\" or any number of reference words (e.g. we can enter the first K-words into the reference set of the model and then start generating new words), our model can calculate the probability distribution of the next word: P (w | w1: n \u2212 1, I). Then we can select the next word from this probability distribution. In practice, we find that selecting the word with the maximum probability works slightly better than scanning. After that, we enter the selected word into the model and continue the process until the model gives the final result \"# END #.\""}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Datasets", "text": "We test our method on three benchmark sets of annotations at the set level: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].IAPR TC-12 Benchmark. This set consists of approximately 20,000 images taken from locations around the world, including images of various sports and actions, people, animals, cities, landscapes, etc. For each image, they provide at least one sentence comment. On average, there are about 1.7 sentence comments for an image. We take over the publicly available separation of training and test sets like previous work [9, 16]. There are 17,665 images for training and 1962 images for testing. Flickr8K Benchmark. This set consists of 8,000 images extracted from Flickr. For each image, it provides five sets of annotations. The grammar of annotations for this set is simpler than that for the IAPR TC-12 datasets."}, {"heading": "7.2 Evaluation metrics", "text": "Sentence Generation. Following previous work, we take sentence perplexity and BLEU scores (i.e. B-1, B-2 and B-3) as a benchmark. BLEU scores were originally designed for automatic machine translation, where they evaluate the quality of a translated sentence based on multiple reference sentences. We can treat the sentence generation task as the \"translation\" of the image into sentences. BLEU remains the default assessment method for sentence generation methods for images, although it has drawbacks. For some images, the reference sentences may not contain all possible descriptions in the image, and BLEU may punish some correctly generated sentences. To make a fair comparison, we take the same sentence generation steps and experiment settings as [16], and generate as many words as there are in the reference sentences to calculate BLEU descriptions."}, {"heading": "7.3 Results on IAPR TC-12", "text": "The results of the sentence generation task are presented in Table 1. BACK-OFF GT2 and GT3 are n-gram methods with Katz backoff and Good-Turing discounting [2, 16]. Our-RNN base serves as the base method for our m-RNN model. It has the same architecture with m-RNN except that we will not enter the image characteristics into the network. To perform a fair comparison, we followed the same experimental settings of [16], include the context length to calculate the BLEU values and perplexity. These two evaluation parameters do not necessarily correlate with each other for the following reasons. As mentioned in Section 4, perplexity is calculated according to the conditional probability of the word in a sentence given all its previous reference words. Therefore, a strong language model that successfully captures the distribution of words in sentences is."}, {"heading": "7.4 Results on flickr8K", "text": "This dataset has been widely used as a benchmark dataset for retrieving images and sets. R @ K and Med r of different methods are presented in Table 3. Our model greatly exceeds modern methods (i.e. Socher-Decaf, DeViSE-Decaf, DeepFE-Decaf) in using the same image characteristics (i.e. Decaf characteristics). We also list the performance of methods using more complex characteristics in Table 3. \"-avg-rcnn\" refers to methods with features of average CNN activation of all objects above a detection trust threshold. DeepFE-rcnn [15] uses a fragment mapping strategy to better exploit the results of object detection. Results show that using these characteristics will improve performance. Even without the help of object detection methods, our method performs better than these methods in most evaluation metrics. We will develop our framework using better image characteristics."}, {"heading": "7.5 Results on flickr30K", "text": "This data set is a new data set, and so far there are only a few methods that report their retrieval results on it. First, we show the R @ K evaluation metric in Table 4. Our method exceeds the state of the art in most evaluation metrics. The results of sentence generation with a comparison of our RNN baseline are shown in Table 5."}, {"heading": "8 Conclusion", "text": "We propose a state-of-the-art multimodal Recurrent Neural Network (m-RNN) framework that works in three areas: record generation, record retrieval for a given query image, and image retrieval for a given query set. Our m-RNN can be expanded to include more complex image functions (e.g. object recognition functions) and more complex language models."}], "references": [{"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. De Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "JMLR, 3:1107\u20131135", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A survey of smoothing techniques for me models", "author": ["S.F. Chen", "R. Rosenfeld"], "venue": "TSAP, 8(1):37\u201350", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In ECCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "et al", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Devise: A deep visual-semantic embedding model. In NIPS, pages 2121\u20132129", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The iapr tc-12 benchmark: A new evaluation resource for visual information systems", "author": ["M. Grubinger", "P. Clough", "H. M\u00fcller", "T. Deselaers"], "venue": "International Workshop OntoImage, pages 13\u201323", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiple instance metric learning from automatically labeled bags of faces", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "ECCV, pages 634\u2013647", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "From image annotation to image description", "author": ["A. Gupta", "P. Mannem"], "venue": "ICONIP", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Choosing linguistics over vision to describe images", "author": ["A. Gupta", "Y. Verma", "C. Jawahar"], "venue": "AAAI", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. JAIR, 47:853\u2013899", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "ICCV, pages 2407\u20132414", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "arXiv:1406.5679", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Zemel", "R. Salakhutdinov"], "venue": "ICML", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["C.-Y. Lin", "F.J. Och"], "venue": "ACL, page 605", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, pages 1045\u20131048", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "ICASSP, pages 5528\u20135531", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "EACL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "ICML, pages 641\u2013648. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL, pages 311\u2013318", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL-HLT workshop 2010, pages 139\u2013147", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1988}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q. Le", "C. Manning", "A. Ng"], "venue": "TACL", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, pages 2222\u20132230", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13]).", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "The image part contains a deep Convulutional Neural Network (CNN) [17] which extracts image features.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "In the experiments, we validate our model on three benchmark datasets: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].", "startOffset": 82, "endOffset": 85}, {"referenceID": 26, "context": "In the experiments, we validate our model on three benchmark datasets: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "al [17] proposed a deep convolutional neural networks with 8 layers (denoted as AlexNet) for image classification tasks and outperformed previous methods by a large margin.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "al [7] proposed a object detection framework based on AlexNet.", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "For natural language, the Recurrent Neural Network shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning [22, 23, 24].", "startOffset": 160, "endOffset": 172}, {"referenceID": 21, "context": "For natural language, the Recurrent Neural Network shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning [22, 23, 24].", "startOffset": 160, "endOffset": 172}, {"referenceID": 22, "context": "For natural language, the Recurrent Neural Network shows the state-of-the-art performance in many tasks, such as speech recognition and word embedding learning [22, 23, 24].", "startOffset": 160, "endOffset": 172}, {"referenceID": 11, "context": "Many works treat the task of describe images as a retrieval task and formulate the problem as a ranking or embedding learning problem [12, 6, 30].", "startOffset": 134, "endOffset": 145}, {"referenceID": 5, "context": "Many works treat the task of describe images as a retrieval task and formulate the problem as a ranking or embedding learning problem [12, 6, 30].", "startOffset": 134, "endOffset": 145}, {"referenceID": 28, "context": "Many works treat the task of describe images as a retrieval task and formulate the problem as a ranking or embedding learning problem [12, 6, 30].", "startOffset": 134, "endOffset": 145}, {"referenceID": 28, "context": "al [30] uses dependency tree Recursive Neural network to extract sentence features) as well as the image features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "al [15] showed that object level image features based on object detection results will generate better results than image features extracted at the global level.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "They parse the sentence and divide it into several parts [25, 10].", "startOffset": 57, "endOffset": 65}, {"referenceID": 9, "context": "They parse the sentence and divide it into several parts [25, 10].", "startOffset": 57, "endOffset": 65}, {"referenceID": 16, "context": "[18] uses a Conditional Random Field model and [5] uses a Markov Random Field model).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[18] uses a Conditional Random Field model and [5] uses a Markov Random Field model).", "startOffset": 47, "endOffset": 50}, {"referenceID": 29, "context": "sentences and images), using for example, Deep Boltzmann Machines [31], and topic models [1, 14].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "sentences and images), using for example, Deep Boltzmann Machines [31], and topic models [1, 14].", "startOffset": 89, "endOffset": 96}, {"referenceID": 12, "context": "sentences and images), using for example, Deep Boltzmann Machines [31], and topic models [1, 14].", "startOffset": 89, "endOffset": 96}, {"referenceID": 14, "context": "[16], which is built on a Log-BiLinear model [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[16], which is built on a Log-BiLinear model [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman network [4] that is widely used for many natural language processing tasks, such as speech recognition [22, 23].", "startOffset": 80, "endOffset": 83}, {"referenceID": 20, "context": "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman network [4] that is widely used for many natural language processing tasks, such as speech recognition [22, 23].", "startOffset": 175, "endOffset": 183}, {"referenceID": 21, "context": "We briefly introduce the simple Recurrent Neural Network (RNN) or Elman network [4] that is widely used for many natural language processing tasks, such as speech recognition [22, 23].", "startOffset": 175, "endOffset": 183}, {"referenceID": 27, "context": "Accordingly, when we do the backpropagation, we need to propagate the error through recurrent connections back in time [29].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "Secondly, the dense word embedding encodes the semantic meanings of the words [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.", "startOffset": 45, "endOffset": 60}, {"referenceID": 5, "context": "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.", "startOffset": 45, "endOffset": 60}, {"referenceID": 28, "context": "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.", "startOffset": 45, "endOffset": 60}, {"referenceID": 14, "context": "Most of the sentence-image multimodal models [15, 6, 30, 16] use pre-computed word embedding vectors as the initialization of their model.", "startOffset": 45, "endOffset": 60}, {"referenceID": 15, "context": ") as the Rectified Linear Unit (ReLU), inspired by its the recent success when training very deep structure in computer vision field [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "When backpropagation through time (BPTT) [29] is conducted for RNN with sigmoid function, the vanishing gradient problem appears since even the simplest RNN model can have a large temporal depth.", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "Previous methods [22, 23] used heuristics, such as truncated BPTT, to avoid this problem.", "startOffset": 17, "endOffset": 25}, {"referenceID": 21, "context": "Previous methods [22, 23] used heuristics, such as truncated BPTT, to avoid this problem.", "startOffset": 17, "endOffset": 25}, {"referenceID": 15, "context": "Here we connect the seventh layer of AlexNet [17] to the multimodal layer (please refer to Section 5 for more details).", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": ") is the element-wised scaled hyperbolic tangent function [19]:", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "the AlexNet [17]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "For the image part, we connect the seventh layer of a pre-trained Convolutional Neural Network [17, 3] (denoted as AlexNet).", "startOffset": 95, "endOffset": 102}, {"referenceID": 2, "context": "For the image part, we connect the seventh layer of a pre-trained Convolutional Neural Network [17, 3] (denoted as AlexNet).", "startOffset": 95, "endOffset": 102}, {"referenceID": 2, "context": "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].", "startOffset": 94, "endOffset": 97}, {"referenceID": 14, "context": "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].", "startOffset": 146, "endOffset": 161}, {"referenceID": 5, "context": "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].", "startOffset": 146, "endOffset": 161}, {"referenceID": 13, "context": "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].", "startOffset": 146, "endOffset": 161}, {"referenceID": 28, "context": "The same features extracted from the seventh layer of AlexNet (also denoted as decaf features [3]) are widely used by previous multimodal methods [16, 6, 15, 30].", "startOffset": 146, "endOffset": 161}, {"referenceID": 13, "context": "A recent multimodal retrieval work [15] showed that using the RCNN object detection framework [7] combined with the decaf features significantly improves the performance.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "A recent multimodal retrieval work [15] showed that using the RCNN object detection framework [7] combined with the decaf features significantly improves the performance.", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "In the experiments, we show that our method performs much better than [15] when the same image features are used, and is better than or comparable to their results even when they use more sophisticated features based on object detection.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "We test our method on three benchmark datasets with sentence level annotations: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].", "startOffset": 91, "endOffset": 94}, {"referenceID": 26, "context": "We test our method on three benchmark datasets with sentence level annotations: IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "of training and testing set as previous works [9, 16].", "startOffset": 46, "endOffset": 53}, {"referenceID": 14, "context": "of training and testing set as previous works [9, 16].", "startOffset": 46, "endOffset": 53}, {"referenceID": 13, "context": "We follow the previous work [15] which used 1,000 images for testing.", "startOffset": 28, "endOffset": 32}, {"referenceID": 25, "context": "B-1, B-2, and B-3) [27, 20] as the evaluation metrics.", "startOffset": 19, "endOffset": 27}, {"referenceID": 18, "context": "B-1, B-2, and B-3) [27, 20] as the evaluation metrics.", "startOffset": 19, "endOffset": 27}, {"referenceID": 14, "context": "To conduct a fair comparison, we adopt the same sentence generation steps and experiment settings as [16], and generate as many words as there are in the reference sentences to calculate BLEU.", "startOffset": 101, "endOffset": 105}, {"referenceID": 28, "context": "Sentence Retrieval and Image Retrieval For Flickr8K and Flickr30K datasets, we adopted the same evaluation metrics as previous works [30, 6, 15] for both the tasks of sentences retrieval and image retrieval.", "startOffset": 133, "endOffset": 144}, {"referenceID": 5, "context": "Sentence Retrieval and Image Retrieval For Flickr8K and Flickr30K datasets, we adopted the same evaluation metrics as previous works [30, 6, 15] for both the tasks of sentences retrieval and image retrieval.", "startOffset": 133, "endOffset": 144}, {"referenceID": 13, "context": "Sentence Retrieval and Image Retrieval For Flickr8K and Flickr30K datasets, we adopted the same evaluation metrics as previous works [30, 6, 15] for both the tasks of sentences retrieval and image retrieval.", "startOffset": 133, "endOffset": 144}, {"referenceID": 14, "context": "For IAPR TC-12 datasets, we adopt exactly the same evaluation metrics as [16] and plot the mean number of matches of the retrieved groundtruth sentences or images with respect to the percentage of the retrieved sentences or images for the testing set.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "For sentences retrieval task, [16] used a shortlist of 100 images which are the nearest neighbors of the query image in the feature space.", "startOffset": 30, "endOffset": 34}, {"referenceID": 1, "context": "BACK-OFF GT2 and GT3 are n-grams methods with Katz backoff and Good-Turing discounting [2, 16].", "startOffset": 87, "endOffset": 94}, {"referenceID": 14, "context": "BACK-OFF GT2 and GT3 are n-grams methods with Katz backoff and Good-Turing discounting [2, 16].", "startOffset": 87, "endOffset": 94}, {"referenceID": 14, "context": "To conduct a fair comparison, we followed the same experimental settings of [16], include the context length to calculate the BLEU scores and perplexity.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "059 LBL [26] 20.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "068 MLBL-B-DeCAF [16] 24.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "098 MLBL-F-DeCAF [16] 21.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "[11] / 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "01 Gupta & Mannem [10] / 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "For sentence retrieval task, we used a shortlist of 100 images as the three comparing methods shown in [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "[16] further improve their results after the publication.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "DeepFE-rcnn [15] uses a fragment mapping strategy to better exploit the object detection results.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "0 500 Socher-decaf [30] 4.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "0 29 Socher-avg-rcnn [30] 6.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "7 25 DeViSE-avg-rcnn [6] 4.", "startOffset": 21, "endOffset": 24}, {"referenceID": 13, "context": "6 29 DeepFE-decaf [15] 5.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "5 32 DeepFE-rcnn [15] 12.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "0 500 DeViSE-avg-rcnn [6] 4.", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": "6 29 DeepFE-rcnn [15] 16.", "startOffset": 17, "endOffset": 21}], "year": 2014, "abstractText": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12 [8], Flickr 8K [28], and Flickr 30K [13]). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.", "creator": "LaTeX with hyperref package"}}}