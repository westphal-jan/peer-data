{"id": "1410.2045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2014", "title": "Supervised learning Methods for Bangla Web Document Categorization", "abstract": "This paper explores the use of machine learning approaches, or more specifically, four supervised learning Methods, namely Decision Tree(C 4.5), K-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM) for categorization of Bangla web documents. This is a task of automatically sorting a set of documents into categories from a predefined set. Whereas a wide range of methods have been applied to English text categorization, relatively few studies have been conducted on Bangla language text categorization. Hence, we attempt to analyze the efficiency of those four methods for categorization of Bangla documents. In order to validate, Bangla corpus from various websites has been developed and used as examples for the experiment. For Bangla, empirical results support that all four methods produce satisfactory performance with SVM attaining good result in terms of high dimensional and relatively noisy document feature vectors.", "histories": [["v1", "Wed, 8 Oct 2014 10:01:47 GMT  (228kb)", "http://arxiv.org/abs/1410.2045v1", "13 pages, International Journal of Artificial Intelligence &amp; Applications (IJAIA), Vol. 5, No. 5, September 2014"]], "COMMENTS": "13 pages, International Journal of Artificial Intelligence &amp; Applications (IJAIA), Vol. 5, No. 5, September 2014", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ashis kumar mandal", "rikta sen"], "accepted": false, "id": "1410.2045"}, "pdf": {"name": "1410.2045.pdf", "metadata": {"source": "CRF", "title": "SUPERVISED LEARNING METHODS FOR BANGLA WEB DOCUMENT CATEGORIZATION", "authors": ["Ashis Kumar Mandal", "Rikta Sen"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijaia.2014.5508 93This paper examines the use of machine learning approaches, or more precisely four supervised learning methods, namely Decision Tree (C 4.5), K-Nearest Neighbour (KNN), Na\u00efve Bays (NB) and Support Vector Machine (SVM) to categorize Bangla web documents. This is an automatic sorting of a set of documents into categories from a predefined set. While a wide range of methods have been used for English text categorization, relatively few studies on text categorization have been conducted in Bangla. Therefore, we are trying to analyze the efficiency of these four methods for categorizing Bangla documents. For validation, Bangla corpus has been developed by various websites and used as examples of the experiment. For Bangla, empirical results support that all four methods provide satisfactory performance, with a good result in terms of highly compact and relatively compact documents."}, {"heading": "1. INTRODUCTION", "text": "Text categorization is an active field of research in text mining, where documents are classified with supervised, unsupervised, or semi-supervised knowledge. Traditionally, this task has been solved manually, but such manual classification has been expensive in scale and labor-intensive. Therefore, researchers have investigated the use of machine learning approach to automatically classify text documents [1]. Under various machine learning methods in document categorization, most popular learning processes are monitored, where the underlying input-output relationship is learned through a small number of training data, and then output values for invisible input points are predicted [2]. Different types of verified learning methods, such as association rules [3], Neural Network [1], K-Nearest Neighbour [4], Decision Tree [5], Na\u00efve Bays [6], Support Vector Machine [7], and text gramming [8] have been used for."}, {"heading": "2. RELATED WORKS", "text": "In addition to the English language, there are many studies conducted in European languages, such as French, German, Spanish [14] and in Asian languages, such as Arabic, Chinese and Japanese [15. For some southern Indian languages Na\u00efve Bayes, neural networks were applied to news articles to automatically categorize predefined classes [17, 18].NB is the most commonly used approach for text classification. It works by calculating the probability that document D belongs to class C. It is easy to implement and calculate. In [19], authors use simple NB approach for the automatic classification of websites based on content from homepages. They have about 80% accuracy for text classification. In [20], Jiang et al. Introduced locally weighted NB text classifiers and showed that this version is significantly better than the original NB text classifiers in terms of precision."}, {"heading": "3. CLASSIFICATION ALGORITHMS", "text": "In this section we briefly describe C 4.5, KNN, NB and SVM classification methods used in our study."}, {"heading": "3.1 Na\u00efve Bays (NB)", "text": "A classifier for naive bays (NB) is a simple probabilistic classifier based on the application of Bayes's theorem [6]. Depending on the exact nature of the probability model, this classifier can be trained very efficiently in a supervised learning environment.When classifying text documents using the Bayes classifier, it is assumed that the document belongs to a class. Then, the probabilities of document characteristics w in all categories C are calculated using the formula (1). |,.... = log + economic protocol (1) The probability is calculated using the formula (2) = | | | (2), where | | is the number of texts belonging to the class; m is the number of all classes. The probability is calculated using the following formula (3) = |! | | (3), where | | is the number of texts belonging to class C; the number of documents in which a certain characteristic is found."}, {"heading": "3.2. K-Nearest Neighbours (KNN)", "text": "KNN is a statistical sample reorganization algorithm that has been extensively studied for text categorization applications [31]. It is a method for classifying objects based on the closest training examples in the Feature Space. The summary of the algorithm is as follows: If several of the K closest neighbors of x are found among all training documents, and the candidates of the category are evaluated using Category K. The similarity of x and each neighbor document is the value of the category of the neighboring document. If several of the K closest neighbors belong to the same category, then the sum of the evaluation of this category is the similarity of the category with respect to the test document x. By sorting the results of the candidate categories, the system assigns to the test document the candidate category with the highest score x. The decision rule of KNN can be written as follows: # $= argmax) * +, - $, - $, - $, 1 1 1 1 1 1 1 1 1 3, 66 test document is assigned to the test document (x), where the test document is assigned to the test category x (x)."}, {"heading": "3.3. Decision tree classifier", "text": "A decision tree consists of nodes and branches that connect the nodes. Nodes at the bottom of the tree are called leaves and indicate terminal nodes. The topmost node in the tree, called the root, contains all training examples that are to be divided into classes. Summary of the algorithm as follows: Using a training example S, find the most discriminating (significant) characteristics. In the next step, divide the entire set S that is located at the root of the tree into several subsets using the selected characteristic. Then, recursively find the most significant characteristic for each subset generated in the above step and then divide it top down into subsets. If each subset contains examples that belong to only one class (a leaf node), then stop; otherwise, continue this step. Your robustness in the face of loud data and your ability to learn disjunctive expressions seem appropriate for classifying documents [32]."}, {"heading": "3.4. Support Vector Machines (SVM)", "text": "SVM was introduced by Cortes and Vapnik [7] as a class of supervised machine learning techniques, which is actually a binary classifier. SVM is based on the principle of structural risk minimization. In linear classification, SVM creates a hyperplane that separates the data into two groups with the maximum margin. A hyperplane with the maximum margin has the distances from the hyperplane to points when the two sides are equal. Mathematically, SVMs learn the drawing function) () (bwxsignxf + =, where there is no weighted vector in nR. SVMs find the hyperplane bwxy + = by dividing the space nRin two half spaces with the maximum margin. These linear SVMs can be generalized for nonlinear and multi-class problems. The former occurs by mapping data to another space H and executing the linear SVM algorithm over this new space, while the latter decomposes the multi-class problem."}, {"heading": "4. METHODOLOGY", "text": "Before applying document categorization techniques to the Bangla language using the above-mentioned classifiers, it is inevitable to prepare suitable data sets for testing and training. At the same time, as with English text classification, pre-processing Bangla documents and extracting feature sets before training and building a model for successful document categorization are also required. Figure 1 illustrates the overall system of the Bangla text classification process used in this project. In the following subsections, we have described the model."}, {"heading": "4.1 Proposed Data Sets", "text": "Although many corpora (document collections) are available online in different languages for research purposes, they are rare in relation to Bangla language. Therefore, we have decided to create our own corpus from different Bangla websites and have called it BD corpus, as shown in Table 1. Each text document belongs to five categories: (business), (sports), (health), (technology), (education). The corpus comprises 1000 documents with a total of 22,218 words. It is also noted that without stopwords, digits, punctuation and steaming word size is 18,190, all of which come from Bangla news sources such as prothom-alo.com, online _ dhaka.com, bdnews24.com, dailykalerkantha.com, bbc.co.uk / Bengali, ittefaque.com, etc., but all documents are thematically linked."}, {"heading": "4.2 Pre-processing", "text": "It is expected that the proper function extraction will extract the relevant information from the input data, and due to the high dimensionality of the feature sets, feature extraction can be performed to reduce the dimensionality of the feature space and improve efficiency [33]. However, before extracting features from the documents, preprocessing is required. In the preprocessing phase, we present each original text document as a \"Bag of words.\" Then, the following operations are performed on each document: Tokenization: Tokenization is the process of breaking the sentences as well as the text file into word separated by white space or tab or new line. Outcome of this tokenization phase is a set of word delimited by white space."}, {"heading": "4.3 Feature Extraction", "text": "The collection of words that remain in the document after all these steps is considered a formal representation of the document, and we refer to the words in this collection as \"terms.\" This is our final text corpus. Various types of statistical approaches can be used to extract features from this text corpus. We use normalized (term frequency-inverse document frequency) TFIDF weighting with length normalization to extract features from the document, as this method works better than many other methods [34]. A combination of term frequency and inverse document frequency called TFIDF is commonly used to numerically represent the term weight. Weight for a term i in relation to TF-IDF is given by =; < =? @ A; BC DDE; < = \u00d7? @ A; BC DDDFC (5), where N is the total number of documents and ni is the frequency of the document i."}, {"heading": "5. EXPERIMENT AND RESULT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Training and Testing Setup", "text": "In our experiment, after feature extraction and normalization, we used the aforementioned BD corpus as a training dataset to obtain models, and then, using these models, we predict test datasets. Algorithms experimented in this study are performed using Rapid Miner [35] and Java language. K-fold strategy (with k = 10) was applied to determine the number of training and test examples. Nine folds, i.e. 900 examples were used as a training set to create the classifier, and the remaining 100 examples were used to test the classifier for accuracy. Table 2 describes the K = 10-fold strategy. KNN experiments use the simple Euclidean distance as a measure of similarity. On each sample set, which contains a different number of documents, different values of K are tested from 1 to 10 and the best results are reported. SVM experiments use the K = 10-fold strategy."}, {"heading": "5.2. Performance Measures", "text": "Many assessment yardsticks are used in text categorization. Our experiments use the most commonly used performance yardsticks, including recall, precision, and F-yardstick (F1). Faced with a test set A that contains documents marked with category c and a prediction set B marked by the classifier with category c, the recall (R) and precision measurements (P) are defined by G = | H-J | | (6) or = H-J | (7), with the standard | \u00b7 | denoting the size of the document set [38]. It is a common practice to combine recall and precision measurements in any way, so that classifiers can be compared with a single assessment. Our experiments used the F1 rating and it is defined as: K = LM L \"M (8) tests were performed on four classifiers, NB, NB, SVN, DN, and F."}, {"heading": "5.3. Number of Training Examples and Accuracy", "text": "In this experiment, we are particularly interested in the learning ability of classifiers in different training groups. We train and test classifiers in 5 steps, increasing the input by 30 documents each. Figure 3 shows the number of training examples versus the accuracy in terms of the average F measurement. The accuracy of all classifiers was not very promising when only 30 documents were provided as training data. However, the accuracy increases each time classifiers are supplied with additional documents. In general, Figure 2 suggests that all four classifiers are reasonably good at learning as long as we have a sufficient number of text documents.Figure 3. Number of training documents versus accuracy"}, {"heading": "5.4. Training time", "text": "Short training times are always required, but the familiarization phase for all classifiers usually takes time due to the high dimensionality of the test data sets [39]. Figure 4 shows the average training time in seconds for the four text classifiers. This figure shows that DT (C4.5) requires a significant amount of training time compared to the other three classifiers. DT (C4.5) is not scalable in the high-dimensional data set, especially when classifying large text documents. The classifier with the smallest training time is SVM with about 9 seconds, while KNN and NB have relatively similar training times at 12.24 seconds and 14.15 seconds, respectively."}, {"heading": "6. CONCLUSIONS AND FUTURE DIRECTION", "text": "Developing efficient Bangla text classification is an indispensable topic, as Bangla is a rich language with a lot of diversity, and Bangla-based electronic documents have emerged very quickly both online and offline. In this article, we have re-examined four state-of-the-art supervised machine learning algorithms, including DT (C4,5), NB, KNN and SVM, and empirically compared their classification performance on Bangla text documents. In this regard, we have developed our own corpora, namely BD corpora, and implemented a tool for feature extraction and selection. The main results of our experiments are summarised as follows: \u2022 On small and well-organized training sets, KNN and NB are more capable than SVM and DT (C4.5) in categorizing documents. However, reducing SVM data is more important than those of other classifiers in text categorization. \u2022 Comparing four classifiers in terms of the same training time means that SVM does not need all classifiers to be used."}], "references": [{"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM computing surveys (CSUR), vol. 34, pp. 1-47, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Machine learning in non-stationary environments: Introduction to covariate shift adaptation", "author": ["M. Sugiyama", "M. Kawanabe"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Visual text mining using association rules", "author": ["A. Lopes", "R. Pinho", "F.V. Paulovich", "R. Minghim"], "venue": "Computers & Graphics, vol. 31, pp. 316-326, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A k-nearest neighbor classification rule based on Dempster-Shafer theory", "author": ["T. Denoeux"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on, vol. 25, pp. 804-813, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine learning, vol. 1, pp. 81-106, 1986.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1986}, {"title": "Feature selection for text classification with Na\u00efve Bayes", "author": ["J. Chen", "H. Huang", "S. Tian", "Y. Qu"], "venue": "Expert Systems with Applications, vol. 36, pp. 5432-5435, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, pp. 273-297, 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics, vol. 18, pp. 467-479, 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Research on Bangla language processing in Bangladesh: progress and challenges", "author": ["M. Islam"], "venue": "8th ILDC, Dhaka, Bangladesh (June 2009), 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Text-based intelligent systems: Current research and practice in information extraction and retrieval", "author": ["P.S. Jacobs"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Developing an efficient search suggestion generator, ignoring spelling error for high speed data retrieval using Double Metaphone Algorithm", "author": ["A.K. Mandal", "M.D. Hossain", "M. Nadim"], "venue": "Computer and Information Technology (ICCIT), 2010 13th International Conference on, 2010, pp. 317-320.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of n-gram based text categorization for bangla in a newspaper corpus", "author": ["M. Mansur"], "venue": "BRAC University, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Text Classification Using Machine Learning Methods-A Survey", "author": ["B. Agarwal", "N. Mittal"], "venue": "Proceedings of the Second International Conference on Soft Computing for Problem Solving (SocProS 2012), December 28-30, 2012, 2014, pp. 701-709.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Flexible text classification for financial applications: the FACILE system", "author": ["F. Ciravegna", "L. Gilardoni", "A. Lavelli", "S. Mazza", "W.J. Black", "M. Ferraro"], "venue": "ECAI, 2000, pp. 696-700.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic seed word selection for unsupervised sentiment classification of Chinese text", "author": ["T. Zagibalov", "J. Carroll"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics- Volume 1, 2008, pp. 1073-1080.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Support vector machines based Arabic language text classification system: feature selection comparative study", "author": ["A. Moh\u2019d Mesleh"], "venue": "Advances in Computer and Information Sciences and Engineering, ed: Springer, 2008, pp. 11-16.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic classification of Tamil documents using vector space model and artificial neural network", "author": ["K. Rajan", "V. Ramalingam", "M. Ganesan", "S. Palanivel", "B. Palaniappan"], "venue": "Expert Systems with Applications, vol. 36, pp. 10914-10918, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithm for Punjabi Text Classification", "author": ["N. a. V. Gupta"], "venue": "International Journal of Computer Applications, vol. 37, pp. 30-35, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated classification of web sites using Naive Bayesian algorithm", "author": ["A.S. Patil", "B. Pawar"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists, 2012, pp. 14-16.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Naive Bayes text classifiers: a locally weighted learning approach", "author": ["L. Jiang", "Z. Cai", "H. Zhang", "D. Wang"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence, vol. 25, pp. 273-286, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Enhancing naive bayes with various smoothing methods for short text classification", "author": ["Q. Yuan", "G. Cong", "N.M. Thalmann"], "venue": "Proceedings of the 21st international conference companion on World Wide Web, 2012, pp. 645-646.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "KNN based Machine Learning Approach for Text and Document Mining", "author": ["V. Bijalwan", "V. Kumar", "P. Kumari", "J. Pascual"], "venue": "International Journal of Database Theory and Application, vol. 7, pp. 61-70, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An improved k-nearest neighbor algorithm for text categorization", "author": ["B. Li", "S. Yu", "Q. Lu"], "venue": "arXiv preprint cs/0306099, 2003.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "An improved K-nearest neighbor classification using Genetic Algorithm", "author": ["N. Suguna", "K. Thanushkodi"], "venue": "International Journal of Computer Science Issues, vol. 7, pp. 18-21, 2010. International Journal of Artificial Intelligence & Applications (IJAIA), Vol. 5, No. 5, September 2014 105", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Appropriateness in applying SVMs to text classification", "author": ["X.-L. Liu", "S. Ding", "H. Zhu", "L. Zhang"], "venue": "Comput Eng Sci, vol. 32, pp. 106-108, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Text Classifiers for Cricket Sports News", "author": ["T. Zakzouk", "H. Mathkour"], "venue": "proceedings of International Conference on Telecommunications Technology and Applications ICTTA, 2011, pp. 196-201.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Study on SVM Compared with the other Text Classification Methods", "author": ["L. Zhijie", "L. Xueqiang", "L. Kun", "S. Shuicai"], "venue": "Education Technology and Computer Science (ETCS), 2010 Second International Workshop on, 2010, pp. 219-222.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparing Neural Network Approach with N-Gram Approach for Text Categorization", "author": ["A.S. Babu", "P. Kumar"], "venue": "Int J Comput Sci Engin, vol. 2, pp. 80-83, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Decision trees for hierarchical multilabel classification", "author": ["C. Vens", "J. Struyf", "L. Schietgat", "S. D\u017eeroski", "H. Blockeel"], "venue": "Machine Learning, vol. 73, pp. 185-214, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Twitter brand sentiment analysis: A hybrid system using ngram analysis and dynamic artificial neural network", "author": ["M. Ghiassi", "J. Skinner", "D. Zimbra"], "venue": "Expert Systems with Applications: An International Journal, vol. 40, pp. 6266-6282, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, 1999, pp. 42-49.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "Classification of text documents", "author": ["Y.H. Li", "A.K. Jain"], "venue": "The Computer Journal, vol. 41, pp. 537- 546, 1998.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Extremely fast text feature extraction for classification and indexing", "author": ["G. Forman", "E. Kirshenbaum"], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management, 2008, pp. 1221-1230.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Document frequency and term specificity", "author": ["H. Joho", "M. Sanderson"], "venue": "Large Scale Semantic Access to Content (Text, Image, Video, and Sound), 2007, pp. 350-359.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Text mining with rapidminer", "author": ["G. Ertek", "D. Tapucu", "\u0130. Ar\u0131n"], "venue": "RapidMiner: Data Mining Use Cases and Business Analytics Applications, p. 241, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), vol. 2, p. 27, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, researchers have explored the use of machine learning approach to automatic classification of text documents[1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "Among various machine learning approaches in document categorization, most popular is supervised learning where underlying input-output relation is learned by small number of training data and then output values for unseen input points are predicted[2].", "startOffset": 249, "endOffset": 252}, {"referenceID": 2, "context": "Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Na\u00efve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Na\u00efve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Na\u00efve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.", "startOffset": 118, "endOffset": 121}, {"referenceID": 4, "context": "Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Na\u00efve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Na\u00efve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Na\u00efve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "Various number of supervised learning techniques ,such as association rules[3], Neural Network[1], K-Nearest Neighbour[4], Decision Tree[5], Na\u00efve Bays[6], Support Vector Machine[7], and N-grams[8], has been used for text document categorization.", "startOffset": 194, "endOffset": 197}, {"referenceID": 8, "context": "Besides, Bangla is spoken by about 245 million people of Bangladesh and two states of India, with being 7 th most spoken language[9].", "startOffset": 129, "endOffset": 132}, {"referenceID": 9, "context": "To address this type of problem, text categorization process can be used successfully, such as text categorization conducted on English language for automated text retrieval [10, 11 ].", "startOffset": 174, "endOffset": 183}, {"referenceID": 10, "context": "To address this type of problem, text categorization process can be used successfully, such as text categorization conducted on English language for automated text retrieval [10, 11 ].", "startOffset": 174, "endOffset": 183}, {"referenceID": 11, "context": "One of the works in Bangla document categorization is applying N-gram technique to categorize Bangla newspaper corpus [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "Literacy review indicates that considerable works have been done in text categorization of the English documents[1, 13].", "startOffset": 112, "endOffset": 119}, {"referenceID": 12, "context": "Literacy review indicates that considerable works have been done in text categorization of the English documents[1, 13].", "startOffset": 112, "endOffset": 119}, {"referenceID": 13, "context": "In addition to English language, there are many studies have conducted in European languages, such as French, German, Spanish [14] and in Asian languages, such as Arabic, Chinese and Japanese[15 .", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "For some Southern Indian Languages Na\u00efve Bayes , Neural networks have been applied to news articles to automatically categorized predefine classes [17, 18].", "startOffset": 147, "endOffset": 155}, {"referenceID": 17, "context": "For some Southern Indian Languages Na\u00efve Bayes , Neural networks have been applied to news articles to automatically categorized predefine classes [17, 18].", "startOffset": 147, "endOffset": 155}, {"referenceID": 18, "context": "In [19], authors use simple NB approach for the automatic classification of web sites based on content of home pages.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "In [20] , Jiang et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Besides, authors in [21] proposed various smoothing methods for enhancing Bayesian leaning and used it for short text classification.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "In paper [22], authors successfully classified text document with KNN and result shows that performance of their method is better than Na\u00efve Bays and Term-Graph .", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "In [23], propose a improved KNN model which combine constrained one pass clustering algorithm and KNN text categorization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "Similarly, in [24] ,authors proposed another improved version of KNN based on genetic algorithm.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "In [25] ,authors explained the appropriateness of applying SVMs to text classification primarily from a theoretical perspective.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "In paper [26], authors developed a", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "In [27], Zhijie et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "Authors in [28] compared Neural network approach to N-gram approach for text classification and demonstrated that classification rate of Neural networks is similar to the corresponding Ngram approach but former is five time faster than latter.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "in paper [29] consider Decision Tree for hierarchical text classification problem.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "In [30] ,authors proposed hybrid system based on N-gram analysis and dynamic artificial neural network for Twitter brand sentiment analysis.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "A Na\u00efve Bays (NB) classifier is a simple probabilistic classifier based on applying Bayes' theorem [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 30, "context": "KNN is a statistical pattern reorganization algorithm which has been studied extensively for text categorization applications [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "Decision tree is another popular supervised learning method [5].", "startOffset": 60, "endOffset": 63}, {"referenceID": 31, "context": "Their robustness to noisy data and their capabilities to learn disjunctive expressions seem suitable for document classification [32].", "startOffset": 129, "endOffset": 133}, {"referenceID": 6, "context": "SVM was introduced by Cortes and Vapnik [7] as a class of supervised machine learning techniques which is actually a binary classifier.", "startOffset": 40, "endOffset": 43}, {"referenceID": 32, "context": "It is expected that proper feature extraction will extract the relevant information from the input data, and due to the high dimensionality of feature sets, feature extraction can be performed to reduce the dimensionality of the feature space and improve the efficiency[33].", "startOffset": 269, "endOffset": 273}, {"referenceID": 33, "context": "We use normalized (term frequency\u2013inverse document frequency) TFIDF weighting with length normalization to extract the features from the document as this method performs better than many other methods [34].", "startOffset": 201, "endOffset": 205}, {"referenceID": 34, "context": "Algorithms experimented in this study are conducted with the help of Rapid Miner[35] and Java language.", "startOffset": 80, "endOffset": 84}, {"referenceID": 35, "context": "SVM experiments use LIBSVM by Chung Chang and Chih-Jen Lin [36].", "startOffset": 59, "endOffset": 63}], "year": 2014, "abstractText": "This paper explores the use of machine learning approaches, or more specifically, four supervised learning Methods, namely Decision Tree(C 4.5), K-Nearest Neighbour (KNN), Na\u00efve Bays (NB), and Support Vector Machine (SVM) for categorization of Bangla web documents. This is a task of automatically sorting a set of documents into categories from a predefined set. Whereas a wide range of methods have been applied to English text categorization, relatively few studies have been conducted on Bangla language text categorization. Hence, we attempt to analyze the efficiency of those four methods for categorization of Bangla documents. In order to validate, Bangla corpus from various websites has been developed and used as examples for the experiment. For Bangla, empirical results support that all four methods produce satisfactory performance with SVM attaining good result in terms of high dimensional and relatively noisy document feature vectors.", "creator": "PScript5.dll Version 5.2.2"}}}