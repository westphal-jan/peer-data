{"id": "1410.2386", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2014", "title": "Bayesian Robust Tensor Factorization for Incomplete Multiway Data", "abstract": "We propose a generative model for robust tensor factorization in the presence of both missing data and outliers. The objective is to explicitly infer the underlying low-CP-rank tensor capturing the global information and a sparse tensor capturing the local information (also considered as outliers), thus providing the robust predictive distribution over missing entries. The low-CP-rank tensor is modeled by multilinear interactions between multiple latent factors on which the column sparsity is enforced by a hierarchical prior, while the sparse tensor is modeled by a hierarchical view of Student-$t$ distribution that associates an individual hyperparameter with each element independently. For model learning, we develop an efficient closed-form variational inference under a fully Bayesian treatment, which can effectively prevent the overfitting problem and scales linearly with data size. In contrast to existing related works, our method can perform model selection automatically and implicitly without need of tuning parameters. More specifically, it can discover the groundtruth of CP rank and automatically adapt the sparsity inducing priors to various types of outliers. In addition, the tradeoff between the low-rank approximation and the sparse representation can be optimized in the sense of maximum model evidence. The extensive experiments and comparisons with many state-of-the-art algorithms on both synthetic and real-world datasets demonstrate the superiorities of our method from several perspectives.", "histories": [["v1", "Thu, 9 Oct 2014 08:50:31 GMT  (6742kb,D)", "http://arxiv.org/abs/1410.2386v1", null], ["v2", "Thu, 16 Apr 2015 05:36:23 GMT  (5041kb,D)", "http://arxiv.org/abs/1410.2386v2", "in IEEE Transactions on Neural Networks and Learning Systems, 2015"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["qibin zhao", "guoxu zhou", "liqing zhang", "rzej cichocki", "shun-ichi amari"], "accepted": false, "id": "1410.2386"}, "pdf": {"name": "1410.2386.pdf", "metadata": {"source": "CRF", "title": "Robust Bayesian Tensor Factorization for Incomplete Multiway Data", "authors": ["Qibin Zhao", "Guoxu Zhou", "Liqing Zhang", "Andrzej Cichocki", "Shun-ichi Amari"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Tensor Factorization, Tensor Completion, Robust Factorization, Ranking, Variable Bayesian Conclusion, Video Background Modeling"}, {"heading": "1 INTRODUCTION", "text": "This year, it has come to the point that there is only one occasion when there is a scandal."}, {"heading": "2 RELATED WORK", "text": "Our work relates in some ways to probabilistic approaches for robust PCA [38], [39] and robust matrix factorization [40], [41], [42]. In [38], the beta-Bernoulli distribution is used exclusively for outliers and the low matrix, but this results in high model complexity and slow inference. Missing values are taken into account in [39], where the number of latent components must be specified in advance. In [42], Jeffrey's predecessor is adopted for both noise and outliers, but cannot deal with missing values. PRMF [40] uses laplace distribution to model the residuals, while the rank of the underlying model should be specified in advance. A fully Bayesian treatment of PRMF [41] uses a hierarchical view of the Laplace distribution as the noise model, and applies MCMC sampling for model reference."}, {"heading": "3 PRELIMINARIES AND NOTATIONS", "text": "The order of a tensor is the number of dimensions also known as paths or modes. Vectors are denoted by lowercase letters, e.g. A. Matrices are denoted by uppercase letters, e.g. A. Highly ordered tensors (order \u2265 3) are denoted by calligraphic letters, e.g. A. A tensor of the second order is denoted by letters of the third order, e.g. (i1, i2,.., iN) th entry is denoted by Xi1i2... iN, where the indices typically range from 1 to their uppercase, e.g. in = 1, 2,."}, {"heading": "4 BAYESIAN ROBUST CP FACTORIZATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Model Specification", "text": "Let Y be an incomplete N-order tensor of size I1 \u00b7 I2 \u00b7 \u00b7 \u00b7 IN with missing entries (1). We also define an indicator tensor O whose entry Oi1i2 \u00b7 \u00b7 iN is equal to 1 if (i1, i2, \u00b7 \u00b7 \u00b7, iN) is otherwise equal to 0. We assume that Y is a noisy measurement of the true latent tensor X, and is corrupted by outlier S, i.e. Y = X + S + \u03b5, where X is generated by tensor factorization representing the global information, S is forced to be economical in representing the local information, and it is isotropic noise (see Fig. 1).The default CP factorization with a low CP rank representing the global information."}, {"heading": "4.2 Model Learning via Bayesian Inference", "text": "Since the exact Bayesian conclusion of our model is analytically insoluble, we have to resort to the approximate conclusion. Although the variational Bayesian (VB) conclusion [44] is difficult to derive, especially when multiple interactions of latent factors are involved, it5 has advantages of closed rear shapes and high efficiency compared to sample-based inference methods. Therefore, we use the VB conclusion to learn our model and present only the main results, while the detailed derivatives and evidence are provided in the appendix. Therefore, we aim at a distribution q (q) to seek the true rear distribution p (HQ) in the sense of minimizing the KL divergence, i.e. that isKL (q), q (HE), p (HE), p (HE), p (HE), p (HE), lp (HE), q (HE), q (HE), that this model refers to (HE)."}, {"heading": "4.2.1 Posterior distribution of factor matrices", "text": "From the graphic model shown in Figure 2, the conclusion of the factor matrix A (n) is that (n), which (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n), (n (n), (n), (n), (n (n), (n), (n), (n), (n (n), (n), n (n), n (n), n (n, n (n), n (n, n (n), n (n, n (n), n (n, n, n (n), n (n, n, n, n (n, n, n, n, n, n (n, n, n, n, n, n (n, n, n, n, n, n, n (n), n (n, n (n, n, n, n, n, n, n (n, n, n, n, n, n, n (, n), n (n (n, n, n, n, n (, n), n (n (n, n), n (n (n, n, n (n), n (n (n), n (, n (n), n (n (n, n, n), n (n (n), n (n (n, n), n (n, n (n (n), n (n (n), n (n (, n (, n, n), n (n (n), n (, n (n (n), n (, n (, n, n (, n), n (, n (, n), n (, n (, n, n (, n (, n), n (, n (, n), n (, n (, n, n), n (, n"}, {"heading": "4.2.2 Posterior distribution of hyperparameters \u03bb", "text": "From Fig. 2, the conclusion of \u03bb can be drawn by receiving messages from N factor matrices and including messages from its hyperprior. We can show that the rear parameters of \u03bbr [1, R] are independent of the gamma distribution and can be updated by (see paragraph 4 of the appendix for details) crM = c0 + 12 N \u2211 n = 1 In, d r M = d0 + 1 2 N \u2211 n = 1 Eq [a (n) T \u00b7 r a (n) \u00b7 r]. (16) The rear expectation term in (16) can be evaluated by the rear parameters in (13), d r M = d0 + 2 N \u2211 n = 1 Eq [a (n) T \u00b7 r a (n) \u00b7 r] in the rear expectation component in (16)."}, {"heading": "4.2.3 Posterior distribution of sparse tensor S", "text": "By combining the priors in (8) and the probability in (6), we can calculate the rear approximation of S as (see paragraph 5 of the appendix for details) q (S) = (i1,..., iN) as (Si1... iN,... iN,... i1... iN \u2212 Eq (Yi1... iN \u2212 Eq [a (1) i1,..., a (N) iN], (N) i1... iN = (Eq... iN]) \u2212 1. (19) Note that S collects information that is not explained by the lower-level CP approximation, while the quantity is influenced by the previous interpretation."}, {"heading": "4.2.4 Posterior distribution of hyperparameters \u03b3", "text": "By including the primary and hyperprior of S2i1 in (8), we show that the posterior value of \u03b3 is also factored in as independent distributions of the individual entries (see paragraph 6 of the appendix), which are set by q (\u03b3) = (i1,..., iN). (20) The posterior parameters can be updated by one point: i1... iN M = a \u03b3 0 + 1 2, b \u03b3i1... iN M = b \u03b3 0 + 1 2 (S-2i1... iN + \u03c32i1... iN). (21) This indicates that the smaller equation [S2i1... iN] leads to a larger equation, which forces S-2i1... iN] to be stronger by (19) zero, and vice versa. In other words, the smallest elements with a size of nitude, which should essentially be increased by the size of the individual elements, while the information should be increased by the further S-size."}, {"heading": "4.2.5 Posterior distribution of hyperparameter \u03c4", "text": "We can show that the posterior variation is a gamma distribution (see section 7 of the appendix), i.e. q (dot) = Ga (dot) q (dot) q (dot), where the posterior parameters can be updated by dot (dot). (..., iN Oi1... iN, b) Q (dot) 0 + 1 Eq (dot) O (dot) (dot) O (dot). (..., A (dot). (dot). (dot). (dot). (dot). (dot). (dot). (dot). (dot). (dot). (dot). (dot). (dot)."}, {"heading": "4.2.6 Lower bound of model evidence", "text": "Since at each step of the iterative reestimation procedure the value of this limit should not decrease, we can monitor the lower limit to check convergence for convergence; the lower limit of the log limit probability can also be written as L (q) = Eq (swaps) [ln p (YB)] + H (Q))), (25), with the first term denoting the lower expectation of the common probability density, and the second term denoting the entropy of the q distribution. Taking the parametric form of the q distributions derived in the previous section, it can then be evaluated by an explicit form (see section 10 of the appendix for details); the uppermost hyperparameters a\u03b30, b \u00b2 0, a \u0432 0, b \u00b2 0, b \u00b2 N \u2212 b0, b \u00b2 n are usually set very small values that lead to a non-informative before or zero."}, {"heading": "4.2.7 Initialization of model inference", "text": "The variable Bayesian conclusion is only guaranteed to converge to a local minimum. To alleviate the fact that q is stuck in bad local solutions, it is important to choose an initialization point. In our model, the hyperparameters of the top level, including c0, d0, a\u03c40, b\u03c40, a \u03b3i1... iN] = 1. For the factor matrices, E [A (n), the expectation of hyperparameters can be initialized by E [p] = I, E [p] = 1 and E [p] n. One is randomly taken from N (0, I) for each row vector {a (n) in}, the other is set to A (n) = U (n)."}, {"heading": "4.3 Predictive Distribution", "text": "The distribution of predictions of missing entries is also analytically insoluble taking into account the observed entries. Therefore, we can approximate them by variation of all the rear parameters in servation, resulting in a Student-t distribution (see paragraph 11 of the appendix for details) p (Y | Y) = i1,..., iNT (Yi1... iN | Y-i1... iN,... iN,... iN) (27) with its parameters iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "4.4 Computational Complexity", "text": "The calculation costs of N-factor matrices in (13) are O (R2M \u2211 n In + R 3 \u2211 n In), where M denotes the number of observations, R denotes the complexity of the model and is generally much smaller than the data size, i.e. R M. The calculation costs are O (R2M + R3) for \u03bb, O (R2M) for \u03c4 and O (MNR) for S. Therefore, the total complexity of our algorithm O (R2M + R3) \u2211 n In), which scales linearly with the data size, but polynomically with the tensor rank. Note, however, that due to the automatic model reduction, the excessive latent components in the first iterations are streamlined in such a way that R rapidly decreases in practice. The solution presented in this paper focuses mainly on a general tensor factoring problem. However, if the data is extremely sparse or large, an alternative strategy for non-endangered inferences can be developed, the inferences observed by each insert.8 will be updated accordingly."}, {"heading": "4.5 Case of Complete Tensor", "text": "For fully observed tensor data, we can simply define O with all elements as 1 and apply the model conclusion described above. However, several essentially different properties arise during the inference, leading to the possibility of a more efficient calculation for approximate stragglers. Therefore, for the conclusion of factor matrices shown in (13), A (\\ n) is always equal (1, In), so that {V (n) in} (Y (n) are all equal. Therefore, only one V (n) has to be calculated for each mode-n, and {a) in each case (n) in {TA). In} In = 1can simultaneously of A (n) = Eq [n) \u2212 Eq [S (n))))))))) Eq [A (n), V (n), (n)."}, {"heading": "5 ADVANTAGES", "text": "Because our model is based on a hierarchical probabilistic framework and a fully Bayesian treatment, several advantages are gained and discussed as follows: \u2022 Our method is characterized as a parameter-free approach, and all model parameters can be learned automatically from observed data. \u2022 Existing tensor factoring methods, by contrast, require either predefined rank or penalty parameters and tensor completion methods using nuclear standards to adjust regulation parameters. \u2022 Automatic ranking allows us to recognize the basic truth of the CP rank, while the automatic thrift model can automatically adjust the model to various types of outliers or non-gassed noise. \u2022 Unlike point estimations by most existing tensor methods, the most elegant feature is that the trade-off between the slight approximation and the sparse representation can be learned automatically in terms of maximizing model evidence. \u2022 Unlike point estimations, most of the existing problem can override all model uncertainty parameters."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Validation on Synthetic Data", "text": "In fact, most of them are in a position to outdo themselves \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "6.2 Video Background Modeling", "text": "In this context, it should be noted that the case concerns a case in which a person has been injured."}, {"heading": "6.3 Facial Image Denoising", "text": "In this section we will examine the characteristics of robust people who are unable to move. We will use the CMU database for multilinear model analysis and evaluation."}, {"heading": "7 CONCLUSION", "text": "In this paper, we have proposed a fully Bayesian generative model of robust tensor factorization that can naturally process missing data and outliers, along with the corresponding algorithm for efficient model conclusions. Our method has several important features: 1) a general framework for an arbitrary order sensor; 2) robustness to outliers, non-Gaussian noise, and overfits; 3) free tuning parameters through automatic ranking and automatic parameter selection; 4) closed shape VB algorithm for efficient and deterministic inference. Comprehensive experiments and comparisons with synthetic and real data sets have confirmed the superiority of BRTF over modern robust methods and tensor factorization methods. Therefore, BRTF has proven to be promising for robust tensor factorization, robust tensor completion, and outlier detection."}], "references": [{"title": "Tensor Decompositions and Applications", "author": ["T. Kolda", "B. Bader"], "venue": "SIAM Review, vol. 51, no. 3, pp. 455\u2013500, 2009.  13", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonnegative Matrix and Tensor Factorizations", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.I. Amari"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Temporal collaborative filtering with Bayesian probabilistic tensor factorization", "author": ["L. Xiong", "X. Chen", "T.-K. Huang", "J. Schneider", "J.G. Carbonell"], "venue": "Proceedings of SIAM Data Mining, vol. 2010, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic latent tensor factorization model for link pattern prediction in multi-relational networks", "author": ["S. Gao", "L. Denoyer", "P. Gallinari", "J. GUO"], "venue": "The Journal of China Universities of Posts and Telecommunications, vol. 19, pp. 172\u2013181, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Higher order partial least squares (hopls): A generalized multilinear regression method", "author": ["Q. Zhao", "C.F. Caiafa", "D.P. Mandic", "Z.C. Chao", "Y. Nagasaka", "N. Fujii", "L. Zhang", "A. Cichocki"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 7, pp. 1660\u20131673, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous tensor decomposition and completion using factor priors", "author": ["Y. Chen", "C. Hsu", "H. Liao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 3, pp. 577\u2013591, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A multilinear singular value decomposition", "author": ["L. De Lathauwer", "B. De Moor", "J. Vandewalle"], "venue": "SIAM journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1253\u20131278, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Canonical polyadic decomposition with a columnwise orthonormal factor matrix", "author": ["M. S\u00f8rensen", "L.D. Lathauwer", "P. Comon", "S. Icart", "L. Deneire"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 33, no. 4, pp. 1190\u20131213, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable tensor factorizations for incomplete data", "author": ["E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. M\u00f8rup"], "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 106, no. 1, pp. 41\u201356, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimizationbased algorithms for tensor decompositions: Canonical polyadic decomposition, decomposition in rank-(Lr, Lr, 1) terms, and a new generalization", "author": ["L. Sorber", "M. Van Barel", "L. De Lathauwer"], "venue": "SIAM Journal on Optimization, vol. 23, no. 2, pp. 695\u2013720, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic models for incomplete multi-dimensional arrays", "author": ["W. Chu", "Z. Ghahramani"], "venue": "JMLR Workshop and Conference Proceedings, vol. 5, 2009, pp. 89\u201396.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Exponential family tensor factorization for missing-values prediction and anomaly detection", "author": ["K. Hayashi", "T. Takenouchi", "T. Shibata", "Y. Kamiya", "D. Kato", "K. Kunieda", "K. Yamada", "K. Ikeda"], "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on. IEEE, 2010, pp. 216\u2013225.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Infinite Tucker decomposition: Nonparametric Bayesian models for multiway data analysis", "author": ["Z. Xu", "F. Yan", "A. Qi"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012, pp. 1023\u20131030.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Tensor rank, invariants, inequalities, and applications", "author": ["E.S. Allman", "P.D. Jarvis", "J.A. Rhodes", "J.G. Sumner"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 34, no. 3, pp. 1014\u20131045, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Most tensor problems are NPhard", "author": ["C.J. Hillar", "L.-H. Lim"], "venue": "Journal of the ACM (JACM), vol. 60, no. 6, p. 45, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["V. De Silva", "L.-H. Lim"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 3, pp. 1084\u20131127, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Tensor rank: Some lower and upper bounds", "author": ["B. Alexeev", "M.A. Forbes", "J. Tsimerman"], "venue": "Computational Complexity (CCC), 2011 IEEE 26th Annual Conference on. IEEE, 2011, pp. 283\u2013291.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Geometric complexity theory and tensor rank", "author": ["P. B\u00fcrgisser", "C. Ikenmeyer"], "venue": "Proceedings of the 43rd annual ACM symposium on Theory of computing. ACM, 2011, pp. 509\u2013518.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic relevance determination for multi-way models", "author": ["M. M\u00f8rup", "L.K. Hansen"], "venue": "Journal of Chemometrics, vol. 23, no. 7-8, pp. 352\u2013363, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 208\u2013220, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensor completion via a multi-linear low-n-rank factorization model", "author": ["H. Tan", "B. Cheng", "W. Wang", "Y.-J. Zhang", "B. Ran"], "venue": "Neurocomputing, vol. 133, pp. 161\u2013169, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with tensors: a framework based on convex opti-  mization and spectral regularization", "author": ["M. Signoretto", "Q.T. Dinh", "L. De Lathauwer", "J.A. Suykens"], "venue": "Machine Learning, pp. 1\u201349, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Composite splitting algorithms for convex optimization", "author": ["J. Huang", "S. Zhang", "H. Li", "D. Metaxas"], "venue": "Computer Vision and Image Understanding, vol. 115, no. 12, pp. 1610\u20131622, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Tensor factorization using auxiliary information", "author": ["A. Narita", "K. Hayashi", "R. Tomioka", "H. Kashima"], "venue": "Data Mining and Knowledge Discovery, vol. 25, no. 2, pp. 298\u2013324, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust principal component analysis?", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Robust PCA via outlier pursuit.", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Transactions on Information Theory, vol. 58,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Online robust PCA via stochastic optimization", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 404\u2013412.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust principal component analysis with non-greedy L1-norm maximization", "author": ["F. Nie", "H. Huang", "C. Ding", "D. Luo", "H. Wang"], "venue": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence. AAAI Press, 2011, pp. 1433\u2013 1438.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Direct robust matrix factorizatoin for anomaly detection", "author": ["L. Xiong", "X. Chen", "J. Schneider"], "venue": "IEEE 11th International Conference on Data Mining (ICDM). IEEE, 2011, pp. 844\u2013853.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical low-rank matrix approximation under robust L1norm", "author": ["Y. Zheng", "G. Liu", "S. Sugimoto", "S. Yan", "M. Okutomi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 1410\u20131417.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint schatten pnorm and lp-norm robust matrix completion for missing value recovery", "author": ["F. Nie", "H. Wang", "H. Huang", "C. Ding"], "venue": "Knowledge and Information Systems, pp. 1\u201320, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient computation of robust weighted low-rank matrix approximations using the l1 norm", "author": ["A. Eriksson", "A. Van den Hengel"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1681\u20131690, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "GoDec: randomized low-rank & sparse matrix decomposition in noisy case", "author": ["T. Zhou", "D. Tao"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML), 2011, pp. 33\u201340.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust tensor factorization using R1 norm", "author": ["H. Huang", "C. Ding"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2008, pp. 1\u20138.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust Tucker tensor decomposition for effective image representation", "author": ["M. Zhang", "C. Ding"], "venue": "The IEEE International Conference on Computer Vision (ICCV), December 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust tensor subspace learning for anomaly detection", "author": ["J. Li", "G. Han", "J. Wen", "X. Gao"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 2, no. 2, pp. 89\u201398, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust low-rank tensor recovery: Models and algorithms", "author": ["D. Goldfarb", "Z. Qin"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 35, no. 1, pp. 225\u2013253, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian robust principal component analysis", "author": ["X. Ding", "L. He", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 20, no. 12, pp. 3419\u20133430, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian robust PCA of incomplete data", "author": ["J. Luttinen", "A. Ilin", "J. Karhunen"], "venue": "Neural processing letters, vol. 36, no. 2, pp. 189\u2013202, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic approach to robust matrix factorization", "author": ["N. Wang", "T. Yao", "J. Wang", "D.-Y. Yeung"], "venue": "Computer Vision\u2013 ECCV. Springer, 2012, pp. 126\u2013139.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian robust matrix factorization for image and video processing", "author": ["N. Wang", "D.-Y. Yeung"], "venue": "Proceedings of International Conference on Computer Vision, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse Bayesian methods for low-rank matrix estimation", "author": ["S.D. Babacan", "M. Luessi", "R. Molina", "A.K. Katsaggelos"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 3964\u2013 3977, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "The Journal of Machine Learning Research, vol. 1, pp. 211\u2013244, 2001.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "Variational message passing", "author": ["J.M. Winn", "C.M. Bishop"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 661\u2013694, 2005.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Moving object detection by detecting contiguous outliers in the low-rank representation", "author": ["X. Zhou", "C. Yang", "W. Yu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 3, pp. 597\u2013610, 2013. 14", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "The CMU pose, illumination, and expression database", "author": ["T. Sim", "S. Baker", "M. Bsat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 12, pp. 1615\u20131618, 2003.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 3, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 196, "endOffset": 199}, {"referenceID": 5, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 201, "endOffset": 204}, {"referenceID": 6, "context": "The two most popular tensor factorization frameworks are Tucker [7] and CANDECOMP/PARAFAC (CP) [1], also known as canonical polyadic decomposition (CPD) [8], which naturally results in two different definitions of tensor", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "The two most popular tensor factorization frameworks are Tucker [7] and CANDECOMP/PARAFAC (CP) [1], also known as canonical polyadic decomposition (CPD) [8], which naturally results in two different definitions of tensor", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "The two most popular tensor factorization frameworks are Tucker [7] and CANDECOMP/PARAFAC (CP) [1], also known as canonical polyadic decomposition (CPD) [8], which naturally results in two different definitions of tensor", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "CP factorization with missing values has been developed by employing CP weighted optimization (CPWOPT) [9] and nonlinear least squares (CPNLS) [10].", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "CP factorization with missing values has been developed by employing CP weighted optimization (CPWOPT) [9] and nonlinear least squares (CPNLS) [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 205, "endOffset": 209}, {"referenceID": 13, "context": "It is important to emphasize that our knowledge about the properties of tensor rank, especially CP rank, is surprisingly limited [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "There is no straightforward algorithm to compute CP rank of an explicitly given tensor, and the problem has been shown to be NP-hard [15], [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "There is no straightforward algorithm to compute CP rank of an explicitly given tensor, and the problem has been shown to be NP-hard [15], [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "In fact, determining or even bounding the tensor rank is quite difficult in contrast to matrix rank [17], [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "In fact, determining or even bounding the tensor rank is quite difficult in contrast to matrix rank [17], [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "In [19], ARD framework was applied to estimate the multilinear rank.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "In addition, the auxiliary information can be exploited to improve completion accuracy [6], [24], which, however, is only suitable to some specific applications.", "startOffset": 87, "endOffset": 90}, {"referenceID": 23, "context": "In addition, the auxiliary information can be exploited to improve completion accuracy [6], [24], which, however, is only suitable to some specific applications.", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 141, "endOffset": 145}, {"referenceID": 29, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 147, "endOffset": 151}, {"referenceID": 30, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 153, "endOffset": 157}, {"referenceID": 31, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 159, "endOffset": 163}, {"referenceID": 32, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 165, "endOffset": 169}, {"referenceID": 33, "context": "[34] and robust Tucker decompositions were studied in [35], [36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[34] and robust Tucker decompositions were studied in [35], [36].", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "[34] and robust Tucker decompositions were studied in [35], [36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "To handle both missing data and outliers, the nuclear norm regularization has been combined with L1-norm loss function, which leads to a robust tensor completion [37].", "startOffset": 162, "endOffset": 166}, {"referenceID": 37, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 72, "endOffset": 76}, {"referenceID": 38, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 119, "endOffset": 123}, {"referenceID": 40, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 125, "endOffset": 129}, {"referenceID": 41, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 131, "endOffset": 135}, {"referenceID": 37, "context": "In [38], Beta-Bernoulli distribution is exploited to model outliers and the low-rank matrix exclusively, which, however, results in high model complexity and slow inference.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "Missing values are considered in [39], where the number of latent components needs to be specified in advance.", "startOffset": 33, "endOffset": 37}, {"referenceID": 41, "context": "In [42], Jeffreys prior is adopted to model both noise and outliers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "PRMF [40] uses Laplace distribution to model the residuals, while the rank of the underlying model should be given in advance.", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "A fully Bayesian treatment of PRMF [41] employs a hierarchical view of Laplace distribution as the noise model, and applies MCMC sampling for model inference.", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "Higher-order robust PCA (HORPCA), proposed very recently in [37], is the only existing tensor method that can handle both missing data and outliers.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "The Kronecker product [1] of matrices A \u2208 RI\u00d7J and B \u2208 RK\u00d7L is a matrix of size IK \u00d7 JL, denoted by A \u2297B.", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "The standard CP factorization [1] is expressed by", "startOffset": 30, "endOffset": 33}, {"referenceID": 42, "context": "The priors in (7) and (8) are related to the framework of sparse Bayesian learning (SBL) [43] which is usually employed for variable selections.", "startOffset": 89, "endOffset": 93}, {"referenceID": 43, "context": "Although variational Bayesian (VB) inference [44] is difficult for derivations, especially when multiple interactions of latent factors are involved, it", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "(FME) [10] between the estimated factors and groundtruth was also evaluated.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 140, "endOffset": 143}, {"referenceID": 18, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 206, "endOffset": 209}, {"referenceID": 9, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 36, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 285, "endOffset": 289}, {"referenceID": 24, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 66, "endOffset": 70}, {"referenceID": 40, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 77, "endOffset": 81}, {"referenceID": 44, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 150, "endOffset": 154}, {"referenceID": 45, "context": "We use the CMU-PIE face database [46] for multilinear model analysis and evaluation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "7) and compare BRTF with tensor based methods including HORPCA, CP-ALS, TuckerARD [19], and HOSVD.", "startOffset": 82, "endOffset": 86}], "year": 2017, "abstractText": "We propose a generative model for robust tensor factorization in the presence of both missing data and outliers. The objective is to explicitly infer the underlying low-CP-rank tensor capturing the global information and a sparse tensor capturing the local information (also considered as outliers), thus providing the robust predictive distribution over missing entries. The lowCP-rank tensor is modeled by multilinear interactions between multiple latent factors on which the column sparsity is enforced by a hierarchical prior, while the sparse tensor is modeled by a hierarchical view of Student-t distribution that associates an individual hyperparameter with each element independently. For model learning, we develop an efficient closed-form variational inference under a fully Bayesian treatment, which can effectively prevent the overfitting problem and scales linearly with data size. In contrast to existing related works, our method can perform model selection automatically and implicitly without need of tuning parameters. More specifically, it can discover the groundtruth of CP rank and automatically adapt the sparsity inducing priors to various types of outliers. In addition, the tradeoff between the low-rank approximation and the sparse representation can be optimized in the sense of maximum model evidence. The extensive experiments and comparisons with many state-of-the-art algorithms on both synthetic and real-world datasets demonstrate the superiorities of our method from several perspectives.", "creator": "LaTeX with hyperref package"}}}