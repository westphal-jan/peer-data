{"id": "1410.3791", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2014", "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition", "abstract": "The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance.", "histories": [["v1", "Tue, 14 Oct 2014 18:37:32 GMT  (781kb,D)", "http://arxiv.org/abs/1410.3791v1", "9 pages, 4 figures, 5 tables"]], "COMMENTS": "9 pages, 4 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rami al-rfou", "vivek kulkarni", "bryan perozzi", "steven skiena"], "accepted": false, "id": "1410.3791"}, "pdf": {"name": "1410.3791.pdf", "metadata": {"source": "CRF", "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition", "authors": ["Rami Al-Rfou", "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena"], "emails": ["skiena}@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require human-annotated NER records or language-specific resources such as tree banks, parallel corpora, and orthographic rules. The novelty of the approach is that we use only language agnostic techniques while delivering competitive performance at the same time. Our method learns distributed word representations (word embeddings) that encode semantic and syntactic features of words in each language. We then automatically generate data sets from Wikipedia link structure and freebase attributes. Finally, we apply two pre-processing steps (oversampling and exact match of interface shapes) that do not require linguistic expertise."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work", "text": "Wikipedia has been used as a resource for many tasks in NLP and IR [13, 19]. There is a lot of literature on Wikipedia pre-processing for NER [9, 14, 15, 22, 23, 26], which is summarized in Table 1. All previous work depends on language-specific pre-processing steps such as tagging and parallel corpora. Dependence on language-specific processing represents a bottleneck in the scalability and diversity of languages covered by the previous systems. In contrast, our work relies exclusively on language-agnostic techniques. The closest related work is Nothman et al. [22]. Compared to their approach, we find that only oversampling is a sufficient replacement for their entire proposed language-dependent pre-processing pipeline (see Section 4.2.1)."}, {"heading": "3 Semi-supervised Learning", "text": "The aim of Named Entity Recognition (NER) is to identify the sequences of tokens that are divided into one of several categories. We are following the approach proposed by [7] to take into account the entire sentence structure, which ignores the dependencies between word tags and could therefore not capture some limitations on the appearance of a word in the text. However, empirically, our assessment does not indicate the manifestation of this problem. More importantly, this word-based formulation allows us to use simpler translations and exact matching mechanisms, such as we capture semantic and syntactic features of words in Section 4.3.1 Word Embeddings."}, {"heading": "4 Extracting Entity Mentions", "text": "Our approach to creating a designated training corpus from Wikipedia consists of two steps: First, we find which Wikipedia articles correspond to the entities using Freebase [4]; second, we deal with the missing annotations problem by categorizing the article topics into one of the following categories: Y = {PERSON, LOCATION, ORGANIZATION, ORGANIZATION, NONENTITY}. For each entity category, we define the corresponding freebase attributes as follows: \u2022 LOCATION / {city, country, region, district, administrative _ division} \u2022 ORGANIZATION / sports / team, / organization / organization, \u2022 PERSON / people / internal result we."}, {"heading": "5 Results", "text": "In this section we analyze the errors that the qualitative evaluation system has produced. In addition, we evaluate the performance of POLYGLOT-NER in terms of datasets to demonstrate the efficiency of the proposed solutions that deal with missing links in Wikipedia."}, {"heading": "6 Distant Evaluation", "text": "This year it is more than ever before in the history of the city."}, {"heading": "7 Conclusion & Future Work", "text": "We have successfully built a multilingual NER system for 40 languages with no language-specific knowledge or knowledge. We use automatically learned features and apply language agnostic data processing techniques. The system outperforms previous work in multiple languages and is competitive in the remaining data sets with human annotations. We demonstrate its performance in the other languages through comparative analysis using machine translation. Our approach results in highly consistent performance in all languages. Wikipedia links are used in combination with Freebase to extend our approach to all languages in the future."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Nltk: the natural language toolkit. In 2 2 2 2 2 2 2 2 2 Wikipedia(Size", "author": ["Steven Bird"], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of SIGMOD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Named entity extraction using adaboost", "author": ["Xavier Carreras", "Llu\u00eds M\u00e0rques", "Llu\u00eds Padr\u00f3"], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Building a multilingual named entity-annotated corpus using annotation projection", "author": ["Maud Ehrmann", "Marco Turchi", "Ralf Steinberger"], "venue": "In RANLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["Charles Elkan", "Keith Noto"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Training and evaluating a german named entity recognizer with semantic generalization", "author": ["Manaal Faruqui", "Sebastian Pad\u00f3"], "venue": "In Proceedings of KONVENS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Named entity recognition through classifier combination", "author": ["Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "Proceedings of CoNLL-2003,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "In IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Exploiting Wikipedia as external knowledge for named entity recognition", "author": ["Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": "In Proceedings of (EMNLP-CoNLL),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Multilingual named entity recognition using parallel data and metadata from wikipedia", "author": ["Sungchul Kim", "Kristina Toutanova", "Hwanjo Yu"], "venue": "In Proceedings of ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["Wee Sun Lee", "Bing Liu"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Learning to classify texts using positive and unlabeled data", "author": ["Xiaoli Li", "Bing Liu"], "venue": "In IJCAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Building text classifiers using positive and unlabeled examples", "author": ["Bing Liu", "Yang Dai", "Xiaoli Li", "Wee Sun Lee", "Philip S Yu"], "venue": "In Data Mining,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["Rada Mihalcea", "Andras Csomai"], "venue": "In Proceedings of CIKM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Analysing Wikipedia and gold-standard corpora for NER training", "author": ["Joel Nothman", "Tara Murphy", "James R. Curran"], "venue": "In Proceedings of EACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Mining wiki resources for multilingual named entity recognition", "author": ["Alexander E Richman", "Patrick Schone"], "venue": "In ACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of CoNLL-2003,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Introduction to the conll- 2002 shared task: Language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang"], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "A proposal to automatically build and maintain gazetteers for Named Entity Recognition by using Wikipedia", "author": ["A. Toral", "R. Munoz"], "venue": "Proceedings of the EACL-2006 Workshop on NEW TEXT-Wikis and blogs and other dynamic text sources", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Mengqiu Wang", "Christopher D. Manning"], "venue": "In Proceedings of IJCNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Successful approaches to address NER rely on supervised learning [5, 12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 10, "context": "Successful approaches to address NER rely on supervised learning [5, 12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 8, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 14, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 15, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 16, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 24, "context": "Language Specific Agnostic S1 S2 S3 S4 S5 S6 S7 # Language code Toral and Munoz [26] X X X 1 en Kazama and Torisawa [14] X X 1 en Richman and Schone [23] X X X 7 en, es, fr, uk, ru, pl, pt Ehrmann et al.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Language Specific Agnostic S1 S2 S3 S4 S5 S6 S7 # Language code Toral and Munoz [26] X X X 1 en Kazama and Torisawa [14] X X 1 en Richman and Schone [23] X X X 7 en, es, fr, uk, ru, pl, pt Ehrmann et al.", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "Language Specific Agnostic S1 S2 S3 S4 S5 S6 S7 # Language code Toral and Munoz [26] X X X 1 en Kazama and Torisawa [14] X X 1 en Richman and Schone [23] X X X 7 en, es, fr, uk, ru, pl, pt Ehrmann et al.", "startOffset": 149, "endOffset": 153}, {"referenceID": 7, "context": "[9] X X X X 6 en, es, fr, de, ru, cs Kim et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[15] X X X 3 en, ko, bg Nothman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] X X X X 9 en, es, fr, de, ru, pl, pt, it, nl POLYGLOT-NER X X 40 en, es, fr, de, ru, pl, pt, it, nl ar, he, hi, zh, ko, ja, tl, ms, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "2 Related Work Wikipedia has been used as a resource for many tasks in NLP and IR [13, 19].", "startOffset": 82, "endOffset": 90}, {"referenceID": 17, "context": "2 Related Work Wikipedia has been used as a resource for many tasks in NLP and IR [13, 19].", "startOffset": 82, "endOffset": 90}, {"referenceID": 7, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 12, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 13, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 20, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 21, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 24, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We follow the approach proposed by [7] to model NER as a word level classification problem.", "startOffset": 35, "endOffset": 38}, {"referenceID": 18, "context": "1 Word Embeddings capture semantic and syntactic characteristics of words through unsupervised learning [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "They have been successfully used as features for several tasks including NER [7, 27] and proposed as a cornerstone for developing multilingual applications [1].", "startOffset": 77, "endOffset": 84}, {"referenceID": 25, "context": "They have been successfully used as features for several tasks including NER [7, 27] and proposed as a cornerstone for developing multilingual applications [1].", "startOffset": 77, "endOffset": 84}, {"referenceID": 0, "context": "They have been successfully used as features for several tasks including NER [7, 27] and proposed as a cornerstone for developing multilingual applications [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "We use the Polyglot embeddings [1] as our sole features for each language under investigation2.", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "The Polyglot embeddings were trained using an objective function proposed by [6] which takes ordered sequences of words as its input.", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "As the stochastic optimization performance is dependent on a good choice of the learning rate, we automate the learning rate selection through an adaptive update procedure [8].", "startOffset": 172, "endOffset": 175}, {"referenceID": 22, "context": "Table 2 shows the performance of our annotators given CONLL training datasets [24, 25] and the word embeddings as features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Table 2 shows the performance of our annotators given CONLL training datasets [24, 25] and the word embeddings as features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 26, "context": "Our results on English are similar to the ones reported by [28].", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "4 Extracting Entity Mentions Our procedure for creating a named entity training corpus from Wikipedia consists of two steps; First, we find which Wikipedia articles correspond to entities, using Freebase [4].", "startOffset": 204, "endOffset": 207}, {"referenceID": 8, "context": "In such a setting, [10, 17, 18] show that considering the unlabeled set as negative examples while modifying the objective loss to accommodate different penalties for misclassifying each set of examples outperforms other heuristics and other iterative EM-based methods.", "startOffset": 19, "endOffset": 31}, {"referenceID": 15, "context": "In such a setting, [10, 17, 18] show that considering the unlabeled set as negative examples while modifying the objective loss to accommodate different penalties for misclassifying each set of examples outperforms other heuristics and other iterative EM-based methods.", "startOffset": 19, "endOffset": 31}, {"referenceID": 16, "context": "In such a setting, [10, 17, 18] show that considering the unlabeled set as negative examples while modifying the objective loss to accommodate different penalties for misclassifying each set of examples outperforms other heuristics and other iterative EM-based methods.", "startOffset": 19, "endOffset": 31}, {"referenceID": 20, "context": "[22] 67.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] 61.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "However, [21] show that Wikipedia is better suited than human annotated datasets as a source of training for domain adaptation scenarios.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "Figure 3 shows the performance of our system compared to other annotators; OPENNLP {English, Spanish, Dutch} [2], NLTK English [3], and STANFORD German [11].", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "Figure 3 shows the performance of our system compared to other annotators; OPENNLP {English, Spanish, Dutch} [2], NLTK English [3], and STANFORD German [11].", "startOffset": 152, "endOffset": 156}], "year": 2014, "abstractText": "The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein using only language agnostic techniques, while achieving competitive performance. Our method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Then, we automatically generate datasets from Wikipedia link structure and Freebase attributes. Finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise. Our evaluation is two fold: First, we demonstrate the system performance on human annotated datasets. Second, for languages where no gold-standard benchmarks are available, we propose a new method, distant evaluation, based on statistical machine translation.", "creator": "LaTeX with hyperref package"}}}