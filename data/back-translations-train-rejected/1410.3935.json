{"id": "1410.3935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2014", "title": "A Logic-based Approach to Generatively Defined Discriminative Modeling", "abstract": "Conditional random fields (CRFs) are usually specified by graphical models but in this paper we propose to use probabilistic logic programs and specify them generatively. Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs in machine learning to compare generative and discriminative models and choose the best model. We implemented our approach as the D-PRISM language by modifying PRISM, a logic-based probabilistic modeling language for generative modeling, while exploiting its dynamic programming mechanism for efficient probability computation. We tested D-PRISM with logistic regression, a linear-chain CRF and a CRF-CFG and empirically confirmed their excellent discriminative performance compared to their generative counterparts, i.e.\\ naive Bayes, an HMM and a PCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF versions of Bayesian network classifiers and probabilistic left-corner grammars respectively and easily implementable in D-PRISM. We empirically showed that they outperform their generative counterparts as expected.", "histories": [["v1", "Wed, 15 Oct 2014 06:01:03 GMT  (148kb)", "http://arxiv.org/abs/1410.3935v1", "20 pages, 2 figures"]], "COMMENTS": "20 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["taisuke sato", "keiichi kubota", "yoshitaka kameya"], "accepted": false, "id": "1410.3935"}, "pdf": {"name": "1410.3935.pdf", "metadata": {"source": "CRF", "title": "A Logic-based Approach to Generatively Defined Discriminative Modeling", "authors": ["Taisuke Sato"], "emails": ["sato@mi.cs.titech.ac.jp", "kubota@mi.cs.titech.ac.jp", "ykameya@meijo-u.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 141 0.39 35v1 [cs.LG] 1 5O ctKeywords CRFs \u00b7 D-PRISM \u00b7 logic-based Taisuke Sato Tokyo Institute of Technology, Japan Tel.: + 81-3-5743-2186 E-mail: sato @ mi.cs.titech.ac.jpKeiichi Kubota Tokyo Institute of Technology, Japan Tel.: + 81-3-5743-2186 E-mail: kubota @ mi.cs.titech.ac.jpYoshitaka Kameya Meijo University, Japan Tel.: + 81-52-838-2567 E-mail: ykameya @ meijo-u.ac.jp"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to survive themselves are not able to survive themselves, \"he told the German Press Agency in an interview with the Frankfurter Allgemeine Sonntagszeitung (FAZ).\" It's not that we are able to change the world, \"he said.\" But it's not that we are able to change the world. \""}, {"heading": "2 Conditional random fields", "text": "Conditional random fields (CRFs) [11] are popular probability models that yield a conditional distribution p (y | x) over the output sequence y, which takes the following form: p (y | x) \u2261 1Z (x) exp {K \u2211 k = 1\u03bbkfk (x, y)}.Here, fk (x, y) and p (1 \u2264 k \u2264 K) are each a real estimated function (function) and the associated weight (parameter) a normalizing constant. Since Z (x) is the sum of exponentially many terms, the exact calculation is generally intractable and takes the maximum number of possible values for each component of y and hence approximation methods. However, if p (y | x) has a recursive structure of a certain kind as a graphical model such as linear chain CRFs, Z (x) is efficiently compressible."}, {"heading": "4 D-PRISM", "text": "After seeing the basic models of CRFs, we will next show how they are captured by a logic-based modeling of the language PRISM (22,23) with a simple modification of its probability calculation. The modified language is called D-PRISM (discriminatory PRISM). (msw) Before proceeding, we will quickly verify whether there is a high-level generative modeling language based on prologue, enhanced by a rich array of probability statements for different types of probability inference and parameter learning. Specifically, in addition to EM algorithms, it offers viterbi training (VT), variable bayes (VB), variable VT (VB-VT) and MCMC for Bayesian inference. PRISM was applied to music and bioinformatics. (25,1,18).Syntactically a PRISM program is a prologue and runs like prologue."}, {"heading": "5 Experiments with three basic models", "text": "In this section, we conduct learning experiments with CRFs8. CRFs are encrypted by D-PRISM programs, while their generative counterparts are encoded by PRISM programs. We compare their accuracy in discriminatory tasks. We look at three basic models, logistic regression, a linear chain of CRF and a CRF-CFG, and learn their parameters by L-BFGS.5.1 Logistic regression with UCI datasetsWe select four datasets with non-missing data from the UCI Machine Learning Repository [6] and compare their accuracy, one by logistic regression written in D-PRISM and the other by a naive Bayes model (NB). We use the program in Fig. 1 with a reasonable change in values / 2 explanations. The result of ten-fold cross-validation is shown in Table 1 with standard deviations."}, {"heading": "6 Exploring new models", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "7 Program transformation for incomplete data", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "8 Discussion and future work", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "9 Conclusion", "text": "As examples show, D-PRISM programs are only PRISM programs with probabilities replaced by weights. It is the first modeling language of our knowledge that generatively defines CRFs and their extensions to probabilistic grammars. We can generate logistical regressions, linear CRFs, CRF-CFGs, or new models at almost the same modeling cost as PRISM, while achieving better performance on discriminatory tasks."}], "references": [{"title": "Stochastic simulation and modelling of metabolic networks in a machine learning framework", "author": ["M. Biba", "F. Xhafa", "F. Esposito", "S. Ferilli"], "venue": "Simulation Modelling Practice and Theory 19(9), 1957\u20131966", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparing bayesian network classifiers", "author": ["J. Cheng", "R. Greiner"], "venue": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (bUAI\u201999), pp. 101\u2013108", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Non-discriminating arguments and their uses", "author": ["H. Christiansen", "J. Gallagher"], "venue": "Proceedings of the 25th International Conference on Logic Programming (ICLP\u201909), LNCS 5649, pp. 55\u201369", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Parameter estimation in stochastic logic programs", "author": ["J. Cussens"], "venue": "Machine Learning 44(3), 245\u2013271", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["J. Finkel", "A. Kleeman", "C. Manning"], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL\u201908), pp. 959\u2013967", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "UCI machine learning repository", "author": ["A. Frank", "A. Asuncion"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine learning 29(2), 131\u2013163", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "TildeCRF: Conditional random fields for logical sequences", "author": ["B. Gutmann", "K. Kersting"], "venue": "In Proceedings of the 15th European Conference on Machine Learning (ECML-06), pp. 174\u2013185", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint and conditional estimation of tagging and parsing models", "author": ["M. Johnson"], "venue": "Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-01), pp. 322\u2013329", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "The Alchemy system for statistical relational AI", "author": ["S. Kok", "P. Singla", "M. Richardson", "P. Domingos"], "venue": "Technical report, Department of Computer Science and Engineering, University of Washington", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the 18th International Conference on Machine Learning (ICML\u201901), pp. 282\u2013289", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M. Jordan"], "venue": "Proceedings of the 25th international conference on Machine learning (ICML\u201908), pp. 584\u2013591", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D. Liu", "J. Nocedal"], "venue": "Mathematical Programming 45, 503\u2013528", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Probabilistic parsing using left corner language models", "author": ["C. Manning"], "venue": "Proceedings of the 5th International Conference on Parsing Technologies (IWPT-97), pp. 147\u2013158. MIT Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "The MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["M. Marcus", "B. Santorini", "M. Marcinkiewicz"], "venue": "Computational Linguistics 19, 313\u2013330", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": "Advances in Neural Information Processing Systems 22, pp. 1249\u20131257", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Evaluating bacterial gene-finding HMM structures as probabilistic logic programs", "author": ["S. M\u00f8rk", "I. Holmes"], "venue": "Bioinformatics 28(5), 636\u2013642", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "On Discriminative vs", "author": ["A. Ng", "M. Jordan"], "venue": "Generative Classifiers: A comparison of logistic regression and naive Bayes. In: NIPS, pp. 841\u2013848", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Transformation of logic programs: Foundations and techniques", "author": ["A. Pettorossi", "M. Proietti"], "venue": "Journal of Logic Programming 19,20, 261\u2013320", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62, 107\u2013136", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "PRISM: a language for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI\u201997), pp. 1330\u20131335", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Parameter learning of logic programs for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "Journal of Artificial Intelligence Research 15, 391\u2013454", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Generative modeling with failure in PRISM", "author": ["T. Sato", "Y. Kameya", "N.F. Zhou"], "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI\u201905), pp. 847\u2013852", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic-logical modeling of music", "author": ["J. Sneyers", "J. Vennekens", "D. De Schreye"], "venue": "Proceedings of the 8th International Symposium on Practical Aspects of Declarative Languages (PADL\u201906), vol.3819, LNCS, pp. 60\u201372", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning 4(4), 267\u2013373", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Unfold/fold transformation of logic programs", "author": ["H. Tamaki", "T. Sato"], "venue": "Proceedings of the 2nd International Conference on Logic Programming (ICLP\u201984), Lecture Notes in Computer Science, pp. 127\u2013138. Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1984}, {"title": "ATR integrated speech and language database", "author": ["N. Uratani", "T. Takezawa", "H. Matsuo", "C. Morita"], "venue": "Technical Report TR-IT-0056, ATR Interpreting Telecommunications Research Laboratories", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "Maximum-likelihood training of the PLCG-based language model", "author": ["D. Van Uytsel", "D. Van Compernolle", "P. Wambacq"], "venue": "Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop 2001 (ASRU\u201901), pp. 210\u2013213", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 10, "context": "Conditional random fields (CRFs) [11] are probabilistic models for discriminative modeling defining a conditional distribution p(y | x) over output y given input x.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "They are quite popular for labeling sequence data such as text data and biological sequences [26].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs [19] in machine learning to compare generative and discriminative models and choose the best model.", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "The use of logical expressions to specify CRFs is not new but they have been used solely as feature functions [8,21].", "startOffset": 110, "endOffset": 116}, {"referenceID": 20, "context": "The use of logical expressions to specify CRFs is not new but they have been used solely as feature functions [8,21].", "startOffset": 110, "endOffset": 116}, {"referenceID": 20, "context": "For example in Markov logic networks (MLNs)[21], weighted clauses are used as feature functions to define (conditional) Markov random fields and probabilities are obtained by Gibbs sampling.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "In contrast, our approach is implemented by a generative modeling language PRISM [22,23] where clauses have no weights; they simply constitute a logic program DB computing possible output y from input x by proving a top-goal Gx,y that relates x to y.", "startOffset": 81, "endOffset": 88}, {"referenceID": 22, "context": "In contrast, our approach is implemented by a generative modeling language PRISM [22,23] where clauses have no weights; they simply constitute a logic program DB computing possible output y from input x by proving a top-goal Gx,y that relates x to y.", "startOffset": 81, "endOffset": 88}, {"referenceID": 20, "context": "Our approach to CRFs is general in the sense that, like other statistical relational learning (SRL) languages for CRFs [21,17], programs in D-PRISM have no restriction such as the exclusiveness condition in PRISM [23] except for the use of binary features and we can write any program, i.", "startOffset": 119, "endOffset": 126}, {"referenceID": 16, "context": "Our approach to CRFs is general in the sense that, like other statistical relational learning (SRL) languages for CRFs [21,17], programs in D-PRISM have no restriction such as the exclusiveness condition in PRISM [23] except for the use of binary features and we can write any program, i.", "startOffset": 119, "endOffset": 126}, {"referenceID": 22, "context": "Our approach to CRFs is general in the sense that, like other statistical relational learning (SRL) languages for CRFs [21,17], programs in D-PRISM have no restriction such as the exclusiveness condition in PRISM [23] except for the use of binary features and we can write any program, i.", "startOffset": 213, "endOffset": 217}, {"referenceID": 8, "context": "We point out that binary features are the most common features and they can encode basic CRF models such as logistic regression, linear-chain CRFs and CRF-CFGs [9,5,26].", "startOffset": 160, "endOffset": 168}, {"referenceID": 4, "context": "We point out that binary features are the most common features and they can encode basic CRF models such as logistic regression, linear-chain CRFs and CRF-CFGs [9,5,26].", "startOffset": 160, "endOffset": 168}, {"referenceID": 25, "context": "We point out that binary features are the most common features and they can encode basic CRF models such as logistic regression, linear-chain CRFs and CRF-CFGs [9,5,26].", "startOffset": 160, "endOffset": 168}, {"referenceID": 18, "context": "In machine learning it is well-known that naive Bayes and logistic regression form a generative-discriminative pair [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "As is empirically demonstrated in [19], classification accuracy by discriminative models such as logistic regression is generally better than their corresponding generative models such as naive Bayes when there is enough data but generative models reach their best performance more quickly than discriminative ones w.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "Also the theoretical analysis in [12] suggests that when a model is wrong in generative modeling, the deterioration of prediction accuracy is more severe than in discriminative modeling.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "For any particular data set, it is impossible to predict in advance whether a generative or a discriminative model will perform better\u201d [26].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "Conditional random fields (CRFs) [11] are popular probabilistic models defining a conditional distribution p(y | x) over the output sequence y given an input sequence x which takes the following form:", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "As Z(x) is the sum of exponentially many terms, the exact computation is generally intractable and takes O(M ) time where M is the maximum number of possible values for each component of y and hence approximation methods have been developed [26].", "startOffset": 241, "endOffset": 245}, {"referenceID": 25, "context": "The problem here is that the expectation E(fk | x ) is difficult to compute and hence a variety of approximation methods such as stochastic gradient descent (SDG) [26] have been proposed.", "startOffset": 163, "endOffset": 167}, {"referenceID": 14, "context": "However in this paper we focus on cases where exact computation by dynamic programming is possible and use an algorithm that generalizes inside probability computation in probabilistic context free grammars (PCFGs) [15].", "startOffset": 215, "endOffset": 219}, {"referenceID": 10, "context": "2 Linear-chain CRFs CRFs [11] are generally intractable and a variety of approximation methods such as sampling and loopy BP have been developed.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "3 CRF-CFGs PCFGs [15] are a basic class of probabilistic grammars extending CFGs by assigning selection probabilities \u03b8 to production rules.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "Seeking better parsing accuracy, Johnson attempted parameter learning by maximizing conditional likelihood: \u03b8 = argmax\u03b8 \u220fT t=1 p(\u03c4t | st, \u03b8) but found the improvement is not statistically significant [9].", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "generalized PCFGs to conditional random field context free grammars (CRF-CFGs) [5] where the conditional probability p(\u03c4 | s) of a parse tree \u03c4 given a sentence s is defined by", "startOffset": 79, "endOffset": 82}, {"referenceID": 15, "context": "conducted learning experiments with a CRF-CFG using the Penn Treebank [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": ", sT in the corpus by maximizing conditional likelihood just like [9] but this time they obtained a significant gain in parsing accuracy [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": ", sT in the corpus by maximizing conditional likelihood just like [9] but this time they obtained a significant gain in parsing accuracy [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 21, "context": "Having seen basic models of CRFs, we next show how they are uniformly subsumed by a logic-based modeling language PRISM [22,23] with a simple modification of its probability computation.", "startOffset": 120, "endOffset": 127}, {"referenceID": 22, "context": "Having seen basic models of CRFs, we next show how they are uniformly subsumed by a logic-based modeling language PRISM [22,23] with a simple modification of its probability computation.", "startOffset": 120, "endOffset": 127}, {"referenceID": 24, "context": "PRISM has been applied to music and bioinformatics [25,1,18].", "startOffset": 51, "endOffset": 60}, {"referenceID": 0, "context": "PRISM has been applied to music and bioinformatics [25,1,18].", "startOffset": 51, "endOffset": 60}, {"referenceID": 17, "context": "PRISM has been applied to music and bioinformatics [25,1,18].", "startOffset": 51, "endOffset": 60}, {"referenceID": 22, "context": "DB defines a probability measure pDB(\u00b7) over Herbrand interpretations (possible worlds)[23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Currently we use L-BFGS [13] to maximize l(\u03bb | D).", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "1 Logistic regression with UCI datasets We select four datasets with no missing data from the UCI Machine Learning Repository [6] and compare prediction accuracy, one by logistic regression written in D-PRISM and the other by a naive Bayes model (NB) written in PRISM.", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "We here compare a linear-chain CRF encoded by a D-PRISM program and an HMM encoded by a PRISM program using sequence data extracted from the Penn Treebank [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "As learning data, we use two sets of pairs of sentence and POS tag sequence extracted from the Penn Treebank [16]: section-02 in the WSJ (Wall Street Journal articles) corpus referred to here as WSJ02-ALL and its subset referred to as WSJ02-15, consisting of data of length less-than or equal to 15.", "startOffset": 109, "endOffset": 113}, {"referenceID": 27, "context": "As a dataset, we use the ATR tree corpus and its associated CFG [28].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Table 6 tells us that when a tree corpus is available, as reported in [5], shifting from PCFG (PRISM) to CRF-CFG (D-PRISM) yields much better prediction performance (and shifting cost is almost zero if we use D-PRISM) but at the same time this shifting incurs almost two orders of magnitude longer learning time.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "1 CRF-BNCs Bayesian network classifiers (BNCs) [7,2] are a generalization of naive Bayes classifiers.", "startOffset": 47, "endOffset": 52}, {"referenceID": 1, "context": "1 CRF-BNCs Bayesian network classifiers (BNCs) [7,2] are a generalization of naive Bayes classifiers.", "startOffset": 47, "endOffset": 52}, {"referenceID": 5, "context": "3 is an example of Bayesian network for the car dataset in the UCI Machine Learning Repository [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "CRF-LCGs are a CRF version of probabilistic left-corner grammars (PLCGs) and considered dual to CRF-CFGs [5] in the sense that the former is based on bottom-up parsing, i.", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "left-corner parsing [14,29] whereas the latter is based on top-down parsing.", "startOffset": 20, "endOffset": 27}, {"referenceID": 28, "context": "left-corner parsing [14,29] whereas the latter is based on top-down parsing.", "startOffset": 20, "endOffset": 27}, {"referenceID": 13, "context": "B-tree is projected by a CFG rule A -> B C to complete a G-tree where G and A are in the left-corner relation [14,15].", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "B-tree is projected by a CFG rule A -> B C to complete a G-tree where G and A are in the left-corner relation [14,15].", "startOffset": 110, "endOffset": 117}, {"referenceID": 13, "context": "5 (details omitted)[14].", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "One way to achieve this is to use meaning preserving unfold/fold transformation for logic programs [27,20].", "startOffset": 99, "endOffset": 106}, {"referenceID": 19, "context": "One way to achieve this is to use meaning preserving unfold/fold transformation for logic programs [27,20].", "startOffset": 99, "endOffset": 106}, {"referenceID": 26, "context": "the least model of programs is preserved through transformation (see [27,20] for details).", "startOffset": 69, "endOffset": 76}, {"referenceID": 19, "context": "the least model of programs is preserved through transformation (see [27,20] for details).", "startOffset": 69, "endOffset": 76}, {"referenceID": 26, "context": "It derives an HMM program for hmm0(X) computing incomplete data from a program for hmm0(X,Y) computing complete data using a transformation system described in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 9, "context": "There are already discriminative modeling languages for CRFs such as Alchemy [10] based on MLNs and Factorie [17] based on factor graphs.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "There are already discriminative modeling languages for CRFs such as Alchemy [10] based on MLNs and Factorie [17] based on factor graphs.", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "TildeCRF [8] learns CRFs over sequences of ground atoms.", "startOffset": 9, "endOffset": 12}, {"referenceID": 22, "context": "Compared to PRISM, D-PRISM has no restriction on programs such as the uniqueness condition, exclusiveness condition and independence condition [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 23, "context": "It is developed for PRISM programs with failure [24] and learns parameters from a conditional distribution of the form pDB(Gx | success) where success = \u2203xGx and Gx is a goal for incomplete data x that may fail.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "The point in [24] is to automatically synthesize failure predicate such that pDB(success) = 1 \u2212 pDB(failure) and rewrite the conditional distribution as an infinite series pDB(Gx | success) = pDB(Gx)(1+pDB(failure)+pDB(failure) + \u00b7 \u00b7 \u00b7) to which EM is applicable (the FAM algorithm [4]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "The point in [24] is to automatically synthesize failure predicate such that pDB(success) = 1 \u2212 pDB(failure) and rewrite the conditional distribution as an infinite series pDB(Gx | success) = pDB(Gx)(1+pDB(failure)+pDB(failure) + \u00b7 \u00b7 \u00b7) to which EM is applicable (the FAM algorithm [4]).", "startOffset": 282, "endOffset": 285}, {"referenceID": 2, "context": "Y is a non-discriminating argument in the sense of [3].", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "Christiansen and Gallagher gave a deterministic algorithm to eliminate such non-discriminating arguments without affecting the program\u2019s runtime behavior [3].", "startOffset": 154, "endOffset": 157}], "year": 2014, "abstractText": "Conditional random fields (CRFs) are usually specified by graphical models but in this paper we propose to use probabilistic logic programs and specify them generatively. Our intension is first to provide a unified approach to CRFs for complex modeling through the use of a Turing complete language and second to offer a convenient way of realizing generative-discriminative pairs in machine learning to compare generative and discriminative models and choose the best model. We implemented our approach as the D-PRISM language by modifying PRISM, a logic-based probabilistic modeling language for generative modeling, while exploiting its dynamic programming mechanism for efficient probability computation. We tested D-PRISM with logistic regression, a linear-chain CRF and a CRF-CFG and empirically confirmed their excellent discriminative performance compared to their generative counterparts, i.e. naive Bayes, an HMM and a PCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF versions of Bayesian network classifiers and probabilistic left-corner grammars respectively and easily implementable in D-PRISM. We empirically showed that they outperform their generative counterparts as expected.", "creator": "LaTeX with hyperref package"}}}