{"id": "1410.4176", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2014", "title": "Learning Distributed Word Representations for Natural Logic Reasoning", "abstract": "Natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic. We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks. Our experiments evaluate the models' ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics.", "histories": [["v1", "Wed, 15 Oct 2014 19:27:10 GMT  (15kb)", "http://arxiv.org/abs/1410.4176v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["samuel r bowman", "christopher potts", "christopher d manning"], "accepted": false, "id": "1410.4176"}, "pdf": {"name": "1410.4176.pdf", "metadata": {"source": "CRF", "title": "Learning Distributed Word Representations for Natural Logic Reasoning", "authors": ["Samuel R. Bowman", "Christopher D. Manning"], "emails": ["sbowman@stanford.edu", "cgpotts@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 0.41 76v1 [cs.CL] 1 5O ctNatural logic provides a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations that have proven valuable in a wide range of challenging linguistic tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical thinking that is captured by natural logic. We address this question with two neural network-based models of learning embedding: simple neural networks and neural tensor networks. Our experiments evaluate the models \"ability to learn the basic algebra of natural logical relations from simulated data and from the WordNet noun diagram. Overall, the positive results are promising for the future of taught distributed representations in applied modeling of logical semantics."}, {"heading": "1 Introduction", "text": "Natural logic offers a powerful relational conception of semantics: the meanings of expressions are given, at least in part, by their inferential connections with other expressions [1, 2]. For example, turtle is analyzed, not primarily by its extension in the world, but by its lexical network: it brings reptiles with it, excludes stool, is brought with it by sea turtle, and so on. By generalizing terms of entanglement and contradiction, these relationships can be defined for all lexical categories, as well as for complex phrases, sentences, and even texts. The resulting theories of meaning offer valuable new analytical tools for tasks involving data conclusions, relational extraction, and textual entanglements. Natural logic is well-suited to distributed (e.g. vector) representations that are also related to each other. Distributed representations have been successfully used in a wide range of complex language tasks (e.g. in nebula)."}, {"heading": "2 Neural network models for relation classification", "text": "The architecture we use, which is limited to pairs of single terms (such as words) only, is illustrated in Figure 1. The model presents the two input terms as embedded in a comparison function based on one of two types of neural network layer functions to create a representation of the relationship between the two terms, which is then fed into a Softmax classifier that outputs a distribution over possible labels. The entire network, including the embedding, is tracked by backpropagation.The simpler version of the comparison links the two input vectors before they are fed into a standard layer of neural networks (NN).The more powerful version of neural tensor networks (NTN) uses an additional third-order tensor parameter to allow the multiplicative interactions between the two input vectors before they are fed into a standard layer of neural networks (NTN)."}, {"heading": "3 Reasoning about semantic relations", "text": "In this experiment, we test the ability to learn and use the natural logical conclusions schematized in Figure 2a. Thus, it follows that a person in the table contains a point for which no valid conclusions can be drawn in our logic. Experiments to explore these inferential patterns create artificial Boolean structures in which terms denote entities from a small domain (e.g. Figure 2b), deal with logical conclusions, divide them into trains and test sets, and remove them from the tests."}, {"heading": "4 Reasoning about lexical relations in WordNet", "text": "In fact, most people who are able to move are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance,"}, {"heading": "5 Conclusion", "text": "The results suggest that at least the neural tensor network is able to meet this challenge with relatively large training sets, both by learning to embed a vocabulary in such a way that it encodes a multitude of relationships, and by using these embeddings to derive new relationships from them. In [5], we expand these results to include complex expressions involving logical connectives and quantifiers, with similar conclusions about (recursive versions) these models. These results hold promise for the future of learned distributed representations in applied modeling of logical semantics."}], "references": [{"title": "A brief history of natural logic", "author": ["J. van Benthem"], "venue": "Logic, Navya-Nyaya and Applications: Homage to Bimal Matilal,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "An extended model of natural logic", "author": ["B. MacCartney", "C.D. Manning"], "venue": "Proc. IWCS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JLMR, 12", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Recent progress on monotonicity", "author": ["T.F. Icard", "L.S. Moss"], "venue": "Linguistic Issues in Language Technology, 9(7)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Recursive neural networks for learning logical semantics", "author": ["S.R. Bowman", "C. Potts", "C.D. Manning"], "venue": "arXiv manuscript 1406.1827", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["D. Chen", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proc. ICLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "WordNet", "author": ["C. Fellbaum"], "venue": "Theory and Applications of Ontology: Computer Applications. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. EMNLP", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Natural logic offers a powerful relational conception of semantics: the meanings for expressions are given, at least in part, by their inferential connections with other expressions [1, 2].", "startOffset": 182, "endOffset": 188}, {"referenceID": 1, "context": "Natural logic offers a powerful relational conception of semantics: the meanings for expressions are given, at least in part, by their inferential connections with other expressions [1, 2].", "startOffset": 182, "endOffset": 188}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Using the natural logic of [2] as our formal model, we address this open question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks (NTNs).", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "Its formal properties are now well-understood [4], so it provides a rigorous set of goals for our neural models.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Table 1: The seven natural logic relations of [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "We build embedding-based models using the method of [5], which is centered on the task of labeling a pair of words or sentences with one of a small set of logical relations.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "The more powerful neural tensor network (NTN) version uses an additional third-order tensor parameter to allow for multiplicative interactions between the two inputs [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "For more details on the implementation and training of the layer functions, see [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "However, the relations in WordNet [7] come close and pose the same substantive challenges within a somewhat easier classification problem.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Embeddings were fixed at 25 dimensions and were initialized randomly or using distributional vectors from GloVe [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "In [5], we extend these results to include complex expressions involving logical connectives and quantifiers, with similar conclusions about (recursive versions of) these models.", "startOffset": 3, "endOffset": 6}], "year": 2014, "abstractText": "Natural logic offers a powerful relational conception of meaning that is a natural counterpart to distributed semantic representations, which have proven valuable in a wide range of sophisticated language tasks. However, it remains an open question whether it is possible to train distributed representations to support the rich, diverse logical reasoning captured by natural logic. We address this question using two neural network-based models for learning embeddings: plain neural networks and neural tensor networks. Our experiments evaluate the models\u2019 ability to learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph. The overall positive results are promising for the future of learned distributed representations in the applied modeling of logical semantics.", "creator": "LaTeX with hyperref package"}}}