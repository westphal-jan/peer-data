{"id": "1411.1112", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "Learning of Agent Capability Models with Applications in Multi-agent Planning", "abstract": "One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of each other. An important aspect of these models of other agents is that they are often partial and incomplete. Thus far, there are two common representations of agent models: MDP based and action based, which are both based on action modeling. In many applications, agent models may not have been given, and hence must be learnt. While it may seem convenient to use either MDP based or action based models for learning, in this paper, we introduce a new representation based on capability models, which has several unique advantages. First, we show that learning capability models can be performed efficiently online via Bayesian learning, and the learning process is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). While high degrees of incompleteness in plan execution traces presents learning challenges for MDP based and action based models, capability models can still learn to {\\em abstract} useful information out of these traces. As a result, capability models are useful in applications in which such incompleteness is common, e.g., robot learning human model from observations and interactions. Furthermore, when used in multi-agent planning (with each agent modeled separately), capability models provide flexible abstraction of actions. The limitation, however, is that the synthesized plan is incomplete and abstract.", "histories": [["v1", "Tue, 4 Nov 2014 23:54:07 GMT  (408kb,D)", "http://arxiv.org/abs/1411.1112v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.MA", "authors": ["yu zhang", "subbarao kambhampati"], "accepted": false, "id": "1411.1112"}, "pdf": {"name": "1411.1112.pdf", "metadata": {"source": "CRF", "title": "Learning of Agent Capability Models with Applications in Multi-agent Planning", "authors": ["Yu Zhang", "Subbarao Kambhampati"], "emails": ["yzhan442@asu.edu", "rao@asu.edu"], "sections": [{"heading": "Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Motivating Example", "text": "In this example, we have temporarily recruited human deliverers, and the tasks involve providing different packages to their specific destinations. An agent may not be able to carry a package because the agent may not be strong enough. While the agent may use a trolley to load a package, the agent may not remember to bring a trolley. The agent may visit a local rental office to rent a trolley, but there is a chance that the agent can bring money for the rental. The trolley increases the likelihood that the agent can move the package or does not remember to bring a trolley."}, {"heading": "Related Work", "text": "In MDP-based representation, each node represents a state, and each edge represents a transition between the two interconnected states after the application of an action. In action-based representation, each action has a set of preconditions that must be met in the current state, and a set of effects that indicate changes in the current state when the action is applied. Assuming that the model structures (as in the competence models) are given by the domain expert, the learning task involves only learning the model parameters. Learning is done on the basis of a series of training examples that are traces of the agent whose model is being learned. For MDP-based representation, the agent is often presented as a partially observable MDP (Kaelbling, Littman, and Moore 1996) or POMDP with an underlying Hidden Markov Model (HMM), whereby the learning process presents the transitional probabilities between states as well as the probabilities of observations based on the hidden states."}, {"heading": "Capability Model", "text": "For simplicity, we assume in this paper that the state of the world is defined as a series of Boolean variables that include the agent state. (...) More specifically, for all Xi-X types, Xi (Xi) has = \"true, false.\" First, we formally define the capability for Agent-D, in which we designate the set of variables relevant to the validity. (...) Definition of validity 1 (capability) is an explanation of the existence of plans with initial state I and initial state G that satisfy the following state: Xi-C = true, Xi-D, I [Xi] = false, Xi-D [Xi] = false, Xi [Xi] = false, Xi] = true, Xi-B, G [Xi] = false.In which S [Xi] represents the value of Xi in state S."}, {"heading": "Model Learning", "text": "The parameters (i.e. fij for F) in the performance models of the agents can be learned from the previous tracks of plan execution. Traces of plan execution can be collected each time an operation succeeds or fails. If the values of these parameters are accepted, the conditional distributions in Def. 3 are also known. Definition 4 (Trace of plan execution). A track of plan execution is a sequence of discontinuous state observations, T = {S1, Si,..., Sj, SK}. In the worst case, we only assume that S1 and SK are given. Note that the transition between related state observations is not necessarily the result of a single action (S1, Si,..., SK). If more than the initial and final states are specified in the track, it can be considered as a series of tracks in the form of {S1, Si}, SJ, SJ, SJ, SJ, Scan."}, {"heading": "More about Capability Models", "text": "Theorem 1: Planning with a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using such a model. The proof is simple because planning with a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using a single agent using such a model. In individual cases, the output is just a measurement of probability using a single agent (i.e., the most abstract plan using a different agent)."}, {"heading": "MAP-MM", "text": "In this section and the next, we will focus on how performance models can be used together with other more complete models to refine the synthesized plans, and the settings are similar to our motivational example. In the first problem, which we call multi-agent planning with mixed models (MAP-MM), this formulation is particularly useful for applications involving both human and robot agents. While robot agents are programmed and therefore have predetermined models (we are assuming action-based models here), the models of human agents need to be learned. Therefore, we are using the agent skills model to model human agents and are using the STRIPS action model to let robot agent agent agents model. We anticipate that the models for the human agent-ACs models will already be learned in the following discussions."}, {"heading": "Planning for MAP-MM", "text": "In this section, we will discuss how to perform an A * search process to perform planning for an MAP-MM problem (= q). The current planning state S is composed of three sets of proposals that are designated as T (S), N (S) and U (S). (S) is the set of proposals that are recognized as true in the current state, N (S) is the set of proposals that are recognized as false, and U (S) is the set of proposals that are unknown. Given the current state S, the (centralized) planner can expand it in the next step with the following options: \u2022 Choose an action a (r) so that Pre (a) can apply a human operation to human agents."}, {"heading": "MAP-MMI", "text": "There are many enhancements to the MAP-MM problem. An immediate enhancement is the consideration of the interaction costs between the human and robot agents, which we call MAP-MMI (for interaction-conscious MAP-MM). While MAP-MM is useful for a plan that is most likely to succeed, the MAP-MM formulation is useful when the agents have to execute a conditional plan during the execution phase, and communication is limited. In particular, human agents need to be informed when to perform an operation. In such scenarios, the plan is similar to a conditional plan. Specifically, the plan should specify the planning branches depending on whether an execution of the plan succeeds or fails. Failure would be reported if that operation cannot be executed in the current situation. In MAP-MMI, for every requirement of a human operation (which is planned during the execution phase but occurs during the execution phase), there is a cost associated problem."}, {"heading": "Conclusions", "text": "In this paper, we present a new representation of model agents based on competence models that has several unique advantages. The structures underlying these models are generalized 2-TBNs, and we assume that the (potentially partial) causal relationships between the nodes in the networks are provided by the domain expert. First, we show that learning ability models can be efficiently performed online via Bayesian learning, and that the learning process is robust to a high degree of incompleteness in the tracks of plan execution (e.g., with only start and end states). Furthermore, our models provide flexible abstraction of actions that can potentially improve planning performance, but the limitation is that the synthesized plan is incomplete and abstract. Instead of determining at each step what actions to execute as with other models, the plan only defines a partial state that can be achieved."}], "references": [{"title": "From One to Many: Planning for Loosely Coupled Multi-Agent Systems", "author": ["R.I. Brafman", "C. Domshlak"], "venue": "ICAPS, 28\u201335. AAAI Press.", "citeRegEx": "Brafman and Domshlak,? 2008", "shortCiteRegEx": "Brafman and Domshlak", "year": 2008}, {"title": "A framework for task planning in heterogeneous multi robot systems based on robot capabilities", "author": ["J. Buehler", "M. Pagnucco"], "venue": "AAAI Conference on Artificial Intelligence.", "citeRegEx": "Buehler and Pagnucco,? 2014", "shortCiteRegEx": "Buehler and Pagnucco", "year": 2014}, {"title": "Htn planning: Complexity and expressivity", "author": ["K. Erol", "J. Hendler", "D.S. Nau"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence, 1123\u20131128. AAAI Press.", "citeRegEx": "Erol et al\\.,? 1994", "shortCiteRegEx": "Erol et al\\.", "year": 1994}, {"title": "PDDL2.1: An extension to pddl for expressing temporal planning domains", "author": ["M. Fox", "D. Long"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Fox and Long,? \\Q2003\\E", "shortCiteRegEx": "Fox and Long", "year": 2003}, {"title": "Interactive pomdps: Properties and preliminary results", "author": ["P.J. Gmytrasiewicz", "P. Doshi"], "venue": "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3, AAMAS \u201904, 1374\u20131375. Washington, DC, USA: IEEE Computer Soci-", "citeRegEx": "Gmytrasiewicz and Doshi,? 2004", "shortCiteRegEx": "Gmytrasiewicz and Doshi", "year": 2004}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "J. Artif. Int. Res. 4(1):237\u2013285.", "citeRegEx": "Kaelbling et al\\.,? 1996", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Learning Bayesian networks", "author": ["R.E. Neapolitan"], "venue": "Prentice Hall.", "citeRegEx": "Neapolitan,? 2004", "shortCiteRegEx": "Neapolitan", "year": 2004}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "New York, NY, USA: John Wiley & Sons, Inc., 1st edition.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Relational dynamic influence diagram language (rddl): Language description", "author": ["S. Sanner"], "venue": null, "citeRegEx": "Sanner,? \\Q2011\\E", "shortCiteRegEx": "Sanner", "year": 2011}, {"title": "Memory-bounded dynamic programming for dec-pomdps", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907, 2009\u20132015. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.", "citeRegEx": "Seuken and Zilberstein,? 2007", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Modelbased online learning of pomdps", "author": ["G. Shani", "R.I. Brafman", "S.E. Shimony"], "venue": "In ECML.", "citeRegEx": "Shani et al\\.,? 2005", "shortCiteRegEx": "Shani et al\\.", "year": 2005}, {"title": "Online linear regression and its application to model-based reinforcement learning", "author": ["E.L. Strehl", "M.L. Littman"], "venue": "In Advances in Neural Information Processing Systems 20 (NIPS-07, 737\u2013744.", "citeRegEx": "Strehl and Littman,? 2007", "shortCiteRegEx": "Strehl and Littman", "year": 2007}, {"title": "Causal model progressions as a foundation for intelligent learning environments", "author": ["B.Y. White", "J.R. Frederiksen"], "venue": "Artificial Intelligence 42(1):99 \u2013 157.", "citeRegEx": "White and Frederiksen,? 1990", "shortCiteRegEx": "White and Frederiksen", "year": 1990}, {"title": "Action-model acquisition from noisy plan traces", "author": ["H.H. Zhuo", "S. Kambhampati"], "venue": "Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI\u201913, 2444\u20132450. AAAI Press.", "citeRegEx": "Zhuo and Kambhampati,? 2013", "shortCiteRegEx": "Zhuo and Kambhampati", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Thus far, there are two common representations of agent models: MDP based (Puterman 1994) and action based (Fox and Long 2003), which are both based on action modeling.", "startOffset": 74, "endOffset": 89}, {"referenceID": 3, "context": "Thus far, there are two common representations of agent models: MDP based (Puterman 1994) and action based (Fox and Long 2003), which are both based on action modeling.", "startOffset": 107, "endOffset": 126}, {"referenceID": 12, "context": "One remark is that learning causal relationships to dynamically update the structures of capability models is possible (White and Frederiksen 1990), which is to be considered in future work.", "startOffset": 119, "endOffset": 147}, {"referenceID": 11, "context": "Although researchers have investigated efficient learning methods (Strehl and Littman 2007), in general, learning in such models is still an intense computational task.", "startOffset": 66, "endOffset": 91}, {"referenceID": 8, "context": "One note is that capability model can be considered as a special factored-MDP, which is often represented as a dynamic Bayesian network (Sanner 2011).", "startOffset": 136, "endOffset": 149}, {"referenceID": 13, "context": ", (Zhuo and Kambhampati 2013).", "startOffset": 2, "endOffset": 29}, {"referenceID": 9, "context": "While Dec-POMDP (Seuken and Zilberstein 2007) and other related models (e.", "startOffset": 16, "endOffset": 45}, {"referenceID": 4, "context": ", (Gmytrasiewicz and Doshi 2004)) have been popular, multi-agent planning for action based models has so far been concentrated on planning with deterministic models (Brafman and Domshlak 2008).", "startOffset": 2, "endOffset": 32}, {"referenceID": 0, "context": ", (Gmytrasiewicz and Doshi 2004)) have been popular, multi-agent planning for action based models has so far been concentrated on planning with deterministic models (Brafman and Domshlak 2008).", "startOffset": 165, "endOffset": 192}, {"referenceID": 1, "context": ", (Buehler and Pagnucco 2014), they are still based on action modeling.", "startOffset": 2, "endOffset": 29}, {"referenceID": 6, "context": "We model the capability model of each agent as an augmented Bayesian network (Neapolitan 2004).", "startOffset": 77, "endOffset": 94}, {"referenceID": 6, "context": "Furthermore, we make the causal embedded faithfulness assumption (Neapolitan 2004).", "startOffset": 65, "endOffset": 82}, {"referenceID": 6, "context": ", embedded faithfully (Neapolitan 2004)) in a causal DAG that contains these variables and the hidden variables (i.", "startOffset": 22, "endOffset": 39}, {"referenceID": 6, "context": "While there are exceptions in which this assumption does not hold, their discussion is beyond the scope of this paper (see (Neapolitan 2004) for details).", "startOffset": 123, "endOffset": 140}, {"referenceID": 6, "context": "Refer to (Neapolitan 2004) for arguments for this distribution.", "startOffset": 9, "endOffset": 26}], "year": 2014, "abstractText": "One important challenge for a set of agents to achieve more efficient collaboration is for these agents to maintain proper models of each other. An important aspect of these models of other agents is that they are often partial and incomplete. Thus far, there are two common representations of agent models: MDP based and action based, which are both based on action modeling. In many applications, agent models may not have been given, and hence must be learnt. While it may seem convenient to use either MDP based or action based models for learning, in this paper, we introduce a new representation based on capability models, which has several unique advantages. First, we show that learning capability models can be performed efficiently online via Bayesian learning, and the learning process is robust to high degrees of incompleteness in plan execution traces (e.g., with only start and end states). While high degrees of incompleteness in plan execution traces presents learning challenges for MDP based and action based models, capability models can still learn to abstract useful information out of these traces. As a result, capability models are useful in applications in which such incompleteness is common, e.g., robot learning human model from observations and interactions. Furthermore, when used in multi-agent planning (with each agent modeled separately), capability models provide flexible abstraction of actions. The limitation, however, is that the synthesized plan is incomplete and abstract.", "creator": "TeX"}}}