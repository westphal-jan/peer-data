{"id": "1411.1170", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "An Intelligent Personal Robot Assistant", "abstract": "Recent development in developing humanoid robot poses new challenges to human-machine interaction communication. A major challenge is to develop robots that can behave like and interact with human in the most natural way possible. This paper proposes a system to develop a robot that can receive command, and talk to people in natural language. In addition, the robot can also be \"trained\" to become an expert in sepcific areas to provide expert advice to human-beings. Most important of all, the robot can display emotions through facial expression, speech and gesture so that the interaction process will become more comprehensive and compelling.", "histories": [["v1", "Wed, 5 Nov 2014 07:24:59 GMT  (109kb)", "http://arxiv.org/abs/1411.1170v1", null], ["v2", "Tue, 2 Dec 2014 01:13:31 GMT  (0kb,I)", "http://arxiv.org/abs/1411.1170v2", "This paper has been withdrawn by the author due to a crucial sign error"]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.HC", "authors": ["ong sing goh", "lance fung"], "accepted": false, "id": "1411.1170"}, "pdf": {"name": "1411.1170.pdf", "metadata": {"source": "CRF", "title": "AN INTELLIGENT PERSONAL ROBOT ASSISTANT", "authors": ["Ong Sing Goh"], "emails": [], "sections": [{"heading": null, "text": "The number of recognized phrases of this robot is very limited, only 100 to 650 phrases, and speech phrases over 300 to 3000. This is a hindrance factor in the natural language processing field. Man tends to give commands in different ways. If man is forced to give commands with only limited phrases, then the interaction will not be natural. In addition, the Papero robot is not able to do jobs for man, so he can only be treated as an entertainment robot, and not as a robot that can act as an intelligent assistant. Morpha Projects has also developed a personal robot assistant that can do work, but unfortunately he cannot speak [15]. Therefore, he will perform poorly if his skills are poorly suited for the tasks at hand. Moreover, if the robot has a problem, he has no way to ask for help. Therefore, the robot is not able to perform more meaningful work for man and achieve better results. To address this natural language system, we can communicate through the natural language system so that he can communicate."}, {"heading": "2. SYSTEM DESIGN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Architecture", "text": "This project emphasizes the area of interaction between man and machine and intends to improve the current personal robot assistant by adding natural speech processing functions to it. We use a natural speech processing system called AINI (Artificial Intelligent Neural-Network Identity) [6, 7, 8]. The Chatrobot was tested and is currently used on the Multimedia University website and has participated in the MSC-Expo 2001, the MSC-Asia Pacific Information and Technology Award (MSC-APICTA) and the Multimedia Super Corridor Nationwide Conference 2002. The overall concept of this system can be represented in Figure 1. There are three ways in which man can interact with the robot: by speech (natural speech), gesture (vision) and radio. Man can give commands to the robot by using natural language. This is the most natural way of interaction, because man tends to use language to give commands or receive instructions. In addition to the interaction by speech, the robot can also indicate to the robot to move by hand, for example, the robot to indicate its direction."}, {"heading": "3.2. The Robot", "text": "This is a Pioneer 2-AT off-railing robot from ActivRobotics. Equipped with differential steering, sonar sensor, laser scanner and a colour camera with pan / tilt zoom system, but without robotic arm, gripper and electronic devices / sensors for the interface. An image of this robot shows Figure 2. This navigation robot is powerful, easy to use, reliable but flexible. It has Ethernet-based communication, lasers, DGPS and other autonomous functions. In addition, it has powerful motors and four monster wheels that can reach speeds of eight meters per second and can carry a payload of up to thirty kilograms. This function allows the robot to help its master to transport many things. As the robot is made for outdoor use, it can follow its master to walk on many earth surfaces, including stone, sand or paved surfaces. In addition to detection, this robot is very suitable for communicating with the body in a natural environment."}, {"heading": "3.3. AINI", "text": "The Artificial Intelligent Neural Network Identity (AINI) is a chatterbot system that uses artificial intelligence and natural language processing. AINI means \"I love you\" in Chinese, \"beautiful\" in Malay, and \"eye\" in Arabic. It is based on ALICE (Artificial Linguistic Internet Computer Entity), the entertainment chatterbot developed by Dr. Wallace in 1995. ALICE won the Loebner Prize in 2000 and 2001, a limited Turing test [13] to evaluate the level of \"humanity\" of chatterbots. Knowledge of the robot can be represented in the patterns of the XML specification called AIML (Artificial Intelligent Markup Language). An example of the pattern is as follows: < aiml > < category > < pattern > Hello < pattern > < pattern > < template > < template > Hi, how languages are processed."}, {"heading": "3.4. Speech Recognition", "text": "Speech recognition is one of the input components for this project. It focuses on capturing spoken words from humans and converting the language into text for NLP processing. Dragon Naturally Speaking [4] is used as the main language recognition engine for this system. It has high accuracy, fast performance and an extensive vocabulary that gives our system outstanding recognition. Its powerful speech recognition feature provides true continuous language that allows the user to speak with the robot in a natural way at normal pace without pausing between words. It also provides high recognition accuracy as it has a large vocabulary. New words can be quickly added with the revolutionary Vocabulary Trainer."}, {"heading": "3.5. Emotion Recognition", "text": "In fact, the majority of them are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3. CONCLUSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Benefits", "text": "It can benefit a lot of individual users and organizations. Some potential applications of the robot include: \u2022 Personal robot assistant for visually impaired people. It can also be used as a personal assistant for blind people to guide their direction. predefined route can also be programmed into the robot so that it can safely guide the visually impaired on their way. The robot can also act as their companion to talk and understand their feelings. \u2022 Travel companion. The robot can be used as a travel companion in different places, for example in university, organizations and also famous tourist hotspots. It can promote the places and give explanations to visitors. \u2022 Human companion (family pet). The robot can act as an individual's personal assistant by helping him / her memorize important dates or appointments, schedule and other workplaces. Most importantly of all, the robot can act as a companion or family pet, as it can speak to humans in natural language and understand their emotions."}, {"heading": "3.2. Limitations", "text": "Although there are significant benefits to developing a robot with natural speech processing capabilities, we recognize that there are limitations. First, current speech recognition technology is not very precise. Second, building an effective emotion controller is not easy, because human speech is influenced by social environment and individual background. Some words may sound polite in a society, but insulting in others. Therefore, extensive studies need to be done to develop a successful emotion controller. Finally, when human-robot interaction is adaptable, the flow of control and information through the system fluctuates with time and situation, which could make debugging, validation, and verification problematic because it becomes more difficult to accurately identify an error condition or duplicate an error situation."}], "references": [{"title": "The Role of Expressiveness and Attention in Human-Robot Interaction", "author": ["Allison Bruce", "Illah Nourbakhsh", "Reid Simmons"], "venue": "Robotics and Automation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "A Computational Architecture to Model Human Emotions", "author": ["Arun Chandra"], "venue": "Intelligent Information Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Artificial Intelligent Neural-network Identity (AINI)\u2013The Next Generation of the Virtual Advisor", "author": ["Goh Ong Sing"], "venue": "MSC\u2013Asia Pacific ICT Award (MSC-APICTA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Intelligent Agent for E- Management\" at IEEE International Conference on Artificial Intelligent in Engineering and Technology (ICAIET 2002)", "author": ["Goh Ong Sing", "Teoh Kung Keat"], "venue": "Universiti Sabah Malaysia, Sabah on 17-", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Intelligent virtual doctor system\" at 2nd IEE Seminar on Appropriate Medical Technology for Developing Countries on", "author": ["Goh Ong Sing", "Teoh Kung Keat"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Social Role Awareness in Animated Agents", "author": ["Helmut Prendinger", "Mitsuru Ishizuka"], "venue": "Proceedings of the fifth international conference on Autonomous agents, May 28- June", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Emotional Speech Classification with Prosodic Prameters by using Neural Network.", "author": ["H.Sato", "Y.Mitsukura", "M. Fukumi", "N.Akamatsu"], "venue": "Intelligent Information Systems Conference, The Seventh Australian and New Zealand", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Bimodal Emotion Recognition", "author": ["Liyanage C. De Silva", "Pei Chi Ng"], "venue": "Automatic Face and Gesture Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Stochastic pronunciation modeling from hand-labelled phonetic corpora", "author": ["Michael Riley", "William Byrane", "Michael Finke", "Sanjeev Khudanpur", "Andrej Ljolje", "John McDonouh", "Harriet Nock", "Murat Saraclar", "Charles Wooters", "George Zavaliagkos"], "venue": "Speech Communication", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Collaboration, Dialogue, and Human-Robot Interaction", "author": ["Terrence Fong", "Charles Thorpe", "Charles Baur"], "venue": "International Symposium of Robotics Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Emotion recognition in speech signals: Experimental study, development, and application", "author": ["V. Petrushin"], "venue": "In Proc. ICSLP 2000,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Speech-Driven Cartoon Animation with Emotions.", "author": ["Yan Li", "Feng Lu", "Ying-Qing Xu", "Eric Chang", "Heung-Yeing Shum"], "venue": "Proceedings of the ninth ACM international conference on Multimedia", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}], "referenceMentions": [{"referenceID": 2, "context": "We are using a natural language processing system called AINI (Artificial Intelligent Neural-network Identity) [6, 7, 8].", "startOffset": 111, "endOffset": 120}, {"referenceID": 3, "context": "We are using a natural language processing system called AINI (Artificial Intelligent Neural-network Identity) [6, 7, 8].", "startOffset": 111, "endOffset": 120}, {"referenceID": 4, "context": "We are using a natural language processing system called AINI (Artificial Intelligent Neural-network Identity) [6, 7, 8].", "startOffset": 111, "endOffset": 120}, {"referenceID": 6, "context": "The researches from University of Tokushima try to classify emotions by using pitch [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "Researchers in National University of Singapore try to analyze facial expressions and speech in order to classify emotions [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "com are also trying to classify emotions based on emotions derived from speech signal [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Recent research has identified five main emotions in human: normal, happy, sad, afraid, and angry [20].", "startOffset": 98, "endOffset": 102}], "year": 2002, "abstractText": "Recent development in developing humanoid robot poses new challenges to human-machine interaction communication. A major challenge is to develop robots that can behave like and interact with human in the most natural way possible. This paper proposes a system to develop a robot that can receive command, and talk to people in natural language. In addition, the robot can also be \u201c trained\u201d to become an expert in specific areas to provide expert advice to human-beings. Most important of all, the robot can display emotions through facial expression, speech and gesture so that the interaction process will become more comprehensive and compelling.", "creator": "http://www.fineprint.com"}}}