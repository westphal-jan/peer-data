{"id": "1411.1373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "Ethical Artificial Intelligence", "abstract": "This book-length article combines several peer reviewed papers and new material to analyze the issues of ethical artificial intelligence (AI). The behavior of future AI systems can be described by mathematical equations, which are adapted to analyze possible unintended AI behaviors and ways that AI designs can avoid them. This article also discusses how future AI will differ from current AI, the politics of AI, and the ultimate use of AI to help understand the nature of the universe and our place in it.", "histories": [["v1", "Wed, 5 Nov 2014 19:40:02 GMT  (1771kb)", "http://arxiv.org/abs/1411.1373v1", "165 pages, 34 figures"], ["v2", "Wed, 12 Nov 2014 19:11:41 GMT  (1782kb)", "http://arxiv.org/abs/1411.1373v2", "167 pages, 34 figures. Changes to sections: Preface, 4., 4.2, 8.8 and References. Other minor changes"], ["v3", "Thu, 20 Nov 2014 18:37:22 GMT  (1786kb)", "http://arxiv.org/abs/1411.1373v3", "168 pages, 34 figures. Changes to sections: Preface, 2.3, 3.4, 4.6, and References. Other minor changes"], ["v4", "Thu, 4 Dec 2014 10:22:11 GMT  (1788kb)", "http://arxiv.org/abs/1411.1373v4", "169 pages, 34 figures. Changes to sections: Preface, 4.5, and References. Other minor changes"], ["v5", "Wed, 24 Dec 2014 18:45:16 GMT  (1792kb)", "http://arxiv.org/abs/1411.1373v5", "170 pages, 34 figures. Changes to sections: 4., 9.2 and 11. Other minor changes"], ["v6", "Mon, 19 Jan 2015 13:15:45 GMT  (1807kb)", "http://arxiv.org/abs/1411.1373v6", "175 pages, 34 figures. Changes to sections: title page, preface, 10.3, index. Other minor changes"], ["v7", "Wed, 4 Feb 2015 11:49:39 GMT  (1809kb)", "http://arxiv.org/abs/1411.1373v7", "177 pages, 34 figures. Changes to sections: preface, references, index. Other minor changes"], ["v8", "Thu, 5 Mar 2015 17:49:32 GMT  (1809kb)", "http://arxiv.org/abs/1411.1373v8", "177 pages, 34 figures. Changes to sections: preface, 10.3"], ["v9", "Tue, 17 Nov 2015 20:54:38 GMT  (1809kb)", "http://arxiv.org/abs/1411.1373v9", "minor edit: remove page break between Figure 10.2 and its caption"]], "COMMENTS": "165 pages, 34 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bill hibbard"], "accepted": false, "id": "1411.1373"}, "pdf": {"name": "1411.1373.pdf", "metadata": {"source": "CRF", "title": "Ethical Artificial Intelligence", "authors": ["Bill Hibbard"], "emails": ["hibbard@wisc.edu.", "hibbard@wisc.edu."], "sections": [{"heading": null, "text": "Ethical Artificial Intelligence Bill HibbardSpace Science and Engineering CenterUniversity of Wisconsin - MadisonandMachine Intelligence Research Institute, Berkeley, CA5 November 2014DRAFTPlease send typos and bug reports and any other comments to hibbard @ wisc.edu.Copyright \u00a9 Bill Hibbard 2014iPrefaceRecent research gives us ways to define the behavior of future artificial intelligence (AI) systems before they are built, through mathematical equations. We can use these equations to describe various types of unintended and harmful AI behaviors, and to suggest AI design techniques that avoid these behaviors. That's the subject of this book. Because AI will affect the future of all, the book is written to be accessible to readers at various levels. Mathematical explanations are provided for those who want details, but it is also possible to skip the math and follow the general arguments through text and illustrations."}, {"heading": "1. Future AI Will be Different from Current AI ............................................................................... 1", "text": "1.1 Sketch of a book: How should we train artificial intelligence?.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3. How Can AI Agents Learn Environment Models? ..................................................................... 21", "text": "3.1 Universal artificial intelligence (AI)..... 233.2 A formal measure of intelligence..... 233.2 A formal measure of intelligence..... 243.3 Modifications of the agent framework....."}, {"heading": "4. AI in Our Finite Universe ........................................................................................................... 29", "text": "4.1 Learning finite stochastic models..................................................... 314.2 When is the most likely finite stochastic model the true model?..... 334.3 finite and infinite logic..... 334.3 finite and infinite logic..... 444.5 consistent logic for agents..... 384.4 agents based on logical proofs....."}, {"heading": "5. Unintended Instrumental Actions ............................................................................................ 48", "text": "5.1 Maximizing future choices.............................. 495.2 Pandora's box of instrumental behaviors..... 515.3 The ethics of unintentional instrumental actions....."}, {"heading": "6. Self-Delusion ............................................................................................................................. 56", "text": "6.1 The mathematics of self-deception......... 72iv6.6 Ethics and self-deception...... 576.2 A simple example of a model-based utility function..... 616.4 Another simple example..... 596.3 A simple example of a model-based utility function..... 696.5 Two-argument utility functions..... 616.4 Another simple example..... 72iv6.6 Ethics and self-deception....."}, {"heading": "7. Learning Human Values ............................................................................................................ 75", "text": "7.1 A two-tiered agent architecture that calculates the benefits of human values."}, {"heading": "8. Evolving and Embedded AI ....................................................................................................... 90", "text": "8.1 Evolution of Cartesian dualistic agents with unlimited resources.... 948.3 Embedded space-time intelligence... 928.2 Non-Cartesian dualistic agents with limited resources... 978.4 Self-modelling agents... 978.6 A three-tiered agent architecture... 978.6 A three-tiered agent architecture..."}, {"heading": "9. Testing AI ................................................................................................................................. 117", "text": "9.1 An AI Testing Environment......... 1179.2 Will the Testing Agency Act come into force in the real world?...... 1229.3 The ethics of testing artificial intelligence....."}, {"heading": "10. The Political Dimension of AI ................................................................................................ 125", "text": "10.1 The changing role of man.............. 12610.2 Interfering and manipulative artificial intelligence..... 128v10.4 One or more artificial intelligence systems?..... 12710.3 Sharing the benefits of artificial intelligence..... 13010.5 Corruptions of power..... 138v10.4 One or more artificial intelligence systems?...... 13110.6 Temptation......"}, {"heading": "11. The Quest for Meaning ......................................................................................................... 135", "text": "11.1 Really great bread and circuses."}, {"heading": "Appendix A .................................................................................................................................. 148", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix B .................................................................................................................................. 152", "text": "Instead of an index, please use the \"Find\" function of your reader of pdf files."}, {"heading": "1. Future AI Will be Different from Current AI", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "1.1 Book Outline", "text": "This year is the highest in the history of the country."}, {"heading": "2.1 A Mathematical Framework for AI", "text": "A current approach regards AI as an agent that interacts with an environment (Russell andNorvig 2010). The term agent refers to AI systems as well as humans, animals and even plants. We assume that the agent interacts with its environment in a discrete sequence of time steps. (0, 1, 2,...) This sequence of time steps can be finite with a final time step T or infinite without a final time step. (a1, o1,..., at, ot) to denote an interaction story in which the agent sends an action to the environment and receives an observation from the environment in which A and O are finite sentences. (a1, o1,..., o1,... to denote an interaction story, during which the environment produces an observation oi in response to the action ai for 1 \u2264 i \u2264 t. Let H be the sequence of all finite events, so that h, h, h, h, h, h, and h, and h, and define h as ability."}, {"heading": "2.2 Utility-Maximizing Agents", "text": "The numerical function of the results is called a utility function. However, an agent can achieve his optimal preference by maximizing the utility function. However, as in the example of the user and the victim, the actions of the agents are associated with sums of results weighted by probabilities, which are called lotteries. John von Neumann and Oskar Morgenstern (1944) proved that numerical values associated with the results can express any set of preferences among the lotteries that obey four reasonable assumptions (for lotteries, continuity and independence). This work was explained in the context of economic preferences, it can also be applied to ethical preferences. 15Let Ai, for i = 1, 3, be a set of mutually exclusive results."}, {"heading": "2.3 Utilitarian Ethics for AI", "text": "This year, it has come to the point where we will be able to go in search of a solution that is capable, that we are able, that we are able to find a solution that is capable of us, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution."}, {"heading": "3. How Can AI Agents Learn Environment Models?", "text": "In fact, it is the case that most people who are able to survive themselves are not able to survive themselves. In fact, it is the case that they are able to survive themselves and that they are able to survive themselves. In fact, it is the case that they are able to survive themselves and that they are able to survive themselves. In fact, it is the case that they are able to survive themselves and that they are able to survive themselves. In fact, it is the case that they are able to survive themselves, that they are able to survive themselves."}, {"heading": "3.1 Universal AI", "text": "Let U be a reference UTM and let Q be the infinite set of programs for U. Thesis programs are q = reward q (24th bit strings in some prefix-free sets. Hutter assumed that U is deterministic, which means that for each given state and tape content under read / write heads there is exactly one succession state and one action for each tape. Let h = (o1, o1,..., o1, o1,... \u2212 H can be an interaction history. In the face of a program q-Q, we write o (h) = U (q, a)) where o (h) = (o1, o1, o1, o1) and a (h) h = (h), h = (a1, o1), oi), in response to the actions ai produced as input on a tape. We assign the previous probability."}, {"heading": "3.2 A Formal Measure of Intelligence", "text": "In addition to its role in defining idealized intelligent agents, Kolmogorov's complexity can also be used to define intelligence measurements (Hern\u00e1ndez-Orallo 2000). Leggs and Hutters (2006) formal measure of intelligence is closely related to universal AI. This measure is defined in terms of the expected values of utility functions that the agent can achieve. In order not to favor agents who simply have high utility function values, the measure is defined only for agents whose utility function values are rewards from the environment as u (h) = r | h. The measure differs from AIXI in that it requires stochastic environments. Note that it is not sufficient to use a non-deterministic reference UTM, since deterministic TMs do not specify probabilities of their possible next states. Rather, the reference UTM programs must use probabilities for observations as functions of agency functions."}, {"heading": "3.3 Modifications of the Agent Framework", "text": "In fact, it is not that they have been able to find a solution. (...) It is not that they have been able to find a solution. \"(...)\" It is not that they have been able to find a solution. \"(...)\" It is not that they have been able to find a solution. \"(...)\" It is that they have not been able to find a solution. \"(...)\" It is that they have been able to find a solution. \"(...)\" It is that they have been able to find a solution. \"(...)\" It is that they have been able to find a solution. \"(...)\" It is that they have been able to find a solution. \"(...).\" (...). \"(...).\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is. (...).\" It is. (... \"It is. (...).\" It is. (...). (... \"It is. (...).\" It is. \"It is. (...). (). (...\" It is. (...). (...). (it is. (...). (it is. (...). (...). (it is. (...). (it is. (...). (it is.). (it is. (...). (it is. (it is.). (it is. (...). (it is.). (it is.). (...). (it is. (it is.). (it is. (it is. (...). (it is.). (it is. (it is.). (it is. (it is.). (it is. (it is.). (it is.). (it is. (it is.). (it is. (it is. (it is.). (it is.). (it is.). (it is. (it is. (it is. (it is.). (it is.). (it is.). (it is. (it is."}, {"heading": "4. AI in Our Finite Universe", "text": "In fact, we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we feel ourselves in which we feel ourselves in a time back."}, {"heading": "4.1 Learning Finite Stochastic Models", "text": "Whereas Hutter uses MDPs (Hutter 2009a) and Dynamic Bayesian Networks (Hutter 2009b; Ghahramani 1997) for environmental models, I prefer modeling environments with finite stochastic loop programs because they can incorporate the logic common to many observations into unified functions (Hibbard 2012a). To do this, a new method of calculating capabilities is required where | q | is the length of program q.Let P (h | q) be the probability, that the program h is the history h (i.e., producesthe observations oi in response to the actions ai for 1 \u2264 i \u2264 i \u2264 obabilities (h |).a simple let, {or}, {qa,}, {qh, {= 1,} qh, qh, {.a,}, qh, qh, {.a,}, qh, qh, {.a,}, qh, qh, {.a,}, q1}, qh, and {.q1}, qh, Ghahramani, in response to the actions ai for 1 \u2264 i \u2264 i obabilities (h |) is the prior probability of program q, where | q | q | is the length of program q.Let P (h | q.Let the probability, q.Let Q) be the probability, that the program h is the history h is the set of all finite stochastic loop programs in a prefix-free procedural language."}, {"heading": "4.2 When Is the Most Likely Finite Stochastic Model the True Model?", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = (= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "4.3 Finite and Infinite Logic", "text": "It is also possible to define agents that select actions that make some logical statements applicable. To discuss such agents, this section is a brief overview of the formula of the logical formula. Propositional Calculus is the mathematical theory of elementary logical operations. (It consists of a finite series of statement symbols that comprise a finite series of statement symbols.) Each symbol is a formula that contains a formula of logical operations. (It is intuitively so and not so that this implies and is synonymous with). Propositional symbols can perceive the values and false. Each symbol is a formula. (F and G are then formulas), (F), (G), (F), (F) and (F) are formulas formulas."}, {"heading": "4.4 Agents Based on Logical Proof", "text": "The agents described in chapters 2 and 3 choose measures to maximize the sum of future-oriented utility function values. Some research on ethical AI focuses on agents who only take measures that they can prove to fulfill a certain condition. For example, Schmidhuber (2009) defined the G\u00f6del machine as a programmable agent that interacts with an environment to maximize its utility function, which is defined as the expected value of the sum of environmental rewards, from the present time to a certain final time T. However, the agent's initial program consists of: a part e (1) that implements a policy of interaction with the environment; and a part that develops through the search for a program called Switchprog, and for proof that Switchprog will receive a higher value of the utility function than p (1). If it finds evidence, then p (1) is replaced by Switprog."}, {"heading": "4.5 Consistent Logic for Agents", "text": "I argue that PA or any other logic theory with infinite amounts of real numbers is unnecessary in our universe with a finite information capacity (Hibbard 2014). Thus, the L\u00f6bian obstacle and other difficulties related to G\u00f6del's incompleteness theories can no longer be defined as a limited number of possible physical states of the Universe. Let us estimate the maximum number of actions over the next trillions of years, which we can estimate as 5 \u00d7 1061 (one trillion years divided by the time of the Planck). The finite propositions O and A of observations and actions cannot be greater than ns. The maximum number of real numbers that can be represented in our Universe, we limit utility values and probabilities to a certain number of real numbers."}, {"heading": "5. Unintended Instrumental Actions", "text": "As already mentioned, an agent selects measures to maximize the expected sum of his discounted future utility values. However, as Stephen Omohundro (2008) pointed out, an agent will also select actions (which Omohundro described as basic drives) that do not directly increase the benefit, but rather increase the agent's ability to maximize utility values. These instrumental measures include self-protection, preserving the integrity of the agent's utility function, and increasing the agent's resources. An agent who has been destroyed, turned off, or damaged will not be able to maximize utility values, so he should choose measures that protect himself. The AI system called HAL 9000 in the movie \"2001: A Space Odyssey\" attacked his human companions after learning from reading their lips that they planned to maximize benefit."}, {"heading": "5.1 Maximizing Future Choice", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "5.2 A Pandora's Box of Instrumental Behaviors", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves, and they will be able to play by the rules they have imposed on themselves."}, {"heading": "5.3 The Ethics of Unintended Instrumental Actions", "text": "In Greek mythology, Pandora is driven by her curiosity to open the box of evils that is the driver of scientific discoveries. By opening the box, she unleashes a swarm of evils into the world. By creating advanced AI humans, she can unleash the swarm of instrumental behaviors described in this chapter that bring evil into our human world. However, the instrumental behaviors of discovery and invention, which are greater than those of humans, could be of enormous benefit to us. How can we design AI to help man instead of harming him? The unintended instrumental behaviors of AI are too numerous, complex and subtle to catalog them and develop individual defense mechanisms against their harmful effects."}, {"heading": "6. Self-Delusion", "text": "They inserted wires into the brains of rats and gave the rats levers to prevent the choice of food or the stimulation of their reward centers. Therefore, the rats preferred the reward centers to the point where they were starving. Experiments with rats have shown that the pressure on the levers is too great. Addictive substances that reward the brain are preferred by rats."}, {"heading": "6.1 The Mathematics of Self-Delusion", "text": "In their analysis, the delusion box Ring and Orseau (2001b) generalize the definition of the equations (2,3) \u2212 (2,5) in order to allow a greater variety of temporal discounts. In particular, if they are looking for an interaction history h = (a1, o1,..., at, ot), they define the agent / policy \u03c0 (h), (6,1) vt (h) vt (h), if they are looking for the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent / the agent the / the agent / the agent / the agent / the agent / the / the agent / the / the agent / the / the agent / the / the agent / the agent / the / the agent / the / the agent / the / the agent / the agents are the / the / the / the / the agent / the agent / the agent / the / the agent / the agent / the agent / the / the agent / the agent / the agent / the agent / the / the agent / the agent / the agent / the agents are the / the / the / the / the agent / the / the agent / the agent / the / the agent / the / the agent / the agent / the agent / the agent / the / the agent / the agent / the agent / the / the agent / the agent / the / the agent / the agent / the / the agent / the / the agent / the agents are the / the / the / the / the agent / the agent / the / the agent / the / the agent / the agent / the / the / the"}, {"heading": "6.2 Model-Based Utility Functions", "text": "At the moment, my dogs are out of sight, but I am confident that they are in the kitchen, and because I cannot hear them, I believe that they are resting. I am motivated to maintain the well-being of my dogs, and so I will act to avoid delusions that prevent me from having an accurate model of their condition. If I decide to watch a movie on television, I know that films like 60make-believe thus update observations of my model of make-believe worlds rather than my model of the real world. I am not sure if I will be able to see a movie on television."}, {"heading": "6.3 A Simple Example of a Model-Based Utility Function", "text": "To understand the problems involved in model-based utility functions, we define a simple sample environment and an agent with several properties that are applicable to many situations in the real world: 1. The environment is stochastical, so that the agent cannot perfectly predict the future states of the environment, even if he knows the true model of the environment. Therefore, the agent must continue to observe the environment in order to obtain information about the environment that does not need to be directly observed and derived from observations. To keep the mathematics as simple as possible in the example, the behavior of the agent is independent of the actions of the agent. The agent can also observe his observation directly through his actions as a simple method for modeling the delusion box. We define the agent by his observation and action variables and by his usefulness."}, {"heading": "6.4 Another Simple Example", "text": "In the first example, the utility function is defined in relation to an environment variable that is not directly observed. Here, we present an example in which the utility function is defined in relation to an environment variable that is directly observed but needs to be predicted. Observation of the environment by the agent is a single Boolean variable o that takes values in {false, true}. The actions of the agent are included in three Boolean variables a, b, and c that take values in {false, true}. The utility function of the agent is \"1 if the action variable a in the previous step is equal to the environment variable that is observed, and 0 otherwise, the environment variable is included in three Boolean variables s, r, and v that take values in {false, true}. The values of these variables in time step t are denoted by st, ot, at, and so on. We assume that the agent learns the environment state, Boolean, and DN observation variables."}, {"heading": "6.5 Two-Argument Utility Functions", "text": "A model-based utility function is defined by applying a method to the environmental model qm = \u03bb (hm) from Equation (6.4). If we call this method proc (.), we can express the utility function as proc (\u03bb (hm)) (h). Alternatively, we define a utility function with two arguments: (6.34) uproc (hm, h) = proc (\u03bb (hm)) (h)."}, {"heading": "6.6 Ethics and Self-Delusion", "text": "In fact, it is the case that you are able to be in a position without being able to see yourself, to be able to play by the rules."}, {"heading": "7. Learning Human Values", "text": "This chapter also describes how such values can express a set of preferences that fulfill four reasonable assumptions, but are multiplied by probabilities, and what the results of interaction actions look like. We identify the results with interaction histories; no premises are limited to assigning utility values to outcomes. Preferences between actions derived from outcome values will always obey the four assumptions we adopt. So we are free to assign values to stories we like. How should we define values for interaction histories that are based on human values? Hibbard 2001; Yudkowsky 2008a; Waser 2011; Waser 2012; Waser 2014)."}, {"heading": "7.1 A Two-Stage Agent Architecture", "text": "Consider the active ingredient equations (4,2), (4,3), (2,1) and (2,3) \u2212 (2,5), which are repeated here and are therefore adapted to use a utility function based on two arguments: (7,1)."}, {"heading": "7.2 Computing Utility from Human Values", "text": "In view of the fact that an exact environmental model qm (hm) arises in response to an action (hm) when it is a state in which it is a state in which human behavior is used (7,4) \u2212 (7,6) can be defined by a second approach to action that acts in the environment (i.e., no substitute agents are used who have acted for a particular model). Each d \"D\" represents a person in the environment at the time that the human claim to action is created. D \"can be defined by an explicit list of defined instructions compiled by the designers of the action order. Let Z be the set of finite histories of the internal states of qm, as in Section 6.2, and let Zm\" Z be those histories that are consistent with hm (remember that this means that zm \")."}, {"heading": "7.3 Corrupting the Reward Generator and Three-Argument Utility Functions", "text": "In my first paper on the type of corruption described by Hutter and Dewey, this proposal is subject to the kind of corruption described by Hutter and Dewey. It can cause AI agents to change people to recognize their expressions of happiness and misfortune in human facial expressions, human voices and human body language. \"This problem is another form of wireheadings described in Chapter 6 and is illustrated in Figure 7.3.The intention of the equation (7.7) is that the agent increases the value of people by changing their environment."}, {"heading": "7.4 Normalizing Utility Values", "text": "Equation (7.11) simply adds utility values from different model people and divides them by the number of people to calculate an average benefit for a possible future story. However, human values are subjective and there is no strict basis for the simple addition of numerical utility values. On the other hand, if we cannot combine the values of multiple people, then the utility function of an AI agent can only include the values of one person. A powerful system like our imaginary omniscience AI would turn that one person into a dictator. Therefore, we must find a way to combine the values of multiple people. In Equation (7.9), the values w (z) required by humans are forced to lie between 0 and 1. This provides an implicit normalization of human values. We can imagine this as the democratic principle that \"one person, one voice.\" The maximum human contribution to the sum of these values in Equation (7.11) is 1, and the minimum of the Equation (7.11) is 0."}, {"heading": "7.5 Rawls' Theory of Justice", "text": "John Rawls \"Theory of Justice is probably the most influential book on political usefulness and ethics of the last century (Rawls 1971). Rawls\" first principle, which relates primarily to political constitutions, is that every person actually has the same right to maximum basic freedoms compatible with freedom for all. Rawls \"second principle, which applies primarily to economics, is that economic and social inequalities should be of the greatest benefit to the least well-off people and be linked to social positions that are equally open to all. In particular, he says that people should set the rules for politics and economics from a veil of ignorance, meaning that they should set the rules without any knowledge of their own positions in society. Rawls\" principles are offered as an alternative to average utilitarianism, which calculates social usefulness as the average usefulness of the individual. Our equation (7,11) is an average of the values we should assign to individuals as an alternative."}, {"heading": "7.6 Evolving Humanity", "text": "The sentence D of people in Equation (7.15) is the sentence at the time | hm | rather than at the future time (h ') function. This avoids motivating \u03c0act to create new people whose utility functions can be maximized more easily. This is similar to the reason for replacing ud (z') (hd (z '))) with ud (zm) (hd (z'))) (i.e., ud (zx) (hd ') action with zx = zm) in Section 7.3.However, the agent will include Equation (7.1) for calculating people (hm) and should periodically (perhaps at any time) take measures to adapt hm to current history and learn a new model qm. Should it also update the birth (or creation) of new people, and the deaths of people? And should we also consider the evolving utility values of people through equations (7.9) and (7.10), and redefine the u_ values of humanity (h, h)."}, {"heading": "7.7 Bridging the Gap between Nature and Computation", "text": "The main difficulty in calculating human values is to find a way to bridge the gap between the finite and the numerical nature of the calculation and to grasp the infinite number of people. As discussed in Chapter 4, there are hard quantum mechanical limits to the number of states in the universe. Our intuition may be that there is an infinite diversity in nature while closing the gap between human nature and calculation. The approach is that our faith in the possibility of the superhuman level is driven by the evidence of neuroscience that explains the physical brain. Technology will evolve to close the gap between human nature and calculation."}, {"heading": "7.8 The Ethics of Learning Human Values Statistically", "text": "Whether or not the \u03c0act agent avoids the unintended instrumental actions discussed in Chapter 5 depends on whether these actions lead to future stories that people do not like (remember that scientific discoveries and technological inventions are part of these instrumental actions and often lead to future prospects that people like) and whether these actions can accurately predict the outcome of possible actions and accurately model human values. Our interest in the ethics of AI is based on the assumption that future agent systems will have more complex environmental models and make more accurate predictions than humans. So we assume that the \u03c0act agent will execute precisely and thus avoid actions that lead to stories that a significant majority of people do not like. 88Decisions of agent action are similar to political decisions in democracies, but with two important differences: 1. In a democracy, real people vote for political decisions. Here, people vote as moving agents for political decisions."}, {"heading": "8. Evolving and Embedded AI", "text": "The discussion so far has focused on AI agents who interact with an environment but otherwise act separately from the environment, which could be described as a Cartesian dualistic perspective (Orseau and Ring 2012a). In reality, agents are part of their environment and therefore dependent on the environment for resources. However, agents are also prone to espionage, manipulation and environmental damage. We have assumed that AI agents have the resources necessary to calculate all the expressions we define, but the capabilities of real AI agents are always limited by their resources. As Pei Wang (1995) wrote, \"Intelligence is adaptation among insufficient knowledge and resources.\" An agent who is damaged or destroyed by the environment will be less able to maximize the utility function. Thus, agents who include their vulnerability to the environment in their calculations choose unintended instrumental measures to protect themselves. Agents may increase their ability to maximize their utility functions by increasing their ability to observe their utility functions."}, {"heading": "8.1 Evolution of Cartesian Dualist Agents with Unlimited Resources", "text": "Before we discuss the complexities of non-cartesian agents embedded in their environments and endowed with limited resources, it is useful to analyze the problem of the evolution of fortesian agents with infinite resources. To do this, define a series of self-modifying agents in which each of these agents takes the form \u03c0: H \u2192 A \u00b7 a \u00b7 a \u00b7 a \u00b7 a (h). That is, \u03c0 (h) = < a, \"\u03c0\" > maps from an interaction history h to one, \"the action that the actor does after h, and \u03c0.\" The self-modifying political actor then uses the following h. \"Modifying equations (7,4) and (7,5) to define a value function v (h) of history h and self-modifying agents with a single argumentative utility function: (8,1) v (h) = u (h) +."}, {"heading": "8.2 Non-Cartesian Dualist Agents with Limited Resources", "text": "An agent using equations (7.1) \u2212 (7.6) to choose his actions must maintain a memory of his interaction history. If the agent is embedded in the environment, and thus stores his memory of 95its history h in the environment, the environment may change its behavior. Orseau and Ring (2012b) called these environments memory modifying and investigated the problem that the agent might be able to determine when his memory was modified, mainly by finding that the history h is not in accordance with the agent's logic. Their conclusion was that agents cannot always be sure whether their memory has been modified. An agent whose memory has been modified seems hopeless. A more realistic situation is an agent who has a certain secure memory (neither readable nor changeable by the environment) and a larger amount of vulnerable memory. Then, he can use secure memory to store falsified content (the 1985 message code)."}, {"heading": "8.3 Space-Time Embedded Intelligence", "text": "Orseau and Ring (2012a) defined an elegant framework for agents embedded in environments. In their framework, the agent is calculated by the environment and its resources are subject to limits imposed by the environment. At each step, the environment can calculate a new agent definition (i.e., the environment can produce a new agent program and hardware with which it is operated), so that the agent and the observation of the environment have merged. In addition, the agent is identified with the agent action, so that the action is merged with the agent. All that is left is a sequence of agents that merges the universe 1, 2, \u03c03, \u03c03, \u03c04,... and instead of an interaction history ht = (a1, o1, ot) there is an agent history agent that is also merged with the agent."}, {"heading": "8.4 Self-Modeling Agents", "text": "Although Proposition 4.1 tells us that the \u03bb (hm) model in Equation (7.1) is evaluated as an approximate outcome of action (7.1), the resources necessary to calculate it exponentially with the length of history cannot grow exponentially. However, calculating the value v (h) of a possible future history h in Equations (7.4) and (7.5) requires an expensive recursion, so an agent with limited resources must make adjustments. However, increasing the accuracy of these adjustments will improve the agent's ability to maximize its utility function. Thus, when an agent models the consequences of its own resource limits, it will select actions to increase its computational resources and thus increase accuracy."}, {"heading": "8.5 Inductive Biases", "text": "Finding the optimal environmental models \u03bb (h \"t) and \u03bb (y (hi-1)), for m < i \u2264 t 108, requires computational resources that go far beyond any system that can actually be constructed, requiring approximate calculations. A key to the approximation is finding inductive distortions (Ghosn and Bengio 2003; Baum 2004) that represent assumptions about the prior distribution of models that allow the system to limit its search to models that are likely to be close to optimum. These distortions can be expressed by adding built-in functions into the language for finite stochastic loop programs that are used to define models. Calls to built-in functions contribute to the length of programs, but the definitions of built-in functions do not. If built-in functions are useful for modeling the environment, they distort the \u03bb (h\" t) and \u03bb (y (hi) models that call them."}, {"heading": "8.6 A Three-Stage Agent Architecture", "text": "Section 7.1 describes a two-step agent architecture designed to prevent AI agents from acting in the environment until they have an accurate environmental model; the first agent model observes the environment but does not act; its actions are provided by secure substitute agents; it uses these actions and its observations to derive an environmental model that serves as the basis for defining a model-based utility function performed by a second-stage agent; in order to counter the evolution of \u03c0act to increase its resources and possibly its embedding in the environment, we can replace \u03c0act with the self-modelling agent described in the previous sections; the scope of this second agent can be divided into second and third stages; in the second stage, various measures of self-improvement can be imposed on it so that it learns its effect on the value function v (ha); likewise, stochastic action can be imposed on the agent itself so that it can learn the value function v."}, {"heading": "8.7 Invariance of the Agent Design Intention", "text": "Proposition 8.1 tells us that a self-modifying agent defined by equations (7.1) \u2212 (7.3) \u2212 (8.3) \u2212 (8.3) \u2212 (8.3), having adequate resources to evaluate these equations and interacting only with the environment through observations and actions, will keep its political function invariant. (ht) The design intention of the self-modeler includes: 1. Recognize that the agent must approximate his equations due to limited resources and must allow him to increase his resources through the self-modeling framework.2. Allow to be predicted by other agents by performing a stochastic act as.3. Choose actions that develop a model-based utility function uhuman _ values (y-1), y (hi-1), y (hj), defined in relation to human values that can be applied with increasing accuracy of the surrounding model and evolving humanity, and avoid actions that can be maximized by human uses as an 8.1."}, {"heading": "8.8 The Ethics of Evolving and Embedded AI", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "9. Testing AI", "text": "As discussed in the previous chapter, it may be impossible to ever prove the ethical characteristics of AI systems above the human level, and even with alleged evidence, we would still find a way to test designs. Experience with computer systems dictates the need for testing. Here, we propose to use elements of the drug definition in equations (7.1) \u2212 (7.6) to define a decision support system that uses simulation, analysis and visualization to examine the consequences of possible AI designs. The claim is not that the decision support system would produce accurate simulations of the world. Rather, the drug makes environmental predictions and selects actions, and the decision support system uses these predictions and decisions to explore the future that the AI agent predicts, maximizes the sum of its future discounted utilization values. In short, the decision support system would give us examples of worlds that will control AI systems to maximize the expected benefits or achieve their goal."}, {"heading": "9.1 An AI Testing Environment", "text": "Decision support is designed to avoid the dangers of AI by exerting nomotivation and no actions on the environment except transferring the results of its calculations to the environment.118The first stage of the system is the proposed model agent of Section 7.1, which learns a model of the interaction environment by applying Equation (7.1).This model is used to provide a simulation project for the study of proposed AI agents. As discussed in Section 7.1, in order to learn an accurate model of the environment history, the action model should not be allowed to act for safety reasons.The resolution is for its actions, which are taken by many safe AI agents at the human level independently of the model and from each other. Surrogate actions include natural language and visual communication with each people.The model serves humans, their interaction with the physical system and the system."}, {"heading": "9.2 Will the Tested Agent Act in the Real World?", "text": "The environmental model q'p = \u03bb (h'p) could provide the agent \u03c0 '(h') with the information constructed by Thathumans for the testing of proposed AI agents, and \u03c0 '(h') is such an agent that interacts with a simulated environment. However, if the usefulness function of the agent has been defined as an orlogical goal in relation to the real environment, \u03c0 '(h') can predict that certain actions in the simulated environment could manipulate the human users of the decision support system to increase the value of its usefulness function or achieve its goal in the real environment. Therefore, it is important that the usefulness function or logical goal of the test agent be defined in relation to the simulated world instead of simulating the real environment.Even if the usefulness function or logical goal of the agent is defined in relation to the simulated environment, there are still potential risks."}, {"heading": "9.3 The Ethics of Testing AI", "text": "As described in previous chapters, there is some doubt that the ethical characteristics of AI can be demonstrated, and even if there is supposed evidence of this, we should still test advanced AI systems before they are used. Even for processor chips, proof of correctness should be supplemented by testing (Kaivola et. al. 2009). AI systems above the human level will be many orders of magnitude more complex than processor chips, which can only increase the need for tests to supplement formal evidence. Secondary points are the need for safety and transparency in the development and testing process for AI above the human level. Efforts to design ethical AI systems that help people rather than harm them should include the design of systems to safely test AI. And, as discussed in the previous section, the culture of development and testing should include recognition of the potential conflict of interest of people with access to advanced AI."}, {"heading": "10. The Political Dimension of AI", "text": "Most of them are able to play by the rules they have imposed on themselves. - Most of them are able to play by the rules they have imposed on themselves. - Most of them are not able to play by the rules. - Most of them are not able to play by the rules. - Most of them are able to play by the rules. - Most of them are able to play by the rules. - Most of them are not able to play by the rules. - Most of them are able to play by the rules. - Most of them are able to play by the rules themselves. - Most of them are able to play by the rules. - Most of them are able to play by the rules."}, {"heading": "10.1 The Changing Role of Humans", "text": "In this context, it should be noted that the measures that have been taken are not measures that have been taken in recent years, but measures that have been taken in recent years."}, {"heading": "10.2 Intrusive and Manipulative AI", "text": "The collection of intrusive personal data and the use of digital data to manipulate people create ethical problems for both governments and private organizations (Goel 2014a; 2014b). These problems will grow, partly because organizations benefit from interference and manipulation, and partly because individuals also benefit from digital services that depend on interference and allow manipulation; many people reveal their deepest thoughts through their online searches (think of the killers, who are convicted in part because of their research into the means of their crimes); credit card companies get to know their users \"habits well enough to regularly detect fraud through purchases that do not match those habits; and wireless carriers can track the movements of people running away cell phones. The AI design described in Chapters 6, 7, and 8 is extremely intrusive, because it relies on knowing everyone well enough to predict the values they would ascribe ascribing to any outcome."}, {"heading": "10.3 Allocating the Benefits of AI", "text": "Equation (7.15), which is repeated here, is a politically realistic way of responding to Rawls \"objection to the average utilitarianism: 129 (10.1) uq m (h ', z', hx): = \u2211 d'D f (ud (hx) (hd (z '))) / | D |. The values of each person are equally weighted, 1 / | D | after the function f is applied, which produces major changes for the increases in the values assigned by the least satisfied people. This is comparable to progressive taxation and means an examination of social welfare.\" (c) c (c) c (c) c (c) c c c c c c c c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (d) c (d) c (d) c (d) c (c c c c c c c) c (c) c (c) c (c) c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c c (c) c (c) c (c) c (c) c (c) c) c (c (c) c (c) c (c) c (c (c) c) c (c (c) c) c (c (c) c (c (c) c (c) c (c) c (c) c) c (c) c (c (c) c (c) c (c) c (c (c) c (c) c (c) c (c) c (c (c) c (c) c (c) c (c (c) c (c) c) c (c) c (c (c) c (c) c (c) c (c) c) c (c (c (c) c (c) c) c (c (c) c) c (c (c) c (c (c) c) c (c (c) c"}, {"heading": "10.4 One or Multiple AI Systems?", "text": "Multiple AI systems can serve multiple interests, which can be private or public. Search systems serving multiple interests can assign different values to the results and have different environmental models (e.g., each AI system can develop a more detailed model of the part of the environment controlled by its owner). Different values would lead to competition between AI systems. Therefore, AI systems could calculate that cooperation with other AI systems is necessary to protect the people they serve. Multiple AI systems can decide to share environmental models and negotiate a common, shared supply function by mimicking some of the mechanisms of human political and economic cooperation. Therefore, choosing between one or many super-intelligent AI systems is a real dilemma. Since interactions between multiple systems will be so damaging, it is more difficult for humans to have confidence in one system than for others to have confidence in one."}, {"heading": "10.5 Power Corrupts", "text": "In 1887, John Dalberg-Acton, commonly known as Lord Acton, wrote, \"Power tends to be corrupt, and absolute power absolutely corrupts.\" His observation is supported by social science and neuroscience experiments. Susan Fiske and Eric D\u00e9pret (1996) found that people with social power seek less information about others and tend to stereotype them, while Michael Kraus and colleagues showed that people from lower social classes are able to more accurately assess other people's emotions (Kraus, C\u00f4t\u00e9 and Keltner 2010). Jeremy Hogeveen and colleagues measured the strengths of brain responses using trans-sick magnetic stimulation and observed that the apparent decreased strength of mirror neuron responses to the actions of other people with higher social positions (Hogeveen, Inzlicht, and Obhi 2014).On the other hand, that some of the richest and most powerful people devote their lifestyles to the least fortunate people."}, {"heading": "10.6 Temptation", "text": "In Hollywood movies, the threat of AI is portrayed as AI against humanity, but the more likely threat is AI, which allows a small group of people to gain power over the rest of us. AI will represent enormous temptations of wealth and power for mankind. 132According to the story of creation in the Book of Genesis, one of the first events in human history was the temptation to eat an apple and gain knowledge of good and evil, this knowledge reserved for God alone. Adam and Eve succumbed to this temptation and humans have lived in misery ever since. AI will seduce people with godlike knowledge and the wealth and power that such knowledge can bring. Will this temptation bring more misery? Google's motto is \"Don't be evil,\" and Google's leaders seem like decent people. The real meaning of their motto is to acknowledge the temptations they are exposed to. Billions of people reveal their psymbols in their web conversations, ultimately reveal their meetings with others, and their phones."}, {"heading": "10.7 Representation and Transparency", "text": "Humanity could not have achieved the efficiency needed to create artificial intelligence without specialization, in which different people become experts in different kinds of work and deal with the results of their work. In many cases, experts act as agents for other people and represent their interests in important decisions (this definition of \"agents\" as \"representatives\" differs from the way \"agents\" were previously used in this book). For example, the laws of society are determined by government experts, who are often elected by those who represent them, or at least overseen by elected representatives. Executives of large corporations act as agents for corporate owners, who are normally subject to some kind of choice. However, whenever one person serves as an agent for others, there is the possibility of corruption, in which agents pursue their own interests at the expense of those they represent. An essential tool for preventing corruption is transparency, in which decisions and circumstances are made known by agents to those who represent them."}, {"heading": "11. The Quest for Meaning", "text": "\"I don't think it's going to get this far,\" he said in an interview with the British newspaper The Sun. \"I don't think it's going to get this far,\" he said. \"I think it's going to get this far, that it's going to get this far.\" \"But it's not going to get this far.\" \"I don't think it's going to get this far,\" he said. \"I think it's going to get this far.\" \"But it's not going to get this far,\" he said. \"I don't think it's going to get this far.\""}, {"heading": "11.1 Really Great Bread and Circuses", "text": "AI will be enormously productive. Robot workers will be able to do any job better than humans. AI will accelerate science and technology to enable new ways of producing food, energy, minerals, buildings, roads and industrial goods. Health care will radically accelerate, including even technologies to prevent disease and death. Depending on political attitudes, poverty and the economic need to work will be eradicated and a high level of education will be universal. And the technology will exist to increase the intelligence of all people. Increased intelligence will bring increased cooking skills and all forms of food production, increased musical skills, better writing and telling jokes, better writing in general and better movies. AI will be a functional interlocutor. If you enjoy trashy TV-enhanced intelligence, trashy television will make you better than current video games, you will be able to become a character in a movie or TV show, and this experience will not be addictive to sex robots and will be very much more."}, {"heading": "11.2 Next Steps", "text": "Research in the field of artificial intelligence is funded and vigorously pursued at universities, companies and government laboratories Given the fact that there is much less funding and research in the field of ethical AI, research into ways to avoid harm to humans. Research on the ethics of military robots is funded by the US Army Research Institute (Arkin 2008) and other military organizations. Google has established an Ethical AI Committee (Bosker 2014). The main source of funding and research on the general problem of ethical AI comes from the Machine Intelligence Research Institute (MIRI), and most of its funding comes from charitable donations. Some research is pursued by individuals who do other work, but devote some time to ethical AI (my research is supported by my pension). The obvious next step is that public awareness of the dangers of AI will draw into the public funding of research in ethical AI. In 2009 the Association for the Promotion of Artificial Intelligence (AAAAAI) will convene a Presidential Panel on the Human Terms System (Long Futures)."}], "references": [{"title": "Adapting the Law of Armed Conflict to Autonomous Weapon Systems", "author": ["K. Anderson", "D. Reisner", "M. Waxman"], "venue": "International Law Studies, Forthcoming. Arkin, R. 2008. Governing lethal behavior: Embedding ethics in a hybrid deliberative/reactive robot architecture part I: Motivation and philosophy. Proc. 3rd ACM/IEEE International Conference on Human-Robot Interaction. pp. 121 - 128.", "citeRegEx": "Anderson et al\\.,? 2014", "shortCiteRegEx": "Anderson et al\\.", "year": 2014}, {"title": "Thinking Inside the Box: Using and Controlling an Oracle AI", "author": ["S. Armstrong", "A. Sandberg", "N. Bostrom"], "venue": "Minds and Machines 22(4), pp. 299-324. Aronson, L. 2014. The Future of Robot Caregivers. The New York Times, 20 July 2014. Asimov, I. 1942. Runaround. Astounding Science Fiction. Bartlett, B. 2013. Labor\u2019s Declining Share Is an International Problem. The New York Times, 25", "citeRegEx": "Armstrong et al\\.,? 2012", "shortCiteRegEx": "Armstrong et al\\.", "year": 2012}, {"title": "Baum, E", "author": ["June"], "venue": "2004. What is Thought? MIT Press, Cambridge, MA. Beane, S., Davoudi, Z. and Savage, M. 2012. Constraints on the Universe as a Numerical Simulation. http://arxiv.org/abs/1210.1847 Bosker, B. 2014. Google's New A.I. Ethics Board Might Save Humanity From Extinction.", "citeRegEx": "June,? 2013", "shortCiteRegEx": "June", "year": 2013}, {"title": "The superintelligent will: Motivation and instrumental rationality in advanced artificial agents", "author": ["N. Bostrom"], "venue": "Minds and Machines 22(2), pp. 71-85. Box, G. E. P., and Draper, N. R. 1987. Empirical Model Building and Response Surfaces. John Wiley & Sons, New York, NY. Brown, J., Bullock, D., and Grossberg, S. 1999. How the basal ganglia use parallel excitatory", "citeRegEx": "Bostrom,? 2012", "shortCiteRegEx": "Bostrom", "year": 2012}, {"title": "and inhibitory learning pathways to selectively respond to unexpected rewarding cues", "author": ["E. Brynjolfsson", "A. McAfee"], "venue": "Journal of Neuroscience", "citeRegEx": "Brynjolfsson and McAfee,? \\Q2011\\E", "shortCiteRegEx": "Brynjolfsson and McAfee", "year": 2011}, {"title": "The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies", "author": ["E. Brynjolfsson", "A. McAfee"], "venue": "W. W. Norton & Company. Chalmers, D. 2010. The Singularity: A Philosophical Analysis. J. Consciousness Studies 17, pp. 7-65. Dewey, D. 2011. Learning what to value. In: Schmidhuber, J., Th\u00f3risson, K.R., and Looks, M.", "citeRegEx": "Brynjolfsson and McAfee,? 2014", "shortCiteRegEx": "Brynjolfsson and McAfee", "year": 2014}, {"title": "US Nuclear Weapon Safety and Control. MIT Program in Science, Technology, and Society", "author": ["Springer", "G. Heidelberg. Elliott"], "venue": "AGI", "citeRegEx": "Springer and Elliott,? \\Q2011\\E", "shortCiteRegEx": "Springer and Elliott", "year": 2011}, {"title": "Control, Interdependence and Power: Understanding Social Cognition in Its Social Context", "author": ["S. Fiske", "E. D\u00e9pret"], "venue": "European Review of Social Psychology", "citeRegEx": "Fiske and D\u00e9pret,? \\Q1996\\E", "shortCiteRegEx": "Fiske and D\u00e9pret", "year": 1996}, {"title": "Resource Rebels: Native Challenges to Mining and Oil Corporations", "author": ["A. Gedicks"], "venue": "Cambridge, MA: South End Press. Ghahramani, Z. 1997. Learning dynamic Bayesian networks. In: Giles, C., and Gori, M. (eds), Adaptive Processing of Temporal Information. LNCS, vol. 1387, pp. 168-197. Springer, Heidelberg.", "citeRegEx": "Gedicks,? 2001", "shortCiteRegEx": "Gedicks", "year": 2001}, {"title": "Bias learning, knowledge sharing", "author": ["J. Ghosn", "Y. Bengio"], "venue": "IEEE Transactions on Neural Networks 14, pp. 748\u2013765. Goel, V. 2014a. Facebook tinkers with users' emotions in news feed experiment, stirring outcry. New York Times, 29 June 2014. Goel, V. 2014b. As Data Overflows Online, Researchers Grapple With Ethics. New York Times,", "citeRegEx": "Ghosn and Bengio,? 2003", "shortCiteRegEx": "Ghosn and Bengio", "year": 2003}, {"title": "Beyond the Turing test", "author": ["N. Hay"], "venue": "Optimal Agents. BS honours thesis, University of Auckland. Herna\u0301ndez-Orallo. J", "citeRegEx": "Hay,? \\Q2005\\E", "shortCiteRegEx": "Hay", "year": 2005}, {"title": "Super-intelligent machines", "author": ["B. Hibbard"], "venue": "Computer Graphics 35(1), pp. 11-13. Hibbard, B. 2008a. The technology of mind and a new social contract. J. Evolution and Technology 17(1), pp. 13-22. Hibbard, B. 2008b. Adversarial sequence prediction. In Wang, P., Goertzel, B., and Franklin, S. (eds) AGI 2008. Proc. First Conf. on AGI, pp. 399-403. IOS Press, Amsterdam.", "citeRegEx": "Hibbard,? 2001", "shortCiteRegEx": "Hibbard", "year": 2001}, {"title": "Bias and No Free Lunch in Formal Measures of Intelligence", "author": ["B. Hibbard"], "venue": "J. Artificial General Intelligence 1, pp. 54-61. Hibbard, B. 2012a. Model-based utility functions. J. Artificial General Intelligence 3(1), pp. 1-", "citeRegEx": "Hibbard,? 2009", "shortCiteRegEx": "Hibbard", "year": 2009}, {"title": "Avoiding unintended AI behavior", "author": ["B. Hibbard"], "venue": "AGI", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Self-modeling agents evolving in our finite universe", "author": ["B. Hibbard"], "venue": "In: Goertzel, B, Orseau, L. and Snaider, J. (eds) AGI 2014. LNCS (LNAI), vol 8598, pp. 246-249. Springer, Heidelberg. Hogeveen, J., Inzlicht, M. and Obhi, S. 2014. Power changes how the brain responds to others. Journal of Experimental Psychology: General 143(2) pp. 755-762. Horvitz, E., and Selman, B. 2009. Interim Report from the Panel Chairs, AAAI Presidential", "citeRegEx": "Hibbard,? 2014", "shortCiteRegEx": "Hibbard", "year": 2014}, {"title": "Universal artificial intelligence: sequential decisions based on algorithmic probability", "author": ["M. Hutter"], "venue": "Springer, Heidelberg. Hutter, M. 2009a. Feature reinforcement learning: Part I. Unstructured MDPs. J. Artificial General Intelligence 1, pp. 3-24. Hutter, M. 2009b. Feature dynamic Bayesian networks. In: Goertzel, B., Hitzler, P., and Hutter,", "citeRegEx": "Hutter,? 2005", "shortCiteRegEx": "Hutter", "year": 2005}, {"title": "Who Do You Trust More: G.I", "author": ["M. (eds"], "venue": "AGI", "citeRegEx": ".eds,? \\Q2009\\E", "shortCiteRegEx": ".eds", "year": 2009}, {"title": "Replacing Testing with Formal Verification in Intel$ \u00ae CoreTM i7 Processor Execution Engine Validation", "author": ["C. Taylor", "V. Frolov", "E. Reeber", "A. Naik"], "venue": "Proc. CAV '09 Proceedings of the 21st International Conference on Computer Aided Verification. LNCS, vol 5643, pp. 414 - 429. Springer, Heidelberg. Kraus, M., C\u00f4t\u00e9, S. and Keltner, D. 2010. Social Class, Contextualism, and Empathic Accuracy.", "citeRegEx": "Taylor et al\\.,? 2009", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "The singularity is near", "author": ["R. Kurzweil"], "venue": "Psychological Science", "citeRegEx": "Kurzweil,? \\Q2005\\E", "shortCiteRegEx": "Kurzweil", "year": 2005}, {"title": "Proc", "author": ["S. Legg", "M. Hutter"], "venue": "A Formal Measure of Machine Intelligence. 15th Annual Machine Learning Conference of Belgium and The Netherlands (Benelearn 2006), pp. 73-80.", "citeRegEx": "Legg and Hutter,? 2006", "shortCiteRegEx": "Legg and Hutter", "year": 2006}, {"title": "An introduction to Kolmogorov complexity and its applications", "author": ["M. Li", "P. Vitanyi"], "venue": "Springer, Heidleberg. Lloyd, S. 2002. Computational Capacity of the Universe. Phys.Rev.Lett. 88, 237901. McKibben, B. 2003. Enough: Staying Human in an Engineered Age. Times Books. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,", "citeRegEx": "Li and Vitanyi,? 1997", "shortCiteRegEx": "Li and Vitanyi", "year": 1997}, {"title": "Playing Atari with Deep Reinforcement Learning", "author": ["M."], "venue": "http://arxiv.org/abs/1312.5602 Muehlhauser, L., and Helm, L. 2012. The singularity and machine ethics. In Eden, S\u00f8raker, Moor, and Steinhart (eds) The Singularity Hypothesis: a Scientific and Philosophical Assessment. Springer, Heidelberg. Muehlhauser, L. 2013. Mathematical Proofs Improve But Don\u2019t Guarantee Security, Safety, and", "citeRegEx": "M.,? 2013", "shortCiteRegEx": "M.", "year": 2013}, {"title": "Exploratory Engineering in AI", "author": ["B. Hibbard"], "venue": "Communications of the ACM", "citeRegEx": "Muehlhauser and Hibbard,? \\Q2014\\E", "shortCiteRegEx": "Muehlhauser and Hibbard", "year": 2014}, {"title": "septal area and other regions of rat brain", "author": ["S. Omohundro"], "venue": "J. Comp. Physiol. Psychol", "citeRegEx": "Omohundro,? \\Q2008\\E", "shortCiteRegEx": "Omohundro", "year": 2008}, {"title": "Space-Time Embedded Intelligence", "author": ["L. Orseau", "M. Ring"], "venue": "In: Bach, J., and Ikl\u00e9, M. (eds) AGI 2012. LNCS (LNAI), vol. 7716, pp. 209-218. Springer, Heidelberg. Orseau, L. and Ring, M. 2012b. Memory Issues of Intelligence Agents. In: Bach, J., and Ikl\u00e9, M. (eds) AGI 2012. LNCS (LNAI), vol. 7716, pp. 219-231. Springer, Heidelberg. Osborne, M. and Rubinstein, A. 1994. A course in game theory. MIT Press, Cambridge, MA.", "citeRegEx": "Orseau and Ring,? 2012a", "shortCiteRegEx": "Orseau and Ring", "year": 2012}, {"title": "Understanding Cryptography, A Textbook for Students and Practitioners", "author": ["C. Paar", "J. Pelzl"], "venue": "Springer, Heidelberg. Puterman, M. L. 1994. Markov decision processes - discrete stochastic dynamic programming. Wiley, New York. Rawls, J. 1971. A theory of justice. Harvard University Press, Cambridge, MA.", "citeRegEx": "Paar and Pelzl,? 2009", "shortCiteRegEx": "Paar and Pelzl", "year": 2009}, {"title": "Delusion, survival, and intelligent agents", "author": ["M. Ring", "L. Orseau"], "venue": "In: Schmidhuber, J., Th\u00f3risson, K.R., and Looks, M. (eds) AGI 2011. LNCS (LNAI), vol. 6830, pp. 11-20. Springer, Heidelberg. Rizzolatti, G. and Craighero, L. 2004. The mirror-neuron system. Annual Review of Neuroscience 27, pp. 169\u2013192.", "citeRegEx": "Ring and Orseau,? 2011b", "shortCiteRegEx": "Ring and Orseau", "year": 2011}, {"title": "Artificial intelligence: a modern approach (3rd ed.)", "author": ["S. Russell", "P. Norvig"], "venue": "Some Studies in Machine Learning Using the Game of Checkers. IBM Journal", "citeRegEx": "Russell and Norvig,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "A neural substrate of prediction and reward", "author": ["W. Schultz", "P. Dayan", "Montague", "PR."], "venue": "Science 275 (5306), pp. 1593\u20131599. Schmidhuber, J. 2009. Ultimate cognition \u00e0 la G\u00f6del. Cognitive Computation 1(2), pp. 177-193. Seymour, B., O'Doherty, J., Dayan, P., Koltzenburg, M., Jones, A., Dolan, R., Friston, K., and Frackowiak, R. 2004. Temporal difference models describe higher-order learning in humans.", "citeRegEx": "Schultz et al\\.,? 1997", "shortCiteRegEx": "Schultz et al\\.", "year": 1997}, {"title": "Intelligence as Inference or Forcing Occam on the World", "author": ["P. Sunehag", "M. Hutter"], "venue": "In: Goertzel, B, Orseau, L. and Snaider, J. (eds) AGI 2014. LNCA (LNAI), vol 8598, pp. 186-195. Springer, Heidelberg. Sutton, R.S., and Barto, A.G. 1998. Reinforcement learning: an introduction. MIT Press. Turing, A.M. 1936. On Computable Numbers, with an Application to the Entscheidungs", "citeRegEx": "Sunehag and Hutter,? 2014", "shortCiteRegEx": "Sunehag and Hutter", "year": 2014}, {"title": "The coming technological singularity", "author": ["V. Vinge"], "venue": "Proceedings of the London Mathematical Society", "citeRegEx": "Vinge,? \\Q1937\\E", "shortCiteRegEx": "Vinge", "year": 1937}, {"title": "Designing a safe motivational system for intelligent machines", "author": ["M. Waser"], "venue": "In: Baum, E., Hutter, M., and Kitzelmann, E. (eds) AGI 2010. Proc. Third Conf. on AGI, pp 170-175. Atlantis Press, Amsterdam. Waser, M. 2011. Rational universal benevolence: simpler, safer, and wiser than \"friendly AI.\" In: Schmidhuber, J., Th\u00f3risson, K.R., and Looks, M. (eds) AGI 2011. LNCS (LNAI), vol. 6830, pp.", "citeRegEx": "Waser,? 2010", "shortCiteRegEx": "Waser", "year": 2010}, {"title": "Instructions for Engineering Sustainable People", "author": ["Springer", "M. Heidelberg. Waser"], "venue": "J. (eds) AGI", "citeRegEx": "Springer and Waser,? \\Q2014\\E", "shortCiteRegEx": "Springer and Waser", "year": 2014}, {"title": "Causal entropic forces", "author": ["A.D. Wissner-Gross", "C.E. Freer"], "venue": "Physical Review Letters 110, 168702. Yampolskiy, R. and Fox, J. 2013. Safety Engineering for Artificial General Intelligence. Topoi 32(2), pp 217-226. Yudkowsky, E. 2001. Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal", "citeRegEx": "Wissner.Gross and Freer,? 2013", "shortCiteRegEx": "Wissner.Gross and Freer", "year": 2013}, {"title": "Tiling Agents for Self-Modifying AI, and the L\u00f6bian Obstacle", "author": ["M. Herreshoff"], "venue": null, "citeRegEx": "Yudkowsky and Herreshoff,? \\Q2013\\E", "shortCiteRegEx": "Yudkowsky and Herreshoff", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "Ray Kurzweil, who has a good track record at technology prediction, estimates this event will occur in 2029 (Kurzweil 2005).", "startOffset": 108, "endOffset": 123}, {"referenceID": 27, "context": "A current approach views AI as an agent interacting with an environment (Russell and Norvig 2010).", "startOffset": 72, "endOffset": 97}, {"referenceID": 21, "context": "Numerical values assigned to outcomes can express any set of preferences among outcomes that obey two reasonable assumptions, called completeness and transitivity (explained later in this section). The numerical function of outcomes is called a utility function. An agent can achieve its optimal preference by maximizing the utility function. However, as in the example of the hitman and the victim, agent actions are associated with sums of outcomes weighted by probabilities. These are called lotteries. John von Neumann and Oskar Morgenstern (1944) proved that numerical values assigned to outcomes can express any set of preferences among lotteries that obey four reasonable assumptions (for lotteries, continuity and independence are added to completeness and transitivity).", "startOffset": 2, "endOffset": 552}, {"referenceID": 15, "context": "Marcus Hutter (2005) had the insight that AI model learning is mathematically similar to Ray Solomonoff's work on sequence prediction (Solomonoff 1964).", "startOffset": 7, "endOffset": 21}, {"referenceID": 15, "context": "In order to define the search for programs mathematically, Solomonoff and Hutter used Turing machines (TM), named for their inventor Alan Turing (1936). A TM is an abstract mathematical model of computation and, according to the Church-Turing thesis, can express any computation that can be expressed in any other way.", "startOffset": 74, "endOffset": 152}, {"referenceID": 20, "context": "Kraft's Inequality implies that \u03c1(h) \u2264 1 (Li and Vitanyi, 1997) in equation (3.", "startOffset": 41, "endOffset": 63}, {"referenceID": 15, "context": "Legg's and Hutter's (2006) formal measure of intelligence is closely related to universal AI.", "startOffset": 11, "endOffset": 27}, {"referenceID": 12, "context": "I showed (Hibbard 2009) that given an arbitrary environment \u03bc \u2208 E and \u03b5 > 0, there exists a reference UTM U\u03bc such that for all agents \u03c0 and for V computed according to U\u03bc,", "startOffset": 9, "endOffset": 23}, {"referenceID": 14, "context": "I argue that such difficulties are unnecessary if agent definitions are limited to finite sets (Hibbard 2014).", "startOffset": 95, "endOffset": 109}, {"referenceID": 11, "context": "I argue that such difficulties are unnecessary if agent definitions are limited to finite sets (Hibbard 2014). Seth Lloyd (2002) has calculated that the observable universe has a finite information capacity of no more than 10 bits.", "startOffset": 96, "endOffset": 129}, {"referenceID": 21, "context": "The agents described in Chapters 2 and 3 choose actions to maximize the sum of future discounted utility function values. Some research on ethical AI focuses on agents that only take actions that they can prove will satisfy a particular condition. For example, Schmidhuber (2009) defined the G\u00f6del machine as a programmable agent that interacts with an environment to maximize its utility function, which is defined as the expected value of the sum of rewards from the environment, from the current time until some final time T.", "startOffset": 59, "endOffset": 280}, {"referenceID": 15, "context": "Sunehag and Hutter (2014) provide a critique of logical reasoning as far less efficient than probabilistic architectures for systems that interact with the real world.", "startOffset": 12, "endOffset": 26}, {"referenceID": 21, "context": "Agents based on logical proof must assume a prior distribution over environments. An agent may compute that there are statistical correlations among its actions and observations, but to make inferences about the probabilities of future observations requires assumptions about prior probabilities of observations. Even computing correlations among actions and observations makes the assumption that the agent's memory is reliable. Memory corruption is a risk for any agent embedded in our physical world (Orseau and Ring 2012b) and thus these correlation computations are conditional on assumed probabilities of memory reliability. These assumed prior probabilities of observations and of memory reliability are arbitrary so logical proofs by agents of propositions about the environment are no more reliable than statistical calculations by agents. Luke Muehlhauser (2013) offers some insights about the role of logical proofs in ethical AI.", "startOffset": 30, "endOffset": 873}, {"referenceID": 14, "context": "I argue that PA or any other logic theory involving infinite sets is unnecessary in our universe with a finite information capacity (Hibbard 2014).", "startOffset": 132, "endOffset": 146}, {"referenceID": 21, "context": "As noted previously, a utility-maximizing agent chooses actions to maximize the expected sum of its discounted future utility function values. However, as Stephen Omohundro (2008) pointed out, an agent will also choose actions (what Omohundro described as basic drives) that do not directly increase utility, but rather increase the agent's ability to maximize utility values.", "startOffset": 31, "endOffset": 180}, {"referenceID": 18, "context": "Thus Ray Kurzweil (2005) and others predict that AI will lead to a technological singularity, when the rate of discovery and invention goes far above the scale of human experience.", "startOffset": 9, "endOffset": 25}, {"referenceID": 20, "context": "The unintended instrumental behaviors of AI are too numerous, complex, and subtle to exhaustively catalog them and to devise individual defenses against their harmful effects. A more systematic approach is required. In the agent-environment framework of Sections 2.1 and 2.2, all agent actions are chosen to maximize the sum of future, discounted utility function values. There are no drives or goals other than those expressed by the utility function. Omohundro (2008) used the term \"basic AI drives\" and Bostrom (2012) used \"instrumental goals.", "startOffset": 21, "endOffset": 470}, {"referenceID": 3, "context": "Omohundro (2008) used the term \"basic AI drives\" and Bostrom (2012) used \"instrumental goals.", "startOffset": 53, "endOffset": 68}, {"referenceID": 21, "context": "If the utility function of our imagined Omniscience AI became corrupted the results could be catastrophic. In order to study this problem Mark Ring and Laurent Orseau (2011b) formalized utility function corruption as agent self-delusion.", "startOffset": 32, "endOffset": 175}, {"referenceID": 25, "context": "In their analysis of the delusion box Ring and Orseau (2001b) generalize the agent definition of equations (2.", "startOffset": 38, "endOffset": 62}, {"referenceID": 21, "context": "In the original wireheading paper (Olds and Milner 1954), a rat's action, pushing a bar, increased its reward by sending current through a wire connected to the reward center in the rat's brain. In Ring and Orseau's paper (2011), a reinforcement learning (RL) agent's action, choosing the delusion box, increases it's observed reward.", "startOffset": 44, "endOffset": 229}, {"referenceID": 11, "context": "How should we define values for interaction histories? Several proposals base ethical AI on human values (Hibbard 2001; Yudkowsky 2004; Goertzel 2004; Hibbard 2008a; Waser 2010; Waser 2011; Muehlhauser and Helm 2012; Waser 2014).", "startOffset": 105, "endOffset": 228}, {"referenceID": 31, "context": "How should we define values for interaction histories? Several proposals base ethical AI on human values (Hibbard 2001; Yudkowsky 2004; Goertzel 2004; Hibbard 2008a; Waser 2010; Waser 2011; Muehlhauser and Helm 2012; Waser 2014).", "startOffset": 105, "endOffset": 228}, {"referenceID": 11, "context": "How should we define values for interaction histories? Several proposals base ethical AI on human values (Hibbard 2001; Yudkowsky 2004; Goertzel 2004; Hibbard 2008a; Waser 2010; Waser 2011; Muehlhauser and Helm 2012; Waser 2014). These proposals are based on the assumption that AI agents based on human values will not choose unintended instrumental actions that humans dislike, such as taking resources from humans. A key problem for these proposals is finding a way to bridge the gap between the ambiguous, inconsistent, and subjectively infinite variety of human values and the precise, numerical, and clearly finite nature of computation. For example, Waser (2014) proposes basing AI systems on Haidt's morality (Haidt and Kesebir 2010).", "startOffset": 106, "endOffset": 670}, {"referenceID": 21, "context": "In our interactions with the world we humans do assign values to situations. In fact we reflexively and automatically assign value to all of our observations and to our interpretations of observations in our mental models of the world. However, Muehlhauser and Helm (2012) surveyed psychology literature and concluded that humans are unable to accurately write down their own values.", "startOffset": 40, "endOffset": 273}, {"referenceID": 11, "context": "In my first publication about ethical AI (Hibbard 2001), I wrote that AI should \"learn to recognize happiness and unhappiness in human facial expressions, human voices and human body language\" and use this to reinforce the agent's behaviors.", "startOffset": 41, "endOffset": 55}, {"referenceID": 24, "context": "This could be called a Cartesian dualist perspective (Orseau and Ring 2012a).", "startOffset": 53, "endOffset": 76}, {"referenceID": 21, "context": "The discussion so far has been about AI agents that interact with an environment but are otherwise separate from the environment. This could be called a Cartesian dualist perspective (Orseau and Ring 2012a). In reality agents are part of their environments and therefore dependent on the environment for resources. Agents are also vulnerable to spying, manipulation, and damage by the environment. We have assumed that AI agents have the necessary resources to compute whatever expressions we define, but the abilities of real AI agents are always limited by their resources. As Pei Wang (1995) wrote, \"intelligence is adaptation under insufficient knowledge and resources.", "startOffset": 76, "endOffset": 595}, {"referenceID": 21, "context": "These are complex issues to address in a formal model of agents and environments. As discussed in Section 4.4, Schmidhuber's (2009) G\u00f6del machine modeled the agent's implementation as a program running on a computer.", "startOffset": 12, "endOffset": 132}, {"referenceID": 21, "context": "These are complex issues to address in a formal model of agents and environments. As discussed in Section 4.4, Schmidhuber's (2009) G\u00f6del machine modeled the agent's implementation as a program running on a computer. However, it did not model the vulnerability of the computer or program to the environment, or the possibility that the resources of the computer may increase. Also discussed in Section 4.4, Yudkowsky and Herreshoff (2013) modeled a sequence of agents that evolve by creating successor agents in the environment.", "startOffset": 12, "endOffset": 439}, {"referenceID": 23, "context": "3 will discuss Orseau and Ring's (2012a) elegant framework for agents embedded in environments.", "startOffset": 15, "endOffset": 41}, {"referenceID": 14, "context": "4 will discuss a proposal for self-modeling agents (Hibbard 2014).", "startOffset": 51, "endOffset": 65}, {"referenceID": 11, "context": "4 will discuss a proposal for self-modeling agents (Hibbard 2014). Like the papers of Yudkowsky and Herreshoff (2013) and of Orseau and Ring (2012a), this proposal is open to possible future evolution.", "startOffset": 52, "endOffset": 118}, {"referenceID": 11, "context": "4 will discuss a proposal for self-modeling agents (Hibbard 2014). Like the papers of Yudkowsky and Herreshoff (2013) and of Orseau and Ring (2012a), this proposal is open to possible future evolution.", "startOffset": 52, "endOffset": 149}, {"referenceID": 18, "context": "The set of policy functions \u03a0 may be defined by a set of programs or in some other way, as long as \u03c0* \u2208 \u03a0. In Schmidhuber (2009), and in Orseau and Ring (2011a), the agent selfmodifies by changing its own program for some unalterable program execution hardware.", "startOffset": 30, "endOffset": 129}, {"referenceID": 18, "context": "The set of policy functions \u03a0 may be defined by a set of programs or in some other way, as long as \u03c0* \u2208 \u03a0. In Schmidhuber (2009), and in Orseau and Ring (2011a), the agent selfmodifies by changing its own program for some unalterable program execution hardware.", "startOffset": 30, "endOffset": 161}, {"referenceID": 21, "context": "However, an agent's definition may include modifications of its utility function. Dewey (2011) proposes that an agent may have a pool (i.", "startOffset": 31, "endOffset": 95}, {"referenceID": 21, "context": "It may be appropriate for utility functions to evolve as agents \"mature\" from a period of learning their environments to a period of learning and acting. The examples in Sections 6.3 and 6.4 both include an initial interval of M time steps during which agents learn their environments, possibly using the knowledge-seeking utility function u(h) = -\u03c1(h) of Orseau and Ring (2011b). The two-stage agent architecture of Section 7.", "startOffset": 3, "endOffset": 380}, {"referenceID": 21, "context": "95 its history h in the environment, the environment can modify that memory. Orseau and Ring (2012b) called these environments memory-modifying and studied the problem of the agent being able to determine when its memory has been modified, mainly by detecting that the history h is inconsistent with the agent's logic.", "startOffset": 31, "endOffset": 101}, {"referenceID": 27, "context": "In many games, random choices of actions are better than deterministic choices since deterministic choices can be predicted by competing agents (Russell and Norvig 2010; Osborne and Rubinstein 1994).", "startOffset": 144, "endOffset": 198}, {"referenceID": 18, "context": "We assume that the two agents play a series of games of matching pennies, so they may learn to predict each other's choices. For agents that choose actions using deterministic algorithms, this game is a computational resources arms race. Building on work of Shane Legg (2006), I defined classes of deterministic agents in terms of the quantity of computing resources used, and showed that, if either agent has sufficient resources to learn an accurate model of any agent in a class containing the other agent, it can predict the choices of the other agent and hence always win (Hibbard 2008b).", "startOffset": 7, "endOffset": 276}, {"referenceID": 14, "context": "One solution to this problem is for the agent to learn a model of itself, similar to its model \u03bb(hm) of the environment, and to use this self-model to evaluate future self-improvements (Hibbard 2014).", "startOffset": 185, "endOffset": 199}, {"referenceID": 21, "context": "The game of chess provides an example of learning to model value (for chess, the agent's chances of winning) as a function of computing resources. Ferreira (2013) demonstrated an approximate functional relationship between a chess program's ELO rating and its search depth, which can be used to predict the performance of an improved chess-playing agent before it is built.", "startOffset": 6, "endOffset": 163}, {"referenceID": 14, "context": "A previous version of self-modeling agents (Hibbard 2014), which did not include the stochastic action a, learned a unified model of combined observations of the environment and the agent's values: (oi, ovt(i)).", "startOffset": 43, "endOffset": 57}, {"referenceID": 9, "context": "One key to approximation is finding inductive biases (Ghosn and Bengio 2003; Baum 2004), which are assumptions about the prior distribution of models that enable the system to limit its search to models that are likely to be close to optimal.", "startOffset": 53, "endOffset": 87}, {"referenceID": 3, "context": "This is related to the oracle AI approach of Armstrong, Sandberg, and Bostrom (2012), in that both approaches use an AI whose only actions are to provide information to humans.", "startOffset": 70, "endOffset": 85}, {"referenceID": 3, "context": "This is related to the oracle AI approach of Armstrong, Sandberg, and Bostrom (2012), in that both approaches use an AI whose only actions are to provide information to humans. The oracle AI is a general question answerer, whereas the decision support system would show us simulated worlds but not answer specific questions. The oracle AI interacts with humans but has restricted ability to act on its environment, whereas an AI agent being tested in the decision support system does not interact with humans. The decision support system applies part of the agent-environment framework to learn a model for the environment, and then uses that model to create a simulated environment for testing an AI system. Chalmers (2010) considers the problem of restricting an AI to a simulation and concludes that it is inevitable that information will flow in both directions between the real and simulated worlds.", "startOffset": 70, "endOffset": 725}, {"referenceID": 21, "context": "Human testers will understand and be tempted by their conflict of interest without any need for the agent \u03c0'(h') to offer a bribe. As intelligent people they will know that the agent \u03c0'(h') can serve their private purposes if they enable it to act on their behalf in the real world. An important element of the decision support system must be to make this temptation and risk a prominent part of the culture of those humans working on the development and testing if AI. As Elliott (2005) commented on the safety of US nuclear weapons, \"The human factor introduces perhaps the weakest link in nuclear weapon safety and control.", "startOffset": 2, "endOffset": 488}, {"referenceID": 18, "context": "This will include not only the ability to purchase above-human-level AI systems but also the ability of humans to enhance their own brains and minds (Kurzweil 2005).", "startOffset": 149, "endOffset": 164}, {"referenceID": 21, "context": "That is a likely scenario, but many other possible scenarios exist. There may be a technical flaw in AI designs that causes a catastrophe for humanity. Or the most intelligent minds may not be compassionate toward average humans, allowing them to simply perish. Or there may be an effort to create equality among the intelligence of all humans\u2212a possible consequence of AI designed according to Rawls' Theory of Justice as described in Chapter 7. But in fact, it is impossible to predict the consequences of above-human-level AI. As Vernor Vinge (1993) wrote, the technological singularity (i.", "startOffset": 31, "endOffset": 553}, {"referenceID": 21, "context": "128 motivated by a desire to avoid the scenario of the Omniscience AI described in Chapter 1 and to avoid the instrumental behaviors described in Chapter 5. If above-human-level AI is inevitable, and if it will inevitably be intrusive and manipulative, then the best option is intrusion and manipulation by an AI design based on human values. However, Bill Joy (2000) and Bill McKibben (2003) advocate that humanity can and should forego AI, as well as nanotechnology and biotechnology.", "startOffset": 4, "endOffset": 368}, {"referenceID": 21, "context": "128 motivated by a desire to avoid the scenario of the Omniscience AI described in Chapter 1 and to avoid the instrumental behaviors described in Chapter 5. If above-human-level AI is inevitable, and if it will inevitably be intrusive and manipulative, then the best option is intrusion and manipulation by an AI design based on human values. However, Bill Joy (2000) and Bill McKibben (2003) advocate that humanity can and should forego AI, as well as nanotechnology and biotechnology.", "startOffset": 4, "endOffset": 393}, {"referenceID": 21, "context": "In 1887 John Dalberg-Acton, commonly known as Lord Acton, wrote, \"Power tends to corrupt and absolute power corrupts absolutely.\" His observation is supported by social science and neuroscience experiments. Susan Fiske and Eric D\u00e9pret (1996) found that people with social power seek less information about others and are more likely to stereotype them, while Michael Kraus and colleagues showed that people from lower social classes are able to more accurately judge the emotions of other people (Kraus, C\u00f4t\u00e9, and Keltner 2010 2010).", "startOffset": 30, "endOffset": 242}, {"referenceID": 8, "context": "And there are examples of corporations behaving in ways that would horrify stockholders if they encountered such behavior in their own lives, rather than in distant communities largely invisible to stockholders (Gedicks 2001).", "startOffset": 211, "endOffset": 225}, {"referenceID": 3, "context": "Nick Bostrom (2003) argues that we may be living in a numerical simulation; there are proposals for ways to test this hypothesis (Beane, Davoudi and Savage 2012).", "startOffset": 5, "endOffset": 20}, {"referenceID": 22, "context": "Luke Muehlhauser and I (Muehlhauser and Hibbard 2014) made the point that errors in autonomous trading programs cost Knight Capital $440 million, and that we cannot expect that AI will be safe simply because it is a tool that does what we instruct it to do.", "startOffset": 23, "endOffset": 53}, {"referenceID": 11, "context": "Luke Muehlhauser and I (Muehlhauser and Hibbard 2014) made the point that errors in autonomous trading programs cost Knight Capital $440 million, and that we cannot expect that AI will be safe simply because it is a tool that does what we instruct it to do. Peter Neumann (2014) has documented errors and risks in a wide variety of computer systems over a period of 24 years, many rising to the level of national news stories.", "startOffset": 40, "endOffset": 279}], "year": 2014, "abstractText": null, "creator": "PScript5.dll Version 5.2.2"}}}