{"id": "1411.1420", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "Eigenvectors of Orthogonally Decomposable Functions", "abstract": "In this paper we formulate the framework of recovering a hidden orthonormal basis given access to a certain \"Basis Encoding Function\". We describe the class of Basis Encoding Functions (BEF), such that their local maxima on the unit sphere are in one-to-one correspondence with the basis elements. This description relies on a certain \"hidden convexity\" property of these functions. A number of theoretical and practical problems of recent interest can be interpreted as recovering a hidden basis from potentially noisy observations. Specifically, we show how our simple and general framework applies to Independent Component Analysis (ICA), tensor decompositions, spectral clustering and Gaussian mixture learning.", "histories": [["v1", "Wed, 5 Nov 2014 21:07:20 GMT  (64kb)", "http://arxiv.org/abs/1411.1420v1", "39 pages"], ["v2", "Mon, 11 May 2015 16:08:28 GMT  (84kb,D)", "http://arxiv.org/abs/1411.1420v2", "45 pages"], ["v3", "Tue, 3 Nov 2015 17:22:20 GMT  (89kb,D)", "http://arxiv.org/abs/1411.1420v3", "56 pages"], ["v4", "Tue, 24 May 2016 18:10:04 GMT  (93kb,D)", "http://arxiv.org/abs/1411.1420v4", "61 pages"], ["v5", "Sat, 26 Nov 2016 20:03:30 GMT  (104kb,D)", "http://arxiv.org/abs/1411.1420v5", "77 pages"]], "COMMENTS": "39 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mikhail belkin", "luis rademacher", "james voss"], "accepted": false, "id": "1411.1420"}, "pdf": {"name": "1411.1420.pdf", "metadata": {"source": "CRF", "title": "Learning a Hidden Basis Through Imperfect Measurements: An Algorithmic Primitive", "authors": ["Mikhail Belkin"], "emails": ["mbelkin@cse.ohio-state.edu", "lrademac@cse.ohio-state.edu", "vossj@cse.ohio-state.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.14 20v1 [cs.LG] 5 NWe describe a new algorithm, the \"gradient iteration,\" for the verifiable restoration of the hidden basis. We offer a complete theoretical analysis of the gradient iteration both for the exact case and for the case when the observed function is a disturbance of the \"true\" underlying BEF. In both cases, we show convergence and complexity limits of polynomial dimensions and other relevant parameters, such as the perturbation size. Our perturbation results can be considered a very general nonlinear version of the classic Davis-Kahan theorem for eigenvectors of perturbations of symmetric matrices. Furthermore, we show that in the exact case, the algorithm converges superlinearly and provides conditions relating to the degree of convergence with the properties of the basic coding function. Our algorithm can be considered a generalization of the classical symmetry method for the inheritance analysis."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Problem description and main results", "text": "More formally, a function on a closed standard ball F: B (0, 1) \u2192 R is defined by \"contrast functions\" gi: [\u2212 1, 1] \u2192 R like: F (u): = m \u00b2 i = 1gi (< u, zi >). (2) We refer to F as a base coding function (BEF) with the corresponding tuples {(gi, zi) | i [m]. The goal is to recover the hidden base vectors zi for i \u00b2 [m] until the given weighting access to F and its gradients is given. We assume that d \u00b2 BEGI = BEGI = BEGI or BEGI = \u221a i \u00b2 x \u00b2 is trivial. We consider only the contrast functions gi \u00b2 C (2) ([\u2212 1, 1] which fulfill the following assumptions: 4. BEGI = BEGI is either odd or equal to BEGI = \u221a i \u00b2."}, {"heading": "2.1 Examples of algorithmic problems solvable via basis encoding functions", "text": "We now describe a prototypical version of the method that works in two phases [5, 21, 23, 27]. The first phase, spectral embedding, constructs a similarity diagram based on the characteristics of the data and then embeds the data in Rd (where d is the number of clusters) using the lower d eigenvectors of the Laplace matrix of the similarity graph. The second phase, in which the embedded data is based on a variation of k-mean algorithms. A key aspect in justifying spectral clustering is the following observation: If the graph has connected components, then a pair of data points is assigned either to the same vector if they are in the same connected component or mapped to orthogonal vectors if they are in different connected components."}, {"heading": "2.2 Summary of the results", "text": "& & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; &"}, {"heading": "3 Extrema structure of the optimization framework", "text": "The optimal structure of F relies heavily on a hidden convexity structure, which is implied by Assumption A2. To better understand this structure, we define the functions hi: [\u2212 1, 1] and [2]. We can therefore use either strictly convex or strictly concave each of these functions. The functions hi have the following properties: Lemma 3.1). Let s: 0, \u2212 1} be a character value. Let s: 0, \u2212 1} be a character value. Let s if it is a character value + = 1 if it is convex."}, {"heading": "4 Interpreting the gradient iteration", "text": "In this section we show that we have two essential interpretations, firstly as an adaptive form of gradient ascent (Section 4.1) and secondly as a generalization of gradient ascent (Section 4.2). These dual interpretations closely link gradient iteration to other methods of finding the maxima of a function3. Both interpretations of gradient iteration are easiest to understand when a specific form of BEF. Definition 4.1. A BEF F (u) = 1 Gi = 1 Gi = 1 Gi = 1 Gi = 1 Gi = 1 Gi is referred to as a positive base coding function (PBEF) when x 7 \u2192 Gi (x).A PBEF has several beautiful properties that are not shared by all BEFs."}, {"heading": "4.1 Gradient iteration as adaptive gradient ascent", "text": "For the rest of this section, let's assume F to be a PBEF. < u > u > u > u u (u > u > u) u > u (u > u) u > u (u > u) u (u > u) u (u > u) u (u > u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u (u) u) u (u) u (u) u) u (u) u) u (u) u (u) u, u (u) u (u) u (u) u (u), u (u) u (u) u (u) u (u) u (u) u (u), u (u), u (u (u), u (u), u (u (u), u (u), u (u (u), u (u), u (u), u (u), u (u), u (u), u (u), u (u), u (u), u (u), u (u), u (u), u (u), u (u (u), u (u), u (u), u (u), u (u), u (u), u (u (u), u (u), u (u), u (u), u (u), u (u (u), u (u), u (u), u (u), u (u), u (u (u), u (u), u), u (u (u), u (u), u), u (u), u (u (u), u), u (u (u), u (u (u), u), u (u (u), u (u), u (u), u (u (u), u (u), u (u), u (u ("}, {"heading": "4.2 Gradient iteration extends the power method", "text": "We now show how our gradient iteration relates to the classical method for matrix eigenvectors. (...) We show how our gradient iteration relates to the classical method for matrix eigenvectors. (...) We show that we can restore the new starting points in the orthogonal complementarity of eigenvectors. (...) We show how our gradient iteration can be converted to the classical method for matrix eigenvectors. (...) We show that we can restore the new starting points in the orthogonal complementarity of eigenvectors. (...) We show how we can restore the new starting points in the orthogonal complementation of eigenvectors. (...) We show that we can restore the new starting points in the orthogonal complementation of eigenvectors. (...) We show how we can restore the new starting points in the orthogonal complementation of eigenvectors."}, {"heading": "5 Fixed point structure of the gradient iteration", "text": "In this section, we assume a formal analysis of the gradient iteration algorithm. In particular, in section 5.1, we show that the distinguishable basic directions [z1],... [zm] are the only stable fixed points of the gradient iteration actualization (theorem 2.2). Furthermore, in section 5.2, we show that the convergence to these stable fixed points is fast (theorem 2.3). In the course of this section, we assume that F (u) = \u2211 mi = 1 gi (ui) is a PBEF, unless otherwise specified, and that the functions hi for i [d] with respect to F are defined as in section 3, unless otherwise specified. We analyze the associated gradient iteration function G on domain Qd \u2212 1 +. It is sufficient to analyze this function on the basis of conclusion 4.4."}, {"heading": "5.1 Fixed point stability", "text": "We now proceed with the proof of theory 2,2. \u2212 u The proof for this theory has two main parts, namely the proof that the statements z1,. < u \u00b2 n stable attraction points of G | Qd \u2212 1 + and the proof that all other fixed points of G | Qd \u2212 1 + are unstable. < u (The first part is the simpler part, and we prove it first. \u2212 u \u2212 n > is the elementary force of uProposition 5.1. The statements z1,., zm are attraction points of G | Qd \u2212 1 +.lt. The proof for the statement is based on the analysis of the properties of the power method iteration in a small neighborhood of the hidden base direction.Proof. It is sufficient to show that z1 is an attraction point of G | Qd \u2212 1 +."}, {"heading": "5.2 Fast convergence of the gradient iteration", "text": "We now proceed with the proof of theorem 2.3. Stability analysis is based on the variable u changing 7 \u2192 u < 2 > (which led to the definitions of hi for i) due to the fact that for each i [m], gi (x1 / 2) is in fact convex to [0, 1]. The rapid convergence of gradient iteration algorithms is based on a more general variation of variable u 7 \u2192 u < r > where r 2, and in particular gi (x1 / r) is assumed to be [0, 1] for each i [m] convex. We encode this potentially stronger connectivity within our BEF by extending the definition of hi's from section 3 to the more general family of maps."}, {"heading": "6 A robust gradient iteration algorithm", "text": "In Section 5 we saw that the only stable fixed points of gradient iteration correspond to the hidden base elements zi and that the convergence to these points is a super-linear one. However, the analysis is incomplete for two reasons: First, it is possible (though probably unlikely) that the gradient iteration converges to an unstable fixed point and is therefore unable to regenerate all the base elements."}, {"heading": "6.1 Controlling \u2016P0u\u2016", "text": "The following two examples let us show that the number of base elements we need to recover. It is necessary that we recover the number of base elements. It is necessary that we recover the number of base elements. It is necessary that we recover the number of base elements. It is necessary that we recover the number of base elements. It is necessary that we recover the number of base elements. It is necessary that we recover the number of base elements. It is necessary that we recover the number of base elements. It is necessary that we recover the number of base elements."}, {"heading": "6.2 Progress of the gradient iteration", "text": "The basic idea behind our robust gradient iteration is that we have the first part of these zero trapping effects.Lemma 6.8 The basic idea behind our robust gradient iteration is that we have the first part of this zero trapping effects.Lemma 6.8 The basic idea behind our robust gradient iteration is that we have the second part of the zero trapping effects.Lemma 6.8 The basic idea behind our robust gradient iteration is that we have the first part of this zero trapping effects.Lemma 6.8 The basic idea behind our robust gradient iteration is that we have the first part of this zero trapping effects.Lemma 6.8 The basic idea behind our zero trapping effects.Lemma 6.8 The basic idea behind our robust gradient iteration is that we have the first part of this zero trapping effects.Lemma 6.8 The basic idea behind our zero trapping effects.The basic idea behind our zero trapping Iteration is that we have the first part of our zero trapping effects.Lemma 6.8 The basic idea behind our rugged gradient iteration is that we have the first part of our zero trapping effects.Lemma 6.8"}, {"heading": "6.3 Gradient iteration proof of robustness", "text": "We have all the technical tools we need to prove that we are able to reconstruct the hidden base elements. (...) We have then shown that we will be able to reconstruct the hidden base elements to reconstruct the basic theoretical results of this section. (...) We show that we can reconstruct any theorem before their proof.Theorem 6.14. (...) We have the hidden base elements to reconstruct the basic theoretical results of this section (...). (...) We have the conjecture before their proof.Theorem 6.14. (...) We have the hidden base elements 7cmin 10240. (...) 2m3 / 2d2. (...) We are not negative that we have a permutation of [...], and let them s1. (...). (...)"}, {"heading": "A Chart of notation", "text": "For the reader's reference, we list notations used throughout the paper. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "B Function bounds", "text": "In this section we set some useful limits for (cmin, cmax) -rugged BEFs and PBEFs.Lemma B.1. For a (cmin, cmax) -rugged BEF, we have the following limits for each U-B (0, 1): 1. 2mcmin, cmax).Proof. We first have F (u): 1. 2cmax, 2cmax, 2c, 2c, 2c, 2c, 2c, 2c, 2h, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c, 2c"}, {"heading": "C Error bounds on eigenvalues and eigenspaces", "text": "As part of our error analysis of ROBUSTGIRECOVERY in Section 6, we require limits on the error in estimating eigenvalues and eigenvectors of HF (u) that allows access to HF (u). The following inequality is a known version of Weyl's inequality for eigenvalues of the matrix. However, let the eigenvalues of A, A, and H be symmetrical (or generally hermetic) n matrices so that A + H. Let the eigenvalues of A, A, and H be symmetrical with [3], [4], [4], and eigenvalues of A, A, and H. Let the eigenvalues of A, A, and H lie."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper we formulate the framework of recovering a hidden orthonormal basis given access to a<lb>certain \u201cBasis Encoding Function\u201d. We describe the class of Basis Encoding Functions (BEF), such that<lb>their local maxima on the unit sphere are in one-to-one correspondence with the basis elements. This<lb>description relies on a certain \u201chidden convexity\u201d property of these functions. A number of theoretical<lb>and practical problems of recent interest can be interpreted as recovering a hidden basis from potentially<lb>noisy observations. Specifically, we show how our simple and general framework applies to Independent<lb>Component Analysis (ICA), tensor decompositions, spectral clustering and Gaussian mixture learning.<lb>We describe a new algorithm, \u201cgradient iteration\u201d, for provable recovery of the hidden basis. We<lb>provide a complete theoretical analysis of Gradient Iteration both for the exact case as well as for the<lb>case when the observed function is a perturbation of the \u201ctrue\u201d underlying BEF. In both cases we show<lb>convergence and complexity bounds polynomial in dimension and other relevant parameters, such as<lb>perturbation size. Our perturbation results can be considered as a very general non-linear version of<lb>the classical Davis-Kahan theorem for eigenvectors of perturbations of symmetric matrices. In addition<lb>we show that in the exact case the algorithm converges superlinearly and give conditions relating the<lb>degree of convergence to properties of the Basis Encoding Function. Our algorithm can be viewed as a<lb>generalization of the classical power iteration method for eigenanalysis of symmetric matrices as well as<lb>a generalization of power iterations for tensors. Moreover, the Gradient Iteration algorithm can be easily<lb>and efficiently implemented in practice.", "creator": "LaTeX with hyperref package"}}}