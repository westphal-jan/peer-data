{"id": "1411.1623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "A Hybrid Recurrent Neural Network For Music Transcription", "abstract": "We investigate the problem of incorporating higher-level symbolic score-like information into Automatic Music Transcription (AMT) systems to improve their performance. We use recurrent neural networks (RNNs) and their variants as music language models (MLMs) and present a generative architecture for combining these models with predictions from a frame level acoustic classifier. We also compare different neural network architectures for acoustic modeling. The proposed model computes a distribution over possible output sequences given the acoustic input signal and we present an algorithm for performing a global search for good candidate transcriptions. The performance of the proposed model is evaluated on piano music from the MAPS dataset and we observe that the proposed model consistently outperforms existing transcription methods.", "histories": [["v1", "Thu, 6 Nov 2014 14:18:39 GMT  (54kb)", "http://arxiv.org/abs/1411.1623v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siddharth sigtia", "emmanouil benetos", "nicolas boulanger-lewandowski", "tillman weyde", "artur s d'avila garcez", "simon dixon"], "accepted": false, "id": "1411.1623"}, "pdf": {"name": "1411.1623.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 1.16 23v1 [cs.LG] 6 Nov 2Index Terms - Recursive Neural Networks, Polyphonic Music Transcription, Music Language Models"}, {"heading": "1. INTRODUCTION", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2. RECURRENT NEURAL NETWORKS", "text": "An RNN, when used as a generative model, defines a distribution over a sequence z in the following way: P (z) = T = 1P (zt | At) (1), where At \u2261 {z\u03c4 | \u03c4 < t} is the sequence history at a time t. The hidden state of an RNN with a single layer of hidden units is defined by the following recurring relationship: ht = \u03c3 (Wzhzt \u2212 1 + Whhht \u2212 bh) (2), where Wzh is the weights from the inputs at t \u2212 1 to the hidden units at t, Whh are the recurring weights between hidden units at t \u2212 1 and t and bh are the hidden biases. The output vector at the time zt is achieved in the following way: zt = f (Whzht + bz)."}, {"heading": "3. HYBRID ARCHITECTURE", "text": "Architecture is a generative graphical model that generalizes the HMM architecture by generalizing predictions at a given time t on all previous predictions \u03c4 < t as opposed to HMM. Hybrid architecture factorizes the common probability of the sequence of acoustic vectors x and their corresponding terms z in the following way: P (z, x) = P (z1.. xT) (4) = P (z1) P (z1) T (zt | 2) P (zt | z).At) P (xt | zt). (5) In the above factorization, the symbolic prediction of terms P (zt | At) is slightly logical."}, {"heading": "4. INFERENCE", "text": "In hybrid architecture, predicting zt at time t of the entire historyAt sequence is a high priority due to the RNN language model, forcing successive frames to be coherent and thus performing a temporal smoothing. In addition to time smoothing, an accurate language model can impose musical rules and limitations on output transcriptions. While decoding, proceeding in a greedily chronological manner, results are suboptimal because the sequence history has not been optimally determined. At the same time, an exhaustive search for the globally optimal sequence requires intractable, since each non-lef-based node in the search chart has 2N descendants. Instead, we conduct a global search for the most likely sequence using the beam search, a tree search algorithm that tracks only the most promising paths at any depth."}, {"heading": "5. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Acoustic Modelling", "text": "We are experimenting with the use of 3 different neural network architectures to learn relevant features from spectrogram inputs. First, we are using a deep, forward-facing neural network (DNN) as an acoustic classifier. DNNs are currently state-of-the-art in acoustic modeling in language [9] and have been successfully applied to the transcription of music in the past [13, 3]. DNNs \"ability to learn a hierarchy of increasingly complex features makes them an ideal choice for acoustic modeling. Although they are powerful frame-level classifiers, DNN output is often loud because they do not take into account dependencies between input frames. To avoid this problem, we are also experimenting with the use of an RNN acoustic model. DNNs base their predictions on a single frame of input, while the predictions of an RNN at given time are to take into account the dependencies between inframes."}, {"heading": "5.2. Language Modelling", "text": "As mentioned in section 2, the RNN can be used as a generative model to define distributions over sequences. Unlike speech recognition, where the language model calculates a multinomial distribution over a discrete set of phoneme names, the MLM must calculate distributions over high-dimensional binary vectors. To capture the interactions between the output variables in each time step, we prefer the RNN NADE to the RNN as MLM. At each step, the conditional NADE defines a common distribution across the space of high-dimensional binary output vectors. At test date, the conditional NADE at t provides the probability of observing the vectors predicted by the acoustic model, conditioned by all previous predictions."}, {"heading": "5.3. Experimental Setup", "text": "The MAPS dataset consists of 270 piano pieces with their Ground Truth MIDI transcriptions, 210 of which are rendered by software synthesizers, while 60 are played on real pianos. For our experiments, we randomly select 200 tracks for training, 20 for validation and 50 for test1. We use the entire length of the training and validation tracks and use the first 30 seconds of the tracks for testing. Preprocessing of the data consisted of sampling the tracks down to 16 kHz and calculating the size spectrogram. The spectrograms were calculated with a window size of 64 ms and a hop size of 32 ms for the training and validation tracks. For the test tracks, the spectrograms were calculated every 10 ms [1]. The spectrograms were further processed by subtracting the mean and dividing by the standard deviation of the respective training and validation tracks."}, {"heading": "5.4. Training", "text": "The acoustic and linguistic models were trained according to equations 8 and 9 by gradient drop. The output layers of the acoustic models DNN and RNN consisted of sigmoid units; each output of the acoustic model can be interpreted as the independent probability of a pitch present in this framework; the acoustic classifiers were trained by minimizing an entropy effort, since the target vectors for all frames are high-dimensional binary vectors; for both models DNN and RNN, the weights were randomly initialized by sampling values from a Gaussian distribution with 0 mean and 0.01 standard deviation; we also used a pulse of 0.9 with simultaneous updating of weights; the DNN models were trained on independent frames of spectrograms extracted from the training set; to train the stacked RNN models, the training paths were further divided into sub-sequences of length 200 and the models were back propagated by BT (Through TT)."}, {"heading": "5.5. Evaluation Metrics", "text": "We evaluate the performance of our system using the evaluation metrics used in MIREX [1]. We present F metrics for both frame-based and note-only tracking evaluation metrics. In addition, we report on precision, recall and accuracy measurements for the three most powerful models."}, {"heading": "6. RESULTS", "text": "In Table 1, we present F-measures for the various systems using different combinations of acoustic models and post-1training / test data information at: www.eecs.qmul.ac.uk / ~ sss31 / processing. We report F-measures for both frame-based and note onset based evaluation metrics [1]. The best DNN acoustic model consists of 3 layers of 100 units each. The RNN acoustic models have two stacked hidden layers of 250 hidden units each. For speech modelling, the conditional NADEs have 150 hidden units and the RNN has 100 hidden units. Four types of post-processing are considered in the experiments. No post-processing, in which the most likely results are selected from the classifiers; learning independent thresholds for each classifier output based on the training; HMM Post Processing assumes that each pitch class is independent; and finally the proposed hybrid architecture with a beam width of 100 =."}, {"heading": "7. CONCLUSION", "text": "We present a hybrid RNN-based architecture for the inclusion of symbolic priorities in an automatic transcription system for music. The architecture combines acoustic and highly symbolic predictions in a principled way, and we propose an efficient algorithm for inferences. The model generalizes the popular technique of using independent HMMs to smooth out predictions of acoustic classifiers. Evaluation of the MAPS dataset suggests that the model outperforms related transcription systems for music. In the future, we plan to work on improving the individual components of the architecture, namely acoustic and linguistic modelling. We also want to explore ways of improving beam search to make it feasible for real-time applications. Finally, we plan to extend our assessments to data sets with multiple instruments."}, {"heading": "8. REFERENCES", "text": "[1] Mert Bay, Andreas F Ehmann, and J Stephen Downie. Evaluation of multiple-F0 networks and tracking systems. In International Society for Music Information Retrieval Conference, pages 315-320, 2009. [2] Sebastian Bock and Markus Schedl. Polyphonic piano note transcription with recurrent neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 121-124. IEEE, 2012. [3] Nicolas Boulanger-lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 1159-1166, 2012. [4] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent."}], "references": [{"title": "Evaluation of multiple-F0 estimation and tracking systems", "author": ["Mert Bay", "Andreas F Ehmann", "J Stephen Downie"], "venue": "In International Society for Music Information Retrieval Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Polyphonic piano note transcription with recurrent neural networks", "author": ["Sebastian Bock", "Markus Schedl"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Nicolas Boulanger-lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "High-dimensional sequence transduction", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Phone sequence modeling with recurrent neural networks", "author": ["Nicolas Boulanger-Lewandowski", "Jasha Droppo", "Mike Seltzer", "Dong Yu"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Multipitch estimation of piano sounds using a new probabilistic spectral smoothness principle", "author": ["Valentin Emiya", "Roland Badeau", "Bertrand David"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Measuring invariances in deep networks", "author": ["Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": "In Representation Learning Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Signal Processing Methods for Music Transcription", "author": ["Anssi Klapuri", "Manuel Davy", "editors"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Learning recurrent neural networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "In Proceedings of the 28th International Conference Machine on Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A classification-based polyphonic piano transcription approach using learned feature representations", "author": ["Juhan Nam", "Jiquan Ngiam", "Honglak Lee", "Malcolm Slaney"], "venue": "In International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A discriminative model for polyphonic piano transcription", "author": ["Graham E Poliner", "Daniel PW Ellis"], "venue": "EURASIP Journal on Advances in Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Dynamic bayesian networks for symbolic polyphonic pitch modeling", "author": ["S.A Raczynski", "E. Vincent", "S. Sagayama"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "An RNN-based music language model for improving automatic music transcription", "author": ["Siddharth Sigtia", "Emmanouil Benetos", "Srikanth Cherla", "Tillman Weyde", "Artur S. dAvila Garcez", "Simon Dixon"], "venue": "In International Society for Music Information Retrieval Conference (ISMIR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Hierarchical and coupled non-negative dynamical systems with application to audio modeling", "author": ["Umut Simsekli", "Jonathan Le Roux", "John R. Hershey"], "venue": "In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "Automatic Music Transcription (AMT) involves identifying the pitches present in a given polyphonic acoustic signal and generating a corresponding symbolic, score-like transcription [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 2, "context": "However, MLMs have not been extensively applied to AMT because polyphonic symbolic music prediction is quite a difficult problem and simple models such as n-grams which are used in speech are insufficient for modeling sequences of polyphonic music [3].", "startOffset": 248, "endOffset": 251}, {"referenceID": 2, "context": "RNNs and their more complex variants [3], have recently been applied successfully to the problem of symbolic music prediction.", "startOffset": 37, "endOffset": 40}, {"referenceID": 10, "context": "SS is supported by a City University London Pump-Priming Grant; EB is supported by a City University London Research Followship;NB is currently working at Google Inc, Mountain View, California, USA acoustic and language models and then renormalizing, like in a product of experts, suffers from the label bias problem for low entropy sequences [11].", "startOffset": 343, "endOffset": 347}, {"referenceID": 3, "context": "The model proposed in [4], is an input-output variant of the RNN-RBM model for music transcription.", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "The system in [15], uses a family of Dynamic Bayesian Network (DBN) language models to complement the acoustic model, though the search space of possible transcriptions must be constrained in order for the method to be tractable.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "In [19], the authors propose a novel dynamical system for incorporating symbolic information into a non-negative factorisation based transcription model.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The method proposed in [18], incorporates symbolic information into a PLCA based transcription system using Dirichlet priors.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Another shortcoming of the model in [18] is that the acoustic and language models are trained independently by optimising different objectives.", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "The popular technique of superposing a Hidden Markov Model (HMM) to the outputs of a frame-level classifier, like in state-of-theart speech recognition systems [9] is intractable for AMT tasks.", "startOffset": 160, "endOffset": 163}, {"referenceID": 13, "context": "HMMs can be applied to polyphonic AMT systems under the assumption that each pitch is independent of all the other pitches [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 4, "context": "In this paper we employ the architecture in [5], which was originally proposed for modelling sequences of phonemes in speech recognition.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "Therefore, instead of using the RNN to predict the probabilities of pitches directly, we can use the RNN to predict the parameters of a high-dimensional distribution estimator like the Restricted Boltzmann Machine (RBM) or the Neural Autoregressive Density Estimator (NADE) [3].", "startOffset": 274, "endOffset": 277}, {"referenceID": 11, "context": "Another advantage of using the RNNNADE is that the gradients of the objective function can be calculated exactly and therefore we can make use of more powerful optimisers like Hessian Free (HF) [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 7, "context": "Instead, we perform a global search for the most likely sequence using beam search, a breadth-first tree search algorithm that keeps track of only the w most promising paths at any depth t [8, 4, 5].", "startOffset": 189, "endOffset": 198}, {"referenceID": 3, "context": "Instead, we perform a global search for the most likely sequence using beam search, a breadth-first tree search algorithm that keeps track of only the w most promising paths at any depth t [8, 4, 5].", "startOffset": 189, "endOffset": 198}, {"referenceID": 4, "context": "Instead, we perform a global search for the most likely sequence using beam search, a breadth-first tree search algorithm that keeps track of only the w most promising paths at any depth t [8, 4, 5].", "startOffset": 189, "endOffset": 198}, {"referenceID": 3, "context": "In addition to the beam width w, the high-dimensional variant of the beam-search algorithm outlined in [4] requires an additional parameter, the branching factor K.", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Algorithm 1 High Dimensional Beam Search [4]", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "Enumerating the most likely solutions with a DP algorithm is more efficient than stochastic sampling [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "Unlike [4], the high-dimensional beam search algorithm outlined in algorithm 1 does not require the branching factor K to be specified in advance and allows the use of much larger beam widths.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "DNNs currently form the state of the art for acoustic modelling in speech [9] and have been successfully applied to music transcription in the past [13, 3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "DNNs currently form the state of the art for acoustic modelling in speech [9] and have been successfully applied to music transcription in the past [13, 3].", "startOffset": 148, "endOffset": 155}, {"referenceID": 2, "context": "DNNs currently form the state of the art for acoustic modelling in speech [9] and have been successfully applied to music transcription in the past [13, 3].", "startOffset": 148, "endOffset": 155}, {"referenceID": 1, "context": "Previous work on using RNNs as acoustic models for transcription demonstrates that RNNs are very good at predicting note-onsets [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 16, "context": "We use the stacked RNN architecture, where several recurrent hidden layers are stacked in order to encourage each recurrent layer to operate at a different timescale [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "The motivation for doing this is that the features learnt by the DNN are believed to disentangle the factors of variation present in the inputs [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "We perform experiments on the MAPS dataset [6] to test the performance of the hybrid architecture and compare its performance to other models.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "For the test tracks, spectrograms were computed every 10 ms [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 15, "context": "For training the stacked RNN models, the training tracks were further divided into sub-sequences of length 200 and the models were trained by Back-Propagation Through Time (BPTT) [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "We evaluate the performance of our system using the evaluation metrics used in MIREX [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "We report F-measures for both frame-based and note onset based evaluation metrics [1].", "startOffset": 82, "endOffset": 85}], "year": 2014, "abstractText": "We investigate the problem of incorporating higher-level symbolic score-like information into Automatic Music Transcription (AMT) systems to improve their performance. We use recurrent neural networks (RNNs) and their variants as music language models (MLMs) and present a generative architecture for combining these models with predictions from a frame level acoustic classifier. We also compare different neural network architectures for acoustic modeling. The proposed model computes a distribution over possible output sequences given the acoustic input signal and we present an algorithm for performing a global search for good candidate transcriptions. The performance of the proposed model is evaluated on piano music from the MAPS dataset and we observe that the proposed model consistently outperforms existing transcription methods.", "creator": "LaTeX with hyperref package"}}}