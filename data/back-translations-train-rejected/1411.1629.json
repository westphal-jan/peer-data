{"id": "1411.1629", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "The Limitations of Standardized Science Tests as Benchmarks for Artificial Intelligence Research: Position Paper", "abstract": "In this position paper, I argue that standardized tests for elementary science such as SAT or Regents tests are not very good benchmarks for measuring the progress of artificial intelligence systems in understanding basic science. The primary problem is that these tests are designed to test aspects of knowledge and ability that are challenging for people; the aspects that are challenging for AI systems are very different. In particular, standardized tests do not test knowledge that is obvious for people; none of this knowledge can be assumed in AI systems. Individual standardized tests also have specific features that are not necessarily appropriate for an AI benchmark. I analyze the Physics subject SAT in some detail and the New York State Regents Science test more briefly. I also argue that the apparent advantages offered by using standardized tests are mostly either minor or illusory. The one major real advantage is that the significance is easily explained to the public; but I argue that even this is a somewhat mixed blessing. I conclude by arguing that, first, more appropriate collections of exam style problems could be assembled, and second, that there are better kinds of benchmarks than exam-style problems. In an appendix I present a collection of sample exam-style problems that test kinds of knowledge missing from the standardized tests.", "histories": [["v1", "Thu, 6 Nov 2014 14:44:12 GMT  (1237kb)", "https://arxiv.org/abs/1411.1629v1", "24 pages, 5 figures"], ["v2", "Fri, 16 Oct 2015 20:17:31 GMT  (24kb)", "http://arxiv.org/abs/1411.1629v2", "24 pages, 5 figures"]], "COMMENTS": "24 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ernest davis"], "accepted": false, "id": "1411.1629"}, "pdf": {"name": "1411.1629.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["davise@cs.nyu.edu"], "sections": [{"heading": null, "text": "In fact, most people are able to survive on their own."}, {"heading": "1 Testing science that any fool knows", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Gaps in specific standardized tests", "text": "There is no point in asking questions that all test takers will ask correctly. However, beyond that, each standardized test has its own gaps, viewed from the standpoint of an AI benchmark, which primarily reflects the fact that the test was not written with the aim of serving as an AI benchmark. Here, of course, one has to look at each test individually; different tests have different strengths and weaknesses. Generally, if an AI researcher wants to use a particular test as a benchmark, she should carefully examine it to determine which of the goals of her research are achieved by the test and which are not. I have examined two specific tests and will discuss them below: the SAT Subject Test in Physics and the New York State Regents Test, which does not consider fourth-grade AI grades as grades. First, a general caveat: in the world of education, there is a great deal of expertise on these standardized tests, which I often do not question, and another does not."}, {"heading": "2.1 The SAT Subject test in Physics", "text": "In fact, most of them will be able to orient themselves in a certain direction in which they are able to."}, {"heading": "2.2 Regents\u2019 fourth-grade science test", "text": "It is not as if this is a real problem, but a complex system involving real knowledge, and a set of terms that go far beyond a series of topics, but place a particular emphasis on biology. Questions discussed in Section 1 are far less acute than in physics, although they have not completely disappeared. Appendix A.1 includes a set of biology questions of various kinds, which are more fundamental than any other test of the rule (consider, for example, questions that any fool knows). Covering basic physics is not sufficient for AI purposes, although they are suitable for fourth grade."}, {"heading": "3 Apparent advantages of standardized tests", "text": "It is just as it is that this is a country in which it is not a country, but a country in which it is not a country, but a country in which it is not a country, but a country in which it is a country, a country in which it is not a country but a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "4 What benchmarks would be better than the standardized", "text": "Tests? What benchmarks should be used instead of standardized tests? However, if we want to stick to the short answer test formats, which do have advantages, then standardized tests are certainly a starting point (if we have the bad apples.) However, we should complement those with many more problems that require real-world knowledge; problems that combine knowledge from different theories; and problems that use ways of thinking and forms of geometric knowledge that are overlooked or underused. More importantly, however, we should not limit ourselves to exam tasks in the style of benchmarks and goals. Rather, we should consider a variety of tasks such as: \u2022 understanding texts of different kinds: textbooks, device manuals, text in narratives based on physical knowledge (Davis 2013), etc. \u2022 Exam questions with essay-style answers. \u2022 Reflections on variants of physical situations (Davis 1998). \u2022 Integration with planners in situations that include complex physical reasoning (2014)."}, {"heading": "5 Final observation", "text": "Standardised tests carry an immense social burden and must meet a host of very strict constraints; they are taken annually by millions of students under very simple exam conditions (without the use of computers, let alone the Internet) and must therefore be graded automatically or by inexperienced human graders; they play a disproportionate (and currently increasing) role in determining the future of these students; they must be fair to a wide range of students; they must adhere to existing curricula; they must maintain a constant level of difficulty, both through the variants offered in one year and from one year to the next; they must be subjected to intense scrutiny by a large number of critics, many of whom are unfriendly; these constraints place serious restrictions on what can be asked for and how tests can be structured; in developing physical intelligence benchmarks, why are we not subject to any of these constraints; why are we tying our hands by limiting ourselves to standardised tests? Why are we not taking advantage of our freedom?"}, {"heading": "Appendix: Collection of problems", "text": "Appendix A.1 is a collection of problems, mainly in physics, a bit of biology that fourth-graders should find easy to solve. (I'm no expert on what to realistically expect from students of different ages.) Appendix A.2 is a collection of problems that high school physics students should find easy; they start with problems that for some may require a few minutes of thought. The large number of problems related to containers reflects the fact that I have been thinking about containers for several years. I have included a comparatively large number of problems related to astronomy, firstly because astronomy is a very fruitful source of problems of qualitative geometric thinking and thinking on orders of magnitude; and secondly because I like astronomy. I am quite surprised that it is not part of the material on the physical SATs. There are no questions about electrical circuits because I find them boring; furthermore, they are not suitable for common sense."}, {"heading": "Appendix A.1: Easy problems", "text": "/ / * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "Appendix A.2: Physics/astronomy problems that should be easy for highschool physics students", "text": "You have packed some objects into a 6 \"\u00d7 4\" \u00d7 8 \"box. * You have an empty box that is 12\" \u00d7 6 \"\u00d7 12.\" Will the same objects fit into an empty box? A. Yes, they will fit. * No, they will not fit into an empty box. * Yes, they will not fit. * You have packed some objects into a 6 \"\u00d7 4\" box. * * * You have an empty box that is 6 \"* * * 6\" -6. \"Will the same objects fit into an empty box. * No, they will not fit."}], "references": [{"title": "A Question-Answering System for AP Chemistry: Assessing KR&R Technologies,\u201d KR-2004", "author": ["K. Barker"], "venue": null, "citeRegEx": "Barker,? \\Q2004\\E", "shortCiteRegEx": "Barker", "year": 2004}, {"title": "Selected Grand Challenges in Cognitive Science,", "author": ["R. Brachman"], "venue": "MITRE Technical Report", "citeRegEx": "Brachman,? \\Q2005\\E", "shortCiteRegEx": "Brachman", "year": 2005}, {"title": "Chemistry: The Central Science, (9th edn.) Upper Saddle River, NJ", "author": ["T.L. Brown", "H.E. LeMay", "B. Bursten"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Brown et al\\.", "year": 2003}, {"title": "A Study of the Knowledge Base Requirements for Passing an Elementary Science Test,", "author": ["P. Clark", "P. Harrison", "N. Balasubramanian"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2013}, {"title": "The Naive Physics Perplex,\u201d AI Magazine, vol", "author": ["E. Davis"], "venue": "19, no. 4, Winter 1998, pp. 51-79.", "citeRegEx": "Davis,? 1998", "shortCiteRegEx": "Davis", "year": 1998}, {"title": "Qualitative Spatial Reasoning in Interpreting Text and Narrative.", "author": ["E. Davis"], "venue": "Spatial Cognition and Computation,", "citeRegEx": "Davis,? \\Q2013\\E", "shortCiteRegEx": "Davis", "year": 2013}, {"title": "Top Artificial Intelligent system is as smart as a 4-year old", "author": ["S. Gaudin"], "venue": "Computerworld, July", "citeRegEx": "Gaudin,? \\Q2013\\E", "shortCiteRegEx": "Gaudin", "year": 2013}, {"title": "The initial knowledge state of college physics students", "author": ["I. Haroun", "D. Hestenes"], "venue": "American Journal of Physics, 53(11), 1043-1055.", "citeRegEx": "Haroun and Hestenes,? 1985", "shortCiteRegEx": "Haroun and Hestenes", "year": 1985}, {"title": "Conceptnet 3: A flexible multilingual semantic network for common sense knowledge", "author": ["C. Havasi", "R. Speer", "J. Alonso"], "venue": "Recent Advances in Natural Language Processing,", "citeRegEx": "Havasi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Havasi et al\\.", "year": 2007}, {"title": "Kaplan SAT Subject Test: Physics", "author": ["Kaplan"], "venue": "2013-2014. Kaplan Publishing.", "citeRegEx": "Kaplan,? 2013", "shortCiteRegEx": "Kaplan", "year": 2013}, {"title": "Twilight of the Lecture.", "author": ["C. Lambert"], "venue": "Harvard Magazine, March-April,", "citeRegEx": "Lambert,? \\Q2012\\E", "shortCiteRegEx": "Lambert", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For example Brachman et al. (2005) suggest developing a program that can pass the SATs.", "startOffset": 12, "endOffset": 35}, {"referenceID": 0, "context": "For example Brachman et al. (2005) suggest developing a program that can pass the SATs. Clark, Harrison, and Balasubramanian (2013) propose a project of passing the New York State Regents Science Test for 4th graders.", "startOffset": 12, "endOffset": 132}, {"referenceID": 0, "context": "For example Brachman et al. (2005) suggest developing a program that can pass the SATs. Clark, Harrison, and Balasubramanian (2013) propose a project of passing the New York State Regents Science Test for 4th graders. Strickland (2013) proposes developing an AI that can pass the entrance exams for the University of Tokyo.", "startOffset": 12, "endOffset": 236}, {"referenceID": 0, "context": "For example Brachman et al. (2005) suggest developing a program that can pass the SATs. Clark, Harrison, and Balasubramanian (2013) propose a project of passing the New York State Regents Science Test for 4th graders. Strickland (2013) proposes developing an AI that can pass the entrance exams for the University of Tokyo. Ohlsson et al. (2013) evaluated the performance ConceptNet system (Havasi, Speer, and Alonso 2007) on a preprocessed form of the Wechesler Preschool and Primary Scale of Intelligence test.", "startOffset": 12, "endOffset": 346}, {"referenceID": 0, "context": "Barker et al. (2004) describes the construction of a knowledge-based system that (more or less) scored a 3 (passing) on two section of the high school chemistry Advanced Placement test.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Indeed Barker et al. (2004) were able to draw some interesting and fruitful conclusions for knowledge representation and knowledge engineering from their project aimed at passing the chemistry AP test.", "startOffset": 7, "endOffset": 28}, {"referenceID": 10, "context": "Lambert (2012) tells of one Harvard physics student who, confronted with David Hestenes\u2019 test of basic physics concepts, asked \u201cHow should I answer these questions \u2014 according to what you taught me, or how I usually think about these things?\u201d", "startOffset": 0, "endOffset": 15}, {"referenceID": 6, "context": "Thus, an article in ComputerWorld (Gaudin 2013) had the headline \u201cTop Artificial Intelligence System is as smart as a 4-year-old\u201d; the Independent published an article \u201cAI System found to be as clever as a young child after taking IQ test\u201d; and articles with similar titles were published in many venues.", "startOffset": 34, "endOffset": 47}, {"referenceID": 6, "context": "Thus, an article in ComputerWorld (Gaudin 2013) had the headline \u201cTop Artificial Intelligence System is as smart as a 4-year-old\u201d; the Independent published an article \u201cAI System found to be as clever as a young child after taking IQ test\u201d; and articles with similar titles were published in many venues. These headlines are of course absurd; a four-year old can make up stories, chat, occasionally follow directions, invent words, learn language at an incredible pace; ConceptNet (the AI system in question) can do none of these. Finally, some standardized tests, including the SAT\u2019s, are not published and are available to researchers only under stringent non-disclosure agreements. It seems to me that AI researchers should under no circumstances use such a test with such an agreement. The loss from the inability to discuss the program\u2019s behavior on specific examples far outweighs the gain from using a test with the imprimatur of the official test designer. This applies equally to Haroun and Hestenes\u2019 (1985) well-known basic physics test; in any case, it would seem from the published information that that test focuses on testing understanding of force and energy rather than testing the relation of formal physics to basic world knowledge.", "startOffset": 35, "endOffset": 1017}, {"referenceID": 5, "context": "\u2022 Understanding texts of various kinds: Textbooks, equipment manuals, text in narrative that draws on physical knowledge (Davis 2013) and so on.", "startOffset": 121, "endOffset": 133}, {"referenceID": 4, "context": "\u2022 Reasoning about variants of physical situations (Davis 1998).", "startOffset": 50, "endOffset": 62}], "year": 2015, "abstractText": "In this position paper, I argue that standardized tests for elementary science such as SAT or Regents tests are not very good benchmarks for measuring the progress of artificial intelligence systems in understanding basic science. The primary problem is that these tests are designed to test aspects of knowledge and ability that are challenging for people; the aspects that are challenging for AI systems are very different. In particular, standardized tests do not test knowledge that is obvious for people; none of this knowledge can be assumed in AI systems. Individual standardized tests also have specific features that are not necessarily appropriate for an AI benchmark. I analyze the Physics subject SAT in some detail and the New York State Regents Science test more briefly. I also argue that the apparent advantages offered by using standardized tests are mostly either minor or illusory. The one major real advantage is that the significance is easily explained to the public; but I argue that even this is a somewhat mixed blessing. I conclude by arguing that, first, more appropriate collections of exam style problems could be assembled, and second, that there are better kinds of benchmarks than exam-style problems. In an appendix I present a collection of sample exam-style problems that test kinds of knowledge missing from the standardized tests. It has often been proposed that a standardized tests constitute useful goals for AI systems doing automated scientific reasoning and informative benchmarks for progress. For example Brachman et al. (2005) suggest developing a program that can pass the SATs. Clark, Harrison, and Balasubramanian (2013) propose a project of passing the New York State Regents Science Test for 4th graders. Strickland (2013) proposes developing an AI that can pass the entrance exams for the University of Tokyo. Ohlsson et al. (2013) evaluated the performance ConceptNet system (Havasi, Speer, and Alonso 2007) on a preprocessed form of the Wechesler Preschool and Primary Scale of Intelligence test. Barker et al. (2004) describes the construction of a knowledge-based system that (more or less) scored a 3 (passing) on two section of the high school chemistry Advanced Placement test. In this position paper, I want to discuss specifically the project of developing AI programs to pass standardized science tests, as a step toward developing AI programs with powerful abilities to reason about science; and I will argue that focusing narrowly on this goal is not the best way of advancing AI understanding of basic science, and that this benchmark is not the best way of measuring progress. The dangers of focusing on too narrow and idiosyncratic a target are well illustrated by Watson. IBM set itself the goal of winning at Jeopardy, and with huge labors, in an extraordinary tour de force, succeeded. However, it is not at all clear what contribution this has made to AI technology;", "creator": "LaTeX with hyperref package"}}}