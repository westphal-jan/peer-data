{"id": "1411.2328", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2014", "title": "Modeling Word Relatedness in Latent Dirichlet Allocation", "abstract": "Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA.", "histories": [["v1", "Mon, 10 Nov 2014 05:24:41 GMT  (1135kb,D)", "http://arxiv.org/abs/1411.2328v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["xun wang"], "accepted": false, "id": "1411.2328"}, "pdf": {"name": "1411.2328.pdf", "metadata": {"source": "CRF", "title": "Modeling Word Relatedness in Latent Dirichlet Allocation", "authors": ["Xun Wang"], "emails": ["xunwang45@gmail.com", "Permissions@acm.org."], "sections": [{"heading": "1. INTRODUCTION", "text": "This year it is so far that it will only take one year to reach an agreement."}, {"heading": "2. RELATED WORK", "text": "Topic models as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] are widely used in many different fields as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17]. for their ability in discovering themes latent in the document collection. Standard themas models suffer from the disadvantage that researchers only use word co-appearance frequency for topic modeling [1, 13], text classification [10, 15] or facet mining [7, 17]. However, similar or synonymous two terms, if they do not appear in the document at the same time, can hardly be classified into the same topic. This disadvantage is found in multi-lingual topic modeling, where different languages never co-come with each other."}, {"heading": "3. LDA TOPIC MODEL", "text": "The latent Dirichlet Model (LDA) (Lead et al., 2003) q = q q (\u03b2 \u03b2 q) = q (\u03b2 \u03b2 q) = q (\u03b2 \u03b2 = q) = q (\u03b2 \u03b2 = q). Specifically, each document is presented as a mixture of topics drawn from a Dirichlet distribution and each topic is presented as a mixture of words. Document words are queried from a topic-specific word distribution specified by a drawing of the word topic vector. Let K be the number of topics; V be the number of terms in a vocabulary. \u03b2 is a K \u00b7 V matrix and \u03b2k is the distribution vector over V terms where \u03b2ij = P = j | zw = i) the probability word w is generated from topic k. The generation for LDA is presented as follows: 1. For each document m \u00b2 [1, M]: Draw topic distribution, where \u03b2j = w = w w the word from the word (w w w w w)."}, {"heading": "4. WR-LDA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Description", "text": "In this section, we present our WR-LDA model and show how it can integrate word correlation into topic models. Let's have G = (V, E) denote the graph, where V = {wi} i = Vi = 1 denotes the collection of words and E = {ew, w \u00b2: w \u00b2 V, w \u00b2 V}. Assuming that similar words should have a similar probability generated by different topics, we introduce the loss function R (\u03b2), inspired by the idea of graph harmonic function (Zhu et al., 2003; Mei et al., 2008).R (\u03b2) = 12 \u2211 w \u00b2 w \u00b2 w \u00b2 w \u00b2 w \u00b2, w \u00b2 (\u03b2kw \u2212 \u03b2kw \u00b2) 2 (5), the intuitive R (\u03b2 \u2212) the difference between p (w \u2212 \u03b2) and p \u00b2.b \u00b2."}, {"heading": "4.2 Inference", "text": "In this subsection we describe the variational conclusion for WRLDA = \u03b2 \u03b2 \u03b2 (\u03b2 \u03b2 \u03b2) step (\u03b2 \u03b2 \u03b2) and briefly describe the part that is similar. We use the variational distribution q (\u03b8, z-LDA) to find \u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u03b2\u00df\u00df\u00df\u00df\u00df\u00df\u00df\u00dfssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss"}, {"heading": "5. EXPERIMENTS", "text": "In this section, we compare WR-LDA with standard LDA and other baselines in multiple applications."}, {"heading": "5.1 Text Modeling", "text": "We first compare the text modeling of the WR-LDA with the standard LDA on the 20 newsgroups data set2 with a standard list of 598 stopwords removed. We adapt the data set to 110 topics. To evaluate the performance of the WR-LDA with different \u03bb, we have the following function. M (3) = 1C1 20-Gi = 1-Gi-Gi (17) 2http: / / qwone.com / jason / 20Newsgroups / Gi denotes the collection of documents with the label i-Gi-Gi-Gi-1 (KL) 2http: / / qwone.com / jason / 20Newsgroups / Gi."}, {"heading": "5.2 Regression", "text": "We evaluate the monitored version of WR-LDA on TripAdvisor3 hotel reviews. As in Zhu and Xing (2010), we take logs of response values to make them approximate normal. We compare the results from the following approaches: LDA + SVR, WR + SVR and sWR-LDA. For LDA + SVR and WCLDA + SVR, we use their low ratings for reviews. We compare the results from the following approaches: LDA + SVR, WR + SVR and sWR-LDA. For LDA + SVR, we use their low presentation of documents as input features and name this method."}, {"heading": "6. CROSS-LINGUAL TOPIC EXTRACTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Dataset Construction", "text": "In order to qualitatively compare our approach with the basic method, we are testing different models for cross-border topic extraction in different respects. The data set we used in this experiment comes from the Chinese news website Sina 4 and Google News5. Selected news stories talk about 5 topics categorized by news websites such as \"Sports,\" \"Entertainment,\" \"World,\" \"Science\" and \"Economy.\" Each Chinese news passage is linked to an English news passage dealing with the same event (e.g. passage titled \"Protests no Turkish Spring, says Erdogan\") We select 1,000 English passages and 1,000 Chinese passages for the period from March 8, 2001 to June 23, 2013, including {Sports: 360, WordNews: 296, Bussiness: 92, 68: Entertainments: 4}."}, {"heading": "6.2 Details", "text": "LetCc stands for the Chinese news collection, where Cc = {d1c, d2c,..., dNc}, N = 1000. Ce is the English news collection Ce = {d1e, d2e,..., dNe}, and dic is the corresponding passage of Die, I [1, N]. First, we have rare diction6 and common words 7. A bilingual Chinese-English dictionary would give us a many-to-many picture between the vocabulary of the two languages. If a word can potentially be translated into another word, the two words would be associated with an edge. Specifically, we should designate an English word and wc a Chinese word. Cwe means the amount of Chinese words that we translate, Cwe = {wc | e (wc, we) 6 = 0} (we, wc) = {1 if wc: Cwe 0, if wc: Cwe (18)."}, {"heading": "6.3 Evaluation", "text": "A part of the results is in the table 1 / 10 times in the Corpus, where M. \"cn 5https: / / news.google.com 6Words that appeared less than 3 times in the Corpus list.\" A. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D.D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D."}, {"heading": "7. CONCLUSION", "text": "In this paper, we present WR-LDA, a revised version of the LDA theme model that takes word correlations into account. Experiments in text modeling, evaluation prediction, and cross-language theme modeling demonstrate the effectiveness of our model. WR-LDA has two disadvantages compared to LDA. (1) The value of the parameter lambda included in the model is difficult to adjust, while LDA does not involve any additional parameters. (2) Due to the introduction of the streamlining function in WR-LDA, we need to record parameters in variable derivative for all documents, which significantly increases the cost of both memory and time."}, {"heading": "8. REFERENCES", "text": "[1] C. C. Aggarwal and C. Zhai. Mining text data. Springer, 2012. [2] D. Andrzejewski, X. Zhu, and M. Craven. Incorporatingdomain knowledge into topic modeling via dirichlet forest priors. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 25-32. ACM, 2009. [3] D. M. Blei, A. Y. Ng., and M. I. Jordan, the Journal of Machine Learning research, 3: 993-1022, 2003. [4] J. Boyd-Graber and D. M. Blei. Multilingual topic models for unaligned text. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, S. AUAI Press, 2009. [5] T. Hofmann. Probabilistic latent semantic indexing."}], "references": [{"title": "Mining text data", "author": ["C.C. Aggarwal", "C. Zhai"], "venue": "Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating domain knowledge into topic modeling via dirichlet forest priors", "author": ["D. Andrzejewski", "X. Zhu", "M. Craven"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 25\u201332. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Multilingual topic models for unaligned text", "author": ["J. Boyd-Graber", "D.M. Blei"], "venue": "Proceedings of the Twenty-Fifth  Conference on Uncertainty in Artificial Intelligence, pages 75\u201382. AUAI Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Timeline generation: tracking individuals on twitter", "author": ["J. Li", "C. Cardie"], "venue": "Proceedings of the 23rd international conference on World wide web, pages 643\u2013652. International World Wide Web Conferences Steering Committee", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Identifying manipulated offerings on review portals", "author": ["J. Li", "M. Ott", "C. Cardie"], "venue": "EMNLP, pages 1933\u20131942", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Major life event extraction from twitter based on congratulations/condolences speech acts", "author": ["J. Li", "A. Ritter", "C. Cardie", "E. Hovy"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Weakly supervised user profile extraction from twitter", "author": ["J. Li", "A. Ritter", "E. Hovy"], "venue": "ACL", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised topic models", "author": ["J.D. Mcauliffe", "D.M. Blei"], "venue": "Advances in neural information processing systems, pages 121\u2013128", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Polylingual topic models", "author": ["D. Mimno", "H.M. Wallach", "J. Naradowsky", "D.A. Smith", "A. McCallum"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 880\u2013889. Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Mining multilingual topics from wikipedia", "author": ["X. Ni", "J.-T. Sun", "J. Hu", "Z. Chen"], "venue": "Proceedings of the 18th international conference on World wide web, pages 1155\u20131156. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised learning of human action categories using spatial-temporal words", "author": ["J.C. Niebles", "H. Wang", "L. Fei-Fei"], "venue": "International journal of computer vision, 79(3):299\u2013318", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Word features for latent dirichlet  allocation", "author": ["J. Petterson", "W. Buntine", "S.M. Narayanamurthy", "T.S. Caetano", "A.J. Smola"], "venue": "Advances in Neural Information Processing Systems, pages 1921\u20131929", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 248\u2013256. Association for Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Bilingual lsa-based translation lexicon adaptation for spoken language translation", "author": ["Y.-C. Tam", "T. Schultz"], "venue": "INTERSPEECH, pages 2461\u20132464", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["I. Titov", "R. McDonald"], "venue": "Proceedings of the 17th international conference on World Wide Web, pages 111\u2013120. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Bitam: Bilingual topic admixture models for word alignment", "author": ["B. Zhao", "E.P. Xing"], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions, pages 969\u2013976. Association for Computational Linguistics", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 17, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 15, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 10, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 11, "context": "In cross-lingual topic extraction, researchers try to match word pairs and discover the correspondence aligned either at sentence level or document level [18, 16, 11, 12].", "startOffset": 154, "endOffset": 170}, {"referenceID": 17, "context": "Zhao and Xing [18] incorporated word \u2212 pair, sentence \u2212 pair and document \u2212 pair knowledge information into statistical models.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Boyd-Graber and Blei [4] developed the unaligned topic approach called MUTO where topics are distributions over", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "[2] and the approach developed by Petterson et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] where word relations are considered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "\u2019s work [2], domain knowledge is used to model vocabulary relations such as Must-Link or Cannot-Link.", "startOffset": 8, "endOffset": 11}, {"referenceID": 13, "context": "\u2019s work[14], they use a sophisticated biased prior in Dirichlet distribution to consider the word correlations rather than the uniform one used in Standard LDA.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 197, "endOffset": 206}, {"referenceID": 7, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 197, "endOffset": 206}, {"referenceID": 8, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 197, "endOffset": 206}, {"referenceID": 0, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 224, "endOffset": 231}, {"referenceID": 12, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 224, "endOffset": 231}, {"referenceID": 9, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 253, "endOffset": 261}, {"referenceID": 14, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 253, "endOffset": 261}, {"referenceID": 6, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 278, "endOffset": 285}, {"referenceID": 16, "context": "RELATED WORK Topic models such as Latent Dirichlet Allocation (LDA) [3] and PLSA (Probabilistic Latent Semantic Analysis) [5] have been widely used in many different fields such as social analysis [6, 8, 9], event detection [1, 13], text classification [10, 15] or facet mining [7, 17].", "startOffset": 278, "endOffset": 285}, {"referenceID": 1, "context": "Previous work on multilingual topic models mostly require parallelism at either the sentence level or document level [2] and the approach developed by Petterson et al.", "startOffset": 117, "endOffset": 120}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proposed the approach by considering domain knowledge to model vocabulary relations such as MustLink or Cannot-Link by using a Dirichlet Tree prior in topic models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14]uses a biased Dirichlet prior by using a Logistic word smoother which takes accounts word relations.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA.", "creator": "LaTeX with hyperref package"}}}