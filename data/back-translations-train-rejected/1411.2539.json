{"id": "1411.2539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2014", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.", "histories": [["v1", "Mon, 10 Nov 2014 19:09:41 GMT  (2905kb,D)", "http://arxiv.org/abs/1411.2539v1", "13 pages. NIPS 2014 deep learning workshop"]], "COMMENTS": "13 pages. NIPS 2014 deep learning workshop", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["ryan kiros", "ruslan salakhutdinov", "richard s zemel"], "accepted": false, "id": "1411.2539"}, "pdf": {"name": "1411.2539.pdf", "metadata": {"source": "CRF", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "authors": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "emails": ["zemel}@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "1.1 Multimodal representation learning", "text": "Popular approaches are the learning of common image-word embedding [4, 5] and the embedding of images and sentences in a common space [6, 15]. Our proposed pipeline makes direct use of these ideas. Further approaches to multimodal learning include the use of deep Boltzmann machines [16], logbilinear neural language models [2], autoencoders [17], recursive neural networks [7], and theme models [18]. Several bidirectional approaches to the evaluation of images and captions have also been proposed, based on kernel CCA [3], normalized CCA [19], and recursive networks with dependency trees [6]. From an architectural point of view, our encoder decoder model most closely resembles [20], which suggested a two-step embedding and generation process for semantic parsing."}, {"heading": "1.2 Generating descriptions of images", "text": "We group approaches to generation into three types of methods, each of which is described in more detail here: template-based methods that are based on improvements. Template-based methods include filling in sentence templates, such as triplets, that are based on the results of object recognition and spatial relationships. [21, 22, 23, 24, 25] While these approaches can produce precise descriptions, they are often more \"robotic\" in nature and do not generalize the fluidity and naturalness of human-written captions. These approaches aim to utilize existing caption databases by extracting and assembling components of related captions to generate new descriptions. [26, 27] The advantage of these approaches is that they enable a much broader and more meaningful class of captions that are more fluid and human-like than the then template-based approach-based neural network-based approaches to generate neural network-based captions by amplifying them."}, {"heading": "1.3 Encoder-decoder methods for machine translation", "text": "The goal of NMT is to develop an end-to-end translation system with a large neural network, as opposed to using a neural network as an additional feature function for an existing phrase-based system. NMT methods are based on the encoder-decoder principle, that is, an encoder is used to map an English sentence to a distributed vector, and a decoder is then conditioned on this vector to generate a French translation from the source text. Current methods include the use of a revolutionary encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11]. Although it is still a young field of research, these methods have already achieved a performance comparable to strong phrase-based systems, and have improved over the beginning of art, when they argue for the restoration of an existing image (that we use the value of an existing part)."}, {"heading": "2 An encoder-decoder model for ranking and generation", "text": "In this section, we describe our caption generation pipeline. We first review LSTM RNNNs used to encode sentences, followed by learning multimodal distributed representations. We then review log-bilinear neural language models [29], multiplicative neural language models [30], and then present our structural neural language model."}, {"heading": "2.1 Long short-term memory RNNs", "text": "IS long-term memory [1] is a recursive neural network containing a built-in memory cell to store information and exploit the context over long distances. LSTM memory cells are surrounded by gating units, which are used, among other things, to read, write and reset information. LSTMs have been used to achieve state-of-the-art performance in various tasks, such as handwriting recognition [31], sequence generation [32], speech recognition [33] and machine translation [11]. Strategies have also been proposed to prevent translation into deep LSTMs. [35] Let Xt mark a matrix of training instances at a specific time \u00b7 \u00b7 Xt is used to mark a matrix of word representations for the t-th word of each sentence in the training unit. Let WIS (It, Ft, Ct, Ot, Ot, Mt, Mt, Mt, Mt) select the function, the selected time, the output and Wxi WIS = 1 Wxi WIS (Wxi). WIS = WIS 1 WIS 1 Wxi Wxi WIS = Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi Wxi"}, {"heading": "2.2 Multimodal distributed representations", "text": "Suppose we get image description pairs corresponding to an image and a description that correctly describe the image. Images are represented as the top layer (before the softmax) of a revolutionary network formed on the ImageNet classification task [36]. Let D be the dimensionality of an image attribute vector (e.g. 4096 for AlexNet [36]), K be the dimensionality of the embedding space and let V be the number of words in the vocabulary. Let WI-RK \u00b7 D and WT-RK \u00b7 V be the image embedding matrix and word embedding matrices, or K be the dimensionality of the embedding space x. In the face of an image description S = {w1,.., wN} with words w1,."}, {"heading": "2.3 Log-bilinear neural language models", "text": "The log-bilinear language model (LBL) [29] is a deterministic model that can be considered a forward-facing neural network with a single linear hidden layer. Each word w in the vocabulary is represented as a K-dimensional real vector w-RK, as in the case of the encoder. Let R denote a V-K matrix of word representation vectors 4, where V is the word size. Let (w1,.. wn-1) be a tuple of n-1 words, where n-1 is the context size. The LBL model makes a linear prediction of the next word representation asr-n-1-i = 1 C (i) wi, (7) where C (i), i = 1, n-1 K are context parameter matrices."}, {"heading": "2.4 Multiplicative neural language models", "text": "rE \"s for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead for the lead."}, {"heading": "2.5 Structure-content neural language models", "text": "We will now describe the structure contents of neural language model. Let us suppose that along with a description S = {w1,.., wN}, we will also use a sequence of word-specific structure variables T = {t1,.., tN}. During our experiments, each ti corresponds to the part-of-speech for word wi, although other possibilities can be used instead. Given an embedding u (the content vector), our goal is to extract the distribution P (wn = i | w1: n + k, u) from the previous word context w1: n \u2212 1 and forward structural context tn: n + k, where k is the context size. Figure 3 gives an illustration of the model and prediction problem. Intuitively, the structure variables perform the model during generation phrase and can be thought of as a soft template to avoid the model of generative grammatical nonsense."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Image-sentence ranking", "text": "Our main quantitative results is the determination of the effectiveness of using an LSTM sentence encoder for the ranking of images and descriptions. We perform the same experimental procedure as of [15] on the Flickr8K [3] and Flickr30K [42] records. These records come with 8,000 and 30,000 images respectively with each image commented using 5 sets of independent commentators. As of [15], we do not have explicit word preprocessing. We used two Constitutional network architectures for extracting 4096 dimensional image features: the Toronto ConvNet 5 as well as the 19-layer OxfordNet [43], which took 2nd place in the ILSVRC 2014 classification contest. Following the protocol of [15], 1000 images are used for validation, 1000 for testing and the rest are used for training. The evaluation is carried out with Recall @ K, namely the average number of images used for correcting the text definition."}, {"heading": "3.1.1 Results", "text": "Tables 1 and 2 illustrate our results on Flickr8K and Flickr30K, respectively, and the performance of our model is comparable to that of the m-RNN. On some measures, we exceed or compare existing results, while on others, m-RNN exceeds our model. m-RNN does not learn explicit embedding between images and sentences, and relies on perplexity as a means of retrieval. Methods that allow 5https: / / github.com / TorontoDeepLearning / convennetlearn to explicitly embed rooms have a significant speed advantage over perplexity-based query methods, as the query can easily be performed using a single matrix that multiplies the vectors from the dataset by the query vector. Therefore, explicit embedding methods are much better suited to accessing large datasets, as both our method and the oxmetrix models that surpass existing ones, which show net integration results in almost all areas, are more or less suitable for large datasets."}, {"heading": "3.2 Multimodal linguistic regularities", "text": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibe linguistic regularities that can find the next vector to \"king\" - \"man\" + \"woman\" is to? be available by finding the next vector to \"king\" - \"man\" + \"woman.\" A natural question we ask ourselves is whether multimodal vector spaces exhibit the same phenomenon. Would * image of a blue car * - \"blue\" + \"red\" be near images of red cars? Suppose we form an embedding model with a linear encoder, namely v = improved Ni = 1 wi for word vectors wi and sentence vector v (where both v and the image embedding are normalized to unit length). Using our example above, let vblue, vred and vcar denote the word embeddings for blue, red and vcar respectively."}, {"heading": "3.3 Image caption generation", "text": "We created image descriptions for about 800 images from the SBU-captioned photographic dataset [40], the same images that represented the results of the current state-of-the-art composition-based approach TreeTalk [27]. 7 Our LSTM encoder and SC-NLM decoder were trained by linking the Flickr30K dataset to the recently released Microsoft COCO dataset [46], which provides us with over 100,000 images and over 500,000 descriptions for training, and the SBU dataset contains 1 million images each with a single description and was used by [27] to train their model. While the SBU dataset is larger, the annotated descriptions are louder and more personalized, and the results generated can be found at http: / / www.cs.toronto.edu / ~ rkiros / lstm _ scnlm.html 8."}, {"heading": "4 Discussion", "text": "When creating a description, it is often the case that only a small region is relevant at a given time. We develop an attention-based model that collectively learns to align parts of captions with images and use this alignment to determine where to look next to dynamically modify the vectors used to condition the decoder. We also plan to experiment with LSTM decoders as well as deep and bi-directional LSTM encoders."}, {"heading": "Acknowledgments", "text": "We would like to thank Nitish Srivastava for his support with his ConvNet package and for the preparation of the Oxford Convolutional Network. We would also like to thank the anonymous reviewers of the NIPS 2014 Deep Learning Workshop for their comments and suggestions."}, {"heading": "5 Supplementary material: Additional experimentation and details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Multimodal linguistic regularities", "text": "Figure 4 illustrates sample results using a model trained on the SBU dataset. All queries were downloaded online and retrieved images are taken from the SBU images used for training. Interestingly, the resulting images depend to a large extent on the image used for the query. For example, the search for the word \"night\" retrieves arbitrary images taken at night. On the other hand, an image with a building as its focal point provides predominantly night images when \"day\" is subtracted and \"night\" is added. A similar phenomenon occurs using the example of cats, bowls and boxes. As additional visualizations, we calculated PCA projections of cars and their corresponding colors, as well as images and weather events in Figure 5. These results provide us with strong evidence of the laws that occur in multimodal vector spaces trained with linear encoders. Of course, reasonable results are only likely if (a) the content of the image is correctly recognized (the image is relevant), the subtraction, and the word is relevant."}, {"heading": "5.2 Image description generation", "text": "The SC-NLM was trained on the concatenation of training sets of both Flickr30K and Microsoft COCO. In view of an image, we first map it into the multimodal space. From this embedding, we define 2 sets of candidate conditioning vectors onto the SC-NLM: Image Embedding. The embedded image itself. Note that the SC-NLM was not formed with images, but can be conditioned on images, since the embedding space is multimodal. Top-N next words and sentences. After the first calculation of the image embedding, we get the top-N next adjacent words and training sets using cosmic similarity. These calls are treated as a \"bag of concepts,\" for which we calculate an embedding vector as the mean value of each concept. All our results use N = 5. Together with the candidate conditioning vectors, which we align to the POS sequences, we calculate candidate sequences of the NM-SC are used."}], "references": [{"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "metrics. JAIR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeffrey Dean"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Q Le", "C Manning", "A Ng"], "venue": "TACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille"], "venue": "arXiv preprint arXiv:1410.1090,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In NAACL-HLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "ICLR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Multilingual models for compositional distributional semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Li Fei-Fei"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Ng"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Yangqing Jia", "Mathieu Salzmann", "Trevor Darrell"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "In ECCV", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A deep architecture for semantic parsing", "author": ["Phil Blunsom", "Nando de Freitas", "Edward Grefenstette", "Karl Moritz Hermann"], "venue": "In ACL 2014 Workshop on Semantic Parsing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth"], "venue": "In ECCV", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["Siming Li", "Girish Kulkarni", "Tamara L Berg", "Alexander C Berg", "Yejin Choi"], "venue": "CONLL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Yezhou Yang", "Ching Lik Teo", "Hal Daum\u00e9 III", "Yiannis Aloimonos"], "venue": "In EMNLP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["Margaret Mitchell", "Xufeng Han", "Jesse Dodge", "Alyssa Mensch", "Amit Goyal", "Alex Berg", "Kota Yamaguchi", "Tamara Berg", "Karl Stratos", "Hal Daum\u00e9 III"], "venue": "In EACL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Collective generation of natural image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Alexander C Berg", "Tamara L Berg", "Yejin Choi"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Treetalk : Composition and compression of trees for image descriptions", "author": ["Polina Kuznetsova", "Vicente Ordonez", "Tamara L. Berg", "Yejin Choi"], "venue": "TACL,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan Thater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In ICCV,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "A multiplicative model for learning distributed text-based attribute representations", "author": ["Ryan Kiros", "Richard S Zemel", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "In IEEE Workshop on ASRU,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey Hinton"], "venue": "In CVPR, pages", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["Alex Krizhevsky", "Geoffrey E Hinton"], "venue": "In AISTATS,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg"], "venue": "In NIPS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Fast and robust neural network joint models for statistical machine", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Julia Hockenmaier"], "venue": "TACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "For the encoder, we learn a joint image-sentence embedding where sentences are encoded using long short-term memory (LSTM) recurrent neural networks [1].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "We show that sampling from an SC-NLM allows us to generate realistic image captions, significantly improving over the generated captions produced by [2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Furthermore, we argue that this combination of approaches naturally fits into the experimentation framework of [3], that is, a good encoder can be used to rank images and captions while a good decoder can be used to generate new captions from scratch.", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 7, "endOffset": 16}, {"referenceID": 4, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 7, "endOffset": 16}, {"referenceID": 5, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 7, "endOffset": 16}, {"referenceID": 1, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 8, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 9, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 10, "context": "Furthermore, our method builds on analogous approaches being used in machine translation [8, 9, 10, 11].", "startOffset": 89, "endOffset": 103}, {"referenceID": 11, "context": "We show that using a linear sentence encoder, linguistic regularities [12] also carry over to multimodal vector spaces.", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "This is analogous to pairwise ranking methods used in machine translation [13, 14].", "startOffset": 74, "endOffset": 82}, {"referenceID": 13, "context": "This is analogous to pairwise ranking methods used in machine translation [13, 14].", "startOffset": 74, "endOffset": 82}, {"referenceID": 3, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 64, "endOffset": 70}, {"referenceID": 4, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 64, "endOffset": 70}, {"referenceID": 5, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 133, "endOffset": 140}, {"referenceID": 14, "context": "Popular approaches include learning joint image-word embeddings [4, 5] as well as embedding images and sentences into a common space [6, 15].", "startOffset": 133, "endOffset": 140}, {"referenceID": 15, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 125, "endOffset": 128}, {"referenceID": 16, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 6, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 175, "endOffset": 178}, {"referenceID": 17, "context": "Other approaches to multimodal learning include the use of deep Boltzmann machines [16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and topic-models [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 2, "context": "Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive networks [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 18, "context": "Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive networks [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 5, "context": "Several bi-directional approaches to ranking images and captions have also been proposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive networks [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 19, "context": "From an architectural standpoint, our encoder-decoder model is most similar to [20], who proposed a two-step embedding and generation procedure for semantic parsing.", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 21, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 22, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 23, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 24, "context": "Template-based methods involve filling in sentence templates, such as triplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25].", "startOffset": 148, "endOffset": 168}, {"referenceID": 25, "context": "These approaches aim to harness existing image-caption databases by extracting components of related captions and composing them together to generate novel descriptions [26, 27].", "startOffset": 169, "endOffset": 177}, {"referenceID": 26, "context": "These approaches aim to harness existing image-caption databases by extracting components of related captions and composing them together to generate novel descriptions [26, 27].", "startOffset": 169, "endOffset": 177}, {"referenceID": 1, "context": "The initial work in this area, based off of multimodal neural language models [2], generated captions by conditioning on feature vectors from the output of a deep convolutional network.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "These ideas were recently extended to multimodal recurrent networks with significant improvements [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 26, "context": "The methods described in this paper produce descriptions that at least qualitatively on par with current state-of-the-art composition-based methods [27].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "While Bleu and Rouge have been used in the past, [3] has argued that such automated evaluation methods are unreliable and do not match human judgements.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 9, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 10, "context": "Current methods include using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10] and LSTM encoder with LSTM decoder [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "This point of view has also been used by [28] and allows us to make use of existing ideas in the machine translation literature.", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "We then review log-bilinear neural language models [29], multiplicative neural language models [30] and then introduce our structure-content neural language model.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "We then review log-bilinear neural language models [29], multiplicative neural language models [30] and then introduce our structure-content neural language model.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Long short-term memory [1] is a recurrent neural network that incorporates a built in memory cell to store information and exploit long range context.", "startOffset": 23, "endOffset": 26}, {"referenceID": 30, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 160, "endOffset": 164}, {"referenceID": 10, "context": "LSTMs have been used to achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence generation [32] speech recognition [33] and machine translation [11] among others.", "startOffset": 189, "endOffset": 193}, {"referenceID": 33, "context": "Dropout [34] strategies have also been proposed to prevent overfitting in deep LSTMs.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "[35]", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Images are represented as the top layer (before the softmax) of a convolutional network trained on the ImageNet classification task [36].", "startOffset": 132, "endOffset": 136}, {"referenceID": 35, "context": "4096 for AlexNet [36]), K the dimensionality of the embedding space and let V be the number of words in the vocabulary.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "We note that other approaches for computing sentence representations for image-text embeddings have been proposed, including dependency tree RNNs [6] and bags of dependency parses [15].", "startOffset": 146, "endOffset": 149}, {"referenceID": 14, "context": "We note that other approaches for computing sentence representations for image-text embeddings have been proposed, including dependency tree RNNs [6] and bags of dependency parses [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 36, "context": "For all of our experiments, we initialize the word embeddings WT to be pre-computed K = 300 dimensional vectors learned using a continuous bag-of-words model [37].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "The log-bilinear language model (LBL) [29] is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer.", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": "A multiplicative neural language model [30] models the distributionP (wn = i|w1:n\u22121,u) of a new wordwn given context from the previous words and the vector u.", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": "[38, 39], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7G and W \u2208 RF\u00d7V , such that T u = (Wfv)> \u00b7 diag(Wu) \u00b7W (9) where diag(\u00b7) denotes the matrix with its argument on the diagonal.", "startOffset": 0, "endOffset": 8}, {"referenceID": 38, "context": "[38, 39], we re-represent T in terms of three matrices W \u2208 RF\u00d7K , W \u2208 RF\u00d7G and W \u2208 RF\u00d7V , such that T u = (Wfv)> \u00b7 diag(Wu) \u00b7W (9) where diag(\u00b7) denotes the matrix with its argument on the diagonal.", "startOffset": 0, "endOffset": 8}, {"referenceID": 29, "context": "In [30], the conditioning vector u is referred to as an attribute and using a third-order model of words allows one to model conditional similarity: how meanings of words change as a function of the attributes they\u2019re conditioned on.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "We compared the multiplicative model against an additive variant [2] and found on large datasets, such as the SBU Captioned Photo dataset [40], the multiplicative variant significantly outperforms its additive counterpart.", "startOffset": 65, "endOffset": 68}, {"referenceID": 39, "context": "We compared the multiplicative model against an additive variant [2] and found on large datasets, such as the SBU Captioned Photo dataset [40], the multiplicative variant significantly outperforms its additive counterpart.", "startOffset": 138, "endOffset": 142}, {"referenceID": 40, "context": "Note that this model shares a resemblance with the NNJM of [41] for machine translation, where the previous word context are predicted words in the target language, and the forward context are words in the source language.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "0 500 SDT-RNN [6] 4.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "0 29 \u2020 DeViSE [5] 4.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "6 29 \u2020 SDT-RNN [6] 6.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "7 25 DeFrag [15] 5.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "5 32 \u2020 DeFrag [15] 12.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "5 15 m-RNN [7] 14.", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.", "startOffset": 75, "endOffset": 78}, {"referenceID": 41, "context": "We perform the same experimental procedure as done by [15] on the Flickr8K [3] and Flickr30K [42] datasets.", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "As with [15], we did not do any explicit text preprocessing.", "startOffset": 8, "endOffset": 12}, {"referenceID": 42, "context": "We used two convolutional network architectures for extracting 4096 dimensional image features: the Toronto ConvNet 5 as well as the 19-layer OxfordNet [43] which finished 2nd place in the ILSVRC 2014 classification competition.", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "Following the protocol of [15], 1000 images are used for validation, 1000 for testing and the rest are used for training.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "The deep visual semantic embedding model [5] was proposed as a way of performing zeroshot object recognition and was used as a baseline by [15].", "startOffset": 41, "endOffset": 44}, {"referenceID": 14, "context": "The deep visual semantic embedding model [5] was proposed as a way of performing zeroshot object recognition and was used as a baseline by [15].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "The semantic dependency tree recursive neural network [6] is used to learn sentence representations for embedding into a joint image-sentence space.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Deep fragment embeddings [15] were proposed as an alternative to embedding full-frame image features and take advantage of object detections from the R-CNN [44] detector.", "startOffset": 25, "endOffset": 29}, {"referenceID": 43, "context": "Deep fragment embeddings [15] were proposed as an alternative to embedding full-frame image features and take advantage of object detections from the R-CNN [44] detector.", "startOffset": 156, "endOffset": 160}, {"referenceID": 6, "context": "The multimodal recurrent neural network [7] is a recently proposed method that uses perplexity as a bridge between modalities, as first introduced by [2].", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "The multimodal recurrent neural network [7] is a recently proposed method that uses perplexity as a bridge between modalities, as first introduced by [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "0 500 \u2020 DeViSE [5] 4.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "7 25 \u2020 SDT-RNN [6] 9.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "1 16 \u2020 DeFrag [15] 14.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "2 14 \u2020 DeFrag + Finetune CNN [15] 16.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "5 13 m-RNN [7] 18.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "This is contradictory to [6], where recurrent networks are the worst performing models.", "startOffset": 25, "endOffset": 28}, {"referenceID": 36, "context": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibit linguistic regularities that allow these models to perform analogical reasoning.", "startOffset": 39, "endOffset": 43}, {"referenceID": 44, "context": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibit linguistic regularities that allow these models to perform analogical reasoning.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Word embeddings learned with skip-gram [37] or neural language models [45] were shown by [12] to exhibit linguistic regularities that allow these models to perform analogical reasoning.", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "We generated image descriptions for roughly 800 images from the SBU captioned photo dataset [40].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "These are the same images used to display results by the current state-of-the-art composition based approach, TreeTalk [27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "The SBU dataset contains 1 million images each with a single description and was used by [27] for training their model.", "startOffset": 89, "endOffset": 93}], "year": 2014, "abstractText": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.", "creator": "LaTeX with hyperref package"}}}