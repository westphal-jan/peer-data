{"id": "1411.2635", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2014", "title": "A chain rule for the expected suprema of Gaussian processes", "abstract": "The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models.", "histories": [["v1", "Mon, 10 Nov 2014 21:41:32 GMT  (14kb)", "http://arxiv.org/abs/1411.2635v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andreas maurer"], "accepted": false, "id": "1411.2635"}, "pdf": {"name": "1411.2635.pdf", "metadata": {"source": "CRF", "title": "A chain rule for the expected suprema of Gaussian processes", "authors": ["Andreas Maurer"], "emails": ["am@andreas-maurer.eu"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.26 35v1 [cs.LG] 1 0N ov"}, {"heading": "1 Introduction", "text": "Rademacher and Gaussian averages ([1], see also [5], [11]) provide an elegant method to show generalizations for a variety of learning algorithms and are particularly well suited to analyzing kernel machines, where the use of other classical methods based on the coverage of numbers becomes cumbersome. To briefly describe the use of Gaussian averages (Rademacher averages will not affect us), let Y averages (Rademacher averages will not affect us), let Y-Rn and let a vector be \u03b3 = (\u03b31,..., \u03b3n) of independent normal standard variables. We define the (expected superiority of) Gaussian averages of Y asG (Y) = E sup y value of Y < \u03b3, y >, where < denotes the inner product in Rn. Let us consider a loss class F of functions f: X \u2192 R, where X value is, where X-value is, where < y >."}, {"heading": "2 Proving the chain rule", "text": "To prove theorem 2, we need the theory of majorization and generic concatenation. Our use of these techniques is summarized in the following theory, which is also the origin of our great constants. \u2212 Let us look at Xy as a random process indexed by a finite set of Y. \u2212 Let us consider that there is a number K, which is for all distinguishable members y, y and all other members y, y and any other kind of (Y). \u2212 Let us look at Xy as a random process (\u2212 s22) \u2212 Let us then look at any number K (sup y y). \u2212 Let us look at YE (sup y y y y) for any number. \u2212 Let us look at one (n)."}, {"heading": "3 Applications", "text": "First, we specify some elementary properties of the quantity R (F, Y) that appears in Theorem 2. Then we apply Theorem 2 to a two-layer core machine and specify a limit for multi-task learning of low-dimensional representations."}, {"heading": "3.1 Some properties of R (F )", "text": "(R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R)) (R) (R) (R) (R) (R) (R) (R) (R) (R)) (R) (R)) (R) (R) (R) (R) (R) (R)"}, {"heading": "3.2 A double layer kernel machine", "text": "The corresponding optimization problem is clearly not convex and we are not aware of any efficient optimization method."}, {"heading": "3.3 Multitask learning", "text": "As a second illustration, we modify the above model to adapt to multiple learning processes. (1) We are looking at a two-layer situation in which the bottom layer H consists of functions. (2) We are looking at a two-layer situation in which the bottom layer H consists of functions h: X \u2192 Rm, and the top layer function class is formFT = (x). (2) We are looking at a two-layer situation in which the bottom layer H consists of functions h: X \u2192 Rm, and the top layer function class is formFT = (x). (2) F (x), fT (x). (2) We are looking at a two-layer situation in which the bottom layer H is optimized for the entire layer H, in which the top layer of each function is associated with the top layer of each function."}, {"heading": "3.4 Iteration of the bound", "text": "We apply the chain rule to multi-layer or \"deep\" learning machines, a topic that seems to be of topical interest. Here, we have the function classes F1,..., FK, where Fk consists of functions f: Rnk \u2212 1 \u2192 Rnk, and we are interested in the generalization properties of the composite classification FK \u0445 F1 = {x-Rn0 7 \u2192 fK (fK \u2212 1 (... (f1 (x)))): fk-Fk}. To specify our binding, we get some examples x in Rn0 and introduce the notation Y0 = xYk = Fk (Yk \u2212 1) = Fk-1 (x) = Fk-p (x) F1 (x) Rnk, for k > 0 Gk = miny-Yk \u2212 1 (x) G (Fk (y)))))). Under the convention that the product is above an empty index set 1, the induction shows that CG (Ck) is the prediction (Yk = 1K-1K) \u2212 K."}, {"heading": "4 Proof of Theorem 3", "text": "Talagrand produced the following result ([14])."}], "references": [{"title": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3: 463\u2013 482,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Theoretical Models of Learning to Learn, in Learning to Learn, S.Thrun, L.Pratt Eds", "author": ["J. Baxter"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "A Model of Inductive Bias Learning", "author": ["J. Baxter"], "venue": "Journal of Artificial Intelligence Research 12:149\u2013198,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Concentration Inequalities", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Rademacher processes and bounding the risk of function learning", "author": ["V.I. Koltchinskii", "D. Panchenko"], "venue": "In E. Gine, D. Mason, and J. Wellner, editors, High Dimensional Probability II, pages 443\u2013459.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "M", "author": ["M. Ledoux"], "venue": "Talagrand. Probability in Banach Spaces, Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Bounds for linear multi-task learning", "author": ["A. Maurer"], "venue": "Journal of Machine Learning Research, 7:117\u2013139,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Transfer bounds for linear feature learning", "author": ["A. Maurer"], "venue": "Machine Learning, 75(3): 327\u2013350,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "K-dimensional coding schemes in Hilbert spaces", "author": ["A. Maurer", "M. Pontil"], "venue": "IEEE Transactions on Information Theory, 56(11): 5839\u20135846,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalization error bounds for Bayesian mixture algorithms", "author": ["R. Meir", "T. Zhang"], "venue": "Journal of Machine Learning Research, 4: 839\u2013860,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "l -norm and its application to learning theory", "author": ["S. Mendelson"], "venue": "Positivity, 5:177\u2013191,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Regularity of Gaussian processes", "author": ["M. Talagrand"], "venue": "Acta Mathematica. 159: 99\u2013149,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1987}, {"title": "A simple proof of the majorizing measure theorem", "author": ["M. Talagrand"], "venue": "Geometric and Functional Analysis. Vol 2, No.1: 118\u2013125,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "Majorizing measures without measures", "author": ["M. Talagrand"], "venue": "Ann. Probab. 29: 411\u2013417,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The Generic Chaining", "author": ["M. Talagrand"], "venue": "Upper and Lower Bounds for Stochastic Processes. Springer, Berlin,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "1 Introduction Rademacher and Gaussian averages ([1], see also [5],[11]) provide an elegant method to demonstrate generalization for a wide variety of learning algorithms and are particularly well suited to analyze kernel machines, where the use of more classical methods relying on covering numbers becomes cumbersome.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Then we have the following result [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "Let the members of F take values in [0, 1] and let X,X1, .", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "2 The utility of Gaussian averages is not limited to functions with values in [0, 1].", "startOffset": 78, "endOffset": 84}, {"referenceID": 5, "context": "For real functions \u03c6 with Lipschitz constant L (\u03c6) we have G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)) (see also Slepian\u2019s Lemma, [6], [4]), where \u03c6 \u25e6 F is the class {x 7\u2192 \u03c6 (f (x)) : f \u2208 F}.", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "For real functions \u03c6 with Lipschitz constant L (\u03c6) we have G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)) (see also Slepian\u2019s Lemma, [6], [4]), where \u03c6 \u25e6 F is the class {x 7\u2192 \u03c6 (f (x)) : f \u2208 F}.", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "The inequality G ((\u03c6 \u25e6 F ) (x)) \u2264 L (\u03c6) G (F (x)), which in the above form holds also for Rademacher averages [10], is extremely useful and in part responsible for the success of these complexity measures.", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "This is a direct consequence of Slepian\u2019s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "This is a direct consequence of Slepian\u2019s Lemma and can be applied to the analysis of clustering or learning to learn ([9] and [8]).", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 173, "endOffset": 176}, {"referenceID": 13, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "The constants C1 and C2 as they result from the proof are rather large, because they accumulate the constants of Talagrand\u2019s majorizing measure theorem and generic chaining [6][14][15][16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 15, "context": "This is obtained from Talagrand\u2019s majorizing measure theorem (Theorem 6 below) combined with generic chaining [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "An early version of a similar result is Theorem 15 in [13], where the author remarks that his method of proof (which we also use) is very indirect, and that a more direct proof would be desirable.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "6 in [4]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[4], section 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "3 Multitask learning As a second illustration we modify the above model to accommodate multitask learning [2][3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "3 Multitask learning As a second illustration we modify the above model to accommodate multitask learning [2][3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "This reproduces a general property of multitask learning [3]: in the limit T \u2192 \u221e the contribution of the common representation (including the intermediate dimension m1) to the estimation error vanishes.", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "4 Proof of Theorem 3 Talagrand has proved the following result ([14]).", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "As explained in [14], the above Theorem is equivalent to the existence of a measure \u03bc on Y such that sup y\u2208Y \u222b \u221e 0 \u221a", "startOffset": 16, "endOffset": 20}], "year": 2014, "abstractText": "The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models.", "creator": "LaTeX with hyperref package"}}}