{"id": "1411.2738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2014", "title": "word2vec Parameter Learning Explained", "abstract": "The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been proven to be able to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec, I notice that there lacks a material that comprehensively explains the parameter learning process of word2vec in details, thus preventing many people with less neural network experience from understanding how exactly word2vec works.", "histories": [["v1", "Tue, 11 Nov 2014 09:24:00 GMT  (242kb,D)", "http://arxiv.org/abs/1411.2738v1", null], ["v2", "Fri, 13 Nov 2015 19:33:04 GMT  (569kb,D)", "http://arxiv.org/abs/1411.2738v2", null], ["v3", "Sat, 30 Jan 2016 21:35:51 GMT  (569kb,D)", "http://arxiv.org/abs/1411.2738v3", null], ["v4", "Sun, 5 Jun 2016 07:17:40 GMT  (569kb,D)", "http://arxiv.org/abs/1411.2738v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xin rong"], "accepted": false, "id": "1411.2738"}, "pdf": {"name": "1411.2738.pdf", "metadata": {"source": "CRF", "title": "word2vec Parameter Learning Explained", "authors": ["Xin Rong"], "emails": ["ronxin@umich.edu"], "sections": [{"heading": null, "text": "This note contains detailed derivations and explanations of the parameter update equations for the word2vec models, including the original Continuous Word Bag (CBOW) and Skip-gram models, as well as advanced tricks, hierarchical soft-max and negative samples. The appendix provides an overview of the basics of neuron network models and reverse propagation."}, {"heading": "1 Continuous Bag-of-Word Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 One-word context", "text": "We assume that there is only a single word per context, which means that the model will predict a specific target word that is like a Bigram model. Figure 1 shows the network model under the simplified context definition. In our setting, the vocabulary is only a single word per context, and the hidden layer size is N. \"The nodes on adjacent layers are fully interconnected; the input vector is a uniform encoded vector that means for a specific input context word, only a single word of {x1, xV} will be 1, and all other nodes are 0.The weights between the input layer and output layer can be represented by a V \u00b7 N layer. Each row of W is the N dimension vector vw of Figures 1, 2, 3, and the rest is not."}, {"heading": "1.2 Multi-word context", "text": "Figure 2 shows the CBOW model with a multi-word context setting. When calculating the hidden layer output, instead of copying the input vector of the input context word directly, the CBOW model takes the average of the vectors of the input context words and uses the product of the input \u2192 hidden weight matrix and the average vector as output. h = 1C W \u00b7 (x1 + x2 + \u00b7 + xC) (17) = 1C \u00b7 (vw1 + vw2 + \u00b7 \u00b7 \u00b7 + vwC) (18), where C is the number of words in the context, w1, \u00b7 \u00b7, wC are the words in the context and vw is the input vector of a word w. The loss function is E = = = \u2212 log p (wI, 1, \u00b7 \u00b7 \u00b7, wI), wI, wI, wI, wI, wI, wI, wI, (19) \u2212 wj, wp ()."}, {"heading": "2 Skip-Gram Model", "text": "Figure 3 shows the Skipgram model. It is the opposite of the CBOW model. The target word is now at the input level, and the context words are hidden at the output level. We still use vwI to denote the input vector of the only word at the input level, and therefore we have the same definition of the output level h as in (1), which means that h simply copies a line of the input matrix, W associated with the input word wI. We copy the definition of the input word wI below: h = W (k): vwI, (24) At the output level, simply copy a line of the input matrix, W associated with the input word wI. Each output is calculated using the same hidden output matrix."}, {"heading": "3 Optimizing Computational Efficiency", "text": "So far, the models we have discussed (\"Bigram\" model, CBOW and Skip-gram) are both in their original form, without using efficiency optimization tricks. For all of these models, there are two vector representations for each word in the vocabulary: the input vector vw and the output vector v \u2032 w. Learning the input vectors is cheap, but learning the output vectors is very expensive. From the update equations (22) and (33), we can figure out that in order to update v \u2032 w for each training instance, we need to run through every word wj in the vocabulary, their net input uj, the probability prediction yj (or yc, j for Skip-gram), their prediction error ej (or EIj for Skip-gram), and finally use their prediction error error to update their output vectors."}, {"heading": "3.1 Hierarchical Softmax", "text": "The model uses a binary tree to represent all words in the vocabulary; and this path is used to estimate the probability of the word represented by the leaf node. See Figure 4 for an example tree.In the hierarchical Soft Max model, there is no output vector for words. Instead, each of the V \u2212 1 inner nodes has an output vector. And the probability that a word is the output word is defined as asp (w = wO) = L (w) \u2212 j = 1 relative representation for words."}, {"heading": "3.2 Negative Sampling", "text": "The idea of negative sampling is simpler than hierarchical softmax: to deal with too many nodes that need to be calculated for an iteration, we can simply sample some of the nodes we compute. Obviously, we need to hold the output word as a positive sample in the basic truth. We need to sample several negative examples (i.e. the name \"negative sampling\"). In word2vec (Mikolov et al., 2013b), instead of using a form of negative sampling that produces a well-defined posterior multinomic distribution, the authors use the following simplified training target function per output word."}, {"heading": "4 Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Why softmax?", "text": "Using Softmax can help us obtain a well-defined probabilistic (multinomial) distribution between words. Cross-entropy is the loss function for this model. Theoretically, we can also use the square sum as an error function, but then the efficiency tricks developed for Softmax are not applied."}, {"heading": "4.2 Relationship between input vectors and output vectors?", "text": "Input vectors are taken from lines of the Input \u2192 Hidden Weight Matrix and output vectors are taken from columns of the Hidden \u2192 Output Weight Matrix. By definition, these are two different vector representations for words. Intuitively, based on the parameter update equations (16) and (11), the vectors of the same word should be close to each other. To hierarchical Softmax, the output vectors have different meanings, but if scanned negatively, you can experiment with forcing the two vectors to be the same and see what will happen."}, {"heading": "A Back Propagation Basics", "text": "We will discuss two example decisions for a single neural figure 5 showing an artificial neural (function) (step 1). {x1, \u00b7 \u00b7, xK} are input values; {w1, \u00b7 \u00b7, wK} are weights; y is a scalar output; and f is the connecting function (also called activation / decision / transfer function).The neuron works in the following way: y = f (u), (61) where u is a scalar number indicating the net input (or \"new input\") of the neuron. u is defined as asu = K \u2211 i = 0 wixi. (62) With vector notation we can write u = wTx (63) that we ignore the bias term here, which can simply add an input dimension (e.g. x0) that is constant."}], "references": [{"title": "word2vec explained: deriving mikolov et al.\u2019s negativesampling word-embedding method. arXiv:1402.3722 [cs, stat", "author": ["Y. Goldberg", "O. Levy"], "venue": null, "citeRegEx": "Goldberg and Levy,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Information Processing Systems,", "citeRegEx": "Mnih and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Curran Associates", "F. Inc. Morin", "Y. Bengio"], "venue": "Information Processing Systems", "citeRegEx": "Associates et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Associates et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "1 One-word context We start from the simplest version of the continuous bag-of-word model (CBOW) introduced in Mikolov et al. (2013a). We assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model.", "startOffset": 111, "endOffset": 134}, {"referenceID": 2, "context": "1 Hierarchical Softmax Hierarchical soft-max is an efficient way of computing soft-max (Morin and Bengio, 2005; Mnih and Hinton, 2009).", "startOffset": 87, "endOffset": 134}, {"referenceID": 0, "context": ", 2013b), word2vec uses unigram distribution raised to 3/4th power for best quality of results Goldberg and Levy (2014) provide a theoretical analysis on the reason of using this objective function.", "startOffset": 95, "endOffset": 120}], "year": 2014, "abstractText": "The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been proven to be able to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec, I notice that there lacks a material that comprehensively explains the parameter learning process of word2vec in details, thus preventing many people with less neural network experience from understanding how exactly word2vec works. This note provides detailed derivations and explanations of the parameter update equations for the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram models, as well as advanced tricks, hierarchical soft-max and negative sampling. In the appendix a review is given on the basics of neuron network models and backpropagation. 1 Continuous Bag-of-Word Model 1.1 One-word context We start from the simplest version of the continuous bag-of-word model (CBOW) introduced in Mikolov et al. (2013a). We assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model. Figure 1 shows the network model under the simplified context definition1. In our setting, the vocabulary size is V , and the hidden layer size is N . The nodes on adjacent layers are fully connected. The input vector is a one-hot encoded vector, which means for a given input context word, only one node of {x1, \u00b7 \u00b7 \u00b7 , xV } will be 1, and all other nodes are 0. The weights between the input layer and the output layer can be represented by a V \u00d7 N matrix W. Each row of W is the N -dimension vector representation vw of the In Figures 1, 2, 3, and the rest of this note, W\u2032 is not the transpose of W, but a different matrix instead. 1 ar X iv :1 41 1. 27 38 v1 [ cs .C L ] 1 1 N ov 2 01 4 Input layer Hidden layer Output layer x1 x2 x3 xk", "creator": "LaTeX with hyperref package"}}}