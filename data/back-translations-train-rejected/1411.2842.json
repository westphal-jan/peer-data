{"id": "1411.2842", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2014", "title": "Logical Limitations to Machine Ethics with Consequences to Lethal Autonomous Weapons", "abstract": "Lethal Autonomous Weapons promise to revolutionize warfare -- and raise a multitude of ethical and legal questions. It has thus been suggested to program values and principles of conduct (such as the Geneva Conventions) into the machines' control, thereby rendering them both physically and morally superior to human combatants.", "histories": [["v1", "Tue, 11 Nov 2014 15:05:01 GMT  (22kb)", "http://arxiv.org/abs/1411.2842v1", null]], "reviews": [], "SUBJECTS": "cs.CY cs.AI", "authors": ["matthias englert", "sandra siebert", "martin ziegler"], "accepted": false, "id": "1411.2842"}, "pdf": {"name": "1411.2842.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Martin Ziegler"], "emails": ["ziegler@ianus.at"], "sections": [{"heading": null, "text": "ar Xiv: 141 1,28 42v1 [cs.CY] 1 1N ov"}, {"heading": "1 Introduction and Motivation", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "1.1 Philosophical Disclaimer", "text": "The definition of autonomy and the question of whether it really exists touches on profound philosophical problems such as the separation of cause and consequence and the question of free will. Kant argued, for example, that ethics is based on autonomy. Responsibility arises only in a situation in which the (re-) actions of the actor / being are not predetermined by the circumstances in which there is the freedom to choose between several possibilities - which excludes any deterministic behavior. Indeed, many agree that responsibility also requires a kind of intelligence [Nucc14] - which poses another fundamental problem for machines [Turi50, NRZ09]. However, the considerations of the present work are independent of such hypotheses: Our first three dilemmas show different kinds of limitations of each actor, whether human or otherwise, on acting morally; while the fourth (Example 4b + c) applies to a mechanical device controlled by a Turing Machine - recognizes the general formalization of any turmiprised behavior of the church]."}, {"heading": "2 Machine Ethics and its Limitations", "text": "We present theoretical situations that confront an agent with iteratively refined types of dilettantes: they represent variants of the well-known trolley problem [Thom85]: Example 1 (lesser of two evils) An uncontrolled trolley hurls down a track towards a group of children playing and threatens a serious, if not fatal, accident. You happen to be at a level crossing and have the choice of moving it to another track - where, however, some men are at work and would seriously injure themselves. In such a case, there is simply no absolutely right choice (and classical ethics advises in many variations which of the two evils could be the lesser, i.e. a relatively preferred choice). The following situations refine this crude scenario to always present an undoubtedly advantageous of two possibilities - which the agent will find difficult to discern."}, {"heading": "2.1 Limitations to Morally Act on the Future", "text": "\"Every decision (but also the lack of it) influences the future. In order to fully assess the morality of one action against another, all its consequences must be taken into account - which is generally impossible for any agent, of course: Example 2 (lack of predetermination) Again, the car is heading towards a switch, which fortunately this time is directed towards a deserted track that will slow it down. However, you are now some distance away when you discover a notorious villain at this very switch, ready to flip it with the workers onto the other track. Your only means of stopping them is to shoot them with a gun. However, the suspect currently has a revelation to renounce all evil and let the car pass; so your shot would seriously hurt them without preventing a fatality (since this would not have occurred anyway).Note that this dilemma depends on the situation in which there is a lack of predetermination, in the sense that the villains can change their minds or not have the will to adoxic.56 Therefore, the three of the switches are sufficient to adoxic.56"}, {"heading": "2.2 Recursion-Theoretic Limitations to Machine Ethics", "text": "As the highlight of this section, example 4b + c), another variant of the trolley problem describes wherei. There is a unique \"right\" action among two elections. ii) All information becomes evidence. iii) All action is completely deterministic. iv) But it is fundamentally impossible for a computer to detect the correctness of the choice. We point out that a requirement known as (ii) in cryptography as Kerkhoff's principle is contrary to security by ambiguity: a cryptosystem should remain secure even if the enemy knows it."}, {"heading": "3 Recap of the Theory of Computation", "text": "In fact, it is such that most people who are able to establish themselves in the world, are not able to establish themselves in the world, but in the world in which they live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in,"}, {"heading": "4 Conclusion and Perspectives", "text": "In fact, it is such that most people will be able to move to another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to change the world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "4.1 LASs and the Perfect (War) Crime", "text": "In fact, it is the case that a person who disregards himself and his environment can at the same time be an object of desire (as an autonomous individual), and this to the extent that he takes himself into duty. (...) It is not as if he takes himself into duty. (...) It is as if he takes himself into duty. (...) It is as if he takes himself into duty. (...) It is as if he takes himself into duty. (...) It is as if he takes himself into duty. (...) It is as if he is taken into duty. (...) It is as if he is taken into duty. (...) It is as if he is taken into duty. (...) It is as if he is taken into duty. (...) It is as if he is taken into duty. (...) It is as if he is taken into duty."}, {"heading": "4.2 Recommended Regulations concerning AIs", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country."}], "references": [{"title": "ASIMOV: \u201cRunaround\u201d, in I, Robot (1950). Bish09. J.M. BISHOP: \u201cA Cognitive Computation Fallacy? Cognition, Computations", "author": ["I. Asim"], "venue": null, "citeRegEx": "Asim50.,? \\Q1950\\E", "shortCiteRegEx": "Asim50.", "year": 1950}, {"title": "Case09", "author": ["Editors", "Cambridge University Press"], "venue": "E. CASEY: Handbook of Digital Forensics and Investigation, Academic Press (2009). CHKW01. C.S. CALUDE, P. HERTLING, B. KHOUSSAINOV, Y. WANG: \u201cRecursively enu-", "citeRegEx": "Editors and Press,? 2014", "shortCiteRegEx": "Editors and Press", "year": 2014}, {"title": "F\u00fcre82", "author": ["Evolutionary Perspectives", "Berghahn Books"], "venue": "M. F\u00dcRER: \u201cThe tight deterministic time hierarchy\u201d, pp.8\u201316 in Proc. 14th ACM Sym-", "citeRegEx": "Perspectives and Books,? 1998", "shortCiteRegEx": "Perspectives and Books", "year": 1998}, {"title": "KMG*14", "author": ["Arms Control"], "venue": "S. KOHLBRECHER, J. MEYER, T. GRABER, K. PETERSEN, O. VON STRYCK,", "citeRegEx": "Control,? 2013", "shortCiteRegEx": "Control", "year": 2013}, {"title": "Kris09", "author": ["LNAI"], "venue": "A. KRISHNAN: Killer Robots: Legality and Ethicality of Autonomous Weapons, Ash-", "citeRegEx": "LNAI,? 2014", "shortCiteRegEx": "LNAI", "year": 2014}, {"title": "Lem61. S. LEM: Powr\u00f3t z gwiazd (Engl.: Return from the Stars, German: Transfer)", "author": ["LIM D"], "venue": "Robotics (Intelligent Robotics and Autonomous Agents,", "citeRegEx": "D.,? \\Q2012\\E", "shortCiteRegEx": "D.", "year": 2012}, {"title": "Sips97", "author": ["Penguin Press"], "venue": "M. SIPSER: Introduction to the Theory of Computation, PWS Publishing (1997). Spa09a. R. SPARROW: \u201cBuilding a Better WarBot: Ethical issues in the design of unmanned sys-", "citeRegEx": "Press,? 2009", "shortCiteRegEx": "Press", "year": 2009}, {"title": "TONKENS: \u201cShould autonomous robots be pacifists?", "author": ["R. Tonk"], "venue": "Ethics and Information Technology", "citeRegEx": "Tonk13.,? \\Q2013\\E", "shortCiteRegEx": "Tonk13.", "year": 2013}, {"title": "TURING: \u201cOn Computable Numbers, with an Application to the Entscheidungsproblem", "author": ["A.M. Turi"], "venue": "Proc. London Math. Soc", "citeRegEx": "Turi36.,? \\Q1936\\E", "shortCiteRegEx": "Turi36.", "year": 1936}, {"title": "TURING: \u201cComputing Machinery and Intelligence", "author": ["A.M. Turi"], "venue": "Mind", "citeRegEx": "Turi50.,? \\Q1950\\E", "shortCiteRegEx": "Turi50.", "year": 1950}, {"title": "ALLEN: Moral Machines: Teaching Robots Right from Wrong", "author": ["C. WaAl10. W. WALLACH"], "venue": null, "citeRegEx": "WALLACH,? \\Q2010\\E", "shortCiteRegEx": "WALLACH", "year": 2010}, {"title": "WINFIELD: \u201cFive roboethical principles \u2014 for humans", "author": ["A. Winf"], "venue": "New Scientist", "citeRegEx": "Winf11.,? \\Q2011\\E", "shortCiteRegEx": "Winf11.", "year": 2011}, {"title": "ZIEGLER: \u201cComputational Power of Infinite Quantum Parallelism", "author": ["M. Zieg"], "venue": "pp.2057\u20132071 in International Journal of Theoretical Physics (IJTP)", "citeRegEx": "Zieg05.,? \\Q2005\\E", "shortCiteRegEx": "Zieg05.", "year": 2005}, {"title": "ZIEGLER: \u201cPhysically-Relativized Church-Turing Hypotheses", "author": ["M. Zieg"], "venue": "Applied Mathematics and Computation", "citeRegEx": "Zieg09.,? \\Q2009\\E", "shortCiteRegEx": "Zieg09.", "year": 2009}], "referenceMentions": [], "year": 2014, "abstractText": "Lethal Autonomous Weapons promise to revolutionize warfare \u2014 and raise a multitude of ethical and legal questions. It has thus been suggested to program values and principles of conduct (such as the Geneva Conventions) into the machines\u2019 control, thereby rendering them both physically and morally superior to human combatants. We employ mathematical logic and theoretical computer science to explore fundamental limitations to the moral behaviour of intelligent machines in a series of Gedankenexperiments: Refining and sharpening variants of the Trolley Problem leads us to construct an (admittedly artificial but) fully deterministic situation where a robot is presented with two choices: one morally clearly preferable over the other \u2014 yet, based on the undecidability of the Halting problem, it provably cannot decide algorithmically which one. Our considerations have surprising implications to the question of responsibility and liability for an autonomous system\u2019s actions and lead to specific technical recommendations.", "creator": "LaTeX with hyperref package"}}}