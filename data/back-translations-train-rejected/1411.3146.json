{"id": "1411.3146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2014", "title": "Distributed Representations for Compositional Semantics", "abstract": "The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP.", "histories": [["v1", "Wed, 12 Nov 2014 11:26:51 GMT  (624kb,D)", "http://arxiv.org/abs/1411.3146v1", "DPhil Thesis, University of Oxford, Submitted and accepted in 2014"]], "COMMENTS": "DPhil Thesis, University of Oxford, Submitted and accepted in 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karl moritz hermann"], "accepted": false, "id": "1411.3146"}, "pdf": {"name": "1411.3146.pdf", "metadata": {"source": "CRF", "title": "Distributed Representations for Compositional Semantics", "authors": ["Karl Moritz Hermann"], "emails": [], "sections": [{"heading": null, "text": "Distributed substitutions for compositional semanticsKarl Moritz Hermann New CollegeUniversity of OxfordA PhD thesis on the doctorate of philosophyHilary 2014ar Xiv: 141 1,31 46v1 [cs.CL] 1 2N ov2 014"}, {"heading": "Acknowledgements", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) (...) (...) () () () (...) () () () () () () () () () () () ()) () () () () () ()) () () () () ()) () () () () () () () () () () () () () () () () ()) () () () () () () () ()) () () () () () () () () () ()) () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () (() () () (() () () (() () () (() () () (() () () () (() () () (() () () ((() (() (() () () ((() () () (() ("}, {"heading": "1 Introduction 1", "text": "1.1 Objectives of this thesis.................................................................................................................................................."}, {"heading": "I Distributed Semantics 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Distributed Semantic Representations 8", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Frame Semantic Parsing with Distributed Representations 21", "text": "The question of whether the dead really are human beings has not yet been clarified, and the question of whether the missing are people who were in the vicinity has not yet been clarified."}, {"heading": "II Compositional Semantics 46", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Compositional Distributed Representations 47", "text": "....,..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 The Role of Syntax in Compositional Semantics 74", "text": ". Introduction..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 Multilingual Approaches for Learning Semantics 90", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "III Conclusions and Further Work 107", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Further Research 108", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Conclusions 110", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Semantic-Frame Parsing: Argument Identification 113", "text": "A.1 Learning and Conclusion.................................. 114"}, {"heading": "B FrameNet Development Data 116", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C CCG Categories for CCAE Models 117", "text": "The results of the study are not manageable; the results of the study are not manageable; the results of the study are not manageable; the results of the study are not manageable; the results of the study are not manageable; the results of the study are not manageable; the results of the study are not manageable; the results of the study are not manageable; and the results of the study are not manageable."}, {"heading": "1.1 Aims of this thesis", "text": "The primary objective of this paper is to investigate the use of distributed representations to capture semantics and to evaluate their effectiveness in solving tasks in the NLP for which a certain amount of semantic understanding would be beneficial. Our hypothesis is that distributed representations are a highly appropriate mechanism for capturing and manipulating semantics, and furthermore that this meaning can be encrypted both at the word level and beyond at the distribution level. In the course of this work, we evaluate this hypothesis in several ways. In order to determine the suitability and effectiveness of distributed representations for capturing semantics, we apply these representations to a number of popular and important tasks in the NLP. We evaluate the performance of models supported by distributed semantic representations in relation to the performance of alternative, state-of-the-art solutions. As we ultimately analyze the previous state of the art for a number of such problems by using relatively simple models in combination with distributed semantic representations, we place this part of the hypothesis on two distributed representations."}, {"heading": "1.2 Contributions", "text": "Here we summarize the main contributions of this theory.The task in Chapter 3 - Frame semantic analysis - is a very popular and important task within the NLP. The chapter contributes to the thesis. Firstly, by presenting a complete frame semantic pipeline as part of our experiments, and secondly, by putting a new state of the art on this task, we discover a number of important factors to consider when using distributed representations. Secondly, we present a complete frame semantic pipeline as part of our experiments, and contribute to the field by defining a new state of art in this way. Thus, we have not only validated our thesis about distributed semantic representations, but also the overall performance of this pipeline."}, {"heading": "1.3 Thesis Structure", "text": "This year, it has come to the point where you are able to live in a country where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country."}, {"heading": "2.1 Introduction", "text": "In this chapter we give an overview of popular methods for learning distributed word similarities. We begin in \u00a7 2.2 by discussing the role of semantics in the processing of natural language. \u00a7 2.3 describes the distribution calculation of semantics and how it can be used to learn distributed representations in an unattended environment. \u00a7 2.4 then discusses alternative methods for learning distributed representations that go beyond a purely distributive approach. Finally, we examine the previous work on and use of distributed representations for words in \u00a7 2.6. In literature, distributed word representations are often referred to as word embeddings; please note that we will use these two terms interchangeably during these theories. Words can be presented as discrete units by mapping a string of integers by looking up words in a dictionary. However, it is often better to represent words by 8s by going beyond their surface shape and trying to capture semantic aspects of their representation."}, {"heading": "2.2 Semantics", "text": "Although there is little doubt about the usefulness of semantic information, the question of how this knowledge can be \"acquired, organized, and ultimately used in language processing and comprehension has been a topic of great debate in cognitive science\" (Mitchell and Lapata, 2010). Semantics has been represented in many ways throughout literature. Broadly speaking, such representations of semantics can be categorized into function-based models and semantic spaces. Also, the related concept of semantic networks deserves a mention and is briefly discussed along with the other two presentations below."}, {"heading": "2.2.1 Feature-Based Representations", "text": "Feature-based models attempt to capture specific aspects of semantics, either through a list of pre-defined features or by learning attributes that are relevant to the sense of a word by human annotators (Andrews et al., 2009; McRae et al., 1997, et al.).Thesauri and other lexicographic resources as the WordNet project (Fellbaum et al., 1998) can provide some such semantic features by providing relational information for words like hypernomy and hyponymy, meronymy or synonymy and antonymy.9Related lines of work include super-tagging (Bangalore and Joshi, 1999) and later supersense-tagging (Ciaramita and Johnson, 2003; Curran, 2005)."}, {"heading": "2.2.2 Semantic Networks", "text": "Semantic networks describe semantic relationships between entities or concepts. Conventionally, such networks are represented as directed or undirected graphs, with nodes representing concepts and vertices (edges) between nodes representing relationships. Peirce (1931) first proposed the idea, applying it to semantics proposed in Richens (1956) and Richens (1958) and developed by Collins and Quillian (1969). WordNet, introduced in \u00a7 2.2.1, is an example of such a semantic network in which words - concepts - are linked by relationships such as synonymy or meronymy. Alternative networks use more explicit relationships, such as IS-A and SIBLING-OF relationships. Semantic similarity tends to be measured by the path length between two concepts. Semantic networks are popular for certain tasks."}, {"heading": "2.2.3 Semantic Space Representations", "text": "An alternative approach to the representation of words we are examining in this thesis is distributed representations or semantic spatial representations, where words are represented by mathematical objects, often vectors. Conventional dictionary-based methods of representing words as indexes can be used to represent words as vectors. In this case, word vectors would have the size of the dictionary and each word would be captured by a vector containing zeros in all positions except one in the position of their index. This is known as the most consistent representation. Among the obvious shortcomings of uniform representations are their high dimensionality, their inability to handle words from the vocabulary (OOV), and also their lack of robustness in terms of conciseness, since no information about words is shared. Better results can be achieved by presenting words as continuous vectors, with each dimension representing a latent category (e.g. a semantic or syntactic property)."}, {"heading": "2.3 Distributional Representations", "text": "Distributional representations encode an expression through its environment by assuming the context-dependent nature of the meaning according to which \"a word will be known from the company that holds it\" (Firth, 1957). The underlying idea of this distribution strategy of semantics - in short, from the quote above - is that the meaning of words can be derived from their use and the context in which they appear, which also means that words with similar distributions across the contexts in which they appear have a similar meaning. Thus, for example, we assume that the words bicycle and bike would occur in similar contexts, whereas the contexts of bicycle and oranges would be quite different. This distribution hypothesis provides the basis for the statistical semantics that permits the inferences of the semantics of distributed information."}, {"heading": "2.3.1 Learning distributional representations", "text": "The vectors for semantic distribution models are generally generated from corpus data using the following method: 1. For each word in a lexicon, its appearance relationships are collected from a corpus, based on a context selection criterion (e.g. tokens within k words of the target word or linked together by a dependency or other syntactical relationship).2. These contexts are processed to transform or filter the information contained therein (e.g. only taking into account the context words from the most common n words in a corpus or those from specific syntactic classes).3. These contexts of occurrence are encoded in a vector in which each vector component corresponds to a possible context, and each component weight corresponds to how often the target word occurs in that context. 4. Optionally, the vector component weights are reweighted by a function (e.g. Term-frequency-document-frequency-frequency-ratio-frequency)."}, {"heading": "2.3.2 Weighting Techniques", "text": "A commonly used mechanism for improving the quality of extracted distribution vectors is to apply some form of normalization; the purpose of normalizing vectors is to maximize their information content and / or to pre-process such vectors for later use in composition models or other functions where input is expected to be bound by a certain range or correspond to a certain distribution; likewise, vectors are often normalized to form a probability distribution. TF-IDF The term frequency-inverse document frequency (TF-IDF) is a statistical measure often used for this purpose (Spa \ufffd rck Jones, 1988). TF-IDF derives from information retrieval and text mining and is used to weight words by their semantic content. In its simplest form, TF-IDF provides a weight for each word composed by taking into account two statistics."}, {"heading": "2.4 Neural Language Models", "text": "Neural language models are another popular approach to generating distributed word representations, first developed by Y. Bengio and co-authors (Bengio et al., 2003), and then explored by others (Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010), performing well on various tasks. For example, the neural language model described by Mikolov et al. (2010) learns word embeddings along with additional transformation matrices that are used to predict the next word when a context vector is generated by the previous words. Collobert et al. (2011) expanded the popularization of neural network architectures for learning word embeddings from large amounts of largely unidentified data by showing that the embeddings can then be used to improve standard monitored tasks, e.g. in a semi-monitored setup. Unmonitored word embeddings can easily be embedded in a variety of NLPs."}, {"heading": "2.5 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.5.1 LSA, LSI and LDA", "text": "Latent Semantic Analysis (LSA, henceforth) (Dumais et al., 1988) describes a mechanism for extracting latent semantic information from words in context. In the context of information extraction, LSA is also known as latent semantic indexing (LSI). 15LSA uses a term-document matrix that describes the occurrence of terms in documents. A lower approximation is found for this matrix X, using the k largest singular values of X = UVT, where U and V are orthogonal matrices, and a diagonal matrix that questions the singular values. Using the decomposition UVV makes it possible to find the best k-rank approximation for X. While LSA is typically used in conjunction with word models that focus on learning topic representations for documents, it can also be applied to distribution possibilities."}, {"heading": "2.5.2 Dimensionality Reduction Techniques", "text": "In addition to specific methods such as LSA, there are a number of general statistical methods for reducing the rank of vectors that can easily be applied to both distributed and distributed word representations. As some of the methods presented in this chapter, especially those for extracting distribution representations, can lead to very large vectors, these methods are very useful both for reducing the disparity through smoothing and for improving the efficiency of subsequent models taking such representations into account. The main difference is that PCA uses a term coding matrix instead of a term-document matrix (above), which is processed using decomposition and orthogonal vectors to take into account the correlation between individual vector elements (matrix series)."}, {"heading": "2.5.3 Similarity Metrics", "text": "For many tasks it is necessary to evaluate the similarity between multiple distributed representations, such as word-word similarity tasks (similarity between two 17 representations), unattended clustering (cluster a number of units with distributed representations), or annotation tasks (search for the nearest name for a representation in a given room).Depending on the model settings and normalization options, cosmic similarities (Eq.2.1) or euclidean distances (Eq. 2.2) can be used to create such tasks.Cos (~ x, ~ y) = cos (\u03b8) = x \u00b7 y \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2."}, {"heading": "2.6 Applications", "text": "Semantic spatial representations can easily be incorporated into a variety of tasks related to NLP. The following list is by no means exhaustive, but is intended to demonstrate the usefulness of such representations in a wide range of areas in NLP. Topic modeling has been researched using distributed representations, especially in the context of LDA (Lead et al., 2003; Steyvers and Griffiths, 2005) Other areas include thesaurus extraction (Grefenstette, 1994; Curran, 2004), literal discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer et al., 1997), synonymy or word-word similarity (McDonald, 2000; Griffiths et al., 2007; Mitchell and Lapata, 2008), designated holistic representations (Collobert et al.; Dumais, 1997), synonymy or word-word similarity (Griffiths et al."}, {"heading": "2.7 Summary", "text": "In this chapter, we have examined the field of semantics in the context of processing natural language. After briefly describing the various strands of semantic frameworks popular in this area, this chapter will then focus on distributed semantic representations in particular. Having explained how such distributed representations can be learned, we are now able to evaluate the hypothesis that takes up this thesis. Chapter 3 begins this evaluation by describing a relatively simple model that uses distributed representations to solve the step of framework identification of the semantic framework analysis task. As this is both an important and a popular task within the NLP community, it surpasses a number of previous models of this task with this simple approach, supporting our hypothesis on the effectiveness of distributed representations and their use in solving challenging tasks in the NLP."}, {"heading": "3.1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2 Frame-Semantic Parsing", "text": "In fact, it is so that it will be able to erenieSn the aforementioned cerebrospinal vessels cerebrospinal vessels csrteeSe rf\u00c3 \u00bc r the cerebrospinal vessels csrteeaeSn."}, {"heading": "3.2.1 FrameNet", "text": "The FrameNet project (Baker et al., 1998) is a lexical database that contains information about words and phrases (represented as a lexicon in conjunction with a rough part of the speaker24John bought an Auto.COMMERCE _ BUY buy.VBuyer GoodsJohn bought an Auto.buy.01 buy.VA0 A1Mary sold an Auto.COMERCE _ BUY sell.VMary sold an Auto.sell.01 sell.Vtag), which is called lexical units, with a set of semantic frames that they could evoke. For each frame there is a list of associated frame elements (or roles, in the future), which can also be distinguished as core or non-core elements.1 sentences are commented on using this universal frame inventory. Consider, for example, the pair of sentences in Figure 3.1 (a). COMMERCE BUY is a frame that can be morphologically variants of the example of two evocative units."}, {"heading": "3.2.2 PropBank", "text": "The PropBank project (Palmer et al., 2005) is another popular resource related to the semantic role designation. The PropBank corpus has verbs that are commented on with meaning frames and their arguments. Like FrameNet, it also has an lexical database that stores type information about verbs in the form of meaning frames and possible semantic roles that any frame could assume. There are modifier roles that are shared across verb frames, similar to the noncore roles in FrameNet. Figure 3.1 (b) shows annotations for two verbs that are \"bought\" and \"sold\" with their lexical units in FrameNet and their verb frames buy.01 and 1Additional information such as a finer distinction of the core properties of roles, the relationship between the frames and the roles are also present, but we do not use this information in this work."}, {"heading": "3.3 Model Overview", "text": "This year it is more than ever before."}, {"heading": "3.4 Frame Identification with Embeddings", "text": "The fact is that we see ourselves in a position to be in and that we are in a position to be in a position to be in a position to be in another world, to be in a position to be in another world, to be in a position to be in, to be in, to be in, \"he said."}, {"heading": "3.4.1 Context Representation Extraction", "text": "Basically, it is as it is."}, {"heading": "3.4.2 Learning", "text": "We model our objective function following Weston et al. (2011) using a weighted approximate value: pair loss (learned with stochastic gradient descent).The mapping of g (x) to the low-dimensional space Rm is a linear transformation, so that the model parameters to be learned minimize the matrix M, Rkn \u00b7 m, as well as the embedding of any frame label, represented as another matrix Y, RF \u00b7 m, if there are total F frames. The training objective function minimizes x, y, and L (ranky (x, y) - the embedding of any frame label, represented as another matrix, y are the training inputs and their correct frames, and y are negative frames (which do not correspond with x), the margin is s (x, y) is the score between an31input and a frame."}, {"heading": "3.5 Experiments", "text": "In this section we present our experiments in the field of frame semantic parsing. As already explained in \u00a7 3.1, we first evaluate our frame identification system in isolation to assess whether distributed representations can be a suitable choice for a problem. Then, we combine our system with a standard argument identification system (Appendix A), which allows us to compare performance with the previous state of the art. While we used a standard approach for the step of reasoning, we used a user-defined implementation and a number of new features. To evaluate the 32 effects of the distributed model fairly, we therefore also combine our argument identification model with a standard log-linear frame identification model for comparison (see 3.5.2)."}, {"heading": "3.5.1 Data", "text": "For FrameNet, we use the full-text annotations of FrameNet 1.5 Release 4, used by Das et al. (2014). We used the same test set as Das et al., which contained 23 documents with 4,458 predicates. Of the remaining 55 documents, 16 were randomly selected for development (Appendix B), resulting in a development set with a similar number of predicates. For experiments with PropBank, we used the OntoNotes corpus (Hovy et al., 2006), version 4.0, and only the Wall Street Journal documents. In accordance with the convention, we used Sections 2-21 for training, Section 24 for development, and Section 23 for testing. This split is also similar to the setup of Punyakanok et al. (2008). All verb frame files in OntoNotes were used to create our framework lexicon."}, {"heading": "3.5.2 Frame Identification Baselines", "text": "In this case, the base models use the following probabilities: p (y, \"x\") = e, where argmax is iterated over the possible frames y, F \"if\" s in the lexicon or the training data, if it is unseen, like the argmaxy.f \"E\" s is where argmax is iterated. \""}, {"heading": "3.5.3 Common Experimental Setup", "text": "We process our PropBank and FrameNet training, development and test corpora with a shift-reducing dependency parser that uses the Stanford conventions (de Marneffe and Manning, 2013) and which is based on an arc-shaped transition system with a beam size of 8; the parser and its properties are described by Zhang and Nivre (2011). Before parsing the data, it is provided with a POS tagger trained on a contingent random field (Lafferty et al., 2001) with the following emission characteristics: the word, the word cluster, word suffixes of length 1, 2 and 3, uppercase whether it has a hyphen, a digit, and punctuation. Beyond the bias transition characteristic, we have two cluster characteristics for the left and right words in the transition. We use brown clusters that we have learned using the Uszkoreit and Brants algorithm (2008) on a large news wire cluster."}, {"heading": "3.5.4 Experimental Setup for FrameNet", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.5.4.1 Hyperparameters", "text": "We optimize the hyperparameters for the WSABIE training based on our developmental data. There are three relevant parameters: the stochastic learning rate of the gradient descent, which we set to 0.0001; the margin \u03b3, which we set to 0.01; and the dimensionality of the articular space Rm, where we set m to 256. See Table 3.1 for an overview of the parameters we evaluated in the hyperparameter search. Our hyperparameter sweep optimizes the accuracy of frame identification, i.e. the performance of the model in terms of identifying frames of lexical units with more than one possible semantic frame."}, {"heading": "3.5.4.2 Argument Candidates", "text": "Since the original algorithm was designed for verbs, we expanded the rulebook to include non-verbal predicates as follows: 1. We added the predicate itself as an argument candidate; 2. We added the range from the position of the sentence to the right side of the predicate to the right index of the sub-tree under the guidance of the predicate head; this helps to capture cases such as \"a few months\" where few are the predicate and months are the argument; 353. We added the range from the leftmost index of the predicate tree under the guidance of the predicate head to the position immediately in front of the predicate, for cases such as \"your gift to goodwill,\" where to the predicate and your gift is the argument. Note that Das et al. (2014) described the state of the art in FrameNet analysis based on larger network strategies, but considered all dependencies to be possible."}, {"heading": "3.5.4.3 Frame Lexicon", "text": "In our experimental setup, we scanned the XML files in the Frames directory of the FrameNet 1.5 release, which lists all frames, the corresponding roles, and the related lexical units, and created a Frame Lexicon that can be used in our Frame and Argument Identification Models. A Frame Lexicon extracted with such a procedure includes all the relevant units for our test case. Therefore, at the Frame Disambiguation Time, we only have to evaluate the frames in F \"for each predicate.\" Essentially, this means that we only had to select a semantic frame from a small list of candidates in all cases, and not from the global set of available frames in FrameNet. Please refer to \u00a7 3.4 and \u00a7 3.5.2 for more details. We refer to this setup as FULL LEXICON.When trying to compare our system with the previous state of the art (that we found unvisible in 2014, al, several of which we have found)."}, {"heading": "3.5.4.4 ILP Constraints", "text": "For FrameNet, we used three integer linear programming constraints (ILP) in arguing (Appendix A): first, each span could have only one role; second, each central role could only have one role; and third, all open arguments did not have to overlap. 36"}, {"heading": "3.5.5 Experimental Setup for PropBank", "text": "From the point of view of frame identification, PropBank and FrameNet differ in one important respect. PropBank frames are predicate-specific, so for example run.01 is a sense of run, meaning to operate a machine or a company, and has a very similar meaning to operate.01, but if the system sees the word running in a sentence, the sense run.01 is available, while operate.01 is not available. In contrast, FrameNet frames are divided between verbs and both operational and executed can evoke the frame OPERATING A SYSTEM. Due to this difference, the possible experiments for PropBank and FrameNet differ in a number of aspects."}, {"heading": "3.5.5.1 Hyperparameters", "text": "We report on the hyperparameters in Table 3.1, which we selected by searching over the same space as described above for the FrameNet case. Again, we optimized the search for ambiguous lexical units in the hyperparameter search."}, {"heading": "3.5.5.2 Argument Candidates", "text": "For PropBank we use the algorithm of Xue and Palmer (2004), which is applied to dependency trees. Since PropBank only takes into account predicates, there was no need to adapt the algorithm."}, {"heading": "3.5.5.3 Frame Lexicon", "text": "For the PropBank experiments we scanned the frame files for theses in OntoNotes 4.0 and stored possible core roles for each verb frame. The lexical units were simply the verb associated with the verb frames. At test times there were no invisible verbs."}, {"heading": "3.5.5.4 ILP Constraints", "text": "For PropBank, we used five ILP constraints: the first three were the same as those for FrameNet (\u00a7 3.5.4.4). We also ensured that: 4) continuation arguments of the form C-A * could only occur after the corresponding open A * argument and 5) relative arguments of the form R-A * could only occur if the corresponding A * argument was present. These constraints are identical to those proposed and applied in Punyakanok et al. (2008) 3738"}, {"heading": "3.5.6 FrameNet Results", "text": "Tables 3.2 and 3.3 present accuracy results for frame identification. We present results for all predicates, ambiguous predicates that can be seen in the lexicon or training data, and rare ambiguous predicates that show up \u2264 11 times in training data. WSABIE EMBEDDING model performs significantly better than the LOG LINEAR WORDS base model, while LOGLINEAR EMBEDDING performs worse in all metrics.For the SEMAFOR LEXICON configuration, we compare with the state of the art of Das et al. (2014), which used a semi-monitored learning method to improve a monitored latent-variable log-linear model. We outperform their system on every metric, including the invisible predicate setting. In removing the artificial limitations on our lexicon - introduced for a fair comparison with the ABet al model - increase the accuracy BEING 73% in 2014."}, {"heading": "3.5.7 PropBank Results", "text": "Tables 3.6 and 3.7 show the results of frame identification on the PropBank development and test data, respectively. On the development scale, our best model performs best on all and ambiguous predicates, but performs worse on rare ambiguous predi-3940cates. On the test scale, the LOG-LINEAR WORDS base model performs best by a very narrow margin. We analyze this in the discussion section \u00a7 3.6. Following the FrameNet structural set of experiments, we present the results on full frame semantics in Tables 3.8 and 3.9. The results follow the same trend as for the frame identification task. Finally, we present in Tables 3.10 and 3.11 SRL results, which measure argument performance only, regardless of the choice of the frame. This task, for which we use the evaluation script from CoNLL 2005 (Carreras and Ma rquez, 2005), allows us to compare the results of the PropBank with the results of the system, sometimes the results of the SL are better than the results of the system."}, {"heading": "3.6 Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.7 Summary", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4.1 Introduction", "text": "So far, we have covered distributed models of semantics at the word level in this thesis. However, a semantic representation of individual words is not sufficient for a number of important problems, but rather a semantic representation of a larger structure - e.g. a phrase or a sentence. As explained in \u00a7 3.7, the WSABIE EMBEDDDING model of Chapter 3 was a borderline case, as the model has learned to predict terms consisting of several words, but these words are separated. In other cases, if one wants to determine the perception or correctness of a sentence, such an approach is probably insufficient, and instead a complete representation of the full sentence is more likely to succeed. Therefore, research in recent years has shifted from the use of distributed methods to model the semantics of words to the use of words."}, {"heading": "4.2 Theoretical Foundations", "text": "The meaning of an enunciation is a function of the meanings of its parts and their compositional principles. (G. Frege, 1892) Ever since Frege explained his \"principle of semantic compositivity\" in 1892, researchers have thought about both how the meaning of a complex expression is determined by the meanings of its parts, and how these parts are combined (Frege, 1892; Pelletier, 1994). More than a hundred years after the choice of the representative entity for this process of compositional semantics, and how these units combine with each other, open questions remain. Within natural language processing, work on semantic representations can be roughly divided into two different categories. On the one hand, we are dealing with Montagovian approaches dealing with symbolic entities composed into logical representations of meaning. Here, meaning is typically associated with sentences expressed as logical formulas abstract types. On the other hand, we are starting from distributed approaches such as learning from this thesis, continuous representations."}, {"heading": "4.3 Architectures", "text": "Different architectures exist for the composition of distributed representations. We provide an overview of a number of relevant approaches as proposed in the literature, based on the function defined above (Equation 4.1). We are trying to structure previous work in this field on the basis of two dimensions (see Figure 4.1). Firstly, we distinguish between the type of composition function, which can be roughly divided between algebraic composition models and lexical function models. On the sec-51ond axis, we distinguish between distributed and distributed approaches, where distributed approaches are based on collocational or other distributed information for model learning and word representation."}, {"heading": "4.3.1 Algebraic Composition", "text": "Algebraic composition functions simplify Equation 4.1 by removing the background knowledge K, and often also the relationship information R, effectively reducing the function signature top = f (u, v) (4.2). Addition can be considered the simplest form of algebraic composition. Suppose representations for words red, apple as ~ vapple. Then, under the additive model, we would call red apples ~ vred apple = ~ vred apple = ~ vappleAddition (or mean) is successfully applied to some problems, such as the gradation of words (Landauer and Dumais, 1997) or selective preferences (Kintsch, 2001). However, addition as a composition function does not make use of syntactic information or for this material word."}, {"heading": "4.3.2 Lexical Function Models", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they are able to live, in which they, in which they, in which they"}, {"heading": "4.3.3 Recursive Composition", "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far."}, {"heading": "4.4 Learning Signals", "text": "The models discussed in paragraph 4.3.3 provide an architecture for creating composed representations with a lexicon of input (word) embedding and a number of model parameters \u03b8, while we are agnostic about how these parameters and input embedding are learned. In this section we will examine the most popular types of learning signals used in conjunction with compositional vector space models."}, {"heading": "4.4.1 Autoencoders", "text": "Autoencoders are a useful tool for compressing information. One can imagine an autoencoder as a funnel through which information must flow (see Figure 4.5). By forcing the autoencoder to reconstruct an input that contains only the reduced amount of information, it serves as a compression tool that represents high-dimensional objects in a low-dimensional space. Typically, a given autoencoder, i.e. the functions for encoding and reconstructing data, is used on multiple inputs. By optimizing the two functions to minimize the difference between all inputs and their respective reconstructions, this autoencoder will effectively uncover some hidden structures within the data that can be used to render them more efficiently. As a simple example, one can assume that input vectors xi and Rn, i.. N), weight matrices W enc and W are used."}, {"heading": "4.4.1.1 Recursive Autoencoders", "text": "In \u00a7 4.4.1 we have introduced autocoders as a simple mechanism to extract latent structures by forcing data to learn a common compression and reconstruction regime. It is possible to apply multiple autocoders to each other to create a deep autocoder (Bengio et al., 2007; Hinton et al., 2006). For such a multi-layer model to61learn everything that a single layer could learn, a non-linear transformation g must be applied to each layer. Usually, a variant of the logistics (\u03c3) or hyperbolic tangent (tanh) function is used for g (LeCun et al., 1998b).f enc enc (W encxi + b enc) f rec (ei) f rec (W recei) = g (W recei + b rec) rec)."}, {"heading": "4.4.1.2 Unfolding Autoencoders", "text": "Autoencoder unfolding is an extension of recursive autoencoders, where each reconstruction step is applied recursively until an original input is reconstructed (Socher et al., 2011a). Figure 4.7 shows this: Where a standard RAE would stop the y2 reconstruction step at y \u2032 1, the unfolding autoencoder continues its recursive reconstruction by unfolding y \u2032 1 in x \u2032 1 and x \u2032 2, thus reconstructing data from the input layer. Autoencoder unfolding has a number of nice properties. In particular, the unfolding prevents an RAE from degenerating, as standard recursive autoencoders receive incentives to learn small weights for all internal layers, shrinking the entire reconstruction error. Since the unfolding autoencoder always measures its error function by comparing it with input weights, this strategy will of course become null if input weights of the learning process are updated to zero as part of the learning process."}, {"heading": "4.4.1.3 Denoising Autoencoders", "text": "Another variant of autoencoders denotes autoencoders. Here, the idea is to force the hidden layer to discover structures in the data by reconstructing the input data from a corrupt version of itself, with the idea that this process will improve the robustness of the detected representations (Vincent et al., 2008). Denosis of autoencoders is effectively a stochastic version of regular autoencoders that corrupt the input layer before it is fed into the encryption function. However, there are a number of possibilities for this corruption function. In Vincent et al. (2008), it consists of randomly setting a number of inputs to zero. Alternative corruption processes could introduce a random amount of Gauss noise before being fed into the encryption function.Denoization of autoencoders therefore works as follows (using RecNN notation): x 1 q (x 1 x 1) x 2 q + (x) x (2) x (x) x (2) x (x x) 1 (x)."}, {"heading": "4.4.2 Classification", "text": "While autoencoders simply learn to compress data efficiently by using latent structures within the data, we often want to build models for a specific task (e.g., for a paraphrase recognition task, for which we are interested in a robust semantic representation that allows us to identify phrases with similar meaning, and therefore autoencoders can be a suitable mechanism for learning such a system. On the other hand, if we want to train a system to detect feelings in the text, it might be more useful to use some training data to support the composition function on aspects of representation that are relevant to feelings. To this end, classification systems and errors associated with a semantic composition process can be used. An overarching classification layer can be applied to the root node of a recursive composition model, or indeed to all tree nodes of the model that are relevant to feeling.For this purpose, classification systems and errors associated with a composing process can be used."}, {"heading": "4.4.3 Bilingual Constraints", "text": "In addition to the autocoding and classification signals discussed in the previous subsections, we can also use specificity in the data for training compositions. For example, when training a model on data that we know collects the same information from two inputs, we can use this knowledge to force the model to assign similar representations to both inputs, which is of particular interest when it comes to inputs that are different at the surface level but equivalent at a higher (i.e. composed) level. In computer vision, multiple photos of the same object would be useful, in compositional semantics, two sentences with the same underlying meaning.In semantic representations, paraphrases and multilingual corpora are an obvious source of such data. In particular multilingual-aligned data have beautiful theoretical properties that could allow models to learn representations that are further removed from monolingual surface realizations. As this is a separate part of the multilingual chapter presented in this new work, we refer to this chapter 6."}, {"heading": "4.4.4 Signal Combination", "text": "A nice feature of the space of recursive functions that we are examining here is that the various learning signals proposed in this chapter are easily combinable. For example, in the following chapter we will combine recursive autoencoders with a classification signal. Due to the type of gradient learning used in the formation of such models, additional signals can be added at costs that are linear in the number of training instances. 66"}, {"heading": "4.5 Learning", "text": "Here we discuss strategies for efficient learning of word embedding and model parameters for compositional vector space models. First, we investigate how gradients can be calculated in recursive and dynamic structures. Second, we give an overview of gradient descent algorithms used to update weights."}, {"heading": "4.5.1 Signal Propagation in Recursive Structures", "text": "In neural networks and other deep, recursive or multi-layered setups, back propagation can be used to efficiently calculate these gradients (Goller and Ku \ufffd chler, 1996).The back propagation works by calculating the partial derivatives relative to all temporary and internal nodes in a network and applying the chain rule to efficiently calculate partial derivatives of nodes lower in the network (closer to the output / error signal).A simple example is that one assumes a two-tiered neural network with input i, intermediate e and output o all in Rn: z = W ai + ba (4.20) e = \u03c3 (z) k = W be + bbo = Diversity (k) E = Diversity 2, with W a Rm \u00d7 n and W Rm \u00d7 n encoding all in Rn and the intermediative values relative to the intermediative values (z) e + W be = bbo = the intermediative values in RMB and iMB."}, {"heading": "4.5.1.1 Backpropagation Through Structure", "text": "For deep recursive networks, back propagation by structure (BPTS) is an efficient mechanism for learning gradients (Goller and K\u00fcchler, 1996). BPTS is essentially the extension of simple back propagation to general structured models. In contrast, for example, in \u00a7 4.5.1, the node x receives several signals from the structure, since the objective function will include terms covering both reconstruction layers as well as 68 propagated signals from higher levels of the model. We know that a minor modification of the previous example (Equation 4.20) can serve to demonize this. Suppose that an equivalent network with a new error functionE =.o \u2212 i = successor of xThis enables us to efficiently calculate all partial derivatives related to E.A (Equation 4.20)."}, {"heading": "4.5.2 Gradient Update Functions", "text": "Using the reverse propagation strategy described above, we can show a model during training by a vector of its parameters \u03b8t and an equal vector of gradients \u0442\u043dt = \u0441J (\u03b8t) at a time when there is no objective function J. To minimize J, we have a number of strategies for updating weights."}, {"heading": "4.5.2.1 Standard Gradient Descent", "text": "A standard (or stack) gradient descend method updates weights using a uniformly weighted gradient subtraction test. Therefore, all model parameters are iteratively updated due to their gradient in relation to this error function, \u03b1 being the step size or learning rate (these two terms can be used interchangeably).For convex problems, convergence of the stack gradient is guaranteed as long as the learning rate \u03b1 is appropriately selected.For large amounts of training data, it can be prohibitively expensive and inconvenient to perform a full stack downward correction.An alternative is the use of a stack descend or stochastic stack downward correction.Here, gradients are calculated for a random subset (in stack downward correction a substance of size one) of the training data. Suppose a mini downward correction of the stack size and a corpus of this stack downward correction function is more cost-specific examples of each stack downward correction."}, {"heading": "4.5.2.2 L-BFGS", "text": "The Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS, henceforth), and in particular the limited-memory version of this algorithm (L-BFGS), offers an alternative to the standard gradient descend mechanism. BFGS is a quasi-Newtonian method that estimates the Hessian matrix Bt (the matrix of the second derivatives) of J and uses it to determine the search direction pt of its gradient update step. BFGS uses this search direction in combination with a line search method to perform its gradient update while simultaneously estimating its Hessian. While BFGS has been proven to find a global optimum for convex optimization problems, it also works well for non-convex problems, especially when used in conjunction with 70Wolfe-like or Armijo-like line searches, BFGS is globally congendered for these optimization problems, not related to the convex problems."}, {"heading": "4.5.2.3 Adaptive Gradient Descent", "text": "Adaptive (sub) gradient lineage (AdaGrad, henceforth) is another popular gradient lineage algorithm (Duchi et al., 2011) that adjusts the learning rate on a trait basis. A general class of restricted optimization problems can be described with the following function: \u03b8t + 1 = diag (G\u03b8) 1 / 2\u03b8 (\u03b8t \u2212 \u03b7diag (Gt) \u2212 1 / 2 \u03c9J (\u03b8t)))), (4,28) where Gt = \u2211 t\u0442 = 1 \u03b8 J (\u03b8\u0442), \u0394\u043e J (\u0445\u0442) > is the outer product of the subgradients, diag (x) extracts the diagonal of a matrix x x, and \u043fAX (y) = argminx \u0432X (x \u2212 y) \u00b7 A (x \u2212 y) stands for the projection of a point y on X to A. \u03b7 stands for a fixed gradient lineage function."}, {"heading": "4.6 Applications", "text": "The step from word representation to the capture of the semantics of sentences and other compound linguistic units has produced models that are applied to a variety of tasks related to NLP. At this point, we offer a brief (and by no means exhaustive) overview of some of these applications. A number of publications have focused on the recognition of paraphrases (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012 et al.), mood analysis (Socher et al., 2011b; Socher et al., 2012b; Socher et al., 2013), and semantic relationship classification (ibid.). Other tasks include relational similarities (Turney, 2012) and discourse analysis (Kalchbrenner et al., 2013). With regard to the types of composition models discussed in this chapter, most efforts to date approach the problem of modelling of phravesector meanings by means of linear alsenses."}, {"heading": "4.7 Summary", "text": "In this chapter we have measured the field of compositional semantics in the context of distributed representations. After having shown in chapter 3 that distributed representations are useful for solving semantically demanding tasks, we have subsequently examined and rejected compositional semantics from the perspective of distributed representations at higher levels. To this end, we first proposed the distinction between distributed and distributed representations and then developed a model for categorizing existing and new methods of semantic composition."}, {"heading": "5.1 Introduction", "text": "In fact, most people who have chosen to take such a step are in a position to do so."}, {"heading": "5.2 Formal Accounts of Semantic Composition", "text": "There are a number of formal approaches to language that offer mechanisms of compositionality. Generative grammars (Jackendoff, 1972) treat semantics, and thus compositionality, essentially as an extension of syntax, whereby the generative (syntactic) process produces a semantically interpretable structure. Montague grammar, on the other hand, achieves a greater separation between semantics and syntactics by expressing meaning by means of Lambda calculus. However, this greater separation between surface form and meaning comes at a cost in the form of reduced accountability. While this lies outside the framework of this thesis, see e.g. Kracht (2008) for a detailed analysis of compositionality in these formalisms. 76"}, {"heading": "5.2.1 Combinatory Categorial Grammar", "text": "In this chapter, we focus on CCG, a linguistically expressive but computationally efficient grammar formalism. It uses a constituency structure with complex syntactic types (categories) from which sentences with a small number of combinators can be derived. CCG relies on combinatorial logic (as opposed to lambda calculus) to form its expressions. For a detailed introduction and analysis over other grammar formalisms, see e.g. Steedman and Baldridge (2011). CCG has been described as a transparent surface between syntactics and semantics. It is this property that makes it attractive for our purposes to provide a conditioning structure for semantic operators. A second advantage of formalism is that it is designed with computational efficiency in mind. While one can debate the relative merits of various linguistic formalisms, the existence of mature tools and resources, such as the CCG, the 2007 Hockenmaier, and Steedmann."}, {"heading": "5.3 Model", "text": "This year it is more than ever before."}, {"heading": "5.4 Learning", "text": "In the following we describe how these models can be extended to enable semi-supervised training and evaluation. Let \u03b8 = (W, B, L) be our model parameters and \u03bb be a vector with regulation parameters for all model parameters. W represents the set of all weight matrices, B the set of all distortions and L the set of all word vectors. Let N the set of training data consisting of tree nodes tn with inputs xn, yn and reconstruction rn. The error with tn given \u03b8 is: E (tn | \u03b8) = 12..."}, {"heading": "5.4.1 Supervised Learning", "text": "The previously described unsupervised method learns a vector representation for each sentence. To extract the mood from our models, we extend it with a supervised classifier by using the learned representations v as input for a binary classification model: pred (l = 1 | v, \u03b8) = sigmoid (Wlabel v + blabel) (5,8). Given our corpus of CCG parses with label pairs (N, l), the new objective function will be: J = 1N (N, l) E (N, l) + 1 (5,9) Suppose each node n \u00b2 N contains children xn, yn, coding en and reconstruction rn, so that n = {x, y, e, r \u00b2 this is split into: E (N, l) + 2 (5,9)."}, {"heading": "5.5 Experiments", "text": "In the following, we will perform a small qualitative analysis of the model to better understand whether the combination of CCG parse structures and RAE can learn semantically meaningful embeddings. In our experiments, we use the hyperbolic tangent as non-linearity. Unless otherwise stated, we use word vectors of size 50, initialized using embeddings provided by Turian et al. (2010), based on the model by Collobert and Weston (2008). 1We use the C & C parser (Clark and Curran, 2007) to analyze CCG trees for the data used in our experiments. For the CCAE-C and CCAE-D models, we use the 25 most common CCG categories (as extracted from the British National Corpus) with an additional general weight matrix to capture all remaining species (appendix)."}, {"heading": "5.5.1 Sentiment Analysis", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.5.2 Qualitative Analysis", "text": "To gain a better understanding of our models, we also perform a small qualitative analysis. Using one of the models trained at the MPQA corpus, we create word-level representations of all the phrases in this corpus and then identify the most closely related expressions by the cosinal distance measure. We perform this experiment on all expressions of length 5, considering all expressions with a word length between 3 and 7 as potential matches.85 As can be seen in Table 5.6, this works with varying success. Linking expressions such as the message of peace and the preservation (ing) of peace and security suggests that the model learns some form of semantics. On the other hand, the link between their satisfaction and support for expressing their admiration and surprise suggests that the content of the pure word level still has an impact on the model analysis. Likewise, the expressions are a success story and a faithful advocate of a certain lexical but little semantic overlap."}, {"heading": "5.6 Discussion", "text": "This year, it has come to the point where we are going to be able to be where we are, \"he said.\" We need to be able to be, \"he said.\" We need to be able to be able to be, \"he said.\" We need to be able to be able to be able to be, \"he said.\" We need to be able to be able to hide, \"he said.\" We need to be able to be in the position we are able to be in, \"he said."}, {"heading": "5.7 Summary", "text": "This year, we will be able to move into a new world, where we will be able to change the world, where we will be able to change the world, and where we will be able to change the world, \"he said."}, {"heading": "6.1 Introduction", "text": "As we have previously stated in this thesis, distributed representations are a highly suitable mechanism for encoding semantics, both at the word level and at higher levels, as shown in the sensitivity analysis task in Chapter 5. However, as shown in both the previous chapter and in the analysis of compositional semantics, it is difficult to distinguish between semantic and task-specific representations when considering a task such as sentimental analysis. To remove such task-specific distortions from the learning process of representation, a different approach is required. Within a monolingual context, the distribution hypothesis (Firth, 1957) forms the basis of most approaches to learning word representations. This hypothesis is attractive because it offers an approach to learning distributed representations independently of a given task or signal, and thus the promise to learn task-independent semantic representations."}, {"heading": "6.2 Overview", "text": "Distributed representation learning describes the task of learning continuous representations for discrete objects. Here, we focus on learning secondary representations and examine how the use of multilingual data can improve the learning of such representations at word and higher levels. We present a model that learns to represent each word in a lexicon by a continuous vector in Rd. Such distributed representations allow a model to divide the meaning between similar words and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, among others).We describe a multilingual objective function that uses a noise-contrasting actualization between semantic representations of different languages to learn these word embeddings."}, {"heading": "6.3 Approach", "text": "Most previous work on learning compositional semantic representations uses parse trees on their training data to structure their compositional functions (Socher et al., 2012a; Hermann and Blunsom, 2013, et al.) Furthermore, these approaches typically depend on specific semantic signals such as sentiment or topic margins for their objective functions. While these methods work in some cases, the need for parse trees and annotated data limits such approaches for resource-favored languages. Our novel method of learning compositional vectors eliminates these requirements and as such can be more easily applied to languages with limited resources. Specifically, we are trying to learn semantics from multilingual data. The idea is that with enough parallel data, a common representation of two parallel sentences would be forced to capture the common elements between these two sentences. Which parallel sentences are divided, of course, is their semantics."}, {"heading": "6.3.1 Two Composition Models", "text": "The objective function in Equation 6.2 could be coupled with any two predetermined vector composition functions f, g from literature. As we strive to apply our approach to a wide range of languages, we focus on composition functions that do not require syntactical information. We evaluate the two following composition functions. The first model, ADD, represents a sentence by the sum of its word vectors. This is a distributed bag-of-words approach as a sentence order that is not taken into account by the model.94 Second, the BI model is designed to capture bigram information using a nonlinear bigrampaar in its composition function: f (x) = n \u00b2 i = 1 tanh (xi \u2212 1 + xi) (6.3) The use of a nonlinearity allows the model to learn interesting interactions between words in a document that the bag-of-words approach of the ADD cannot learn."}, {"heading": "6.3.2 Document-level Semantics", "text": "While most approaches to compositional distributed semantics end at the sentence level, our model naturally extends to document-level learning by applying the composition and objective function (Equation 6.2) recursively to compose sentences in documents. This is achieved by first computing semantic representations for 95 sentences in a document, and then using these representations as inputs in a parent CVM to compute a semantic representation of a document (Figure 6.2). This recursive approach integrates document-level representations into the learning process. We can therefore use parallel document companies - whether they are sentence alignment or not - to propagate a semantic signal back to the individual words (Figure 6.2). If a sentence alignment exists, the document signal can simply be combined with the sentence signal, as we have done with the experiments described in Section 6.5.3."}, {"heading": "6.4 Corpora", "text": "We use two corpus to learn semantic representations and perform the experiments described in the following paragraph.The Europarl corpus v71 (Koehn, 2005) was used in the initial development and testing of our approach, as well as to learn the presentations used for the task of Cross-Lingual Document Classification described in \u00a7 6.5.2. We included the English-German and English-French language pairs from this corpus, from which the last 100,000 sentences of each pair were reserved for development. Second, we developed a massively multilingual corpus based on the TED corpus 2 for IWSLT 2013 (Cettolo et al., 2012).This corpus contains English transcriptions and multilingual, sentence-aligned translations of lectures from the TED conference. While the corpus focuses on machine translation tasks, we use the keywords associated with each conversation to build a subsidiary corpus for multilingual classification."}, {"heading": "6.5 Experiments", "text": "We report on the results of two experiments: First, we replicate the task of linguistic document classification by Klementiev et al. (2012), learn distributed representations of the Europarl corpus, and evaluate documents from the Reuters RCV1 / RCV2 corpus. Subsequently, we design a multilingual classification task using the TED corpus, both for training and for evaluation. Using a wider language spectrum in the second experiments allows us to better evaluate the capabilities of our models in learning a common multilingual semantic representation. Furthermore, we examine the learned embedding from a qualitative perspective in \u00a7 6.5.4."}, {"heading": "6.5.1 Learning", "text": "All model weights were randomly initialized using a Gaussian distribution (\u00b5 = 0, \u03c32 = 0.1). We used the available development data to determine our model parameters. For each positive sample, we used a number of random sound samples taken from the corpus in each training epoch. All of our embeddings have the dimensionality d = 128, with the margins removed from English to Arabic, German, French, Spanish, Italian, Dutch, Polish, Brazilian, Portuguese, Romanian, Russian, and Turkish. Chinese, Farsi, and Slovenian were removed due to the small size of these datasets. 5http: / / cdec-decoder.org / 97set to m = d.6 We also use the L2 regulation with \u03bb = 1 and step size in {0.01, 0.05}. We use 100 iterations for the RCV task, 500 for the TED single and 97set to m = d.6 We also use the L2 regulation with \u03bb = 1 and step size in {0.05, 0.05}. We use 100 iterations for the RCV task, 500 for the common grades for the TED single and 5 for the modules (we use the dual grades, the 10)."}, {"heading": "6.5.2 RCV1/RCV2 Document Classification", "text": "We evaluate our models on the basis of the cross-border document classification (CLDC, henceforth), first described in Klementiev et al. (2012). This task involves learning language-independent embedding, which is then used to classify documents within the English language pair. CLDC uses a special type of monitoring, namely the use of monitored training data in one language and the evaluation without further monitoring in another. CLDC can thus be used to determine whether our learned representations in multiple languages are semantically useful. We follow this in Klementiev et al. (2012), except that we learn our embedding using only the Europarl data and which are used in the classification task and tests. Each document in the classification task is represented by the average of d-dimensional representations."}, {"heading": "6.5.3 TED Corpus Experiments", "text": "In fact, most of us are able to outdo ourselves by putting ourselves at the center of attention."}, {"heading": "6.5.4 Linguistic Analysis", "text": "While the classification experiments focused on determining the semantic content of the sentence-level representations, we will also briefly examine the induced word embeddings using the BI + model developed at the Europarl corpus. Figure 6.4 shows the t-SNE projections for a number of English, French and German words. Although the model did not use parallel French-German data during the training, it still managed to learn semantic word-word similarities in these two languages.Going a step further, Figure 6.5 shows t-SNE projections for a number of short phrases in these three languages. We use the English of the President and gender-specific expressions Mr. President and Madam President, as well as gender-specific equivalents in French and German. The projection shows a number of interesting results: Firstly, the model groups the words correctly into three groups that correspond to the three En-103glish forms and their associated translations."}, {"heading": "6.6 Related Work", "text": "As shown in chapters 2 and 4, most research on distributed representation has focused on individual languages. English, with its large number of annotated resources, receives the most attention. However, there is a body of previous work on learning multilingual embeddings or using parallel data to transmit linguistic information across languages. (104) One has to distinguish between approaches such as Al-Rfou'et al. (2013), which learn embeddings in a wide variety of languages, and models such as ours, which learn common embeddings, which is a projection into a common semantic space across multiple languages.) In relation to our work, Yih et al. (2011) proposed to learn common embeddings of tf-idf vectors for comparable documents. Their architecture optimizes the cosmic similarity of documents by showing relative semantic similarities during learning."}, {"heading": "6.7 Summary", "text": "This year is the highest in the history of the country."}, {"heading": "Appendix A", "text": "This model is used after the Frame Identification step to find arguments that fill the semantic roles of a given frame. It is based on existing work in this area (Xue and Palmer, 2004; Das et al., 2010) and contains only a small number of changes, since its primary purpose is to enable us to evaluate our frame identification model in the context of a complete frame semantic parsing system. Implementation of the argument identification system and the changes explained here and later in \u00a7 3.5.4.2 are the work of my co-authors in Hermann et al. (2014) They are included in this thesis as they are essential for understanding the experiments discussed in this chapter, but should not be considered as part of the contribution of this theory.Given x, the sentence with a marked predicate, the reasoning model assumes that the predicate has been worked out by us."}, {"heading": "Appendix B", "text": "FrameNet Development DataTable B.1 contains a list of the 16 randomly selected documents from the FrameNet 1.5 corpus that we used to develop the frame semantic parsing task described in Chapter 3. The resulting development set consists of approximately 4,500 predicates. We used the same test set as in Das et al. (2014), which included 23 documents and 4,458 predications.116"}, {"heading": "Appendix C", "text": "CCG Categories for CCAE Models"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The mathematical representation of semantics is a key issue for Natural Language Processing (NLP). A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces. Distributional approaches\u2014meaning distributed representations that exploit co-occurrence statistics of large corpora\u2014have proved popular and successful across a number of tasks. However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level. Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP. This dissertation explores methods for learning distributed semantic representations and models for composing these into representations for larger linguistic units. Our underlying hypothesis is that neural models are a suitable vehicle for learning semantically rich representations and that such representations in turn are suitable vehicles for solving important tasks in natural language processing. The contribution of this thesis is a thorough evaluation of our hypothesis, as part of which we introduce several new approaches to representation learning and compositional semantics, as well as multiple state-of-the-art models which apply distributed semantic representations to various tasks in NLP. Part I focuses on distributed representations and their application. In particular, in Chapter 3 we explore the semantic usefulness of distributed representations by evaluating their use in the task of semantic frame identification. Part II describes the transition from semantic representations for words to compositional semantics. Chapter 4 covers the relevant literature in this field. Following this, Chapter 5 investigates the role of syntax in semantic composition. For this, we discuss a series of neural network-based models and learning mechanisms, and demonstrate how syntactic information can be incorporated into semantic composition. This study allows us to establish the effectiveness of syntactic information as a guiding parameter for semantic composition, and answer questions about the link between syntax and semantics. Following these discoveries regarding the role of syntax, Chapter 6 investigates whether it is possible to further reduce the impact of monolingual surface forms and syntax when attempting to capture semantics. Asking how machines can best approximate human signals of semantics, we propose multilingual information as one method for grounding semantics, and develop an extension to the distributional hypothesis for multilingual representations. Finally, Part III summarizes our findings and discusses future work.", "creator": "LaTeX with hyperref package"}}}