{"id": "1411.3302", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2014", "title": "Using Gaussian Measures for Efficient Constraint Based Clustering", "abstract": "In this paper we present a novel iterative multiphase clustering technique for efficiently clustering high dimensional data points. For this purpose we implement clustering feature (CF) tree on a real data set and a Gaussian density distribution constraint on the resultant CF tree. The post processing by the application of Gaussian density distribution function on the micro-clusters leads to refinement of the previously formed clusters thus improving their quality. This algorithm also succeeds in overcoming the inherent drawbacks of conventional hierarchical methods of clustering like inability to undo the change made to the dendogram of the data points. Moreover, the constraint measure applied in the algorithm makes this clustering technique suitable for need driven data analysis. We provide veracity of our claim by evaluating our algorithm with other similar clustering algorithms. Introduction", "histories": [["v1", "Wed, 12 Nov 2014 20:14:48 GMT  (9185kb,D)", "http://arxiv.org/abs/1411.3302v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["chandrima sarkar", "atanu roy"], "accepted": false, "id": "1411.3302"}, "pdf": {"name": "1411.3302.pdf", "metadata": {"source": "CRF", "title": "Using Gaussian Measures for Efficient Constraint Based Clustering", "authors": ["Chandrima Sarkar", "Atanu Roy"], "emails": ["sarkar@cs.umn.edu,", "atanu@cs.umn.edu", "sarkar@cs.umn.edu"], "sections": [{"heading": null, "text": "In fact, we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is capable of finding a solution."}, {"heading": "1 Literature Review", "text": "This year, the time has come for such a process to take place, and the question is to what extent such a process will take place."}, {"heading": "2 Proposed Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preliminaries", "text": "Finding useful patterns in large databases without any prior knowledge has attracted a lot of research over time [28, 6]. One of the most frequently investigated problems is the identification of clusters. There are a lot of methods that can be used for clustering data, namely partitioning methods, grid-based methods, hierarchical methods, density-based methods, model-based methods and constraint-based models [7]. Those that interest us primarily in this research are hierarchical and constraint-based models."}, {"heading": "2.1.1 Centroid, Radius and Diameter", "text": "Considering the n d-dimensional data points, the center of the cluster is defined as the average of all points in the cluster and can be asx0 = \u2211 n i = 0 xi n (1) The radius of a cluster is defined as the average distance between the center and the element values of the cluster, while the diameter is defined as the average distance between the individual members of the clusterR = \u221a \u2211 n i = 1 (xi \u2212 x0) 2n (2) D = \u221a \u221a \u221a ni = 1 \u0445 nj = 1 (xi \u2212 xj) 2 n (n \u2212 1) (3) Although equations 1.2, 3 in [28, 7] are defined, we redefine the concepts in this essay because they form an integral part of the essay."}, {"heading": "2.1.2 CF-tree", "text": "In [28], the authors use an innovative data structure called the CF tree (cluster attribute) to summarize information about the data points. Cluster attribute is a 3-dimensional data structure [7] that contains the number of data points, their linear sum (LS) and their square sum (SS), which can also be represented as \u2211 n i = 1 xi or \u2211 n i = 1 x 2 i. CF = < n, LS, SS > (4)"}, {"heading": "2.1.3 Gaussian Distribution", "text": "In probability theory, Gaussian or normal distribution is defined as a continuous probability distribution of real random variables that tend to cluster around the center of a given class.Multivariate (k-dimensional) Gaussian distribution is a generalization of the univariate normal distribution. It is generally written with the following notation: X \u0445 Nk (\u00b5, \u03a3).X refers to the k-dimensional vector (X1, X2, \u00b7 \u00b7, Xk).\u00b5 refers to the k-dimensional mean vector and \u2211 refers to the k x k variance matrix. Thus, the normal distribution of a matrix where each row is an observation and each column an attribute can be written as follows: fX (x) = 1 (2\u03c0) k / 2."}, {"heading": "2.2 Overview", "text": "We start our algorithm with an input data set. As our algorithm is best suited for continuously evaluated variables, we use attribute selection measures to sort out the non-continuous attributes, and the resulting data set serves as input for the first phase of our algorithm. In this phase, we create a CF tree originally proposed by [28]. The creation of a hierarchical CF tree pools our data and all original data points are located in the leaf nodes of the cluster. These are also called micro clusters [7]. In the second phase of the algorithm, which is the novelty of our approach, we use the multivariate Gaussian density distribution function 5 to split the clusters. There is related research in which the authors use the number of data points [28] and the variance [?] in a cluster as measures to split a cluster based on a CF tree."}, {"heading": "2.3 Details", "text": "Theorem From [28] - For n-dimensional data points in a cluster: {Ci}, where i = 1, 2,... n, the cluster attribute (CF) vector of the cluster is defined as triple, 4.Theorem From [28] - Suppose that CF1 = < N1, LS1, SS1 >, CF2 = < N2, LS2, SS2 > are the CF vectors of two disjointed clusters, the CF vector of the cluster formed by merging the two disjointed clusters CF1 and CF2 is: CF1 + CF2 = < N1 + N2, LS1 + LS2, SS1 + SS2 >."}, {"heading": "2.3.1 Creation of CF-Tree", "text": "\u2022 The algorithm builds a dendrogram called the Clustering Feature Tree (CF tree) while scanning the dataset [28]. \u2022 Each node without a leaf contains a number of child nodes. \u2022 The number of children a node without a leaf can contain is limited by a threshold, the branching factor. \u2022 The diameter of a sub-cluster under a leaf node must not exceed a custom threshold. \u2022 The linear sum and square sum for a node is already defined in 2.1.2. \u2022 For a node without a leaf, the child node n1, n2,..., nk ~ LS = k \u2211 i = 1 ~ LSNi (6) ~ SS = k \u0445 i = 1 ~ SSNi (7)."}, {"heading": "2.3.2 Algorithm", "text": "The data instance is inserted into the next sub-cluster. If the insertion of a data instance into a sub-cluster causes the diameter of the sub-cluster to exceed the threshold, a new sub-cluster function is created and demonstrated in 1. In this figure, the red and yellow circles are called clusters. Creating a new sub-cluster can cause its parents to cross the branching factor threshold, causing a split in the parent node. Splitting of the parent node is performed by the first identification of the pair of sub-clusters separated by the largest intercluster distance."}, {"heading": "3 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data Set", "text": "To evaluate the performance of our algorithm, we conducted a series of experiments with abalone data published in 1995 [5]. Of the eight dimensions in the original database, we chose seven continuous dimensions (length, diameter, height, total weight, shuck weight, visceral weight, shell weight).The eighth dimension, sex, is a nominal dimension and can only assume three different values, so we excluded it from our experiments.In our data set, we have 4177 cases, each representing a single abalone within an entire colony. The primary goal of the data set is to predict the age of an abalone based on its physical measurements. Although this data set is primarily used for classification, we use it as a supervised evaluation technique for cluster validity [25]."}, {"heading": "3.2 Micro-Cluster Analysis", "text": "We present the results of our microcluster analysis in 5. For the purpose of this test we created the first CF tree with different distance measurement. We started our test with a distance measurement of 0.1 and went all the way up to 1.0 with an increase of 0.1 in each step. The primary purpose of this test is to hypothesize the trend in the number of microclusters that our algorithm will generate for a specific CF tree input. X-axis in 5 represents the distance measurement and Y-axis the number of microclusters generated. In this test we compare the number of microclusters generated by BIRCH [28] with our approach. From 5 we can conclude that if the number of microclusters generated by BIRCH is low, our algorithm generates almost twice as many microclusters as the number of microclusters generated by BIRCH."}, {"heading": "3.3 Scalability Test", "text": "For the scaling test, we use the same dataset. As the number of tuples in the dataset is not elephant-like, we scale our data by appending an instance of the dataset to the end itself. In the 6X axis, the number of datasets in our dataset stands, while the Y axis represents the corresponding runtimes. In this way, we can create datasets that are multiples of the original dataset. We present our results in Figure 6. From the results, we can conclude that our algorithm has a constant time component and a linearly ascending component. To give credence to our conclusion, we have also provided the time difference between two consecutive results. This curve follows a horizontal line, which means that the difference is constant."}, {"heading": "3.4 Supervised Measures of Cluster Validity", "text": "To test the accuracy of our model, we compared our algorithm with two related approaches: BIRCH [28] and CDC [6]. We compared our results on the basis of the metrics presented in [25], namely entropy, purity, precision and retrieval. For this analysis, we set the distance measure in BIRCH to 0.27, so that it generates 30 micro clusters. In response to BIRCH, our algorithm created 47 micro clusters. We also tuned the experiment so that BIRCH would generate almost the same number of clusters as the number of class names in the original dataset. This gave BIRCH slight but significant advantages over our approach. The CDC algorithm was also set to generate approximately the same number of clusters as the original class names. To achieve this, we set the signature measurement in CDC to 80 and a variance of 0.34. A signature measure of 80 ensures that class labels will have 179 in all CDC labels."}, {"heading": "4 Conclusion and Future Work", "text": "In this paper, we present an efficient multiphase iterative hierarchical constraint technique for clustering. Our algorithm is based on the cluster identification tree and uses Gaussian probabilistic density distribution yardsticks as a constraint for refining the micro-clusters. It efficiently clustered and ultimately achieved a better result than the traditional BIRCH algorithm for this particular dataset. Also, our algorithm provides comparable results when compared with the CDC algorithm [6]. The scalability result shows that our algorithm works more efficiently even when tested with large datasets. We tested our algorithm and BIRCH with the same dataset, and our algorithm produced more micro-clusters than BIRCH. Nevertheless, the entropy and precision of a four-phase algorithm was higher than that of BIRCH. This shows the accuracy of our algorithm and BIRCH have ample scope for future work in this area."}, {"heading": "Acknowledgment", "text": "We thank Prof. Rafal Angryk for his feedback, support and critical evaluations during the development of this project. Data and notes [1] http: / / www.cs.montana.edu / atanu.roy / csci550 / results.xlsx. [2] Muhammad Aurangzeb Ahmad, Brian Keegan, Atanu Roy, Dmitri Williams, Jaideep Sri-vastava, and Noshir S. Contractor, Guilt by association?: network based propagation approaches for gold farmer detection, Advances in Social Networks Analysis and Mining 2013, ASONAM '13, Niagara, ON, Canada - August 25 - 2013, pp. 121-126. [3] Arindam Banerjee, Scalable clustering algorithms with balancing constraints, Data Min-ing Knowledge Discovery 13, 2006. [4] Sugato Basu, A. Banjeree, ER."}], "references": [{"title": "Guilt by association?: network based propagation approaches for gold farmer detection, Advances in Social Networks Analysis and Mining 2013, ASONAM \u201913, Niagara", "author": ["Muhammad Aurangzeb Ahmad", "Brian Keegan", "Atanu Roy", "Dmitri Williams", "Jaideep Srivastava", "Noshir S. Contractor"], "venue": "ON, Canada - August", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Scalable clustering algorithms with balancing constraints, Data Mining Knowledge Discovery", "author": ["Arindam Banerjee"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Active semi-supervision for pairwise constrained clustering", "author": ["Sugato Basu", "A. Banjeree", "ER. Mooney", "Arindam Banerjee", "Raymond J. Mooney"], "venue": "In Proceedings of the 2004 SIAM International Conference on Data Mining (SDM-04,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Data Mining: Concepts and Techniques", "author": ["Jiawei Han", "Micheline Kamber", "Jian Pei"], "venue": "Second Edition (The Morgan Kaufmann Series in Data Management Systems),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Data mining applications in healthcare", "author": ["Hian Chye Koh", "Gerald Tan"], "venue": "Journal of Healthcare Information ManagementVol", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Social patterns: Community detection using behavior-generated network datasets", "author": ["Alice Leung", "William Dron", "John P. Hancock", "Matthew Aguirre", "Jon Purnell", "Jiawei Han", "Chi Wang", "Jaideep Srivastava", "Amogh Mahapatra", "Atanu Roy", "Lisa Scott"], "venue": "Proceedings of the 2nd IEEE Network Science Workshop,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Healthcare and data mining using data for clinical, customer service and financial results", "author": ["Anne Milley"], "venue": "Health Management Technology", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Geoinformatic surveillance of hotspot detection, prioritization and early warning for digital governance, DG.O", "author": ["G.P. Patil", "K. Sham Bhat", "Raj Acharya", "S.W. Joshi"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Fuzzy clustering with partial supervision", "author": ["Witold Pedrycz", "James Waletzky"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "The ones that got away: False negative estimation based approaches for gold farmer detection, 2012 International Conference on Privacy, Security, Risk and Trust, PASSAT", "author": ["Atanu Roy", "Muhammad Aurangzeb Ahmad", "Chandrima Sarkar", "Brian Keegan", "Jaideep Srivastava"], "venue": "International Confernece on Social Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Socialization and trust formation: a mutual reinforcement? an exploratory analysis in an online virtual setting, Advances in Social Networks Analysis and Mining 2013, ASONAM \u201913, Niagara", "author": ["Atanu Roy", "Zoheb Hassan Borbora", "Jaideep Srivastava"], "venue": "ON, Canada - August", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Using taxonomies to perform aggregated querying over imprecise data, ICDMW", "author": ["Atanu Roy", "Chandrima Sarkar", "Rafal A. Angryk"], "venue": "The 10th IEEE International Conference on Data Mining Workshops, Sydney, Australia,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Feature analysis for computational personality recognition using youtube personality data set", "author": ["Chandrima Sarkar", "Sumit Bhatia", "Arvind Agarwal", "Juan Li"], "venue": "Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Robust feature selection technique using rank aggregation", "author": ["Chandrima Sarkar", "Sarah Cooley", "Jaideep Srivastava"], "venue": "Applied Artificial Intelligence", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Improved feature selection for hematopoietic cell transplantation outcome prediction using rank aggregation, Computer Science and Information Systems (FedCSIS)", "author": ["Chandrima Sarkar", "Sarah A. Cooley", "Jaideep Srivastava"], "venue": "Federated Conference on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Correlation based feature selection using rank aggregation for an improved prediction of potentially preventable", "author": ["Chandrima Sarkar", "Prasanna Desikan", "Jaideep Srivastava"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Predictive overlapping co-clustering", "author": ["Chandrima Sarkar", "Jaideeep Srivastava"], "venue": "arXiv preprint arXiv:1403.1942", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Impact of density of lab data in ehr for prediction of potentially preventable events, Healthcare", "author": ["Chandrima Sarkar", "Jaideep Srivastava"], "venue": "Informatics (ICHI),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Understanding co-evolution in large multi-relational social networks, CoRR", "author": ["Ayush Singhal", "Atanu Roy", "Jaideep Srivastava"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Introduction to Data Mining, us ed ed", "author": ["Pang-Ning Tan", "Michael Steinbach", "Vipin Kumar"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "When is constrained clustering beneficial, and why", "author": ["Kiri L. Wagstaff"], "venue": "in AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "An efficient data clustering method for very large databases", "author": ["Tian Zhang", "Raghu Ramakrishnan", "Miron Livny", "Birch"], "venue": "Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "Scalable, balanced model-based clustering", "author": ["Shi Zhong", "Joydeep Ghosh"], "venue": "SIAM Data Mining,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "Data clustering with size constraints, Know.", "author": ["Shunzhi Zhu", "Dingding Wang", "Tao Li"], "venue": "Based Syst", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "information to do a variety of tasks such as finding association rules, clustering heterogeneous groups of information and build predictive models [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 223, "endOffset": 227}, {"referenceID": 17, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 229, "endOffset": 233}, {"referenceID": 14, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 235, "endOffset": 239}, {"referenceID": 13, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 240, "endOffset": 244}, {"referenceID": 12, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 268, "endOffset": 272}, {"referenceID": 18, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 273, "endOffset": 277}, {"referenceID": 0, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 279, "endOffset": 282}, {"referenceID": 10, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 284, "endOffset": 288}, {"referenceID": 5, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 290, "endOffset": 294}, {"referenceID": 9, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 296, "endOffset": 300}, {"referenceID": 11, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 302, "endOffset": 306}, {"referenceID": 16, "context": "It can also be considered as an important tool for data segmentation, selection, exploration and building models using the vast data stores to discover previously unknown patterns in various domains such as healthcare [12] [21], [23], [20] [19], social media analysis [18],[24], [2], [16], [11], [15], [17], finances and various other domains [22].", "startOffset": 343, "endOffset": 347}, {"referenceID": 3, "context": "But there are some inherent problems associated with greedy hierarchical algorithmic approaches (AGNES, DIANA) [7] like vagueness of termination criteria of the algorithms and the inability to revisit once constructed clusters for the purpose of their improvement.", "startOffset": 111, "endOffset": 114}, {"referenceID": 21, "context": "To overcome these defects, an efficient technique called the multiphase clustering technique [28] can be used.", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "It falls in a category in which we can divide the clustering methods into either data-driven or need-driven [3].", "startOffset": 108, "endOffset": 111}, {"referenceID": 21, "context": "In phase 1 first we aim at implementing a data structure similar to the CF-tree [28].", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "It has been illustrated by several researchers that constraints can improve the results of a variety of clustering algorithms [30].", "startOffset": 126, "endOffset": 130}, {"referenceID": 20, "context": "In this respect, [27] has shown that inconsistency and incoherence are two important properties of constraint based measures.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "In real world applications such as image coding clustering, spatial clustering, geoinformatics [13], and document clustering [10], some background information is usually obtained dealing with the data objects\u2019 relationships or the approximate size of each group before conducting clustering.", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "This information is very helpful in clustering the data [30].", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "Previous research has looked at using instance-level background information, such as must-link and cannot-link constraints [4, 26].", "startOffset": 123, "endOffset": 130}, {"referenceID": 2, "context": "[4] also considered must-link and cannot-link constrains to learn an underlying metric between points while clustering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "There are also works on different types of knowledge hints used in fuzzy clustering, including partial supervision where some data points have been labeled [14] and domain knowledge represented in the form of a collection of viewpoints (e.", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": ", externally introduced prototypes/ representatives by users) [29].", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "recent past is balancing constraints in which clusters are of approximately the same size or importance [3, 8].", "startOffset": 104, "endOffset": 110}, {"referenceID": 21, "context": "Constraint driven clustering has been also investigated by [6], where the author proposes an algorithm based on [28] with respect to two following constraints \u2013 minimum significance constraints and minimum variance constraints.", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "[28] and a constraint based Gaussian density distribution function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Finding useful patterns in large databases without any prior knowledge has attracted a lot of research over time [28, 6].", "startOffset": 113, "endOffset": 120}, {"referenceID": 3, "context": "There are a lot of methods which can be used for data clustering namely partitioning methods, grid based methods, hierarchical methods, density based methods, model based methods, and constraint based models [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 21, "context": "R = \u221a\u2211n i=1(xi \u2212 x0) n (2) D = \u221a\u221a\u221a\u221a\u2211ni=1 \u2211nj=1(xi \u2212 xj) n(n\u2212 1) (3) Although equations 1,2 ,3 are defined in [28, 7], but we redefine the concepts in this paper since they form an integral part of the paper.", "startOffset": 109, "endOffset": 116}, {"referenceID": 3, "context": "R = \u221a\u2211n i=1(xi \u2212 x0) n (2) D = \u221a\u221a\u221a\u221a\u2211ni=1 \u2211nj=1(xi \u2212 xj) n(n\u2212 1) (3) Although equations 1,2 ,3 are defined in [28, 7], but we redefine the concepts in this paper since they form an integral part of the paper.", "startOffset": 109, "endOffset": 116}, {"referenceID": 21, "context": "In [28], the authors use an innovative data structure called the CF-tree (clustering feature) to summarize information about the data points.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "Clustering feature is a 3-dimensional data structure [7] containing the number of data points, their linear sum (LS) and their squared sum (SS) which can also be represented as \u2211n i=1 xi and \u2211n i=1 x 2 i respectively.", "startOffset": 53, "endOffset": 56}, {"referenceID": 21, "context": "In this phase we create a CF-tree originally proposed by [28].", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "These are also referred to as the micro-clusters [7].", "startOffset": 49, "endOffset": 52}, {"referenceID": 21, "context": "There are related research where the authors use the number of data points [28] and variance [?] in a cluster as measures to split a CF-Tree based cluster.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "3 Details theorem From [28] - Given n d-dimensional data points in a cluster: {Ci} where i = 1, 2, .", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "theorem From [28] \u2013 Assume that CF1 = \u3008N1, LS1,SS1\u3009, CF2 = \u3008N2, LS2,SS2\u3009 are the CF vectors of two disjoint clusters.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "1 Creation of CF-Tree \u2022 The algorithm builds a dendrogram called clustering feature tree (CF-Tree) while scanning the data set [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "Although this data set is primarily used in classification, we use it as supervised evaluation technique for cluster validity [25].", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "In this test we compare the number of micro-clusters generated by BIRCH [28] with our approach.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "To test the accuracy of our model we have compared our algorithm with two related approaches: BIRCH [28] and CDC [6].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "We have compared our results based on the measures presented in [25] namely entropy, purity, precision and recall.", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "[2] Muhammad Aurangzeb Ahmad, Brian Keegan, Atanu Roy, Dmitri Williams, Jaideep Srivastava, and Noshir S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Arindam Banerjee, Scalable clustering algorithms with balancing constraints, Data Mining Knowledge Discovery 13, 2006.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Sugato Basu, A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[7] Jiawei Han, Micheline Kamber, and Jian Pei, Data Mining: Concepts and Techniques, Second Edition (The Morgan Kaufmann Series in Data Management Systems), 2 ed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[9] Hian Chye Koh, Gerald Tan, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[11] Alice Leung, William Dron, John P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[12] Anne Milley, Healthcare and data mining using data for clinical, customer service and financial results, Health Management Technology 21 (2000), no.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[13] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[14] Witold Pedrycz and James Waletzky, Fuzzy clustering with partial supervision, IEEE Transactions on Systems, Man, and Cybernetics, Part B 27 (1997), no.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[15] Atanu Roy, Muhammad Aurangzeb Ahmad, Chandrima Sarkar, Brian Keegan, and Jaideep Srivastava, The ones that got away: False negative estimation based approaches for gold farmer detection, 2012 International Conference on Privacy, Security, Risk and Trust, PASSAT 2012, and 2012 International Confernece on Social Computing, SocialCom 2012, Amsterdam, Netherlands, September 3-5, 2012, 2012, pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[16] Atanu Roy, Zoheb Hassan Borbora, and Jaideep Srivastava, Socialization and trust formation: a mutual reinforcement? an exploratory analysis in an online virtual setting, Advances in Social Networks Analysis and Mining 2013, ASONAM \u201913, Niagara, ON, Canada - August 25 - 29, 2013, 2013, pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[17] Atanu Roy, Chandrima Sarkar, and Rafal A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[18] Chandrima Sarkar, Sumit Bhatia, Arvind Agarwal, and Juan Li, Feature analysis for computational personality recognition using youtube personality data set, Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition, ACM, 2014, pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[19] Chandrima Sarkar, Sarah Cooley, and Jaideep Srivastava, Robust feature selection technique using rank aggregation, Applied Artificial Intelligence 28 (2014), no.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[20] Chandrima Sarkar, Sarah A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[21] Chandrima Sarkar, Prasanna Desikan, and Jaideep Srivastava, Correlation based feature selection using rank aggregation for an improved prediction of potentially preventable events, (2013).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[22] Chandrima Sarkar and Jaideeep Srivastava, Predictive overlapping co-clustering, arXiv preprint arXiv:1403.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[23] Chandrima Sarkar and Jaideep Srivastava, Impact of density of lab data in ehr for prediction of potentially preventable events, Healthcare Informatics (ICHI), 2013 IEEE International Conference on, IEEE, 2013, pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[24] Ayush Singhal, Atanu Roy, and Jaideep Srivastava, Understanding co-evolution in large multi-relational social networks, CoRR abs/1407.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[25] Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, Introduction to Data Mining, us ed ed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[27] Kiri L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[28] Tian Zhang, Raghu Ramakrishnan, and Miron Livny, Birch: An efficient data clustering method for very large databases, Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, Montreal, Quebec, Canada, June 4-6, 1996 (H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[29] Shi Zhong and Joydeep Ghosh, Scalable, balanced model-based clustering, SIAM Data Mining, 2003, pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[30] Shunzhi Zhu, Dingding Wang, and Tao Li, Data clustering with size constraints, Know.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "In this paper we present a novel iterative multiphase clustering technique for efficiently clustering high dimensional data points. For this purpose we implement clustering feature (CF) tree on a real data set and a Gaussian density distribution constraint on the resultant CF tree. The post processing by the application of Gaussian density distribution function on the micro-clusters leads to refinement of the previously formed clusters thus improving their quality. This algorithm also succeeds in overcoming the inherent drawbacks of conventional hierarchical methods of clustering like inability to undo the change made to the dendogram of the data points. Moreover, the constraint measure applied in the algorithm makes this clustering technique suitable for need driven data analysis. We provide veracity of our claim by evaluating our algorithm with other similar clustering algorithms.", "creator": "LaTeX with hyperref package"}}}