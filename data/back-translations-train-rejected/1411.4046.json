{"id": "1411.4046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2014", "title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy", "abstract": "Nowadays this is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. In this paper we present an improvement in a common method that is usually used in training of RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can more accurately compute gradient of log probability of training data. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in those papers using this dataset. The implemented method is available online at \"", "histories": [["v1", "Fri, 14 Nov 2014 16:57:48 GMT  (715kb)", "http://arxiv.org/abs/1411.4046v1", "18 pages. arXiv admin note: substantial text overlap witharXiv:1408.3264"]], "COMMENTS": "18 pages. arXiv admin note: substantial text overlap witharXiv:1408.3264", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mohammad ali keyvanrad", "mohammad mehdi homayounpour"], "accepted": false, "id": "1411.4046"}, "pdf": {"name": "1411.4046.pdf", "metadata": {"source": "CRF", "title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy", "authors": ["Mohammad Ali Keyvanrad", "Mohammad Mehdi Homayounpour"], "emails": ["homayoun}@aut.ac.ir"], "sections": [{"heading": null, "text": "The new method uses free energy as a criterion to obtain elite samples from the generative model. We argue that these samples can more accurately calculate the course of the log probability of training data. According to the results, a 0.99% error rate was achieved with the MNIST test set, which shows that the proposed method exceeds the method introduced in the first publication to introduce DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (1.6% error rate). In another test with ISOLET data set, the letter classification error dropped to 3.59%, compared to 5.59% error rate achieved in these papers. The implemented method is available online at \"http / ri.ctaut.Net / Devane."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Deep Belief Networks (DBNs) and Restricted", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2.1. Computing gradient of log probability of train data", "text": "According to Equation (5), the log () can be expressed as follows [14]: (14) = Log () = (,) (,)"}, {"heading": "2.1.1. Contrastive Divergence (CD)", "text": "Since Gibbs sampling method is slow, the algorithm Contrastive Divergence (CD) is used [13], which initializes visible units based on training data, then computes binary hidden units according to Equation (12), after which the values according to Equation (13) are recalculated. Finally, the probability of hidden unit activations is calculated and from these values of hidden units and visible units is calculated, <. The calculation steps of the CD1 method are shown in Figure 3.Although the CD1 method is not a perfect sampling method, its results are acceptable [13]. By repeating the Gibbs sampling steps, the CDk method is achieved. The k parameter is the number of repetitions of Gibbs sampling steps. This method has a higher performance and can calculate the gradient more accurately [16]."}, {"heading": "2.1.2. Persistent Contrastive Divergence (PCD)", "text": "While CDk has some drawbacks and is not exact, other methods are proposed in RBM. One of these methods is the PCD, which is very popular [17]. Unlike the CD method, which uses training data as the baseline for visible units, the PCD method uses the last chain state in the last update step. In other words, the PCD successively uses Gibbs sampling runs to estimate <. Although all model parameters are changed at each step, good samples can be obtained from the model distribution with a few Gibbs sampling steps, as the model parameters change slightly [18]. Many persistent chains can run in parallel and we will call the current state in each of these chains a new sample or a \"fantasy particle\" [9], [17]. Improving the PCD method is the novelty of this work, which is described in section 3."}, {"heading": "2.2. Deep Belief Network", "text": "After learning an RBM, the activities of its hidden units (if controlled by data) can be used as \"data\" to learn a higher-quality RBM. [19] The idea behind DBN is to allow each RBM model in the sequence a different representation of the data. The model performs a nonlinear transformation of its input vectors and produces as an output the vectors used in the sequence as input for the next model [4]."}, {"heading": "3. Free Energy in Persistent Contrastive", "text": "One of the most important challenges in RBMs is training their energy chain. As already discussed, the calculation of the course of the model is unfeasible; therefore, sampling methods for gradient estimation are used to obtain suitable samples from the model that need to be run for many times and this is impossible. Therefore, different methods were proposed as CD or PCD. In this paper, a new method for creating elite patterns as described later was proposed. In PCD method as described, many permanent chains can be run in parallel and we will refer to the current state in each of these chains as a \"fantasy.\""}, {"heading": "4. Results", "text": "The method proposed in this paper has been evaluated by applying it to the MNIST and ISOLET datasets. Furthermore, we used the DeeBNet toolbox [20] (the implemented toolbox with authors) in the experiments. Furthermore, our FEPCD method has been implemented and is available online1."}, {"heading": "4.1. MNIST dataset", "text": "The results are placed in the middle of each 28 * 28 image. Image pixels have discrete values between 0 and 255, that most of them have the values at the edge of that interval. [22] Image pixel values were normalized between 0 and 1. Data sets were split to train and test, including 60,000 and 10,000 images. In our experiments, these discrete values were mapped to the interval. [0-1] Using min-max normalization methods, the discriminatory RBMs were used. The structure of this RBM is 784-500. In other words, this RBM has 784 visible units (images have 28 pixels) and 500 binary units."}, {"heading": "4.2. ISOLET dataset", "text": "In ISOLET dataset, 150 subjects twice typed the name of each letter of the alphabet. Therefore, there are 52 training examples of each loudspeaker. The speakers are grouped into groups of 30 speakers each and are designated as isolet1, isolet2, isolet3, isolet4 and isolet5. Data appear in isolet1 + 2 + 3 + 4 data in sequential order, i.e. first the speakers of isolet1, then isolet2, and so on. The test set, isolet5, is a separate file3. Due to the lack of three examples, there are a total of 7797 examples of isolet1-isolet5 (6238 training examples and 1559 test examples). The characteristics vector has 617 characteristics including spectral coefficients, contour characteristics, distinctive characteristics, pre-distinctive characteristics, pre-distinctive characteristics and post-distinctive characteristics."}, {"heading": "5. Conclusion", "text": "In this paper, we discussed one of the main problems in learning Deep Belief Networks. Since the invention of DBN, since the gradient of the log probability of training data is insoluble, the scanning methods are used to estimate this gradient. The main part of this estimation is scanning from the model whose parameters have been tuned based on training data. In our new method, the elite samples are found in several scanning chains, with the free energy value being proportional to the probability of training data. These selected samples can be used in calculating the gradient with greater accuracy. According to the results, this method improves performance on MNIST and ISOLET data sets and outperforms other general scanning methods such as CD or PCD. For future work, we would like to use free energy criteria as a fitness function in evolutionary algorithms. In this new idea, we can achieve new improved chains through evolutionary operations. An additional improvement in the idea consists of using both EP- simultaneous CD and CD speed."}], "references": [{"title": "Discriminative deep belief networks for visual data classification", "author": ["Y. Liu", "S. Zhou", "Q. Chen"], "venue": "Pattern Recognition, vol. 44, no. 10\u201311, pp. 2287\u20132296, Oct. 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "Advances in neural information processing systems, vol. 20, pp. 873\u2013880, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": "Advances in neural information processing systems, vol. 22, pp. 1096\u20131104, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Comparing multilayer perceptron to Deep Belief Network Tandem features for robust ASR", "author": ["O. Vinyals", "S.V. Ravuri"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, 2011, pp. 4596\u20134599.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep belief networks for phone recognition", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "NIPS Workshop on Deep Learning for Speech Recognition and Related Applications, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning features from music audio with deep belief networks", "author": ["P. Hamel", "D. Eck"], "venue": "11th International Society for Music Information Retrieval Conference (ISMIR 2010), 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "Proceedings of the international conference on artificial intelligence and statistics, 2009, vol. 5, pp. 448\u2013455.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Machine Learning Group, University of Toronto, Technical report, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A tutorial on stochastic approximation algorithms for training Restricted Boltzmann Machines and Deep Belief Nets", "author": ["K. Swersky", "B. Chen", "B. Marlin", "N. de Freitas"], "venue": "Information Theory and Applications Workshop (ITA), 2010, 2010, pp. 1 \u201310.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G.E. Hinton"], "venue": "Artificial Intelligence and Statistics, 2005, vol. 2005, p. 17.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Using fast weights to improve persistent contrastive divergence", "author": ["T. Tieleman", "G. Hinton"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, New York, NY, USA, 2009, pp. 1033\u2013 1040.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Quickly Generating Representative Samples from an RBM-Derived Process", "author": ["O. Breuleux", "Y. Bengio", "P. Vincent"], "venue": "Neural Computation, vol. 23, no. 8, pp. 2058\u20132073, Apr. 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., vol. 2, no. 1, pp. 1\u2013127, Jan. 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "Proceedings of the 25th international  18  conference on Machine learning, New York, NY, USA, 2008, pp. 1064\u2013 1071.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "arXiv:1206.5538, Jun. 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in Cognitive Sciences, vol. 11, no. 10, pp. 428\u2013434, Oct. 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "A brief survey on deep belief networks and introducing a new object oriented MATLAB toolbox (DeeBNet)", "author": ["M.A. Keyvanrad", "M.M. Homayounpour"], "venue": "arXiv:1408.3264 [cs], Aug. 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "MNIST handwritten digit database", "author": ["Y. LeCun", "C. Cortes"], "venue": "AT&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Visual Object Recognition Using Generative Models of Images", "author": ["V. Nair"], "venue": "University of Toronto, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Detonation Classification from Acoustic Signature with the Restricted Boltzmann Machine", "author": ["Y. Bengio", "N. Chapados", "O. Delalleau", "H. Larochelle", "X. Saint-Mleux", "C. Hudon", "J. Louradour"], "venue": "Computational Intelligence, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Spoken Letter Recognition", "author": ["M.A. Fanty", "R.A. Cole"], "venue": "presented at the Advances in Neural Information Processing Systems, 1991, pp. 220\u2013226.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1991}, {"title": "Multi-class boosting with asymmetric binary weak-learners", "author": ["A. Fern\u00e1ndez-Baldera", "L. Baumela"], "venue": "Pattern Recognition, vol. 47, no. 5, pp. 2080\u20132090, May 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "One important tool for dealing with this problem is to use DBNs (Deep Neural Network) that can create neural networks including many hidden layers [1].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "Using unlabeled data in high level feature extraction [2] and also increasing discrimination between extracted features are the benefits of DBN for feature learning [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "Using unlabeled data in high level feature extraction [2] and also increasing discrimination between extracted features are the benefits of DBN for feature learning [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Hinton presented DBNs and used it in the task of digit recognition on MNIST data set [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "In another paper from this author [3], he used DBN as a nonlinear model for feature extraction and dimension reduction.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "Another paper proposed DDBN (Discriminative Deep Belief Network) is based on DBN as a new classifier [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "3 DBN applications are not limited to image processing and can be used in voice processing [5]\u2013[8] with significant efficiency.", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "3 DBN applications are not limited to image processing and can be used in voice processing [5]\u2013[8] with significant efficiency.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "the joints between hidden units and also between visible units are disconnected [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "The probability that the network assigns to a training image can be increased by adjusting the weights and biases to lower the energy of that image and to raise the energy of other images, especially those images that have low energies and therefore make a big contribution to the partition function [11].", "startOffset": 300, "endOffset": 304}, {"referenceID": 10, "context": "Therefore the partial derivative with respect to wij of the above objective is given by [12] :", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Since there are no direct connections between hidden units in an RBM, these hidden units are independent given visible units [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "Finally due to impossibility of this method and large run-times, Contrastive Divergence (CD) method is used [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "According to equation (5), the log P(v) can be expressed as follows [14]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Contrastive Divergence or Persistent Contrastive Divergence) is in sampling in their negative phase [15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The chain is initialized by setting the binary states of the visible units to be the same as a data vector [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 11, "context": "Since Gibbs sampling method is slow, Contrastive Divergence (CD) algorithm is used [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "Although CD1 method is not a perfect gradient computation method, but its results are acceptable [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "This method has a higher performance and can compute gradient more exactly [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "One of these methods is PCD that is very popular [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Although all model parameters are changed in each step, but can receive good samples from model distribution with a few Gibbs sampling steps because the model parameters change slightly [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 8, "context": "Many persistent chains can be run in parallel and we will refer to the current state in each of these chains as new sample or a \u201cfantasy\u201d particle [9], [17].", "startOffset": 147, "endOffset": 150}, {"referenceID": 15, "context": "Many persistent chains can be run in parallel and we will refer to the current state in each of these chains as new sample or a \u201cfantasy\u201d particle [9], [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "After an RBM has been learned, the activities of its hidden units (when they are being driven by data) can be used as the \u2018data\u2019 for learning a higher-level RBM [19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 3, "context": "The model performs a nonlinear transformation on its input vectors and produces as output, the vectors that will be used as input for the next model in the sequence [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "Pretraining helps generalization and the very limited information in the data is used only to slightly adjust the weights found by pretraining [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 9, "context": "11 The proposed criterion for selecting the best chain is the free energy of visible sample v which is defined as follows [11]:", "startOffset": 122, "endOffset": 126}, {"referenceID": 9, "context": "Therefore F(v) can be computed as follows [11]:", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "Thus the derivative of P(v) according to any parameter \u03b8 is as follows [16]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "Also we used the DeeBNet toolbox [20] (the implemented toolbox with authors) in the experiments.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "MNIST dataset includes images of handwritten digits [21] (10 classes of digits 09).", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "The image pixels have discrete values between 0 and 255 that most of them have the values at the edge of this interval [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "In our experiments, these discrete values have been mapped to interval [0-1] using min-max normalization method.", "startOffset": 71, "endOffset": 76}, {"referenceID": 9, "context": "Classification is done by computing P(y|v) in each class [11], [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "Classification is done by computing P(y|v) in each class [11], [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "Finally in the main experiment, a DBN with an structure of 784-500-500-2000, similar to the structure proposed by Hinton [4] was trained.", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "As described before, in many DBN papers, after training a DBN, it can be fine-tuned using BackPropagation (BP) method [1], [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "As described before, in many DBN papers, after training a DBN, it can be fine-tuned using BackPropagation (BP) method [1], [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "These results are better than the best results obtained in Hinton [4] (0.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "014 error rate) [3].", "startOffset": 16, "endOffset": 19}, {"referenceID": 22, "context": "The features vector has 617 features including spectral coefficients, contour features, sonorant features, pre-sonorant features, and post-sonorant features [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "Since the features have real values, the Gaussian visible units is used [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "0353 classification error rate that is better than the best results obtained in new articles like [25] with 0.", "startOffset": 98, "endOffset": 102}], "year": 2014, "abstractText": "Nowadays this is very popular to use deep architectures in machine learning. Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted Boltzmann Machines (RBM) to create a powerful generative model using training data. In this paper we present an improvement in a common method that is usually used in training of RBMs. The new method uses free energy as a criterion to obtain elite samples from generative model. We argue that these samples can more accurately compute gradient of log probability of training data. According to the results, an error rate of 0.99% was achieved on MNIST test set. This result shows that the proposed method outperforms the method presented in the first paper introducing DBN (1.25% error rate) and general classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error rate). In another test using ISOLET dataset, letter classification error dropped to 3.59% compared to 5.59% error rate achieved in those papers using this dataset. The implemented method is available online at \u201chttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html\u201d.", "creator": "Microsoft\u00ae Word 2013"}}}