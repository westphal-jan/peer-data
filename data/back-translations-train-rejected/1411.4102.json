{"id": "1411.4102", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2014", "title": "Anisotropic Agglomerative Adaptive Mean-Shift", "abstract": "Mean Shift today, is widely used for mode detection and clustering. The technique though, is challenged in practice due to assumptions of isotropicity and homoscedasticity. We present an adaptive Mean Shift methodology that allows for full anisotropic clustering, through unsupervised local bandwidth selection. The bandwidth matrices evolve naturally, adapting locally through agglomeration, and in turn guiding further agglomeration. The online methodology is practical and effecive for low-dimensional feature spaces, preserving better detail and clustering salience. Additionally, conventional Mean Shift either critically depends on a per instance choice of bandwidth, or relies on offline methods which are inflexible and/or again data instance specific. The presented approach, due to its adaptive design, also alleviates this issue - with a default form performing generally well. The methodology though, allows for effective tuning of results.", "histories": [["v1", "Sat, 15 Nov 2014 02:05:22 GMT  (4697kb,D)", "http://arxiv.org/abs/1411.4102v1", "British Machine Vision Conference, 2014"]], "COMMENTS": "British Machine Vision Conference, 2014", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rahul sawhney", "henrik i christensen", "gary r bradski"], "accepted": false, "id": "1411.4102"}, "pdf": {"name": "1411.4102.pdf", "metadata": {"source": "CRF", "title": "Anisotropic Agglomerative Adaptive Mean-Shift", "authors": ["Rahul Sawhney", "Henrik I. Christensen", "Gary R. Bradski"], "emails": ["rahul.sawhney@gatech.edu", "hic@gatech.edu", "gbradski@magicleap.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world, in which they can move to another world."}, {"heading": "1.1 Motivation and Background", "text": "We use the exposure style of [5]. Let {xi} ni = 1 \"Rd, be a set of d-dimensional data points with their sample point kernel density estimation (KDE) being p (x) = 1\" p \"(x) p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" s \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" s \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" p \"p\" s \"p\" p \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" s \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"p\" s \"p\" p \"p\" p \"s\" p \"p\" p \"p\" s \"p\" p \"p\" p \"p\" s \"p\" p \"p\" s \"p\" p \"p\" p \"p\" p \"s\" p \"p\" p \"p\" p \"s\" p \"p\" p \"s\" p \"p\" p \"p\" s \"p\" s \"p\" p \"p\" p \""}, {"heading": "2 Methodology", "text": "A data point that would indicate its cardinality is the first index value that indicates its unique identification as before, and the second index that indicates its current, exclusive membership in a cluster. (That is, clusters are merged only if they tend to be in the same mode - that is, all members of a cluster that will eventually converge to a common local mode, say, to share a common local bandwidth.) This bandwidth develops each iteration when the cluster points are set, Tu, gets additional elements that survive in iteration."}, {"heading": "2.1 Update Equations", "text": "Considering pi = 1 / n and the limitation of sums to the neighbourhood points, is Nex (ustr.), the fixed point iteration, Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq. (ustr.), Eq (ustr.), q (ustr. q q., q q ustr. (ustr. q q., q q ustr., q ustr ustr., q ustr ustr (q q q q q, q q ustr), q ustr ustr (q q q q q q q q, q ustr, q ustr ustr, q ustr ustr, q q ustr, q q q q q ustr, q ustr ustr, q q q ustr q q q q q q q q q ustr, q q ustr, q q q q ustr q q q q ustr, q q q q q q q ustr q q q q q q ustr, q q q q q ustr q q q q q q q q ustr q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q"}, {"heading": "2.2 Bandwidth Estimation", "text": "The underlying local distribution is actually a localized subset of the common nonparametric density represented by the entire dataset - it also has significant contributions from adjacent structures. The local structure could also be asymmetric and / or without tail (s). One solution lies in considering points resulting from mean shifts over the mode in which the cluster converges - the cluster trajectivity requirement, Tu. We use the variance of Tu in relation to the underlying density as an estimate. Since Tu builds up each iteration, this also applies to mean shifts over the mode in which the cluster converges - the cluster trajectivity requirement, Tu. We use the variance of Tu in relation to the underlying density as an estimate."}, {"heading": "2.3 Cluster Merging", "text": "For all given data points, if their middle displacement trajectories overlap, they will fit into a common local mode. Thus, near the trajectory of a data point (which moves up in a certain mode), all data points whose displacement vectors overlap with that trajectory could be bundled together in sufficient proximity. Eventually, they will approach in the same local mode. Therefore, we generally consider the data points near a cluster trajectory to be verifiable, then by transitivity - all members of their parent cluster, with an epsilon \u03b5 delimiting the environment. If a data point, y, is detected nearby (in MergeCheck) to move into the same mode as verifiable, then by transitivity - all members of its parent cluster (y) also move into this mode - the clusters u and vice versa (y) can merge together."}, {"heading": "2.4 Post Processing", "text": "Once data has been partitioned, a post-processing step merges clusters with proximate modes, ensuring a minimum cluster size (in conventional mean shift, clusters are delimited only during the post-processing process). In addition, for structured data, cluster contiguity could be enforced within a relatively large distance threshold. We use graph operations. For structured data as in images, adjacence connections between clusters can be added naturally using a spatial grid structure to ensure that a connected graph is created. Bhattacharya diver-5Wearing on the surface + 1 is mu. Wearing on the surface, given by y, is the mean shift vector resulting from the first iteration over the trivial cluster containing y; it is stored for consistent use. Algorithm 2: Post Processing \u2022 (for structured data only)."}, {"heading": "3 Results", "text": "The number of clusters decreases rapidly in all early iterationen.Due to agglomeration, the number of clusters decreases monotonically. Only a fraction of clusters remains in all early iterationen.Due to agglomeration.Due to agglomeration, the number of clusters decreases monotonically."}, {"heading": "10 SAWHNEY, CHRISTENSEN, BRADSKI: ANISOTROPIC ADAPTIVE MEAN SHIFT", "text": "This year, it's gotten to the point where it's kind of a tantrum, \"he said.\" We've never experienced anything like it, \"he said.\" But we're not there yet, \"he said.\" We're not as far as we imagined it to be, \"he said.\" We've never waited so long to know, \"he said.\" We've never waited so long. \""}, {"heading": "4 Conclusion", "text": "A generalized methodology for sharing feature pace and searching for modes was presented - leveraging synergies of adaptive anisotropic mean shift and controlled agglomeration. Unmonitored adjustment of full anisotropic bandwidth is useful, as well as enabling Mean Shift Clustering. We are excited by the prospect of spot clouds and video streams. Our experiments showed that sparse data is a problem, which is understandable as it impedes cluster growth and bandwidth development, with AAMS then behaving like a conventional Mean Shift."}], "references": [{"title": "On a measure of divergence between two multinomial populations", "author": ["Anil Bhattacharyya"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1946}, {"title": "Real time face and object tracking as a component of a perceptual user interface", "author": ["Gary R Bradski"], "venue": "In Applications of Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Carreira-Perpi\u00f1\u00e1n. Fast nonparametric clustering with gaussian blurring mean-shift", "author": ["\u00c1. Miguel"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Gaussian Mean-Shift is an EM algorithm", "author": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting", "author": ["Jos\u00e9 E Chac\u00f3n", "Tarn Duong"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Mean shift, mode seeking, and clustering", "author": ["Yizong Cheng"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Synergism in low level vision", "author": ["Christopher M Christoudias", "Bogdan Georgescu", "Peter Meer"], "venue": "In Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Kernel-based object tracking", "author": ["D. Comaniciu", "V. Ramesh", "P. Meer"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "An algorithm for data-driven bandwidth selection", "author": ["Dorin Comaniciu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Mean shift: A robust approach toward feature space analysis", "author": ["Dorin Comaniciu", "Peter Meer"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "The variable bandwidth mean shift and data-driven scale selection", "author": ["Dorin Comaniciu", "Visvanathan Ramesh", "Peter Meer"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Feature significance for multivariate kernel density estimation", "author": ["Tarn Duong", "Arianna Cowling", "Inge Koch", "MP Wand"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Information theoretic feature selection and projection. In Speech, Audio, Image and Biomedical Signal Processing using Neural Networks, pages 1\u201322", "author": ["Deniz Erdogmus", "Umut Ozertem", "Tian Lan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "The estimation of the gradient of a density function, with applications in pattern recognition", "author": ["Keinosuke Fukunaga", "Larry Hostetler"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1975}, {"title": "Mean shift based clustering in high dimensions: A texture classification example", "author": ["Bogdan Georgescu", "Ilan Shimshoni", "Peter Meer"], "venue": "In Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Vopatov\u00e1. Full bandwidth matrix selectors for gradient kernel density estimate", "author": ["Ivana Horov\u00e1", "Jan Kol\u00e1\u010dek", "Kamila"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Adaptive mean-shift tracking with novel color model", "author": ["Mun-Ho Jeong", "Bum-Jae You", "Yonghwan Oh", "Sang-Rok Oh", "Sang-Hwi Han"], "venue": "In Mechatronics and Automation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Segmenting brain mri using adaptive mean shift", "author": ["R.J. Jimenez-Alaniz", "M. Pohi-Alfaro", "V. Medina-Bafluelos", "O. Yaflez-Suarez"], "venue": "In Engineering in Medicine and Biology Society,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Event detection in crowded videos", "author": ["Yan Ke", "Rahul Sukthankar", "Martial Hebert"], "venue": "In Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Learning full pairwise affinities for spectral segmentation", "author": ["Tae Hoon Kim", "Kyoung Mu Lee", "Sang Uk Lee"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["Pushmeet Kohli", "Philip HS Torr"], "venue": "International Journal of Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Scale-invariant object categorization using a scaleadaptive mean-shift search", "author": ["Bastian Leibe", "Bernt Schiele"], "venue": "In Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["David Martin", "Charless Fowlkes", "Doron Tal", "Jitendra Malik"], "venue": "In Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "An adaptive mean-shift framework for mri brain segmentation", "author": ["Arnaldo Mayer", "Hayit Greenspan"], "venue": "Medical Imaging, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Mean shift spectral clustering", "author": ["Umut Ozertem", "Deniz Erdogmus", "Robert Jenssen"], "venue": "Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A topological approach to hierarchical segmentation using mean shift", "author": ["Sylvain Paris", "Fr\u00e9do Durand"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["Jamie Shotton", "Toby Sharp", "Alex Kipman", "Andrew Fitzgibbon", "Mark Finocchio", "Andrew Blake", "Mat Cook", "Richard Moore"], "venue": "Communications of the ACM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Hierarchical evolving mean-shift", "author": ["M. Surkala", "K. Mozdren", "R. Fusek", "E. Sojka"], "venue": "In Image Processing (ICIP),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Hierarchical blurring mean-shift", "author": ["Milan \u0160urkala", "Karel Mozd\u0159e\u0148", "Radovan Fusek", "Eduard Sojka"], "venue": "In Advances Concepts for Intelligent Vision Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Toward objective evaluation of image segmentation algorithms", "author": ["Ranjith Unnikrishnan", "Caroline Pantofaru", "Martial Hebert"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Quick shift and kernel methods for mode seeking", "author": ["Andrea Vedaldi", "Stefano Soatto"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Robust scale-adaptive mean-shift for tracking", "author": ["Tomas Vojir", "Jana Noskova", "Jiri Matas"], "venue": "In Image Analysis,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Image and video segmentation by anisotropic kernel mean shift", "author": ["Jue Wang", "Bo Thiesson", "Yingqing Xu", "Michael Cohen"], "venue": "Vision-ECCV", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Multiple class segmentation using a unified framework over mean-shift patches", "author": ["Lin Yang", "Peter Meer", "David J Foran"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Agglomerative mean-shift clustering", "author": ["Xiao-Tong Yuan", "Bao-Gang Hu", "Ran He"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Accelerated convergence using dynamic mean shift", "author": ["Kai Zhang", "Jamesk T Kwok", "Ming Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "\u2018Mean Shift\u2019 ([15, 7], MS) is a powerful nonparametric technique for unsupervised pattern clustering and mode seeking.", "startOffset": 14, "endOffset": 21}, {"referenceID": 5, "context": "\u2018Mean Shift\u2019 ([15, 7], MS) is a powerful nonparametric technique for unsupervised pattern clustering and mode seeking.", "startOffset": 14, "endOffset": 21}, {"referenceID": 9, "context": "References [11, 3] established it\u2019s utility in low-level perception tasks such as feature clustering, filtering and in tracking.", "startOffset": 11, "endOffset": 18}, {"referenceID": 1, "context": "References [11, 3] established it\u2019s utility in low-level perception tasks such as feature clustering, filtering and in tracking.", "startOffset": 11, "endOffset": 18}, {"referenceID": 26, "context": "It has been in popular use since, as a very useful tool for pattern clustering of sensor data ([28, 14] for example).", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "It has been in popular use since, as a very useful tool for pattern clustering of sensor data ([28, 14] for example).", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "It has also found niche as a preprocessor (a priori segmentation, smoothing) before higher level image & video analysis tasks such as scene parsing, object recognition, detection ([22, 35, 20]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 33, "context": "It has also found niche as a preprocessor (a priori segmentation, smoothing) before higher level image & video analysis tasks such as scene parsing, object recognition, detection ([22, 35, 20]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 18, "context": "It has also found niche as a preprocessor (a priori segmentation, smoothing) before higher level image & video analysis tasks such as scene parsing, object recognition, detection ([22, 35, 20]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 20, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 24, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 19, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 29, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 28, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 9, "context": "Its popular standard form, [11], utilizes fixed, scalar bandwidth assuming homoscedasticity and isotropicity.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "The adaptive Mean Shift variants, [12, 16], ascertain variable bandwidths, but they still assume isotropicity.", "startOffset": 34, "endOffset": 42}, {"referenceID": 14, "context": "The adaptive Mean Shift variants, [12, 16], ascertain variable bandwidths, but they still assume isotropicity.", "startOffset": 34, "endOffset": 42}, {"referenceID": 4, "context": "Offline bandwidth selection methods for Mean Shift ([6, 17, 10]), typically estimate a single, global bandwidth, and/or are data specific/non-automatic.", "startOffset": 52, "endOffset": 63}, {"referenceID": 15, "context": "Offline bandwidth selection methods for Mean Shift ([6, 17, 10]), typically estimate a single, global bandwidth, and/or are data specific/non-automatic.", "startOffset": 52, "endOffset": 63}, {"referenceID": 8, "context": "Offline bandwidth selection methods for Mean Shift ([6, 17, 10]), typically estimate a single, global bandwidth, and/or are data specific/non-automatic.", "startOffset": 52, "endOffset": 63}, {"referenceID": 14, "context": "(b) Comparitive results with standard MS (left) and variable-bandwidth isotropic MS ([16], right), at similar clustering levels, 25 & 27 respectively, are shown.", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "MS with correctly chosen bandwidth detected more coherent modes than [16], but looses partition saliency (bushes, water, sky in background).", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "[16] better adapts to scales but oversegments at places, and smooths over others (face).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "We utilize the exposition style of [5].", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "K(t), t \u2265 0, is a d-variate kernel with compact support satisfying some regularity constraints, mild in practice ([11, 5] for details).", "startOffset": 114, "endOffset": 121}, {"referenceID": 3, "context": "K(t), t \u2265 0, is a d-variate kernel with compact support satisfying some regularity constraints, mild in practice ([11, 5] for details).", "startOffset": 114, "endOffset": 121}, {"referenceID": 9, "context": "In our approach, data points (pertaining to a cluster) converging to a common local mode share a common bandwidth - one which reflects this mode\u2019s structure, and to an extent, its basin of attraction ([11]).", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "We refer to it as the local bandwidth ([10] utilizes local bandwidths in a related sense).", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 28, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 35, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 25, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 34, "context": "[36] utilizes set covering based iterative agglomeration for improved efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "\u03c3i is estimated using a variation of the following two heuristics ([12, 16]) - 1) kth nearest neighbor, xk i , distance heuristic \u2192 \u03c3i \u221d \u2016xi\u2212 xk i \u2016, or 2) Abramson\u2019s heuristic \u2192 \u03c3i \u221d \u03c3o(\u03c0(xi))/2, where \u03c0(x) is the pilot density estimate obtained by first running mean shift with analysis bandwidth, \u03c3o.", "startOffset": 67, "endOffset": 75}, {"referenceID": 14, "context": "\u03c3i is estimated using a variation of the following two heuristics ([12, 16]) - 1) kth nearest neighbor, xk i , distance heuristic \u2192 \u03c3i \u221d \u2016xi\u2212 xk i \u2016, or 2) Abramson\u2019s heuristic \u2192 \u03c3i \u221d \u03c3o(\u03c0(xi))/2, where \u03c0(x) is the pilot density estimate obtained by first running mean shift with analysis bandwidth, \u03c3o.", "startOffset": 67, "endOffset": 75}, {"referenceID": 23, "context": "They have found more use in smoothing type applications as reported in [25, 19].", "startOffset": 71, "endOffset": 79}, {"referenceID": 17, "context": "They have found more use in smoothing type applications as reported in [25, 19].", "startOffset": 71, "endOffset": 79}, {"referenceID": 16, "context": "Variants have also been used in tracking scenarios, where the bandwidths are adapted in a task specific fashion (see [18, 9], for example).", "startOffset": 117, "endOffset": 124}, {"referenceID": 7, "context": "Variants have also been used in tracking scenarios, where the bandwidths are adapted in a task specific fashion (see [18, 9], for example).", "startOffset": 117, "endOffset": 124}, {"referenceID": 21, "context": "[23, 33] adapt isotropic bandwidths to object scales, to unimodally track, search for them.", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[23, 33] adapt isotropic bandwidths to object scales, to unimodally track, search for them.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 28, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 35, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 2, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 27, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 30, "context": "[32] presents improvements over the somewhat related Mediod Shift.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In offline settings, [10] presents a supervised methodology.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Only recently were automatic full bandwidth selectors for density gradient estimation proposed in [17, 6], for offline settings.", "startOffset": 98, "endOffset": 105}, {"referenceID": 4, "context": "Only recently were automatic full bandwidth selectors for density gradient estimation proposed in [17, 6], for offline settings.", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "A very useful variant is Joint Domain Mean Shift, [11], which is used to create partitions jointly respecting the dataset\u2019s multiple feature domains which are mutually independent; For example, \u3008color,space\u3009 in color based segmentation & smoothing, and \u3008color, f low\u3009 in motion segmentation.", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "As noted in [31] on color segmentation, \u03c3 r and \u03c3 s need to be selected carefully.", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": "Reference [34] utlizes an anisotropic \u03a3i for visual data segmentations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "A reasonable test of significance for \u03a3u estimates, is to check if the kernel weighted point count or Effective Sample Size (ESS, [13]) is above some value, \u03bb .", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "As a binomial rule of thumb ([13]), \u03bb = 5 is chosen as the minimum ESS, which is analogous to choosing 5 as the minimum individual expected cell counts in a \u03c72 test of independence.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Bandwidth estimates based on a cluster\u2019s member data point locations are not reliable ([10] notes this too).", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "4), kernel induced feature space metrics ([26]), information-theoretic ones like Renyi\u2019s entropy ([14]) seem viable, interesting possibilities for MergeCheck.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "4), kernel induced feature space metrics ([26]), information-theoretic ones like Renyi\u2019s entropy ([14]) seem viable, interesting possibilities for MergeCheck.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "34 Prior Art [21] FullSpectralOverMS [21] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "34 Prior Art [21] FullSpectralOverMS [21] 0.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "21 JMS [8] 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 19, "context": "41 NCut [21] - Ref.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "[27] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "19 MNCUT [21] - Ref.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "[6] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "17 GBIS [21] - Ref.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "[9] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "67 Saliency [21] - Ref.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "[8] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "24 JSEG [21] - Ref.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "[7] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Table 1: Results on BSD300 [24].", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "For perspective, we also reproduce results from [21] of unsupervised image segmentation methods.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "[21] selects segment levels per image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Note that [21], which has the next best values, operates over a priori Mean Shift segmentations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "gence ([2], dB) was used as the merging criteria.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "Improvements in efficency based on fast nearest neighbor search such as exploiting grid structure of spatial domain, locally sensitive hashing ([16]) are applicable in our methodology too.", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "1 7) are shown with joint domain Mean Shift implementation (JMS) from EDISON ([8]), over Berkely Segmentation Dataset ([24], BSD300).", "startOffset": 78, "endOffset": 81}, {"referenceID": 22, "context": "1 7) are shown with joint domain Mean Shift implementation (JMS) from EDISON ([8]), over Berkely Segmentation Dataset ([24], BSD300).", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "Indicated values are of AAAMS / MS / VariableMS ([16]) respectively, with best values in red.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "2 shows initial results, along with comparisons with single domain standard Mean Shift (MS), and [16]\u2019s isotropic variable bandwidth implementation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "[16] first determines isotropic point bandwidths using the kth nearest neighbor distance heuristic, and subsequently utlizes them in single kernel mean shift iterations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Note that [16] internally normalizes the data, while AAAMS & MS results are without any normalizations.", "startOffset": 10, "endOffset": 14}], "year": 2014, "abstractText": "Mean Shift today, is widely used for mode detection and clustering. The technique though, is challenged in practice due to assumptions of isotropicity and homoscedasticity. We present an adaptive Mean Shift methodology that allows for full anisotropic clustering, through unsupervised local bandwidth selection. The bandwidth matrices evolve naturally, adapting locally through agglomeration, and in turn guiding further agglomeration. The online methodology is practical and effecive for low-dimensional feature spaces, preserving better detail and clustering salience. Additionally, conventional Mean Shift either critically depends on a per instance choice of bandwidth, or relies on offline methods which are inflexible and/or again data instance specific. The presented approach, due to its adaptive design, also alleviates this issue with a default form performing generally well. The methodology though, allows for effective tuning of results.", "creator": "LaTeX with hyperref package"}}}