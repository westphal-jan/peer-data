{"id": "1411.4114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2014", "title": "Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading", "abstract": "In this paper, we defined the viseme (visual speech element) and described about the method of extracting visual feature vector. We defined the 10 visemes based on vowel by analyzing of Korean utterance and proposed the method of extracting the 20-dimensional visual feature vector, combination of static features and dynamic features. Lastly, we took an experiment in recognizing words based on 3-viseme HMM and evaluated the efficiency.", "histories": [["v1", "Sat, 15 Nov 2014 05:44:10 GMT  (166kb)", "http://arxiv.org/abs/1411.4114v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["ha jong won", "li gwang chol", "kim hyok chol", "li kum song"], "accepted": false, "id": "1411.4114"}, "pdf": {"name": "1411.4114.pdf", "metadata": {"source": "CRF", "title": "Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading", "authors": ["Ha Jong Won", "Li", "Gwang Chol", "Kim", "Hyok Chol", "Kum Song", "Kim Il Sung"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they are able"}, {"heading": "3.1.1 Region feature", "text": "In this paper, we have defined the 8 most important lip shapes (shown in Figure-3) and defined the other lip shapes that differ from the main lip shapes as the middle lip shape that appears and is considered as such during the progression between the main lip shapes, in which the 8 most important lip shapes are included in different definitions. Therefore, each lip shape contains information about all 8 main lip shapes in it and is determined differently according to the degrees of the main lip shapes. Basic forms of lip shapes of lip shapes are included by the 8 main lip shapes and each degree of similarity between the selected lip shapes is defined."}, {"heading": "3.1.2 Contour feature", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "4.1 Recognition performance evaluation according to utterance unit", "text": "In this thesis, we evaluate the appropriateness of the proposed visual function by recognizing an isolated word whose utterance unit the speaker has a word. First, we define trivial HMMs on the basis of individual visems defined in Section 2, and evaluate the models using HTK3.4. Tension data to estimate trivial HMMs are characteristic vector data extracted from 5, 6000 lip motion videos via standard training sets. Video is the video that the standard study sets of 7 speakers are spoken and it is the front-page video. To evaluate the performance of the isolated word, we used 300 words that are included in the training sets and 200 words that are not included in the training sets. In the case of N-best detection, the detection result is evaluated as true if 3 candidates have correct word sequences. We also took recognition in cases where visemanual was given, and the following table is as if the result is not given."}, {"heading": "4.2 Evaluation of Recognition according to speaker", "text": "In the dissertation we did experiments on different speakers for the robustness of the visual feature vector, which we proposed to intensify the motion sequences. Therefore, we made 50 lip videos of each of the 3 speakers, one person is the speaker and the others are non-training speakers. And then we did a recognition test with 150 lip videos.At that time we gave the same information for the same sentence of each speaker. The result of the speaker recognition experiment is as follows."}], "references": [{"title": "Stiefelhagen,\u201dTowards unrestricted lipreading\u201d, Interactive Systems Laboratories", "author": ["Ue Meier", "Rainer"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "M.W.Powers,\u201dAudio-Visual Speech Reconition using Red Exclusion and Neural NetWorks\u201d, School of Informatics and Engineering Flinders University of South Australia, PO Box 2100,Adelaide,South Australia", "author": ["Trent W.Lewis", "David"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Speaker Independent Audio- Visual Continuous Speech Recognition\u201d, Microcomputer Research Labs,Intel Corporation Santa Clara,CA,95052,2003", "author": ["Luhong Liang", "Xiaoxing Liu", "Yibao Zhao"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Extended Pose-Invariant Lipreading System", "author": ["Patrick Lucey", "Gerasimos Potamianos", "\u201dAn"], "venue": "IBM Thomas J.Watson Research Center, Yorktown Heights,NY", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "F.Cootes and J.Andrew Bangham,\u201dExtraction of Visual Features fr Lipreading", "author": ["Iain Matthews", "Timothy"], "venue": "IEEE Trans on Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "H, \u201cImproving Visual Features for Lipreading\u201d, School of Computing Sciences, University of East Anglia,UK", "author": ["Yuxuan Lan", "Barry-John. T", "Richard"], "venue": "School of Electronics and Physical Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A REAL-TIME AUTOMATIC LIPREADING SYSTEM", "author": ["S.L. Wang", "W.H. Lau", "S.H. Leung", "H. Yan"], "venue": "ISCAS 2004,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "SPEAKER  18  IDENTIFICATION BY LIPREADING\u201d, Dept.of Electronic and Electrical Engineering University of Sheffield,Sheffield S1 3JD,UK", "author": ["Juergen Luettin", "Neil A. Thacker", "Steve W. Beet"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Visual Features Extracting & Selecting for Lipreading", "author": ["Hong-xun Yao", "Wen Gao", "Wei Shan"], "venue": "AVBPA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Real-time Lipreading LSI for Word Recognition", "author": ["Kazuhiro Nakamura", "Noriaki Murakami", "Kazuyoshi Takagi", "Naofumi Takagi", "\u201dA"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Pitas,\u201dApplication of Support Vector Machines Classifiers to Visual Speech Recognition\u201d,IEEE", "author": ["Mihaela Gordan", "Constantine Kotropoulos", "Ioannis"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "[1] It is McGruk effect that is at the bottom of the attempt to use visual information in speech recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Reference [3] described about the definitions of 9 English visemes such as AB,LDF,IDF,LSH,ALF,LLL,RRR,PAL,WWW.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "While reference [4] described about the definitions of 14 English visemes based on the MPEG-4 Multimedia Standard rule, in reference [5] sip and sp, phonemes to represent no sound used in HTK was introduced.", "startOffset": 133, "endOffset": 136}, {"referenceID": 2, "context": "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are", "startOffset": 275, "endOffset": 278}, {"referenceID": 5, "context": "A lot of methods such as PCA based region feature extracting method [5], DCT based region feature extracting method [6], region feature extracting method using 2DPCA called BDPCA (Bidirectional PCA)[7], AAM(Active Appearance Model) based lip region feature extracting method [8] and Hi-LDA based region feature extracting method [9] are", "startOffset": 329, "endOffset": 332}, {"referenceID": 6, "context": "There are some methods of extracting lip contour features such as contour feature extracting method applying ASM to extracted lip region [10], after extracting contour of lip, applying affine robust Fourier transform to get Fourier coefficients and to use as feature vector to represent the type of contour [11], a method that gets extracted 10 geometrical parameters from outer and inner contour of lip and use them as a visual feature vector for lip-reading[4] and so on.", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "There are some methods of extracting lip contour features such as contour feature extracting method applying ASM to extracted lip region [10], after extracting contour of lip, applying affine robust Fourier transform to get Fourier coefficients and to use as feature vector to represent the type of contour [11], a method that gets extracted 10 geometrical parameters from outer and inner contour of lip and use them as a visual feature vector for lip-reading[4] and so on.", "startOffset": 307, "endOffset": 311}, {"referenceID": 8, "context": "Whereas in reference [12], it describes about the implementation of lip \u2013 reading system based on the HMMs for each viseme, in reference [13], HMMs for each words were formed to use for lip-reading of word.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "Whereas in reference [12], it describes about the implementation of lip \u2013 reading system based on the HMMs for each viseme, in reference [13], HMMs for each words were formed to use for lip-reading of word.", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Reference [14] proposed a method to apply SVM to lip-reading system.", "startOffset": 10, "endOffset": 14}], "year": 2014, "abstractText": "In this paper, we defined the viseme (visual speech element) and described about the method of extracting visual feature vector. We defined the 10 visemes based on vowel by analyzing of Korean utterance and proposed the method of extracting the 20-dimensional visual feature vector, combination of static features and dynamic features. Lastly, we took an experiment in recognizing words based on 3-viseme HMM and evaluated the efficiency. Keywordslip-reading, viseme, feature extraction, visual feature vector, HMM", "creator": "PScript5.dll Version 5.2.2"}}}