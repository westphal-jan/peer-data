{"id": "1411.4116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2014", "title": "Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning", "abstract": "This paper aims to explore the effect of prior disambiguation on neural network- based compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.", "histories": [["v1", "Sat, 15 Nov 2014 06:32:49 GMT  (13kb)", "http://arxiv.org/abs/1411.4116v1", "NIPS 2014"]], "COMMENTS": "NIPS 2014", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["jianpeng cheng", "dimitri kartsaklis", "edward grefenstette"], "accepted": false, "id": "1411.4116"}, "pdf": {"name": "1411.4116.pdf", "metadata": {"source": "CRF", "title": "Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning", "authors": ["Jianpeng Cheng"], "emails": ["jianpeng.cheng@cs.ox.ac.uk", "kartsak@cs.ox.ac.uk", "etg@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.41 16v1 [cs.CL] 1 5N ov"}, {"heading": "1 Introduction", "text": "While distributed representations of meaning began largely at the word level, the need for representations of the semantics of larger text units, from phrases to entire documents, is evident. Early attempts at compositionality in distributed representations used fixed algebraic operations such as vector additions and component-by-component multiplications [11] to obtain semantic representations of larger text units from their constituent representations. More recently, models have represented relational, net-based compositional approaches that derive a sentence vector by applying recursively neuronal networks to each word vector [15, 16, 6]. Besides these principally multilinear methods of composition, we observe the emergence of non-linear neural compositional approaches that represent a sentence vector through recursive application of neural networks to each word vector [15,] prior to composition."}, {"heading": "2 Neural networks for composing meaning", "text": "In this work, we try to grasp the meaning of a sentence by composing the vector representations of the words it contains. In the most general form of such a composition, a neural network is applied to each word pair w1 and w2: \u2212 \u2192 v = f (W [\u2212 \u2192 w1: \u2212 \u2192 w2] + b) (1), where [\u2212 w1: \u2212 \u2192 w2] denotes the concatenation of the two vectors associated with the words, W and b are model parameters, and f is a nonlinear activation function. The compositional result \u2212 \u2192 v is a vector that represents the meaning of the bigram, and can in turn be used as an input to compose the representation of a larger text component in a recursive manner. This process continues until all the vectors of the words are concealed in a sentence."}, {"heading": "3 Combining NNs with context-based word sense disambiguation", "text": "In this paper, we evaluate a new methodology in which each input word is associated with a series of vectors, each representing different meanings of the word in the training corpus. As an input into the compositional network, we select the most likely meaning vector for each word that has its context. Our general methodology reformulates the approach of [9] in a deep learning environment; this is illustrated in Figure 1. First, we use a word induction step to discover the latent senses of each target word. For each occurrence of a target word wt in the corpus, we calculate a context vector as the average of its neighbors, i.e., we use a word induction step to discover the latent senses of each target word. \u2212 wj \u2212 wj, the word wt in the context is a context vector that represents the average of its neighbors, i.e., we tap the vector as a whole."}, {"heading": "4 Experiments", "text": "To test the effect of the previous dislocation on the deeper models, we need to include the constituent words of the subject-verb-object form and verb-phrases in a series of tasks before composition. We need to assess the quality of the compositions in a sentence - a good model that is able to construct sentences that are usually ambiguous."}, {"heading": "5 Discussion", "text": "The results are promising because they suggest that disambiguity as an additional step before composition can bring at least marginal benefits to deep learning of compositional models. If we compare the numbers obtained from the three sets of data, the effect of disambiguity is most noticeable for the M & L dataset. In other words, the results suggest that disambiguity for a generic set before disambiguity can act as a useful pre-processing step that could improve the end result (if the set has ambiguous words) or not (if all words are intentionally non-ambiguous), but never reduce performance. The effect of disambiguity also appears to be quite clear for the K & S dataset, while the result for the G & S dataset, while positive, is less ambiguous."}, {"heading": "6 Conclusion", "text": "The main contribution of this paper is that it suggests that explicit engagement with the issue of disambiguity can be an effective method of improving the performance of deep learning compositional meaning models. Therefore, the benefits of our simple approach of adding a previous disambiguity step to word vectors are small. A sensible future direction would be to include an explicit disambiguity step in the architecture of the compositional model that addresses ambiguity during the training process itself. Current work shows that such an approach, which is much more consistent with the concept of deep learning, could lead to drastic improvements in the performance of a compositional model."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark"], "venue": "arXiv preprint arXiv:1003.4394,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics", "author": ["E. Grefenstette"], "venue": "PhD thesis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["K.M. Hermann", "P. Blunsom"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Resolving lexical ambiguity in tensor regression models of meaning", "author": ["D. Kartsaklis", "N. Kalchbrenner", "M. Sadrzadeh"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol. 2: Short Papers),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["D. Kartsaklis", "M. Sadrzadeh"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Separating disambiguation from composition in distributional semantics", "author": ["D. Kartsaklis", "M. Sadrzadeh", "S. Pulman"], "venue": "In Proceedings of 17th Conference on Natural Language Learning (CoNLL),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Dynamic and static prototype vectors for semantic composition", "author": ["S. Reddy", "I.P. Klapaftis", "D. McCarthy", "S. Manandhar"], "venue": "In IJCNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "Early attempts at compositionality in distributed representations used fixed algebraic operations such as vector addition and component-wise multiplication [11] to obtain semantic representations of larger units of text from their constitutent representations.", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "More recently, models represented relational words as tensors of various orders and tensor contraction was adopted as the mean of composition [1, 3, 4].", "startOffset": 142, "endOffset": 151}, {"referenceID": 2, "context": "More recently, models represented relational words as tensors of various orders and tensor contraction was adopted as the mean of composition [1, 3, 4].", "startOffset": 142, "endOffset": 151}, {"referenceID": 3, "context": "More recently, models represented relational words as tensors of various orders and tensor contraction was adopted as the mean of composition [1, 3, 4].", "startOffset": 142, "endOffset": 151}, {"referenceID": 14, "context": "Alongside these principally multi-linear methods of composition, we are observing an emergence of non-linear neural network-based compositional approaches, which derive a sentence vector by recursively applying neural networks to each pair of word vectors [15, 16, 6].", "startOffset": 256, "endOffset": 267}, {"referenceID": 15, "context": "Alongside these principally multi-linear methods of composition, we are observing an emergence of non-linear neural network-based compositional approaches, which derive a sentence vector by recursively applying neural networks to each pair of word vectors [15, 16, 6].", "startOffset": 256, "endOffset": 267}, {"referenceID": 5, "context": "Alongside these principally multi-linear methods of composition, we are observing an emergence of non-linear neural network-based compositional approaches, which derive a sentence vector by recursively applying neural networks to each pair of word vectors [15, 16, 6].", "startOffset": 256, "endOffset": 267}, {"referenceID": 13, "context": "[14] propose to disambiguate each word vector before composition for simple additive and multiplicative compositional models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This idea has now been successfully tested on a series of multi-linear compositional distributional models by Kartsaklis and colleagues [9, 10, 8].", "startOffset": 136, "endOffset": 146}, {"referenceID": 9, "context": "This idea has now been successfully tested on a series of multi-linear compositional distributional models by Kartsaklis and colleagues [9, 10, 8].", "startOffset": 136, "endOffset": 146}, {"referenceID": 7, "context": "This idea has now been successfully tested on a series of multi-linear compositional distributional models by Kartsaklis and colleagues [9, 10, 8].", "startOffset": 136, "endOffset": 146}, {"referenceID": 8, "context": "In Section 2 we discuss the models evaluated in this paper, followed by a description of the disambiguation procedure of [9], adapted to these models, in Section 3.", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "Deep learning algorithms are capable of modelling complex relationships between inputs and outputs in NLP [16, 17, 6].", "startOffset": 106, "endOffset": 117}, {"referenceID": 16, "context": "Deep learning algorithms are capable of modelling complex relationships between inputs and outputs in NLP [16, 17, 6].", "startOffset": 106, "endOffset": 117}, {"referenceID": 5, "context": "Deep learning algorithms are capable of modelling complex relationships between inputs and outputs in NLP [16, 17, 6].", "startOffset": 106, "endOffset": 117}, {"referenceID": 14, "context": "This class of models is known as recursive neural networks (RecNNs) [15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "A recursive auto-encoder (RAE) [16, 6] learns to reconstruct the input, encoded via a hidden layer, as faithfully as possible.", "startOffset": 31, "endOffset": 38}, {"referenceID": 5, "context": "A recursive auto-encoder (RAE) [16, 6] learns to reconstruct the input, encoded via a hidden layer, as faithfully as possible.", "startOffset": 31, "endOffset": 38}, {"referenceID": 8, "context": "Our general methodology essentially recasts the approach of [9] in a deep learning setting; this is depicted in Figure 1.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Other works that combine NNs with WSD, but not in a compositional setting as here, are [7, 13].", "startOffset": 87, "endOffset": 94}, {"referenceID": 12, "context": "Other works that combine NNs with WSD, but not in a compositional setting as here, are [7, 13].", "startOffset": 87, "endOffset": 94}, {"referenceID": 4, "context": "Towards this purpose we use three phrase similarity datasets from the work of Grefenstette and Sadrzadeh [5] (G&S), Kartsaklis et al.", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "[10] (K&S) and Mitchell and Lapata [12] (M&L), consisting of pairs of sentences or phrases annotated with similarity scores by human evaluators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[10] (K&S) and Mitchell and Lapata [12] (M&L), consisting of pairs of sentences or phrases annotated with similarity scores by human evaluators.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "In fact, similar findings have been reported previously in the study of Blacoe and Lapata [2] and Kartsaklis et al.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "This paper aims to explore the effect of prior disambiguation on neural networkbased compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.", "creator": "LaTeX with hyperref package"}}}