{"id": "1411.4491", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Joint cross-domain classification and subspace learning for unsupervised adaptation", "abstract": "Domain adaptation aims at adapting a prediction function trained on a source domain, for a new different but related target domain. Recently several subspace learning methods have proposed adaptive solutions in the unsupervised case, where no labeled data are available for the target. Most of the attention has been dedicated to searching a new low-dimensional domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks.", "histories": [["v1", "Mon, 17 Nov 2014 14:29:35 GMT  (1963kb,D)", "http://arxiv.org/abs/1411.4491v1", null], ["v2", "Tue, 18 Nov 2014 15:55:50 GMT  (1963kb,D)", "http://arxiv.org/abs/1411.4491v2", "Paper is under consideration at Pattern Recognition Letters"], ["v3", "Wed, 29 Apr 2015 02:51:00 GMT  (1980kb,D)", "http://arxiv.org/abs/1411.4491v3", "Paper is under consideration at Pattern Recognition Letters"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["basura fernando", "tatiana tommasi", "tinne tuytelaars"], "accepted": false, "id": "1411.4491"}, "pdf": {"name": "1411.4491.pdf", "metadata": {"source": "CRF", "title": "Joint cross-domain classification and subspace learning", "authors": ["Basura Fernando", "Tatiana Tommasi", "Tinne Tuytelaars"], "emails": [], "sections": [{"heading": null, "text": "Domain adaptation aims to adapt a predictive function trained on a source domain to a new but related target domain. Recently, several subspace learning methods have proposed adaptive solutions for the unattended case where no marked data is available for the target. Most attention has been paid to finding a new low-dimensional domain invariant representation, leaving the definition of the predictive function to a second level. At this point, we propose to learn both together. Specifically, we learn the source subregion that best corresponds to the target subregion, while minimizing a regulated misclassification loss."}, {"heading": "1 Introduction", "text": "In the real world applications with a probability distribution mismatch between the training and the test data is more often the rule than an exception. Think of a part of the language tagging across different text corpora [5], localization over time with wifi signal distributions that are slightly obsolete [33], or biological models that are used across different subjects are also challenged in this regard: real conditions can alter the image statistics in many complex shapes (lighting, background, motion, etc.), not to mention even the difference in the quality of the capture device (e.g. resolution), or the high number of possible artificial modifications made through post-processing (e.g. filtering). Due to this great variability, a learning algorithm that is set to a final target file that is poorly produced."}, {"heading": "2 Related Work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "3 Problem Setup and Back-", "text": "erdLet us see a classification problem where the data instances are in the form (xi, yi).Here xi-RD is the feature vector for the i-th sample and yi-th sample and yi-th sample and yi-th sample and yi-th sample and yi-th sample and yi-th sample and yi-th sample and yi-th sample. C) is the corresponding label. We assume that ns labeled training samples are drawn from one source domain distribution, while a number of unlabeled test samples come from another target distribution, so that for the common probabilities Ds = P (xs, ys), Dt = P (xt, yt) it holds Ds 6 = Dt.A labeled on target domain error Theoretical studies on DA allow to establish the conditions in which a classifier can be trained on the source data to work well on the target data."}, {"heading": "4 Proposed Approach", "text": "In the context of the covariate shift assumption, we can consider the error \u03bb in (1) negligible."}, {"heading": "5 Experiments", "text": "We validate our approach across multiple domain adaptation tasks. In the following, we first describe our experimental setting and then report on the results obtained with a detailed analysis of how JCSL effectively optimizes against source classification errors and cross-domain similarity."}, {"heading": "5.1 Datasets, baselines and implementation details", "text": "We select three image datasets provided by Hoffman (see Figure 1) and a Wifi signal dataset [19]. This dataset was created by combining the Office dataset [28] with Caltech256 [16] and contains images of 10 object classes across four ranges: Amazon, Dslr, Webcam, and Caltech. Amazon consists of images from online retailers \"catalogs,\" while Dslr and webcam domains are composed of high and low resolution images respectively. Finally, Caltech corresponds to a subset of the original Caltech256. We use the functions provided by Gong et al. [12] We use the data already used in several previous publications: SURF descriptors standardized into histograms of visual words and standardized Z-score. All possible source domain pairs are considered."}, {"heading": "5.2 Results - Office+Caltech and MNIST+USPS", "text": "The obtained results via the Office + Caltech and the MNIST + USPS datasets are presented in Table 1. Overall JCSL, showing the considered baselines in 8 source-target pairs out of 14 and showing the best average results across the two datasets. Thus, we can conclude that by minimizing a target between source-target similarity and the source classification error pays off compared to just reducing cross-domain representation divergence divergence in some of the cases considered, the discriminatory LDA subspace is most likely to be exploited. With respect to JCSL, TCA seems to work particularly well when the domain shift is small (i.e. Amazon \u2192 Caltech, Dslr \u2192 Webcam) is the only method that consistently analyzes the NA via MNISL + USPS parameters to better understand the performance of JCSL."}, {"heading": "5.3 Results - Bing+Caltech", "text": "Due to the way it has been defined, Bing + Caltech can be considered a much more demanding testing environment for unattended domain adjustments than the other data sets used (see also Figure 1). At the same time, it also meets one of the most realistic scenarios in which domain adjustments are required: We have access to only a limited number of noiselessly labeled source images that we obtain from the Web, and we want to use them to classify through a curated collection of object images. At best, the use of all available information is crucial to this problem. As the source is not entirely reliable, the coding of its discriminatory information in the representation (e.g. by LDA or PLS) can be misleading. On the other hand, using the portion of the non-noiseless target data to control the learning process can be much more beneficial. As shown in Figure 3, JCSL is the only method that consistently improves over the non-adaptive approach regardless of the number of classes considered."}, {"heading": "5.4 Results - WiFi Localization", "text": "In order to demonstrate the universality of the proposed algorithm, we also evaluate JCSL on non-visual data. Specifically, the results of the WiFi localization task are shown in Table 3. The obtained classification accuracy confirms the value of our method compared to other sub-space-based techniques."}, {"heading": "6 Conclusions", "text": "Motivated by the theoretical results of Ben-David et al. [2], we have proposed in this paper to integrate the learning process of the source prediction function with the optimization of the invariant subspace for unattended domain fitting. In particular, JCSL learns a representation that minimizes the divergence of the Bregman matrix between the source and target subrespaces while optimizing the classification model. Extensive experimental results have shown that JCSL, by utilizing the combination of principles described and without the need to traverse the evaluation of data distributions, outperforms several other subspatial domain adaptation methods that focus only on the representational part. Recently, several papers have shown that the performance of revolutionary neural networks is resilient to domain shifts [6, 26]. When evaluating at a high level, we can identify the cause of such robustness on the basis of the same idea: learning pre-discriminating architectures and a highly transformative representation in CSL."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of the FP7 EC project AXES."}], "references": [{"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["Mahsa Baktashmotlagh", "Mehrtash Harandi", "Brian Lovell", "Mathieu Salzmann"], "venue": "In ICCV 2013,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Analysis of representations for domain adaptation", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": "In NIPS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach", "author": ["Alessandro Bergamo", "Lorenzo Torresani"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Domain adaptation with coupled subspaces", "author": ["John Blitzer", "Dean Foster", "Sham Kakade"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "CoRR, abs/1310.1531:1\u20138,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Domain transfer multiple kernel learning", "author": ["Lixin Duan", "Ivor W. Tsang", "Dong Xu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Domain transfer svm for video concept detection", "author": ["Lixin Duan", "Ivor W. Tsang", "Dong Xu", "Stephen J. Maybank"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["Basura Fernando", "Amaury Habrard", "Marc Sebban", "Tinne Tuytelaars"], "venue": "In ICCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["Boqing Gong", "Yuan Shi", "Fei Sha", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "Ruonan Li", "R. Chellappa"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A kernel method for the two sample problem", "author": ["Arthur Gretton", "Karsten Borgwardt", "Malte Rasch", "Bernhard Schlkopf", "Alexander Smola"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Caltech-256 object category dataset", "author": ["Gregory Griffin", "Alex Holub", "Pietro Perona"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Discovering latent domains for multisource domain adaptation", "author": ["Judy Hoffman", "Brian Kulis", "Trevor Darrell", "Kate Saenko"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Efficient learning of domain-invariant image representations", "author": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "venue": "In ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Co-regularization based semisupervised domain adaptation", "author": ["Abhishek Kumar", "Avishek Saha", "Hal Daume"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["Mingsheng Long", "Jianmin Wang", "Guiguang Ding", "Jiaguang Sun", "P.S. Yu"], "venue": "In ICCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In COLT,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Subspace interpolation via dictionary learning for unsupervised domain adaptation", "author": ["Jie Ni", "Qiang Qiu", "Rama Chellappa"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Domain adaptation via transfer component analysis", "author": ["Sinno Jialin Pan", "Ivor W. Tsang", "James T. Kwok", "Qiang Yang"], "venue": "In IJCAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Adapting visual category models to new domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "In ECCV,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Generalized domain-adaptive dictionaries", "author": ["Sumit Shekhar", "Vishal M. Patel", "Hien V. Nguyen", "Rama Chellappa"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Bregman divergence-based regularization for transfer subspace learning", "author": ["Si Si", "Dacheng Tao", "Bo Geng"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A two-stage weighting framework for multi-source domain adaptation", "author": ["Qian Sun", "Rita Chattopadhyay", "Sethuraman Panchanathan", "Jieping Ye"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Pattern Analysis and Machine Intellingence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Estimating location using wi-fi", "author": ["Qiang Yang", "Sinno Jialin Pan", "Vincent Wenchen Zheng"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Think about part of speech tagging across different text corpora [5], localization over time with wifi signal distributions that get easily outdated [33], or biological models to be used across different subjects [31].", "startOffset": 65, "endOffset": 68}, {"referenceID": 31, "context": "Think about part of speech tagging across different text corpora [5], localization over time with wifi signal distributions that get easily outdated [33], or biological models to be used across different subjects [31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 29, "context": "Think about part of speech tagging across different text corpora [5], localization over time with wifi signal distributions that get easily outdated [33], or biological models to be used across different subjects [31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "In general all the techniques based on this idea focus on transforming the representation of the source and target samples to maximize some notion of similarity between them [12, 13, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 11, "context": "In general all the techniques based on this idea focus on transforming the representation of the source and target samples to maximize some notion of similarity between them [12, 13, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 8, "context": "In general all the techniques based on this idea focus on transforming the representation of the source and target samples to maximize some notion of similarity between them [12, 13, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 1, "context": "As thoroughly discussed in [2, 24], the choice of the feature representation able to reduce the domain divergence is indeed a crucial factor for the adaptation.", "startOffset": 27, "endOffset": 34}, {"referenceID": 22, "context": "As thoroughly discussed in [2, 24], the choice of the feature representation able to reduce the domain divergence is indeed a crucial factor for the adaptation.", "startOffset": 27, "endOffset": 34}, {"referenceID": 18, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 179, "endOffset": 187}, {"referenceID": 26, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 179, "endOffset": 187}, {"referenceID": 7, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 232, "endOffset": 242}, {"referenceID": 16, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 232, "endOffset": 242}, {"referenceID": 6, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 232, "endOffset": 242}, {"referenceID": 15, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 315, "endOffset": 322}, {"referenceID": 19, "context": "The unlabeled part of the target has also been used for co-regularization [21] with the aim of better integrate the source and the target classifiers.", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "In the more challenging unlabeled setup most of the approaches resort to the estimating the data distributions and minimizing a distance measure between them while re-weighting/selecting the samples [31, 11] or while looking for a new representation [1].", "startOffset": 199, "endOffset": 207}, {"referenceID": 9, "context": "In the more challenging unlabeled setup most of the approaches resort to the estimating the data distributions and minimizing a distance measure between them while re-weighting/selecting the samples [31, 11] or while looking for a new representation [1].", "startOffset": 199, "endOffset": 207}, {"referenceID": 0, "context": "In the more challenging unlabeled setup most of the approaches resort to the estimating the data distributions and minimizing a distance measure between them while re-weighting/selecting the samples [31, 11] or while looking for a new representation [1].", "startOffset": 250, "endOffset": 253}, {"referenceID": 12, "context": "The Maximum Mean Discrepancy (MMD) [14] which maps two sets of data to a reproducing Kernel Hilbert Space has been largely used as distance measure between two distributions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "Although endowed with nice properties, the choice of the kernel and kernel parameters is critical and if non-optimal can lead to very poor estimate of the distribution distance [15].", "startOffset": 177, "endOffset": 181}, {"referenceID": 23, "context": "Dictionary learning methods have also been used with the goal of defining new representations that overcome the domain shift [25, 29].", "startOffset": 125, "endOffset": 133}, {"referenceID": 27, "context": "Dictionary learning methods have also been used with the goal of defining new representations that overcome the domain shift [25, 29].", "startOffset": 125, "endOffset": 133}, {"referenceID": 30, "context": "As for dictionary learning, the approaches presented in this framework are mostly linear, but can be easily extended to non-linear spaces through explicit feature mappings [32].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "In [4] Canonical Correlation Analysis (CCA) has been applied to find a coupled domain-invariant subspace.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "This idea was introduced in [13] where the path across the domains is defined as a geodesic curve over a Grassmann manifold.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "This approach has been further extended in [12] where all the intermediate subspaces are integrated to define a cross-domain similarity measure.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "Recently the Subspace Alignment (SA) method [10] demonstrated that it is possible to map directly from the source to the target subspace without necessarily passing through intermediate steps.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "A first attempt in this direction has been done in [12] by substituting the use of PCA over the source subspace with Partial Least Squares (PLS).", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "We underline here that, although solutions similar in spirit have been exploited in the semi-supervised setup [18, 7], we consider the specific case of subspace modeling and the unsupervised case where no access to the target labels is available, not even for hyperparameter cross validation.", "startOffset": 110, "endOffset": 117}, {"referenceID": 6, "context": "We underline here that, although solutions similar in spirit have been exploited in the semi-supervised setup [18, 7], we consider the specific case of subspace modeling and the unsupervised case where no access to the target labels is available, not even for hyperparameter cross validation.", "startOffset": 110, "endOffset": 117}, {"referenceID": 1, "context": "Specifically, the following generalization bound on the target error t has been demonstrated in [2]:", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "The domain shift can by reduced by aligning the associated basis sets and minimizing the following Bregman matrix divergence [10]", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "This approach has shown promising results for visual crossdomain classification tasks outperforming other subspace adaptive methods [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "However, on par with its competitors [13, 12], it keeps the domain adaptive process (learning M) and the classification process (e.", "startOffset": 37, "endOffset": 45}, {"referenceID": 10, "context": "However, on par with its competitors [13, 12], it keeps the domain adaptive process (learning M) and the classification process (e.", "startOffset": 37, "endOffset": 45}, {"referenceID": 8, "context": "We followed previous literature in using PCA to define the target subspace T [10, 12].", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "We followed previous literature in using PCA to define the target subspace T [10, 12].", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "Office + Caltech [12].", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "This dataset was created by combining the Office dataset [28] with Caltech256 [16] and it contains images of 10 object classes over four domains: Amazon, Dslr, Webcam and Caltech.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "This dataset was created by combining the Office dataset [28] with Caltech256 [16] and it contains images of 10 object classes over four domains: Amazon, Dslr, Webcam and Caltech.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "[12] already used in several previous publications: SURF descriptors quantized into histograms of 800 bag-of-visual words and standardized by z-score normalization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "MNIST [22] + USPS [19].", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "MNIST [22] + USPS [19].", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "By following [23] we uniformly re-scale all images to size 16 \u00d7 16 and we use the gray-scale pixel values as feature vectors.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "Bing+Caltech [3].", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "We run experiments varying the number of categories (5, 10, 15, 20, 25 and 30) and the number of source examples per category (5 and 10) using the same train/test split adopted in [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "As typically done for this dataset, Classemes features are used as image representation [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 31, "context": "WiFi [33].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 188, "endOffset": 192}, {"referenceID": 8, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 1, "context": "Measuring the domain shift For the same domain pairs considered above we also evaluate empirically the H\u2206H divergence measure defined in [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 31, "context": "Table 3: Classification accuracy obtained over WiFi localization dataset [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "[2], in this paper we proposed to integrate the learning process of the source prediction function with the optimization of the invariant subspace for unsupervised domain adaptation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Recently several works have demonstrated that Convolutional Neural Network performance are robust to domain shift [6, 26].", "startOffset": 114, "endOffset": 121}, {"referenceID": 24, "context": "Recently several works have demonstrated that Convolutional Neural Network performance are robust to domain shift [6, 26].", "startOffset": 114, "endOffset": 121}], "year": 2017, "abstractText": "Domain adaptation aims at adapting a prediction function trained on a source domain, for a new different but related target domain. Recently several subspace learning methods have proposed adaptive solutions in the unsupervised case, where no labeled data are available for the target. Most of the attention has been dedicated to searching a new lowdimensional domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks.", "creator": "LaTeX with hyperref package"}}}