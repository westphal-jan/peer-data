{"id": "1411.4618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Relations World: A Possibilistic Graphical Model", "abstract": "We explore the idea of using a \"possibilistic graphical model\" as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user's family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model.", "histories": [["v1", "Mon, 17 Nov 2014 20:15:00 GMT  (850kb)", "http://arxiv.org/abs/1411.4618v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["christopher j c burges", "erin renshaw", "andrzej pastusiak"], "accepted": false, "id": "1411.4618"}, "pdf": {"name": "1411.4618.pdf", "metadata": {"source": "CRF", "title": "Relations World: A Possibilistic Graphical Model1", "authors": ["Christopher J.C. Burges", "Erin Renshaw"], "emails": [], "sections": [{"heading": null, "text": "As a first step, we have developed a system that uses text-based dialogues to derive a model of the user's family relationships, using its world model to infer relational triples, to learn to recover from the errors and ambiguities of upstream correlation resolution, and to learn context-dependent paraphrase models, and to examine some theoretical aspects of the underlying graphical model."}, {"heading": "Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "A Possibilistic Graphical Model", "text": "uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuzm uzm uzm uzm uzm uzm uzm uzm uzm uzm uzm uzm uzm uzm uzm uzm ucahcnlsrsrsrsrsrsrsrsrsuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu"}, {"heading": "A3, \ud835\udc451", "text": "It is not even a year since the Presidency of the Council of the European Union got old, but it is not yet in a position to leave the Presidency of the Council of the European Union, \"he said.\" It is not yet so far, \"he said.\" The Presidency of the Council of the European Union is still in progress, that the Presidency of the Council of the European Union will leave the Presidency of the Council of the European Union. \""}, {"heading": "Natural Language Processing", "text": "In fact, it is such that most of them are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, and live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, and live, life, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, that they live, that they live, that they live, in which they live, in which they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, in which they live, that they live,"}, {"heading": "Error Correction and Language Learning", "text": "Our prototype can correct upstream NLP errors and ambiguities both automatically and with the help of the user. For example, in the sentence My brother is called Bill and my father is called Bill, the Stanford systems incorrectly identify the two bills as referring to the same entity; however, our system automatically recognizes that this may not be the case and corrects the error (note that this is a simple example of a world model that is required for the correction solution; in the sentence My father is called Bill and my mother's husband is called Bill, the two bills are coreferent, despite the similar sentence structure) Similar reasoning can be used to resolve ambiguities as his in John's father was called Carl. His wife was called Theresa in terms of error correction through dialogue, one example is as follows: When I presented with the sentence I have a daughter, the name Susan, the correction daughter fails with Susan; the system asks for the name of the daughter, and when Susan is asked."}, {"heading": "Discussion", "text": "The main objectives of this work are twofold: firstly, to explore the questions of the correctness and interpretability of learned models that use dialogue as the main source of new information; and secondly, our research into the meaning of projections in general, to model the semantics of the text; the demand for explicit, efficient and feasible modelling of uncertainties led us to possible graphic models that provide an easy way to capture and update uncertainties relating to the finest granularity; and, in order to accommodate these desires, we have so far avoided using statistical techniques to classify statistical techniques as necessary in order to obtain them, to maintain projections, to maintain the projections that we have used to detect entity, coreferation and naming, and to detect a high level of family relationships. We chose the latter as a very interesting step because it is so well studied and because the relational structure is not clear."}, {"heading": "Acknowledgements", "text": "We would like to thank Aitao Chen for his generous help with his NLPLib package and Nicolaj Bj\u00f8rner for his generous help with Z3."}], "references": [{"title": "Towards the Machine Comprehension of Text: An Essay", "author": ["C.J.C. Burges"], "venue": "Microsoft Research Technical Report MSR-TR-2013-125", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "From machine learning to machine reasoning", "author": ["L. Bottou"], "venue": "arXiv abs/1102.1808", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge engineering for large belief networks", "author": ["M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion"], "venue": "UAI 1994, as mentioned in D. Koller and N. Friedman, Probabilistic graphical models: principles and techniques, MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Society of Mind", "author": ["M. Minsky"], "venue": "Simon and Schuster", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1988}, {"title": "Machine Perception Of Three-Dimensional Solids", "author": ["L.G. Roberts"], "venue": "PhD thesis, MIT", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1963}, {"title": "Introduction to Logic", "author": ["M. Genesereth", "E. Kao"], "venue": "2 edition, Synthesis Lectures on Compute Science", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "MSR SPLAT: a language analysis toolkit", "author": ["C. Quirk", "P. Choudhury", "J. Gao", "H. Suzuki", "K. Toutanova", "M. Gamon", "W. Yih", "L. Vanderwende", "C. Cherry"], "venue": "Proceedings of the NAACL-HLT 2012: Demonstration Session", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the ACL conference on Empirical methods in natural language processing, 10:1 -8. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "A", "author": ["R. Weischedel", "S. Pradhan", "L. Ramshaw", "J. Kaufman", "M. Franchini", "M. ElBachouti", "N. Xue", "M. Palmer", "M. Marcus"], "venue": "Taylor, OntoNotes Release 4.0, Technical Report, BBN Technologies, December 24", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Bootstrapping path-based pronoun resolution", "author": ["S. Bergsma", "D. Lin"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pp. 33\u201440", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Scripts", "author": ["R.C. Schank", "R.P Abelson"], "venue": "plans, goals, and understanding: An inquiry into human knowledge structures, Psychology Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "the machine comprehension of text [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "those modules, is a fundamental principle of software design but is also expected to be a key design principle for large semantic models [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "models (neural nets, trees, ensembles of these, and even moderately sized probabilistic graphical models) are not easily interpretable (see for example [3]): in general we do not understand why our statistical systems make", "startOffset": 152, "endOffset": 155}, {"referenceID": 3, "context": "The idea is roughly analogous to the \u201cagents\u201d in Minsky\u2019s Society of Mind [4].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "The work can be viewed as a semantic modeling analog of Blocks World [4,5], in that it takes a simple task and uses it to explore the ideas.", "startOffset": 69, "endOffset": 74}, {"referenceID": 4, "context": "The work can be viewed as a semantic modeling analog of Blocks World [4,5], in that it takes a simple task and uses it to explore the ideas.", "startOffset": 69, "endOffset": 74}, {"referenceID": 5, "context": "[6], but with data structures chosen to meet the above desiderata.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We emphasize that inference in this system is simply using relational logic [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "We use three systems: SPLAT, a publically available NLP toolkit [7]; NLPLib, an internal NLP toolkit that uses the averaged perceptron algorithm [8] trained on the part of speech and constituency tree tags and data in OntoNotes Release 4.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "We use three systems: SPLAT, a publically available NLP toolkit [7]; NLPLib, an internal NLP toolkit that uses the averaged perceptron algorithm [8] trained on the part of speech and constituency tree tags and data in OntoNotes Release 4.", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "0 [9], which we used for entity detection [10]; and the Stanford natural language system for labeled dependency trees and coreference resolution [11].", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": "For a particularly elegant example of using the statistics of large datasets to solve what is otherwise a thorny coreference problem, see [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "The use of meaning projections to automatically build higher level semantic constructs such as scripts and plans [14], which encapsulate typically observed sequences of events, is also an interesting direction for research.", "startOffset": 113, "endOffset": 117}], "year": 2014, "abstractText": "We explore the idea of using a possibilistic graphical model as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user\u2019s family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model.", "creator": "Microsoft\u00ae Word 2013"}}}