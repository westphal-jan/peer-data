{"id": "1411.5007", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2014", "title": "A Unified View of Large-scale Zero-sum Equilibrium Computation", "abstract": "The task of computing approximate Nash equilibria in large zero-sum extensive-form games has received a tremendous amount of attention due mainly to the Annual Computer Poker Competition. Immediately after its inception, two competing and seemingly different approaches emerged---one an application of no-regret online learning, the other a sophisticated gradient method applied to a convex-concave saddle-point formulation. Since then, both approaches have grown in relative isolation with advancements on one side not effecting the other. In this paper, we rectify this by dissecting and, in a sense, unify the two views.", "histories": [["v1", "Tue, 18 Nov 2014 20:43:39 GMT  (23kb)", "http://arxiv.org/abs/1411.5007v1", "AAAI Workshop on Computer Poker and Imperfect Information"]], "COMMENTS": "AAAI Workshop on Computer Poker and Imperfect Information", "reviews": [], "SUBJECTS": "cs.AI cs.GT", "authors": ["kevin waugh", "j", "rew bagnell"], "accepted": false, "id": "1411.5007"}, "pdf": {"name": "1411.5007.pdf", "metadata": {"source": "CRF", "title": "A Unified View of Large-scale Zero-sum Equilibrium Computation", "authors": ["Kevin Waugh", "J. Andrew Bagnell"], "emails": ["waugh@cs.cmu.edu", "dbagnell@ri.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.50 07v1 [cs.AI] 18 Nov 201 4"}, {"heading": "Introduction", "text": "The first annual computer poker competition was held in 2007 and provides a testing environment for opposing decision-making with imperfect information. Although incredible advances have been made since then, solving an abstract game remains a critical component of the top agents. The strength of such a strategy correlates with how well the abstract game models solve the insolubly large full game. Algorithmic improvements in equilibrium calculation allow teams to solve larger abstract games, thereby improving the overall strength of their agents. The first off-the-shelf linear programming packages used to solve the abstract games. Shortly thereafter, two specialized equilibrium finding techniques emerged that drastically reduce resource requirements by allowing implicit game representations. A method, Contrafactual Repentance Minimization (CFR), brings together many simple, non-regretful learners to minimize the overall regret and as a whole, they converge to an equilibrium vich (another technique currently applied to GT), the NGT technique (the other one)."}, {"heading": "Zero-Sum Extensive-form Games", "text": "A comprehensive formal game is a partition of player history. (N, H, p, \u03c3c, I, u) (see, e.g. Osborne and Rubinstein) The game has no players whose set of strategies is [N]. (H, form a tree that is rooted in \u03c6, the empty story. (H, we denote the number of actions available from A (h). (N) The action is a player who is either a player or nature is a child of h. The leaves of the tree or the terminal story are of Z'H. In a non-terminal story we will use the player choice function, p: H\\ Z \u2192 [N] the action will determine who should act, either a player or nature. Nature's policy defines a distribution over the actions when it comes to act. (h) The information division is a (h). (h) The information division, I = a partition is a partition of player history. N () is a partition of player history."}, {"heading": "Counterfactual Regret Minimization", "text": "There is an important link between no-regret learning and zero-sum equilibrium computation. Two no-regret algorithms in Self-play converge to a Nash-equilibrium converge to a Nash-equilibrium converge to a-equilibrium equilibrium computation. Two no-regret algorithms in Self-play converge to a-equilibrium equilibrium converge to a Nash equilibrium. Operationally, the player receives reward ut = \u2212 ATxt.Theorem 1 If two no-regret algorithms in Self-play converge to a Nash equilibrium."}, {"heading": "Connection to Convex Optimization", "text": "Another way of looking at the \"no-x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "Dual Averaging", "text": "It is not as if it is not also a question of whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is"}, {"heading": "Initialization with a Prior", "text": "When calculating a balance or in an online setting, the initial strategy is generally uniformly random. Although the initial error recedes fairly quickly, it is often the case that we have good priorities. Especially in an online setting, it is preferable to start with a good strategy rather than essentially learn how to play the game and how to exploit an opponent at the same time. From an optimization perspective, we are now discussing sensible approaches. First, let's examine how we can initialize the double weights - the cumulative counterfactual regret. In the dual weight shift, the double weights are after x, g = Ay, the utilities after x the optimal strategy of the opponent. If we have guesses on x and y, we can use them to initialize the double weights. This view of the double weights is a simplification of what Brown and Sandholm are doing."}, {"heading": "Convergence of the Current Policy", "text": "Experimentally, the current policy has been shown to work quite well in practice despite the lack of limits on its performance. A large selection of current work on stochastic convex optimization has examined this problem in depth and examined various mean and step size selection schemes. If you look at the problem from the point of view of convex optimization, these results are transmitted without modification. In particular, it has been shown that using a step size that decreases by 1 / 2,000 tons will lead to a convergence of the current iterate for non-smooth optimization. That is, if the adversary gives an optimal answer, as in CFR-BR, we do not need to save the average policy that reduces the memory requirement by 50%."}, {"heading": "Acceleration and Sampling", "text": "An important reason that no regret algorithms have emerged as the dominant approach to large-scale equilibrium calculation is their adaptability to various forms of scanning. At a high level, by introducing stochasticity, we can drastically reduce the calculation for each iteration by introducing different types of thrift, while only marginally increasing the number of necessary iterations. Theorem 7 (by Cesa-Bianchi and Lugosi 2006) can drastically reduce the sequence of individual procedures by introducing different types of thrift, while only slightly increasing the number of needed iterations. For all other types we can think of, with the probability that the sequence x x turns out to be explicit."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The task of computing approximate Nash equilibria in large zero-sum extensive-form games has received a tremendous amount of attention due mainly to the Annual Computer Poker Competition. Immediately after its inception, two competing and seemingly different approaches emerged\u2014one an application of noregret online learning, the other a sophisticated gradient method applied to a convex-concave saddle-point formulation. Since then, both approaches have grown in relative isolation with advancements on one side not effecting the other. In this paper, we rectify this by dissecting and, in a sense, unify the two views.", "creator": "LaTeX with hyperref package"}}}