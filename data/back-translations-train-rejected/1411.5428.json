{"id": "1411.5428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2014", "title": "Differentially Private Algorithms for Empirical Machine Learning", "abstract": "An important use of private data is to build machine learning classi- fiers. While there is a burgeoning literature on differentially private classification algorithms, we find that they are not practical in real applications due to two reasons. First, existing differentially private classifiers provide poor accuracy on real world datasets. Second, there is no known differentially private algorithm for empirically evaluating the private classifier on a private test dataset.", "histories": [["v1", "Thu, 20 Nov 2014 03:10:47 GMT  (238kb)", "https://arxiv.org/abs/1411.5428v1", null], ["v2", "Fri, 21 Nov 2014 20:41:04 GMT  (238kb)", "http://arxiv.org/abs/1411.5428v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ben stoddard", "yan chen", "ashwin machanavajjhala"], "accepted": false, "id": "1411.5428"}, "pdf": {"name": "1411.5428.pdf", "metadata": {"source": "CRF", "title": "Differentially Private Algorithms for Empirical Machine Learning", "authors": ["Ben Stoddard", "Yan Chen", "Ashwin Machanavajjhala"], "emails": ["stodds@cs.duke.edu", "yanchen@cs.duke.edu", "ashwin@cs.duke.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.54 28v2 [cs.LG] 2 1N ov2 01In this paper, we develop differentiated private algorithms that reflect empirical workflows of machine learning in the real world. We consider the algorithm for training private classifiers to be a black box. We present private algorithms for selecting features that are entered into the classifier. Although adding a pre-processing step removes part of the data protection budget from the actual classification process (potentially making it louder and less accurate), we show that our novel pre-processing techniques significantly increase classification accuracy on three real data sets."}, {"heading": "1. INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2. NOTATION", "text": "Let D be a dataset with d attributes, and let D denote the set of all such datasets. One of the attributes is called the identifier L. The rest of the attributes is called the identifier F. We assume that all attributes are binary (although all results in the essay can be extended to nonbinary attributes). In text classification datasets (used in our experiments), binary attributes correspond to the presence or absence of certain words from a given vocabulary. For each tuple t in the dataset D, t [L] should denote the value of the identifier of the tuple, and t [F] the value of the identifier F for that tuple. We assume that attribute vectors are sparse; each tuple has at most s attributes with t [F] 6 = 0. We denote the number of tuples in D with n and the number of tuples in D with n."}, {"heading": "2.1 Differential Privacy", "text": "An algorithm satisfies the differential privacy if its output on a data set does not change significantly due to the presence or absence of a single tuple in the data sets. (D1) A randomized algorithm satisfies the differentiated privacy if it satisfies the privacy (D1, D2). (D2) A randomized algorithm satisfies the privacy if it satisfies the privacy (M) and the privacy (D1, D2). (D2) A differentiated algorithm satisfies the privacy if it satisfies the privacy (M) and the privacy (D1, D2)."}, {"heading": "3. PRIVATE FEATURE SELECTION", "text": "In this area, we are able to assert ourselves that we are able to be ourselves, and that we are able to be ourselves, that we will be able to be ourselves, that we will be able to be ourselves, that we will be able to be ourselves, that we will be able to be ourselves, that we will be able to be ourselves, that we will be able to be ourselves, that we will be able to be ourselves, that we will be able to be ourselves, that we will be able to put ourselves in a position, that we will be able to put ourselves in a position. \""}, {"heading": "3.1 Example Scoring Functions", "text": "Total score: The total score for a feature F denoted by TC (F, D) is nF = 1 (PD = 1) the number of features denoting t [F] = 1. Selecting features with a high total number of points eliminates features that rarely have the value 1. Difference count: The difference score for a feature F denoted by DC (F, D) is defined as: DC (F, D) = | nF = 1 \u00b2 L = 1 \u2212 nF = 1 \u00b2 L = 0 (3) DC (F, D) is large whenever one label is more common than the other label for F = 1. The difference count is lowest if both markings are equally probable for tuples with F = 1. The difference count is greatest if L is either all 1s or all 0s if it is conditioned to F = 1. Purity index [11]."}, {"heading": "3.2 Score Perturbation", "text": "A simple strategy for selecting the characteristics is: (a) disturbance of the characteristics by using the Laplace mechanism, and (b) selection of the characteristics whose noise level exceeds the threshold of the threshold value (or selection of the top k characteristics by sound level); the scale of Laplace noise required for privacy is S (Q) \u00b7 \u2206 (Q) / \u0445 fs, where (i) S (Q) is the global sensitivity of the scoring function for a characteristic; and (ii) \u0445 (Q) is the number of characteristics affected by the addition or removal of a tuple. The sensitivity of the total number TC, difference level DC and purity index PI are all 1. The sensitivity of the characteristics of the information gain function is demonstrably O (log n) [9, 29] where n (an upper limit) is the number of tuples in the dataset."}, {"heading": "3.3 Cluster Selection", "text": "The shortcoming of the score error is that we each add a separate scoring to the scores of all features. As the number of features increases, the probability of undesirable features being selected increases. 1If we use a limited differential privacy in which adjacent datasets have the same number of tuples, we can show that \u2206 (Q) \u2264 2 \u00d7 s counts for each scoring function, as the adjacent datasets differ in values of no more than 2 \u00d7 s attributes. Algorithm 2 Private Threshold Testing (D, Q, \u03c4) Privacy. + Lap (1 / \u0432) for each query Qi-Q doif Qi (D) -doif Qi (D) clusters differ in terms of the respective attributes."}, {"heading": "3.4 Private Threshold Testing", "text": "In this section, we present a novel mechanism called private threshold testing (PTT). (PTT) For the SCOREBASEDFS problem, the utility of which is independent of both sides and the number of features (PTT). (PTT) Instead of disrupting the results of all functions, PTT disrupts a threshold and returns the number of features greater than the disturbed threshold. (PTT) We believe that PTT has applications beyond the attribute selection, and therefore we describe them more generally. (Let Q = {Q1, Q2,.) Denote a series of real evaluations over a dataset D, all of which have the same sensitivity. (In our case, any Qi = Q (Fi =), and m = F |). PTT has as input the number of Q queries and a real number of queries, and outputs a vector v (0, 1), where and only if (D)."}, {"heading": "4. PRIVATE EVALUATION OF CLASSIFIERS", "text": "In this section we describe an algorithm for quantifying the accuracy of any binary classifier under differential privacy on a test dataset containing sensitive information."}, {"heading": "4.1 ROC curves", "text": "Real number of tuples with a real identification 1 and 0 of a real identification 1 of a real identification 1 and 0 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 and 0 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 and 0 of a real identification 1 of a real identification 1 of a real identification 1 and 0 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 and 0 of a real identification 1 of a real identification 1 of a real identification 1 of a real identification 1 and 1 of a real identification 1."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we experimentally evaluate our differentiated private feature selection algorithms (Section 5.1) and the generation of ROC curves (Section 5.2). The most important side effects from the experimental evaluation of the differentiated private feature selection are: \u2022 Spending part of the privacy budget on private feature selection can significantly improve the error rate (10% - 15%) of a differentiated private classifier, despite a more noisy classifier due to the lower privacy budget. \u2022 Characteristic selection using private threshold testing consistently leads to classifiers with higher accuracy than the selection of features using score perturbation and cluster selection PTT, even significantly better than a related NOISYCUT technique in solving the SCOREBASEDFS problem. \u2022 In the differential data protection system, simple assessment techniques (such as the total TC) of the ROC curve line are used as well as REC or even better than private measures."}, {"heading": "5.1 Feature Selection", "text": "This year it is more than ever before."}, {"heading": "5.2 Private Evaluation", "text": "We do not use the pre-stored test sets of SMS and TWITTER records from the previous section. SMS test sets contain 558 data, and each TWITTER record has a real label. The TWITTER test sets contain 684 different TWITTER records, of which 385 thresholds are equal to the real ROC curves and the differentiated private ROC curves for both SMS and TWITTER records using different households. Recall that, in k-RECURSIVEMEDIANS, 1 household is used to select a set of thresholds."}, {"heading": "6. RELATED WORK", "text": "In this context, it should be noted that this is a mere formality, and that it is not an isolated case, but an isolated case."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we present algorithms that can facilitate the introduction of differentiated private methods of classifying private data. We present novel algorithms for private feature selection and demonstrate experimentally, using three real high-dimensional datasets, that using a portion of the privacy budget for feature selection can improve the predictive accuracy of the classifier trained for the selected characteristics. In addition, we also solve the problem of private generation of ROC curves, which allows a user to quantify the predictive accuracy of a binary classifier for a private test dataset. In conjunction with these algorithms, a data analyst can now mimic typical \"big data workflows\" that (a) pre-process the data (i.e., select characteristics), (b) build a model (i.e. train a classifier), and (c) evaluate the model on a pre-set test (i.e., generate a ROC curve without much precision)."}, {"heading": "8. REFERENCES", "text": "[1] T. A. Almeida, J. M. G. Hidalgo, and T. P. Silva. Towards sms spamfiltering: Results under a new dataset. International Journal of Information Security Science, pp. 1-18, 2012. [2] A. and Sarwate and K. Chaudhuri. Privacy trees, Inc., Secaucus, NJ, USA, 2006. [4] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: The sulq framework. PODS '05, pp. 128-138, New York, USA, 2005. ACM. [5] M. Dash and H. Liu. Feature selection for classification."}, {"heading": "A. PROOF OF THEOREM 2", "text": "PROOF. For all two adjacent rows D and D we want to show: P (vD) is the output of the PTT. Leave N1 = (i) the answers given by PTT for queries 1 to i \u2212 1. ThenP (vD) = (i) = 0) = 1 and 0 answers or replies of the PTT. Leave v (< i) the answers given by PTT for queries 1 to i \u2212 1. ThenP (vD) P (vD) = 0) = 1 and 0 answers or replies of the PTT. Leave v (v) = v [< i] the answers given by PTT for queries 1 to i \u2212 1. ThenP (vD) = 1 (vD) = 1 (vD) = 1 (Qi) = 1 (Qi) = 1 (Qi)."}], "references": [{"title": "Towards sms spam filtering: Results under a new dataset", "author": ["T.A. Almeida", "J.M.G. Hidalgo", "T.P. Silva"], "venue": "International Journal of Information Security Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Differentially Private Empirical Risk Minimization", "author": ["Sarwate", "K. Chaudhuri"], "venue": "CoRR, abs/0912.0,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Practical privacy: The sulq framework", "author": ["A. Blum", "C. Dwork", "F. McSherry", "K. Nissim"], "venue": "PODS \u201905,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Feature selection for classification", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent Data Analysis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "In in ICALP, pages 1\u201312. Springer,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Privacy in pharmacogenetics: An end-to-end case study of  personalized warfarin dosing", "author": ["M. Fredrikson", "E. Lantz", "S. Jha", "S. Lin", "D. Page", "T. Ristenpart"], "venue": "USENIX Association", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Data mining with differential privacy", "author": ["A. Friedman", "A. Schuster"], "venue": "KDD \u201910,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Twitter sentiment classification using distant supervision", "author": ["A. Go", "R. Bhayani", "L. Huang"], "venue": "CS224N Project Report,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing)", "author": ["I. Guyon", "S. Gunn", "M. Nikravesh", "L.A. Zadeh"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A multiplicative weights mechanism for privacy-preserving data analysis", "author": ["M. Hardt", "G.N. Rothblum"], "venue": "In In FOCS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "A Study of Privacy and Fairness in Sensitive Data Analysis", "author": ["M.A.W. Hardt"], "venue": "PhD thesis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Boosting the accuracy of differentially private histograms through consistency", "author": ["M. Hay", "V. Rastogi", "G. Miklau", "D. Suciu"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A Practical Differentially Private Random Decision Tree Classifier", "author": ["G. Jagannathan"], "venue": "Trans. Data Privacy,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Practical differential privacy via grouping and smoothing", "author": ["G. Kellaris", "S. Papadopoulos"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Top-k frequent itemsets via differentially private fp-trees", "author": ["J. Lee", "C.W. Clifton"], "venue": "KDD \u201914,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "An examination of data confidentiality and disclosure issues related to publication of empirical roc curves", "author": ["G.J. Matthews", "O. Harel"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Mechanism design via differential privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "FOCS \u201907,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "STOC \u201907,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Differentially private support vector machines", "author": ["A.D. Sarwate", "K. Chaudhuri", "C. Monteleoni"], "venue": "CoRR, abs/0912.0071,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Differentially private feature selection via stability arguments, and the robustness of the lasso", "author": ["A.G. Thakurta", "A. Smith"], "venue": "In Conference on Learning Theory,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Differentially private naive bayes classification", "author": ["J. Vaidya", "B. Shafiq", "A. Basu", "Y. Hong"], "venue": "WI-IAT \u201913,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Differentially private network data release via structural inference", "author": ["Q. Xiao", "R. Chen", "K.-L. Tan"], "venue": "KDD \u201914,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Differential privacy via wavelet transforms", "author": ["X. Xiao", "G. Wang", "J. Gehrke"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Differentially private histogram publication", "author": ["J. Xu", "Z. Zhang", "X. Xiao", "Y. Yang", "G. Yu"], "venue": "ICDE \u201912,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Privbayes: Private data release via bayesian networks", "author": ["J. Zhang", "G. Cormode", "C.M. Procopiuc", "D. Srivastava", "X. Xiao"], "venue": "SIGMOD \u201914,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Privgene: Differentially private model fitting using genetic algorithms", "author": ["J. Zhang", "X. Xiao", "Y. Yang", "Z. Zhang", "M. Winslett"], "venue": "In SIGMOD,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "The promise of effectively utilizing such \u2018big-data\u2019 has been realized in part due to the success of off-the-shelf machine learning algorithms, especially supervised classifiers [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "For instance, Fredrikson et al [8] proposed a model inversion attack using which properties (genotype) of individuals in the training dataset can be learnt from linear regression models built on private medical data.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "To address this concern, recent work has focused on developing private classifier training algorithms that ensure a strong notion of privacy called \u01eb-differential privacy [6] \u2013 the classifier output by a differentially private training algorithm does not significantly change due to the insertion or deletion of any one training example.", "startOffset": 171, "endOffset": 174}, {"referenceID": 23, "context": "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 21, "context": "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].", "startOffset": 146, "endOffset": 150}, {"referenceID": 8, "context": "Differentially private algorithms have been developed for training Naive Bayes classifiers [25], logistic regression [2], support vector machines [23] and decision trees [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "In fact, Fredrikson et al [8] also show that differentially private algorithms for the related problem of linear regression result in unacceptable error when applied to real medical datasets.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "In particular, real datasets usually have many features that are of little to no predictive value, and feature selection techniques [5] are used to identify the predictive subset of features.", "startOffset": 132, "endOffset": 135}, {"referenceID": 17, "context": "Recent work [19] has shown that releasing an ROC curve computed on a private test set can leak sensitive information to an adversary with access to certain properties of the test dataset.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "Nevertheless, we show on real datasets and with two differentially private classifiers (Naive Bayes [25] and logistic regression [2]) that private feature selection indeed leads to significant improvement in the classifiers prediction accuracy.", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "Nevertheless, we show on real datasets and with two differentially private classifiers (Naive Bayes [25] and logistic regression [2]) that private feature selection indeed leads to significant improvement in the classifiers prediction accuracy.", "startOffset": 129, "endOffset": 132}, {"referenceID": 25, "context": "Thus we can utilize algorithms from prior work ([27]) to accurately compute the statistics needed for the ROC curve.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "DEFINITION 1 (DIFFERENTIAL PRIVACY [6]).", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "A popular differentially private algorithm is the Laplace mechanism [7] defined as follows:", "startOffset": 68, "endOffset": 71}, {"referenceID": 18, "context": "THEOREM 1 (COMPOSITION [20]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Thus without loss of generality we can define the classifier as outputting a real number p \u2208 [0, 1] which corresponds to the probability of L = 1.", "startOffset": 93, "endOffset": 99}, {"referenceID": 2, "context": "Two examples of such classifiers include the Naive Bayes classifier and logistic regression [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "Feature selection is a dimensionality reduction technique that typically precedes classification, where only a subset of the features F \u2032 \u2282 F in the dataset are retained based on some criterion of how well F \u2032 predicts the label L [11].", "startOffset": 231, "endOffset": 235}, {"referenceID": 10, "context": "Feature selection methods can be categorized as filter, wrapper and embedded methods [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "Purity Index [11]: The purity index for a feature F , denoted by PI(F,D), is defined as:", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "The sensitivity of information gain function has been shown to be O(log n) [9, 29], where n is (an upper bound on) the number of tuples in the dataset.", "startOffset": 75, "endOffset": 82}, {"referenceID": 27, "context": "The sensitivity of information gain function has been shown to be O(log n) [9, 29], where n is (an upper bound on) the number of tuples in the dataset.", "startOffset": 75, "endOffset": 82}, {"referenceID": 15, "context": "This is akin to recent work on data dependent mechanisms for releasing histograms and answering range queries that group categories with similar counts and release a single noisy count for each group [16, 18, 28].", "startOffset": 200, "endOffset": 212}, {"referenceID": 26, "context": "This is akin to recent work on data dependent mechanisms for releasing histograms and answering range queries that group categories with similar counts and release a single noisy count for each group [16, 18, 28].", "startOffset": 200, "endOffset": 212}, {"referenceID": 3, "context": "We use private k-means clustering [4] to cluster the points.", "startOffset": 34, "endOffset": 37}, {"referenceID": 12, "context": "This is a significant improvement over the related sparse vector technique (SVT) first described in Hardt [13], which allows releasing upto a constant c query answers that are above a threshold \u03c4 .", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "technique used in Lee et al [17].", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "For every tuple t \u2208 Dtest, let t[L] \u2208 {0, 1} denote the true label, and p(t) \u2208 [0, 1] denote the prediction returned by some classifier (probability that t[L] = 1).", "startOffset": 79, "endOffset": 85}, {"referenceID": 17, "context": "Recent work [19] has shown that releasing the actual ROC curves on a private test dataset can allow an attacker with prior knowledge to reconstruct the test dataset.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Instead, using strategies like the hierarchical mechanism [14] or Privelet [27] allow answering each onesided range query with no more than O(log n/\u01eb) error.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "Instead, using strategies like the hierarchical mechanism [14] or Privelet [27] allow answering each onesided range query with no more than O(log n/\u01eb) error.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "A simple data-independent strategy for picking the set of thresholds is to choose them uniformly from [0, 1].", "startOffset": 102, "endOffset": 108}, {"referenceID": 0, "context": "More precisely, if n is the cardinality of Dtest, we choose the number of thresholds to be an \u03b1 \u2208 [0, 1] fraction of n, and choose the set of thresholds to be \u0398 = {0, 1 \u230a\u03b1n\u230b , 2 \u230a\u03b1n\u230b , .", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "This strategy works well when the predictions P = {p(t)|t \u2208 Dtest} are uniformly spread out in [0, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 19, "context": "Since median has a high global sensitivity (equal to right if all values are in the range (left, right)), we use the smooth sensitivity framework [21] for computing the noisy median.", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "We leverage the ordering constraint between the TPR and FPR values to boost the accuracy by using the constrained inference method proposed by Hay et al [14].", "startOffset": 153, "endOffset": 157}, {"referenceID": 9, "context": "The TWITTER dataset [10] was collected for the task of sentiment classification.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "The SMS dataset [1] contains 5574 SMS messages associated with spam/ham label.", "startOffset": 16, "endOffset": 19}, {"referenceID": 23, "context": "We choose to evaluate our feature selection algorithm on two state of the art differentially private classifiers \u2013 Naive Bayes [25], and the differentially private ERM implementation of logistic regression [2].", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "We choose to evaluate our feature selection algorithm on two state of the art differentially private classifiers \u2013 Naive Bayes [25], and the differentially private ERM implementation of logistic regression [2].", "startOffset": 206, "endOffset": 209}, {"referenceID": 20, "context": "For non-private logistic regression, we have used the prepackaged Scikit-learn logistic regression classifier [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "We use an implementation of Chaudhuri et al\u2019s [2] differentially private empirical risk minimization (henceforth called ERM) for logistic regression.", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "Figure 3 shows the F1 scores for 4 private feature selection methods using TC \u2013 score perturbation, clustering, PTT and NOISYCUT [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 0, "context": "SMS test set contains 558 data, and each tuple t has a true label t[L] \u2208 {0, 1} as well as a prediction p(t) \u2208 [0, 1] for the label 1.", "startOffset": 111, "endOffset": 117}, {"referenceID": 0, "context": "The FixedSpace line shows the error of \u03b1-FIXEDSPACE, with \u03b1 \u00b7 n thresholds chosen uniformly in [0, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 13, "context": "But in this case, the postprocessing step that enforces monotonicity results in error that has a strong dependence on the number of distinct TPR and FPR values, and not the total number of thresholds (see Theorem 2 [14]).", "startOffset": 215, "endOffset": 219}, {"referenceID": 23, "context": "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.", "startOffset": 122, "endOffset": 129}, {"referenceID": 14, "context": "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.", "startOffset": 122, "endOffset": 129}, {"referenceID": 1, "context": "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.", "startOffset": 151, "endOffset": 158}, {"referenceID": 28, "context": "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.", "startOffset": 151, "endOffset": 158}, {"referenceID": 1, "context": "Previous work has produced differentially private training algorithms for Naive Bayes classification [25], decision trees [9, 15], logistic regression [2, 30] and support vector machines [2] amongst others.", "startOffset": 187, "endOffset": 190}, {"referenceID": 24, "context": "[26] present a generic algorithm for differentially private parameter tuning and model selection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] present an algorithm for model selection again assuming strong stability assumptions about the model training algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Private Threshold Testing: As mentioned before, private threshold testing (PTT) is inspired by the sparse vector technique (SVT) [13] which was first used in the context of the multiplicative weights mechanism [12].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "Private Threshold Testing: As mentioned before, private threshold testing (PTT) is inspired by the sparse vector technique (SVT) [13] which was first used in the context of the multiplicative weights mechanism [12].", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "Lee et al [17] solve the same problem as PTT in the context of frequent itemset mining.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "However, directly releasing the ROC curve may reveal the sensitive information of the input dataset [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "Chaudhuri et al [26] proposes a generic technique for evaluating a classifier on a private test set.", "startOffset": 16, "endOffset": 20}], "year": 2014, "abstractText": "An important use of private data is to build machine learning classifiers. While there is a burgeoning literature on differentially private classification algorithms, we find that they are not practical in real applications due to two reasons. First, existing differentially private classifiers provide poor accuracy on real world datasets. Second, there is no known differentially private algorithm for empirically evaluating the private classifier on a private test dataset. In this paper, we develop differentially private algorithms that mirror real world empirical machine learning workflows. We consider the private classifier training algorithm as a blackbox. We present private algorithms for selecting features that are input to the classifier. Though adding a preprocessing step takes away some of the privacy budget from the actual classification process (thus potentially making it noisier and less accurate), we show that our novel preprocessing techniques signficantly increase classifier accuracy on three real-world datasets. We also present the first private algorithms for empirically constructing receiver operating characteristic (ROC) curves on a private test set.", "creator": "gnuplot 4.6 patchlevel 4"}}}