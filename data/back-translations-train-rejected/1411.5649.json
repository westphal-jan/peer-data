{"id": "1411.5649", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2014", "title": "No-Regret Learnability for Piecewise Linear Losses", "abstract": "In the convex optimization approach to online regret minimization, many methods have been developed to guarantee a $O(\\sqrt{T})$ regret bound for subdifferentiable convex loss functions with bounded subgradients by means of a reduction to bounded linear loss functions. This suggests that the latter tend to be the hardest loss functions to learn against. We investigate this question in a systematic fashion by establishing $\\Omega(\\sqrt{T})$ lower bounds on the minimum achievable regret for a class of piecewise linear loss functions that subsumes the class of bounded linear loss functions. These results hold in a completely adversarial setting. In contrast, we show that the minimum achievable regret can be significantly smaller when the opponent is greedy.", "histories": [["v1", "Thu, 20 Nov 2014 19:38:11 GMT  (50kb,D)", "https://arxiv.org/abs/1411.5649v1", null], ["v2", "Wed, 7 Jan 2015 01:41:37 GMT  (57kb,D)", "http://arxiv.org/abs/1411.5649v2", null], ["v3", "Thu, 8 Jan 2015 08:40:58 GMT  (57kb,D)", "http://arxiv.org/abs/1411.5649v3", null], ["v4", "Fri, 27 May 2016 21:29:52 GMT  (23kb)", "http://arxiv.org/abs/1411.5649v4", null], ["v5", "Sat, 17 Sep 2016 16:14:50 GMT  (26kb)", "http://arxiv.org/abs/1411.5649v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["arthur flajolet", "patrick jaillet"], "accepted": false, "id": "1411.5649"}, "pdf": {"name": "1411.5649.pdf", "metadata": {"source": "CRF", "title": "No-Regret Learnability for Piecewise Linear Losses", "authors": ["Arthur Flajolet"], "emails": ["flajolet@mit.edu", "jaillet@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 1.56 49v5 [cs.LG] 1 7Se p \u221a T) tied to regret about subdifferentiable convex loss functions with limited subgradients by applying a reduction to linear loss functions. This suggests that linear loss functions tend to be the hardest to learn, regardless of the underlying decision spaces. We systematically examine this question by looking at the interplay between the amount of possible motion for both the decision maker and the adversary environment, which allows us to highlight sharp, distinctive behaviors regarding the learnability of piecemeal loss functions. On the one hand, if the decision group of the decision maker is a polyhedron, we set lower limits on regret about a large class of piecemeal linear loss functions with important applications in linear online optimization, repeated zero sum games of Stackberg, online learning scope is not determined by the on-page predictive and on-stage optimization of the other."}, {"heading": "1 Introduction", "text": "The question of whether it relates to a manner in which the player is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in which he is able to behave in a manner in a manner in which he is able to behave in a manner in which he is able to behave in a manner in a manner in which he is able to behave in a manner in a manner in which he is and manner in which he is able to behave in a manner in which he is able to behave in a manner in a manner in which he is and manner in which he is able to behave in a manner in a manner in which he is and manner in which he is able to behave in a manner in a manner in which he is capable of behave in a manner in a manner in which he is and manner in which he is able to behave in a manner in which he is able to behave in a manner in a manner in a manner in which he is able and manner in which he is able to behave in a manner in a manner in which he is able to behave in a manner in which he is able and manner in which he is able to behave in a manner in a manner in which he is able to behave in a manner in a manner in a manner in which he is in a manner in which he is able and manner in which he is able to behave in a manner in a manner in a manner in a manner in which he is able to behave in a manner in which he"}, {"heading": "1.1 Applications", "text": "We list examples of situations where losses of type (2) arise.Online linear optimization is important in analyzing that the loss function is not given in this constellation (1). (1) This constellation is defined by another player (1). (1) This constellation is led by another player (1). (1) This constellation is led by another player (1) who has a different strategy (1). (2) The constellation is led by another player who selects a distribution over the experts (in which case is also a polyhedron) and the opponent reveals a cost framework for each of the experts.In linear optimization, a randomized zero mean is often introduced (1). However, this is only possible if 0 is in the constellation of the constellation of the Z constellation, which is not typically the case of online optimization."}, {"heading": "1.2 Related work", "text": "The first example studied by Abernethy et al. [1] is the quadratic loss, in which the opponent z \u2212 f) = z \u00b7 f + f + f = 0, L2 balls limited by Z and F. [2] The second example examined by Vovk [4] is the quadratic loss situation, in which the opponent z = (z) f + 0, L2 balls limited by Z and F. [3] The second example examined by Vovk [4] is the quadratic loss situation, in which the opponent z = (x). [4] \u00b7 B balls (0, 1) for Cy > 0 (B) denotes the unit L balls (0, 1) denotes the unit L balls (y, x), which is loss."}, {"heading": "2 Lower bounds", "text": "The following results of a canonical choice leading to a distribution variable (Z1, ZT) in ZT.Any choice for a lower distribution variable (Z1, FT) in ZT.A (Z1, FT) in ZT.A (Z1, FT) in ZT.A (Z1, FT) in ZT.A (Z1, FT) in ZT.A (Z2, F) in ZT.A (Z2, F) in ZT.A (Z1, FT) in ZT.A (Z1, FT) in ZT.A (Z2, FT) in ZT.A (ZT) in ZT.A (Z2, F) in ZT.A (Z2, F) in ZT, T in ZF (Z1) in ZT.A (Z1) in ZT.A (ZT) in ZT.A (ZT) in ZT.A (ZT) in ZT.A (ZT) in ZT.A (ZT) in ZT (ZT), T in ZF, T in ZF (Z1) in ZF, ZT in ZT (T) in ZF, ZT in ZT (T) in ZF, ZT in ZT (ZT (ZT) in ZT, ZT in ZT (ZT) in ZT, ZT (ZT in ZT, ZT (ZT) in ZT in ZT, ZT (ZT) in ZT in ZT.A (ZT (ZT) in ZT.A (ZT) in ZT.A (ZT.A (ZT) in ZT in ZT.A (ZT) in ZT.A (ZT (ZT) in ZT.A (ZT) in ZT.A (ZT (ZT) in ZT.A (ZT in ZT), ZT in ZT (ZT in ZT (ZT) in ZT in ZT.A (ZT, FT) in ZT in ZT (ZT in ZT (ZT, FT) in ZT (ZT in ZT in ZT.A (ZT, FT) in ZT (Z"}, {"heading": "RT (\u2113,Z,F) = 0 if and only if the game is trivial.", "text": "The following results show that in most cases of interest, we can drastically limit the power of the opponent while we do not yet know the nature of the game (1) (2). (3) Answering the question (1) that we are able to focus on the case in which there is a limited number of players who have opted for a large class of piecewise linear losses is not trivial. (2) We are now ready to present the most important results of this section. (2) To present the best of our knowledge, we present these results as the first systematic solution (2). (3) Answering the question about a large class of piecewise linear losses is not trivial. (2) Theorem 2 is a polyhedron. (1) In each of the following cases: 1. Z has limited cardinality, 2. F) the game is for a large class of piecewise linear losses. (2) Theorem is a polyhedron."}, {"heading": "3 Upper bounds", "text": "It is not only the manner in which this allegation requires further investigation, but also the manner in which the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the answer to the question, whether the answer to the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the answer to the question, whether the answer to the question, whether the answer to the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the answer to the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the answer to the question, whether the question, whether the answer to the question, whether the answer to the question, whether the answer to the question, whether the answer to the answer to the answer to the question, whether the answer to the question of the question, whether the answer to the question,"}, {"heading": "Acknowledgments", "text": "The authors thank Alexander Rakhlin for his valuable contribution and in particular for drawing our attention to the possibility of having limitations of regret in the linear setting o (\u221a T)."}, {"heading": "4 Appendix: proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Proof of Theorem 1", "text": "The assumptions of theorem 1 in [2] are satisfactory for the game (, Z, F) under assumption 1 and the fact that each loss function of form (2) is such that (z, \u00b7) is continuous for each z Z."}, {"heading": "4.2 Proof of Lemma 1", "text": "The proof follows from the repeated use of the \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z.Z\".Z.Z \".Z.Z\".Z.Z.Z \".Z.Z.Z.Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z.Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z\".Z \".Z"}, {"heading": "4.3 Proof of Lemma 2", "text": "We follow the analysis of theorem 19 of [2]. If we use theorem 1 with p as the distribution of the i.i.d. copies of Z, we get the lower limit: RT \u2265 T inf, F E [(Zt, f)] \u2212 E [inf, FT t = 1 (Zt, f)] \u2265 T sup f, f1, f2} E [(Zt, f)] \u2212 E [inf f f f f f, f1) {f1, f2} T t = 1 (Zt, f)] \u2265 E [max {T, t = 1E [(Zt, f1)] \u2212 (Zt, f1) \u2212 (Zt), T t = 1E [(Zt, f2)] \u2212 E [max, f2] (Zt, f), t (Zt, f1) \u2212 (Zt, f2), using the fact that the inf, f, f, f, f, f, f, f, and f, (t)."}, {"heading": "4.4 Proof of Lemma 3", "text": "The fact that RT \u2265 0 is occupied in Lemma 3 of [2] and follows from Theorem 1 by holding the Zt's to be deterministic and all equal (z-Z). If the game is trivial, then RT = 0, because this value is reached for f1, \u00b7 \u00b7 \u00b7, fT = f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f"}, {"heading": "4.5 Proof of Lemma 4", "text": "Suppose in contradiction that we cannot find such a finite subset. Since Z is compact, it is also divisible, so it contains a countable dense subset {zn | n \u00b2 N}. According to the assumption, the game (, {zk | k \u2264 n}, F) must be trivial for each n \u00b2 F, i.e. there is fn \u00b2 F so that: (zk, fn) \u2264 min f \u00b2 F (z, f), k \u00b2 n.Since F is compact, we can find a sequence of (fn) n \u00b2 N so that fn \u00b2 F. Without loss of universality, we shall continue to refer to this sequence as (fn) n \u00b2 n \u00b2 N. If the boundary n \u00b2 in the above is fixed for all k \u00b2 N results in: (zk, f \u00b2), we can find a sequence of (fn) n \u00b2 N so that fn \u00b2 F. Without loss of universality, we shall continue to refer to this sequence as (z), (z \u00b2, f \u00b7 f \u00b2, c, c, c \u00b2, c, c \u00b2, c (c) c (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c (c, c, c, f, c, c, c, c, c, c, c, c, f, c, c, c, c, f, c, c, c, c, c, c, c,"}, {"heading": "4.6 Proof of Theorem 2", "text": "Without loss of universality, we can assume that the game is not trivial and that X (z) is finite for all z (z), otherwise, if X (z) is a polyhederon, the maximum in (2) must be reached at an extreme point of X (z) (z) (there are endless such points for each z). Furthermore, we can also assume that Z is discrete from Lemma 4, since we must borrow the notations of Lemma 4, we have: RT (z, F) more than RT (, Z, F). Write Z = {zn \u2264 N} and denote by p0 the uniform distribution of Z, i.e. p0 = 1 N zn, where it is the Dirac distribution supported by zn. We can assume that there is a single equivalence class in argminf F 0."}, {"heading": "4.7 Alternative proof of Theorem 2 by exhibiting an equalizing strategy when \u2113 is linear", "text": "With Lemma 1, we can assume without loss of generality that Z is a convex problem. \u00b7 If this is a linear approach, we can also guarantee that for all x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4.8 Proof of Theorem 3", "text": "Uncomplicated from Theorem 2, because common is continuous."}, {"heading": "4.9 Proof of Lemma 5", "text": "Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "4.10 Proof of Theorem 4", "text": "A common inequality based on the FTL strategy is: rT (zt) t = 1, \u00b7 \u00b7 (ft) t = 1, \u00b7 \u00b7, (ft) t = 1, \u00b7 \u00b7 \u00b7, T) \u2264 T = 1 zTt (ft \u2212 ft + 1), We use sensitivity analysis to control this last quantity. Specifically, we show that the mapping analysis: z \u2192 argminf, F (f) = 0 zTf is Lipschitz on Z. Use of this property: rT (zt) t = 1, \u00b7, T, (ft) t = 1, \u00b7 \u00b7, T) \u2264 T (t) \u2264 T, T = 1 zTf is Lipschitz on Z."}, {"heading": "4.11 Proof of Lemma 6", "text": "With Lemma 1, we can assume that Z is convex. Since Z is compact and convex, and since 0 / 2 Z, we can strictly separate 0 from Z, and z \u00b2 6 = 0 so that Z \u00b2 B2 (z \u00b2, x \u00b2 z \u00b2) is \u03b1 < 1. We can assume that the number of random variables (Z1, \u00b7, ZT) is so high that the number of random variables (Z1, \u00b7, ZT) is so high that the number of random variables (Z1, Z1) is O (1). Write Zt = z = z and E [Wt] so high that the number of random variables (Z1, \u00b7, ZT) t is so high that the number of random variables (Z1, Zt) is so high that the number of random variables (Zt) is so high."}, {"heading": "4.12 Proof of Theorem 5", "text": "Since Z is not empty, we can z = 0 and \u03b1 = 1 = 1 = 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 T 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 2 \u00b7 T."}, {"heading": "4.13 Proof of Lemma 7", "text": "We start with the same inequality: rT = q q = q = 1, \u00b7 \u00b7, T, (ft) t = 1, \u00b7 \u00b7, T \u2212 2, \u2212, T \u2212, T \u2212, T \u2212, T \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, i.e. there is c > 0, so that: z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212 \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z \u2212, Z, Z \u2212, Z \u2212, Z, Z \u2212, Z, Z \u2212, Z, Z \u2212, Z, Z, Z \u2212, Z, Z, Z, Z, Z, Z, Z,, Z,,, Z,, Z, Z,,,,, Z,,,, Z,,, Z,,,,,, Z,,, Z,,,,,, Z,,,,,,,,,,,,, Z,,,,,,,,,,, Z,,,,,,,,, Z,,,,,,,,,,, Z,,,,,,,,,,,,, Z,,,,"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>In the convex optimization approach to online regret minimization, many methods<lb>have been developed to guarantee a O(<lb>\u221a<lb>T ) bound on regret for subdifferentiable<lb>convex loss functions with bounded subgradients, by using a reduction to linear<lb>loss functions. This suggests that linear loss functions tend to be the hardest ones<lb>to learn against, regardless of the underlying decision spaces. We investigate this<lb>question in a systematic fashion looking at the interplay between the set of pos-<lb>sible moves for both the decision maker and the adversarial environment. This<lb>allows us to highlight sharp distinctive behaviors about the learnability of piece-<lb>wise linear loss functions. On the one hand, when the decision set of the deci-<lb>sion maker is a polyhedron, we establish \u03a9(<lb>\u221a<lb>T ) lower bounds on regret for a<lb>large class of piecewise linear loss functions with important applications in on-<lb>line linear optimization, repeated zero-sum Stackelberg games, online prediction<lb>with side information, and online two-stage optimization. On the other hand, we<lb>exhibit o(<lb>\u221a<lb>T ) learning rates, achieved by the Follow-The-Leader algorithm, in<lb>online linear optimization when the boundary of the decision maker\u2019s decision set<lb>is curved and when 0 does not lie in the convex hull of the environment\u2019s decision<lb>set. Hence, the curvature of the decision maker\u2019s decision set is a determining<lb>factor for the optimal learning rate. These results hold in a completely adversarial<lb>setting.<lb>", "creator": "LaTeX with hyperref package"}}}