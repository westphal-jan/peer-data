{"id": "1411.5654", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2014", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "abstract": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-of-the-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over $19.8\\%$ of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.", "histories": [["v1", "Thu, 20 Nov 2014 19:50:27 GMT  (3923kb,D)", "http://arxiv.org/abs/1411.5654v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["xinlei chen", "c lawrence zitnick"], "accepted": false, "id": "1411.5654"}, "pdf": {"name": "1411.5654.pdf", "metadata": {"source": "CRF", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "authors": ["Xinlei Chen", "C. Lawrence Zitnick"], "emails": ["xinleic@cs.cmu.edu", "larryz@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is often this mental image that is forgotten long after the exact sentence [29, 21]. What role should visual memory play in image search and how can it come to an image description? Various approaches have been used to open up the projection screen for images and their descriptions."}, {"heading": "2. Related work", "text": "The task of building a visual memory lies at the heart of two long-standing AI problems: the grounding of natural language symbols in the physical world and the semantic understanding of an image. Learning to map between image cartridges and individual bookmarks remains a popular topic in the computer world. However, there is a growing interest in using entire sentence descriptions in conjunction with pixels to find common embeddings. [13] Viewing corresponding texts and images is a natural option for discovering the shared spaces."}, {"heading": "3. Approach", "text": "In this section, we describe our approach using recurrent neural networks. Our objectives are twofold: firstly, to be able to generate sentences that have a set of visual observations or characteristics; secondly, to calculate the probability that a word wt will be generated at a given time t, given the amount of previously generated words Wt \u2212 1 = w1,.., wt \u2212 1 and the visual characteristics observed; and, secondly, to activate the ability to calculate the probability of visual characteristics V when a sentence of spoken or read words Wt \u2212 1. As we will show later, the latent variables U play the crucial role of generating visual representations of the scene or performing image searches. To accomplish these two tasks, we present a set of latent variables Ut \u2212 1 that encode the visual interpretation of previously generated or read words Wt \u2212 1."}, {"heading": "3.1. Model structure", "text": "In fact, it is a way in which one sees oneself in a position to outdo oneself. (...) It is not as if one is able to outdo oneself. (...) It is not as if one is able to outdo oneself. (...) It is not as if one is able to outdo oneself. (...) It is not as if one is able to outdo oneself. (...) It is as if one is able to outdo oneself. (...) It is not as if one is able to outdo oneself. (...) It is not as if one is able to outdo oneself. (...) It is not as if one is able to outdo oneself. (...)"}, {"heading": "3.2. Implementation details", "text": "In this section we describe the details of our language model and how we learn our network."}, {"heading": "3.3. Language Model", "text": "Our language model typically consists of 3,000 to 20,000 words. Although each word can be predicted independently of each other, this approach is mathematically expensive. Instead, we adopted the idea of word classification [24] and factored the distribution into a product of two terms: P (wt | \u00b7) = P (ct | \u00b7) \u00b7 P (wt | ct, \u00b7). (2) P (wt | \u00b7) is the probability of the word, P (ct | \u00b7) is the probability of the class. The class name of the word is computed unsupervised and groups together words of similar frequency. In general, this approach speeds up the learning process considerably, without great loss of perplexity. The predicted word similarities are calculated using the standard Soft Max function. After each epoch, the perplexity is evaluated on the basis of a separate validation set and learning is reduced (halved in our experiments) if the prediction puplixity does not decrease. To further reduce the confusion of the word expo1, we will expox the results from each."}, {"heading": "3.4. Learning", "text": "For learning, we use the Backpropagation Through Time (BPTT) algorithm [35]. Specifically, the network is unrolled for multiple words and the default backpropagation is applied. Note that we reset the model after an end-of-sentence (EOS) occurs, so that predictions do not exceed sentence limits. As shown in [24], we use online learning for the weights from the recurring units to the output words. The weights for the rest of the network use a one-time update per sentence. Activations for all units are calculated using the sigmoid function \u03c3 (z) = 1 / (1 + Exp (\u2212 z)) with clipping, with the exception of the word predictions that use Soft-Max. We found that reflected linear units (ReLUs) [18] with unlimited activations are numerically unstable and usually \"inflated\" when used in recursive networks."}, {"heading": "4. Results", "text": "In this section, we evaluate the effectiveness of our bidirectional RNN model for multiple tasks. We begin by describing the data sets used for training and testing, followed by our baselines. Our initial evaluations measure the ability of our model to create new image descriptions. As our model is bidirectional, we evaluate its performance in both set and image query. Additional results can be found in the supplementary material."}, {"heading": "4.1. Datasets", "text": "For evaluation, we conduct experiments with multiple standard datasets used for sentence generation and retrieval of sentence images: PASCAL 1K [31] The dataset contains a subset of images from the PASCAL VOC challenge. For each of the 20 categories, it has a random sample of 50 images with 5 descriptions provided by Amazon's Mechanical Turk (AMT). Flickr 8K and 30K [31] These datasets consist of 8,000 and 31,783 images collected by Flickr, respectively. Most images show people participating in various activities. Each image is also paired with 5 sets. These datasets have standard training, validation and test splits.MS COCO [22] The Microsoft COCO dataset contains 82,783 training images and 40,504 validation images, each with 5 human-generated descriptions. The images are collected by Flickr, by searching for general categories of objects and imitation objects."}, {"heading": "4.2. RNN Baselines", "text": "In order to gain insights into the various components of our model, we compared our final model with three RNN baselines. For a fair comparison, the initialization of the random seeds was fixed for all experiments. Hidden levels s and u-sizes are fixed at 100. We tried to increase the number of hidden units, but the results did not improve. For small data sets, more units can lead to an overmatch. RNN-based language model (RNN) This is the basic language model RNN developed by [24], which avoids any input errors of visual properties. RNN with image functions (RNN + IF) This is an RNN model with image functions feeding into the hidden layer, inspired by [27]. As described in section 3v, is associated only with s and not with w."}, {"heading": "4.3. Sentence generation", "text": "This year it has come to the point where we will be able to put ourselves at the top, \"he said in an interview with the Deutsche Presse-Agentur.\" We have to put ourselves at the top, \"he said.\" We have pushed ourselves to the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "4.4. Bi-Directional Retrieval", "text": "rE \"s rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "5. Discussion", "text": "Captions describe both the objects in the image and their relationships. One area of future work is exploring the sequential exploration of an image and its relationship to image descriptions. Many words correspond to spatial relationships that our current model finds difficult to discern. As the most recent essay [16] shows, better localization of features in the image can greatly improve the performance of retrieval tasks, and a similar improvement could be seen in the task of creating descriptions.Finally, we describe the first bidirectional model that is capable of generating both new image descriptions and visual traits. Unlike many previous approaches that used RNNs, our model is capable of learning long-term interactions. This is achieved by using a recurring visual memory that learns to reconstruct visual features when new words are read or generated.We demonstrate state-of-the-art results in the task of sentence generation, image repetition and sentence repetition on numerous data sets."}, {"heading": "6. Acknowledgements", "text": "We thank Hao Fang, Saurabh Gupta, Meg Mitchell, Xiaodong He, Geoff Zweig, John Platt and Piotr Dollar for their thoughtful and insightful discussions in writing this paper."}], "references": [{"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, 5(2):157\u2013166", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Comparing automatic evaluation measures for image description", "author": ["D. Elliott", "F. Keller"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, 14(2):179\u2013211", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, pages 15\u201329. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121\u20132129", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Y. Gong", "L. Wang", "M. Hodosh", "J. Hockenmaier", "S. Lazebnik"], "venue": "ECCV, pages 529\u2013545", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Choosing linguistics over vision to describe images", "author": ["A. Gupta", "Y. Verma", "C. Jawahar"], "venue": "AAAI", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. J. Artif. Intell. Res.(JAIR), 47:853\u2013899", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagery in sentence comprehension: an fmri study", "author": ["M.A. Just", "S.D. Newman", "T.A. Keller", "A. McEleney", "P.A. Carpenter"], "venue": "Neuroimage, 21(1):112\u2013124", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1406.5679", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR, pages 1601\u20131608. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Words versus objects: Comparison of free verbal recall", "author": ["L.R. Lieberman", "J.T. Culpepper"], "venue": "Psychological Reports, 17(3):983\u2013988", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1965}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1410.1090", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "International Conference on Learning Representations: Workshops Track", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. Cernocky"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on, pages 196\u2013201. IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT, pages 234\u2013239", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Midge: Generating image descriptions from  computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "EACL, pages 747\u2013756. Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Why are pictures easier to recall than words? Psychonomic Science", "author": ["A. Paivio", "T.B. Rogers", "P.C. Smythe"], "venue": "11(4):137\u2013138", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1968}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Collecting image annotations using Amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "NAACL HLT Workshop Creating Speech and Language Data with Amazon\u2019s Mechanical Turk", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q. Le", "C. Manning", "A. Ng"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Experimental analysis of the real-time recurrent learning algorithm", "author": ["R.J. Williams", "D. Zipser"], "venue": "Connection Science, 1(1):87\u2013111", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1989}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daum\u00e9 III", "Y. Aloimonos"], "venue": "EMNLP", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "I2T: Image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.-C. Zhu"], "venue": "Proceedings of the IEEE, 98(8):1485\u20131508", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 3009\u20133016. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "\u201d The creation of a mental image may play a significant role in sentence comprehension in humans [15].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "In fact, it is often this mental image that is remembered long after the exact sentence is forgotten [29, 21].", "startOffset": 101, "endOffset": 109}, {"referenceID": 20, "context": "In fact, it is often this mental image that is remembered long after the exact sentence is forgotten [29, 21].", "startOffset": 101, "endOffset": 109}, {"referenceID": 12, "context": "Recently, several papers have explored learning joint feature spaces for images and their descriptions [13, 32, 16].", "startOffset": 103, "endOffset": 115}, {"referenceID": 30, "context": "Recently, several papers have explored learning joint feature spaces for images and their descriptions [13, 32, 16].", "startOffset": 103, "endOffset": 115}, {"referenceID": 15, "context": "Recently, several papers have explored learning joint feature spaces for images and their descriptions [13, 32, 16].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) [13], recursive neural networks [32], or deep neural networks [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) [13], recursive neural networks [32], or deep neural networks [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 15, "context": "Various approaches were used to learn the projection, including Kernel Canonical Correlation Analysis (KCCA) [13], recursive neural networks [32], or deep neural networks [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 5, "context": "We accomplish this using Recurrent Neural Networks (RNNs) [6, 24, 27].", "startOffset": 58, "endOffset": 69}, {"referenceID": 25, "context": "We accomplish this using Recurrent Neural Networks (RNNs) [6, 24, 27].", "startOffset": 58, "endOffset": 69}, {"referenceID": 2, "context": "For instance RNN language models often find difficultly in learning long distance relations [3, 24] without specialized gating units [12].", "startOffset": 92, "endOffset": 99}, {"referenceID": 11, "context": "For instance RNN language models often find difficultly in learning long distance relations [3, 24] without specialized gating units [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "These include the PASCAL sentence dataset [31], Flickr 8K [31], Flickr 30K [31], and the Microsoft COCO dataset [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 28, "context": "When generating novel image descriptions, we demonstrate state-of-the-art results as measured by both BLEU [30] and METEOR [1] on PASCAL 1K.", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "When generating novel image descriptions, we demonstrate state-of-the-art results as measured by both BLEU [30] and METEOR [1] on PASCAL 1K.", "startOffset": 123, "endOffset": 126}, {"referenceID": 17, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 7, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 8, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 117, "endOffset": 127}, {"referenceID": 12, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 30, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 15, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 9, "context": "Whereas learning the mapping between image patches and single text labels remains a popular topic in computer vision [18, 8, 9], there is a growing interest in using entire sentence descriptions together with pixels to learn joint embeddings [13, 32, 16, 10].", "startOffset": 242, "endOffset": 258}, {"referenceID": 12, "context": "Viewing corresponding text and images as correlated, KCCA [13] is a natural option to discover the shared features spaces.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 193, "endOffset": 201}, {"referenceID": 9, "context": "Recent papers seek better objective functions that directly optimize the ranking [13], or directly adopts pre-trained representations [32] to simplify the learning, or a combination of the two [16, 10].", "startOffset": 193, "endOffset": 201}, {"referenceID": 6, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 34, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 18, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 35, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 26, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 10, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 19, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 16, "context": "Numerous papers have explored the area of generating novel image descriptions [7, 36, 19, 37, 28, 11, 20, 17].", "startOffset": 78, "endOffset": 109}, {"referenceID": 34, "context": "These papers use various approaches to generate text, such as using pre-trained object detectors with templatebased sentence generation [36, 7, 19].", "startOffset": 136, "endOffset": 147}, {"referenceID": 6, "context": "These papers use various approaches to generate text, such as using pre-trained object detectors with templatebased sentence generation [36, 7, 19].", "startOffset": 136, "endOffset": 147}, {"referenceID": 18, "context": "These papers use various approaches to generate text, such as using pre-trained object detectors with templatebased sentence generation [36, 7, 19].", "startOffset": 136, "endOffset": 147}, {"referenceID": 19, "context": "Retrieved sentences may be combined to form novel descriptions [20].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "Recently, purely statistical models have been used to generate sentences based on sampling [17] or recurrent neural networks [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "Recently, purely statistical models have been used to generate sentences based on sampling [17] or recurrent neural networks [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 22, "context": "While [23] also uses a RNN, their model is significantly different from our model.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "Specifically their RNN does not attempt to reconstruct the visual features, and is more similar to the contextual RNN of [27].", "startOffset": 121, "endOffset": 125}, {"referenceID": 36, "context": "[38] uses abstract clip art images to learn the visual interpretation of sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "There are numerous papers using recurrent neural networks for language modeling [2, 24, 27, 17].", "startOffset": 80, "endOffset": 95}, {"referenceID": 25, "context": "There are numerous papers using recurrent neural networks for language modeling [2, 24, 27, 17].", "startOffset": 80, "endOffset": 95}, {"referenceID": 16, "context": "There are numerous papers using recurrent neural networks for language modeling [2, 24, 27, 17].", "startOffset": 80, "endOffset": 95}, {"referenceID": 1, "context": "We build most directly on top of [2, 24, 27] that use RNNs to learn word context.", "startOffset": 33, "endOffset": 44}, {"referenceID": 25, "context": "We build most directly on top of [2, 24, 27] that use RNNs to learn word context.", "startOffset": 33, "endOffset": 44}, {"referenceID": 25, "context": "Several models use other sources of contextual information to help inform the language model [27, 17].", "startOffset": 93, "endOffset": 101}, {"referenceID": 16, "context": "Several models use other sources of contextual information to help inform the language model [27, 17].", "startOffset": 93, "endOffset": 101}, {"referenceID": 2, "context": "Despite its success, RNNs still have difficulty capturing long-range relationships in sequential modeling [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": "One solution is Long Short-Term Memory (LSTM) networks [12, 33, 17], which use \u201cgates\u201d to control gradient back-propagation explicitly and allow for the learning of long-term interactions.", "startOffset": 55, "endOffset": 67}, {"referenceID": 31, "context": "One solution is Long Short-Term Memory (LSTM) networks [12, 33, 17], which use \u201cgates\u201d to control gradient back-propagation explicitly and allow for the learning of long-term interactions.", "startOffset": 55, "endOffset": 67}, {"referenceID": 16, "context": "One solution is Long Short-Term Memory (LSTM) networks [12, 33, 17], which use \u201cgates\u201d to control gradient back-propagation explicitly and allow for the learning of long-term interactions.", "startOffset": 55, "endOffset": 67}, {"referenceID": 25, "context": "Note that in previous papers [27, 23] the objective was only to compute P (wt|V,Wt\u22121) and not P (V |Wt\u22121).", "startOffset": 29, "endOffset": 37}, {"referenceID": 22, "context": "Note that in previous papers [27, 23] the objective was only to compute P (wt|V,Wt\u22121) and not P (V |Wt\u22121).", "startOffset": 29, "endOffset": 37}, {"referenceID": 25, "context": "Our recurrent neural network model structure builds on the prior models proposed by [24, 27].", "startOffset": 84, "endOffset": 92}, {"referenceID": 2, "context": "However, s typically only models shortrange interactions due to the problem of vanishing gradients [3, 24].", "startOffset": 99, "endOffset": 106}, {"referenceID": 23, "context": "This simple, yet effective language model was shown to provide a useful continuous word embedding for a variety of applications [25].", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "[27] added an input layer v to the RNN shown by the white box in Figure 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "This layer may represent a variety of information, such as topic models or parts of speech [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "Note that unlike [27], it is not necessary to directly connect v to w\u0303, since v is static for our application.", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": "In [27] v represented dynamic information such as parts of speech for which w\u0303 needed direct access.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "Alternatively, if the hidden units s were connected directly to u, this property would be lost and the network would act as a normal autoencoder [34].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "In order to further reduce the perplexity, we combine the RNN model\u2019s output with the output from a Maximum Entropy language model [26], simultaneously learned from the training corpus.", "startOffset": 131, "endOffset": 135}, {"referenceID": 33, "context": "For learning we use the Backpropagation Through Time (BPTT) algorithm [35].", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We found that Rectified Linear Units (ReLUs) [18] with unbounded activations were numerically unstable and commonly \u201cblew up\u201d when used in recurrent networks.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "We used the open source RNN code of [24] and the Caffe framework [14] to implement our model.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "However, deep convolution neural networks require large amounts of data to train on, but the largest sentence-image dataset has only\u223c80K images [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "Therefore, instead of training from scratch, we choose to finetune from the pre-trained 1000-class ImageNet model [4] to avoid potential over-fitting.", "startOffset": 114, "endOffset": 117}, {"referenceID": 29, "context": "PASCAL 1K [31] The dataset contains a subset of images from the PASCAL VOC challenge.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Flickr 8K and 30K [31] These datasets consists of 8,000 and 31,783 images collected from Flickr respectively.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "MS COCO [22] The Microsoft COCO dataset contains 82,783 training images and 40,504 validation images each with\u223c5 human generated descriptions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "Midge [28] - 2.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "80 Baby Talk [19] - 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "Results are measured using perplexity (PPL), BLEU (%) [30] and METEOR (METR, %) [1].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Results are measured using perplexity (PPL), BLEU (%) [30] and METEOR (METR, %) [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 26, "context": "When available results for Midge [28] and BabyTalk [19] are provided.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "When available results for Midge [28] and BabyTalk [19] are provided.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "DeViSE [8] 17.", "startOffset": 7, "endOffset": 10}, {"referenceID": 30, "context": "5 SDT-RNN [32] 25.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "0 DeepFE [16] 39.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "The protocol of [32] was followed.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "RNN with Image Features (RNN+IF) This is an RNN model with image features feeding into the hidden layer inspired by [27].", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "For the visual features v we used the 4096D 7th Layer output of BVLC reference Net [14] after ReLUs.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "This network is trained on the ImageNet 1000-way classification task [4].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "RNN with Image Features Fine-Tuned (RNN+FT) This model has the same architecture as RNN+IF, but the error is back-propagated to the Convolution Neural Network [9].", "startOffset": 159, "endOffset": 162}, {"referenceID": 26, "context": "We experiment on all the image-sentence datasets described previously and compare to the RNN baselines and other previous papers [28, 19].", "startOffset": 129, "endOffset": 137}, {"referenceID": 18, "context": "We experiment on all the image-sentence datasets described previously and compare to the RNN baselines and other previous papers [28, 19].", "startOffset": 129, "endOffset": 137}, {"referenceID": 30, "context": "[32] 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "0 29 DeViSE [8] 4.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "6 29 DeepFE [16] 12.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "5 15 DeepFE+DECAF [16] 5.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "[13] 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "M-RNN [23] 14.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "The protocols of [32], [13] and [23] are used respectively in each row.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "The protocols of [32], [13] and [23] are used respectively in each row.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "The protocols of [32], [13] and [23] are used respectively in each row.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "DeViSE [8] 4.", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "7 25 DeepFE+FT [16] 16.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "M-RNN [23] 18.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "The protocols of [8] and [23] are used respectively in each row.", "startOffset": 17, "endOffset": 20}, {"referenceID": 22, "context": "The protocols of [8] and [23] are used respectively in each row.", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "We choose three automatic metrics for evaluating the quality of the generated sentences, perplexity, BLEU [30] and METEOR [1].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "We choose three automatic metrics for evaluating the quality of the generated sentences, perplexity, BLEU [30] and METEOR [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 26, "context": "Our approach significantly improves over both Midge [28] and BabyTalk [19] on the PASCAL 1K dataset as measured by BLEU and METEOR.", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "Our approach significantly improves over both Midge [28] and BabyTalk [19] on the PASCAL 1K dataset as measured by BLEU and METEOR.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "It is known that automatic measures are only roughly correlated with human judgment [5], so it is also important to evaluate the generated sentences using human studies.", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "Midge [28] (green) and BabyTalk [19] (blue).", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "Midge [28] (green) and BabyTalk [19] (blue).", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "Since shorter sentences naturally have higher probability of being generated, we followed [23] and normalized the probability by dividing it with the total probability summed over the entire retrieval set.", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 137, "endOffset": 140}, {"referenceID": 22, "context": "We report three scores for Flickr 8K corresponding to the methodologies proposed by [32], [13] and [23] respectively, and for Flickr 30K [8] and [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "As shown in Tables 3 and 4, for Flickr 8K and 30K our approach achieves comparable or better results than all methods except for the recently proposed DeepFE [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 15, "context": "As demonstrated by the recent paper of [16] better feature localization in the image can greatly improve the performance", "startOffset": 39, "endOffset": 43}], "year": 2014, "abstractText": "In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. We propose learning this mapping using a recurrent neural network. Unlike previous approaches that map both sentences and images to a common embedding, we enable the generation of novel sentences given an image. Using the same model, we can also reconstruct the visual features associated with an image given its visual description. We use a novel recurrent visual memory that automatically learns to remember long-term visual concepts to aid in both sentence generation and visual feature reconstruction. We evaluate our approach on several tasks. These include sentence generation, sentence retrieval and image retrieval. State-ofthe-art results are shown for the task of generating novel image descriptions. When compared to human generated captions, our automatically generated captions are preferred by humans over 19.8% of the time. Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features.", "creator": "LaTeX with hyperref package"}}}