{"id": "1411.5899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2014", "title": "Falling Rule Lists", "abstract": "Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.", "histories": [["v1", "Fri, 21 Nov 2014 15:01:56 GMT  (200kb,D)", "http://arxiv.org/abs/1411.5899v1", null], ["v2", "Tue, 27 Jan 2015 04:41:38 GMT  (293kb,D)", "http://arxiv.org/abs/1411.5899v2", "Accepted at AISTATS 2015. Contains new comparisons to ILP methods. in Proceedings of AISTATS 2015. JMLR: W&amp;CP 38"], ["v3", "Sun, 1 Feb 2015 06:46:47 GMT  (293kb,D)", "http://arxiv.org/abs/1411.5899v3", "Accepted at AISTATS 2015. Contains number of rules mined, running times. in Proceedings of AISTATS 2015. JMLR: W&amp;CP 38"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["fulton wang", "cynthia rudin"], "accepted": false, "id": "1411.5899"}, "pdf": {"name": "1411.5899.pdf", "metadata": {"source": "META", "title": "Falling Rule Lists", "authors": ["Fulton Wang", "Cynthia Rudin"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2. Falling Rule Lists Model", "text": "We will look at the binary classification, where the goal is to learn a distribution p (Y | x) where Y is binary. For example, Y could indicate the presence of a disease, and x would be the characteristic of a patient. We will present this state distribution Y | x as a decision list, which is an ordered list of IF... THEN... rules. We will need a special structure for this decision list: that the probability of Y = 1 associated with each rule decreases when moving the decision list down. We will use a Bayesian approach to characterize a posterior distribution vis-\u00e0-vis falling rule lists from the data D = {(xn, yn)} n = 1,..., N, where N is the size of the training data, xn \u00b2 X, the space for patient characteristics and yn \u00b2 {0, 1}. We will present a falling rule list with a series of parameters that approximate the previous pIS (\u00b7; H) and the probability (pyn} of {Carlo}, {| xlin} (uncompromising) and Monte."}, {"heading": "2.1. Parameters of Model", "text": "A falling rule list is parameterized by the following objects: L-Z + (size of the list) (1) cl (\u00b7) and BX (\u00b7), for l = 0,..., L-1 (IF clauses) (2) rl-R, for l = 0,..., L (risk scores) (3) like thatrl + 1 \u2264 rl for l = 0,..., L-1 (monoton) (4), where BX (\u00b7) is the previously known space of Boolean functions in the patient feature room X. BX (\u00b7) is the space of possible IF clauses; cl (x) will be 1 if x meets a given set of conditions. In this work, we do not assume that L, the size of the decision list, is known in advance. The value of rl is fed into the logistic function to generate a risk probability between 0 and 1."}, {"heading": "2.2. Likelihood", "text": "Given these parameters, the probability is given as follows: L, leave Z (x; {cl (\u00b7)} L \u2212 1l = 0): X \u2192 {0,.., L} is the mapping of attribute x to the rule of length L to which it \"belongs\": Z (x; {cl (\u00b7)} L \u2212 1l = 0) = {L if cl (x) = 0 for l = 0,..., L \u2212 1 min (l: cl (x) = 1, l = 0,.., L \u2212 1) otherwise. (5) Then the probability is isyn | L, {cl (\u00b7)} L \u2212 1l = 0, {rl} L = 0,; xn instead of Bernoulli (logistically (rl)))), where (6) zn = Z (xn; {cl ()} L \u2212 1l = 0). (7)"}, {"heading": "2.3. Prior", "text": "Here we describe the previous ones using the parameters L, {cl} L \u2212 1l = 0, {rl} Ll = 0. We will provide a repair parameterization that forces monotonicity constraints, and finally give a generative model for the parameters. As discussed above, to help with the calculation, we will place a positive previous probability of {cl} L \u2212 1l = 0 only via lists consisting of Boolean clauses B returned by a frequent itemset-mining algorithm, with c (\u00b7) B c (\u00b7): X \u2192 {0, 1}. For this particular work, we used FPGrowth (Borgelt, 2005), whose input is a binary dataset in which each x is a Boolean vector and whose output is a series of subsets of the characteristics of the dataset. For example, x2 could indicate the presence of diabetes and x15 the presence of a blood pressure, each of which is a subset of the data set."}, {"heading": "2.3.1. Reparameterization", "text": "To ensure the monotonicity constraints that rl \u2265 rl \u2212 1 for l = 1. L in the back scale, we select the values rl to arrive at a log scale of products of real numbers that must be greater than 1. That is, conditioned on L, we letrl = log (vl) for l = 0,..., L (8) vl = K\u0440 L \u2212 1 l \u00b2 = l \u00b2 for l = 0,..., L \u2212 1 (9) vL = K (10) and demand that l \u00b2 1 for l = 0,..., L \u2212 1 (11) K \u2265 0, (12) so that rL, the risk value associated with the default rule, is logK. The previous value is set to {.l} L \u2212 1l = 0 and K and K will respect these constraints."}, {"heading": "2.3.2. Prior Specifics", "text": "The previous ones above the parameters L, {cl (\u00b7)} L \u2212 1l = 0, {\u03b3l} L \u2212 1 l = 0, K are generated by the following process: 1. Let the hyperparameters H = {B, \u03bb, {\u03b1l} | B | \u2212 1l = 0, {\u03b2l} | B | \u2212 1 l = 0, \u03b1K, \u03b2K, {wl} | B | \u2212 1 l = 0} be specified. 2. Make a list in which the patients have listed their preferences. 4. For l = 0,.., L \u2212 1Drag cl (\u00b7) \u2022 pc (\u00b7) (\u00b7) \u2022 B, {wl} | 1 l = 0) where Epic (\u00b7) (\u00b7) (\u00b7) = cj (\u00b7) | trj."}, {"heading": "3. Fitting the Model", "text": "First, we describe our approach to determining the decision list with the maximum a posteriori probability. Then, we discuss our approach to performing the Monte Carlo sample from the posterior distribution via the decision parameters \u03b8 = {L, c0,..., L \u2212 1 (\u00b7), K, \u03b30,..., L \u2212 1} as described in Equation 13, pposterior (L, c0,..., L \u2212 1 (\u00b7), K, \u03b30,..., L \u2212 1 | y1,..., N; x1,..., N). (15)"}, {"heading": "3.1. Obtaining the MAP decision list", "text": "We have adopted a simulated approach to find a problem. (...) We have adopted a simulated approach to find a problem. (...) We have adopted a problem. (...) We have adopted a problem. (...) We have adopted a simulated approach. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem. (...) We have a problem."}, {"heading": "3.2. Obtaining the posterior over decision lists", "text": "To perform subsequent scanning steps, we use Gibbs scanning steps via {\u03b3l} L \u2212 1l = 0 and K enabled by variable extension, and Metropolis-Hastings steps via L and {cl (\u00b7)}. We describe the variable expansion step, the schedule of updates we use, and finally the details of each individual update step by step. Adding two additional variables Un \u2212 n for n = 1,.., N (19) for n = 1,.., N (1) for n = 1,., N (20) Yn = 1 (Un > 0 \u2212 n) for n = 1,. \u2212 n (21) Marginalization via n = 1,. \u2212 zouzyn (1) for n = 1,. (20) Yn = 1 (20) Yn = 1, p \u2212 n."}, {"heading": "3.2.1. Schedule of Updates", "text": "In view of the extended model, we will proceed through the following steps in the following deterministic order, which will be discussed in detail shortly. In terms of notation, we will use \u03b8aug to refer to the parameters of the extended model: (L, {cl (\u00b7)} L \u2212 1l = 0, K, {\u03b3l} L \u2212 1 l = 0, {Un} Nn = 1, {xn} Nn = 1), so that Gibbs updates can be described more concisely. Step 1: Example 1: Example-1-p.l-p.l-l-l-l-l-l-l-l-l-l-l-l-l-l-l-l-l-l-l-l-l-l, {yn} Nn = 1; {xn} Nn = 1) for l = 0,."}, {"heading": "3.2.2. Update Details", "text": "We now describe each step of the update plan in more detail: Step 1 In this advanced model, the complete conditional distribution of each of these steps is distributed gamma so that it can be taken directly from the sample. Let, for l = 0,.., L \u2212 1\u03c3 (l) k = {K\u041aki = L \u2212 1, i6 = l\u03b3i for 0 \u2264 k \u2212 l 0 for l < k \u2264 L. (28) Then it can be deduced that the sample l (l), {yn} Nn = 1; {xn} Nn = 1 \u0445 gamma (\u03b1l + N \u00b2 n = 1 [zn \u2264 l] Un, \u03b2l + N \u00b2 n \u00b2 n \u00b2 n \u00b2 n (l) zn (l) zn), (29) where we describe the equation before the sample l and zn."}, {"heading": "4. Simulation Studies", "text": "We show that for simulated data generated by a known decision list, our simulated annealing method, which most likely looks for the MAP decision list, restores the true decision list. Given observations with arbitrary characteristics and a collection of rules on these characteristics, we can construct a binary matrix in which the rows represent observations and the columns represent rules, and the entry is 1 if the rule applies to this observation, and 0 otherwise. We only need to simulate this binary matrix to represent the observations without losing the generality. For our simulations, we have created independent binary rule sets with 100 rules by independently setting each attribute value to 1 with a probability of 0.25. We have created a random decision list of size 5 by randomly selecting 5 rules, adding the default rule and setting the default 0,... that the induced p0,.,. p5 were roughly evenly distributed across all the unit."}, {"heading": "5. Experiments", "text": "Our most important experimental result is the use of falling rule lists to predict 30-day hospitalizations from ongoing collaboration with physicians (see Cronin et al., 2014, which describes the collection of data).Since we have extremely limited the properties of the predictive model (namely the monotonicity property, the rarity of rules, and the rarity of conditions per rule), we expect to lose the predictive accuracy over unrestricted methods.The interpretability advantage may or may not be sufficient to compensate for this, but it is highly application-dependent. We have found several cases where there is no loss of performance (with a significant gain in interpretability) by using an outdated rule list instead of, for example, a support vector engine that is consistent with Holte's hypothesis and observations of very simple classifiers that perform well. Later in this section, we aim to quantify the loss of prevalence from other publicly-available methods by fixing the fictiable loss of prevalence from falling lists."}, {"heading": "5.1. Predicting Hospital Readmissions", "text": "We applied falling rule lists to preliminary recognition data compiled through collaboration with a major hospital in the U.S. (Cronin et al., 2014), where the goal is to predict whether a patient will be logged into the hospital at 30 days, with data prior to their release. Of course, the datasets include features and binary readmission results for about 8,000 patients who had no prior history of readmissions; the features are highly detailed and include aspects such as \"impaired behavior,\" \"chronic pain,\" feels unsafe, \"and over 30 other detailed features that could be predictive of readmission. Fortunately, as we will see, a physician is not needed to collect this amount of detailed information to assess whether a particular patient is at high risk for reading.For these experiments and the experiments in the next section, no parameters were matched in Falling Rule Lists (FRL), and the global hyperparameters were chosen as hyperparameters."}, {"heading": "5.2. Performance on Public Datasets", "text": "We performed an empirical comparison of several UCI datasets (Bache & Lichman, 2013) using the experimental setup described above, which enables us to quantify the loss of accuracy due to the limited form of the model.Table 4 shows the results. As mentioned above, we observed that, despite the model's strict limitations, performance levels are still at the level of other methods and are not often much worse.This is probably due to the advantages of not using a greedy splitting method, which results from the space of reduced rules, careful formulation and optimization.To create a complete rule list on the mammographic mass dataset (n = 961, p = 13) it took 35 seconds, for the spam dataset (n = 4601, p = 57) 102 seconds, for breast cancer (n = 683, p = 27) 35 seconds, and for the autodataset (n = 1728, p = 21) this method can be paralleled in a natural way."}, {"heading": "6. Conclusion", "text": "As the Director of the US National Institute of Justice (Ridgeway, 2013) nicely put it, an interpretative model that is actually used is better than one that is more accurate and on the shelf. We imagine that models produced by FRL can be used, for example, by doctors in Third World countries who need models printed on laminated maps. In high-stakes decisions (such as medical decisions), it is important that we know whether to trust the model with which we make decisions; models like FRL help us tell when (and when not) we should trust."}], "references": [{"title": "User-oriented Assessment of Classification Model Understandability. Pages 11\u201319 of: SCAI", "author": ["Allahyari", "Hiva", "Lavesson", "Niklas"], "venue": null, "citeRegEx": "Allahyari et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Allahyari et al\\.", "year": 2011}, {"title": "Learning from sparse data by exploiting monotonicity constraints. arXiv preprint arXiv:1207.1364", "author": ["Altendorf", "Eric E", "Restificar", "Angelo C", "Dietterich", "Thomas G"], "venue": null, "citeRegEx": "Altendorf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Altendorf et al\\.", "year": 2012}, {"title": "The TIMI risk score for unstable angina/non\u2013ST elevation MI", "author": ["Antman", "Elliott M", "Cohen", "Marc", "Bernink", "Peter JLM", "McCabe", "Carolyn H", "Horacek", "Thomas", "Papuchis", "Gary", "Mautner", "Branco", "Corbalan", "Ramon", "Radley", "David", "Braunwald", "Eugene"], "venue": "The Journal of the American Medical Association,", "citeRegEx": "Antman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Antman et al\\.", "year": 2000}, {"title": "Monotonicity maintenance in information-theoretic machine learning algorithms", "author": ["Ben-David", "Arie."], "venue": "Machine Learning, 19(1), 29\u201343.", "citeRegEx": "Ben.David and Arie.,? 1995", "shortCiteRegEx": "Ben.David and Arie.", "year": 1995}, {"title": "An Implementation of the FP-growth Algorithm", "author": ["Borgelt", "Christian."], "venue": "Pages 1\u20135 of: Proceedings of the 1st international workshop on open source data mining: frequent pattern mining implementations. ACM.", "citeRegEx": "Borgelt and Christian.,? 2005", "shortCiteRegEx": "Borgelt and Christian.", "year": 2005}, {"title": "Classification and Regression Trees", "author": ["Breiman", "Leo", "Friedman", "Jerome H", "Olshen", "Richard A", "Stone", "Charles J"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Bayesian CART model search", "author": ["Chipman", "Hugh A", "George", "Edward I", "McCulloch", "Robert E"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Chipman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chipman et al\\.", "year": 1998}, {"title": "Predicting Hospital Readmissions Using Supersparse Interpretable Models. Work In Progress", "author": ["Cronin", "Patrick", "Ustun", "Berk", "Rudin", "Cynthia", "Greenwald", "Jeffrey"], "venue": null, "citeRegEx": "Cronin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cronin et al\\.", "year": 2014}, {"title": "Bayesian isotonic regression for discrete outcomes", "author": ["Dunson", "David B."], "venue": "Tech. rept. Citeseer.", "citeRegEx": "Dunson and B.,? 2004", "shortCiteRegEx": "Dunson and B.", "year": 2004}, {"title": "PRIE: a system for generating rulelists to maximize ROC performance", "author": ["Fawcett", "Tom."], "venue": "Data Mining and Knowledge Discovery, 17(2), 207\u2013224.", "citeRegEx": "Fawcett and Tom.,? 2008", "shortCiteRegEx": "Fawcett and Tom.", "year": 2008}, {"title": "Pruning for monotone classification trees. Pages 1\u201312 of: Advances in intelligent data analysis", "author": ["Feelders", "Ad", "Pardoel", "Martijn"], "venue": null, "citeRegEx": "Feelders et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Feelders et al\\.", "year": 2003}, {"title": "Comprehensible classification models: a position paper", "author": ["Freitas", "Alex A."], "venue": "ACM SIGKDD Explorations Newsletter, 15(1), 1\u201310.", "citeRegEx": "Freitas and A.,? 2014", "shortCiteRegEx": "Freitas and A.", "year": 2014}, {"title": "Validation of clinical classification schemes for predicting stroke", "author": ["Gage", "Brian F", "Waterman", "Amy D", "Shannon", "William", "Boechler", "Michael", "Rich", "Michael W", "Radford", "Martha J"], "venue": "The journal of the American Medical Association,", "citeRegEx": "Gage et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gage et al\\.", "year": 2001}, {"title": "Very simple classification rules perform well on most commonly used datasets", "author": ["Holte", "Robert C."], "venue": "Machine learning, 11(1), 63\u201390.", "citeRegEx": "Holte and C.,? 1993", "shortCiteRegEx": "Holte and C.", "year": 1993}, {"title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models", "author": ["Huysmans", "Johan", "Dejaeger", "Karel", "Mues", "Christophe", "Vanthienen", "Jan", "Baesens", "Bart"], "venue": "Decision Support Systems,", "citeRegEx": "Huysmans et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huysmans et al\\.", "year": 2011}, {"title": "APACHE-acute physiology and chronic health evaluation: a physiologically based classification system", "author": ["Knaus", "William A", "Zimmerman", "Jack E", "Wagner", "Douglas P", "Draper", "Elizabeth A", "Lawrence", "Diane E"], "venue": "Critical Care Medicine,", "citeRegEx": "Knaus et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Knaus et al\\.", "year": 1981}, {"title": "APACHE II: a severity of disease classification system", "author": ["Knaus", "William A", "Draper", "Elizabeth A", "Wagner", "Douglas P", "Zimmerman", "Jack E"], "venue": "Critical Care Medicine,", "citeRegEx": "Knaus et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Knaus et al\\.", "year": 1985}, {"title": "The APACHE III prognostic system. Risk prediction of hospital mortality for critically ill hospitalized adults", "author": ["Knaus", "William A", "DP Wagner", "EA Draper", "JE Zimmerman", "Bergner", "Marilyn", "PG Bastos", "CA Sirio", "DJ Murphy", "T Lotring", "A. Damiano"], "venue": "Chest Journal,", "citeRegEx": "Knaus et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Knaus et al\\.", "year": 1991}, {"title": "The comprehensibility manifesto", "author": ["Y. Kodratoff"], "venue": "KDD Nugget Newsletter, 94(9).", "citeRegEx": "Kodratoff,? 1994", "shortCiteRegEx": "Kodratoff", "year": 1994}, {"title": "Building Interpretable Classifiers with Rules using Bayesian Analysis", "author": ["Letham", "Benjamin", "Rudin", "Cynthia", "McCormick", "Tyler H", "Madigan", "David"], "venue": null, "citeRegEx": "Letham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Letham et al\\.", "year": 2012}, {"title": "Building acceptable classification models. Pages 53\u201374 of: Data", "author": ["Martens", "David", "Baesens", "Bart"], "venue": null, "citeRegEx": "Martens et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2010}, {"title": "Performance of classification models from a user perspective", "author": ["Martens", "David", "Vanthienen", "Jan", "Verbeke", "Wouter", "Baesens", "Bart"], "venue": "Decision Support Systems,", "citeRegEx": "Martens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2011}, {"title": "TIMI risk score for ST-elevation myocardial infarction: a convenient, bedside, clinical score for risk assessment at presentation an intravenous nPA for treatment of infarcting myocardium early II trial", "author": ["Morrow", "David A", "Antman", "Elliott M", "Charlesworth", "Andrew", "Cairns", "Richard", "Murphy", "Sabina A", "de Lemos", "James A", "Giugliano", "Robert P", "McCabe", "Carolyn H", "Braunwald", "Eugene"], "venue": "substudy. Circulation,", "citeRegEx": "Morrow et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Morrow et al\\.", "year": 2000}, {"title": "Knowledge discovery from data", "author": ["Pazzani", "Michael J"], "venue": "Intelligent systems and their applications,", "citeRegEx": "Pazzani and J.,? \\Q2000\\E", "shortCiteRegEx": "Pazzani and J.", "year": 2000}, {"title": "Acceptance of rules generated by machine learning among medical experts", "author": ["MJ Pazzani", "S Mani", "Shankle", "WR"], "venue": "Methods of information in medicine,", "citeRegEx": "Pazzani et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Pazzani et al\\.", "year": 2001}, {"title": "Induction of decision trees", "author": ["Quinlan", "J. Ross."], "venue": "Machine learning, 1(1), 81\u2013106.", "citeRegEx": "Quinlan and Ross.,? 1986", "shortCiteRegEx": "Quinlan and Ross.", "year": 1986}, {"title": "C4", "author": ["Quinlan", "John Ross."], "venue": "5: programs for machine learning. Vol. 1. Morgan kaufmann.", "citeRegEx": "Quinlan and Ross.,? 1993", "shortCiteRegEx": "Quinlan and Ross.", "year": 1993}, {"title": "Prognostic signs and the role of operative management in acute pancreatitis", "author": ["JH Ranson", "KM Rifkind", "DF Roses", "SD Fink", "K Eng", "FC Spencer"], "venue": "Surgery, gynecology & obstetrics,", "citeRegEx": "Ranson et al\\.,? \\Q1974\\E", "shortCiteRegEx": "Ranson et al\\.", "year": 1974}, {"title": "The Pitfalls of Prediction", "author": ["Ridgeway", "Greg."], "venue": "NIJ Journal, National Institute of Justice, 271, 34\u201340.", "citeRegEx": "Ridgeway and Greg.,? 2013", "shortCiteRegEx": "Ridgeway and Greg.", "year": 2013}, {"title": "Learning decision lists", "author": ["Rivest", "Ronald L."], "venue": "Machine learning, 2(3), 229\u2013246.", "citeRegEx": "Rivest and L.,? 1987", "shortCiteRegEx": "Rivest and L.", "year": 1987}, {"title": "Learning interpretable models", "author": ["R\u00fcping", "Stefan."], "venue": "Ph.D. thesis, Universit\u00e4t Dortmund.", "citeRegEx": "R\u00fcping and Stefan.,? 2006", "shortCiteRegEx": "R\u00fcping and Stefan.", "year": 2006}, {"title": "Partially collapsed Gibbs sampling and path-adaptive Metropolis-Hastings in high-energy astrophysics", "author": ["van Dyk", "David A", "Park", "Taeyoung"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "Dyk et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dyk et al\\.", "year": 2011}, {"title": "Building comprehensible customer churn prediction models with advanced rule induction techniques", "author": ["Verbeke", "Wouter", "Martens", "David", "Mues", "Christophe", "Baesens", "Bart"], "venue": "Expert Systems with Applications,", "citeRegEx": "Verbeke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Verbeke et al\\.", "year": 2011}, {"title": "Bayesian Ors of Ands for Interpretable Classification with Application to Context Aware Recommender Systems", "author": ["Wang", "Tong", "Rudin", "Cynthia", "Doshi", "Finale", "Liu", "Yimin", "Klampfl", "Erica", "MacNeille", "Perry"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": ", the TIMI scores, CHADS2 score, Apache scores, and the Ranson score, to name a few (Antman et al. , 2000; Morrow et al. , 2000; Gage et al. , 2001; Knaus et al. , 1981, 1985, 1991; Ranson et al. , 1974). These models can be computed without the use of a calculator, which makes them very practical as decision aids. Of course, we aim for this level of interpretability in purely data-driven classifiers, with no manual feature selection or rounding coefficients. In the industrial world, algorithms that discretize the input space have gained in popularity purely because they yield interpretable models. The CART algorithm of Breiman et al. (1984) seems to be the algorithm of choice, despite its drawbacks of being greedy and not as accurate as other methods.", "startOffset": 85, "endOffset": 650}, {"referenceID": 1, "context": "Since it is possible that decision tree methods can produce results that are inconsistent with monotonicity properties of the data, there is a subfield dedicated to altering these greedy decision tree algorithms to obey monotonicity properties (Ben-David, 1995; Feelders & Pardoel, 2003; Altendorf et al. , 2012). Studies showed that in many cases, no accuracy is lost in enforcing monotonicity constraints, and that medical experts were more willing to use the models with the monotonicity constraints (Pazzani et al. , 2001). Even with (what seem like) rather severe constraints on the hypothesis space such as monotonicity or sparsity in the number of leaves and nodes, it still seems that the set of accurate classifiers is often large enough so that it contains interpretable classifiers (see Holte, 1993). Because the monotonicity properties we enforce are much stronger than those of Ben-David (1995); Feelders & Pardoel (2003); Altendorf et al.", "startOffset": 288, "endOffset": 908}, {"referenceID": 1, "context": "Since it is possible that decision tree methods can produce results that are inconsistent with monotonicity properties of the data, there is a subfield dedicated to altering these greedy decision tree algorithms to obey monotonicity properties (Ben-David, 1995; Feelders & Pardoel, 2003; Altendorf et al. , 2012). Studies showed that in many cases, no accuracy is lost in enforcing monotonicity constraints, and that medical experts were more willing to use the models with the monotonicity constraints (Pazzani et al. , 2001). Even with (what seem like) rather severe constraints on the hypothesis space such as monotonicity or sparsity in the number of leaves and nodes, it still seems that the set of accurate classifiers is often large enough so that it contains interpretable classifiers (see Holte, 1993). Because the monotonicity properties we enforce are much stronger than those of Ben-David (1995); Feelders & Pardoel (2003); Altendorf et al.", "startOffset": 288, "endOffset": 935}, {"referenceID": 1, "context": "Since it is possible that decision tree methods can produce results that are inconsistent with monotonicity properties of the data, there is a subfield dedicated to altering these greedy decision tree algorithms to obey monotonicity properties (Ben-David, 1995; Feelders & Pardoel, 2003; Altendorf et al. , 2012). Studies showed that in many cases, no accuracy is lost in enforcing monotonicity constraints, and that medical experts were more willing to use the models with the monotonicity constraints (Pazzani et al. , 2001). Even with (what seem like) rather severe constraints on the hypothesis space such as monotonicity or sparsity in the number of leaves and nodes, it still seems that the set of accurate classifiers is often large enough so that it contains interpretable classifiers (see Holte, 1993). Because the monotonicity properties we enforce are much stronger than those of Ben-David (1995); Feelders & Pardoel (2003); Altendorf et al. (2012) (we are looking at monotonicity along the whole list rather than for individual features), we do find that accuracy is sometimes sacrificed, but not always, and generally not by much.", "startOffset": 288, "endOffset": 960}], "year": 2017, "abstractText": "Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.", "creator": "LaTeX with hyperref package"}}}