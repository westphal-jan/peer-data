{"id": "1411.5928", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2014", "title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks", "abstract": "We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.", "histories": [["v1", "Fri, 21 Nov 2014 16:01:04 GMT  (9011kb,D)", "http://arxiv.org/abs/1411.5928v1", null], ["v2", "Mon, 5 Jan 2015 12:31:49 GMT  (8191kb,D)", "http://arxiv.org/abs/1411.5928v2", "v2: minor changes. Adjusted the figures (mainly figure 2, figure 11); added urls of videos in the supplementary"], ["v3", "Thu, 3 Dec 2015 09:49:23 GMT  (9391kb,D)", "http://arxiv.org/abs/1411.5928v3", "v3: added new object classes, new architectures, many new experiments"], ["v4", "Wed, 2 Aug 2017 20:53:43 GMT  (7411kb,D)", "http://arxiv.org/abs/1411.5928v4", "v4: final PAMI version. New architecture figure"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["alexey dosovitskiy", "jost tobias springenberg", "maxim tatarchenko", "thomas brox"], "accepted": false, "id": "1411.5928"}, "pdf": {"name": "1411.5928.pdf", "metadata": {"source": "CRF", "title": "Learning to Generate Chairs with Convolutional Neural Networks", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Thomas Brox"], "emails": ["brox}@cs.uni-freiburg.de"], "sections": [{"heading": "1. Introduction", "text": "It has been shown that Convolutionary Neural Networks (CNNs) are very successful on a variety of computer vision tasks, such as image classification [18, 5, 32], recognition [10, 28] and segmentation [10]. All of these tasks have in common that they can be posed as discriminatory monitored learning problems and can therefore be solved with CNNs that are known to have a sufficiently large dataset. Typically, a task solved by monitored CNNs involves learning processes ranging from raw sensor inputs to some kind of condensed, abstract representation, such as object identity, position or scale. In this work, we engage in a verified training, but we apply the standard discriminatory CNN and use it to produce images at the highest level, which we create with models by Aubry et al."}, {"heading": "2. Related work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3. Model description", "text": "Our goal is to train a neural network to generate accurate images of chairs from a high-level description: class, orientation with respect to the camera and additional parameters such as color, brightness, etc. Formally, we assume that we will be given a dataset with examples D = {(c1, v1, \u03b81),.., (cN, vN, \u03b8N)} with targets O = {(x1, s1),..., (xN, sN)}. The input stages consist of three vectors: c is the class name in uniform encoding, v - azimuth and elevation of the camera position (represented by its sine and cosine 1), and \u03b8 - the parameters of additional artificial transformations applied to the images. The goals are the RGB output image and the segmentation mask. We include artificial transformations that are defined by the randomly generated parameters to override the transformation and to reduce the variation in the annex of the data."}, {"heading": "3.1. Network architecture", "text": "We experimented with networks for generating 64 \u00d7 64 and 128 \u00d7 128 images, and the network architectures for both variants are identical, except that the smaller network is reduced by a revolutionary layer; the structure of the larger 128 \u00d7 128 generative network is shown in Figure 2. Conceptually, the generative network, which we formally call g (c, v, \u03b8), looks like an ordinary CNN card that is turned upside down; one can imagine how the composition of two processing steps g = u h.Layers FC-1 to FC-4 initially provides a common, high-dimensional, hidden representation h (c, v) of the input parameters; within these layers, the three input vectors are independently fed by two fully connected layers of 512 neurons each, and then the outputs of these three streams are linked together."}, {"heading": "3.2. Generative training", "text": "The network parameters W, consisting of all plane weights and distortions, are then trained by minimizing the Euclidean error in the reconstruction of the segmented stool image and the segmentation mask (the weights W are omitted in the arguments of h and u for the abbreviation of the notation): min W N \u2211 i = 1 \u03bb \u2211 uRGB (h (ci, vi, \u03b8i)) \u2212 T\u03b8i (xi \u00b7 si) \u0445 22 + \u0445 usegm (h (ci, vi, \u03b8i)) \u2212 T\u03b8isi \u0442 22, (1) where \u03bb is a weighting term that balances between exact reconstruction of the image and its segmentation mask. In all experiments, we set \u03bb = 10. Note that the mask could be indirectly derived from the RGB image by using monotonous background, but we do not rely on it, but require the network to add the mask directly, so that we never have to explicitly display it in the background."}, {"heading": "3.3. Dataset", "text": "As training data for the generative networks, we used the 3D chair models published by Aubry et al. [1]. Specifically, we used the dataset of rendered views that they provide. It contains 1393 chair models, each rendered from 62 angles: 31 azimuth angles (with steps of 11 degrees) and 2 elevation angles (20 and 30 degrees) at a fixed distance from the chair. We found that the dataset contains many nearly duplicated models, models that differ only in color, or models of low quality. After removing these, we returned a reduced dataset of 809 models that we used in our experiments."}, {"heading": "3.4. Training details", "text": "We first trained at a learning rate of 0.0002 for 500 passes through the entire dataset (epochs) and then conducted 300 additional epochs of training, dividing the learning rate by 2 after 100 epochs. We initialized the weights of the network with orthogonal matrices, as recommended by Saxe et al. [27] When we trained the 128 x 128 network from scratch, we observed that its initial energy value never decreases. Assuming that the high-level representation of the 64 x 64 and 128 x 128 networks is very similar, we mitigated this problem by initializing the weights of the 128 x 128 network with the weights of the trained 64 x 64 network, with the exception of the last two layers."}, {"heading": "4. Analysis of the network", "text": "It is well known that neural networks remain largely \"black boxes\" whose function is difficult to understand. In this section, we analyze our trained generative network with the aim of gaining a little intuition about its internal functioning. We present only the most interesting results here; more on this in the supplementary material."}, {"heading": "4.1. Network capacity", "text": "The first observation is that the network successfully modeled the variation in data. Figure 5 shows results where the network was forced to generate chairs that were significantly transformed compared to the original images. Each row shows a different type of transformation. Images in the central pillar are not transformed. Even with major transformations, the quality of the images generated is essentially as good as without transformation. Image quality typically deteriorates a little with unusual stool shapes (such as rotating office chairs) and chairs with fine details such as armrests (see e.g. one of the armrests in Figure 5) or thin elements in the back of the chair (row 3 in Figure 5). An interesting observation is that the network easily handles extreme color transformations, but has some problems that represent large spatial changes, especially translations. Our explanation is that the architecture we use has no means to efficiently model separately, say translations, because the transformations just need to be fully sparse."}, {"heading": "4.2. Activating single units", "text": "One way to analyze a neural network (artificial or real) is to visualize the effect of individual neuron activations. Although this method does not allow us to judge the actual functioning of the network, which involves a clever combination of many neurons, there is still an approximate idea of what kind of representation is generated by the various network layers. Activating individual neurons of Uconv-3 function cards (last feature cards before output) is equivalent to simply viewing the filters of these layers, which are shown in each position."}, {"heading": "4.3. Analysis of the hidden layers", "text": "Instead of activating only individual neurons while all others remain fixed to zero, we can use the network to normally generate an image and then analyze the hidden layer activations by either looking at or modifying them and observing the results. An example of this approach has already been used in Figure 8 above to understand the effect of \"zoom neurons.\" We present two more results in this direction, and several more can be found in the complementary material. To find out how the blurred \"clouds\" generated by individual high-level devolutionary neurons (Figure 9) form perfectly sharp stool images that we gently interpolate between a single activation and the entire stool."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Interpolation between viewpoints", "text": "In this section, we show that the generative network is able to generate hitherto invisible views by interpolating the views contained in the training data, showing that the network learns internally a representation of the chairs that enables it to judge the similarity of the chair and use the known views to reduce computing costs. We separate the chair into two parts: the \"source source\" with 90% styles and the \"objective\" with the remaining 10% chairs."}, {"heading": "5.2. Interpolation between classes", "text": "Remarkably, the generative network can interpolate not only between different points of view of the same object, but also between different objects, so that all intermediate images are meaningful. To obtain such interpolations, we simply linearly change the input-label vector from one class to another. Some representative examples of such morphings are shown in Figure 14. The images are sorted by subjective morphing quality (which decreases from top to bottom).The network generates very natural-looking morphings even in difficult cases, such as the first 5 lines. In the last three lines, the morphings are of lower quality: some of the intermediate samples do not look exactly like real stools. However, the result of the last line is quite fascinating, as different types of intermediate buttons are created. Further examples of morphing are shown in the supplementary material."}, {"heading": "5.2.1 Correspondences", "text": "The generative CNN's ability to interpolate between different chairs allows us to find dense correspondences between different instances of objects, even if their appearance is very dissimilar. Considering two chairs from the training dataset, we use the 128 \u00d7 128 network to generate a morphing consisting of 64 images (fixed view), then calculate the optical flow in the resulting sequence using the code from Brox et al. [4] To compensate for the drift, we refine the calculated optical flow by recalculating it with a step of 9 images initialized by concatenated image flows. The linking of these sophisticated optical flows gives the global vector field connecting the corresponding points in the two stool images. To numerically evaluate the quality of the correspondences, we have a small test set of 30 pairs of images (examples are shown in the supplementary material)."}, {"heading": "6. Conclusions", "text": "A network trained for such a generative task not only learns to generate the training samples, but also generalizes well, enabling it to smoothly transform different object views or object instances into each other, with all intermediate images also making sense. It is fascinating that the relatively simple architecture we are proposing is already capable of learning such complex behavior. Technically, it is impressive that the network is able to process very different inputs - class names, viewing angles, and the parameters of additional chromatic and spatial transformations - using exactly the same standard layers of ReLU neurons."}, {"heading": "1. Analysis of the network", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Activating single neurons and groups of neurons", "text": "In the main paper, we show images generated from individual neuron activations in different fully interconnected levels of the network. However, as highlighted in the main paper, the images generated by individual neurons in each layer look quite similar, regardless of which individual neuron is activated, suggesting that 1) the amount of variation may depend on the activation strength of a neuron and 2) greater variation can be achieved by activating multiple neurons. Here, we are experimentally testing both hypotheses for the FC-1 layer. Activation values vary from top to bottom, from 2 to 25. Figure 7 of the main paper specifies the activation value of a single FC-1 neuron (one neuron per column, one value per line), which are the same neurons as in Figure 7 of the main paper. Activation values vary from top to bottom, from 2 to 25. Figure 7 of the main paper sets the activation value of the neurons in the series 10 to coincide with the number 10."}, {"heading": "1.2. Analysis of the hidden layers", "text": "In Figure 18 we show some representative characteristic maps of different layers from the generating currents (FC-5 to uconv-3). For better viewing, the characteristic maps are modified by cutting out 1 percent of the darkest and brightest values. Characteristic maps uconv-2 and uconv-3 contain some chair-like contours, but FC-5 and uconv-1, which are further away from the initial image, are more abstract and difficult to interpret. Similar to the \"zoom neurons\" described in the main paper, there is a separate single neuron in the FC-4 layer for more or less every artificial transformation we applied during the training. The effect of increasing their values is shown in Figure 19, one neuron per line, activation increasing from left to right."}, {"heading": "2. Interpolation between classes", "text": "In Figure 22 we show some more examples of \"morphings\" generated by the network. Further examples of morphings can be seen in the supplementary video CVPR15 Generate Chairs mov morphing.av."}, {"heading": "2.1. Correspondences", "text": "In Figure 23 we show examples of \"simple\" and \"difficult\" pairs from the test kit. Examples of keypoint tracking using optical flow are shown in the supplementary video CVPR15 Generate Chairs mov tracking.avi. Optical flow does not always follow the points perfectly, but in most cases the results look good, which is also supported by the figures from the main paper."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "We train a generative convolutional neural network<lb>which is able to generate images of objects given object<lb>type, viewpoint, and color. We train the network in a su-<lb>pervised manner on a dataset of rendered 3D chair mod-<lb>els. Our experiments show that the network does not merely<lb>learn all images by heart, but rather finds a meaningful<lb>representation of a 3D chair model allowing it to assess<lb>the similarity of different chairs, interpolate between given<lb>viewpoints to generate the missing ones, or invent new chair<lb>styles by interpolating between chairs from the training set.<lb>We show that the network can be used to find correspon-<lb>dences between different chairs from the dataset, outper-<lb>forming existing approaches on this task.", "creator": "LaTeX with hyperref package"}}}