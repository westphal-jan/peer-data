{"id": "1411.6232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks", "abstract": "In this paper, we propose a novel semi-supervised feature selection framework by mining correlations among multiple tasks and apply it to different multimedia applications. Instead of independently computing the importance of features for each task, our algorithm leverages shared knowledge from multiple related tasks, thus, improving the performance of feature selection. Note that we build our algorithm on assumption that different tasks share common structures. The proposed algorithm selects features in a batch mode, by which the correlations between different features are taken into consideration. Besides, considering the fact that labeling a large amount of training data in real world is both time-consuming and tedious, we adopt manifold learning which exploits both labeled and unlabeled training data for feature space analysis. Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence. Extensive experiments on different applications demonstrate that our algorithm outperforms other state-of-the-art feature selection algorithms.", "histories": [["v1", "Sun, 23 Nov 2014 13:00:18 GMT  (2519kb)", "http://arxiv.org/abs/1411.6232v1", "11 pages, submitted to TNNLS"], ["v2", "Sun, 11 Jan 2015 18:47:26 GMT  (2504kb)", "http://arxiv.org/abs/1411.6232v2", "11 pages, submitted to TNNLS"]], "COMMENTS": "11 pages, submitted to TNNLS", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "yi yang"], "accepted": false, "id": "1411.6232"}, "pdf": {"name": "1411.6232.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks", "authors": ["Xiaojun Chang"], "emails": ["cxj273@gmail.com,", "yi.yang@uq.edu.au)."], "sections": [{"heading": null, "text": "ar Xiv: 141 1.62 32v1 [cs.LG] 2 3N ovIndex Terms - Multi-task feature selection, semi-supervised learning, image annotation, 3D motion data annotation"}, {"heading": "1 INTRODUCTION", "text": "In many computer systems in recent years, the ratios of data representation are usually very high. Recent studies have claimed that not all features of the high-dimensional attribute space are discriminatory and informative, since many features are often noisy or correlated with each other, degrading the performance of subsequent data analysis tasks [1]. Consequently, the selection of features is used to obtain a subset of features from the original high-dimensional attribute space. [5] [8] It has two functions to improve the performance of learning tasks. First, the selection of features is eliminated in order to obtain better representation, facilitating classification and clustering tasks. The second dimension of the selected attribute space becomes much lower, making the subsequent compilation more efficient."}, {"heading": "2 RELATED WORK", "text": "In this section we briefly discuss related research on feature selection, semi-supervised learning and multi-task learning."}, {"heading": "2.1 Feature selection", "text": "Previous work has claimed that feature selection is able to select the most representative features, facilitating subsequent data analysis tasks [14] [15] [16]. Existing feature selection algorithms are designed in different ways. [17] Although these classic feature selection algorithms perform well in different applications, such as Fisher Score [9], they have three main limitations: First, they only use labeled 3-training data to exploit the correlations between features and labels for feature selection. [17] The labeling of a large amount of training data consumes a lot of human labor in real applications. Second, the most representative features are selected one by one, ignoring the correlations between different features. Third, they select features for each task independently of what knowledge is shared by multiple tasks."}, {"heading": "2.2 Semi-supervised learning", "text": "Semi-Supervised Learning has demonstrated its promising performance in various applications [19], [20], [21], [22], [23], [24]. With semi-supervised learning, blank training data can be used to learn data structures that can save labor costs for labeling a large amount of training data [25], [26], [27], [28]. Therefore, semi-supervised learning is beneficial in terms of both human labor costs and data analysis. Semi-supervised learning based on Graph Laplacian has gained increasing interest in its simplicity and efficiency. Never et al. propose a diverse learning framework based on graphics Laplacian and compare its performance with other state-of-the-art semi-supervised algorithms in [29]. Ma et al. propose a semi-supervised feature selection algorithm that builds on diverse learning."}, {"heading": "2.3 Multi-task learning", "text": "Multi-task learning is widely used in many applications, with the attractive advantage of learning multiple related tasks with a common representation [11] [12] [31]. Recent research has shown that learning multiple related tasks together is always more successful than learning these tasks independently of each other. Inspired by the progress of multi-task learning, researchers have introduced it to the field of multimedia and demonstrated its promising performance in multimedia analysis. Thus, Yang et al. propose a novel multi-task feature selection algorithm that improves the performance of feature selection by using shared information between multiple related tasks [6]. In [6], Ma et al. apply knowledge adjustments to the detection of multimedia events and compare their performance with several state-of-the-art algorithms. Despite their good performance, these classic algorithms are all implemented only with labeled training data."}, {"heading": "3 METHODOLOGY", "text": "In this section, we describe the approach of our proposed algorithm in detail."}, {"heading": "3.1 Problem Formulation", "text": "(Wl) The second, third, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh"}, {"heading": "3.2 Optimization", "text": "The proposed function refers to the l2.1 standard and the trace standard, which are difficult to solve in closed form. We propose to solve this problem in the following steps: By changing the derivative from (8) w.r.t to 0, we getbl = 1nl (Fl \u2212 XT l Wl) T 1nl (9) substitute bl in (8), we obtainmin Fl \u2212 Wl, blt = 1 (Tr \u2212 Yl) T Ul (Fl \u2212 Yl) + Tr (Fl \u2212 Yl) + Tr (F \u2212 Yl) + Tr (F l \u2212 LlFl) n (Fl \u2212 LlFl) + 1 (Fl \u2212 Yl) + 1 (Fl \u2212 Wl \u2212 Yl) + Fl (Fl \u2212 Wl) (Fl \u2212 Wl \u2212 Yl) + Fl (Fl \u2212 Yl) \u2212 Yl (Fl \u2212 Yl) + 1 (Fl \u2212 Wl) \u2212 Yl (Fl \u2212 Yl)"}, {"heading": "3.3 Convergence Analysis", "text": "In this section we demonstrate that algorithm m m 1 + Tr (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (1 l) l (1 l) l (1 l) l (1 l) l (1 l) l (W + 1) l (1 l) l (W + 1) l (1 l) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1) l (W + 1 l) l (W + 1 l) l (W + 1 l) l (W + 1 l (W + 1 l) l (W + 1 l) l (W + 1 l) l (W + 1 l (W + 1 l) l (W + 1 l) l (W + 1 l) l (W + 1 l (W + 1 l) l (W + 1 l) l (W + 1 l) l (W + 1 l) l"}, {"heading": "4 EXPERIMENTS", "text": "In this section, experiments will be conducted to evaluate the performance of our algorithm in the areas of video classification, image commentary, human motion detection, and 3D motion data analysis. Further experiments will be performed to examine the influence of performance on the number of selected features and the sensitivity of parameters."}, {"heading": "4.1 Experiment Setup", "text": "We use four different datasets in the experiment, including a CCV [35] video dataset, an image dataset NUSWIDE [36], a human motion dataset HMDB [37], and a 3D motion skeleton dataset HumanEva [38]. To demonstrate the benefits of our algorithm, we compare its performance with the following approaches. [9] All Features: We directly use the original features without feature selection as a starting point. 2) Fisher Score: This is a classic feature selection method that evaluates the importance of the features and selects the most distinctive features individually [9]. 3) Feature selection via common l2,1 standards minimization (FSNM): The common l2,1 standard minimization is applied both to loss function and to regulation for common feature selection [10]. 4) SPEC: It uses spectral graphical theory to perform multiple features simultaneously [5]."}, {"heading": "4.2 Video Classification", "text": "First, we compare the performance of different algorithms in the video classification task with the Columbia Consumer Video Dataset (CCV) [35], which consists of 9, 317 web videos across 20 semantic categories, using 4, 659 videos as training data and 4, 658 videos as test data. Semantic categories include events such as \"Basketball\" and \"Parade,\" scenes such as \"Beach\" and \"Playground,\" and objects such as \"Cat\" and \"Dog,\" on the basis of which we create three different classification tasks. As the original videos of this dataset are not available on the Internet, we use the STIP functions directly with a 5000-dimensional BoWs representation provided by [35]. We set the number of selected characteristics for all algorithms to {2500, 3000, \u00b7 \u00b7, 4500, 5000} and report the best results. We show the video classification results when using different percentages of the described training data in Table 2. From the experimental results, we can achieve the following improvements: The 5% improvement in the proposed training data, if we compare the other observations."}, {"heading": "4.3 Image Annotation", "text": "We use the NUS-WIDE dataset [36] to test the performance of our algorithm. This dataset includes 269648 images of 81 concepts. In this experiment, a 500-dimensional bag-of-words feature based on the SIFT descriptor is used. We take each concept as a separate annotation task, resulting in 81 tasks. It is difficult to report all the results of these 81 tasks so that the average result is reported. In this experiment, we set the number of selected features as {250, 275, \u00b7, 475, 500} and report the best results. We illustrate the experimental results in Table 3. From the experimental results, we can find that the proposed method performs better than the other comparative algorithms. We give the detailed results with 1%, 5% and 10% labeled training data. It is evident that the proposed algorithm is more competitive with less labeled training data."}, {"heading": "4.4 Human Motion Recognition", "text": "We use the HMDB video dataset [37] to compare the algorithms with respect to human motion detection; the HMDB dataset consists of 6,766 videos associated with 51 different action categories, which can be divided into five groups: 1) General face movements, 2) Face movements with object manipulation, 3) General body movements with object interaction, 5) Body movements for human interaction. Therefore, the five groups in this experiment are considered to be five different tasks. Heng et al. claim that motion limit histograms (MBH) are an efficient method of suppressing camera movement in [40] and are therefore used to process the videos. A 2000-dimensional Bag-of-Words function is generated to represent the original data. We set the number of selected features as {1000, 1200, \u00b7 \u00b7, 1800, 2000} for all algorithms and report the best results."}, {"heading": "4.5 3D Motion Data Analysis", "text": "We evaluate the performance of our algorithm in terms of 3D motion data analysis using the HumanEva 3D motion database. There are five different types of actions in this database, including boxing, gestures, walking, throwing-catching and jogging. After working in [41] [42], we randomly select 10,000 samples from two subjects (5,000 per subject). We encode each action as a collection of 16 common coordinates in 3D space and get a 48-dimensional feature vector. In addition, the common relative characteristics between different joints are calculated, resulting in a feature vector with 120 dimensions. We combine the two types of feature vectors and get a 168-dimensional feature. In this experiment, we consider the two subjects as two different tasks. The number of selected characteristics is calculated from {100, 110, \u00b7 \u00b7 \u00b7, 160}. The test results are shown in Table 5. Returns 5 detailed results, if data is called 810% and 85%."}, {"heading": "4.6 Comparison with Other Semi-Supervised Feature Selection Methods", "text": "In this section, experiments with CCV are conducted to compare the proposed algorithm with two state-of-the-art semi-supervised selection algorithms. Following the above experiments, this experiment labels 1%, 5%, 10%, 25%, 50% and 100% training data. We show the experiment results in Figure 2. We can conclude that our method consistently outperforms both LSDF and SFSS. Visible benefits arise when only a few training data are labeled, e.g. 1% or 5% labeled training data. From this, we can conclude that it is beneficial to share information from other related tasks when not enough training data is labeled."}, {"heading": "4.7 Parameter Sensitivity", "text": "We study the influence of the four parameters \u03b1, \u03b2, \u03b3 and the number of selected characteristics by means of a CCV database of training data labelled with 1%. First, we fix \u03b3 and the number of selected characteristics on 1 and 3500, respectively, which are the averages of the coordinated range of parameters. The experimental results are shown in Figure 3. It can be seen that the performance of our algorithm varies when the parameters (\u03b1 and \u03b2) change. Specifically, MAP is higher when \u03b1 and \u03b2 are comparable. Afterwards, \u03b1 and \u03b2 are fixed. Figure 4 shows the results of parameter sensitivity. Note that the common information between the selection functions of several characteristics {W1, \u00b7 \u00b7 \u00b7, Wt} is increased by the parameter \u03b3. From this figure, we can see that mining correlations between several related tasks are advantageous in order to improve performance. In addition, we can find that better performances are achieved when the number of characteristics is about 3500 and 4000."}, {"heading": "5 CONCLUSION", "text": "Since the proposed objective function is not smooth and difficult to solve, we propose a 10x effective algorithm. To evaluate the performance of the proposed method, we apply it to various applications, including video classification, image description, human motion detection and 3D motion data analysis. The experimental results suggest that the proposed method outperforms other comparable algorithms for different applications."}], "references": [{"title": "Feature selection for high-dimensional data: a fast correlation-based filter solution", "author": ["L. Yu", "H. Liu"], "venue": "Proc. ICML, 2003, pp. 856\u2013863.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "An evaluation of filter and wrapper methods for feature selection in categorical clustering", "author": ["L. Talavera"], "venue": "Proc. IDA, 2005, pp. 440\u2013451.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Web image annotation via subspace-sparsity collaborated feature selection", "author": ["Z. Ma", "F. Nie", "Y. Yang", "J.R. Uijlings", "N. Sebe"], "venue": "IEEE Trans. Multimedia, vol. 14, no. 4, pp. 1021\u2013 1030, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminating joint feature analysis for multimedia data understanding", "author": ["M. Zhigang", "F. Nie", "Y. Yang", "J. Uijlings", "N. Sebe", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia, vol. 14, no. 6, pp. 1662 \u2013 1672, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature selection for multimedia analysis by sharing information among multiple tasks", "author": ["Y. Yang", "Z. Ma", "A. Hauptmann", "N. Sebe"], "venue": "IEEE Trans. Multimedia, vol. 15, no. 3, pp. 661 \u2013 669, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["H. Liu", "L. Yu"], "venue": "IEEE Trans. Knowl. Data Engin., vol. 17, no. 4, pp. 491\u2013502, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "An effective feature selection method via mutual information estimation", "author": ["J. Yang", "C.J. Ong"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 42, no. 6, pp. 1550\u20131559, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient and robust feature selection via joint 2, 1-norms minimization", "author": ["N. Feiping", "H. Huang", "X. Cai", "C.H. Ding"], "venue": "Proc. NIPS, 2010, pp. 1813\u20131821.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, no. 1, pp. 41\u201375, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, vol. 73, no. 3, pp. 243\u2013272, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou"], "venue": "Proc. NIPS, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient spectral feature selection with minimum redundancy", "author": ["Z. Zhao", "L. Wang", "H. Liu"], "venue": "Proc. AAAI, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminative feature selection by nonparametric bayes error minimization", "author": ["S.H. Yang", "B.-G. Hu"], "venue": "IEEE Trans. Knowl. Data Engin., vol. 24, no. 8, pp. 1422\u20131434, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative least squares regression for multiclass classification and feature selection", "author": ["S. Xiang", "F. Nie", "G. Meng", "C. Pan", "C. Zhang"], "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 23, no. 11, pp. 1738\u20131754, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "L21-norm regularization discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "proc. IJCAI, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zheng", "H. Liu"], "venue": "Proc. ICML, 2007, pp. 1151\u2013 1157.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Computer Science, University of Wisconsin-Madison, Tech. Rep., 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Semisupervised classification with cluster regularization", "author": ["R.G.F. Soares", "H. Chen", "X. Yao"], "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 23, no. 11, pp. 1779\u20131792, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Semisupervised metric learning by maximizing constraint margin", "author": ["F. Wang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 41, no. 4, pp. 931\u2013939, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Semisupervised learning of classifiers: Theory, algorithms, and their application to human-computer interaction", "author": ["I. Cohen", "F.G. Cozman", "N. Sebe", "M.C. Cirelo", "T.S. Huang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, no. 12, pp. 1553\u20131567, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Multimodal semisupervised learning for image classification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Proc. CVPR, 2010, pp. 902\u2013909.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A convex formulation for semi-supervised multi-label feature selection", "author": ["X. Chang", "F. Nie", "Y. Yang", "H. Huang"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada., 2014, pp. 1171\u20131177.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Ranking with local regression and global alignment for cross media retrieval", "author": ["Y. Yang", "D. Xu", "F. Nie", "J. Luo", "Y. Zhuang"], "venue": "Proc. ACM Multimedia, 2009, pp. 175\u2013184.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Efcient semi-supervised feature selection with noise insensitive trace ratio criterion", "author": ["Y. Liu", "F. Nie", "J. Wu", "L. Chen"], "venue": "Neurocomputing, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised feature analysis for multimedia annotation by mining label correlation", "author": ["X. Chang", "H. Shen", "S. Wang", "J. Liu", "X. Li"], "venue": "Advances in Knowledge Discovery and Data Mining - 18th Pacific-Asia Conference, PAKDD 2014, Tainan, Taiwan, May 13-16, 2014. Proceedings, Part II, 2014, pp. 74\u201385.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Graph construction and b-matching for semi-supervised learning", "author": ["T. Jebara", "J. Wang", "S.-F. Chang"], "venue": "Proc. ICML, 2009, pp. 441\u2013448.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction", "author": ["F. Nie", "D. Xu", "I.-H. Tsang", "C. Zhang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 7, pp. 1921\u20131932, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1921}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Y. Yang", "D. Xu", "F. Nie", "S. Yan", "Y. Zhuang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761\u20132773, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Integrating low-rank and groupsparse structures for robust multi-task learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "Proc. ACM SIGKDD, 2011, pp. 42\u201350.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Statistics and Computing, vol. 20, no. 2, pp. 231\u2013252, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval", "author": ["Y. Yang", "Y.-T. Zhuang", "F. Wu", "Y.-H. Pan"], "venue": "IEEE Trans. Multimedia, vol. 10, no. 3, pp. 437\u2013446, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic manifold learning for image retrieval", "author": ["Y.-Y. Lin", "T.-L. Liu", "H.-T. Chen"], "venue": "Proc. ACM Multimedia, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Consumer video understanding: A benchmark database and an evaluation of human and machine performance", "author": ["Y.-G. Jiang", "G. Ye", "S.-F. Chang", "D. Ellis", "A.C. Loui"], "venue": "Proc. ICMR, 2011, p. 29.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Nus-wide: A real-world web image database from national university of singapore", "author": ["T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y.-T. Zheng"], "venue": "Proc. of ACM Conf. on Image and Video Retrieval (CIVR\u201909), Santorini, Greece., July 8-10, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Hmdb: a large video database for human motion recognition", "author": ["H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "Proc. ICCV, 2011, pp. 2556\u20132563.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Humaneva: Synchronized video and motion capture dataset for evaluation of articulated human motion", "author": ["S. Leonid", "M.J. Black"], "venue": "Brown Univertsity, Tech. Rep. CS-06-08, 2006.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality sensitive semi-supervised feature selection", "author": ["J. Zhao", "K. Lu", "X. He"], "venue": "Neurocomputing, vol. 71, no. 10, pp. 1842\u2013 1849, 2008.  12", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1842}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "Proc. ICCV, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative learning of visual words for 3d human pose estimation", "author": ["N. Huazhong", "W. Xu", "Y. Gong", "T. Huang"], "venue": "Proc. CVPR, 2008, pp. 1\u20138.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Y. Yang", "D. Xu", "F. Nie", "S. Yan", "Y. Zhuang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761\u20132773, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Recent studies have claimed that not all features in the high-dimensional feature space are discriminative and informative, since many features are often noisy or correlated to each other, which will deteriorate the performances of subsequent data analysing tasks [1] [2] [3].", "startOffset": 264, "endOffset": 267}, {"referenceID": 1, "context": "Recent studies have claimed that not all features in the high-dimensional feature space are discriminative and informative, since many features are often noisy or correlated to each other, which will deteriorate the performances of subsequent data analysing tasks [1] [2] [3].", "startOffset": 268, "endOffset": 271}, {"referenceID": 2, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "For example, [10] and [3] implement their methods in a supervised way and Ma et al.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "design their approach in a semi-supervise way in [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "Recent researches have indicated that it is beneficial to learn multiple related tasks jointly [11] [12] [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Recent researches have indicated that it is beneficial to learn multiple related tasks jointly [11] [12] [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "Recent researches have indicated that it is beneficial to learn multiple related tasks jointly [11] [12] [13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "present a novel feature selection algorithm which leverages shared information from related tasks in [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 11, "context": "1 Feature selection Previous works have claimed that feature selection is capable of selecting the most representative features, thus facilitating subsequent data analysing tasks [14] [15] [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 12, "context": "1 Feature selection Previous works have claimed that feature selection is capable of selecting the most representative features, thus facilitating subsequent data analysing tasks [14] [15] [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "1 Feature selection Previous works have claimed that feature selection is capable of selecting the most representative features, thus facilitating subsequent data analysing tasks [14] [15] [16].", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "Classical feature selection algorithms, such as Fisher Score [9], evaluate the weights of all features, rank them accordingly and select the most discriminating features one by one [17].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "propose an algorithm which selects features jointly based on spectral regression with l2,1-norm constraint in [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "adopt l2,1-norm on both regularization term and loss function in [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "propose to select features by leveraging shared knowledge from multiple related tasks in [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 16, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 180, "endOffset": 184}, {"referenceID": 24, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 186, "endOffset": 190}, {"referenceID": 25, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 192, "endOffset": 196}, {"referenceID": 26, "context": "propose a manifold learning framework based on graph Laplacian and compared its performance with other state-of-the-art semi-supervised algorithms in [29].", "startOffset": 150, "endOffset": 154}, {"referenceID": 3, "context": "propose a semi-supervised feature selection algorithm built upon manifold learning in [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 27, "context": "In [30], Yang et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "3 Multi-task learning Multi-task learning has been widely used in many applications with the appealing advantage that it learns multiple related tasks with a shared representation [11] [12] [31].", "startOffset": 180, "endOffset": 184}, {"referenceID": 9, "context": "3 Multi-task learning Multi-task learning has been widely used in many applications with the appealing advantage that it learns multiple related tasks with a shared representation [11] [12] [31].", "startOffset": 185, "endOffset": 189}, {"referenceID": 28, "context": "3 Multi-task learning Multi-task learning has been widely used in many applications with the appealing advantage that it learns multiple related tasks with a shared representation [11] [12] [31].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "propose a novel multi-task feature selection algorithm which improves feature selection performance by leveraging shared information among multiple related tasks [6].", "startOffset": 162, "endOffset": 165}, {"referenceID": 4, "context": "In [6], Ma et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Following the works in [5] [6], we adopt the least square loss function for its simplicity and efficiency.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Following the works in [5] [6], we adopt the least square loss function for its simplicity and efficiency.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "Recent works [10] [17] claim that minimizing the regularization term \u2016Wl\u20162,1 makes Wl sparse, which demonstrates that Wl is especially suitable for feature selection.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "Recent works [10] [17] claim that minimizing the regularization term \u2016Wl\u20162,1 makes Wl sparse, which demonstrates that Wl is especially suitable for feature selection.", "startOffset": 18, "endOffset": 22}, {"referenceID": 29, "context": "Motivated by the works in [32] [6], we propose to leverage shared knowledge among multiple related tasks by minimizing the trace norm of W .", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "Motivated by the works in [32] [6], we propose to leverage shared knowledge among multiple related tasks by minimizing the trace norm of W .", "startOffset": 31, "endOffset": 34}, {"referenceID": 27, "context": "We propose to leverage semi-supervised learning by adopting the Laplacian proposed in [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 27, "context": "Inspired by [30], we construct the Laplacian matrix by exploiting both manifold structure and local discriminant information.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "(5) Note that Manifold Regularization is able to explore the manifold structure possessed by multimedia data [29] [33] [34].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "(5) Note that Manifold Regularization is able to explore the manifold structure possessed by multimedia data [29] [33] [34].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "(5) Note that Manifold Regularization is able to explore the manifold structure possessed by multimedia data [29] [33] [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "According to [19] [5], Fl can be obtained as follows:", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "According to [19] [5], Fl can be obtained as follows:", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "Following the work in [5], we incorporate (7) into (6).", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "Following the works in [10] [6] [5], we have:", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Following the works in [10] [6] [5], we have:", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "Following the works in [10] [6] [5], we have:", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "(20) According to Lemma 1 in [6], we have:", "startOffset": 29, "endOffset": 32}, {"referenceID": 32, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 135, "endOffset": 139}, {"referenceID": 34, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 171, "endOffset": 175}, {"referenceID": 35, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 220, "endOffset": 224}, {"referenceID": 7, "context": "3) Feature Selection via Joint l2,1-Norms Minimization (FSNM): Joint l2,1-norm minimization is utilized on both loss function and regularization for joint feature selection [10].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "4) SPEC: It uses spectral graph theory to conduct feature selection [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "5) Feature Selection with Shared Information among multiple tasks (FSSI): It simultaneously learns multiple feature selection functions of different tasks in a joint framework [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 36, "context": "within-class graph and between-class graph [39].", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "7) Structural Feature Selection with Sparsity (SFSS): It combines strengths of joint feature selection and semi-supervised learning into a single framework [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "Following [5], we fix it at 15.", "startOffset": 10, "endOffset": 13}, {"referenceID": 32, "context": "2 Video Classification First, we compare the performances of different algorithms in terms of video classification task using Columbia Consumer Video dataset (CCV) [35].", "startOffset": 164, "endOffset": 168}, {"referenceID": 32, "context": "Since the original videos of this dataset have not been available on the internet, we directly use the STIP features with 5, 000 dimensional BoWs representation provided by [35].", "startOffset": 173, "endOffset": 177}, {"referenceID": 33, "context": "3 Image Annotation We use NUS-WIDE dataset [36] to test the performance of our algorithm.", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "4 Human Motion Recognition We use HMDB video dataset [37] to compare the algorithms in terms of human motion recognition.", "startOffset": 53, "endOffset": 57}, {"referenceID": 37, "context": "claim that motion boundary histograms (MBH) is an efficient way to suppress camera motion in [40] and thus it is used to process the videos.", "startOffset": 93, "endOffset": 97}, {"referenceID": 38, "context": "Following the work in [41] [42], we randomly select 10, 000 samples of two subjects (5, 000 per subject).", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "Following the work in [41] [42], we randomly select 10, 000 samples of two subjects (5, 000 per subject).", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "In this paper, we propose a novel semi-supervised feature selection framework by mining correlations among multiple tasks and apply it to different multimedia applications. Instead of independently computing the importance of features for each task, our algorithm leverages shared knowledge from multiple related tasks, thus, improving the performance of feature selection. Note that we build our algorithm on assumption that different tasks share common structures. The proposed algorithm selects features in a batch mode, by which the correlations between different features are taken into consideration. Besides, considering the fact that labeling a large amount of training data in real world is both time-consuming and tedious, we adopt manifold learning which exploits both labeled and unlabeled training data for feature space analysis. Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence. Extensive experiments on different applications demonstrate that our algorithm outperforms other state-of-the-art feature selection algorithms.", "creator": "LaTeX with hyperref package"}}}