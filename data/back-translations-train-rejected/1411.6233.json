{"id": "1411.6233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "A Convex Sparse PCA for Feature Analysis", "abstract": "Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology and social science. Classical PCA and its variants seek for linear projections of the original variables to obtain a low dimensional feature representation with maximal variance. One limitation is that it is very difficult to interpret the results of PCA. In addition, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a convex sparse principal component analysis (CSPCA) algorithm and apply it to feature analysis. First we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l 2 , 1 -norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. In addition, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. The objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on six different benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.", "histories": [["v1", "Sun, 23 Nov 2014 13:06:43 GMT  (1210kb)", "http://arxiv.org/abs/1411.6233v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "yi yang", "heng huang"], "accepted": false, "id": "1411.6233"}, "pdf": {"name": "1411.6233.pdf", "metadata": {"source": "CRF", "title": "A Convex Sparse PCA for Feature Analysis", "authors": ["Xiaojun Chang", "Feiping Nie", "Yi Yang", "Heng Huang"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 1.62 33v1 [cs.LG] 2 3N ov2 014 JOURNAL OF LATEX CLASS FILES, VOL. 6, NO. 1, JANUARY 2007 1Index Terms - Main Component Analysis, Convex PCA, Sparse PCA, Feature Analysis"}, {"heading": "1 INTRODUCTION", "text": "In many machine learning and data mining applications, such as facial recognition [1] [2], conceptual indexing [3], collaborative filtering [4], and the dimensionality of input data, which is usually very high. Therefore, it is mathematically expensive to analyze high-dimensional data directly, and plays an important role in data mining. PCA typically becomes high [5] [6] [7] due to its simplicity and effectiveness. To improve efficiency and accuracy, researchers have shown that dimensionality reduction is one of the most effective approaches to data analysis, and plays a significant role in data mining. Due to its simplicity and effectiveness, Component Analysis (PCA) is widely applied to various applications. PCA's goal is to find a projection matrix that maximizes the variance of samples after projection while preserving the linear structure of the original projection as high as possible."}, {"heading": "2 RELATED WORK", "text": "In this section we will briefly discuss three related topics of our work, including the classical PCA, sparse PCAs and robust PCAs. First, we define the terms and notations commonly used in this work. (1) Data matrix denoted by X = [x1, x2, \u00b7 \u00b7 \u00b7, xn], where xi-Rd (1 \u2264 i \u2264 n) is the i-th date and n is the total number of samples; (2) Projection matrix denoted by W; (3) the Frobenius standard denoted by E-X-F; (4) the trace standard denoted by E-W-A."}, {"heading": "2.1 The Classical PCA", "text": "Classical PCA is a statistical technique for reducing dimensionality. Classical PCA techniques, also known as Karhunen-Loeve methods, seek linear projection that reduces dimensionality and maximizes the total dispersion of all projected data points. To be more precise, PCA calculates the PCs by performing an intrinsic decomposition of the covariance of the convariance matrix of all training data. Generally, the entries of the corresponding PCs are dense and non-zero. The objective function of classical PCA is max WTW = ITr (W tXXW), where Tr (\u00b7) stands for trace operator."}, {"heading": "2.2 Sparse PCA", "text": "All major components are a linear combination of variables and most factor coefficients are non-zero. In order to obtain better interpretable results, a sparse PCA is proposed, which leads to a reduction in computation time and an improved generalization. There are numerous implementations of sparse PCAs in the literature [15] [16] [17] [15] [18] [19]. The goals of all methods are to reduce dimensionality reduction and the number of explicitly used variables. An easy way is to set factor coefficients manually with values below a threshold of zero. This simple and naive threshold method is often used in different applications. Nevertheless, it could potentially be misleading in different aspects. Jolliffe etal. SCoTLASS suggest to obtain modified major components with possible zero factor coefficients [18]. Lasso [8] has proven to be an effective method for selecting variable aspects that are irrelevant in different aspects."}, {"heading": "2.3 Robust PCA", "text": "The objective of the robust PCA is to restore a matrix D from highly corrupt measurements X = D + E. Errors E should be poorly supported. Motivated by recent research into the robust solution of overdetermined linear equations in the presence of arbitrary but sparse errors and the calculation of low-level matrix solutions for underdetermined linear equations, John etal. [13] proposes an exact restoration of low-level corrupt matrices by convex optimization. A simple solution for robust PCA is to search for the lowest-ranking matrix that may have generated the data under the compulsion of sparse errors. The objective function of robust PCA is formulated as follows: min D-X-D-D-D-0 + -Rank (D) (1) However, since the equation (1) includes the l0 standard, the objective function is highly convex and it is difficult to find a reversible solution to the standard that can be erected."}, {"heading": "3 THE PROPOSED METHOD", "text": "In this section, we first show the equivalence of PCA and regression, then illustrate the formulation of the convex, sparse PCA method, and then describe a detailed approach to solving the objective function."}, {"heading": "3.1 The Equivalence of Classical PCA and Regression", "text": "The proposed CSPCA is based on our recent finding that the classic PCA = \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 TQ (\u03b2 \u03b2 TQ = \u03b2 \u03b2 TQ) can be reformulated. This conclusion provides us with new insights into PCA from a different perspective and allows us to design the new convex-sparse PCA problem as follows: min rank (W) = k-sparse PCA \u2212 X-2F (4) Proof: Since we have the constraint rank (W) = k, we can easily write W = BAT, where A-Rd \u00b7 k is an orthogonal matrix, B-Rd \u00b7 k and the rank of both A and B are k. The above objective function can be rewritten as follows: min A, B-Rd \u00b7 T1PCUD \u00b7 k, ATA = I-ABTX \u2212 X-2F = zero."}, {"heading": "3.2 The Proposed Objective Function", "text": "In this section we describe the proposed objective function of the SCPCA. Motivated by previous work [20] showing that the l2.1 standard of W is capable of making W sparse, we propose our sparse PCA algorithm as follows: min rank (W) = k \u0442 (WTX \u2212 X) T \u00b2 22 + \u03b1 \u00b2 W \u00b2 2.1, (13) where the l2.1 standard of W \u00b2 2.1 = d \u00b2 i = 1 270 \u00b0 n \u00b2 d \u00b2 j = 1W \u00b2 ij \u00b2. In the above function, the lowest quadratic loss function most frequently used is mathematically comprehensible and easily implemented. However, there are still some existing problems that need to be taken further into account. For example, it is known that the least quadratic loss function is very sensitive to outliers [20]. In order to address this problem, it is important for us to adopt a more robust loss function in the objective setting."}, {"heading": "3.3 Optimization", "text": "As can be seen from Equation (16), the proposed algorithm includes the l2,1-norm, which is not smooth and cannot be solved in a closed form. Therefore, we have proposed to solve this problem as follows: For any arbitrary matrix A, we designate A = [A1, \u00b7 \u00b7 \u00b7, display], where d is the number of features. By putting the derivatives w.r.t W on zero, we have D1, D2 and D3 diagonal matrices, which are defined as: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 2: 3: 2: 3: 2: 2: 3: 2: 2: 2: 3: 2: 2: 2: 2: 2: 2: 3: 2: 3: 2: 2: 3: 2: 3: 2: 3: 3: 3: 3: 3: 3: 3: 3: 3: 4: 3: 3: 4: 3: 4: 4: 4: 3, 4: 3: 4: 4: 4: 3: 4: 4: 4: 4: 4: 4: 4: 3, 4: 4: 2: 2: 2: 2: 2: 2: 2: 2: 2: 3: 2: 3: 3: 3: 3: 3: 3: 3: 3: 3: 4: 4: 3: 4: 4: 4: 4: 3: 4: 4: 3: 4: 4: 4: 4: 4: 4: 4: 4: 4: 3: 4: 4: 3: 4: 3: 3: 3: 3: 4: 3: 4: 4: 4: 4: 4: 3: 3: 4: 3: 3: 4: 3: 4: 3: 4: 3: 4: 3: 3: 4: 3: 4: 3: 4: 4: 4: 4: 3: 4: 3: 4: 3: 4: 4: 4: 3: 4: 4: 3: 3: 4: 4: 3: 3: 3: 4: 4: 4: 3: 3: 3: 3: 3: 3:"}, {"heading": "3.4 Convergence Analysis", "text": "In this section we confirm that the objective function value approaches the optimal W value by the following theorems (+ 1).The objective function value represented in Eq + 1 (16) takes monotonously in each iteration until convergence using the iterative approach in Algorithm 1.Algorithm 1: Algorithm to solve the problem in (16) Data: Data matrix X parameters, \u03b2Result: W 1 sentence t = 0; 2 Initials W0 Rd \u00b7 c randomly; 3 Repeat 4 Compute Et according to Et = (W T t X \u2212 X \u2212 X) T; 5 Calculation of the diagonal matrix D1t as follows: D1t = 1 2 e1t = 2 e1t."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we evaluate the performance of the proposed algorithm, which is applicable to many applications, such as dimension reduction and unattended feature selection. Following earlier, unattended feature selection algorithms [22] [23] [24], we evaluate only the performance of CSPCA for feature selection and compare it to related, unattended feature selections."}, {"heading": "4.1 Experimental Settings", "text": "To demonstrate the effectiveness of the proposed algorithm for function selection, we compare it with a baseline and several unattended function selection methods. The compared algorithms are described as follows: 1) Use of all features (All-Fea): We directly apply the original features without performing the function selection. This approach is used as the baseline. 2) Max Variance: This is a function selection method using the classic PCA criteria. Characteristics with maximum variances are selected for subsequent tasks. 3) Laplacian Score: In order to best obtain the local multiplicity structure, characteristics that are consistent with the Gaussian laplactic matrix are selected [22]. The importance of each feature is determined by its performance. 4) SPEC: This is a spectral regression selection algorithm based on the art function selection algorithm. Characteristics are selected one by one by selecting the work of the selection theory Ftr5) based on M23)."}, {"heading": "4.2 Datasets", "text": "The datasets used in our experiments are described as follows: 1) Facial image data: We use three facial image data sets for facial recognition, namely YaleB [26], ORL [27] and JAFFE [28]. The YaleB dataset contains 2414 close-ups of 38 people under different illumination. We resize each image to 32 x 32. The ORL dataset consists of 40 different subjects, each with 10 images. 2) 3DMotion data: The HumanEVA dataset is used to evaluate the performance of our algorithm in terms of 3D motion annotation (JAFFE). This dataset contains five types of motion. Based on the 16 common coordinates in 3D space, 1590 geometric object descriptions are used."}, {"heading": "4.3 Evaluation Metrics", "text": "Following the associated uncontrolled selection work [22], we adopt cluster accuracy (ACC) and 1. http: / / vision.cs.brown.edu / humaneva / normalized mutual information (NMI) as our evaluation metrics in our experiments. Let Qi represent the cluster labeling result of a cluster algorithm and Pi the corresponding ground truth label of any data point xi. Then, ACC is defined as follows: ACC = \u2211 n = 1 \u03b4 (pi, map (qi) n, (18), where \u03b4 (x, y) = 1 if x = y and \u03b4 (x, y) = 0 otherwise. map (qi) is the best mapping function that performs cluster labels in such a way that it coincides with the ground truth labels, using the Kuhn-Munkres algorithm. A larger ACC shows a better cluster performance at (x, function), cluster = 0, that the mapping algorithm corresponds with the munkard."}, {"heading": "4.4 Experimental Results", "text": "Empirical studies are carried out on six real data sets to validate the performance of the proposed algorithm and compare it with state-of-the-art algorithms. Table 2 and Table 3 summarize the ACC and NMI comparison results of all the compared algorithms across the data sets used. The experimental results result in the following observations. 1) The feature selection algorithms generally perform better than the basic All-Fea model, which shows that feature selection is necessary and effective. It can both significantly reduce the number of features and improve performance. 2) Both SPEC and MCFS use a two-step approach (spectral regression) for feature selection. The difference between them is that MCFS selects features in a batch mode, but SPEC performs this task separately. We can see that MCFS achieves better results than SPEC because it is a better method of collectively selecting characteristics for DFN analysis, which means that we can discriminate the second characteristics from the second result. 3)"}, {"heading": "4.5 Influence of Selected Features", "text": "Since the goal of character selection is to increase accuracy and computational efficiency, experiments are conducted to determine how the number of features selected can affect cluster performance, resulting in the general trade-off between performance and computational efficiency across the entire dataset used. Figure 1 rightly shows the performance variance in the number of features selected relative to ACC's cluster selection. The results reveal the following observations: 1) If the number of features selected is too small, ACC's cluster selection is not competitive in using all features without feature selection, mainly caused by too much information loss. For example, if only 500 features are selected on YaleB, ACC's cluster selection is relatively low at just 0.164. 2) As the number of selected features increases, ACC's cluster selection increases before reaching its peak in the generality of all data sets used."}, {"heading": "4.6 Parameter Sensitivity", "text": "Our proposed algorithm comprises two regularization parameters, which are referred to as \u03b1 and \u03b2 in Equation (16). It is advantageous to learn how they influence the selection of characteristics and thus the performance on the cluster. In this section, we perform several experiments on parameter sensitivity. We use the ACC cluster to reflect the performance variation. Figure 2 demonstrates the cluster variation w.r.t \u03b1 and \u03b2 on the six data sets. From this figure, we learn that the cluster performance changes according to different combinations of \u03b1 and \u03b2. The influence of different combinations of regularization parameters should be related to the individual properties of the data sets. We can observe from the data sets used that better experimental results are achieved when the two regularization parameters \u03b1 and \u03b2 are comparable."}, {"heading": "4.7 Performance Variance w.r.t Different Initializations", "text": "In this section, experiments are performed to assess how different the performance is for different initializations. Clustering ACC is also used to reflect the variation in performance; the Kmeans algorithm has adopted the same initialization; we perform various initializations, including setting all diagonal elements from W to 0.5 (1st initialization), 1 (2nd initialization), 2 (3rd initialization), setting all elements from W to 0.5 (4th initialization), 1 (5th initialization), 2 (6th initialization), and random values (7th initialization). Experimental results are presented in Table 4.From the experimental results, we can conclude that the proposed algorithm always achieves a global optimum."}, {"heading": "4.8 Convergence Study", "text": "It is interesting to know how fast our algorithm converges. In this section we perform several experiments to validate the convergence of the proposed algorithm. We fix the two regularization parameters \u03b1 and \u03b2 at 1, the mean value of the range from which the regularization parameters are adjusted. Figure 3 shows the convergence curves of the proposed algorithm according to the objective function value in equivalents. (16) From these figures we can conclude that the objective function value converges quickly. Specifically, the proposed algorithm can converge within 10 iterations on all data sets used, which is very efficient."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have proposed a novel convex, sparse PCA and applied it to feature analysis. First, we prove that PCA can be formulated as a low-ranking regression optimization problem. Furthermore, we integrate the l2,1 standard minimization into the proposed algorithm to make the regression coefficients sparse and to make the model robust for outliers. In contrast to the state-of-the-art robust PCA, the proposed algorithm is capable of solving out-of-sample problems. Furthermore, we propose an efficient algorithm for optimizing the lens function. To validate the performance of our feature analysis algorithm, we conduct experiments on six real-world clustering datasets. Experimental reuses show that the proposed algorithm outperforms the other, unmonitored feature selection according to the current state of the art as well as the baseline using all features."}], "references": [{"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "venue": "IEEE Trans. PAMI, vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A novel incremental principal component analysis and its application for face recognition", "author": ["P.C.Y. Haitao Zhao", "J. Kwok"], "venue": "IEEE Trans. Systems, Man and Cybernetics, Part B (Cybernetics), 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "On effective conceptual indexing and similarity search in text data", "author": ["C.C. Aggarwal", "P.S. Yu"], "venue": "Proc. ICDM, 2001, pp. 3\u201310.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-task learning for bayesian matrix factorization", "author": ["C. Yuan"], "venue": "Proc. ICDM, 2011, pp. 924\u2013931.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning linear discriminant projections for dimensionality reduction of image descriptors", "author": ["H. Cai", "K. Mikolajczyk", "J. Matas"], "venue": "Trans. PAMI, pp. 338\u2013352, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A framework for feature selection in clustering", "author": ["D.M. Witten", "R. Tibshirani"], "venue": "Journal of the American Statistical Association, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualization of learning in multilayer perceptron networks using principal component analysis", "author": ["M. Gallagher", "T. Downs"], "venue": "IEEE Trans. Systems, Man and Cybernetics, Part B (Cybernetics), 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics, vol. 15, no. 2, pp. 265\u2013286, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301\u2013320, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust principal component analysis by self-organizing rules based on statistical physics approach", "author": ["L. Xu", "A.L. Yuille"], "venue": "IEEE Trans. Neural Networks, vol. 6, no. 1, pp. 131\u2013143, 1995.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization", "author": ["J. Wright", "Y. Peng", "Y. Ma", "A. Ganesh", "S. Rao"], "venue": "Proc. NIPS, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "Proc. NIPS, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. d\u2019Aspremont", "L.E. Ghaoui", "M.I. Jordan", "G.R. Lanckriet"], "venue": "SIAM review, vol. 49, no. 3, pp. 434\u2013448, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Full regularization path for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F.R. Bach", "L.E. Ghaoui"], "venue": "Proc. ICML, 2007, pp. 177\u2013184.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral bounds for sparse pca: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Proc. NIPS, 2005, pp. 915\u2013922.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "A modified principal component technique based on the lasso", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics, vol. 12, no. 3, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["H. Shen", "J.Z. Huang"], "venue": "Journal of multivariate analysis, vol. 99, no. 6, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient and robust feature selection via joint l2,1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C.H. Ding"], "venue": "Proc. NIPS, 2010, pp. 1813\u20131821.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Proc. NIPS, 2005.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "Proc. ICML, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "l2,1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "Proc. AAAI, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "Proc. ACM KDD, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. PAMI, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Proc. Applications of Computer Vision, 1994, pp. 138\u2013142.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "Automatic classification of single facial images", "author": ["M.J. Lyons", "J. Budynek", "S. Akamatsu"], "venue": "IEEE Trans. PAMI, vol. 21, no. 12, pp. 1357\u20131362, 1999.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning a 3d human pose distance metric from geometric pose descriptor", "author": ["C. Chen", "Y. Zhuang", "F. Nie", "Y. Yang", "F. Wu", "J. Xiao"], "venue": "IEEE Trans. Visualization and Computer Graphics, vol. 17, no. 11, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "CUCS-005-96, Columbia University, Tech. Rep., 1996.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Machine Learning Research, vol. 3, pp. 583\u2013617, 2003.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "1 INTRODUCTION In many machine learning and data mining applications, such as face recognition [1] [2], conceptual indexing [3], collaborative filtering [4], the dimensionality of the input data is usually very high.", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "Meanwhile, the noise in a representation may dramatically increase as the dimensionality is getting high [5] [6] [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Meanwhile, the noise in a representation may dramatically increase as the dimensionality is getting high [5] [6] [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Meanwhile, the noise in a representation may dramatically increase as the dimensionality is getting high [5] [6] [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "integrate the lasso penalty [8], which is a variable selection technique, into the regression criterion in [9].", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "integrate the lasso penalty [8], which is a variable selection technique, into the regression criterion in [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 9, "context": "Lasso penalty is implemented via elastic net, which is a generalization of lasso proposed in [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "[12] propose to recover a low-rank matrix from highly corrupted measurements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Therefore, the robust PCA algorithm proposed in [12] is less practical for many real world", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "It has been demonstrated in [9] that the sparse model is a good measure for feature analysis, especially for feature weighting.", "startOffset": 28, "endOffset": 31}, {"referenceID": 11, "context": "Different from [13], our algorithm is inductive and can be directly used to map the unseen data which are outside the training set.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "3) Different from the existing robust PCA algorithms [13] [14], which can only deal with the in-sample data, our algorithm is capable of mapping the data which are unseen during the training phase.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "3) Different from the existing robust PCA algorithms [13] [14], which can only deal with the in-sample data, our algorithm is capable of mapping the data which are unseen during the training phase.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "There are numerous implementations of sparse PCA in the literature [15] [16] [17] [15] [18] [19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "propose SCoTLASS to obtain modified principal components with possible zero factor coefficients [18].", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "Lasso [8] has shown to be a effective variable selection method, which has been shown effective in a variety of applications.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "propose the elastic net in [10] for sparsity based mining.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "[9] propose sparse PCA (SPCA) for estimating PCs with sparse factor coefficients, which can be formulated as follows:", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[13] propose exact recovery of corrupted low-rank matrices by convex optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "To make the objective function robust to outliers, we further replace l1-norm with l2,1-norm as l2,1-norm is indicated to make the objective function robust to outliers in [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "The connection between the stated Theorem 1 and Theorem 2 in [9]: Zou etal.", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "Compared with Theorem 2 proposed in [9], our contribution is that we prove that when \u03bb = 0, PCA problem is completely equivalent to a regression-type problem.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Motivated by previous work [20], which demonstrate that l2,1-norm of W is capable of making W sparse, we propose our sparse PCA algorithm as follows:", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "For example, it is well known that the least square loss function is very sensitive to outliers [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "In [20], Nie etal.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In the above formulation, the loss function \u2016WX\u2212 X\u20162,1 is robust to outliers, as proven in [20].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "Following the work in [13] [21], we restrict W to be a low rank matrix.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Different from the previous robust PCA algorithms [13] [14], the proposed algorithm is inductive, and able to deal with the out-of-sample data which are unseen in the training phase.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "Different from the previous robust PCA algorithms [13] [14], the proposed algorithm is inductive, and able to deal with the out-of-sample data which are unseen in the training phase.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "It has been proven in [20] that for arbitrary non-zero vectors v t| r i=1 we have: \u2211", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "Following previous unsupervised feature selection algorithms [22] [23] [24], we only evaluate the performance of CSPCA for feature selection and compare with related state-of-the-art unsupervised feature selection.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Following previous unsupervised feature selection algorithms [22] [23] [24], we only evaluate the performance of CSPCA for feature selection and compare with related state-of-the-art unsupervised feature selection.", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "Following previous unsupervised feature selection algorithms [22] [23] [24], we only evaluate the performance of CSPCA for feature selection and compare with related state-of-the-art unsupervised feature selection.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "3) Laplacian Score: To best preserve the local manifold structure, feature consistent with Gaussian Laplacian matrix are selected [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "Features are selected one by one by leveraging the work of spectral graph theory [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "5) MCFS: Features are selected based on spectral analysis and sparse regression problem [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "6) UDFS: Features are selected by a joint framework of discriminative analysis and l2,1-norm minimization [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "1) Face Image Data: We use three face image datasets for face recognition, namely YaleB [26], ORL [27] and JAFFE [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "1) Face Image Data: We use three face image datasets for face recognition, namely YaleB [26], ORL [27] and JAFFE [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "1) Face Image Data: We use three face image datasets for face recognition, namely YaleB [26], ORL [27] and JAFFE [28].", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "Based on the 16 joint coordinates in 3D space, 1590 geometric pose descriptors are extracted using the method proposed in [29] to represent 3D motion data.", "startOffset": 122, "endOffset": 126}, {"referenceID": 27, "context": "3) Object Image Data: We use the Coil20 dataset [30] for object recognition.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "3 Evaluation Metrics Following related unsupervised feature selection work [22] , we adopt clustering accuracy (ACC) and", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": "For any two arbitrary variable P and Q, NMI is defined as follows [31]: NMI = I(P,Q) \u221a H(P )H(Q) , (19)", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "NMI metric is then computed as follows [31]:", "startOffset": 39, "endOffset": 43}], "year": 2014, "abstractText": "Principal component analysis (PCA) has been widely applied to dimensionality reduction and data pre-processing for different applications in engineering, biology and social science. Classical PCA and its variants seek for linear projections of the original variables to obtain a low dimensional feature representation with maximal variance. One limitation is that it is very difficult to interpret the results of PCA. In addition, the classical PCA is vulnerable to certain noisy data. In this paper, we propose a convex sparse principal component analysis (CSPCA) algorithm and apply it to feature analysis. First we show that PCA can be formulated as a low-rank regression optimization problem. Based on the discussion, the l2,1-norm minimization is incorporated into the objective function to make the regression coefficients sparse, thereby robust to the outliers. In addition, based on the sparse model used in CSPCA, an optimal weight is assigned to each of the original feature, which in turn provides the output with good interpretability. With the output of our CSPCA, we can effectively analyze the importance of each feature under the PCA criteria. The objective function is convex, and we propose an iterative algorithm to optimize it. We apply the CSPCA algorithm to feature selection and conduct extensive experiments on six different benchmark datasets. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art unsupervised feature selection algorithms.", "creator": "LaTeX with hyperref package"}}}