{"id": "1411.6235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Balanced k-Means and Min-Cut Clustering", "abstract": "Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms.", "histories": [["v1", "Sun, 23 Nov 2014 13:16:25 GMT  (822kb)", "http://arxiv.org/abs/1411.6235v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "zhigang ma", "yi yang"], "accepted": false, "id": "1411.6235"}, "pdf": {"name": "1411.6235.pdf", "metadata": {"source": "CRF", "title": "Balanced k-Means and Min-Cut Clustering", "authors": ["Xiaojun Chang", "Feiping Nie", "Zhigang Ma", "Yi Yang"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 1.62 35v1 [cs.LG] 2 3N ov2 014 1Index Terms - Balanced k-Means, Min-Cut Clustering"}, {"heading": "1 INTRODUCTION", "text": "C LUSTERING is a fundamental research topic in Data Miningand is widely used in many applications in the field of artificial intelligence, statistics and social sciences. [1] [2] [3] [4] [5] [6] [10]. The goal of clustering is to divide the original data points into a number of groups so that the data points within the same cluster are close together, while those in different clusters are far apart [7] [9]. Among different approaches to clustering, two most popular decisions in reality are due to their simplicity and effectiveness. The general procedure of traditional K-Means (TKM) is to randomly initiate cluster centers that assign their nearest clusters to each data point and calculate a new cluster center iteratively. Some researchers claim that the curse of dimensionality could worsen the performance of TKM."}, {"heading": "2 RELATED WORK", "text": "In this section we briefly review the research on k-mean, min-cut and the exclusive lasso.2.1 The classic K-MeansAs one of the most efficient clustering algorithms, k-mean clustering has been widely applied to real-world applications.The centroids of the clusters are used to characterize the data.The goal of k-mean is to minimize the sum of square errors defined by: Jk = K-mean cluster.Previous work [19] has shown that H-orthogonal non-negative matrix factorization (NMF) is synonymous with loose k-mean clustering."}, {"heading": "2.2 Min-Cut", "text": "The principle of min-cut is rooted in graph theory. It requires a graph based on a weight matrix q = q = q = n, constructed from n data points {x1,.., xn}. The min-cut Graph Clustering Lens function can therefore be generalized as: J = 1 \u2264 p < q \u2264 Ks (Cp, Cq) + s (Cp, Cq) = K \u2211 k = 1s (Ck, Ck) (3) where K is the number of clusters, Ck is the k-th cluster (subgraph in G), Ck is the addition of a subset of Ck in Diagram G, and for each set of A \u2212 and Bs (A, B) = 1, where K is the number of clusters, (4) di = jWij. (5) We designate qk (k = 1,., K) as cluster indicators where the i-element is set by k to k."}, {"heading": "2.3 Exclusive Lasso", "text": "Zhou et al. propose the Exclusive Lasso to model the scenario when variables of the same group compete with each other, applying it to the selection of multiple functions and achieving good performance. The Exclusive Lasso [18] is defined as follows: E-\u03b2-e = N-N-N-J = 1 (M-K = 1-E-J) 2, (9) where E-\u03b2-E is a regulator that controls the complexity of combination weights. In [18] the regulator introduces an l1 standard to combine weights for the same category used by different data points, and an l2 standard to combine weights of different categories. Since the l1 standard tends to achieve a sparse solution, the construction in the Exclusive Lasso essentially introduces a competition between different categories for the same data.In our work, the Exclusive Lasso is used as an equilibrium limitation of our clusivity."}, {"heading": "3 THE PROPOSED ALGORITHM", "text": "In this section, we explain the proposed approach in detail."}, {"heading": "3.1 Balance Constraint", "text": "We have the ability to recalculate the number of records in each class, that most balanced clusters can be achieved by using the exclusive lasso.Theorem 1. Considering the fact that we can minimize n2 + n2 + n2 the number of records in each class, it is possible that most balanced clusters can be achieved by using the exclusive lasso.Theorem 1. Considering the fact that n2 + n2 increases the number of records in each class, it comes to its minimum when ni der.Proof. According to the Cauchy inequality, (n21 + n 2 + n2 k.)"}, {"heading": "3.3 Balanced Min-Cut", "text": "Similarly, we strive to integrate n data points X = {x1,.., xn}, xxx \u00b2, into K clusters. First, we use the Gaussian function to construct a weight matrix A. The weight matrix Aij is defined as: Aij = exp (\u2212 xi \u2212 xj \u00b2 2 \u03b42), xi and xj are closest neighbours. 0, otherwise (19) the weight matrix A and the cluster indicator matrix F are formulated as follows: min F \u2212 FT \u2212 TA1 \u2212 Tr (FTAF \u2212 Tr), which corresponds to the following objective function: max F \u2212 IndTr (FTAF), (21) The objective function of the min-cut diagram is formulated as follows: min F \u2212 Ind1TA1 \u2212 TA1 \u2212 Tr (FTAF \u2212 Tr), which corresponds to the following objective function: max F \u2212 IndTr (FTAF)."}, {"heading": "4 EXPERIMENT", "text": "In this section, extensive experiments will be conducted to evaluate the proposed cluster methods. We present two sets of experiments: The first is to compare the proposed balanced clustering of K funds with K funds-based cluster algorithms, including the classic clustering of K funds (KM), DisClusters (DC), DisKmeans (DKM) clusters [15], AKM [19] and HKM [19]; the second is to compare the proposed balanced clustering of min-cut with the classic MinMax-cut clusters, MinMax-cut clusters, Ratio-cut clusters and normalized-cut cluster algorithms."}, {"heading": "4.1 Datasets", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "4.2 Parameter Setting", "text": "There are three parameters in our algorithms: the first is the number of closest neighbours and the second is the parameter \u03b4 in Equation (19). In the following, we set the number of closest neighbours in the experiments to 5. To determine this parameter, the self-tuning method is used. We adjust it for the regularization parameter \u03b3 in Equation (13) and Equation (24) by means of a grid search strategy of {10 \u2212 6, 10 \u2212 4, 10 \u2212 2, 100, 102, 104, 106}. We also adjust the regularization parameters of all comparison algorithms in the above range. The best results of all comparison algorithms are reported."}, {"heading": "4.3 Evaluation Metrics", "text": "Following our work, we adopt clustering accuracy (ACC) and normalized mutual information (NMI) as our benchmarks in our experiments. Let us render the clustering label result from a clustering algorithm and pi represent the corresponding Ground Truth label from any data points. Then, ACC is defined as follows: 6ACC = 6ACC = 600000 (pi, map (qi)) n, (26) where it is -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% 10% -10% 10% -10% -10% 10% -10% 10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -10% -"}, {"heading": "4.5 Comparison among graph clustering algorithms", "text": "To evaluate the performance of the proposed balanced cluster algorithm, we compare it with classic min-cut clustering, MinMax cut clustering [31], ratio cut clustering [32], normalized cut clustering [33], and balanced min-cut clustering on the nine benchmark datasets. The experimental results yield the following observations: 1) Compared to k-mean-based clustering, cluster algorithms generally perform better. This observation indicates that it is advantageous to use the pair-wise similarities between all data points from a weighted graph adaptation matrix that contains a lot of helpful information for clustering. 2) MinMax cut clustering always achieves the second best performance, showing that the min-max clustering principle can lead to more balanced partitions than the other comparison diagram clustering methods. 3) The proposed balanced min-cut clustering algorithm consistently outperforms the other cluster algorithms."}, {"heading": "4.6 Parameter Sensitivity of the Proposed Algorithm", "text": "In this section, we examine the parameter sensitivity of balanced mean values and balanced minimum sections. Figure 1 shows the accuracy (y-axis) of balanced k averages for different \u03b3 values (x-axis). Based on the test result, we can determine that \u03b3 has a significant influence on the performance of balanced K averages. In addition, we show the parameter sensitivity of balanced minimum values in Figure 2. Similar to the proposed balanced k mean 9, the performance is strongly influenced by the parameter \u03b3. More precisely, a better performance is usually achieved when \u03b3 is in the range of {10 \u2212 2, 102}. Experiments on both algorithms suggest the importance of designing a method for automatic adjustment of the parameter selection."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have addressed the problem of the balanced cluster, which has not been studied in data mining. In particular, we have used the exclusive lasso to exercise the equilibrium constraint to introduce its ability to trigger competition between different categories for the same data point. In particular, we have integrated the exclusive lasso into k-medium and min-cut cluster algorithms, which are designed to make it easier for these two mainstream cluster algorithms to better deal with balanced data points. On the other hand, our objective functions are not smooth and difficult to optimize. A new iterative approach is then intended to solve the problems. We have conducted extensive experiments on a variety of data sets to evaluate the performance of the proposed balanced k-mean and balanced courage in terms of cluster accuracy and NMI. Experimental results show that our proposed algorithms always outperform the other state-of-the-art comparison algorithms, confirming that the use of the exclusive cluster actually contributes to achieving the desired result."}], "references": [{"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys, vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Algorithms for clustering data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Mercer kernel-based clustering in feature space", "author": ["M. Girolami"], "venue": "IEEE Trans. Neural Networks, vol. 13, no. 3, pp. 780\u2013784, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient clustering aggregation based on data fragments", "author": ["O. Wu", "W. Hu", "S.J. Maybank", "M. Zhu", "B. Li"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 42, no. 3, pp. 913\u2013926, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Coupled clustering ensemble: Incorporating coupling relationships both between base clusterings and objects", "author": ["C. Wang", "Z. She", "L. Cao"], "venue": "Proc. ICDE, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive distance metric learning for clustering", "author": ["J. Ye", "Z. Zhao", "H. Liu"], "venue": "Proc. CVPR, 2007, pp. 1\u20137.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithms for clustering data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": "Englewood Cliffs,NJ: Prentice-Hall, 1988.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Initialization independent clustering with actively self-training method", "author": ["F. Nie", "D. Xu", "X. Li"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 42, no. 1, pp. 17\u201327, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A convex formulation for shrunk spectral clustering", "author": ["X. Chang", "F. Nie", "Z. Ma", "Y. Yang", "X. Zhou"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, January 2530, 2015, Austin Texas, USA., 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern recognition, vol. 41, no. 1, pp. 176\u2013190, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning bregman distance functions for semi-supervised clustering", "author": ["L. Wu", "S.C. Hoi", "R. Jin", "J. Zhu", "N. Yu"], "venue": "IEEE Trans. Knowledge and Data Eng., vol. 24, no. 3, pp. 478\u2013491, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive dimension reduction using discriminant analysis and k-means clustering", "author": ["C. Ding", "T. Li"], "venue": "Proc. ICML, 2007, pp. 521\u2013528.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Discriminative cluster analysis", "author": ["D. la Torre", "Fernando", "T. Kanade"], "venue": "Proc. ICML, 2006, pp. 241\u2013248.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive distance metric learning for clustering", "author": ["J. Ye", "Z. Zhao", "H. Liu"], "venue": "Proc. CVPR, 2007, pp. 1\u20137.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Discriminative k-means for clustering", "author": ["J. Ye", "Z. Zhao", "M. Wu"], "venue": "Proc. NIPS, 2007, pp. 1649\u20131656.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "A spatially continuous maxflow and min-cut framework for binary labeling problems", "author": ["J. Yuan", "E. Bae", "X.-C. Tai", "Y. Boykov"], "venue": "Numerische Mathematik, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "An information-theoretic derivation of mincut-based clustering", "author": ["A. Raj", "C.H. Wiggins"], "venue": "IEEE PAMI, vol. 32, no. 6, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Exclusive lasso for multi-task feature selection", "author": ["Y. Zhou", "R. Jin", "S. Hoi"], "venue": "Proc. ICAIS, pp. 988\u2013995, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast approximate k-means via cluster closures", "author": ["J. Wang", "J. Wang", "Q. Ke", "G. Zeng", "S. Li"], "venue": "Proc. CVPR, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "K-means clustering algorithm with improved initial center", "author": ["C. Zhang", "S. Xia"], "venue": "Proc. Knowledge Discovery and Data Mining, 2009, pp. 790\u2013792.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "On k-means cluster preservation using quantization schemes", "author": ["D.S. Turaga", "M. Vlachos", "O. Verscheure"], "venue": "Proc. ICDM, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Flexible constrained spectral clustering", "author": ["X. Wang", "I. Davidson"], "venue": "Proc. KDD, 2010, pp. 563\u2013572.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast incremental minimum-cut based algorithm for graph clustering", "author": ["B. Saha", "P. Mitra"], "venue": "Proc. ICDM, pp. 207\u2013211, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. PAMI, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Proc. Applications of Computer Vision, 1994, pp. 138\u2013142.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Automatic classification of single facial images", "author": ["M.J. Lyons", "J. Budynek", "S. Akamatsu"], "venue": "IEEE Trans. PAMI, vol. 21, no. 12, pp. 1357\u20131362, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning a 3d human pose distance metric from geometric pose descriptor", "author": ["C. Chen", "Y. Zhuang", "F. Nie", "Y. Yang", "F. Wu", "J. Xiao"], "venue": "IEEE Trans. Visualization and Computer Graphics, vol. 17, no. 11, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "CUCS-005-96, Columbia University, Tech. Rep., 1996.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Machine Learning Research, vol. 3, pp. 583\u2013617, 2003.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "A min-max cut algorithm for graph partitioning and data clustering", "author": ["C.H. Ding", "X. He", "H. Zha", "M. Gu", "H.D. Simon"], "venue": "Proc. ICDM, 2001.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "New spectral methods for ratio cut partitioning and clustering", "author": ["L. Hagen", "A.B. Kahng"], "venue": "Trans. Computer-aided design of integrated circuits and systems, 1992.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Trans. PAMI, pp. 888\u2013905, 2000.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].", "startOffset": 187, "endOffset": 190}, {"referenceID": 1, "context": "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 2, "context": "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].", "startOffset": 195, "endOffset": 198}, {"referenceID": 3, "context": "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].", "startOffset": 199, "endOffset": 202}, {"referenceID": 4, "context": "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].", "startOffset": 203, "endOffset": 206}, {"referenceID": 5, "context": "1 INTRODUCTION C LUSTERING is a fundamental research topic in data mining and is widely used for many applications in the field of artificial intelligence, statistics and social sciences [1] [2] [3] [4] [5] [6].", "startOffset": 207, "endOffset": 210}, {"referenceID": 6, "context": "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].", "startOffset": 228, "endOffset": 231}, {"referenceID": 7, "context": "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].", "startOffset": 232, "endOffset": 235}, {"referenceID": 8, "context": "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].", "startOffset": 236, "endOffset": 239}, {"referenceID": 9, "context": "The objective of clustering is to partition the original data points into a number of groups so that the data points within the same cluster are close to each other while those in different clusters are far away from each other [7] [8] [9] [10].", "startOffset": 240, "endOffset": 244}, {"referenceID": 10, "context": "Among various approaches for clustering, K-means and mincut are two most popular choices in reality because of their simplicity and effectiveness [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "Some researchers claim that the curse of dimensionality may deteriorate the performance of TKM [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Discriminative analysis has been shown effective in enhancing clustering performance [12] [13] [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Discriminative analysis has been shown effective in enhancing clustering performance [12] [13] [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "Discriminative analysis has been shown effective in enhancing clustering performance [12] [13] [14].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "Motivated by this fact, discriminative k-means (DKM) [15] is proposed to incorporate discriminative analysis and clustering into a single framework to formalize the clustering as a trace maximization problem.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "By contrast, the min-cut clustering is realized by constructing a weighted undirected graph and then partitioning its vertices into two sets so that the total weight of the set of edges with endpoints in different sets is minimized [16] [17].", "startOffset": 232, "endOffset": 236}, {"referenceID": 16, "context": "By contrast, the min-cut clustering is realized by constructing a weighted undirected graph and then partitioning its vertices into two sets so that the total weight of the set of edges with endpoints in different sets is minimized [16] [17].", "startOffset": 237, "endOffset": 241}, {"referenceID": 17, "context": "[18] has been exploited in our approach to fulfill such purpose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Previous work [19] has shown that H-orthogonal non-negative matrix factorization (NMF) is equivalent to relaxed k-means clustering.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "[19] propose a harmony K-means (HKM) algorithm based on harmony search optimization method and applied it to document clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] propose a new neighborhood density method for selecting initial cluster centers for K-means clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] employ quantization schemes to retain the outcome of clustering operations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] propose a flexible and generalized framework for constrained spectral clustering, interpret the algorithm as finding the normalized min-cut of a labeled graph, and apply it to constrained image segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Dynamic graph clustering algorithm, proposed by [23] can provide strong theoretical quality guarantee on clusters.", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "The exclusive lasso [18] is defined as follows: \u2016\u03b2\u2016e = \u221a\u221a\u221a\u221a d \u2211", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "In [18], the regularizer introduces an l1-norm to combine the weights for the same category used by different data points and an l2-norm to combine the weights of different categories.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "The first one is to compare the proposed balanced K-means clustering to K-means based clustering algorithms, including the classical K-means (KM) clustering, DisCluster (DC), DisKmeans (DKM) clustering [15], AKM [19] and HKM [19].", "startOffset": 202, "endOffset": 206}, {"referenceID": 18, "context": "The first one is to compare the proposed balanced K-means clustering to K-means based clustering algorithms, including the classical K-means (KM) clustering, DisCluster (DC), DisKmeans (DKM) clustering [15], AKM [19] and HKM [19].", "startOffset": 212, "endOffset": 216}, {"referenceID": 18, "context": "The first one is to compare the proposed balanced K-means clustering to K-means based clustering algorithms, including the classical K-means (KM) clustering, DisCluster (DC), DisKmeans (DKM) clustering [15], AKM [19] and HKM [19].", "startOffset": 225, "endOffset": 229}, {"referenceID": 23, "context": "1) MNIST Handwritten Digit Dataset: The MNIST handwritten digit dataset [24] is a large-scale dataset of handwritten digits.", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "3) YaleB face dataset: The YaleB dataset [25] contains 2414 near frontal images from 38 persons under different illuminations.", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "4) ORL face dataset: The ORL dataset [26] consists of 40 different subjects with 10 images each.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "5) JAFFE Japanese Female Facial Expression dataset: The JAFFE dataset [27] consists of 213 images of different facial expressions from 10 different Japanese female models.", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Based on the 16 joint coordinates in 3D space, 1590 geometric pose descriptors are extracted using the method proposed in [28] to represent 3D motion data.", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "7) Coil20 Object dataset: We use the Coil20 dataset [29] for object recognition.", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "For any two arbitrary variable P and Q, NMI is defined as follows [30]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 29, "context": "NMI metric is then computed as follows [30]:", "startOffset": 39, "endOffset": 43}, {"referenceID": 30, "context": "To evaluate performance of the proposed balanced min-cut clustering algorithm, we compare it to the classical Min-Cut clustering, MinMax Cut clustering [31], Ratio Cut clustering [32], Normalized Cut Clustering [33] and Balanced Min-Cut clustering on the nine benchmark datasets.", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "To evaluate performance of the proposed balanced min-cut clustering algorithm, we compare it to the classical Min-Cut clustering, MinMax Cut clustering [31], Ratio Cut clustering [32], Normalized Cut Clustering [33] and Balanced Min-Cut clustering on the nine benchmark datasets.", "startOffset": 179, "endOffset": 183}, {"referenceID": 32, "context": "To evaluate performance of the proposed balanced min-cut clustering algorithm, we compare it to the classical Min-Cut clustering, MinMax Cut clustering [31], Ratio Cut clustering [32], Normalized Cut Clustering [33] and Balanced Min-Cut clustering on the nine benchmark datasets.", "startOffset": 211, "endOffset": 215}], "year": 2014, "abstractText": "Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms.", "creator": "LaTeX with hyperref package"}}}