{"id": "1411.6241", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Improved Spectral Clustering via Embedded Label Propagation", "abstract": "Spectral clustering is a key research topic in the field of machine learning and data mining. Most of the existing spectral clustering algorithms are built upon Gaussian Laplacian matrices, which are sensitive to parameters. We propose a novel parameter free, distance consistent Locally Linear Embedding. The proposed distance consistent LLE promises that edges between closer data points have greater weight.Furthermore, we propose a novel improved spectral clustering via embedded label propagation. Our algorithm is built upon two advancements of the state of the art:1) label propagation,which propagates a node\\'s labels to neighboring nodes according to their proximity; and 2) manifold learning, which has been widely used in its capacity to leverage the manifold structure of data points. First we perform standard spectral clustering on original data and assign each cluster to k nearest data points. Next, we propagate labels through dense, unlabeled data regions. Extensive experiments with various datasets validate the superiority of the proposed algorithm compared to current state of the art spectral algorithms.", "histories": [["v1", "Sun, 23 Nov 2014 13:35:29 GMT  (1997kb)", "http://arxiv.org/abs/1411.6241v1", null], ["v2", "Tue, 6 Oct 2015 16:49:39 GMT  (0kb,I)", "http://arxiv.org/abs/1411.6241v2", "Withdraw for a wrong formulation"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "yi yang", "heng huang"], "accepted": false, "id": "1411.6241"}, "pdf": {"name": "1411.6241.pdf", "metadata": {"source": "CRF", "title": "Improved Spectral Clustering via Embedded Label Propagation", "authors": ["Xiaojun Chang", "Feiping Nie", "Yi Yang", "Heng Huang"], "emails": ["cxj273@gmail.com,", "feipingnie@gmail.com,", "yi.yang@uq.edu.au,", "heng@uta.edu."], "sections": [{"heading": null, "text": "ar Xiv: 141 1.62 41v1 [cs.LG] 2 3N ov2 01"}, {"heading": "Introduction", "text": "In fact, the number of those able to identify themselves is very high. (...) It is not the case that they are able to identify themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "Locally Linear Embedding", "text": "Local Linear Embedding (LLE) (Roweis and Saul 2000) aims to identify low-dimensional global coordinates that are located on or very close to a manifold in a high-dimensional space. The purpose is to combine the data points with minimal discrepancy after a different linear dimension reduction has been performed at each point. LLE's main procedure can be summarized in three steps: (1) create a neighborhood for each data point; (2) find the weights to approximate the data in said neighborhood linearly; and (3) find the low-dimensional coordinates that can best be reconstructed by these weights. For example, a dataset matrix X = {x1, x2,.., xn} results in the main steps of LLE being as follows: 1. For each data point xi, find its k nearest neighbors. 2. Calculate the weight matrix A by reconstructing the squares of each neighbor to minimize the rest of its neighbors."}, {"heading": "Spectral Clustering", "text": "Consider a dataset X = {x1, x2,.., xn}, xn}, q = n, where d is the dimension of the data point and n is the total number of data points. The goal of clustering is to distribute X into c clusters Ci | ci = 1 to keep data points within the same cluster close together while data points from different clusters remain apart. Let's designate Y = [y1, y2,..] T-Rn \u00b7 c as a cluster indicator matrix, where yi is the cluster indicator for the date xi. The j element of yi is 1 if xi belongs to the j cluster and 0 otherwise. After working in (Ye, Zhao, and Wu 2007), we designate the scaled cluster indicator matrix F as follows: [F1, F2,."}, {"heading": "The Proposed Framework", "text": "In this section we will illustrate the detailed framework of our algorithm. Our goal is to group the data set into c clusters. Suppose X-Rd \u00b7 n displays the data set; d is the dimension of the data points and n is the total number of data points."}, {"heading": "Distance-Consistent Similarity Learning", "text": "After the work in (Karasuyama and Mamitsuka 2013), we propose to use the manifold regulation based on the Laplacian diagram for label propagation. First, we propose a novel distance consistent Local Linear Embedding (LLE). Intuitively, we expect that narrow data points have similar designations. We create a graph in which all data points are considered as nodes. If xi (xj) is in k-nearest neighbor of xj (xi), then the two nodes are connected to each other. The edge between them is weighted so that the nearest nodes are at euclidean distance, the greater the weight aij. Since we can safely rewrite the objective function of LLE in aq (xi). (1): min On a surface i = 1 [xi,...., xi] aij of the area j = 1xidj."}, {"heading": "Refined Spectral Clustering", "text": "After initially clustering the dataset in c clusters by traditional spectral clustering, we select k data points per cluster closest to each cluster center, and mark them as labeled data points. The remaining points are marked as unlabeled data points. Note that we assume that these k data points are grouped into the correct clusters, so we get the label matrix Y-FT \u2212 Rn \u00b7 c and the diagonal selection matrix U-Rn \u00b7 n, where Yi, j = 1 if Xi is labeled, and Xi belongs to the j-th cluster. Yi, j = 0 otherwise. Uii = \u00b2 ifXi is labeled and Uii = 0 otherwise. In the experiment, we use 1010 to continue the labeling of the labeled data, where Xi belongs to the j-th cluster."}, {"heading": "Optimization", "text": "The proposed function includes the l2,1 standard, which is difficult to solve in a closed form. We propose to solve this problem in the following steps: By setting the derivative of Eq. (17) w.r.t. W to zero, we have W = \u03b1 (\u03b1XXT + \u03b2D) \u2212 1XF, (18) where I am an identity matrix and D is a diagonal matrix that represents as: D = 1 \u00b2 w1 \u00b2 w1 \u00b2 2. (19) letter H (\u03b1XXT + \u03b2D) \u2212 1, the objective becomesmin FTr (FTLF) \u2212 Tr (F \u2212 Y) TU (F \u2212 Y)) + \u03b1 \u03b1XTHXF \u2212 2F (20) \u2212 THF (H) \u2212 1, the objective becomesmin FTLF \u2212 II, the objective becomesmin FTR (FTLF \u2212 Y) TR (F \u2212 Y)))."}, {"heading": "Dataset Description", "text": "We use seven brand data sets to validate the performance of the proposed algorithm. USPS data set consists of 9298 handwritten digital images in grayscale with an image size of 256 images scanned from envelopes in the U.S. Postal Service. Yale B data set (Georghiades, Belhumeur and Kriegman 2001)"}, {"heading": "Dataset Matrix Size Dataset Size Class #", "text": "The AR dataset (Martinez and Benavente 1998) comprises 840 images with a size of 768. The FRGC dataset (Phillips et al. 2005) was collected at the University of Notre Dame and contains 50,000 images. All images were taken in 13 different poses, under 43 different lighting conditions and with four different expressions per person. The MSRA50 dataset (He et al. 2004) consists of 1799 images and 12 classes. The PALM dataset consists of 700 right-hand images, seven samples per person with 100 users, taken with a digital camera. The images are enlarged to the same size of 100 x 100. The human dataset lung carcinoma (LUNG et al. 2002) contains 203 samples and 3312 genes."}, {"heading": "Experiment Setup", "text": "We compare the proposed SCLP with traditional kmeans (TKM) (Wu et al. 2012), discriminatory k-means (DKM) (Ye, Zhao and Wu 2007), Local Learning Clustering (LLC) (Wu et al. 2006), Non-negative Normalized Cut (NNC) (Shi and Malik 2000), Spectral Clustering (SC), LLC (Wu and Schlkopf 2006), CLGR (Wang, Zhang, and Li 2009) and Spectral Embedding Clustering (SEC) (Nie et al. 2009).The size of neighborhood k is set to 5 for all spectral clustering algorithms. For the parameter \u03b4, in NNC, we perform a self-tuning algorithm (Zelnik-Manor and Perona 2004) to determine the best parameter in DKM, LLC, CLGR and SEC."}, {"heading": "Evaluation Metrics", "text": "Following related cluster studies, we use cluster accuracy (ACC) and normalized mutual information (NMI) as evaluation metrics of our experiments. Let Qi represent the cluster label resulting from a cluster algorithm; pi represent the corresponding soil truth label of any data point xi. from there, ACC is defined as follows: ACC = \u2211 n = 1 \u03b4 (pi, map (qi) n, (23) where \u03b4 (x, y) = 1 if x = y and \u03b4 (x, y) = 0 otherwise. map (qi) is the best mapping function that performs cluster markings to match the soil truth markings using the KuhnMunkres algorithm. A larger ACC indicates better cluster performance. For any two variables P and Q, NMI is defined as follows (Strehl and Ghosh 2003) NMI = H (Q) (points) between the opposite data (Q), T (tl) and MP (QL)."}, {"heading": "Experimental Results", "text": "We show the clustering results of different algorithms with respect to ACC and NMI across seven benchmark datasets in Tables 2 and 3. Based on the results of our experiment, we can make the following observations: 1. When comparing k-means-based algorithms (i.e., TKM and DKM), DKM generally outperforms TKM because discriminatory dimension reduction is integrated into a single framework, making each cluster more identifiable and facilitating clustering performance. We can therefore safely conclude that discriminatory information is used for clustering. 2. SC outperforms LLC on the YaleB and USPS datasets, while LLC is more identifiable and facilitates clustering performance on all remaining ones. That is, CLGR achieves better performance on all datasets than both algorithms combined.3. SEC achieves the second-best performance on the seven datasets that \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 \u00b1 2, \u00b1 \u00b1 \u00b1 \u00b1 2, \u00b1 \u00b1 \u00b1 \u00b1 2, \u00b1 \u00b1 \u00b1 \u00b1 2, \u00b1 \u00b1 \u00b1 \u00b1, \u00b1 \u00b1 2, \u00b1 \u00b1 2, \u00b1 \u00b1 \u00b1 \u00b1 \u00b1 2, \u00b1 \u00b1 \u00b1, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, \u00b1 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)."}, {"heading": "Parameter Sensitivity", "text": "In this section we examine the power variance for the regularization parameters \u03b1 and \u03b2. We use the MSRA50 dataset for these experiments. Fig. 2 shows how the cluster power varies with different combinations of \u03b1 and \u03b2. We can see that a better performance occurs when \u03b1 and \u03b2 are comparable."}, {"heading": "Conclusion", "text": "Most of the existing spectral cluster algorithms are based on Gaussian matrices or LLE, each of which is extremely sensitive to parameters, and in addition, the parameters are difficult to adjust. Therefore, we have introduced a novel distance consistent LLE that is parameter-free. Distance consistent LLE can promise that the edge between closer data points has greater weight. Starting from this distance consistent LLE, we have proposed an improved means of spectral clustering using label propagation. The proposed algorithm takes advantage of the advantages of label propagation and diverse learning. With label propagation, we can transfer the labels obtained through spectral clustering to other unmarked data points. By adopting diverse learning methods, we take advantage of the diverse structure between data points. Note that our framework can also be easily applied to out-of-sample data."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation 15(6):1373\u20131396", "author": ["Belkin", "M. Niyogi 2003] Belkin", "P. Niyogi"], "venue": null, "citeRegEx": "Belkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2003}, {"title": "Annotating photo collections by label propagation according to multiple similarity cues", "author": ["Luo Cao", "L. Huang 2008] Cao", "J. Luo", "T.S. Huang"], "venue": "In ACM Multimedia,", "citeRegEx": "Cao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2008}, {"title": "A convex formulation for shrunk spectral clustering", "author": ["Chang"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, January", "citeRegEx": "Chang,? \\Q2015\\E", "shortCiteRegEx": "Chang", "year": 2015}, {"title": "Cluster kernels for semisupervised learning", "author": ["Weston Chapelle", "O. Schlkopf 2002] Chapelle", "J. Weston", "B. Schlkopf"], "venue": "In Proc. NIPS,", "citeRegEx": "Chapelle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2002}, {"title": "Sparsity induced similarity measure for label propagation", "author": ["Liu Cheng", "H. Yang 2009] Cheng", "Z. Liu", "J. Yang"], "venue": "In Proc. CVPR,", "citeRegEx": "Cheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2009}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["Guan Dhillon", "I.S. Kulis 2004] Dhillon", "Y. Guan", "B. Kulis"], "venue": "In Proc. ACM SIGKDD,", "citeRegEx": "Dhillon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2004}, {"title": "Adaptive dimension reduction using discriminant analysis and k-means clustering", "author": ["Ding", "C. Li 2007] Ding", "T. Li"], "venue": "In Proc. ICML,", "citeRegEx": "Ding et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2007}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["Filippone"], "venue": "Pattern recognition", "citeRegEx": "Filippone,? \\Q2008\\E", "shortCiteRegEx": "Filippone", "year": 2008}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["Filippone"], "venue": "Pattern Recognition", "citeRegEx": "Filippone,? \\Q2008\\E", "shortCiteRegEx": "Filippone", "year": 2008}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["Belhumeur Georghiades", "A.S. Kriegman 2001] Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. PAMI 23(6):643\u2013660", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "Face recognition using laplacianfaces", "author": ["He"], "venue": "IEEE Trans", "citeRegEx": "He,? \\Q2004\\E", "shortCiteRegEx": "He", "year": 2004}, {"title": "Algorithms for clustering data", "author": ["Jain", "A.K. Dubes 1988a] Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1988}, {"title": "Algorithms for clustering data", "author": ["Jain", "A.K. Dubes 1988b] Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1988}, {"title": "Data clustering: a review. ACM computing surveys 31(3):264\u2013323", "author": ["Murty Jain", "A.K. Flynn 1999] Jain", "M.N. Murty", "P.J. Flynn"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1999}, {"title": "Correlated label propagation with application to multi-label learning", "author": ["Jin Kang", "F. Sukthankar 2006] Kang", "R. Jin", "R. Sukthankar"], "venue": "In Proc. CVPR,", "citeRegEx": "Kang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2006}, {"title": "Manifold-based similarity adaptation for label propagation", "author": ["Karasuyama", "M. Mamitsuka 2013] Karasuyama", "H. Mamitsuka"], "venue": "In Proc. NIPS,", "citeRegEx": "Karasuyama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karasuyama et al\\.", "year": 2013}, {"title": "Discriminative cluster analysis", "author": ["Fernando la Torre", "D. Kanade 2006] la Torre", "Fernando", "T. Kanade"], "venue": "In Proc. ICML,", "citeRegEx": "Torre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Torre et al\\.", "year": 2006}, {"title": "The ar face database", "author": ["Martinez", "A.M. Benavente 1998] Martinez", "R. Benavente"], "venue": "Technical report,", "citeRegEx": "Martinez et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Martinez et al\\.", "year": 1998}, {"title": "Spectral embedded clustering", "author": ["Nie"], "venue": "In Proc. IJCAI,", "citeRegEx": "Nie,? \\Q2009\\E", "shortCiteRegEx": "Nie", "year": 2009}, {"title": "Flexible manifold embedding: A framework for semisupervised and unsupervised dimension reduction", "author": ["Nie"], "venue": "IEEE Trans. Image Process", "citeRegEx": "Nie,? \\Q2010\\E", "shortCiteRegEx": "Nie", "year": 2010}, {"title": "Overview of the face recognition grand challenge", "author": ["Phillips"], "venue": null, "citeRegEx": "Phillips,? \\Q2005\\E", "shortCiteRegEx": "Phillips", "year": 2005}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "S.T. Saul 2000] Roweis", "L.K. Saul"], "venue": "Science", "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["Shi", "J. Malik 2000] Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence", "citeRegEx": "Shi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2000}, {"title": "Gene expression correlates of clinical prostate cancer behavior. Cancer cell 1(2):203\u2013209", "author": ["Singh"], "venue": null, "citeRegEx": "Singh,? \\Q2002\\E", "shortCiteRegEx": "Singh", "year": 2002}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions. Machine Learning Research 3:583\u2013617", "author": ["Strehl", "A. Ghosh 2003] Strehl", "J. Ghosh"], "venue": null, "citeRegEx": "Strehl et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2003}, {"title": "Label propagation through linear neighborhoods", "author": ["Wang", "F. Zhang 2008] Wang", "C. Zhang"], "venue": "IEEE Trans. Know. Data Engin", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Clustering with local and global regularization", "author": ["Zhang Wang", "F. Li 2009] Wang", "C. Zhang", "T. Li"], "venue": "IEEE Trans. Know. Data Engin", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "A local learning approach for clustering", "author": ["Wu", "M. Schlkopf 2006] Wu", "B. Schlkopf"], "venue": "In Proc. NIPS,", "citeRegEx": "Wu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2006}, {"title": "Learning bregman distance functions for semisupervised clustering", "author": ["Wu"], "venue": "IEEE Trans. Knowledge and Data Eng", "citeRegEx": "Wu,? \\Q2012\\E", "shortCiteRegEx": "Wu", "year": 2012}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Yang"], "venue": "IEEE Trans. Image Process", "citeRegEx": "Yang,? \\Q2010\\E", "shortCiteRegEx": "Yang", "year": 2010}, {"title": "Nonnegative spectral clustering with discriminative regularization", "author": ["Yang"], "venue": "In Proc. AAAI", "citeRegEx": "Yang,? \\Q2011\\E", "shortCiteRegEx": "Yang", "year": 2011}, {"title": "Adaptive distance metric learning for clustering", "author": ["Zhao Ye", "J. Liu 2007a] Ye", "Z. Zhao", "H. Liu"], "venue": "In Proc. CVPR,", "citeRegEx": "Ye et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2007}, {"title": "Adaptive distance metric learning for clustering", "author": ["Zhao Ye", "J. Liu 2007b] Ye", "Z. Zhao", "H. Liu"], "venue": "In Proc. CVPR,", "citeRegEx": "Ye et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2007}, {"title": "Discriminative k-means for clustering", "author": ["Zhao Ye", "J. Wu 2007] Ye", "Z. Zhao", "M. Wu"], "venue": "In Proc. NIPS,", "citeRegEx": "Ye et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2007}, {"title": "Multiclass spectral clustering", "author": ["Yu", "S.X. Shi 2003] Yu", "J. Shi"], "venue": "In Proc. ICCV,", "citeRegEx": "Yu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2003}, {"title": "Self-tuning spectral clustering", "author": ["Zelnik-Manor", "L. Perona 2004] Zelnik-Manor", "P. Perona"], "venue": "In Proc. NIPS,", "citeRegEx": "Zelnik.Manor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zelnik.Manor et al\\.", "year": 2004}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["Zhu", "X. Ghahramani 2002] Zhu", "Z. Ghahramani"], "venue": "Technical report,", "citeRegEx": "Zhu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2002}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 3, Computer Science, University of Wisconsin-Madison", "citeRegEx": "Zhu,? \\Q2006\\E", "shortCiteRegEx": "Zhu", "year": 2006}], "referenceMentions": [{"referenceID": 37, "context": "Hence, F can be obtained as follows (Zhu 2006):", "startOffset": 36, "endOffset": 46}], "year": 2017, "abstractText": "Spectral clustering is a key research topic in the field of machine learning and data mining. Most of the existing spectral clustering algorithms are built upon Gaussian Laplacian matrices, which are sensitive to parameters. We propose a novel parameter-free, distanceconsistent Locally Linear Embedding (LLE). The proposed distance-consistent LLE promises that edges between closer data points have greater weight. Furthermore, we propose a novel improved spectral clustering via embedded label propagation. Our algorithm is built upon two advancements of the state of the art: 1) label propagation, which propagates a node\u2019s labels to neighboring nodes according to their proximity; and 2) manifold learning, which has been widely used in its capacity to leverage the manifold structure of data points. First we perform standard spectral clustering on original data and assign each cluster to k-nearest data points. Next, we propagate labels through dense, unlabeled data regions. Extensive experiments with various datasets validate the superiority of the proposed algorithm compared to current state-of-the-art spectral algorithms.", "creator": "LaTeX with hyperref package"}}}