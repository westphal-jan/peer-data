{"id": "1411.6243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Structure Regularization for Structured Prediction: Theories and Experiments", "abstract": "While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via \\emph{structure decomposition}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.", "histories": [["v1", "Sun, 23 Nov 2014 14:11:01 GMT  (49kb)", "http://arxiv.org/abs/1411.6243v1", null], ["v2", "Fri, 30 Jan 2015 07:41:21 GMT  (137kb)", "http://arxiv.org/abs/1411.6243v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xu sun"], "accepted": false, "id": "1411.6243"}, "pdf": {"name": "1411.6243.pdf", "metadata": {"source": "CRF", "title": "Structure Regularization for Structured Prediction: Theories and Experiments", "authors": ["Xu Sun"], "emails": ["xusun@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 141 1,62 43v1 [cs.LG] 2 3N ov"}, {"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to determine for themselves what they want and what they want. (...) It is not so that they are able to determine for themselves. (...) It is not so that they want it. (...) It is not so that they want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (. \"(...).\" (. \"(.).\" (. \"(...).\" (. \"(.).\" (. \"(.).\" (...). \"(.\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). (.).). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.). (.).\" (.). \"(.).\" (. (.). (.). (.). (.).). (.). (. (.). (.). \"(. (.). (.).). (.). (.). (. (.).). (.). (.).). (.). (.). (.).). (. (.).). (.). (.). (.). (.). (.)."}, {"heading": "2 Structure Regularization", "text": "We first describe the proposed structural regulation method and then provide theoretical results for analyzing generalization risks and convergence rates.1See the code at http: / / klcl.pku.edu.cn / member / sunxu / code.htm"}, {"heading": "2.1 Settings", "text": "In fact, it is the case that it is a way in which people are able to determine for themselves what they want and what they want. (...) It is the case that people are able to decide what they want and what they do not want. (...) It is as if they want it. \"(...)\" It is as if people want it. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2.2 Structure Regularization", "text": "Most existing regulation techniques are for the regulation of model weights / parameters (e.g.: Re = GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS = 11 GS GS = 11 GS GS 11 GS = 11 GS GS 11 GS 11 GS = 11 GS GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = GS 11 GS = 11 GS = 11 GS = GS 11 GS = 11 GS = GS 11 GS = GS 11 GS = GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = GS 11 GS 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS = 11 GS 11 GS = 11 GS = 11 GS GS 11 GS = 11 GS 11 GS = 11 GS GS = 11 GS 11 GS = 11 GS = 11 GS = 11 GS GS 11 GS = 11 GS GS = 11 GS = 11 GS 11 GS = 11 GS = 11 GS = 11 GS GS 11 GS = 11 GS = 11 GS 11 GS = 11 GS 11 GS = 11 GS = GS 11 GS = 11 GS 11 GS = 11 GS = GS 11 GS 11 GS = GS 11 GS = 11 GS = 11 GS 11 GS = 11 GS 11 GS = 11 GS = GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS = GS 11 GS 11 GS = 11 GS 11 GS = 11 GS 11 GS 11 GS = GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS = 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS 11 GS 11 GS = 11 GS = 11 GS 11 GS 11 GS 11 GS 11 GS = 11 GS = 11 GS 11 GS 11 GS 11 GS = 11 GS 11 GS 11 GS 11 GS = 11 GS 11 GS"}, {"heading": "2.3 Stability of Structured Prediction", "text": "In contrast to the simplicity of the algorithm, the theoretical analysis is quite technical. First, we analyze the stability of the structured prediction.Definition 4 (Functional Stability) A really evaluated structured prediction algorithm G has \"Functional Stability\" (\"Functional Stability\" for short-term predictions). (\"Functional Stability\" for short-term predictions). (\"Functional Stability\" for short-term predictions). (\"Functional Stability\" for short-term predictions). (\"Functional Stability\" for short-term predictions). (\"Functional Stability\" for short-term predictions). (\"Functional Stability\" for short-term predictions. \"(\" GS). (\"Functional Stability\"). (\"We.\" \""}, {"heading": "2.4 Reduction of Generalization Risk", "text": "Theorem 9 (generalization vs. structural regulation) Let G be a real-rated structured classification algorithm (Re-q function) with a point-wise loss function [1]. Let R (f) be the generalization risk of f based on S, as defined like before. Let R (f) be the generalization risk of f based on the expected sample z, as defined as before. Let Re (f) be the empirical risk of f based on S, as defined like before. Then, for any other type of (0, 1), with probability at at at at 1 \u2212 precision at the random draw of the training set S, the generalization risk R (f) is bounded byR (f) \u2264 Re (f)."}, {"heading": "2.5 Accelerating Convergence Rates in Training", "text": "We also analyze the impact on the convergence rate of online learning by applying structural regulation. Our analysis is based on the stochastic gradient descendant (SGD) constellation [3, 11, 15], which is probably the most representative online convergence convergence convergence convergence convergence convergence convergence (wt) (21), where Gz (wt) is the stochastic estimate of the objective function based on z randomly drawn from S. In order to indicate our convergence rate analysis results, we need several assumptions that follow (Nemirovski et al. 2009). We assume that g is strongly convex with modulus c, that is, w \"W,\" \"w (w)."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tasks", "text": "We experiment with natural language processing tasks and signal processing tasks. The natural language processing tasks include (1) part-of-speech tagging, (2) biomedical named entity recognition, and (3) Chinese word segmentation. The signal processing task is (4) very different, with which Boolean features and the task (4) are realized. From tasks (1) to (4), the average structure complexity (number of observations) is n, with n = 26.5, 46.6, 67.9. The dimensions of the keywords are also diversified."}, {"heading": "3.2 Experimental Settings", "text": "To test the robustness of the proposed structural regulation method (StructReg), we conduct experiments with both probabilistic and non-probabilistic structural prediction models. We select conditional random fields (CRFs) [10] and structured perceptrons (Perc) [5], which are probably the most popular probabilistic and non-probabilistic structured prediction models. CRFs are made using the SGD algorithm, 9 and the basic method is the traditional weight regulation scheme (WeightReg), which applies the most representative L2 weight regulation, i.e., a Gaussian priority.10 For structured perceptrons, the baseline WeightAvg is the popular implicit regulation technique based on averaging the parameters, i.e., averaged perceptron [5].All methods use the same set of characteristics."}, {"heading": "3.3 Experimental Results", "text": "It is indeed the case that we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution. '"}, {"heading": "4 Proofs", "text": "Our analysis must sometimes use McDiarmid's inequity.Theorem 14 (McDiarmid, 1989) Let S = {q1,.., qm} be independent random variables that record values in space Qm. In addition, let g: Qm 7 \u2192 R be a function of S that meets the requirements of i, i, i, i, i, i, i, Q, g (S) \u2212 g (Si) | \u2264 ci.12See a collection of systems at http: / aclweb.org / aclwiki / index.php? title = POS _ Tagging _ (State _ of _ the _ art). Then you can see a collection of systems at http: / / aclweb.org / aclwiki / index.php? title = POS _ Tagging _ (State _ of _ the _ art)."}, {"heading": "Proof", "text": "ES [R (GS) \u2212 Re (GS)] = 1n ES (Ez (L (GS, z)) \u2212 1mm j = 1L (GS, zj) = 1n (ES, z i (L (GS, z i))) \u2212 1mm j = 1ES (L (GS, z i)) = 1n (ES, z i (L (GS, z i))) \u2212 ES (L (GS, z i)) = 1n (ES, z i (L (GS, z i))) \u2212 ESi (L (GSi, z i)) = 1n ES, z i (L (GS, z i) \u2212 L (GSi, z i)), the third step being based on ESL (GS, z i) = ESL (GS, zj) for z i S and zj S, since G is symmetrical."}, {"heading": "4.1 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 6", "text": "According to (1) we have the following values: \"i,\" \"S,\" \"z,\" \"K,\" \"K,\" \"GS\" (GS, z, k), \"GS\" (x, k), \"c\" (GS, k), \"G\" (GS, k), \"G\" (GS, z), \"G\" (x, k), \"GS\" (x, k), \"G\" (G, k), \"G\" (G, z), \"G\" (G, z), \"G\" (G, z), \"G\" (G, z), \"G\" (G, k), \"G\" (G, k), \"G\" (G, k), \"G\" (G, k)."}, {"heading": "Proof of Theorem 7", "text": "If a konvexe and differentiable function g has a minimal f in space F, has its Bregman-divergence the following property for f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, f'i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i"}, {"heading": "Proof of Corollary 8", "text": "The proof is comparable to the proof of theory 7: First, we use the term \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"\" f, \"\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" f, \"\" f, \"f,\" f, \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" \"f,\" f, \"\" f, \"f,\" \"f,\" f, \"\" f, \"f,\" \"f,\" f, \"f,\" f, \"f,\" \"f,\" f, \"f,\" f, \"\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f,\" f, \"f\" f, \"f\" f, \"f,\" f, \"f,\" f \"f,\" f, \"f,\" f, \"f,\" f, \"f\" f, \"f\" f \"f,\" f \"f\" f \"f,\" f, \"f,\" f, \"f\" f, \"f,\" f, \"f,\" f, \"f\" \"f,\" \"f\" \"f\" f, \"\" \"f,\" \"f\""}, {"heading": "Proof of Theorem 9", "text": "I'm not going to say that I don't know what I'm going to do with it, but I'm not going to say that I'm not going to do anything about it, I'm not going to say that I'm not going to do anything about it, I'm not going to say that I'm not going to do anything about it, I'm not going to say that I'm not going to say that I'm not going to do anything about it, I'm not going to say that I'm going to do anything about it, I'm not going to say that I'm going to do anything about it, I'm not going to say that I'm going to say that I'm going to do anything about it, I'm not going to say that I'm going to say anything about it, I'm not going to say anything about it, I'm going to say that I'm going to say that I'm going to say."}, {"heading": "Based on the bounds of |R(f)\u2212R(f)i\u2032 | and |Re(f)\u2212Re(f)i", "text": "We show that R (f) \u2212 Re (f) \u2212 Re (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) \u00b2 (f) i) \u00b2 (f) \u2212 z (f), z \u2212 z (f), z (f) z (f) z (f), z (z), z (z), z (z) (z), z (z) (z), z (z) (z), z (z), z (z) (z), z (z) (z), z (z), z (z), z (z) (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (f) \u2212 R (i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i \u00b2 (f) i (f) i \u00b2 (f) i \u00b2 (f) i (f) i \u00b2 (f) i \u00b2 (f) i (f) i \u00b2 (f) i \u00b2 (f) i (f) i (f), z (f) z (f) z (f) z (f) \u2212 z (f) \u2212 z (f) i (f), z (z (z (z (f) \u2212 z (f) i, z (z (z (z (z), z (z (z), z (z (z), z (z (z), z (z (z (z), z (z), z (z (z), z (z (z), z (z), z ("}, {"heading": "Proof of Proposition 13", "text": "After subtraktingw on both sides and taking into account the norms for (21), we have (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w), (wt-w-w), (wt-w-w), (wt-w-w), (wt-w-w), (wt-w-w), (wt-w-w), (wt-w-w-w, w-w-w-w-w), (wt-w-w, w-w-w-w-w-w), (wt-w-w-w), (wt-w-w-w-w), (wt-w-w-w-w-w-w, w-w-w-w-w-w-w-w-w), (wt-w-w-w-w-w-w-w-w-w, w-w-w-w-w-w-w-w-w-w-w), (wt-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w, w-w-w-w-w-w-w-w, w-w-w-w-w-w-w-w-w, w-w-w-w-w-w, w-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w), (wt-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w, w-w-w-w, w-w-w-w-w-w-w-w-w-w-w-w-w-w-w-w"}, {"heading": "5 Conclusions", "text": "Our theoretical analysis showed that this method can effectively reduce the risk of generalization and also accelerate the rate of convergence in training. The proposed method does not alter the convexity of objective function and can be used in conjunction with all existing methods for weight regulation. Note that the proposed method and the theoretical results can fit general structures, including linear chains, trees and diagrams. Experimental results showed that our method achieved better results on several highly competitive tasks than state-of-the-art systems, and at a much faster training speed."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the NSFC (No.61300063)."}], "references": [{"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "In Proceedings of NIPS\u201907", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Structured sparsity through convex optimization", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "CoRR, abs/1109.2397,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Online algorithms and stochastic approximations. Online Learning and Neural Networks. Saad, David", "author": ["L. Bottou"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Proceedings of EMNLP\u201902,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Boosting with structural sparsity", "author": ["J.C. Duchi", "Y. Singer"], "venue": "In ICML\u201909,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A comparative study of parameter estimation methods for statistical natural language processing", "author": ["J. Gao", "G. Andrew", "M. Johnson", "K. Toutanova"], "venue": "In Proceedings of ACL\u201907,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Posterior vs parameter sparsity in latent variable models", "author": ["J. Gra\u00e7a", "K. Ganchev", "B. Taskar", "F. Pereira"], "venue": "In Proceedings of NIPS\u201909,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D.N. Metaxas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In ICML\u201901,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Slow learners are fast", "author": ["J. Langford", "A.J. Smola", "M. Zinkevich"], "venue": "In NIPS\u201909,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Pac-bayes generalization bounds for randomized structured prediction", "author": ["B. London", "B. Huang", "B. Taskar", "L. Getoor"], "venue": "In NIPS Workshop on Perturbation, Optimization and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Structured sparsity in structured prediction", "author": ["A.F.T. Martins", "N.A. Smith", "M.A.T. Figueiredo", "P.M.Q. Aguiar"], "venue": "In Proceedings of EMNLP\u201911,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A family of penalty functions for structured sparsity", "author": ["C.A. Micchelli", "J. Morales", "M. Pontil"], "venue": "In Proceedings of NIPS\u201910,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "In NIPS\u201911,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Statistics and Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "An efficient projection for l1,infinity regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "In Proceedings of ICML\u201909,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Convex structure learning in log-linear models: Beyond pairwise potentials", "author": ["M.W. Schmidt", "K.P. Murphy"], "venue": "In Proceedings of AISTATS\u201910,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Learnability and stability in the general learning setting", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In Proceedings of COLT\u201909,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A.K. Joshi"], "venue": "In Proceedings of ACL\u201907,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Large-scale personalized human activity recognition using online multitask learning", "author": ["X. Sun", "H. Kashima", "N. Ueda"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Feature-frequency-adaptive on-line training for fast and accurate natural language processing", "author": ["X. Sun", "W. Li", "H. Wang", "Q. Lu"], "venue": "Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Piecewise pseudolikelihood for efficient training of conditional random fields", "author": ["C.A. Sutton", "A. McCallum"], "venue": "In ICML\u201907,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS\u201903,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Learning with lookahead: Can history-based models rival globally optimized models", "author": ["Y. Tsuruoka", "Y. Miyao", "J. Kazama"], "venue": "In Conference on Computational Natural Language Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Estimating the \u201dwrong\u201d graphical model: Benefits in the computation-limited setting", "author": ["M.J. Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Structural regularized support vector machine: A framework for structural large margin classifier", "author": ["H. Xue", "S. Chen", "Q. Yang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Reranking for biomedical named-entity recognition", "author": ["K. Yoshida", "J. Tsujii"], "venue": "In ACL Workshop on BioNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "For (typically non-structured) classification problems, there are considerable studies on structurerelated regularization, including spectral regularization for modeling feature structures in multi-task learning [1], regularizing feature structures for structural large margin classifiers [27], and many recent studies on structured sparsity.", "startOffset": 212, "endOffset": 215}, {"referenceID": 26, "context": "For (typically non-structured) classification problems, there are considerable studies on structurerelated regularization, including spectral regularization for modeling feature structures in multi-task learning [1], regularizing feature structures for structural large margin classifiers [27], and many recent studies on structured sparsity.", "startOffset": 289, "endOffset": 293}, {"referenceID": 13, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 84, "endOffset": 91}, {"referenceID": 5, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 84, "endOffset": 91}, {"referenceID": 17, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 128, "endOffset": 136}, {"referenceID": 12, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 128, "endOffset": 136}, {"referenceID": 16, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 177, "endOffset": 181}, {"referenceID": 28, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 195, "endOffset": 199}, {"referenceID": 7, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 226, "endOffset": 229}, {"referenceID": 1, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 258, "endOffset": 268}, {"referenceID": 15, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 258, "endOffset": 268}, {"referenceID": 8, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 258, "endOffset": 268}, {"referenceID": 22, "context": "[23] described an interesting heuristic piecewise training method for structured prediction models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] described a \u201clookahead\u201d learning method based on structured perceptrons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Our work differs from [23] and [25] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "Our work differs from [23] and [25] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "[26] suggested consistent approximation for both training and test phase, but there is no indication on structure regularization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 18, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 23, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 102, "endOffset": 110}, {"referenceID": 23, "context": ", expected average loss) [24, 12]:", "startOffset": 25, "endOffset": 33}, {"referenceID": 11, "context": ", expected average loss) [24, 12]:", "startOffset": 25, "endOffset": 33}, {"referenceID": 3, "context": "We follow some notations and assumptions on non-structured classification [4, 19].", "startOffset": 74, "endOffset": 81}, {"referenceID": 18, "context": "We follow some notations and assumptions on non-structured classification [4, 19].", "startOffset": 74, "endOffset": 81}, {"referenceID": 2, "context": "Our analysis is based on the stochastic gradient descent (SGD) setting [3, 11, 15], which is arguably the most representative online training setting.", "startOffset": 71, "endOffset": 82}, {"referenceID": 10, "context": "Our analysis is based on the stochastic gradient descent (SGD) setting [3, 11, 15], which is arguably the most representative online training setting.", "startOffset": 71, "endOffset": 82}, {"referenceID": 14, "context": "Our analysis is based on the stochastic gradient descent (SGD) setting [3, 11, 15], which is arguably the most representative online training setting.", "startOffset": 71, "endOffset": 82}, {"referenceID": 4, "context": "We use the standard benchmark dataset in prior work [5], which is derived from PennTreeBank corpus and uses sections 0 to 18 of the Wall Street Journal (WSJ) for training (38,219 samples), and sections 22-24 for testing (5,462 samples).", "startOffset": 52, "endOffset": 55}, {"referenceID": 24, "context": "Following prior work [25], we use features based on unigrams and bigrams of neighboring words, and lexical patterns of the current word, with 393,741 raw features8 in total.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Following prior work [25], we use word pattern features and POS features, with 403,192 raw features in total.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "Following prior work [7], we use features based on character unigrams and bigrams, with 1,985,720 raw features in total.", "startOffset": 21, "endOffset": 24}, {"referenceID": 20, "context": "This is a task based on real-valued sensor signals, with the data extracted from the Bao04 activity recognition dataset [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "Following prior work in activity recognition [21], we use acceleration features, mean features, standard deviation, energy, and correlation features, with 1228 raw features in total.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "We choose the conditional random fields (CRFs) [10] and structured perceptrons (Perc) [5], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively.", "startOffset": 47, "endOffset": 51}, {"referenceID": 4, "context": "We choose the conditional random fields (CRFs) [10] and structured perceptrons (Perc) [5], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively.", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": ", averaged perceptron [5].", "startOffset": 22, "endOffset": 25}, {"referenceID": 21, "context": "Since the rich edge features [22] can be automatically generated from raw features and are very useful for improving model accuracy, the rich edge features are employed for all methods.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "We also tested on sparsity emphasized regularization methods, including L1 regularization and Group Lasso regularization [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "33 (see [20]) 72.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "28 (see [25]) 97.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "19 (see [7]) Our results 97.", "startOffset": 8, "endOffset": 11}, {"referenceID": 19, "context": "The POS-Tagging task is a highly competitive task, with many methods proposed, and the best report (without using extra resources) until now is achieved by using a bidirectional learning model in [20],12 with the accuracy 97.", "startOffset": 196, "endOffset": 200}, {"referenceID": 24, "context": "On the Bio-NER task, [25] achieves 72.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "28% based on lookahead learning and [28] achieves 72.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "On the Word-Seg task, [7] achieves 97.", "startOffset": 22, "endOffset": 25}, {"referenceID": 21, "context": "19% based on maximum entropy classification and our recent work [22] achieves 97.", "startOffset": 64, "endOffset": 68}], "year": 2017, "abstractText": "While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via structure decomposition, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.", "creator": "LaTeX with hyperref package"}}}