{"id": "1411.6300", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions", "abstract": "In a Bayesian network, we wish to evaluate the marginal probability of a query variable, which may be conditioned on the observed values of some evidence variables. Here we first present our \"border algorithm,\" which converts a BN into a directed chain. For the polytrees, we then present in details, with some modifications and within the border algorithm framework, the \"revised polytree algorithm\" by Peot &amp; Shachter (1991). Finally, we present our \"parentless polytree method,\" which, coupled with the border algorithm, converts any Bayesian network into a polytree, rendering the complexity of our inferences independent of the size of network, and linear with the number of its evidence and query variables. All quantities in this paper have probabilistic interpretations.", "histories": [["v1", "Sun, 23 Nov 2014 21:19:44 GMT  (1591kb,D)", "http://arxiv.org/abs/1411.6300v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["do le paul minh"], "accepted": false, "id": "1411.6300"}, "pdf": {"name": "1411.6300.pdf", "metadata": {"source": "CRF", "title": "Discrete Bayesian Networks: The Exact Posterior Marginal Distributions", "authors": [], "emails": ["dminh@fullerton.edu"], "sections": [{"heading": null, "text": "Keywords: Bavarian networks; Exact inference; Border algorithm; Revised polytree algorithm; Parentless polytree method"}, {"heading": "1 The Bayesian Networks (BNs)", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1.1 The Marginal Distribution", "text": "We want the marginal probability Pr {Q}, in which Q \u00b2 V = 1 = 1 Yp = 1 Yp = 1 Yp = 1. \"This probability may be based on the fact that some other variables are observed in V to capture certain values. Suppose that f is a function defined by a series of variables X. We say that the\" scope \"of f is as wide as necessary as f (X); if not, we simply write f (\u00b7).In this paper, let us assume that X = {Y}, where Y = and Y = {Y1, Yn}. We express Pr {X} as Pr {Y}, Z}."}, {"heading": "2 Partitioning a DAG", "text": "In this section we show how a BN can be divided into smaller parts."}, {"heading": "2.1 The Set Relationships", "text": "Consider a non-empty set of nodes X V. We also refer to it as 1. HX = {\u0432 V-XHV}\\ X as the \"parent\" of X. If HX = \u2205, we say X as \"parentless\" (or \"ancestral\"); for the BN A, H {A, H} = {C, D}.2. LX = {\u0432 V-XLV}\\ {X, HX} as the \"child\" of X. If LX = \u2205, we say X as \"childless.\" For the BN A, L {A, H} = {J, K}. (Although D is a child of A, it is also a parent of H; so it is a member of H {A, H}, not of L {A, H}.) 3. KX = {\u0432 V-XKV}\\ {X, LX} as the \"co-parent\" of X. If a child of V-X is also in X, then all parents in HX} are {X} {X,} so we can {X-V, also {X-V.}"}, {"heading": "2.2 The Growing Parentless Set", "text": "It is about whether and in what form the people of the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether the EU should leave the EU. (...) It is about whether it should leave the EU. (...) It is about whether it should leave the EU. (...) It is about whether it should leave the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (... It is about the EU. (...) It is about the EU. (...) It is about the EU. (...) It is about the EU. (... It is about the EU. (...) It is about the EU. (...) It is about the EU. (... It is about the EU. \"It is about the EU is about the EU.\""}, {"heading": "3 Inferences without Evidences", "text": "In this section we will explain how to calculate the \"previous limit probability\" Pr {Bi}, provided that no variable that takes on a value is observed."}, {"heading": "3.1 The Parentless Set Probabilities", "text": "First, we present the following important problem, which is based on a simple observation that in a BN a parentless group of nodes and their parameterization is a Bayesian subnetwork: Lemma 1 If P \u2212 V is parentless, then Pr {P} = Pr {A} Pr {V | HV}. Proof. The problem results from {A, B, D}, since the BN A \u2212 B \u2212 is not parentless from the rest of the network, Pr {A} Pr {B} Pr {D} Pr {A, B}.We do not use {A, B, D}, because D alone does not separate the PN \u2212 Pr \u2212 0 from the rest of the network. So we start with the parentless P0 = C0 = C0 and define the Pr (C0) = Pr {P0, C0} = Pr."}, {"heading": "3.2 The Border Probabilities", "text": "We now show how Pr {Bj} can be calculated recursively from Pr {Bj \u2212 1}: Theorem 3 For all 1 \u2264 j \u2264 \u03b3, Pr {Bj} = \u2211 Vj \u03a6 (Cj) Pr {Bj \u2212 1}. Proof. Our strategy is not to eliminate all members of Aj simultaneously in the form of Pr {Bj} = \u2211 Aj Pr {Aj, Bj} = \u2211 Aj Pr {Pj} = \u2211 Aj {Pj} = \u2211 Aj \u03a6 (Cj) Pr {Pj \u2212 1}. Rather, we eliminate the variables in Aj individually: after the variable Vj is moved to Aj = {Aj \u2212 1, Vj}, it is eliminated immediately. In other words, because the extent of Pr (Cj, HCj \u2212 1 \u2212 Bj) can be calculated as a Pr value."}, {"heading": "4 Inferences with Evidences", "text": "A variable E is supposed to be a \"proof variable\" if it is observed that it assumes a value only in a subset of Vae (E) and Va (E). Variable V is not evident if Vae (V) = Va (V). Let us say, for example, Va (X) = {1, 2, 3}. If X is observed not to assume a value of 3, then it is evident with Vae (X) = {1, 2}. Let us suppose that E is the totality of all evidence variables. Let us consider Y = {Y1,..., Yn} V. We denote the event that Y occurs through [Y] = {Y], Vae (Y1) \u00d7 Vae (Yn) \u00d7 Pae (Yn)."}, {"heading": "4.1 The Evidence Indicator Columns", "text": "Consider a table with t-rows in which every row of variables corresponds to = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y = Y"}, {"heading": "4.2 The Downward Pass for the Top Evidences", "text": "within the uppermost part Aj, and define the following notation: For all 0 \u2264 j \u2264 \u03b3, by Lemma 4, \u0432 (Bj) = \u0432. (1) The following theoreme is the evidence-based version of Theorem 3: Theorem 5 For all 1 \u2264 j \u2264 j j. (Bj). (Bj). (Bj). (Bj). (Bj). (Bj). (Bj). (Bj). (Bj). (Bj). (B.). (Bj). (Bj). (Bj). (Bj). (Bj). (Bj). (Bj). (B.). (Bj). (Bj.). (Bj.). (Bj.). (Bj.). (Bj.). (Bj.). (B.). (B.). (B.). (B.). (B.). (B.). (B.). (B.). (..... (.....). (B. (.....). (B. (B.). (B. (.....). (B. (B.). (.....). (B. (B.). (B. (.....). (B. (B.). (.....). (B. (Bj.). (B. (B.). (.....). (B. (B.). (.....). (B. (B.). (..... (B.).). (B. (.....). (B. (Bj.). (Bj.). (Bj. (Bj.). (Bj...... (.....). (.....). (B. (.....). (.....).). (B. (B. (B. (.....).). (.....). (B. (B. (.....). (B. (.....)."}, {"heading": "4.3 The Upward Pass for the Bottom Evidences", "text": "In order to collect information about the lowest receipts within DJ, we must take into account the fact that the boundary separates Bj and Dj = V\\ Pj. So, to study DJ, we do not need the information of the whole Pj, but only from Bj.We first present the following problem: Lemon 6 For all 1 / 2 / 2 / 2 / 3 / 3 / 3 / 3 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4 / 4, Pr {Dj \u2212 1, Bj \u2212 1 / 3 / 4 / 4 / 4, Bj \u2212 1 / 4 / 4, Bj \u00b2 1 / 4 / 4, Bj \u00b2 2 / 4 / 4, Bj \u00b2 4 / 5, Bj \u00b2."}, {"heading": "4.4 The Posterior Marginal Distributions", "text": "In combination with the downward trend, this results in: Theorem 8 = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "5 The Revised Polytree Algorithm", "text": "A polytree is a BN that is \"individually connected\"; that is, there is only one undirected path that connects two arbitrary nodes. (Henceforth, \"path\" means \"undirected path.\") The BN A in Figure 1 is not a polytree, because there are two paths from A to H, namely A \u2212 C \u2212 H and A \u2212 D \u2212 H. In other words, while we assume that all BN are acyclic (i.e., they have no directed cycles), a polytree does not have an undirected cycle (or \"loop\") either. BN A has the loop A \u2212 C \u2212 H \u2212 D \u2212 A. For illustration, we use the polytree as shown in Figure 4, which we will call polytree B."}, {"heading": "5.1 To Find the Path Connecting Two Nodes", "text": "Here we present a method to identify the unique path that connects any two nodes in a polytrem. The larger the network, the more nodes we need. For polytree B, we select the nodes J and H as nodes connected by the path I \u2212 M \u2212 D \u2212 C. For each node, we also upload the path from it to the nearest node. For node P, the path is P \u2212 I \u2212 J; for node A, the path is A \u2212 D \u2212 H. To find the path from node X to node Y, we form the (possibly cyclic) path from X to the nearest node X, then to the nearest node Y, then to Y. For nodes P \u2212 I \u2212 J, this path is (I \u2212 M \u2212 D \u2212 C) \u2212 (H \u2212 C \u2212 D \u2212 J."}, {"heading": "5.2 The Decompositions by Nodes", "text": "Node V in a polytree splits the polytree into two parts: 1. PV, the parentless set including V and all nodes connected to V \"from above\" via its parents. (In Figure 4 PD is located in the shaded region.) PV has the boundary V and the upper part AV = PV\\ V. In accordance with Definition (9) we define that all nodes connected via their children LV are connected to V \"from below.\" In accordance with Definition (11) we define (V) = Pr {[DV] | V} IV = HV {DV} IV. (13) 2. DV, the lower set of PV in which all nodes are connected to V \"from below,\" via their children LV."}, {"heading": "5.3 The Decompositions by Edges", "text": "So far, we have concentrated on the nodes in a BN Y Y-Y Y-Y Y. Now, let's look at a typical edge X-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y"}, {"heading": "5.4 The Message Propagations in the Polytrees", "text": "We present the lemmas about the relationships between the two (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (X), (Y), (Y), (X), (Y), (X), (X), (X), (X), (X), (X), (X), (X), (X), (Y), (Y), (X, X, X, (Y), (Y), (X, Y, (X, Y), (X, Y, (X, Y), (X, X, Y, X, Y, Y, Y (Y), (X, X, Y, X, Y, Y (Y), X (X, Y, Y, Y (X), X (X, Y, X, Y, Y (X), X (X), X (X (X), X (X, Y (X), X (X), X (X (X), X (X (X), X (X), X (X (X), X (X (X), X (X (X), X (X), X (X (X), X (X), X (X (X), X (X (X), X (X, Y (X, Y, Y (X), Y (X, Y, Y, Y, X (X), Y (X, Y (X), X, Y (X, Y, Y (X, X, Y, Y, X, Y, Y, Y, X, Y, X, Y, X, X, X, Y, Y, Y (X), X (X, Y, X, X, X, Y, Y, X, Y (X)"}, {"heading": "5.5 The Collection Phase", "text": "In the first phase of the revised polytree algorithm, known as the \"collection phase,\" a randomly selected node P is called a \"pivot\" and the messages are passed (or \"propagated\") to P."}, {"heading": "5.5.1 The Evidential Cores", "text": "The \"evidence core\" (or EC) of a polytree is the smallest part of the polytree that contains all the evidence. In other words, it consists of the evidence core E and all the nodes and edges on the path that connects each pair of evidence object nodes. According to a particular evidence object node, the EC is unique. Figure 5 shows the EC in polytree B, corresponding to E = {B, C, K, L4}, without the nodes or edges in the dash, such as node L1. We call it the EC B.A node in a polytree if it is in the EC; otherwise it is \"outside.\" Likewise, an edge X \u2192 Y is \"inside\" if both X and Y are in the dash; otherwise it is \"outside.\" A path is \"inside\" if all its components are edges in the polytree. It is important to note that for one outer edge X \u2192 Y, the entire EC must only be on either the parent side \u2192 X on the other side of the Y, or Y may not be connected to the other side \u2192 Y on the Y."}, {"heading": "5.5.2 The Boundary Conditions", "text": "We want to use theorems 14 and 15 only for the calculation of the messages along the inner edges. However, this requires knowledge of the boundary conditions, i.e. the messages from the outer neighbours to the inner nodes, e.g. those in the hyphen in Figure 5. Therefore, we need the following sentence: Theorem 16 If we look at an inner node X of an EC. (a) If V-HX is outside, then the entire EC, and thus also TV \u2192 X = Pr {V}, is. (b) If V-LX is outside, then it is reversed. (X) = IX proof. (a) Because the inner X is on the child side of the edge V \u2192 X, the entire EC has no proof, or ITV \u2192 X = 1. This results, by definition (15), inversely X-X (V) = Pr {V}. (b) If V-LX is outside, then because the inner X is on the mother side of the edge X \u2192 V, then the entire EC is; the entire X-X cannot be occupied by the pre-X (V)."}, {"heading": "5.5.3 The Message Initializations", "text": "Like all polytrees, an EC has its own roots and leaves. For example, the groups of roots and leaves of the EC B are each {K, A, C} and {B, L4, M}. According to the definition of the EC, each path in it must end with a proof node, although there are proof nodes that are not at the end of a path (like nodes C in EC B). Also, a proof node at the end of a path can only have an inner child or parent, otherwise the path does not end with it. If it has a child, then it is a root of the EC (like nodes K in EC B); if a parent is a leaf (like nodes B and L4 in EC B). We initialize the multiplication of the message in an EC as follows: Theorem 17 Consider an EC (like the nodes K in EC B). (a) For proof, root R has an inner child (R) = an only child (R)."}, {"heading": "5.5.4 To the Pivot", "text": "In the collection phase, starting with an evidence object root or a leaf in the EC, we send the messages from all inner nodes to a randomly selected inner rotation node P. If P has not received a message along or from an edge, we may track the many inner paths that lead to this edge, and must see either an inner node that is ready to do so, or an evidence object node at the end of an inner path that can send its message to P via Theorem 17. We may need both theorems 14 and 15 to send the messages along a path, because directions may change (for example along path B \u2190 A \u2192 D). In EC B in Figure 5, the collection phase, when we use node D as a pivot point, includes the messages sent along paths B \u2190 A, D, L4 \u2190 H \u2190 C \u2192 D and K \u2192 M \u2190 D."}, {"heading": "5.6 The Distribution Phase", "text": "In the second phase of the revised polytree algorithm, known as the \"distribution phase,\" messages from the P node are forwarded to a query variable."}, {"heading": "5.6.1 The Informed Nodes", "text": "Once a node Q has received messages from all its neighbors, we say that it is \"informed.\" At the end of the survey phase, the rotation node is the first informed node. The posterior boundary probability of an informed node can now be determined: With the messages of all its parents, we can use Lemma 12 to calculate what (Q) is; with the messages of all its children, we can use Lemma 13 to calculate what (Q) is. Pr {Q, [E\\ Q]} IQ can then be calculated by Theorem 8 as \"Q\" (Q) (Q). Alternatively, we can use the following theorem: Theorem 18 with edge Q \u2192 Y, Pr {Q, [E\\ Q]} IQ = \"Y\" (Q) and Y (Q). Proof: By Theorem 8, Lemmas 13 and 10, Pr {Q, [E\\ Q]} IQ = \"Q\" Q \"Q\" (Q) and Y (Q)."}, {"heading": "5.6.2 The Inside Query Variables", "text": "Let p be the number of paths that connect an inner node V to the rest of the network (i.e., the number of its neighbors). If it is not the pivot point, then only one of these paths leads to the pivot node P. If we use theorem 14 or 15 to send a message from V to P in the capture phase, V must have received the messages on all p \u2212 1 paths except the one that leads to P. Now, in the distribution phase, when it receives a message from the informed P. Let J be the set of all informed nodes that we call an \"informed quantity.\" When we send the messages from an informed node V to an uninformed node Q, not only Q, but also all nodes along the path from V to Q are informed. In EC B in Figure 5, the message propagations from the informed node D to an informed node H are also informed."}, {"heading": "5.6.3 The Outside Query Variables", "text": "Let us now look at an external uninformed node Q. The paths that start from Q to all nodes in J must go through a unique \"gate\" T-J; otherwise, since J is connected, there is more than one path that connects Q to the informed node via the various gates. Thus, Q only needs the messages that are sent from T to him (in this direction) to be informed, as this contains all evident information. All other messages sent to him on other paths come from the external neighbors. In this way, the informed group J spreads to Q.For the polytree B in Figure 5, let us assume J = {D, C, H} and the outer node R8 is a query variable. The gate from R8 to J is C and the messages along the path C \u2190 L3 \u2192 R8 are all that is needed to inform R8. Peot & Shachter's revised polytree algorithm is an \u2192 variable message from gate 8 to gate C and R8 to gate R8 is all that is needed to inform R8."}, {"heading": "6 The Border Polytrees (BPs)", "text": "In Section 2, we showed that the boundary algorithm \"stretches\" a BN into a boundary chain, which is a special form of polytrees in which each node has at most one parent and one child. For example, we stretched the BN A in Figure 1 into a boundary chain in Figure 3. We will now show how to convert any BN into a polytree."}, {"heading": "6.1 Stage I: The Macro-node Polytrees", "text": "Our method consists of two phases: In Stage I, we partition the BN to form a polytrem. A series of nodes is called \"combinable\" to form a \"macro node\" if the network remains acyclic after combining its members. Thus, for example, we cannot combine the nodes A and H in BN A, as this leads to the directed loop {A, H} \u2192 D \u2192 {A, H}. According to Chang & Fung's (1989) node aggregation theorem, this amount is combinable. If two loops in a DAG share a common node, they belong to the same \"independent loop set\" (ILS) that ties the node. If we convert all ILSs in a DAG into polytrees, then the DAG itself becomes a polytree. Since each ILS shares a common node, we can inevitably turn both into a node for the purpose of connecting the node."}, {"heading": "6.1.1 The Isolated Loops", "text": "In a DAG, we isolate one of its successive roots (that is, we delete all four isolated roots) and call it an \"isolated loop.\" Some non-root and non-leaf nodes of the DAG can now become roots or leaves of the isolated loop. (See Figure 6 (a). An isolated loop has the same number of roots and leaves and can be organized in a loop sequentially like a star. (2) Two adjacent roots cannot be in a parent-child relationship, so must be connected from the bottom by a leaf. (See Figure 6 (a).) Lemma 19 An isolated loop has the same number of roots and leaves and can be organized into a star shape."}, {"heading": "6.1.2 The Parentless Polytree Method", "text": "In our \"parentless polytree method,\" we construct a growing parentless polytree from a DAG as follows: We start with a root in the DAG and perform the following steps: 1. While there is an un-recruited node that is recruited in a topological order (that is, after all its parents {\u03c01,..., \u03c01}), along with an edge. While there is an un-recruited node that is recruited in a topological order (that is, every resulting path is open), any resulting loop (c) is terminated, while (d) Go to step 12. End whileThe growing polytree must be parentless, so that no directed path can be returned to it, and thus no directed cycle can occur over a node outside the polytree, as a result of any node combinations within the polytree. If we recruit a node that has parents in the polytree, we bring it edges."}, {"heading": "6.2 Stage II: The Border Polytrees", "text": "In the macro node region of BN C, we must begin with the macro node. (R, T), (G), (A), (A), (A), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (B), (), (B), (), (B, \"(), (B), (), (B), (), (B), (), (B), (), (B), (), (B), (), (B), (), (B), (), (B), (), (B,\" (), \"(), (B,\" (), \"(), (), (B,\" (), (), (), (), (), (B, (), (), (), (), (), (B, (), (), (), (), (), (B, (), (), (), (), (), (), (), (), (), (, (), (), (), (), (, (), (), (, (), (), (, (), (, (), (), (), (), (, (), (, (, (), (, (), (, (, (), (, (), (, (), (, (, (), (, (), (, (), (), (, (, (), (, (), (, (, (), (), (, (), (, (), (, (, (), (, (), (, (, ("}, {"heading": "6.3 The Message Propagations in a Border Polytree", "text": "There are two types of non-root boundaries in a BP: 1. Bi = {Bi \u2212 1, Ci}\\ Vi, obtained by the following rules 1-8 in paragraph 2.2, where Bi \u2212 1 is considered a single parent. (If Bi = X and Bi \u2212 1 = Y, then Ci = X, Vi = Y.) 2. Bi = 1, j, obtained by rule 12 or 13, with HBi = {Bi \u2212 1, j = 0,..., r}. Returning to the proofs of Lemmas 10 and 13, we see that they are still valid for the BPs. However, we must modify Lemmas 11 and 12 according to the type of boundary in a BP."}, {"heading": "6.3.1 The Downward Propagations", "text": "First we change Lemma 12: Lemma 20 For limit Bi = {Bi \u2212 1, Ci}\\ Vi, \u0430 (Bi) = \u2211 Vi \u03c6 (Ci) \u0445Bi (Bi \u2212 1)."}, {"heading": "If Bi is the only child of Bi\u22121,", "text": "\"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) \"(Bi)\" (Bi) (Bi) \"(Bi) (Bi) (Bi)\" (Bi) (Bi) \"(Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi)\" (Bi) (Bi) (Bi) \"(Bi) (Bi)\" (Bi) (Bi) (Bi) \"(Bi)\" (Bi) (Bi) \"(Bi) (Bi)\" (Bi) (Bi) (Bi) \"(Bi) (Bi) (Bi)\" (Bi) (Bi) \"(Bi) (Bi) (Bi)\" (Bi) (Bi) (Bi) \"(Bi) (Bi) (Bi)\" (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi) (Bi)"}, {"heading": "6.3.2 The Upward Propagations", "text": "We now present the following BP versions of Lemma 11: Lemma 23 For the limit Bi = {Bi \u2212 1, Ci}\\ Vi, \u0395Bi (Bi \u2212 1) = \u2211 Ci \u03c6 (Ci) \u0445 (Bi)."}, {"heading": "If Bi is the only child of Bi\u22121,", "text": "(Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi). (Bi). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi). (Bi). (Bi). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi. \u2212 \u2212 k). (Bi \u2212 \u2212 k). (Bi \u2212 \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k. (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k. (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k. (Bi \u2212 k). (Bi \u2212 k). (Bi \u2212 k). ("}, {"heading": "6.3.3 The Border Evidential Cores", "text": "A boundary is evident if one of its component variables is within the boundary EC. Suppose that the \"boundary evidence core\" (or \"boundary EC\") is the smallest sub-polytree containing all evidence object variables within its boundaries. For BN C, we assume E = {B, O, Q}. Then, the largest sub-polytree in Figure 8 includes the path {N, M, Q, V} \u2192 {N, P, Q} \u2192 {N, P, Q} \u2192 \u2192... \u2192 {D, F, O, P} and the path {B, C} \u2192 {B, C, G, N, P}. However, its boundary EC is smaller, including only the path {N, P, Q} \u2192 {B, C, G, N, P} \u2192 {B, C, O, P}. For some evidence sets E, the boundary EC is not unique. A boundary EC is \"inside\" if it is within the boundary EC; otherwise it is \"outside\" Bj \"if it is\" within \"both.\""}, {"heading": "6.3.4 The Boundary Conditions", "text": "As a matter of fact, the Bi is a \"Bi\" who is able to orient himself at the limit. (Bi) The Bi-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A."}, {"heading": "6.3.5 The Message Initializations", "text": "Next is the BP version of Theorem 17: Theorem 27 Consider a boundary chain EC. (a) If BR is a root with inner child BC, then BR is a root of the boundary EC. (b) If BL is a sheet, then all evidence variables are in TW \u2192 BR inside BR; so all evidence variables are in PBR = BR, W, HBR TW \u2192 BR. Hence follows IPBR = IBR. From definition (13), from Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section Section"}, {"heading": "6.3.6 The Collection Phase", "text": "The conclusions in a boundary polytree are performed in the same way as in a polytree: Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = B = B = B = B = B = B {B, O, O, P} N {B, P} N {B, P} N {B, P} N {B, P} N {B, P} N {B, P} N {B, P, P} N {B, P} N (N, P) N, P} N (N, P)."}, {"heading": "6.3.7 The Distribution Phase", "text": "In the survey phase, the messages converge to the external variable M (N, Q, Q, Q) Q = Q Q (Q, Q, Q) Q = Q (Q, Q, Q) Q (Q, Q) Q (Q) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B (B) B) B (B) B (B) B) B (B) B, B B B B, B (B) B, B (B) B, B (B) B, B (B) B, B (B) B, B (B) B, B, B (B) B, B (B) B, B, B (B) B) B (B), B, B (B) B (B), B, B (B) B) B (B), B (B) B), B (B), B (B), B (B), B), B (B), B (B), B (B), B), B (B), B (B), B (B), B (B), B), B (B), B (B), B (B), B (B), Q, Q (Q), Q (Q), Q (Q, Q, Q, Q, Q, Q, Q), Q (Q, Q), Q (Q), Q (Q, Q), Q (Q), Q (Q), Q (Q), Q (Q), Q (B), Q (B), Q (B), Q (B), Q (B), Q (B), Q (B), Q (Q (B), Q (B), Q (Q (B), Q (Q (B), Q (Q (B), Q (B), Q (B), Q (Q (B), Q (Q (Q (B), Q (B), Q (B), Q (B), Q (Q (B), Q (Q (B), Q (Q ("}, {"heading": "7 Discussions", "text": "In fact, the fact is that most of us are able to surpass ourselves, both in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move."}], "references": [{"title": "Node Aggregation for Distributed Inference in Bayesian Networks", "author": ["K. Chang", "R. Fung"], "venue": "In Proceedings of the 11th International Joint Conference on Artificial Intelligence, Detroit,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks", "author": ["G.F. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Local conditioning in Bayesian networks", "author": ["F. D\u0131\u0301ez"], "venue": "Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Distributed inference in Bayesian networks", "author": ["F. D\u0131\u0301ez", "J. Mira"], "venue": "Cybernetics and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "A survey of algorithms for real-time Bayesian network inference", "author": ["H. Guo", "W. Hsu"], "venue": "In the joint AAAI-02/KDD-02/UAI-02 workshop on Real-Time Decision Support and Diagnosis Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "A computational model for combined causal and diagnostic reasoning in inference engines", "author": ["J.H. Kim", "Pearl J"], "venue": "Proceedings of the 8th International Joint Conference on Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Probabilistic Graphical Models: Principles and Techniques, Massachusetts", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Performance evaluation of algorithms for soft evidential update in Bayesian networks: First results", "author": ["S. Langevin", "M. Valtorta"], "venue": "Scalable Uncertainty Management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Local computations with probabilities on graphical structures and their applications to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "A comparison of Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer architectures for computing marginals of probability distributions", "author": ["V. Lepar", "P. Shenoy"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "LAZY propagation: A junction tree inference algorithm based on lazy evaluation", "author": ["A.L. Madsen", "F.V. Jensen"], "venue": "Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Incremental Dynamic Construction of Layered Polytree Networks", "author": ["K. Ng", "T.S. Levitt"], "venue": "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "A constraint-propagation approach to probabilistic reasoning", "author": ["J. Pearl"], "venue": "Proceedings of the 2nd Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1986}, {"title": "Fusion, propagation and structuring in belief networks", "author": ["J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Evidential reasoning using stochastic simulation of causal models", "author": ["J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1987}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "Fusion and propagation with multiple observations in belief networks.", "author": ["M.A. Peot", "R.D. Shachter"], "venue": "Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1991}, {"title": "Evidence absorption and propagation through evidence reversals", "author": ["R.D. Shachter"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1990}, {"title": "Symbolic probabilistic inference in belief networks", "author": ["R.D. Shachter", "B. D\u2019Ambrosio", "B.D. Del Favero"], "venue": "Proceedings of the 8th National Conference on Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1990}, {"title": "Probabilistic Expert Systems, Society for Industrial and Applied Mathematics", "author": ["G. Shafer"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Exploiting causal independence in Bayesian network inference", "author": ["N.L. Zhang", "D. Poole"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}], "referenceMentions": [{"referenceID": 8, "context": "In [9], it was repeatedly emphasized that the efficiency and feasibility of their architecture depends on the size of the clique in a junction tree.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "\u201d ([9] referred to Shafer, 1996.", "startOffset": 3, "endOffset": 6}], "year": 2014, "abstractText": "In a Bayesian network, we wish to evaluate the marginal probability of a query variable, which may be conditioned on the observed values of some evidence variables. Here we first present our \u201cborder algorithm,\u201d which converts a BN into a directed chain. For the polytrees, we then present in details, with some modifications and within the border algorithm framework, the \u201crevised polytree algorithm\u201d by Peot & Shachter (1991). Finally, we present our \u201cparentless polytree method,\u201d which, coupled with the border algorithm, converts any Bayesian network into a polytree, rendering the complexity of our inferences independent of the size of network, and linear with the number of its evidence and query variables. All quantities in this paper have probabilistic interpretations.", "creator": "LaTeX with hyperref package"}}}