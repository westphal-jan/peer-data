{"id": "1411.6326", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2014", "title": "Vision and Learning for Deliberative Monocular Cluttered Flight", "abstract": "Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available.", "histories": [["v1", "Mon, 24 Nov 2014 02:09:59 GMT  (9124kb,D)", "http://arxiv.org/abs/1411.6326v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["debadeepta dey", "kumar shaurya shankar", "sam zeng", "rupesh mehta", "m talha agcayazi", "christopher eriksen", "shreyansh daftry", "martial hebert", "j", "rew bagnell"], "accepted": false, "id": "1411.6326"}, "pdf": {"name": "1411.6326.pdf", "metadata": {"source": "CRF", "title": "Vision and Learning for Deliberative Monocular Cluttered Flight", "authors": ["Debadeepta Dey", "Kumar Shaurya Shankar", "Sam Zeng", "Rupesh Mehta", "M. Talha Agcayazi", "Christopher Eriksen", "Shreyansh Daftry", "Martial Hebert", "J. Andrew Bagnell"], "emails": ["dbagnell}@ri.cmu.edu", "peshm@nvidia.com,", "magcayaz@gmu.edu"], "sections": [{"heading": null, "text": "This is particularly important because small UAVs do not have the payload and capability to carry such sensors. Moreover, most modern research on UAVs has been able to focus on high-altitude flights with largely free space [2]. Flying UAVs near the ground have been studied less by dense clutter [3] than by controlling the horizon. [4] It is a classic reasoning scheme that has been much successful in autonomous ground vehicles including five of the six finalists of the DARPA Urban Challenge [5]. Figure 2 shows an illustration of the retreating horizon control on our UAV in motion capture."}, {"heading": "II. APPROACH", "text": "The development and testing of all integrated modules of the shrinking horizon is a major challenge. Therefore, we developed a test protocol in which we assembled a rover in addition to a drone in order to be able to test different modules individually. Here, we describe the hardware platforms and the software architecture of our system."}, {"heading": "A. Hardware and Software Overview", "text": "In this section we describe the hardware platforms used in our experiments.In order to facilitate the parallel testing of the perception and planning of modules while the UAV control systems were being developed, we have put together a ground rover (Figure 3) that achieves the desired motion.The rover (Figure 3) uses an Ardupilot micrcontroller board [12] that takes high levels of control commands from the planner and controls four motors to achieve the desired motion.The rover is skid-controlled, and each motor also counts per rotation."}, {"heading": "B. Monocular Depth Prediction", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "C. Budgeted Feature Selection", "text": "While there are many types of visual characteristics that can be extracted, they must be calculated in real time. The faster the desired speed of the vehicle, the faster the perception and planning modules must work to maintain safety. Along with the limited computing power available on board a small drone, this imposes a very demanding budget within which to make a prediction. Each type of characteristic requires different time periods that must be extracted while contributing different amounts to predictive accuracy. For example, radon transformations could take relatively less time but contribute much to predictive accuracy, while another characteristic might take more time, but also relatively little or vice versa. This problem is made even more complicated by the \"grouping effects,\" where the performance of a particular characteristic is influenced by the presence or absence of other characteristics. In the face of a time budget, the naive but obvious solution is to enumerate all possible features, respect the group and the loss of the budget."}, {"heading": "D. Multiple Predictions", "text": "In fact, most of them are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) It is not so that they are able to understand the rules. (...) It is not so that they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...) \"(...)\" (...) \"((...)\" (...) \"((...)\" (...) \"(...\") (() (() () (()) (() () () () () () () () () ()) (()) (()) (()) (()) (()) (()) (()) (()) (() () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () ()) () () () () () () () () () ()) () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () (() () () (() () () (() () (() (() () ((() () () (() () ((() () () () () () ((() () () (((() () () (() () () (() (() () (((() () ((() () () ((("}, {"heading": "E. Pose Estimation", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "F. Planning and Control", "text": "In fact, it is the case that most people who live in the USA also live in other countries: in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "III. EXPERIMENTS", "text": "In this section, we analyze the performance of our proposed deliberate approach to autonomous drone navigation in confusing natural environments with only monocular vision. All experiments were conducted in a densely populated wooded area, while the drone is inhibited by light weight.Quantitatively, we evaluate the performance of our system by observing the average distance that the drone autonomously travels over several runs before an intervention.An intervention in this context is defined as the point at which the pilot must override the commands generated by our control system to prevent the drone from crashing. Experiments were conducted using both our proposed multiple prediction approach and a single best prediction, and the corresponding comparison was shown in Figure 16. Both tests were conducted in regions with high and low clutter density (approximately 1 tree per 6x6 m2 and 12x12 m2)."}, {"heading": "IV. CONCLUSION", "text": "Although we have achieved promising results with the current approach, a number of challenges remain, such as better management of sudden strong wind disturbances and control programs to make better use of the full dynamic hull of the vehicle. In the ongoing work, we are moving toward full onboard computation of all modules to reduce latency; we can use other sensor modes such as sparse but more accurate stereo depth estimates that can be used as \"anchor points\" to improve dense monocular depth estimation; nor can light weight lidars be actively focused into highly probable obstacle regions to reduce false positives and achieve accurate depth.Another key future effort is to integrate the purely reactive [3] approach with the reflection scheme described here for better performance."}], "references": [{"title": "Flying fast and low among obstacles: Methodology and experiments", "author": ["S. Scherer", "S. Singh", "L.J. Chamberlain", "M. Elgersma"], "venue": "IJRR, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A cascaded method to detect aircraft in video imagery", "author": ["D. Dey", "C. Geyer", "S. Singh", "M. Digioia"], "venue": "IJRR, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning monocular reactive uav control in cluttered natural environments", "author": ["S. Ross", "N. Melik-Barkhudarov", "K.S. Shankar", "A. Wendel", "D. Dey", "J.A. Bagnell", "M. Hebert"], "venue": "ICRA, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward reliable off road autonomous vehicles operating in challenging environments", "author": ["A. Kelly"], "venue": "IJRR, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Special issue on the 2007 darpa urban challenge, part i, ii, iii", "author": ["M. Buehler", "K. Iagnemma", "S. Singh"], "venue": "JFR, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Path diversity is only part of the problem", "author": ["R. Knepper", "M. Mason"], "venue": "ICRA, May 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Autonomous vision-based micro air vehicle for indoor and outdoor navigation", "author": ["K. Schmid", "P. Lutz", "T. Tomi\u0107", "E. Mair", "H. Hirschm\u00fcller"], "venue": "JFR, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Stereo visionbased obstacle avoidance for micro air vehicles using disparity space", "author": ["L. Matthies", "R. Brockers", "Y. Kuwata", "S. Weiss"], "venue": "ICRA, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Dense reconstruction on-the-fly", "author": ["A. Wendel", "M. Maurer", "G. Graber", "T. Pock", "H. Bischof"], "venue": "CVPR, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual control of navigation in insects and its relevance for robotics", "author": ["M.V. Srinivasan"], "venue": "Current opinion in neurobiology, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Vision-based control of near-obstacle flight", "author": ["A. Beyeler", "J.-C. Zufferey", "D. Floreano"], "venue": "Autonomous robots, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Ros: an open-source robot operating system", "author": ["M. Quigley", "K. Conley", "B.P. Gerkey", "J. Faust", "T. Foote", "J. Leibs", "R. Wheeler", "A.Y. Ng"], "venue": "ICRA Workshop on Open Source Software, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Implementation of the pure pursuit path tracking algorithm", "author": ["R.C. Coulter"], "venue": "DTIC Document, Tech. Rep., 1992.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning depth from single monocular images", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "NIPS, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Two-frame motion estimation based on polynomial expansion", "author": ["G. Farneb\u00e4ck"], "venue": "Image Analysis, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Support of radon transforms", "author": ["S. Helgason"], "venue": "Advances in Mathematics, 1980.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1980}, {"title": "A combined corner and edge detector.", "author": ["C. Harris", "M. Stephens"], "venue": "Alvey vision conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "Human detection using oriented histograms of flow and appearance", "author": ["N. Dalal", "B. Triggs", "C. Schmid"], "venue": "ECCV, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Pixel-level hand detection in ego-centric videos", "author": ["C. Li", "K.M. Kitani"], "venue": "CVPR, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Vowpal Wabbit", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": "2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Least squares revisited: Scalable approaches for multiclass prediction", "author": ["A. Agarwal", "S.M. Kakade", "N. Karampatziakis", "L. Song", "G. Valiant"], "venue": "arXiv preprint arXiv:1310.1949, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1949}, {"title": "Efficient feature group sequencing for anytime linear prediction", "author": ["H. Hu", "A. Grubb", "J.A. Bagnell", "M. Hebert"], "venue": "arXiv:1409.5495, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual sequence optimization with application to control library optimization", "author": ["D. Dey", "T.Y. Liu", "M. Hebert", "J.A.D. Bagnell"], "venue": "RSS, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Diverse m-best solutions in markov random fields", "author": ["D. Batra", "P. Yadollahpour", "A. Guzman-Rivera", "G. Shakhnarovich"], "venue": "Computer Vision\u2013ECCV 2012. Springer, 2012, pp. 1\u201316.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Dtam: Dense tracking and mapping in real-time", "author": ["R.A. Newcombe", "S.J. Lovegrove", "A.J. Davison"], "venue": "ICCV, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel tracking and mapping for small ar workspaces", "author": ["G. Klein", "D. Murray"], "venue": "ISMAR, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "An open source and open hardware embedded metric optical flow cmos camera for indoor and outdoor applications", "author": ["D. Honegger", "L. Meier", "P. Tanskanen", "M. Pollefeys"], "venue": "ICRA, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection and tracking of point features", "author": ["C. Tomasi", "T. Kanade"], "venue": "School of Computer Science, Carnegie Mellon Univ.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1991}, {"title": "Optimal sampling in the space of paths: Preliminary results", "author": ["C. Green", "A. Kelly"], "venue": "Robotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RI-TR-06-51, November 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "While autonomous flight with active sensors like lidars has been well studied [1], flight using passive sensors like cameras has relatively lagged behind.", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Additonally, most of the modern research on UAVs has focussed on flying at altitudes with mostly open space [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "Flying UAVs close to the ground through dense clutter [3] has been less explored.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Receding horizon control [4] is a classical deliberative scheme that has achieved much success in autonomous ground vehicles including five out of the six finalists of the DARPA Urban Challenge [5].", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "Receding horizon control [4] is a classical deliberative scheme that has achieved much success in autonomous ground vehicles including five out of the six finalists of the DARPA Urban Challenge [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "This is motivated by our previous work [3] we used imi-", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Since receding horizon control plans for longer horizons it achieves better plans and minimizes the chances of getting stuck [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "\u2022 A method to estimate depth: This can be obtained from stereo vision [7], [8] or dense structure-from-motion (SfM) [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "\u2022 A method to estimate depth: This can be obtained from stereo vision [7], [8] or dense structure-from-motion (SfM) [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "\u2022 A method to estimate depth: This can be obtained from stereo vision [7], [8] or dense structure-from-motion (SfM) [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 9, "context": "Biologists have found strong evidence that birds and insects use optical flow to navigate through dense clutter [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Optical flow has been used for autonomous flight of UAVs [11].", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": "Therefore we follow the same data driven principle as our previous work [3] and use local statistics of optical flow to serve as features in the monocular depth prediction module.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "The onboard processor is a quad-core ARM based computer which runs Ubuntu and ROS [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "The image stream from the front facing camera is streamed to the base station where the depth prediction module processes it; the trajectory evaluation module then finds the best trajectory to follow to minimize probability of collision and transmits it to the onboard computer where the trajectory following module runs a pure pursuit controller to do trajectory tracking [14].", "startOffset": 373, "endOffset": 377}, {"referenceID": 2, "context": "We use the features as used in previous work on monocular imitation learning [3] for UAVs, which are partly inspired by the work of Saxena et al.", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": ", [15].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "\u2022 Optical flow: We use the Farneback dense optical flow [16] implementation in OpenCV to compute for every patch the average, minimum and maximum optical flow values which are used as feature descriptors of that patch.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "\u2022 Radon Transform: The radon transform captures strong edges in a patch [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "\u2022 Structure Tensor: The structure tensor describes the local texture of a patch [18].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "For details on radon transform, structure tensor and Laws\u2019 masks usage see [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 17, "context": "for capturing texture information for human pose estimation as well as object detection [20].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "[21] to train a supervised tree detector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Recent linear regression implementations are very fast and can operate on millions of features in real time [22] but are limited in predictive performance by the inherent linearity assumption.", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "[23] develop fast iterative methods which use linear regression in the inner loop to obtain overall non-linear behavior.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We implemented Algorithm 2 in [23] and found that it lowered the error by 10 % compared to just linear regression, while still allowing real time prediction.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "[24] to efficiently select the nearoptimal set of features which meet the imposed budget constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "\u2019s [24] feature selection procedure.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "\u2019s method [24] and the lower x-axis shows the cumulative time taken for all features up to that point.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Therefore most monocular depth perception systems try to minimize easy to optimize surrogate loss functions like regularized L1 or L2 loss [15].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "In previous work [25], we have developed techniques for predicting a budgeted number of interpretations of an environment with applications to manipulation, planning and", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": ", [26] have also applied similar ideas to structured prediction problems in computer vision.", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "Dense methods such as DTAM [27] build a complete 3D model of the world that make tracking very easy, but require GPUs to run in real time.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "The other alternative is to use sparse mapping approaches like [28] that extract robustly", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "This motivated the use of a variant of a simple algorithm that has been presented quite often, most recently in [29].", "startOffset": 112, "endOffset": 116}, {"referenceID": 27, "context": "tracker [30] to detect where each pixel in a grid of pixels", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "We use a pure pursuit strategy [14] to successfully track it.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "As expected, the narrow FOV is now the least contributor to the failure cases as compared to a more reactive control strategy [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "Another central future effort is to integrate the purely reactive [3] approach with the deliberative scheme detailed here, for better performance.", "startOffset": 66, "endOffset": 69}], "year": 2014, "abstractText": "Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available.", "creator": "LaTeX with hyperref package"}}}