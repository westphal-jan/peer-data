{"id": "1411.7450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2014", "title": "Worst-Case Linear Discriminant Analysis as Scalable Semidefinite Feasibility Problems", "abstract": "In this paper, we propose an efficient semidefinite programming (SDP) approach to worst-case linear discriminant analysis (WLDA). Compared with the traditional LDA, WLDA considers the dimensionality reduction problem from the worst-case viewpoint, which is in general more robust for classification. However, the original problem of WLDA is non-convex and difficult to optimize. In this paper, we reformulate the optimization problem of WLDA into a sequence of semidefinite feasibility problems. To efficiently solve the semidefinite feasibility problems, we design a new scalable optimization method with quasi-Newton methods and eigen-decomposition being the core components. The proposed method is orders of magnitude faster than standard interior-point based SDP solvers.", "histories": [["v1", "Thu, 27 Nov 2014 02:52:56 GMT  (205kb,D)", "http://arxiv.org/abs/1411.7450v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hui li", "chunhua shen", "anton van den hengel", "qinfeng shi"], "accepted": false, "id": "1411.7450"}, "pdf": {"name": "1411.7450.pdf", "metadata": {"source": "CRF", "title": "Worst-Case Linear Discriminant Analysis as Scalable Semidefinite Feasibility Problems", "authors": ["Hui Li", "Chunhua Shen", "Anton van den Hengel", "Qinfeng Shi"], "emails": ["(chunhua.shen@adelaide.edu.au)."], "sections": [{"heading": null, "text": "In fact, it is so that most people are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In the other world it is so that they are able to survive themselves. In the second world it is so that they are living in the third world. In the third world it is so that they are living in the third world. In the third world it is so, in the third world it is so, in the third world it is so, in the third world it is so, in the third world it is so, in the third world it is so, in the third world it is so, in the third world it is so, in the third world it is so far, in the third world it is so, in the third world it is so far, in the third world it is so far, in the third world it is so far, in the third world it is so far, in the third world it is so far. In the third world it is so far, in the third world it is so far, in the third world it is so far, in the third world it is so far."}, {"heading": "II. WORST-CASE LINEAR DISCRIMINANT ANALYSIS", "text": "We will briefly introduce ourselves to the WLDA problem (5)."}, {"heading": "III. A FAST SDP APPROACH TO WLDA", "text": "In this section, problem (8) is first reformulated into a sequence of SDP optimization problems based on the bisection search, then a Frobenius norm regulation is introduced and the SDP problem is solved in each step using Lagrante dual formulation, using this SD-WLDA method to determine the global optimum for the relaxed problem (8), and the computational complexity can also be reduced by solving the dual problem using quasi-Newton methods, compared to solving the original problem directly using an inner point-based algorithm. Algorithm 1 Solving the problem (9) by bisection search. Input: \u03b4l: the lower limit of the ratio; \u03b4u: the upper limit of the ratio; and the tolerance \u03c3 > 0."}, {"heading": "A. Problem Reformulation", "text": "Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z? Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z Z? Z Z Z Z? Z Z Z? Z Z? Z Z? Z Z? Z Z? Z Z? Z Z? Z? Z Z? Z Z? Z Z? Z Z? Z? Z? Z Z? Z? Z? Z? Z? Z? Z? Z Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z Z? Z? Z? Z Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z? Z"}, {"heading": "B. Lagrangian Dual Formulation", "text": "Given the fact that the number of constraints in SDP systems is relatively high, it becomes quite expensive for processing methods such as quasi-Newton. Computational complexity is reduced to O (d3). The primary solution Z? can then be calculated from the dual solution based on Karush-Kuhn-Tucker (KKT)."}, {"heading": "C. Feasibility Condition", "text": "As stated in sentence 3.1, if (10) is feasible, a solution can be found by solving the problem (12). If the following conditions are met to solve the problem (12), an unfeasibility condition of the problem (10) must be iteratively checked, which is presented here: Proposal 3.3. If the following conditions are met, the problem (10) is considered unfeasible. This unfeasibility condition can be derived from a general conical feasibility problem set forth in [37]. Explanations are set out in detail in the appendix. We check this condition every time we iterate quasi-newton algorithms. A + is evaluated in the calculation of the double objective function so that it does not incur any additional computational costs. Once the condition (15) is met, (10) a problem (quasi-newton algorithms) is found and subsequently (Quasim-11) is not feasible (equivalent)."}, {"heading": "D. Solving the Feasibility Problem", "text": "In this subsection, we summarize the procedure for solving the problem (10) with our fast SDP optimization algorithm. It has been domesticated that we can find the viable solution for (10) by solving the dual problem (13) with the quasi-newton algorithm 2 optimization procedure to solve the problem (10). Input: \u03b4, Flag = 1. Initialize dual variables u, v, p.1). Solve the dual (13) with L-BFGS-B. Repeat 1.1). Calculate the goal and the course of the objective function in (13). 1.2). Check the feasibility condition (15): If the condition (15) is met, thenflag = 0 and break if 1.3). Update dual variables u, v, p.until L-BFGS-B is converged. 2) Compute X? with the own decomposition. 3) Decompile problem, FZ-B: Flag is executable."}, {"heading": "E. Computational Complexity Analysis", "text": "The computational complexity of L-BFGS-B is O (Km), where K is a moderate number between 3 and 20, m = 1 2 (c 3 \u2212 c2) + 1 + 12 (d 2 + d) is the problem size to be solved by L-BFGS-B, which corresponds to the number of constraints of the original SDP problem (12). In each iteration of L-BFGSB, the self-decomposition of a 2d \u00d7 2d matrix is performed to calculate A +, which is used to evaluate all dual objective values, gradients and feasibility conditions (15). The computational complexity is O (d3). Therefore, the total computational complexity of our algorithm is SD-WLDA O (Km + d3). Since Km d3 is the computational time of SD-WDP problems much faster than a large case of LDP-3 (on the other scale SDP-d3)."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, experiments are performed to verify the performance of SD-WLDA. We perform comparisons between SD-WLDA and other methods of both classification performance and computational complexity. Classification performance is contrasted between SD-WLDA and LDA, LMNN, OLDA. We also compare the performance of our SDWLDA with the two optimization methods proposed by Zhang et al. in [5] (Zhang et al. (SDP) and Zhang et al. (SOCP) receptive). This can be used to verify the accuracy of our algorithm. Computational complexity is also compared between SD-WLDA, standard interior point algorithms for solving our SDP formulation (SDPT3 [40] and SeDuMi [41])), Zhang et al. (SDP) computational complexity is also set between SD-WLDA, standard interior point algorithms for solving our SDP formulation (SDP 41) and SDP-DDP algorithm (SDP)."}, {"heading": "A. Experiments on UCI Datasets", "text": "It is only a matter of time before it happens, until it happens."}, {"heading": "B. Experiments on Face, Object and Letter Datasets", "text": "In fact, it is in such a way that we see ourselves in a position to enter another world, in which we enter another world, in which we enter another world, in which we enter another world, in which we enter another world, in which we enter another world, in which we enter another world, in which we enter into another world, in which we enter into which we enter into another world, in which we enter into which we enter into which we enter into another world, in which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into another world, in which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we, in which we enter which we enter which we enter into which we enter into which we enter which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we enter into which we into which we into which we enter into which we enter into which we enter into which we"}, {"heading": "V. CONCLUSION", "text": "In this thesis, an efficient SDP optimization algorithm was proposed to solve the problem of linear discriminant analysis in the worst case. WLDA takes into account the paired distance between and within the classes, thereby achieving better classification performance than conventional LDA. To reduce the computational complexity so that it can be applied to major problems, the introduction of the Frobenius standard regulation presented a fast algorithm whose Lagrangian dual can be simplified. Using our algorithm, the global optimum can be achieved in O (d3) time. The algorithm is easy to implement and much faster than conventional SDP solvers. Experimental results in some UCI databases, as well as face and object recognition tasks show the effectiveness in classification performance and efficiency in calculating SD-WLDA."}], "references": [{"title": "Adjustment learning and relevant component analysis", "author": ["N. Shental", "T. Hertz", "D. Weinshall", "M. Pavel"], "venue": "Proc. Eur. Conf. Comp. Vis., 2002, pp. 776\u2013790.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L. Saul"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 18, 2006, pp. 1473\u20131481.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A new lda-based face recognition system which can solve the small sample size problem", "author": ["L.-F. Chen", "H.-Y.M. Liao", "M.-T. Ko", "J.-C. Lin", "G.-J. Yu"], "venue": "Pattern Recogn., vol. 33, no. 10, pp. 1713\u20131726, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems", "author": ["J. Ye", "B. Yu"], "venue": "J. Mach. Learn. Res., vol. 6, no. 4, pp. 483\u2013502, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Worst-case linear discriminant analysis", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 23, 2010, pp. 2568\u20132576.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhanced fisher discriminant criterion for image recognition", "author": ["Q. Gao", "J. Liu", "J. Zhang", "J. Hou", "X. Yang"], "venue": "Pattern Recogn., vol. 45, no. 10, pp. 3717\u20133724, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric mean for subspace selection", "author": ["D. Tao", "X. Li", "X. Wu", "S.J. Maybank"], "venue": "IEEE Trans. Anal. Mach. Intell., vol. 31, no. 2, pp. 260\u2013274, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Harmonic mean for subspace selection", "author": ["W. Bian", "D. Tao"], "venue": "in Proceedings of the 19th International Conference on Pattern Recognition, 2008, pp. 1\u20134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Max-min distance analysis by using sequential sdp relaxation for dimension reduction", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Anal. Mach. Intell., vol. 33, no. 5, pp. 1037\u20131050, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Asymptotic generalization bound of fisher\u2019s linear discriminant analysis", "author": ["\u2014\u2014"], "venue": "CoRR, vol. abs/1208.3030, 2012. [Online]. Available: http://arxiv.org/abs/1208.3030", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A scalable dual approach to semidefinite metric learning", "author": ["C. Shen", "J. Kim", "L. Wang"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011, pp. 2601\u20132608.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational and theoretical analysis of null space and orthogonal linear discriminant analysis", "author": ["J. Ye", "T. Xiong"], "venue": "J. Machine Learning Research, vol. 7, pp. 1183\u20131204, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "General averaged divergence analysis", "author": ["D. Tao", "X. Li", "X. Wu", "S.J. Maybank"], "venue": "Proc. of IEEE International Conference on Data Mining, 2007, pp. 302\u2013311.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Trace ratio problem revisited", "author": ["Y. Jia", "F. Nie", "C. Zhang"], "venue": "IEEE Trans. Neural Netw., vol. 20, no. 4, pp. 729\u2013735, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A generalized foleysammon transform based on generalized fisher discriminant criterion and its application to face recognition", "author": ["Y. Guo", "S. Li", "J. Yang", "T. Shu", "L. Wu"], "venue": "Pattern Recogn. Lett., vol. 24, no. 1-3, pp. 147\u2013158, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Trace ratio vs. ratio trace for dimensionality reduction", "author": ["H. Wang", "S. Yan", "D. Xu", "X. Tang", "T. Huang"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2007, pp. 1\u20138.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A geometric revisit to the trace quotient problem", "author": ["H. Shen", "K.Diepold", "K. Hueper"], "venue": "Proc. of 19th International Symposium of Mathematical Theory of Networks and Systems, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Trace optimization and eigenproblems in dimension reduction methods", "author": ["E. Kokiopoulou", "J. Chen", "Y. Saad"], "venue": "Numerical Linear Algebra with Applications, vol. 18, no. 3, pp. 565\u2013602, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 15, 2003, pp. 521\u2013528.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Distance metric learning for large margin nearest neighbour classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "J. Mach. Learn. Res., vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised dimensionality reduction via sequential semidefinite programming", "author": ["C. Shen", "H. Li", "M.J. Brooks"], "venue": "Pattern Recogn., vol. 41, pp. 3644\u20133652, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast semidefinite approach to solving binary quadratic problems", "author": ["P. Wang", "C. Shen", "A. van den Hengel"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013, pp. 1312\u20131319.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Least-squares covariance matrix adjustment", "author": ["S. Boyd", "L. Xiao"], "venue": "SIAM J. Matrix Anal. Appl., vol. 27, no. 2, pp. 532\u2013546, 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "A dual approach to semidefinite least-squares problems", "author": ["J. Malick"], "venue": "SIAM J. Matrix Anal. Appl., vol. 26, no. 1, pp. 272\u2013284, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "A newton-cg augmented lagrangian method for semidefinite programming", "author": ["X.-Y. Zhao", "D. Sun", "K.-C. Toh"], "venue": "SIAM J. Optim., vol. 20, no. 4, pp. 1737\u20131765, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Calibrating least squares covariance matrix problems with equality and inequality constraints", "author": ["Y. Gao", "D. Sun"], "venue": "SIAM J. Matrix Anal. Appl., vol. 31, pp. 1432\u20131457, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim., vol. 19, no. 4, pp. 1574\u20131609, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Alternating direction augmented lagrangian methods for semidefinite programming", "author": ["Z. Wen", "D. Goldfarb", "W. Yin"], "venue": "Mathematical Programming Computation, vol. 2, no. 3-4, pp. 203\u2013230, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "Proc. Int. Conf. Mach. Learn., 2013, pp. 80\u201388.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient low-rank stochastic gradient descent methods for solving semidefinite programs", "author": ["J. Chen", "T. Yang", "S. Zhu"], "venue": "Proc. Int. Workshop Artificial Intell. & Statistics, 2014, pp. 122\u2013130.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "Proc. Int. Conf. Mach. Learn., 2004, pp. 743\u2013750.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Informationtheoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proc. Int. Conf. Mach. Learn., 2007, pp. 209\u2013216.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 8, 2008, pp. 761\u2013768.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Positive semidefinite metric learning using boosting-like algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "J. Machine Learning Research, vol. 9, no. 1, pp. 1007\u20131036, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "On the sum of the largest eigenvalues of a symmetric matrix", "author": ["M.L. Overton", "R.S. Womersley"], "venue": "SIAM J. Matrix Anal. Appl., vol. 13, no. 1, pp. 41\u201345, 1992.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1992}, {"title": "Projection methods for conic feasibility problems, applications to polynomial sum-of-squares decompositions", "author": ["D. Henrion", "J. Malick"], "venue": "Optim. Methods and Softw., vol. 26, no. 1, pp. 23\u201346, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithm 778: L-BFGS- B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Transaction on Mathematical Software, vol. 23, pp. 550\u2013560, 1997.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1997}, {"title": "Remark on algorithm 778: L-bfgsb: Fortran subroutines for large-scale bound constrained optimization", "author": ["J.L. Morales", "J. Nocedal"], "venue": "ACM Transactions on Mathematical Software (TOMS), vol. 38, no. 1, p. 7, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "SDPT3\u2014a MATLAB software package for semidefinite programming", "author": ["K.C. Toh", "M. Todd", "R.H. Ttnc"], "venue": "Optimizat. Methods & Softw., vol. 11, pp. 545\u2013581, 1999.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones", "author": ["J.F. Sturm"], "venue": "Optimizat. Methods & Softw., vol. 11, pp. 625\u2013 653, 1999.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1999}, {"title": "UCI machine learning repository. University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Frank", "A. Asuncion"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Proc. of 2nd IEEE workshop on Applications of Computer Vision, 1994, pp. 138\u2013142.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "venue": "IEEE Trans. Anal. Mach. Intell., vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1997}, {"title": "Characterizing virtual eigensignatures for general purpose face recognition", "author": ["D.B. Graham", "N.M. Allinson"], "venue": "Face Recognition: From Theory to Applications; NATO ASI Series F, Computer and Systems Sciences, vol. 163, pp. 446\u2013456, 1998.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "Technical Report CUCS-005-96, Feb 1996.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1996}, {"title": "Columbia object image library (coil-100)", "author": ["\u2014\u2014"], "venue": "Technical Report CUCS-006-96, Feb 1996.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1996}, {"title": "The amsterdam library of object images", "author": ["J. Geusebroek", "G. Burghouts", "A. Smeulders"], "venue": "Int\u2019l J. Computer Vision, vol. 61, no. 1, pp. 103\u2013112, 2005.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "There are a large number of extension works to LDA and PCA [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In [5], the problem of WLDA was firstly relaxed to a metric learning problem on Z=WW>, which can be solved by a sequence of SDP optimization procedures, where SDP problems are solved by standard interior-point methods (We denote it as Zhang et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "An alternative optimization procedure was then proposed in [5] for large-scale WLDA problems, in which one column of the transformation matrix was found at each iteration while fixing the other directions.", "startOffset": 59, "endOffset": 62}, {"referenceID": 10, "context": "Motivated by [11], a Frobenius-norm regularized SDP formulation is used, and its Lagrangian dual can be solved effectively by quasi-Newton methods.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 232, "endOffset": 235}, {"referenceID": 2, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 259, "endOffset": 262}, {"referenceID": 3, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 286, "endOffset": 289}, {"referenceID": 5, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 337, "endOffset": 340}, {"referenceID": 6, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 389, "endOffset": 392}, {"referenceID": 7, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 440, "endOffset": 443}, {"referenceID": 8, "context": "Dimensionality reduction In order to overcome the drawbacks of LDA and improve the accuracy in classification, many extensions have been proposed, such as relevant component analysis (RCA) [1], neighborhood component analysis (NCA) [2], null space LDA (NLDA) [3], orthogonal LDA (OLDA) [4], Enhanced fisher discriminant criterion (EFDC) [6], Geometric mean-based subspace selection (GMSS) [7], Harmonic mean-based subspace selection (HMSS) [8], and Max-min distance analysis (MMDA) [9].", "startOffset": 482, "endOffset": 485}, {"referenceID": 0, "context": "Assuming dimensions with large within-class covariance are not relevant to subsequent classification tasks, RCA [1] assigns large weights to \u201crelevant dimensions\u201d and small weights to \u201cirrelevant dimensions\u201d, where the relevance is estimated using equivalence constraints.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "NCA [2] learns the transformation matrix W directly by minimizing the expected leave-one-out classification error of k-nearest neighbours on the transformed space.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "NLDA [3], OLDA [4] and EFDC [6] were proposed to address the problem that standard LDA fails when scatter matrices are singular.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "NLDA [3], OLDA [4] and EFDC [6] were proposed to address the problem that standard LDA fails when scatter matrices are singular.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "NLDA [3], OLDA [4] and EFDC [6] were proposed to address the problem that standard LDA fails when scatter matrices are singular.", "startOffset": 28, "endOffset": 31}, {"referenceID": 11, "context": "The resulting transformation matrices are both orthogonal for NLDA and OLDA, and they are equivalent to each other under a mild condition [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "[13] proposed a general averaged divergence analysis (GADA) framework, which presented a general mean function in place of the arithmetic mean used in LDA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "GMSS [7] investigates the effectiveness of the geometric mean-based subspace selection, which maximizes the geometric mean of Kullback-Leibler (KL) divergences between different class pairs.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "HMSS [8] maximizes the harmonic mean of the symmetric KL divergences between all class pairs.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Instead of assigning weights to class pairs, MMDA [9] directly maximizes the minimum pairwise distance of all class pairs in the low-dimensional subspace, which guarantees the separation of all class pairs.", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "[10] presented an asymptotic generalization analysis of LDA, which enriched the existing theory of LDA further.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Many dimensionality reduction algorithm such as PCA and LDA can be formulated into a trace ratio optimization problem [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "[15] presented a generalized Fisher discriminant criterion, which is essentially a trace ratio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] tackled the trace ratio problem directly by an efficient iterative procedure, where a trace difference problem was solved via the eigendecomposition method in each step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] provided a geometric revisit to the trace ratio problem in the framework of optimization on the Grassmann manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Different from [16], they proposed another efficient algorithm, which employed only one step of the parallel Rayleigh quotient iteration at each iteration.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "[18] also treated the dimensionality reduction problem as trace optimization problems, and gave an overview of the eigenvalue problems encountered in dimensionality reduction area.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Furthermore, different from the iterative algorithm for trace ratio optimization problem [16], we formulate the WLDA problem as a sequence of SDP problems, and propose an efficient SDP solving method.", "startOffset": 89, "endOffset": 93}, {"referenceID": 18, "context": "[19] formulated metric learning as a convex (SDP) optimization problem, and a globally optimal solution can be obtained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] presented a distance metric learning method, which optimizes a Mahalanobis metric", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "MMDA [9] was solved approximately by a sequence of SDP problems using standard interior-point methods.", "startOffset": 5, "endOffset": 8}, {"referenceID": 20, "context": "[21] proposed a novel SDP based method for directly solving trace quotient problems for dimensionality reduction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] proposed a fast SDP approach for solving Mahalanobis metric learning problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] also employed a similar dual approach to solve binary quadratic problems for computer vision tasks, such as image segmentation, co-segmentation, image registration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 33, "endOffset": 37}, {"referenceID": 22, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "SDP optimization method in [11], [22] can be seen as an extension of the works in [23], [24], which considered semidefinite least-squares problems.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "The key motivation of [23], [24] is that the objective function of the corresponding dual problem is continuously differentiable but not twice differentiable, therefore first-order methods can be applied.", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "The key motivation of [23], [24] is that the objective function of the corresponding dual problem is continuously differentiable but not twice differentiable, therefore first-order methods can be applied.", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Malick [24] and Boyd and Xiao [23] proposed to use quasi-Newton methods and projected gradient methods respectively, to solve the Lagrangian dual of semidefinite leastsquares problems.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "Malick [24] and Boyd and Xiao [23] proposed to use quasi-Newton methods and projected gradient methods respectively, to solve the Lagrangian dual of semidefinite leastsquares problems.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "Semismooth Newton-CG methods [25] and smoothing Newton methods [26] are also exploited for semidefinite least-squares problems, which require much less number of iterations at the cost of higher computational complexity per iteration (full eigen-decomposition plus conjugate gradient).", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "Semismooth Newton-CG methods [25] and smoothing Newton methods [26] are also exploited for semidefinite least-squares problems, which require much less number of iterations at the cost of higher computational complexity per iteration (full eigen-decomposition plus conjugate gradient).", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "Alternatively, stochastic (sub)gradient descent (SGD) methods [27] were also employed to solve SDP problems.", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "Combining with alternating direction methods [28], [29], SGD can be used for SDP problems with inequality and equality constraints.", "startOffset": 45, "endOffset": 49}, {"referenceID": 28, "context": "Combining with alternating direction methods [28], [29], SGD can be used for SDP problems with inequality and equality constraints.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "[30] proposed a low-rank SGD method, in which rankk stochastic gradient is constructed and then the projection operation is simplified to compute at most k eigenpairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "In the works of [31], [32], [33], [34], the distance metric is updated by rank-one matrices iteratively, and no eigen-decomposition or only one leading eigenpair is required.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "Note that SGD methods usually need more iterations to converge than the dual approaches based on quasi-Newton methods [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "\u2019s [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We use similar SDP optimization technique as that in [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "However, SDP feasibility problems are considered in our paper while the work in [11] focuses on standard SDP problems with linear objective functions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "We briefly review WLDA problem proposed by [5] firstly.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "As stated in [5], this problem (7) is not easy to optimize with respect to W, so a new variable Z = WW> \u2208 Rd\u00d7d is introduced, and the problem (7) is formulated as a metric learning problem.", "startOffset": 13, "endOffset": 16}, {"referenceID": 34, "context": "This theorem has been widely used and its proof can be found in [35].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "variable Z [5]:", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "In [5], Zhang et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 35, "context": "This infeasibility condition can be deduced from a general conic feasibility problem presented in [37].", "startOffset": 98, "endOffset": 102}, {"referenceID": 36, "context": "In this work, we use L-BFGS-B [38], [39], a limitedmemory quasi-Newton algorithm package, which can handle the problem with box constraints.", "startOffset": 30, "endOffset": 34}, {"referenceID": 37, "context": "In this work, we use L-BFGS-B [38], [39], a limitedmemory quasi-Newton algorithm package, which can handle the problem with box constraints.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "in [5] (Zhang et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 38, "context": "The computational complexity is compared between SD-WLDA, standard interior-point algorithms to solve our SDP formulation (SDPT3 [40] and SeDuMi [41]), Zhang et al.", "startOffset": 129, "endOffset": 133}, {"referenceID": 39, "context": "The computational complexity is compared between SD-WLDA, standard interior-point algorithms to solve our SDP formulation (SDPT3 [40] and SeDuMi [41]), Zhang et al.", "startOffset": 145, "endOffset": 149}, {"referenceID": 40, "context": "Some UCI datasets [42] are used here firstly.", "startOffset": 18, "endOffset": 22}, {"referenceID": 41, "context": "ORL [43] consists of 400 face images of 40 individuals, each with 10 images.", "startOffset": 4, "endOffset": 8}, {"referenceID": 42, "context": "The Yale dataset [44] contains 165 grey-scale images of 15 individuals, 11 images per subject.", "startOffset": 17, "endOffset": 21}, {"referenceID": 43, "context": "UMist dataset [46] 10 20 30 40 50 60 70 80 90 20 25 30 35 40 45", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "In order to illustrate the computational speed of SD-WLDA and both methods in [5] with respect to the number of classes and the input data dimension (here it refers to the dimension after PCA) respectively, more experiments are performed on Yale dataset.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "(SDP), as [5] presented.", "startOffset": 10, "endOffset": 13}, {"referenceID": 44, "context": "2) Object recognition: Three datasets are used here: Coil20 [47], Coil30 [48], and ALOI [49].", "startOffset": 60, "endOffset": 64}, {"referenceID": 45, "context": "2) Object recognition: Three datasets are used here: Coil20 [47], Coil30 [48], and ALOI [49].", "startOffset": 73, "endOffset": 77}, {"referenceID": 46, "context": "2) Object recognition: Three datasets are used here: Coil20 [47], Coil30 [48], and ALOI [49].", "startOffset": 88, "endOffset": 92}], "year": 2014, "abstractText": "In this paper, we propose an efficient semidefinite programming (SDP) approach to worst-case linear discriminant analysis (WLDA). Compared with the traditional LDA, WLDA considers the dimensionality reduction problem from the worstcase viewpoint, which is in general more robust for classification. However, the original problem of WLDA is non-convex and difficult to optimize. In this paper, we reformulate the optimization problem of WLDA into a sequence of semidefinite feasibility problems. To efficiently solve the semidefinite feasibility problems, we design a new scalable optimization method with quasi-Newton methods and eigen-decomposition being the core components. The proposed method is orders of magnitude faster than standard interior-point based SDP solvers. Experiments on a variety of classification problems demonstrate that our approach achieves better performance than standard LDA. Our method is also much faster and more scalable than standard interior-point SDP solvers based WLDA. The computational complexity for an SDP with m constraints and matrices of size d by d is roughly reduced from O(m+md+md) to O(d) (m > d in our case).", "creator": "LaTeX with hyperref package"}}}