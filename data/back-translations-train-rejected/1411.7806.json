{"id": "1411.7806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2014", "title": "Two Gaussian Approaches to Black-Box Optomization", "abstract": "Outline of several strategies for using Gaussian processes as surrogate models for the covariance matrix adaptation evolution strategy (CMA-ES).", "histories": [["v1", "Fri, 28 Nov 2014 10:39:24 GMT  (10kb)", "http://arxiv.org/abs/1411.7806v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["luk\\'a\\v{s} bajer", "martin hole\\v{n}a"], "accepted": false, "id": "1411.7806"}, "pdf": {"name": "1411.7806.pdf", "metadata": {"source": "CRF", "title": "Two Gaussian Approaches to Black-Box Optomization", "authors": ["Luk\u00e1\u0161 Bajer", "Martin Hole\u0148a"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 1.78 06v1 [cs.NE] 2 8N ov2 014Two Gaussian approaches to black box optomization Luka \u0301 s and Bajer Martin Holen"}, {"heading": "1 CMA Evolution Strategy", "text": "CMA-ES [7, 8] is the most advanced evolutionary optimization method, at least in the area of continuous black box optimization. Essentially, it consists in generating new search points by selecting from a multidimensional normal distribution, the mean and variance of which is updated from generation to generation. In particular, the population x (g + 1) 1,.., x (g + 1) Rd follows the g + 1 generation, g (g) 1, the normal distribution with the mean m (g) and the variance (g). Here, the eigensize (g) 2C (g), which results from the update of the g-th generation, x (g + 1) i N (g), (g) 2C (g) 2C (g), eigensize (g) and the step size in the g-th generation."}, {"heading": "2 Optimization Based on Gaussian Processes", "text": "A Gaussian Process (GP) on a d-dimensional Euclidean space X is a collection of random variables, GPX = (f) x x x X, so that the common distribution of any finite number of them is a multidimensional normal distribution. Following [4], we distinguish two types of use of GPs in black box optimization: (i) As a surrogate model to be optimized instead of the black box objective function, the original black box objective function is then evaluated on the optimum or optimum found. This use of GPs was introduced in the popular Global Optimization (EGO). Since the optimization method can also be used for evolutionary optimization, especially CMA-ES [4], this is not the only possibility: For example, traditional low-grade polynomial models, aka response surface models [20] are much more efficient using traditional smooth optimization methods."}, {"heading": "2.1 GP-based criteria to choose the points for evaluation", "text": "While traditional response area and replacement models basically use only one criterion for selecting the points where the black box lens function should be evaluated, namely the global or at least local minimum of the model (if the optimization goal is minimization), GPs offer several additional criteria: (i) minimum of a prescribed quantity (Q\u03b1) of the distribution of f (x) | f (x1),..., f (xn), \u03b1 (0, 1).x; x; \u03b1 \u03b1 = argmin x x x (X, 1), x (x, Y), f (x, Y))). (18) Usually, (18) is expressed with quantities of the standard normal distribution, u\u03b1 = q\u03b1 (N (0, 1).X), x (x), x (x), f), f (argmin), f (x, f), x."}, {"heading": "3 Possible Synergy", "text": "A direct connection between the two Gaussian approaches considered is not possible, since the normal distribution in CMA-ES is a distribution at the entrance area of objective function (fitness), while the normal distribution in general practitioners is within the space of their functional values. Nevertheless, it is still possible to achieve a certain synergy by using information from CMA-ES for the general practitioner and / or information from the general practitioner for CMA-ES. According to the one recalled at the beginning of Section 2, the latter possibility corresponds to the use of general practitioners to control the evolution of CMA-ES."}, {"heading": "3.1 Using Information from CMA-ES for the GP", "text": "We see 2 simple ways in which some information from CMA-ES can be used in GP.1.... The function f (x (g) \u03bb) of individuals of a certain generation or several generations of CMA-ES occurring in (9), (10) and (11) may be as simple as setting f (x (g) 1) for a constant aggregation of the fitness values considered, e.g. their mean or weighted mean, but it may also consist in training with these values of a response surface model, in principle of any kind. 2. Kruisselbrink et al. [16], combining CMA-ES with a GP using the gamma function K, propose to use a quadrix in (15) the Mahalanobis removal of vectors x and x \u00b2 for the use of the model CMA-ES."}, {"heading": "3.2 GP-Based Evolution Control of CMA-ES", "text": "We intend to test the following approaches to using GP for the evolutionary control of CMAES. (i) Basic Approach. (g + 1) Basic Approaches. (g + 1) Basic Approaches. (g + 1) Basic Approaches. (g) Basic Approaches. (g + 1) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Approaches. (g) Basic Applications. (g) Basic Applications. (g) Basic Applications. (g). (g) Basic Applications. (g)."}], "references": [{"title": "Model guided sampling optimization with Gaussian processes for expensive black-box optimization", "author": ["L. Bajer", "V. Charypar", "M. Hole\u0148a"], "venue": "GECCO 2013, pages 1715\u20131716", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigating the local-meta-model cmaes for large population sizes", "author": ["Z. Bouzarkouna", "A. Auger", "D.Y. Ding"], "venue": "EvoApplications 2010, pages 402\u2013411", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Local-meta-model cma-es for partially separable functions", "author": ["Z. Bouzarkouna", "A. Auger", "D.Y. Ding"], "venue": "GECCO 2011, pages 869\u2013876", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating evolutionary algorithms with Gaussian process fitness function models", "author": ["D. B\u00fcche", "N.N. Schraudolph", "P. Koumoutsakos"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 35:183\u2013194", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Evolutionary optimization for computationally expensive problems using Gaussian processes", "author": ["M.A. El-Beltagy", "A.J. Keane"], "venue": "International Conference on Artificial Intelligence, pages 708\u2013714", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Metamodelassisted evolution strategies", "author": ["M. Emmerich", "A. Giotis", "M. \u00d6zdemir", "T. B\u00e4ck", "K. Giannakoglou"], "venue": "PPSN VII, pages 361\u2013370", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "The CMA evolution strategy: A comparing review", "author": ["N. Hansen"], "venue": "Towards a New Evolutionary Computation, pages 75\u2013102. Springer Verlag, Berlin", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Completely derandomized self-adaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermaier"], "venue": "Evolutionary Computation, 9:159\u2013195", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Controlled model assisted evolution strategy with adaptive preselection", "author": ["E. Hoffman", "S. Holemann"], "venue": "International IEEE Symposium on Evolving Fuzzy Systems, pages 182\u2013187", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Surrogate modeling in the evolutionary optimization of catalytic materials", "author": ["M. Hole\u0148a", "D. Linke", "L. Bajer"], "venue": "GECCO 2012, pages 1095\u20131102", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A comprehensive survery of fitness approximation in evolutionary computation", "author": ["Y. Jin"], "venue": "Soft Computing, 9:3\u201312", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Managing approximate models in evolutionary aerodynamic design optimization", "author": ["Y. Jin", "M. Olhofer", "B. Sendhoff"], "venue": "CEC 2001, pages 592\u2013599", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "A taxonomy of global optimization methods based on response surfaces", "author": ["D.R. Jones"], "venue": "Journal of Global Optimization, 21:345\u2013383", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D.R. Jones", "M. Schonlau", "W.J. Welch"], "venue": "Journal of Global Optimization, 13:455\u2013492", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Local metamodels for optimization using evolution strategies", "author": ["S. Kern", "N. Hansen", "P. Koumoutsakos"], "venue": "PPSN IX, pages 939\u2013948", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "A robust optimization approach using kriging metamodels for robustness approximation in the CMA-ES", "author": ["J.W. Kruisselbrink", "M.T.M. Emmerich", "A.H. Deutz", "T. B\u00e4ck"], "venue": "CEC 2010, pages 1\u20138", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparison-based optimizers need comparison-based surrogates", "author": ["I. Loshchilov", "M. Schoenauer", "M. Seba"], "venue": "PPSN XI, pages 364\u2013373", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-adaptive surrogate-assisted covariance matrix adaptation evolution strategy", "author": ["I. Loshchilov", "M. Schoenauer", "M. Seba"], "venue": "GECCO 2012, pages 321\u2013328", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Intensive surrogate model exploitation in self-adaptive surrogate-assisted CMA-ES (saACM-ES)", "author": ["I. Loshchilov", "M. Schoenauer", "M. Seba"], "venue": "GECCO 2013, pages 439\u2013446", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Response Surface Methodology: Proces and Product Optimization Using Designed Experiments", "author": ["R.H. Myers", "D.C. Montgomery", "C.M. Anderson-Cook"], "venue": "John Wiley and Sons, Hoboken", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Gaussian Process for Machine Learning", "author": ["E. Rasmussen", "C. Williams"], "venue": "MIT Press, Cambridge", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Ordinal regression in evolutionary computation", "author": ["T.P. Runarsson"], "venue": "PPSN IX, pages 1048\u20131057", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Evolution strategies assisted by Gaussian processes with improved pre-selection criterion", "author": ["H. Ulmer", "F. Streichert", "A. Zell"], "venue": "IEEE Congress on Evolutionary Computation, pages 692\u2013699", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization.", "startOffset": 7, "endOffset": 13}, {"referenceID": 7, "context": "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization.", "startOffset": 7, "endOffset": 13}, {"referenceID": 3, "context": "Following [4], we differentiate two ways of using GPs in black-box optimization:", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "This use of GPs has been introduced in the popular Efficient Global Optimization (EGO) algorithm [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 3, "context": "As the optimization method, also evolutionary optimization can be used, in particular CMA-ES [4], but this is not the only possibility: For example, traditional low-degree polynomial models, aka response surface models [20], are much more efficiently optimized using traditional smooth optimization methods.", "startOffset": 93, "endOffset": 96}, {"referenceID": 19, "context": "As the optimization method, also evolutionary optimization can be used, in particular CMA-ES [4], but this is not the only possibility: For example, traditional low-degree polynomial models, aka response surface models [20], are much more efficiently optimized using traditional smooth optimization methods.", "startOffset": 219, "endOffset": 223}, {"referenceID": 4, "context": ", for controlling the composition of its population [5, 6, 9, 23].", "startOffset": 52, "endOffset": 65}, {"referenceID": 5, "context": ", for controlling the composition of its population [5, 6, 9, 23].", "startOffset": 52, "endOffset": 65}, {"referenceID": 8, "context": ", for controlling the composition of its population [5, 6, 9, 23].", "startOffset": 52, "endOffset": 65}, {"referenceID": 22, "context": ", for controlling the composition of its population [5, 6, 9, 23].", "startOffset": 52, "endOffset": 65}, {"referenceID": 20, "context": "For the latter, it can be shown [21] that the function describing its mean fulfils (\u2200x \u2208 X) \u03bc0(x;X, Y ) = K(x,X)(K(X,X) + \u03c3 2 noiseIn) Y , (6)", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "In that case, it can be shown [21] that for x \u2208 X,", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "and the notation \u03c6 for the density of N(0, 1), (22) can be expressed as [13] x\u2217EI = argmax x\u2208X \u221a", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "[16], who combine CMA-ES with a GP using the \u03b3-exponential function K, propose to employ in (15) the Mahalanobis distance of vectors x and x given by the covariance matrix obtained in the g-th generation of CMA-ES, (\u03c3)C, instead of their Euclidean distance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].", "startOffset": 47, "endOffset": 57}, {"referenceID": 2, "context": "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].", "startOffset": 47, "endOffset": 57}, {"referenceID": 14, "context": "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].", "startOffset": 47, "endOffset": 57}, {"referenceID": 1, "context": "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].", "startOffset": 174, "endOffset": 184}, {"referenceID": 2, "context": "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].", "startOffset": 174, "endOffset": 184}, {"referenceID": 14, "context": "In this context, it is worth recalling that in [2, 3, 15], Mahalanobis instead of Euclidean distance was used when combining CMA-ES with quadratic response surface models in [2, 3, 15].", "startOffset": 174, "endOffset": 184}, {"referenceID": 9, "context": "This can be done according to various strategies, we intend to use the one described in Algorithm 1, with which we have a good experience from using radial basis function networks for the evolution control in the evolutionary optimization of catalytic materials [10].", "startOffset": 262, "endOffset": 266}, {"referenceID": 1, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 104, "endOffset": 118}, {"referenceID": 2, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 104, "endOffset": 118}, {"referenceID": 14, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 104, "endOffset": 118}, {"referenceID": 22, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 104, "endOffset": 118}, {"referenceID": 16, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 186, "endOffset": 202}, {"referenceID": 17, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 186, "endOffset": 202}, {"referenceID": 18, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 186, "endOffset": 202}, {"referenceID": 21, "context": "The use of a linear ordering relates the proposed approach to ranking-based evolution control of CMA-ES [2, 3, 15, 23], as well as to surrogate modelling of CMA-ES by ordinal regression [17, 18, 19, 22].", "startOffset": 186, "endOffset": 202}, {"referenceID": 22, "context": "[23], the difference being that they don\u2019t use clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "(ii) GP on low-dimensional projections attempts to improve the basic approach in view of the experience reported in the literature [15] and obtained also in our earlier experiments [1] that GPs are actually advantageous only in low dimensional spaces.", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "(ii) GP on low-dimensional projections attempts to improve the basic approach in view of the experience reported in the literature [15] and obtained also in our earlier experiments [1] that GPs are actually advantageous only in low dimensional spaces.", "startOffset": 181, "endOffset": 184}, {"referenceID": 10, "context": "Whereas the previous four approaches represent, in terms of [11, 12], individual-based evolution control, we want to test also one approach that is generation-based, in the sense that the desired number \u03bb of points is evaluated by the original black-box fitness function only in selected generations.", "startOffset": 60, "endOffset": 68}, {"referenceID": 11, "context": "Whereas the previous four approaches represent, in terms of [11, 12], individual-based evolution control, we want to test also one approach that is generation-based, in the sense that the desired number \u03bb of points is evaluated by the original black-box fitness function only in selected generations.", "startOffset": 60, "endOffset": 68}, {"referenceID": 17, "context": "Similarly to the evolution strategy in [18, 19], our approach selects those generations adaptively, according to the agreement between the ranking of considered points by the surrogate model and by the black-box fitness.", "startOffset": 39, "endOffset": 47}, {"referenceID": 18, "context": "Similarly to the evolution strategy in [18, 19], our approach selects those generations adaptively, according to the agreement between the ranking of considered points by the surrogate model and by the black-box fitness.", "startOffset": 39, "endOffset": 47}, {"referenceID": 9, "context": "in the optimization of catalyst preformance described in [10], \u03bbhw is the number of channels in the chemical reactor in which the catalysts are tested).", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Needless to say, the smaller \u03bb, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].", "startOffset": 344, "endOffset": 360}, {"referenceID": 11, "context": "Needless to say, the smaller \u03bb, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].", "startOffset": 344, "endOffset": 360}, {"referenceID": 17, "context": "Needless to say, the smaller \u03bb, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].", "startOffset": 344, "endOffset": 360}, {"referenceID": 18, "context": "Needless to say, the smaller \u03bb, the larger will be the proportion of points from the generation in which the GP was trained in its training set, and the more similar the obtained GP will normally be to a GP trained only with data from that generation, which is the usual way of using surrogate models in traditional generation-based strategies [11, 12, 18, 19].", "startOffset": 344, "endOffset": 360}], "year": 2014, "abstractText": "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization. Basically, it consists in generating new search points by sampling from a multidimensional normal distribtion, the mean and variance of which are updated from generation to generation. In particular, the population x (g+1) 1 , . . . , x (g+1) \u03bb \u2208 R d of the g + 1-st generation, g \u2265 1, follows the normal distribution with mean m \u2208 R and variance (\u03c3)C \u2208 R resulting from the update in the g-th generation,", "creator": "LaTeX with hyperref package"}}}