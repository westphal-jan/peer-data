{"id": "1411.7942", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2014", "title": "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs", "abstract": "The functional approach to compositional distributional semantics considers transitive verbs to be linear maps that transform the distributional vectors representing nouns into a vector representing a sentence. We conduct an initial investigation that uses a matrix consisting of the parameters of a logistic regression classifier trained on a plausibility task as a transitive verb function. We compare our method to a commonly used corpus-based method for constructing a verb matrix.", "histories": [["v1", "Fri, 28 Nov 2014 16:57:34 GMT  (123kb)", "http://arxiv.org/abs/1411.7942v1", null], ["v2", "Fri, 12 Dec 2014 14:19:29 GMT  (143kb,D)", "http://arxiv.org/abs/1411.7942v2", "Full updated paper for NIPS learning semantics workshop, with some minor errata fixed"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tamara polajnar", "laura rimell", "stephen clark"], "accepted": false, "id": "1411.7942"}, "pdf": {"name": "1411.7942.pdf", "metadata": {"source": "CRF", "title": "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs", "authors": ["Tamara Polajnar", "Laura Rimell", "Stephen Clark"], "emails": ["tamara.polajnar@cl.cam.ac.uk", "laura.rimell@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 141 1,79 42v1 [cs.CL] 2 8N ov"}, {"heading": "1 Introduction", "text": "In principle, the field of compositional distribution semantics seeks ways to combine representations of distribution of words into larger units. Besides their theoretical interest, representations of complete sentences have the potential to be useful for tasks such as automatic summarization and recognition of textual connections. A number of recent studies have explored ways to combine representations of distribution of subjects, verbs, objects (SVO) to form transitive sentences, i.e. sentences based on a transitive verb [2-4, 6, 7, 12, 14]. Within the functional approach [1-4], arguing words such as verbs and adjectives are presented as tensors that take word vectors as arguments. A transitive verb can be considered a third-order tensor, with input dimensions for the subject and the object, as well as an output dimension for the meaning of the sentence as a whole."}, {"heading": "2 Methods", "text": "In the definition of the functional approach to compositional distributional semantics [1-4], a transitive verb is a map that takes noun vectors as arguments representing the subject and the object, and generates a vector in the sentence space. Typically, noun vectors for subject and object are located in a \"theme space\" where the dimensions coincide with co-event characteristics; we use a reduced space that deviates from the application of Singular Value Decomposition (SVD) to the raw co-event space. The correct sentence space is less obvious; earlier approaches have either assigned the sentence meaning to the same theme-based noun space [6, 7] or defined a new space for sentence meaning, in particular the plausibility space [11, 14]. If the verb function is a multilinear map, then the verb is naturally represented by a third-order tensor."}, {"heading": "2.1 Verbs", "text": "Regression (reg) Following [14], we formulate regression learning as a plausibility task in which class membership can be estimated with a single variable. To achieve a scalar output, we can learn the parameters for a single K \u00b7 K matrix (V) using standardized logistic regression with the mean square error cost function and the K-dimensional subject (ns) and object (no) noun vectors as input: O (V) = \u2212 1m [N] i log hV (n i s, n i o) + (1 \u2212 ti) log hV (n i s, n i o)], where ti are the true plausibility markers of the N training examples. The function hV (n i s, n i o) log hV (n i o) T (n i o) is a significant transformation of the scalar resulting from the inner product and the target is quadriplexed by the parameter."}, {"heading": "2.2 Training data", "text": "To generate training data, we find plausible SVO triples that occur in a Wikipedia dump from October 2013. To ensure quality, we select triples whose nouns occur at least 100 times. For some verbs, there are thousands of such triples, so we select the 300 most common triples for each verb. For each verb, we generate negative examples by replacing plausible subjects and objects with maximally dissimilar nouns. Specifically, we calculate for a given subject (or object) its average sum with the center of plausible subject (or object) vectors, and then select the frequently matching noun with the least cosmic similarity to that average. The noun vectors are generated from the Wikipedia corpus using the t-test weighting scheme and the normalization techniques described in [13]."}, {"heading": "2.3 Composition Methods", "text": "We examine the following methods for the composition of the verb matrix with the subject and object vectors to form a vector representation for a transitive proposition. Relative: ab [7, 10] the meaning of a transitive sentence is a matrix obtained by the following formula, where outer product is element-wise product and \u00b7 matrix multiplication: subj verb obj = (\u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 verb obj = (\u2212 subj \u00d7 \u2212 obj T) ij \u00b7 verbij (1) Copy subject: ab [9, 10] the meaning of a transitive sentence is a vector obtained by: \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 subj verb obj = \u2212 \u2212 subj (verb \u00d7 \u2212 obj) subj = obj, obj (verb \u00d7 \u2212 obj) = verb \u2212 obj = \u2212 obj."}, {"heading": "3 Tasks", "text": "We examine the performance of the regression learning method in two tasks: verb disambiguation and transitive sentence similarity. In any case, the system must compose SVO triples and compare the resulting semantic representations. For the verb disambiguation task, we use the GS2011 dataset [7]. This dataset consists of pairs of SVO triples, in which subject and object are kept constant, and the verb drawing is manipulated to emphasize various meanings. For example, verb drawing has senses that attract and represent. SVO triple report draws attention to great similarity, but little similarity, in order to display attention. Conversely, the child drawing image bears a great similarity to the child image, but little similarity to the child attraction image. The gold standard consists of human judgments on the similarity between pairs, and we measure the correlation of the system similarity to gold standard values."}, {"heading": "4 Results", "text": "Table 1 and Figure 1 summarize the results of our experiments. Overall, reg performs better on the verb disambiguation than dist, while dist performs better on the sentence similarity. We suspect that the difference lies in the nature of the two tasks. The verb disambiguation task is inherently plausible, since a member of the verb object pairs (with the non-relevant sense of the verb) is always implausible. For example, if the verb disambiguator problem is fulfilled, the triple system has a high plausibility, while the system visitor requirement has a low plausibility. In fact, the verb object (VO) composition method alone is sufficient to disambiguate these triples using rules. On the other hand, both triples in the sentence similarity task tend to be highly plausible even if their topic differs. As the verb object (VO) composition method uses a theme-based space, it can better grasp these distinctions."}, {"heading": "5 Conclusion", "text": "The difference in performance between the two methods highlights the need to find the right sentence spaces for specific tasks. This preliminary study suggests that plausibility training may be better suited for disambiguation. Further work will consist of an in-depth analysis and optimization of the training process as well as exploring ways to learn task-specific sentence spaces cost-effectively."}, {"heading": "Acknowledgements", "text": "Tamara Polajnar is supported by the ERC Starting Grant DisCoTex awarded to Stephen Clark, and Laura Rimell is supported by the EPSRC Grant EP / I037512 / 1: A Unified Model of Compositional and Distributional Semantics: Theory and Applications."}], "references": [{"title": "Frege in space: A program for compositional distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Type-driven syntax and semantics for composing meaning vectors", "author": ["Stephen Clark"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": "Linguistic Analysis (Lambek Festschrift),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["Edward Grefenstette", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni"], "venue": "Proceedings of the 10th International Conference on Computational Semantics (IWCS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Experimenting with transitive verbs in a discocat", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Languge,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Vector space semantic parsing: A framework for compositional vector space models", "author": ["Jayant Krishnamurthy", "Tom M Mitchell"], "venue": "In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL-08,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Improving distributional semantic vectors through context selection and normalisation", "author": ["Tamara Polajnar", "Stephen Clark"], "venue": "In 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Reducing dimensions of tensors in type-driven distributional semantics", "author": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "venue": "In Proceedings of EMNLP", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2\u20134, 6, 7, 12, 14].", "startOffset": 207, "endOffset": 226}, {"referenceID": 2, "context": "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2\u20134, 6, 7, 12, 14].", "startOffset": 207, "endOffset": 226}, {"referenceID": 3, "context": "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2\u20134, 6, 7, 12, 14].", "startOffset": 207, "endOffset": 226}, {"referenceID": 5, "context": "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2\u20134, 6, 7, 12, 14].", "startOffset": 207, "endOffset": 226}, {"referenceID": 6, "context": "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2\u20134, 6, 7, 12, 14].", "startOffset": 207, "endOffset": 226}, {"referenceID": 11, "context": "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2\u20134, 6, 7, 12, 14].", "startOffset": 207, "endOffset": 226}, {"referenceID": 13, "context": "A number of recent studies have investigated ways to combine distributional representations of Subject, Verb, Object (SVO) triples to form transitive sentences, that is, sentences based on a transitive verb [2\u20134, 6, 7, 12, 14].", "startOffset": 207, "endOffset": 226}, {"referenceID": 0, "context": "Under the functional approach [1\u20134], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.", "startOffset": 30, "endOffset": 35}, {"referenceID": 1, "context": "Under the functional approach [1\u20134], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.", "startOffset": 30, "endOffset": 35}, {"referenceID": 2, "context": "Under the functional approach [1\u20134], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.", "startOffset": 30, "endOffset": 35}, {"referenceID": 3, "context": "Under the functional approach [1\u20134], argument-taking words such as verbs and adjectives are represented as tensors, which take word vectors as arguments.", "startOffset": 30, "endOffset": 35}, {"referenceID": 5, "context": "This approach has achieved promising initial results [6\u20139, 14], but many questions remain.", "startOffset": 53, "endOffset": 62}, {"referenceID": 6, "context": "This approach has achieved promising initial results [6\u20139, 14], but many questions remain.", "startOffset": 53, "endOffset": 62}, {"referenceID": 7, "context": "This approach has achieved promising initial results [6\u20139, 14], but many questions remain.", "startOffset": 53, "endOffset": 62}, {"referenceID": 8, "context": "This approach has achieved promising initial results [6\u20139, 14], but many questions remain.", "startOffset": 53, "endOffset": 62}, {"referenceID": 13, "context": "This approach has achieved promising initial results [6\u20139, 14], but many questions remain.", "startOffset": 53, "endOffset": 62}, {"referenceID": 6, "context": "It compares two methods for learning verb representations, the distributional model of [7] in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space.", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "It compares two methods for learning verb representations, the distributional model of [7] in which positive examples of subject-object pairs for a given verb are structurally mixed, and the regression model of [14] in which positive and negative examples of subject-object pairs for a given verb are mapped into a plausibility space.", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "In the definition of the functional approach to compositional distributional semantics [1\u20134], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.", "startOffset": 87, "endOffset": 92}, {"referenceID": 1, "context": "In the definition of the functional approach to compositional distributional semantics [1\u20134], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.", "startOffset": 87, "endOffset": 92}, {"referenceID": 2, "context": "In the definition of the functional approach to compositional distributional semantics [1\u20134], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.", "startOffset": 87, "endOffset": 92}, {"referenceID": 3, "context": "In the definition of the functional approach to compositional distributional semantics [1\u20134], a transitive verb is a map that takes as arguments noun vectors representing the subject and object, and produces a vector in the sentence space.", "startOffset": 87, "endOffset": 92}, {"referenceID": 5, "context": "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].", "startOffset": 142, "endOffset": 148}, {"referenceID": 6, "context": "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].", "startOffset": 142, "endOffset": 148}, {"referenceID": 10, "context": "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].", "startOffset": 226, "endOffset": 234}, {"referenceID": 13, "context": "The correct sentence space to use is less obvious; previous approaches have either mapped sentence meaning to the same topic-based noun space [6, 7] or defined a new space for sentence meaning, particularly plausibility space [11, 14].", "startOffset": 226, "endOffset": 234}, {"referenceID": 6, "context": "However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix [7, 14].", "startOffset": 116, "endOffset": 123}, {"referenceID": 13, "context": "However, tensor training can be expensive and in practice, for some tasks, the verb can be approximated as a matrix [7, 14].", "startOffset": 116, "endOffset": 123}, {"referenceID": 13, "context": "Regression (reg) Following [14], we formulate regression learning as a plausibility task where the class membership can be estimated with a single variable.", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "The regression algorithm is trained through gradient descent with Adagrad [5] and 10% of the training triples are used as a validation set for early stopping.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Distributional (dist) Following [7], we generate a K \u00d7K matrix for each verb as the average of outer products of subject and verb vectors from the positively labelled subset of the training data:", "startOffset": 32, "endOffset": 35}, {"referenceID": 12, "context": "The noun vectors are generated from the Wikipedia corpus using the t-test weighting scheme and normalisation techniques described in [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Relational: from [7, 10], the meaning of a transitive sentence is a matrix, obtained by the following formula, where \u2297 is outer product, \u2299 is elementwise product, and \u00d7 is matrix multiplication:", "startOffset": 17, "endOffset": 24}, {"referenceID": 9, "context": "Relational: from [7, 10], the meaning of a transitive sentence is a matrix, obtained by the following formula, where \u2297 is outer product, \u2299 is elementwise product, and \u00d7 is matrix multiplication:", "startOffset": 17, "endOffset": 24}, {"referenceID": 8, "context": "Copy-subject: from [9, 10], the meaning of a transitive sentence is a vector, obtained by:", "startOffset": 19, "endOffset": 26}, {"referenceID": 9, "context": "Copy-subject: from [9, 10], the meaning of a transitive sentence is a vector, obtained by:", "startOffset": 19, "endOffset": 26}, {"referenceID": 6, "context": "For the verb disambiguation task we use the GS2011 dataset [7].", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "For the transitive sentence similarity task we use the KS2013 dataset [9].", "startOffset": 70, "endOffset": 73}], "year": 2017, "abstractText": "The functional approach to compositional distributional semantics considers transitive verbs to be linear maps that transform the distributional vectors representing nouns into a vector representing a sentence. We conduct an initial investigation that uses a matrix consisting of the parameters of a logistic regression classifier trained on a plausibility task as a transitive verb function. We compare our method to a commonly used corpus-based method for constructing a verb matrix and find that the plausibility training may be more effective for disambiguation tasks.", "creator": "LaTeX with hyperref package"}}}