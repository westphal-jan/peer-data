{"id": "1411.8003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2014", "title": "Guaranteed Matrix Completion via Non-convex Factorization", "abstract": "Matrix factorization is a popular approach for large-scale matrix completion and constitutes a basic component of many solutions for Netflix Prize competition. In this approach, the unknown low-rank matrix is expressed as the product of two much smaller matrices so that the low-rank property is automatically fulfilled. The resulting optimization problem, even with huge size, can be solved (to stationary points) very efficiently through standard optimization algorithms such as alternating minimization and stochastic gradient descent (SGD). However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of whether these algorithms will generate a good solution. In this paper, we establish a theoretical guarantee for the factorization based formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of the factorization based formulation, thus recovering the true low-rank matrix. To the best of our knowledge, our result is the first one that provides recovery guarantee for many standard algorithms such as gradient descent, SGD and block coordinate gradient descent. Our result also applies to alternating minimization, and a notable difference from previous studies on alternating minimization is that we do not need the resampling scheme (i.e. using independent samples in each iteration).", "histories": [["v1", "Fri, 28 Nov 2014 20:52:47 GMT  (353kb,D)", "https://arxiv.org/abs/1411.8003v1", "65 pages, 6 figures"], ["v2", "Fri, 29 May 2015 22:31:49 GMT  (289kb,D)", "http://arxiv.org/abs/1411.8003v2", "76 pages, 6 figures, a linear convergence result added, extended discussion on resampling"], ["v3", "Tue, 11 Oct 2016 17:35:12 GMT  (714kb,D)", "http://arxiv.org/abs/1411.8003v3", "77 pages. Accepted to IEEE Transaction on Information theory. A detailed description of the proof ideas is added, compared to version 2"]], "COMMENTS": "65 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ruoyu sun", "zhi-quan luo"], "accepted": false, "id": "1411.8003"}, "pdf": {"name": "1411.8003.pdf", "metadata": {"source": "CRF", "title": "Guaranteed Matrix Completion via Non-convex Factorization", "authors": ["Ruoyu Sun", "Zhi-Quan Luo"], "emails": ["sunxx394@umn.edu."], "sections": [{"heading": "1 Introduction", "text": "In the era of the big data-based approach, the whole matrix is a problem created by mobile devices, sensors, online merchants, social networks, etc. (1) A prototype example is the problem of low matrix completion, in which the goal is to achieve an unknown low matrix completion. (2) Matrix completion has found numerous applications in various fields such as receiver systems, computer vision, and system identification. (3) There are two popular approaches to imposing the low ranking structure: the nuclear norm-based approach and matrix factorization (MF)."}, {"heading": "1.1 Our contributions", "text": "In fact, most of them are able to determine for themselves what they want to do."}, {"heading": "1.2 Related works", "text": "In fact, most of them will be able to determine for themselves what they want and what they want."}, {"heading": "1.3 Proof Overview and Techniques", "text": "The first question is what kind of property can ensure global convergence. (i) We will examine the local geometry of a regulated object. (i) We will examine the local geometry of a regulated object. (ii) We will discuss the difficulties involved in each step and describe how we address these difficulties. (i) We will begin by considering a simple case that M \u2212 XYT \u2212 2F. The objective function is f (X, Y) = XYT."}, {"heading": "1.4 Other Remarks", "text": "This year it is more than ever before."}, {"heading": "1.5 Notations and organization", "text": "Remarks. < Remarks. Remarks. Remarks. Remarks. < Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks. Remarks..............................................................................................................................................................................................................."}, {"heading": "2 Problem Formulation and Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Assumptions", "text": "The incoherence condition for the Matrix completion problem is first introduced by Cande and Law in [4] and has become a standard assumption for low-level matrix utilization problems (with the exception of some recent works such as [47, 48]). We will define an incoherence condition for a m \u00b7 n Matrix M identical to that in [31]. Definition 2.1 We say that a matrix M = U refers to an incoherence condition for a matrix M which is identical to that in [31]. We say that a matrix M = U refers to an incoherence condition for a matrix M = U refers to an incoherence condition which is identical to that in [31]."}, {"heading": "2.2 Problem formulation", "text": "We consider a variant of (P0) with incoherence control regulation costs as the big solution we always use. In particular, we present two types of regulation terms in addition to the square loss function: the first type is designed to force the Xk, Yk and Yk iterators to be incoherent (i.e. with limited optimization problems), and the second type is designed to exceed the norm of Xk and Yk. Note: (P0) is associated with the Lagrange method, while our regulator is based on the streamlining method for limited optimization problems. We can also consider the regulation mechanisms as \"soft regulation mechanisms,\" which turn out to be \"soft regulators.\" The advantage of the hard regulation method is that it does not distort the optimal solution.Our regulators are smooth functions with simple gradients, so they have algorithms similar to our algorithms."}, {"heading": "2.3 Row-scaled Spectral Initialization", "text": "To be more precise, we would like the starting point to be in an incoherent neighborhood of the original matrix M. (this neighborhood will be specified later) A special initialization is also required in other work on non-convex formulations (see e.g. [31]). We will show that such a starting point can be found by a simple procedure. This procedure consists of two steps: First, using the spectral method (see e.g. [31]), we get M0 = X-0Y-T0, which is near M; second, we scale the rows of (X-0, Y-0) to make them incoherent (i.e. with a limited row standard). Name the best rank-r approximation of a matrix A as Pr (A)."}, {"heading": "2.4 Algorithms", "text": "Our results relate to many standard algorithms such as gradient pedigree, SGD, and block pedigree methods (including alternative minimization, blocked pedigree, blocked pedigree, etc.).We will describe several typical algorithms in this subrange, with G \"0\" (z) = \"1\" (z) = \"2\" (z) and the other lines, the zero.We will first present a gradient pedigree algorithm in Table 2. There are many options such as constant step size, exact line search, limited line search, decreasing step size, and Armijo rule [50]."}, {"heading": "3 Main Results", "text": "The main result of this work is that the algorithms 1-4 (standard optimization algorithms) converge exactly 1-4 (standard optimization algorithms) to the global optimum of the problem (P1) (P1), which is specified in (18) and can be reconstructed with high probability, provided that the number of detected entries is large enough. Similar to the results for the core standard minimization [4-7], the probability is taken with respect to the random choice of the method, and the result also applies to a uniform random model of the results. Theorem 3.1 (Exact Recovery) Assume a rank-r matrix M (Rm) n is in relation to the state number of M and the result also applies to a uniform random model. Then there is a numerical constant C0, so that: if we are uniformly generated, with random size."}, {"heading": "3.1 Proof of Theorem 3.1 and main lemmas", "text": "To prove this, we only need to prove two lemmas that describe the local geometry of the regularized object (P1) and the properties of the algorithms (P1). \u2212 k The first simulation shows that each stationary point of (P1) in a given region is globally optimal, and the second simulation shows that each of the algorithms converges 1-4 to stationary points in that region. \u2212 k The first simulation can be viewed as an \"incoherent neighborhood\" of M and formally defined as K1, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K1, K2, K2, K1, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, K2, 2, K2, K2, K2, K2, K2, K2, K2, 2, K2, K2, 2, K2, K2, 2, K2, 2, K2, K2, K2, K2, K2, K2, K2, K2, K2,"}, {"heading": "3.2 Proof of Theorem 3.2", "text": "The proof of theorem 3.2 can be considered standard for methods of the first order: the convergence rate (or iteration complexity) can be derived from the \"cost-to-go estimate\" and the \"sufficient descent\" condition. - Linear convergence f (xk) \u2212 f (1 \u2212 c1c2) k is a direct consequence of the cost-to-go estimate [1] f (xk) \u2212 f (xk) \u2212 f (xk) \u2212 f (xk + 1) k is a direct consequence of the cost-to-go estimate [2] f (xk). - The f \u00b2 condition is the minimum value of f, and c2 are certain constants. We point out that the use of other optimization frameworks can lead to increased time complexity; this is considered a future work.For our problem, a variant of Lemma 3.1 can be considered the cost-to-go estimate; see Lemma 3.3 below."}, {"heading": "4 Proof of Lemma 3.1", "text": "In Section 4.1, we will show that in order to prove Lemma 3.1, we only need to construct U, V to satisfy three inequalities that in Section 4.2 and subsequent sections exceed the boundaries of the following sections and < X, X, Y, Y, V, F and vice versa. In Section 4.2, we will describe two propositions that indicate the choice of U, V and then show that such U, V satisfies the three desired inequalities in Section 4.2 and subsequent sections."}, {"heading": "4.1 Preliminary analysis", "text": "Da (X, Y), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C, (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C), (C (C), (C), (C), (C (C), (C), (C (C), (C), (C (C), (C), (C (C), (C), (C (C), (C), (C (C), (C), (C (C), (C), (C (C), (C (C), (C), (C (C), (C (C), (C), (C (C), (C (C), (C (C), (C (C), (C (C), (C), (C (C), (C (C), (C (C), (C (C), (C), (C (C), (C (C), (C (C), (C), (C (C), C (C (C), (C (C), (C (C), C (C"}, {"heading": "4.4 Lower bound on \u03c6G", "text": "In this subsection, we will prove the following assertions. < p = < p = < p = p = p = p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p, p = p = p, p = p = p = p, p = p = p, p = p = p = p, p = p = p = p, p = p = p = p, p = p = p = p, p = p = p = p, p = p = p = p = p, p = p = p = p = p, p = p = p = p = p = p, p = p = p = p = p = p = p = p, p = p = p = p = p = p, p = p = p = p = p = p = p, p = p = p = p = p = p, p = p = p = p = p, p = p = p = p = p, p = p = p, p = p = p = p = p, p = p, p = p = p = p, p = p = p, p = p = p = p, p = p = p = p, p = p = p, p, p = p = p, p = p = p, p = p = p = p, p = p, p = p, p, p, p, p = p = p, p = p = p, p, p = p, p = p, p = p, p ="}, {"heading": "5 Proof of Lemma 3.2", "text": "Property (a) in lexicon 3.2 (convergence to stationary points) is a basic requirement for many reasonable algorithms and can be proved by classical results in optimization, so the difficulty lies mainly in how to prove properties (b). The following claim states that algorithms 1-4 fulfill detectable conditions (b) and then show that algorithms 1-4 fulfill these conditions. Proof for this assertion is provided in Appendix D.5.Claim 5.1 Suppose (29), then each boundary point of the sequence 1-4 generated by algorithms is a stationary point of the problem (P1). For property (b) we first show that the starting point (X0, Y0) is in an incoherent neighborhood (2 K1)."}, {"heading": "A Supplemental Material for Section 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Claim 2.1", "text": "This evidence is quite simple and we mainly use the triangular inequalities and the delimitation of the region under consideration (V, Y) (V, V) (V, V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V) (V (V) (V) (V) (V) (V) (V) (V) (V) (V (V) (V) (V) (V (V) (V) (V) (V) (V) (V) (V (V) (V) (V) (V (V) (V) (V) (V (V) (V) (V (V) (V) (V (V) (V (V) (V) (V) (V) (V (V (V) (V) (V) (V) (V (V) (V) (V) (V (V (V) (V), V (V (V (V), V (V (V) (V), V (V (V) (V) (V (V) (V), V (V (V) (V (V) (V) (V), V (V) (V (V (V), V (V), V (V) (V (V (V), V (V (V"}, {"heading": "A.2 Solving the Subproblem of Algorithm 3", "text": "The partial problem of algorithm 3 for the row vector X (i) ismin X (i) F (X (1) k,.., X (i \u2212 1) k, X (i + 1) k \u2212 1., X (m) k \u2212 1., X (m) k \u2212 1., X (j) k \u2212 1., X (j) k \u2212 1., 2.) k (i) k (i) k \u2212 1., 2.) k (j) k \u2212 1., 2.) k (j) k (j) k (j) k = 1., 3.) j (i) i \u2212 1, 4.) k (j) k \u2212 1., 4.) k \u2212 1., 4.) k (x., 5.), 5.), 6. (c), 6. (b), 6. (b), 7. (b), 7. (j)."}, {"heading": "B Proof of Proposition 4.1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Matrix norm inequalities", "text": "We first prove some fundamental inequalities in relation to the matrix norms. These simple results are used in the proof of Propositions 4.1 and 4.2.Proposition B.1 when A, B, Rn1 and 2, then A and B, then A and B, then A and B. (89) Proof: A and B. (A) and B. (B) Proofs: A and B. (B) and B. (B). (B) and B. (B). (B) Proofs: A and B. (B) and B. (B). (B)."}, {"heading": "B.2 Proof of Proposition 4.1", "text": "Let M, X, Y, Y, meet the condition (47). Let us X, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,"}, {"heading": "C Proof of Proposition 4.2", "text": "We first reduce sentence 4.2 to sentence C.1 for r \u00b7 r matrices in Section C.1. This reduction is rather trivial, and the biggest difficulty lies in sentence C.1. For the general r, the proof of sentence C.1 is more involved. We give an overview of the most important evidence ideas in Section C.2. Most readers can skip Section C.1."}, {"heading": "C.1 Transformation to a simpler problem", "text": "In particular, we will show that in order to prove Proposition 4.2, we need to apply only Proposition C.1.Similar to the proof of Proposition 4.1, the requirement Q1 \u00b7 m, Q2 \u00b7 Rn \u00b7 n to name the SVD factors of M (Q1 and Q2 are uniform matrices), and that we need to use X, Y \u2032 r \u2032 1 (X \u2032 1 X \u2032 2), Y \u00b2 (Y \u00b2 1 Y \u00b2 2).DefineU = Q1 (U \u00b2 1 0), V = Q2 (V \u00b2 1 (V \u00b2 1 0), (110), where U \u00b2 Rr \u00b2 r \u00b2 r and V \u00b2 r \u00b2 r are to be determined."}, {"heading": "C.2.1 Perturbation Analysis for Preconditioning.", "text": "We claim that Proposition C.1 is closely related to \"preconditioning,\" which refers to the reduction of the number of conditions (by preprocessing) in numerical linear algebraics.Proposition C.2 (informal) Suppose X-Rr is not singular and vice versa."}, {"heading": "C.2.2 Two Motivating Examples", "text": "We designate the i-th line of X, Y as xi, yi, respectively. In the first example (see Figure 3), we set r = 2, \u03a3 = I (which means that we \"min = 1), d = 1 / (Cdr) and X = Diag (x11, x22) = Diag C, 1 \u2212 d / \u221a 2C, Y = Diag (y11, y22) = Diag 1 \u2212 d / \u221a 2C, C, (116), where C > 1 must be determined, and Diag (w1, w2) = Diag C, 1 \u2212 d = diagonal matrix with diagonal entries w1, w2 \u2212 d \u00b2. In this setting \u03b2T = \u221a rCT max = \u221a 2CT is a big constant. Condition (112) is that we have since XYT \u2212 V = 1 \u2212 d (1 \u2212 d)."}, {"heading": "C.2.3 Proof Ideas of Proposition C.1", "text": "In the two examples above, we have used two different operations: one based on shrinkage / expansion, and the other based on rotation. As we have already mentioned, the first operation cannot handle the second example; also, it is obvious that the second operation cannot handle the first example (the angle between xi and yi is zero, so the rotation only reduces the inner product.) Therefore, both operations are necessary to perform both operations at the same time. Are these two operations sufficient? Fortunately, the answer is yes for the case where XYT is diagonal and < xi, yi > yi (we need additional efforts to reduce the general problem to this case). If all angles between xi and yi are smaller than a constant."}, {"heading": "C.3 Proof of Proposition C.1", "text": "As already mentioned, to simplify the notation, we use X, Y, V, d to replace the properties of X, Y, 1, U, 1, V, 1, d, in the proposition (C.1). In the course of the proof, we choose CT = 20, (120) and Cd = 108, which means that we meet a requirement in each step. (121) There are two \"hard\" requirements for U, V: (114a) and (114b). Our construction of U, V can be considered a two-step approach, where we meet a requirement in each step. In step 1, we construct F-Z-Z-Z, where D, XYT \u2212 Z-Z-Z, then XY-Z-Z (XYT) and Y-Z-Z-Z, i.e. the first requirement is met."}, {"heading": "C.3.1 Proof of Case 1", "text": "Without loss of generality, the presupposition (114a) results directly from the definition of U, V and the fact (114a)."}, {"heading": "C.3.2 Proof of Case 2a", "text": "Denote X0 = X, Y0 = Y, x0k = xk, y 0 k = y-k, \u03b1 0 k = \u03b1k, k = 1,.. \u2212 r Operation (144) We will define Xi \u2212 1, Y \u2212 1 to Xi, Y i so that Xi \u2212 1 to Xi \u2212 1, Y i so depends that Xi \u2212 1, Y i = (yi1,.., Y i i (.), i i i (.), i (.), i (.), i (.), i (.), i (.), i (.), i (.), i (.), i (.), i (.), i (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (. (.). (.). (. (.). (.). (. (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (.). (.).). (.). (.).). (.). (.).). (.). (.).). (.).). (.).).). (.). (. (.).). (.).). (.). (.).). (.). (.).).). (.).). (.).). (.).). (.).).)..). (. (.)..).)...).)...).....................)....................."}, {"heading": "C.3.3 Proof of Case 2b", "text": "Similar to case 2a, case 2a requires that for case 2a (the role of U, X, j = 0,..), (156), (157), (159), (161), (161), (F), (163), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F, (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F (F), (F), (F), (F), (F (F), (F), (F), (F), (F (F), (F), (F (F), (F), (F), (F (F), (F), (F (F (), F (F), F (), (F (F), F (), (F (F), F (, F (), F (, F, F (), F (, F, F, F (), F (, F (), F (, F (, F), F (, F, F (, F), (F (, F), (, F (, F), (), (F, (F (, F, F (, F), (, F (,), (, F (,), (), (F (, (, F (,), (F, (,), (, (,), (,), (, (,), (, (F (,"}, {"heading": "C.4 Proof of Claim C.2", "text": "Suppose claim C.2 applies to 1, 2,., i \u2212 1, we prove claim (C.2) to i. By the property (147a) and (147d) of claim C.2 for i \u2212 1, we implicitly have Xi \u2212 1 (Y i \u2212 1) T = \u03a3. (167b) To simplify the notations, the entire proof of claim C.2, we denote Xi \u2212 1, Y \u2212 1 as X, Y \u2212 i as X. \"The notations are modified in such a way that we denote the notation in the whole series of claims C.2, we denote Xi \u2212 1, Y \u2212 i as X,\" the notations are in the series i \u2212 k. \"(167a), we are modified accordingly in the series. (167a) and (167b) becomeXYT = (168a)."}, {"heading": "D Proofs of the results in Section 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Proof of Claim 5.2", "text": "The proof for this assertion consists of two parts: firstly, by a classic result we have: firstly, that M0, the best rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-Rankings-R"}, {"heading": "D.2 Proof of Claim 3.1", "text": "As mentioned in Section 2.1, we need only consider in this proof the Bernolli model, which fulfills each input of M with the probability p and the expected quantity S. Denotet d, VP-XYT-F. Let us a = U (V-Y) T + (U-X) VT, b = (U-X) (V-Y), where U, V are defined with the properties in episode 4.1. According to (46) we have the properties in episode 4.1. Therefore, we have the properties in episode 4.1."}, {"heading": "D.3 Proof of Proposition 5.1", "text": "We first set a general condition for (X, Y), (K1), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (2), (2), (2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2), (K2)."}, {"heading": "D.4 Proof of Claim 5.3", "text": "The sequence {xt} generated by algorithm 1 with a limited Armijo rule or a restricted line search is fulfilled (67c), because the sequence F (xt) decreases and the requirement d (xt, x0) \u2264 5\u03b4 / 6 is enforced throughout the calculation. Algorithm 2 and algorithm 3 satisfy (67b), as all of them perform an exact minimization of a convex upper limit of the objective function along some directions. Note that xt as the produced solution after t \"iterations\" (a block of variables is updated in an \"iteration,\" in contrast (Xk, Yk) the solution defined in these algorithms is after k \"loops\" (all variables are updated once in a \"loop\"). For (Xk, Yk) generated by algorithm 2, we define x2k, \"x2k,\" x2k. \""}, {"heading": "D.5 Proof of Claim 5.1", "text": "For algorithms 1 with the constant step quantity 1 < p = 1 (defined in (238)), since the objective value F (xt) decreases, we must apply the definitions of K2 in (30) and the definition of K2 in (20), which means that the algorithm generates a sequence in K1 + K2, the constant L (\u03b2T) being constant beyond the specified K2. [50, Proposition 1.2.3], each boundary point of the sequence generated by algorithm 1 with constant step quantity < p = 1 (238) \u2264 2 / L (\u03b2T) is a stationary point of the problem (P1).We then consider algorithms 1 with the restricted Armijo rule."}, {"heading": "E Proof of Lemma 3.3", "text": "We will prove a statement that is stronger than Lemma 3.1: with probability at least 1 - > 1 - > 2 - 3 - 4, for each (X, Y) - K1 - K2 - K (3) and U, V - as defined in Table 7, we have < X - F (X, Y), X - U > + < Y - F (X, Y) (37a), i.e. with probability at least 1 - 1 - 1 / n4 - F = < X - Y), (251), where d = M \u2212 XYT - F. We have already proved (37a), i.e. with probability at least 1 - 1 - 1 - 1 / n4, - F = < XF, X \u2212 U > + < G F, Y \u2212 V > p > p > 4 d2.We will continue to prove a bond to G that is stronger than the bound GU, 1 - 1 - 1l."}], "references": [{"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": "Computer, vol. 42, no. 8, pp. 30\u201337, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Recovering the missing components in a large noisy low-rank matrix: Application to SFM", "author": ["Pei Chen", "David Suter"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 8, pp. 1051\u20131063, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Interior-point method for nuclear norm approximation with application to system identification", "author": ["Zhang Liu", "Lieven Vandenberghe"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 31, no. 3, pp. 1235\u20131256, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013772, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "IEEE Transactions on Information Theory, vol. 56, no. 5, pp. 2053\u20132080, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["David Gross"], "venue": "IEEE Transactions on Information Theory, vol. 57, no. 3, pp. 1548\u20131566, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 3413\u20133430, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 925\u2013936, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 1665\u20131697, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization, vol. 20, no. 4, pp. 1956\u20131982, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1956}, {"title": "Fixed point and bregman iterative methods for matrix rank minimization", "author": ["Shiqian Ma", "Donald Goldfarb", "Lifeng Chen"], "venue": "Mathematical Programming, vol. 128, no. 1-2, pp. 321\u2013353, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Kim-Chuan Toh", "Sangwoon Yun"], "venue": "Pacific Journal of Optimization, vol. 6, no. 615-640, pp. 15, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast global convergence of gradient methods for high-dimensional statistical recovery", "author": ["Alekh Agarwal", "Sahand Negahban", "Martin Jordan Wainwright"], "venue": "The Annals of Statistics, vol. 40, no. 5, pp. 2452\u20132482, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "On the linear convergence of the proximal gradient method for trace norm regularization", "author": ["Ke Hou", "Zirui Zhou", "Anthony Man-Cho So", "Zhi-Quan Luo"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 710\u2013718.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A unified view of matrix factorization models", "author": ["Ajit P Singh", "Geoffrey J Gordon"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 358\u2013373. Springer, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Major components of the gravity recommendation system", "author": ["G\u00e1bor Tak\u00e1cs", "Istv\u00e1n Pil\u00e1szy", "Botty\u00e1n N\u00e9meth", "Domonkos Tikk"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 9, no. 2, pp. 80\u201383, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient algorithms for collaborative filtering", "author": ["Hulikal Keshavan"], "venue": "Ph.D. thesis, Stanford University,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "Proceedings of the forty-fifth annual ACM symposium on Theory of computing (STOC). ACM, 2013, pp. 665\u2013674.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "2014 IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 2014, pp. 651\u2013660.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast matrix completion without the condition number", "author": ["Moritz Hardt", "Mary Wootters"], "venue": "Proceedings of The 27th Conference on Learning Theory (COLT), 2014, pp. 638\u2013678.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale parallel collaborative filtering for the netflix prize", "author": ["Yunhong Zhou", "Dennis Wilkinson", "Robert Schreiber", "Rong Pan"], "venue": "Algorithmic Aspects in Information and Management, pp. 337\u2013348. Springer, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive overrelaxation algorithm", "author": ["Zaiwen Wen", "Wotao Yin", "Yin Zhang"], "venue": "Mathematical Programming Computation, vol. 4, no. 4, pp. 333\u2013361, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Netflix update: Try this at home", "author": ["Simon Funk"], "venue": "http://sifter.org/ simon/journal/20061211.html.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving regularized singular value decomposition for collaborative filtering", "author": ["Arkadiusz Paterek"], "venue": "Proceedings of KDD cup and workshop, 2007, vol. 2007, pp. 5\u20138.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale matrix factorization with distributed stochastic gradient descent", "author": ["Rainer Gemulla", "Erik Nijkamp", "Peter J Haas", "Yannis Sismanis"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2011, pp. 69\u201377.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "author": ["Benjamin Recht", "Christopher R\u00e9"], "venue": "Mathematical Programming Computation, vol. 5, no. 2, pp. 201\u2013226, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast parallel sgd for matrix factorization in shared memory systems", "author": ["Yong Zhuang", "Wei-Sheng Chin", "Yu-Chin Juan", "Chih-Jen Lin"], "venue": "Proceedings of the 7th ACM Conference on Recommender Systems. ACM, 2013, pp. 249\u2013256.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast als-based matrix factorization for explicit and implicit feedback datasets", "author": ["Istv\u00e1n Pil\u00e1szy", "D\u00e1vid Zibriczky", "Domonkos Tikk"], "venue": "Proceedings of the fourth ACM conference on Recommender systems. ACM, 2010, pp. 71\u201378.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit S Dhillon"], "venue": "ICDM, 2012, pp. 765\u2013774.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Matrix Completion via Nonconvex Factorization: Algorithms and Theory, Ph.D", "author": ["Ruoyu Sun"], "venue": "thesis, University of Minnesota,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "IEEE Transactions on Information Theory, vol. 56, no. 6, pp. 2980\u20132998, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1407.1065, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantum state tomography via compressed sensing", "author": ["David Gross", "Yi-Kai Liu", "Steven T Flammia", "Stephen Becker", "Jens Eisert"], "venue": "arXiv preprint, http://arxiv.org/abs/0909.3304v1, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast exact matrix completion with finite samples", "author": ["Prateek Jain", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1411.1087, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Global convergence of stochastic gradient descent for some nonconvex matrix problems", "author": ["Christopher De Sa", "Kunle Olukotun", "Christopher R\u00e9"], "venue": "arXiv preprint arXiv:1411.1134, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2796\u20132804.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "A general theory of concave regularization for high-dimensional sparse estimation problems", "author": ["Cun-Hui Zhang", "Tong Zhang"], "venue": "Statistical Science, vol. 27, no. 4, pp. 576\u2013593, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima", "author": ["Po-Ling Loh", "Martin Wainwright"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 476\u2013484.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Strong oracle optimality of folded concave penalized estimation", "author": ["Jianqing Fan", "Lingzhou Xue", "Hui Zou"], "venue": "The Annals of Statistics, vol. 42, no. 3, pp. 819\u2013849, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Xiao-Tong Yuan", "Tong Zhang"], "venue": "The Journal of Machine Learning Research, vol. 14, no. 1, pp. 899\u2013925, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial time", "author": ["Zhaoran Wang", "Huanran Lu", "Han Liu"], "venue": "arXiv preprint arXiv:1408.5352, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-convex robust pca", "author": ["Praneeth Netrapalli", "UN Niranjan", "Sujay Sanghavi", "Animashree Anandkumar", "Prateek Jain"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1107\u20131115.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["Sivaraman Balakrishnan", "Martin Wainwright", "Bin Yu"], "venue": "arXiv preprint arXiv:1408.2156, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "High dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality", "author": ["Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu"], "venue": "arXiv preprint arXiv:1412.8729, 2014. 76", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["Per-\u00c5ke Wedin"], "venue": "BIT Numerical Mathematics, vol. 12, no. 1, pp. 99\u2013111, 1972.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1972}, {"title": "Spectral techniques applied to sparse random graphs", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Random Structures & Algorithms, vol. 27, no. 2, pp. 251\u2013275, 2005.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Coherent matrix completion", "author": ["Yudong Chen", "Srinadh Bhojanapalli", "Sujay Sanghavi", "Rachel Ward"], "venue": "Proceedings of The 31st International Conference on Machine Learning (ICML), 2014, pp. 674\u2013682.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Universal matrix completion", "author": ["Srinadh Bhojanapalli", "Prateek Jain"], "venue": "arXiv preprint arXiv:1402.2324, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-linear programming via penalty functions", "author": ["Willard I Zangwill"], "venue": "Management science, vol. 13, no. 5, pp. 344\u2013358, 1967.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1967}, {"title": "Nonlinear programming", "author": ["Dimitri P Bertsekas"], "venue": "1999.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1999}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["Paul Tseng"], "venue": "Journal of optimization theory and applications, vol. 109, no. 3, pp. 475\u2013494, 2001.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2001}, {"title": "Improved iteration complexity bounds of cyclic block coordinate descent for convex problems", "author": ["Ruoyu Sun", "Mingyi Hong"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1306\u20131314.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Worst-case complexity of cyclic coordinate descent: O(n2) gap with randomized version", "author": ["Ruoyu Sun", "Yinyu Ye"], "venue": "arXiv preprint arXiv:1604.07130, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu. Nesterov"], "venue": "SIAM Journal on Optimization, vol. 22, no. 2, pp. 341\u2013362, 2012.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["Meisam Razaviyayn", "Mingyi Hong", "Zhi-Quan Luo"], "venue": "SIAM Journal on Optimization, vol. 23, no. 2, pp. 1126\u20131153, 2013.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-layer provision of future cellular networks: A WMMSE-based approach", "author": ["Hadi Baligh", "Mingyi Hong", "Wei-Cheng Liao", "Zhi-Quan Luo", "Meisam Razaviyayn", "Maziar Sanjabi", "Ruoyu Sun"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 6, pp. 56\u201368, 2014.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint base station clustering and beamformer design for partial coordinated transmission in heterogeneous networks", "author": ["Mingyi Hong", "Ruoyu Sun", "H. Baligh", "Zhi-Quan Luo"], "venue": "IEEE Journal on Selected Areas in Communications (JSAC), vol. 31, no. 2, pp. 226\u2013240, February 2013.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion and low-rank svd via fast alternating least squares", "author": ["Trevor Hastie", "Rahul Mazumder", "Jason Lee", "Reza Zadeh"], "venue": "arXiv preprint arXiv:1410.2596, 2014.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "On the convergence of the block nonlinear gauss\u2013seidel method under convex constraints", "author": ["Luigi Grippo", "Marco Sciandrone"], "venue": "Operations Research Letters, vol. 26, no. 3, pp. 127\u2013136, 2000.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2000}, {"title": "On the expected convergence of randomly permuted ADMM", "author": ["Ruoyu Sun", "Zhi-Quan Luo", "Yinyu Ye"], "venue": "arXiv preprint arXiv:1503.06387, 2015.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of an approximate gradient projection method with applications to the backpropagation algorithm", "author": ["Zhi-Quan Luo", "Paul Tseng"], "venue": "Optimization Methods and Software, vol. 4, no. 2, pp. 85\u2013101, 1994.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient convergence in gradient methods with errors", "author": ["Dimitri P Bertsekas", "John N Tsitsiklis"], "venue": "SIAM Journal on Optimization, vol. 10, no. 3, pp. 627\u2013642, 2000.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2000}, {"title": "Perturbation theory for the singular value decomposition", "author": ["Gilbert W Stewart"], "venue": "1998. 77", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "For the matrix completion problem, the nuclear norm based formulation becomes either a linearly constrained minimization problem [4] min Z\u2208Rm\u00d7n \u2016Z\u2016\u2217, s.", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 4, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 5, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 6, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 7, "context": "This result was later generalized to noisy matrix completion, whereby the optimization formulation (2) is adopted [8].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "Using a different proof framework, reference [9] provided theoretical guarantee for a variant of the formulation (3).", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .", "startOffset": 252, "endOffset": 260}, {"referenceID": 11, "context": "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .", "startOffset": 252, "endOffset": 260}, {"referenceID": 12, "context": "Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size.", "startOffset": 130, "endOffset": 138}, {"referenceID": 13, "context": "Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size.", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "Such a matrix factorization model has long been used in PCA (principle component analysis) and many other applications [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons.", "startOffset": 154, "endOffset": 161}, {"referenceID": 15, "context": "It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons.", "startOffset": 154, "endOffset": 161}, {"referenceID": 0, "context": "Third, as elaborated in [1], the factorization model can be easily modified to incorporate additional application-specific requirements.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "A popular factorization based formulation for matrix completion takes the form of an unconstrained regularized square-loss minimization problem [1]:", "startOffset": 144, "endOffset": 147}, {"referenceID": 16, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 17, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 18, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 19, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 20, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 188, "endOffset": 191}, {"referenceID": 21, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 250, "endOffset": 254}, {"referenceID": 0, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 17, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 18, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 20, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 0, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 15, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 22, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 23, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 24, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 52, "endOffset": 59}, {"referenceID": 25, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 52, "endOffset": 59}, {"referenceID": 26, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 52, "endOffset": 59}, {"referenceID": 27, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 145, "endOffset": 152}, {"referenceID": 28, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 145, "endOffset": 152}, {"referenceID": 16, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 17, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 18, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 19, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 30, "context": "The first recovery guarantee for the factorization based matrix completion is provided in [31], where Keshavan, Montanari and Oh considered a factorization model in Grassmannian manifold and showed that the matrix can be recovered by a proper initialization and a gradient descent method on Grassmannian manifold.", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "The factorization model in Euclidean space was first analyzed in an unpublished work [17] of Keshavan 1, as well as a later work of Jain et al.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number.", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number.", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "We will point out a subtle theoretical issue not mentioned in [32], as well as some other practical issues.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization.", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "In this paper when we refer to [17], we are only referring to [17, Ch.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 80, "endOffset": 90}, {"referenceID": 6, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 80, "endOffset": 90}, {"referenceID": 32, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 80, "endOffset": 90}, {"referenceID": 16, "context": ", L, as proposed in [17,18] 2.", "startOffset": 20, "endOffset": 27}, {"referenceID": 17, "context": ", L, as proposed in [17,18] 2.", "startOffset": 20, "endOffset": 27}, {"referenceID": 16, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 17, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 18, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 19, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 16, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 17, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 18, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 19, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 16, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 17, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 18, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 19, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 3, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 4, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 5, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 6, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 33, "context": "In a recent work [34] the authors have managed to remove the dependency of the required sample size on by using a singular value projection algorithm.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "However, [34] considers a matrix variable of the same size as the original matrix, which requires significantly more memory than the matrix factorization approach considered in this paper.", "startOffset": 9, "endOffset": 13}, {"referenceID": 34, "context": "The resampling is also required in the recent work of [35]; see [30, Sec.", "startOffset": 54, "endOffset": 58}, {"referenceID": 31, "context": "Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36].", "startOffset": 98, "endOffset": 106}, {"referenceID": 35, "context": "Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36].", "startOffset": 98, "endOffset": 106}, {"referenceID": 35, "context": "The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm.", "startOffset": 29, "endOffset": 33}, {"referenceID": 31, "context": "The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 17, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 18, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 19, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 36, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 0, "endOffset": 7}, {"referenceID": 37, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 0, "endOffset": 7}, {"referenceID": 38, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 0, "endOffset": 7}, {"referenceID": 39, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 21, "endOffset": 28}, {"referenceID": 40, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 21, "endOffset": 28}, {"referenceID": 41, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 82, "endOffset": 86}, {"referenceID": 42, "context": "Maximization) algorithm [43, 44].", "startOffset": 24, "endOffset": 32}, {"referenceID": 43, "context": "Maximization) algorithm [43, 44].", "startOffset": 24, "endOffset": 32}, {"referenceID": 44, "context": "The difference from traditional perturbation analysis of Wedin [45] (i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 44, "context": "if two matrices are close then their row/column spaces are close) is that in [45] the row/column spaces are fixed while in our problem U,V are up to our choice.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "This inequality is closely related to matrix RIP (restricted isometry property) in [8] (see equation (III.", "startOffset": 83, "endOffset": 86}, {"referenceID": 30, "context": "A solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on \u2016P\u03a9(A)\u2016F for any rank-1 matrix A (possibly dependent on \u03a9).", "startOffset": 27, "endOffset": 31}, {"referenceID": 45, "context": "A solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on \u2016P\u03a9(A)\u2016F for any rank-1 matrix A (possibly dependent on \u03a9).", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "This lemma, combined with another probability result in [4], implies a bound on \u2016P\u03a9(M \u2212 XYT )\u2016F .", "startOffset": 56, "endOffset": 59}, {"referenceID": 30, "context": "A special case of the third condition has been used in [31] for Grassmann manifold optimization.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 17, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 18, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 19, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 30, "context": "Another strategy, as employed in [31], is to use a random graph lemma in [46].", "startOffset": 33, "endOffset": 37}, {"referenceID": 45, "context": "Another strategy, as employed in [31], is to use a random graph lemma in [46].", "startOffset": 73, "endOffset": 77}, {"referenceID": 45, "context": "We apply the random graph lemma of [46] when extending the local geometry of \u2016M\u2212XYT \u2016F to \u2016P\u03a9(M\u2212XY )\u2016F .", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "The difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31].", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "The difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31].", "startOffset": 232, "endOffset": 236}, {"referenceID": 30, "context": "Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ).", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ).", "startOffset": 263, "endOffset": 267}, {"referenceID": 29, "context": "10 of [30] shows that when |\u03a9| is small, in all successful instances the iterates are balanced, while in all failed instances the iterates are unbalanced.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "The incoherence condition for the matrix completion problem is first introduced by Cand\u00e8s and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).", "startOffset": 103, "endOffset": 106}, {"referenceID": 46, "context": "The incoherence condition for the matrix completion problem is first introduced by Cand\u00e8s and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).", "startOffset": 217, "endOffset": 225}, {"referenceID": 47, "context": "The incoherence condition for the matrix completion problem is first introduced by Cand\u00e8s and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).", "startOffset": 217, "endOffset": 225}, {"referenceID": 30, "context": "We will define an incoherence condition for an m \u00d7 n matrix M which is the same as that in [31].", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "For some popular random models for generating M, the incoherence condition holds with a parameter scaling as \u221a r log n (see [31]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "We remark that this model is \u201cequivalent to\u201d a Bernolli model that each entry of M is included into \u03a9 independently with probability p = S mn in the sense that if the success of an algorithm holds for the Bernolli model with a certain p with high probability, then the success also holds for the uniform random model with |\u03a9| = pmn with high probability (see [4] or [31, Sec.", "startOffset": 359, "endOffset": 362}, {"referenceID": 0, "context": "The choice of function G0 is not unique; in fact, we can choose any G0 that satisfies the following requirements: a) G0 is convex and continuously differentiable; b) G0(z) = 0, z \u2208 [0, 1].", "startOffset": 181, "endOffset": 187}, {"referenceID": 30, "context": "In [31], G0 is chosen as G0(z) = I[1,\u221e](z)(e 2 \u2212 1), which also satisfies these two requirements.", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "[49])", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[49]), which motivates our choice of G0 in (14).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 17, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 18, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 19, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 30, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 31, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 35, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 30, "context": "[31]), we obtain M0 = X\u03020\u0176 0 which is close to M; second, we scale the rows of (X\u03020, \u01760) to make it incoherent (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "There are many choices of stepsizes such as constant stepsize, exact line search, limited line search, diminishing stepsize and Armijo rule [50].", "startOffset": 140, "endOffset": 144}, {"referenceID": 30, "context": "Note that the restricted line search rule is similar to that used in [31] for the gradient descent method over Grassmannian manifolds.", "startOffset": 69, "endOffset": 73}, {"referenceID": 50, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 7, "endOffset": 14}, {"referenceID": 51, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 7, "endOffset": 14}, {"referenceID": 52, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 7, "endOffset": 14}, {"referenceID": 53, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 27, "endOffset": 31}, {"referenceID": 53, "context": "Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 54, "context": "Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]).", "startOffset": 287, "endOffset": 291}, {"referenceID": 55, "context": "[56, 57]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 56, "context": "[56, 57]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 57, "context": "[58] proposed an algorithm that could be viewed as a BSUM algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Just considering different choices of the blocks will lead to different algorithms for the matrix completion problem [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 53, "context": "In the regimes of |\u03a9| that the vanilla AltMin fails, G is active and the gradient updates do happen; however, instead of solving the subproblem exactly, one could perform one gradient step and the algorithm becomes the popular variant BCGD [54].", "startOffset": 240, "endOffset": 244}, {"referenceID": 54, "context": "Such a technique has also been used in the alternating least square algorithm for tensor decomposition [55].", "startOffset": 103, "endOffset": 107}, {"referenceID": 58, "context": "Note that for the two-block BCD algorithm, convergence to stationary points can be guaranteed even when the subproblems are not strongly convex [59], thus in Algorithm 2 we do not add the extra terms.", "startOffset": 144, "endOffset": 148}, {"referenceID": 0, "context": "The fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1).", "startOffset": 69, "endOffset": 76}, {"referenceID": 22, "context": "The fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1).", "startOffset": 69, "endOffset": 76}, {"referenceID": 59, "context": ", [60] for one example of such analysis).", "startOffset": 2, "endOffset": 6}, {"referenceID": 60, "context": "In this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {\u03b7k} to go to zero as k \u2192 \u221e, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems).", "startOffset": 90, "endOffset": 98}, {"referenceID": 61, "context": "In this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {\u03b7k} to go to zero as k \u2192 \u221e, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems).", "startOffset": 90, "endOffset": 98}, {"referenceID": 3, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 4, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 5, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 6, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 3, "context": "As demonstrated in [4] (and proved in [5, Theorem 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "log n factor is due to the coupon collector effect [4].", "startOffset": 51, "endOffset": 54}, {"referenceID": 18, "context": "the one proposed in [19]) can reduce the exponents of r and \u03ba.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "1 in [4]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 30, "context": "\u2016P\u03a9(b)\u2016F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.", "startOffset": 61, "endOffset": 69}, {"referenceID": 45, "context": "\u2016P\u03a9(b)\u2016F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.", "startOffset": 61, "endOffset": 69}, {"referenceID": 3, "context": "1 in [4], for |\u03a9| satisfying (27) with large enough C0, we have that with probability at least 1\u22121/(2n4), \u2016PTP\u03a9PT (a) \u2212 pPT (a)\u2016F \u2264 6 p\u2016a\u2016F (note that this bound holds uniformly for all a \u2208 T , thus also holds when a is dependent on \u03a9).", "startOffset": 5, "endOffset": 8}, {"referenceID": 44, "context": "Such a result bears some similarity with the classical perturbation theory for singular value decomposition [45].", "startOffset": 108, "endOffset": 112}, {"referenceID": 44, "context": "In particular, [45] proved that for two low-rank matrices4 that are close, the spaces spanned by the left (resp.", "startOffset": 15, "endOffset": 19}, {"referenceID": 44, "context": "To resolve this issue, we need to prove the second proposition in which there is an additional 4The result in [45] also covered the case of two approximately low-rank matrices, but we only consider the case of exact low-rank matrices here.", "startOffset": 110, "endOffset": 114}, {"referenceID": 54, "context": "This definition is motivated by the block successive upper bound minimization method [55].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "and {xt} satisfies either of the following three conditions: 1) F\u0303(xt + \u03bb\u2206t) \u2264 2F\u0303(x0),\u2200 \u03bb \u2208 [0, 1], where \u2206t = xt+1 \u2212 xt, \u2200 t; (67a) 2) 1 = arg min \u03bb\u2208R \u03c8(xt,\u2206t; \u03bb), where \u03c8 satisfies (65),\u2206t = xt+1 \u2212 xt, \u2200 t; (67b) 3) F\u0303(xt) \u2264 2F\u0303(x0), d(xt,x0) \u2264 5 6 \u03b4, \u2200 t.", "startOffset": 93, "endOffset": 99}, {"referenceID": 30, "context": "1 can be found in [31].", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Since d(x,u\u2217) is a continuous function over x, the relation d(xt,u) \u2264 3\u03b4 and (230) imply that there must exist some x\u2032 = (1\u2212 \u03bb)xt+1 + \u03bbxt, \u03bb \u2208 [0, 1] such that d(x\u2032,u\u2217) = \u03b4.", "startOffset": 143, "endOffset": 149}, {"referenceID": 0, "context": "Since d(x,u\u2217) is a continuous function over x and d(x\u2032,u\u2217) \u2264 \u03b4 (230) < d(xt+1,u), there must exist some x\u2032\u2032 = (1 \u2212 )xt+1 + x\u2032= xt + (1 \u2212 + \u03bb)\u2206t, \u2208 [0, 1] such that d(x\u2032\u2032,u\u2217) = \u03b4.", "startOffset": 147, "endOffset": 153}, {"referenceID": 0, "context": ", F\u0303(xt\u22121 + \u03bb\u2206t\u22121) \u2264 2F\u0303(x0),\u2200\u03bb \u2208 [0, 1], where \u2206t = xt \u2212 xt\u22121.", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "= F\u0303(xt) + \u2016\u2207F\u0303(xt)\u2016( L1 2 \u03bb2\u03b72 \u2212 \u03bb\u03b7) \u2264 F\u0303(xt) \u2212 \u03bb\u03b7 2 \u2016\u2207F\u0303(xt)\u2016 \u2264 F\u0303(xt) \u2264 2F\u0303(x0), \u2200 \u03bb \u2208 [0, 1], (239)", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": ", F\u0303(xk + \u03bb\u2206k) \u2264 2F\u0303(x0),\u2200\u03bb \u2208 [0, 1], where \u2206k = xk+1 \u2212 xk, 0 \u2264 k \u2264 t \u2212 1.", "startOffset": 30, "endOffset": 36}, {"referenceID": 0, "context": "F\u0303(xt + \u03bb(xt+1 \u2212 xt)) \u2264 2F\u0303(x0), \u2200 \u03bb \u2208 [0, 1],", "startOffset": 39, "endOffset": 45}, {"referenceID": 49, "context": "11) of [50]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50] (except that (1.", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.", "startOffset": 103, "endOffset": 107}, {"referenceID": 54, "context": "Algorithm 3 belongs to the class of BSUM methods [55].", "startOffset": 49, "endOffset": 53}, {"referenceID": 54, "context": "It is easy to verify that the objective function of each subproblem in Algorithm 3 is a convex tight upper bound of F\u0303(x) (more precisely, satisfies Assumption 2 in [55]).", "startOffset": 165, "endOffset": 169}], "year": 2016, "abstractText": "Matrix factorization is a popular approach for large-scale matrix completion. The optimization formulation based on matrix factorization can be solved very efficiently by standard algorithms in practice. However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of this formulation. In this paper, we establish a theoretical guarantee for the factorization formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of a factorization formulation, and recover the true lowrank matrix. We study the local geometry of a properly regularized factorization formulation and prove that any stationary point in a certain local region is globally optimal. A major difference of our work from the existing results is that we do not need resampling in either the algorithm or its analysis. Compared to other works on nonconvex optimization, one extra difficulty lies in analyzing nonconvex constrained optimization when the constraint (or the corresponding regularizer) is not \u201cconsistent\u201d with the gradient direction. One technical contribution is the perturbation analysis for non-symmetric matrix factorization.", "creator": "LaTeX with hyperref package"}}}