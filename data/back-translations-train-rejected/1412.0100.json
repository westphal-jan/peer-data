{"id": "1412.0100", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2014", "title": "Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images", "abstract": "State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost.", "histories": [["v1", "Sat, 29 Nov 2014 12:18:14 GMT  (3284kb,D)", "http://arxiv.org/abs/1412.0100v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["stefan mathe", "cristian sminchisescu"], "accepted": false, "id": "1412.0100"}, "pdf": {"name": "1412.0100.pdf", "metadata": {"source": "CRF", "title": "Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images", "authors": ["Stefan Mathe", "Cristian Sminchisescu"], "emails": ["stefan.mathe@imar.ro,", "cristian.sminchisescu@math.lth.se"], "sections": [{"heading": "1. Introduction", "text": "The central problem of constructing an object detector can be dissected into learning a confidence recognition response function and estimating a search strategy. Most often, trust functions are learned in a fully monitored environment, and the search is performed exhaustively. However, this approach is not well suited to the present state of art systems, which are trained with large amounts of image or video data, and require complex multi-layered models. First, the manual annotation of the large amounts of data required by monitored algorithms is becoming more expensive. This highlights the need to support alternative information sources to support system accuracy and efficiency. Second, the targeting complexity is sometimes high in many practical object recognition applications. Human eye movements can provide a rich source of monitoring, which, due to recent developments in eyetracking hardware, is increasingly less invasive and expensive."}, {"heading": "2. Related Work", "text": "Many methods have been proposed to accelerate detectors. Prominent techniques are based on branch-and-soil heuristics [13, 15], hierarchies of classifiers [25] or methods that reuse computation between adjacent regions [28]. Conversely, various functions have been used in designing detector response functions. Deep revolutionary neural networks have surpassed methods that rely on support vectors for many computer education problems, such as image classification [14], object recognition [19] and represent predictions [24]. Multiple instance learning [8] formulations can be considered a generalization of supervised learning processes in which class labels are assigned. Many algorithmic solutions have been proposed, based on SVMs [3, 4], CRFs [4] or controlled classifications [1]."}, {"heading": "3. Action Detection in Static Images", "text": "In view of an image showing one or more people, we want to determine both the existing actions (from a predefined list) and the corresponding spatial support; spatial support is more difficult to objectively define for an action than for an object; in this context, we consider spatial support for an action to be a narrow delimitation box that encloses both the human actor and the additional objects that the action requires; for example, while spatial support for an instance of action that plays guitar is a delimitation box that encloses both the artist and the instrument tightly (see Figure 1), we are not aware of any publicly available data sets that are suitable for this problem. While the Pascal VOC Action Classification Challenge [10] contains a large number of images of human actions, the provided annotations are not useful for action detection for several reasons. First, not all instances of action are annotated."}, {"heading": "4. Problem Formulation", "text": "Considering an input image, we formulate the recognition of actions as a problem of maximizing a trust function fc: R \u2192 R via the set of image regions R: r \u0445 = argmax r \u0420R fc (r) (1) (1) The set of image regions R can be defined either at the rough level of boundary boxes or at the finer level of image segments. In the present work, the region R consists of all segments extracted using the CPMC algorithm [6]. There are two reasons that motivate our choice: firstly, the space of image segments is several orders of magnitude smaller than the space of boundary boxes. Secondly, the figure space consists of image segments that CPMC assigns with reasonable accuracy to objects or groups of related objects representing human actions (see Table 1, column 3)."}, {"heading": "5. Learning Confidence Functions using Eye", "text": "Motion NotesIn the classical (fully monitored) approach, the confidence function fc is the reaction of a classifier trained to separate image areas with soil truth from regions with little or no overlap with the selected positive region. In this section, we consider the more difficult constellation in which no image areas with soil truth are available at training time. Instead, our training signal consists of image captions - indicating the presence or absence of action in the image - and eye movements recorded by human subjects to locate instances of the target action (see Fig. 1). Such data were recently collected and made publicly available [18]. In \u00a7 5.1, we begin with a brief analysis of the localization power of human eye movements. In \u00a7 5.2, we describe our method in detail. We evaluate the model and discuss the experimental results in \u00a7 7.1."}, {"heading": "5.1. Human Fixations and Action Localization", "text": "The sequence of fixations and saccades performed by a human subject is accurately captured in the image, but contains weaker information about the location and magnitude of the search target compared to a delimitation field or a segment. Consider, for example, the case of a person presented with an image of a concert scene in which one of the artists plays a guitar. Typically, the last fixations fall on relevant parts of the target, i.e. the person and the musical instrument. However, the remaining part of the scan path is most likely focused on the background and other objects or people. How weak is the localization signal provided by human fixations? Table 1 lists the percentage of fixed image segments in the Pascal VOC Actions 2012 dataset. It also shows the maximum overlap between a fixed image segment and the ground truth delimitation field. Both numbers are averaged across all images of an action class. Analysis suggests that while fixations cannot provide precise segments across the entire search target, they can provide strong information about the filter location."}, {"heading": "5.2. Method description", "text": "Based on the analysis in \u00a7 5.1 we develop a multiple learning formula in which the individual regions are reflected in different ways. (This is not the case). (This is not the case). (This is not the case). (This is not the case). (This is the case). (This is the case). (1) A positive bag Bj corresponds to an image in which the target activity is present and encompasses all regions considered by humans. (Some of these regions represent the detection target, and some do not. (Negative bags Bj correspond to the images in which the action is absent, and contain all regions in that image (fixed or not). We know that none of these regions represents the detection."}, {"heading": "6. Weakly Supervised Reinforcement Learning", "text": "In this section, we propose a weakly monitored model that aims to minimize the computational load required to localize the target by limiting the search to a subset of R. We present our model in \u00a7 6.1. The methodology and experimental results are discussed in \u00a7 7.2. Algorithm 2 Policy sampling algorithm. 1: Procedure SAMPLE (st = (Ht, St) 2: ct \u2190 maxi-Ht fc (ri) 3: dt-p (dt | st) with (6) 4: if dt = 1 then 5: k: argmaxi-Ht fc (ri) 6: Done. Predict region rk with confidence. 7: otherwise 8: et-p (et | st) with (7) 9: zt-p (zt | st, et) with (8: argmaxi-Ht-fc (ri) 6: Done."}, {"heading": "6.1. Saccade-and-Fixate Model for Detection", "text": "When faced with a recognition task, the human visual system has evolved into a decision in three steps: the model can make a decision on four promising image regions, defined by alternating saccades and fixations (this applies to the regions where we have collected information so far); at each step, the model can end the search; in this case, it returns its confidence for the presence of the target in the image, together with a regional hypothesis for spatial supply. We now describe in detail the state and scope of action of our model (Fig. 2). The model description: At each step, the model keeps the trace of a specified image region that has been observed so far, together with the decision on the appropriate measures in three steps."}, {"heading": "7. Experimental Results and Discussion", "text": "In this section we first discuss our experimental methodology and results for learning trust functions (\u00a7 5) and optimal search strategies for these functions (\u00a7 6)."}, {"heading": "7.1. Learning Confidence Functions", "text": "This year it is more than ever before."}, {"heading": "7.2. Learning Optimal Search Strategies", "text": "We train sequential recognition models for the confidence function, which we learn using eye movements (CMI-EYESEQ) and bounding boxes (BB-SEQ). Note that in both cases, our search model is learned exclusively with image labels. We optimize our objective function (12) using a BFGS optimizer and use the \u03bb regulator to maximize the expected reward on the validation theorem. We use 8 random initializations of the model parameters \u03b8 and execute the model to convergence, which usually takes less than 30 iterations. Rating: Our detector runs on the test theorem using the metrics in \u00a7 5. In addition, we measure the number of weighted segments and the total computation time using a C + + / Matlab implementation on an Intel Xeon E5 2660 2.20GHz CPU. Results: We determine that our search for the optimum PM5 requires the same level of the extraction during the search table (DE5 requires the optimal PM5)."}, {"heading": "8. Conclusions", "text": "We have introduced novel, generic, poorly monitored segmentation-based methods to learn precise and efficient discovery models in static images. Unlike methods trained using the Boundary Truth Bounding Box or segment annotations, our recognition response model uses novel multiple learning techniques with topological constraints to learn precise confidence functions and image classifiers using eye movement data. In addition, we are developing novel sequential models to achieve optimal, efficient search strategies based on reinforcement learning. In large-scale experiments, we demonstrate that our proposed methodology is making significant progress in terms of accuracy and speed under weak supervision."}], "references": [{"title": "Confidence-rated multiple instance boosting for object detection", "author": ["K. Ali", "K. Saenko"], "venue": "In CVPR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["J. Amores"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Support vector machines for multiple instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Multiple instance learning for sparse positive bags", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Infomax control of eye movements", "author": ["N.J. Butko", "J.R. Movellan"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts", "author": ["J. Carreira", "C. Sminchisescu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Predicting human gaze using low-level saliency combined with face detection", "author": ["M. Cerf", "J. Harel", "W. Einhuser", "C. Koch"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T.L. Perez"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Modelling search for people in 900 scenes: A combined source model of eye guidance", "author": ["K.A. Ehinger", "B. Hidalgo-Sotelo", "A. Torralba", "A. Oliva"], "venue": "Visual Cognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "The PASCAL Visual Object Classes Challenge 2012", "author": ["M. Everingham", "L.V. Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Learning to recognize daily actions using gaze", "author": ["A. Fathi", "Y. Li", "J.M. Rehg"], "venue": "In ECCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girschick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Shufflets: Shared mid-level parts for fast object detection", "author": ["I. Kokkinos"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Imagenet classi cation with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Beyond sliding windows: Object localization by efficient subwindow search", "author": ["C.H. Lampert", "M.B. Blaschko", "T. Hofmann"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["H. Larochelle", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Dynamic eye movement datasets and learned saliency models for visual action recognition", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Action from still image dataset and inverse optimal control to learn task specific visual scanpaths", "author": ["S. Mathe", "C. Sminchisescu"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Training object class detectors from eye tracking data", "author": ["D. Padadopoulos", "A. Clarke", "F. Keller", "V. Ferrari"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "An eye fixation database for saliency detection in images", "author": ["S. Ramanathan", "H. Katti", "N. Sebe", "M. Kankanhalli"], "venue": "In ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Action is in the eye of the beholder: Eye-gaze driven model for spatiotemporal action localization", "author": ["N. Shapovalova", "M. Raptis", "L. Sigal", "G. Mori"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Multiple kernels for object detection", "author": ["A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Weakly supervised structured output learning for semantic segmentation", "author": ["A. Vezhnevets", "V. Ferrari", "J.M. Buhmann"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Space-variant descriptor sampling for action recognition based on saliency and eye movements", "author": ["E. Vig", "M. Dorr", "D. Cox"], "venue": "In ECCV,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Efficient histogram-based sliding window", "author": ["Y. Wei", "L. Tao"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R. Williams"], "venue": "Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1992}], "referenceMentions": [{"referenceID": 13, "context": "This approach is however not well suited to contemporary stateof-the-art systems, which are trained using large amounts of image or video data, and require complex multi-layer models[14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 16, "context": "Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public[17, 18, 20].", "startOffset": 130, "endOffset": 142}, {"referenceID": 17, "context": "Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public[17, 18, 20].", "startOffset": 130, "endOffset": 142}, {"referenceID": 19, "context": "Large scale datasets annotated with human eye movements under various task constraints have recently been acquired and made public[17, 18, 20].", "startOffset": 130, "endOffset": 142}, {"referenceID": 16, "context": "Such data has been exploited at training time to support visual classification performance in video[17], but its usefulness as a training signal for detection, in the absence of any additional annotations, remains unexplored.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 61, "endOffset": 69}, {"referenceID": 14, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 61, "endOffset": 69}, {"referenceID": 24, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "Prominent techniques are based on branch-and-bound heuristics[13, 15], hierarchies of classifiers[25] or methods that reuse computation between neighboring regions[28].", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 18, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 206, "endOffset": 210}, {"referenceID": 18, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 233, "endOffset": 237}, {"referenceID": 23, "context": "Deep convolutional neural networks have surpassed methods based on support vector machines on many computer vision problems, such as the image classification[14], object classification[19], object detection[12], action classification[19] and pose prediction[24].", "startOffset": 257, "endOffset": 261}, {"referenceID": 7, "context": "Multiple instance learning[8] formulations can be seen as a generalization of supervised learning, in which class labels are assigned to sets of training examples.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 60, "endOffset": 66}, {"referenceID": 25, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "Many algorithmic solutions have been proposed, based on SVMs[3, 4], CRFs[26] or boosted classifiers[1] (see [2] for a review).", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 45, "endOffset": 48}, {"referenceID": 16, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 60, "endOffset": 72}, {"referenceID": 17, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 60, "endOffset": 72}, {"referenceID": 19, "context": "Search targets include pedestrians[9], faces [7] and actions[17, 18, 20].", "startOffset": 60, "endOffset": 72}, {"referenceID": 16, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 135, "endOffset": 147}, {"referenceID": 26, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 135, "endOffset": 147}, {"referenceID": 10, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 135, "endOffset": 147}, {"referenceID": 21, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "Eye movements have successfully been used to boost the performance of computer vision systems, such as action classification from video[17, 27, 11], action detection from video[22] or image segmentation[21].", "startOffset": 202, "endOffset": 206}, {"referenceID": 26, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 79, "endOffset": 87}, {"referenceID": 20, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 79, "endOffset": 87}, {"referenceID": 16, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 106, "endOffset": 118}, {"referenceID": 10, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 106, "endOffset": 118}, {"referenceID": 21, "context": "Some of these systems assume the availability of eye movement data at test time[27, 21], while some do not[17, 11, 22].", "startOffset": 106, "endOffset": 118}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Unlike [20], our method does not require the availability of ground truth bounding boxes at training time.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 80, "endOffset": 91}, {"referenceID": 4, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 80, "endOffset": 91}, {"referenceID": 17, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 80, "endOffset": 91}, {"referenceID": 15, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 149, "endOffset": 152}, {"referenceID": 17, "context": "Visual analysis systems based on fixate-and-saccade ideas have been proposed in [16, 5, 18], for the problem of digit recognition[16], face detection[5] and saccade prediction[18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 22, "context": "Here, we derive novel, fully trainable models for two challenging problems, namely action recognition and detection in cluttered natural scenes, and in a reinforcement optimal learning setup[23].", "startOffset": 190, "endOffset": 194}, {"referenceID": 9, "context": "While the Pascal VOC Action Classification Challenge[10] contains a large set of images with human actions, the provided annotations are not useful for action detection for several reasons.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "In the present work, the region space R consists of all segments extracted using the CPMC algorithm[6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "Such data has been recently acquired and made publicly available[18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "|r \u2229 r\u2032| / |r\u2032| \u2264 TS} (2) Unconstrained fringe: Previous experience with training sliding window detectors[19] has shown that excluding bounding boxes overlapping the ground truth from the set of negative examples can greatly improve the localization power.", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "1), we end up with the miSVM formulation[3], which treats all training instances as independent from one another.", "startOffset": 40, "endOffset": 43}, {"referenceID": 28, "context": "The gradient of the expected reward can be approximated as[29, 23]:", "startOffset": 58, "endOffset": 66}, {"referenceID": 22, "context": "The gradient of the expected reward can be approximated as[29, 23]:", "startOffset": 58, "endOffset": 66}, {"referenceID": 13, "context": "[14] as feature extractor g.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost.", "creator": "LaTeX with hyperref package"}}}