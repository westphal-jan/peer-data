{"id": "1412.0233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2014", "title": "The Loss Surfaces of Multilayer Networks", "abstract": "We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from the random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function are located in a well-defined narrow band lower-bounded by the global minimum. Furthermore, they form a layered structure. We show that the number of local minima outside the narrow band diminishes exponentially with the size of the network. We empirically demonstrate that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band containing the largest number of critical points, and that all critical points found there are local minima and correspond to the same high learning quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Simultaneously we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.", "histories": [["v1", "Sun, 30 Nov 2014 15:48:16 GMT  (294kb,D)", "http://arxiv.org/abs/1412.0233v1", null], ["v2", "Thu, 4 Dec 2014 21:46:57 GMT  (358kb,D)", "http://arxiv.org/abs/1412.0233v2", null], ["v3", "Wed, 21 Jan 2015 22:25:26 GMT  (367kb,D)", "http://arxiv.org/abs/1412.0233v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna choromanska", "mikael henaff", "michael mathieu", "g\\'erard ben arous", "yann lecun"], "accepted": false, "id": "1412.0233"}, "pdf": {"name": "1412.0233.pdf", "metadata": {"source": "CRF", "title": "The Loss Surface of Multilayer Networks", "authors": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu"], "emails": ["achoroma@cims.nyu.edu", "mbh305@nyu.edu", "mathieu@cs.nyu.edu", "benarous@cims.nyu.edu", "yann@cs.nyu.edu"], "sections": [{"heading": null, "text": "We study the link between the highly non-convex loss function of a simple model of the fully networked forward-facing neural network and the Hamiltonian of the spherical Spingglass model under the assumptions of: i) variable independence, ii) redundancy in network parameterization, and iii) uniformity. These assumptions allow us to explain the complexity of the fully decoupled neural network through the prism of random matrix results. We show that for large decoupled networks, the lowest critical values of random loss function are in a well-defined narrow band limited by the global minimum. Furthermore, they form a layered structure. We show that the number of local minima outside the narrow band decreases exponentially with the size of the network. We demonstrate empirically that the mathematical model behaves similarly to computer simulations."}, {"heading": "1 Introduction", "text": "This year, it will be able to take the lead, \"he said in an interview with the Taiwanese daily Le."}, {"heading": "2 Prior work", "text": "In the nineties of the twentieth century, when the world was still in order, the world was still in order."}, {"heading": "3 Deep network and spin-glass model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminaries", "text": "For theoretical analysis, we will consider a simple model of the fully connected network with a single output and rectified linear units. We will call the network N. We will focus on a binary classification task. Let X specify the random input vector of dimensionality. Let (H \u2212 1) specify the number of hidden layers in the network and we will point to the input layer as the 0th layer and the output layer as the Hth layer. Let ni specify the number of units in the ith layer (note that n0 = d and nH = 1). Let us specify the matrix of weights between (i \u2212 1) th and ith layers of the network H. Let us also specify the activation function that converts the weighted input of a unit into its output activation. We will therefore consider linear rectifiers as such (x) = max. (0, x). We can therefore write that for each given input vector, the network gives X random (output)."}, {"heading": "3.2 Approximation", "text": "In this section, we introduce randomness to the model by assuming Xs and As to be independent. We assume that certain assumptions regarding the neural network model exist. (...) We also make two other assumptions regarding redundancy of network parameters and their uniformity (details in the text below), both of which are justified by empirical evidence in the literature. (...) We assume that any input Xi, j is a normal random variable, such as Xi, j \u00b2 the redundancy of network parameters and their uniformity (details in the text below). The model contains several dependencies as one input into the network. This poses a major theoretical problem in analyzing these models, as it is unclear how to explain these dependencies. Instead, in this paper, we study fully decoupled models. (De la Pen, a and Gine, 1999), where Xi, j's is assumed to be independent."}, {"heading": "4 Theoretical results", "text": "The theoretical results of this section are a direct consequence of the theoretical analysis of the complexity of the spherical spin-glass models by [Auffinger et al., 2010]. We will first introduce the notation and definitions where the number of critical values of lump, H (w) in the fixed values of lump, H (w) in the fixed values B = {X: x, u)} with index1 equals to k. Likewise, we will as CB (B) a random total number of critical values of lump, H (w).Later in the papers of critical values of the loss function that is not divergent (fixed) index that we call non divergent with index."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "To clarify the validity of our theoretical assumptions, we performed Spingglass simulations for various dimensions from 25 to 500. For each of these values, we obtained an estimate of the distribution of minimums by sampling 1000 starting points on the sphere of unity and performing an approximate gradient descent (SGD) to find a minimum energy point. Note that in this section we refer to the energy of the Hamiltonian model of the spinning glass as its loss.Neural Network We conducted an analogous experiment on a scaled version of MNIST, where each image is 10 \u00d7 10 in size. Specifically, we created 1000 networks with a hidden layer and n1 {25, 50, 250, 500} hidden units starting from a random set of parameters within the cube unit. All networks were trained for 200 epochs with the learning rate decay.We were trained to verify the validity of our theoretical assumption that we were uniform within the cube unit."}, {"heading": "5.2 Results", "text": "Figure 3 shows the distribution of scaled losses for both groups of experiments. In the case of spin glasses (left figure), we find that at small values of accuracy in many experiments, we obtain poor local minima, while at larger values of accuracy, the distribution is increasingly concentrated around the energy barrier, where local minima are of high quality. We observe that in all experiments, the left tails touch the hard-to-penetrate barrier and the values are concentrated around \u2212 E \u00b0 with increasing accuracy. In fact, this concentration result has been predicted for a long time, but only proved [Auffinger et al., 2010]. We see that the loss distribution in the neural network experiments (right figure) exhibits a qualitatively similar behavior. Even after scaling, the variance decreases with higher network sizes. This is clearly captured in Figure 4. This indicates that adherence to bad local minima is a major problem for smaller networks, but gradually loses importance as the network size increases."}, {"heading": "5.3 Relationship between train and test loss", "text": "Previous theories and experiments suggest that minima are in a band that gets smaller as the network size increases, suggesting that computable solutions to training errors are becoming increasingly equivalent, but how does this relate to errors in the test set? To determine this, we calculated the correlation between training and test loss for each network size for all solutions with the following result: We see that training and test errors are increasingly decorative as the network size increases, which is another indication that trying to find the absolute minimum possible is of limited use in terms of generalization performance."}, {"heading": "6 Conclusion", "text": "This paper establishes a link between the neural network and the spin-glass model. We show that, under certain assumptions, the loss function of the fully decoupled large-format neural network of depth H exhibits a landscape similar to the Hamiltonian one of the H-Spherical Spin-Glass model. We show empirically that, despite variable dependencies in real networks, both models studied here are very similar even in real environments. To our knowledge, our work is one of the first attempts in the literature to shed light on the theory of the optimization of neural networks."}, {"heading": "7 Proof of Theorem 3.1", "text": "Proof. First, we will prove the lower limit on N. Due to the inequality between arithmetic and geometric mean, mass and size of the network are connected as follows: N \u2265 H \u221a 2 HH \u221a n0nH = H \u221a 2 H H \u221a d, and since H \u221a HH \u221a d = H \u221a Hi = 1 niH \u2265 1 thenN \u2265 H and 2 H \u221a d \u2265 H \u221a H. Next, we will show the upper limit on N. Let us leave nmax = maxi \u0438\u043c\u0438\u043d\u0438\u0439 {1,2,..., H} ni. ThenN \u2264 Hn2max \u2264 H\u0442 2."}, {"heading": "8 Proof of Theorem 3.2", "text": "The proof: We will first use the following more general lemma.Lemma 8.1. Let Y1 and Y2 be the results of two arbitrary binary classifiers = = | These are the results of two arbitrary binary classifiers. \u2212 Assuming that the first classifier says \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 where we assume p \u2264 0.5 and \u2212 1 otherwise (Y1), let us assume the prediction accuracy of the second classifier (Y2) p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 without loss of generality. Evidence. Let us consider two random variables Z1 = character (Y1) and Z2 = the prediction accuracy of the second classifier (Y1) p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 b \u2212 p \u2212 p \u2212 p \u2212 b \u2212 p \u2212 b \u2212 p \u2212 p \u2212 p \u2212 b \u2212 p \u2212 p \u2212 b \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 b \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 b \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 b \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212"}, {"heading": "9 Proof of Theorem 3.3", "text": "Proof. Note that E [Y-s] = 0 and E [Ys] = 0. Furthermore, E [Y-sY\u044b] = q2\u03c12-i1, i2,..., iH = 1min (HQ-H, ti1, i2,..., iH) H-k = 1 w2ikstd (Ys) = q2-i1, i2,..., iH-k = 1 w2ikstd (Ys) = q2,..., iH-K = 1 w2ikstd (Ys) = q2,..., iH-K = 1 ti1, i2,..., iH-k = 1 w2ik. Thereupon, Prognoorr (Y-H, Ys) = 1 w2ikstd (Ys) = 1 w2ikstd (Ys) = 1 w2ikstd (Ys) = 1 min (HQ-H, ti1,..., iH) H-k = 1w2ik, iK-K = 1 K, iK = 1 K, iK = 1 K, iK = 1 K, iK = 1 K, iK = 1 K, iK = 1 K, iK = 1 K, iK = 1 K, iK = 1, iK-H)."}, {"heading": "10 Loss function as a H - spin spherical spin-glass model", "text": "We consider two loss functions, (random) absolute loss spawning, H (w) and (random) hinge loss Lhs, H (w), defined in the main body of the paper. Let us remember that in case of hinge loss max operator can be modeled as Bernoulli random variable, which we will call M, with probability of success (M = 1). We assume that M is independent of Y cases. Therefore, we get the following law, H (w) = {S \u2212 Y cases, if Yt = S + Y cases, if Yt = \u2212 XiLhs, H (w) = EM [M \u2212 Y cases)."}, {"heading": "11 Asymptotics of the mean number of critical points and local minima", "text": "Next we present the asymptotic factors of the medium number of critical points (Theorem 11,1) and the medium number of local minima (Theorem 11,2) that widen the theorem 4,1. These results are the consequences of Theorem 2,17 and 2,18. [Aufzinger et al., 2010].Theorem 11,1. For H \u2265 3 the following is considered Economy Economy Economy Economy Economy Economy Economy Economy Economy Economy Economy Economy Economy Economy, Economy Economy Economy, Economy Economy Economy, Economy Economy Economy Economy, Economy Economy Economy, Economy Economy Economy Economy, Economy Economy Economy Economy, Economy Economy, Economy Economy Economy, Economy, Economy Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Economy, Econ"}, {"heading": "12 Additional Experiments", "text": "Figure 8 shows the distribution of normalized indices, i.e. the proportion of negative eigenvalues, for neural networks with n1 = {10, 25, 50, 100}. We see that all solutions are minima or saddle points with very low index.Figure 6 compares SGD with SA. Figure 7 captures the zoomed-in (n1 = {10, 25, 50, 100}) box plot, which is based on the distribution of scaled losses for the neural network experiment.a) n1 = 25 nhidden = 10 nhidden = 10 nhidden = 10normalized indexF requ ency0.0000 0.0005 0.0010 0.0015 0.0020 0.00250 200 400 600 800b) n1 = 25 nhidden = 25normalized indexF requ ency0.000 0.005 0.010 0.0150 200 400 600 800c) n1 = 50 nhidden = 50"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We study the connection between the highly<lb>non-convex loss function of a simple model of<lb>the fully-connected feed-forward neural net-<lb>work and the Hamiltonian of the spherical<lb>spin-glass model under the assumptions of:<lb>i) variable independence, ii) redundancy in<lb>network parametrization, and iii) uniformity.<lb>These assumptions enable us to explain the<lb>complexity of the fully decoupled neural net-<lb>work through the prism of the results from<lb>the random matrix theory. We show that for<lb>large-size decoupled networks the lowest crit-<lb>ical values of the random loss function are<lb>located in a well-defined narrow band lower-<lb>bounded by the global minimum. Further-<lb>more, they form a layered structure. We<lb>show that the number of local minima out-<lb>side the narrow band diminishes exponen-<lb>tially with the size of the network. We em-<lb>pirically demonstrate that the mathemati-<lb>cal model exhibits similar behavior as the<lb>computer simulations, despite the presence<lb>of high dependencies in real networks. We<lb>conjecture that both simulated annealing and<lb>SGD converge to the band containing the<lb>largest number of critical points, and that<lb>all critical points found there are local min-<lb>ima and correspond to the same high learn-<lb>ing quality measured by the test error. This<lb>emphasizes a major difference between large-<lb>and small-size networks where for the lat-<lb>ter poor quality local minima have non-zero<lb>probability of being recovered. Simultane-<lb>ously we prove that recovering the global<lb>minimum becomes harder as the network size<lb>increases and that it is in practice irrelevant<lb>as global minimum often leads to overfitting.<lb>", "creator": "LaTeX with hyperref package"}}}