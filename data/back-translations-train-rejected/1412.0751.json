{"id": "1412.0751", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2014", "title": "Tiered Clustering to Improve Lexical Entailment", "abstract": "Many tasks in Natural Language Processing involve recognizing lexical entailment. Two different approaches to this problem have been proposed recently that are quite different from each other. The first is an asymmetric similarity measure designed to give high scores when the contexts of the narrower term in the entailment are a subset of those of the broader term. The second is a supervised approach where a classifier is learned to predict entailment given a concatenated latent vector representation of the word. Both of these approaches are vector space models that use a single context vector as a representation of the word. In this work, I study the effects of clustering words into senses and using these multiple context vectors to infer entailment using extensions of these two algorithms. I find that this approach offers some improvement to these entailment algorithms.", "histories": [["v1", "Tue, 2 Dec 2014 00:53:35 GMT  (303kb,D)", "http://arxiv.org/abs/1412.0751v1", "Paper for course project for Advanced NLP Spring 2013. 8 pages"]], "COMMENTS": "Paper for course project for Advanced NLP Spring 2013. 8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john wieting"], "accepted": false, "id": "1412.0751"}, "pdf": {"name": "1412.0751.pdf", "metadata": {"source": "CRF", "title": "Tiered Clustering to Improve Lexical Entailment", "authors": ["John Wieting"], "emails": ["wieting2@illinois.edu"], "sections": [{"heading": null, "text": "Many tasks in the processing of natural language involve the recognition of lexical relationships. Recently, two different approaches to this problem have been proposed, which differ greatly from each other: the first is an asymmetrical measure of similarity, which is designed to achieve high values when the contexts of the narrower term are related to the broader term; the second is a supervised approach, in which a classifier is taught to predict relationships when there is a latent vector representation of the word; both approaches are vector spatial models in which a single context vector is used as a representation of the word. In this paper, I will examine the effects of clustering words in sense and the use of these multiple context vectors to infer connections by extending these two algorithms. I find that this approach offers some improvement in these context algorithms."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to assert themselves."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Defining Lexical Entailment", "text": "In view of two propositions, it is not easy to define lexical contexts in a useful way. (Zhitomirsky-Geffet and Dagan, 2009) defined contexts in terms of substitution. Essentially, they say that word a entails a word if a word can replace b in a sentence and this new sentence entails the original. This approach leads to a high intercommentator agreement on the withdrawal task, but it is argued that this definition does not cover all cases of withdrawal. Thus, Turney et. al. argue that in the sentences Jane dropped the glass and Jane dropped something fragile, the word glass should entail a fragile definition. Subsequently, they define withdrawal by semantic relationships in (Bejar et al., 1991). They claim that some of these contexts define a withdrawal relationship between all pairs of words with this relationship."}, {"heading": "2.2 Approaches for Lexical Entailment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 balAPinc", "text": "This approach, first described in (Kotlerman et al., 2010), aims to reward situations in which the first context vector argument is a subset of the second. In other words, the characteristics of the first context vectors should be included in the second. Naturally, this idea stems from the distribution hypothesis (Geffet, 2005), which states that word a occurs in a subset of the context of word b, then often results in b. The formula for calculating balAPinc (u, v) = \u221a APinc (u, v) \u00b7 LIN (u, v) \u00b7 LIN (u, v) (1) (Ac, Fu (2), Fu (Fu), Fu (Fu) (Fu) (Fu) (Fu) (Fu) (Fu), Fu (Fu) (Fu), Fu (Fu) (Fu), Fu (Fu) (Fu), Fu (Fu) (Fu), Fu (Fu) (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu (Fu), Fu (Fu), Fu (Fu (Fu), Fu (Fu), Fu (Fu), Fu (Fu (Fu), Fu (Fu (Fu), Fu (Fu), Fu (Fu (Fu), Fu (Fu), Fu (Fu (Fu (Fu), Fu) and Fu (Fu (Fu (Fu), Fu (Fu (Fu), Fu (Fu (Fu), Fu (Fu),"}, {"heading": "2.2.2 ConVecs", "text": "ConVecs (Baroni et al., 2012) hypothesizes that the linking of words a and b is a learnable function of the concatenation of their context vectors. The authors propose to reduce the dimensionality of the context vectors by means of singular value decomposition and then to use a nuclear SVM to learn this function. To obtain a score from this model, a probability estimate can be used and the probability for the positive class can be used as a score."}, {"heading": "2.3 Clustering Occurrences", "text": "In (Reisinger and Mooney, 2010b) the authors use the cluster method of mixing Mises-Fisher distributions (movMF) (Banerjee et al., 2005). One disadvantage of this approach is that the number of clusters must be determined in advance. To remove this limitation, (Reisinger and Mooney, 2010a) a staggered cluster is introduced, a non-parametric model that does not require the specification of this parameter. In this paper I also examine the effectiveness of another type of cluster algorithms, the correlation cluster, which also has the advantage that this parameter does not need to be specified. The advantage of this algorithm over a staggered cluster would be that it takes into account the relationship of the words themselves in clustering and not just their number."}, {"heading": "2.3.1 Tiered Clustering", "text": "Since this model is a special case of the nested Chinese restaurant process (Blei et al., 2004), where the depth is set to 2, there are two advantages of this approach over other cluster methods: the first, mentioned above, is that there is no need to specify a priori the number of clusters; the second, that there is a root node that can capture those characteristics that have context-independent deviations, preventing these characteristics from dominating the cluster process; and the sampling algorithm is similar to that of the NCRP. Essentially, in an iteration of the Gibbs sampling method, we first try a cluster for the current word event (or document in the terminology of the NCRP), and this sampling method could lead to the creation of a new cluster."}, {"heading": "2.3.2 Correlation Clustering", "text": "The algorithm is very simple and all that is needed is a single parameter \u03c3 and a similarity metric. Basically, a single point is drawn from the set of points to which no cluster has been assigned yet. Then, every other point is compared with this, using the similarity metric, and if the score of this pair is greater than \u03c3, then these points are placed in the same cluster. The process repeats until all points are assigned a cluster. Due to the long period of time this can take if there are numerous outliers and also to limit the number of clusters, I added a final condition. The algorithm would terminate after having at least two clusters, each containing at least 2.5% of the points, and if the last five clusters that were formed contained less than 2.5% of the points."}, {"heading": "3 Algorithm Extensions", "text": "In the first phase, we can calculate the rating of all possible pairs of words between the two words and use the maximum score for these words. Another idea is to use the average score of these points (Turney and Mohammad, 2013), where the author claims that two words are mutually conditional when a pair of their senses causes each other. It is also mentioned in the second phase, where Reisinger and Mooney use the maximum score for these words. Another idea is to use the average score of these points."}, {"heading": "4 Experiments", "text": "These datasets were chosen because they were both created with different definitions of lexical terms, which were also used in the application practice of 1228 examples. This dataset was precisely balanced to contain the same number of positive and negative examples and was created with the substitution definition of withdrawal. The second dataset, which was set with 720 examples, JMTH, was created by SemEval-2012 Task 2 according to the instructions in the manual. (Turney and Mohammad, 2013) This is a difficult dataset that contains such positive examples as crack withdrawal. It was created with the semantic relation of withdrawal. Data for the experiment in the form of tagged word operations was used in the application practice. (Turney and Mohammad, 2013) This is a difficult dataset with such positive examples as crack withdrawal glass."}, {"heading": "5 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Results", "text": "One thing that is clear from the results is the importance of how the senses are combined to make a classification decision. Surprisingly, it is not always the best option to take only the maximum score. There are very conflicting results that are likely to have to do with clustering and noise in the data. This is best illustrated with ConVecs, as this approach has been able to find a cluster that gave a positive signal in each example of the data set, hence the accuracy of 50.0%. Therefore, some averages tend to reduce the impact of this noise, while senses that appear less frequently in the data are given more influence and influence the decision than they would have in a single prototype approach. However, from the experiments it appears that clustering works better than the baseline if the corresponding algorithm is chosen. For example, in ConVecs, tiered clustering with the mean of the vectors makes sense as it meets the two sets of data."}, {"heading": "5.2 Clusters", "text": "These numbers are interesting for several reasons: First of all, I was skeptical of the root node in the cluster algorithm, because I found that the characteristics were strongly filtered before clustering, and therefore the root node would only serve to eliminate useful features that can be used to assign the occurrence to clusters. However, the root seems to capture noisy features like those that appear to be artifacts of tokenization. Granted, these characteristics could probably be truncated with a kind of tf-idf, but it seems that this node has a useful function. Another thing to note is that there are greater differences between clusters in the tiered word approach and the correlative cluster approach. Therefore, it seems that clustering generates better clusters overall. However, tiered clustering generates an average of 15.6 clusters per occurrence, while clustering takes an average of 1.5 minutes."}, {"heading": "6 Conclusion", "text": "The results show that clustering with a suitable algorithm for combining the senses of a single prototype can consistently produce better results than combining the senses of a single prototype. There is a lot of room for future work in this arena. On the one hand, clustering could be improved. It would be interesting to see if there is a way to include the word relationship in the tiered cluster model. By combining some similar clusters, this approach would be more robust, as the number of clusters would be more limited. Another way to limit the clusters would be to have a higher threshold of occurrences required for a cluster. Perhaps coordinating this parameter would have yielded better results, as the number of clusters would be more limited. Another way to limit the clusters would be to have a higher threshold of occurrences required for a cluster."}, {"heading": "Acknowledgments", "text": "Thanks to Prof. Hockenmaier, Joseph Reisinger and Prof. Roth for their helpful discussions. Thanks also to Dr. Turney and Prof. Baroni for providing the data used in these experiments."}], "references": [{"title": "Clustering on the unit hypersphere using von mises-fisher distributions", "author": ["Arindam Banerjee", "Inderjit S. Dhillon", "Joydeep Ghosh", "Suvrit Sra."], "venue": "Journal of Machine Learning Research, 6:1345\u20131382.", "citeRegEx": "Banerjee et al\\.,? 2005", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Correlation clustering", "author": ["Nikhil Bansal", "Avrim Blum", "Shuchi Chawla."], "venue": "MACHINE LEARNING, pages 238\u2013247.", "citeRegEx": "Bansal et al\\.,? 2002", "shortCiteRegEx": "Bansal et al\\.", "year": 2002}, {"title": "Entailment above the word level in distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung chieh Shan."], "venue": "Walter Daelemans, Mirella Lapata, and Llu\u0131\u0301s M\u00e0rquez, editors, EACL, pages 23\u201332. The Association for Computer", "citeRegEx": "Baroni et al\\.,? 2012", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Cognitive and psychometric analysis of analogical problem solving", "author": ["I.I. Bejar", "R. Chaffin", "S.E. Embretson."], "venue": "Recent research in psychology. SpringerVerlag.", "citeRegEx": "Bejar et al\\.,? 1991", "shortCiteRegEx": "Bejar et al\\.", "year": 1991}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["David M. Blei", "Thomas L. Griffiths", "Michael I. Jordan", "Joshua B. Tenenbaum."], "venue": "Advances in Neural Information Processing Systems, page 2003. MIT Press.", "citeRegEx": "Blei et al\\.,? 2004", "shortCiteRegEx": "Blei et al\\.", "year": 2004}, {"title": "Robust, light-weight approaches to compute lexical similarity", "author": ["Q. Do", "D. Roth", "M. Sammons", "Y. Tu", "V. Vydiswaran."], "venue": "Technical report.", "citeRegEx": "Do et al\\.,? 2009", "shortCiteRegEx": "Do et al\\.", "year": 2009}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["Maayan Geffet."], "venue": "In Proceedings of ACL-2005. Ann Arbor, pages 107\u2013114.", "citeRegEx": "Geffet.,? 2005", "shortCiteRegEx": "Geffet.", "year": 2005}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-geffet."], "venue": "Nat. Lang. Eng., 16(4):359\u2013389, October.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "A mixture model with sharing for lexical semantics", "author": ["Joseph Reisinger", "Raymond J. Mooney."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2010), pages 1173\u20131182, MIT, Massachusetts, USA, October", "citeRegEx": "Reisinger and Mooney.,? 2010a", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J. Mooney."], "venue": "Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-2010), pages", "citeRegEx": "Reisinger and Mooney.,? 2010b", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Experiments with three approaches to recognizing lexical entailment (submitted)", "author": ["P. Turney", "S. Mohammad"], "venue": null, "citeRegEx": "Turney and Mohammad.,? \\Q2013\\E", "shortCiteRegEx": "Turney and Mohammad.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Intell. Res. (JAIR), 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Verbs semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer."], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL \u201994, pages 133\u2013138, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wu and Palmer.,? 1994", "shortCiteRegEx": "Wu and Palmer.", "year": 1994}, {"title": "Bootstrapping distributional feature vector quality", "author": ["Maayan Zhitomirsky-Geffet", "Ido Dagan."], "venue": "Computational Linguistics, 35(3):435\u2013461.", "citeRegEx": "Zhitomirsky.Geffet and Dagan.,? 2009", "shortCiteRegEx": "Zhitomirsky.Geffet and Dagan.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "An example, from (Turney and Mohammad, 2013) would be:", "startOffset": 17, "endOffset": 44}, {"referenceID": 10, "context": "Recently, in (Turney and Mohammad, 2013) three different approaches to lexical entailment were analyzed.", "startOffset": 13, "endOffset": 40}, {"referenceID": 7, "context": "The first, known as balAPinc (balanced average precision for distributional inclusion) (Kotlerman et al., 2010), is an asymmetric similarity measure and the second, ConVecs (concatenated vectors) (Baroni et al.", "startOffset": 87, "endOffset": 111}, {"referenceID": 2, "context": ", 2010), is an asymmetric similarity measure and the second, ConVecs (concatenated vectors) (Baroni et al., 2012) uses a supervised approach.", "startOffset": 92, "endOffset": 113}, {"referenceID": 9, "context": "ters to determine their similarity score (Reisinger and Mooney, 2010b) and (Reisinger and Mooney, 2010a).", "startOffset": 41, "endOffset": 70}, {"referenceID": 8, "context": "ters to determine their similarity score (Reisinger and Mooney, 2010b) and (Reisinger and Mooney, 2010a).", "startOffset": 75, "endOffset": 104}, {"referenceID": 8, "context": "The first follows that in (Reisinger and Mooney, 2010a), in a technique known as tiered clustering.", "startOffset": 26, "endOffset": 55}, {"referenceID": 4, "context": "This is a Dirichlet Process clustering model that is equivalent to the nested Chinese Restaurant Proccess (Blei et al., 2004) with a fixed depth of two.", "startOffset": 106, "endOffset": 125}, {"referenceID": 1, "context": "ation of correlaton clustering (Bansal et al., 2002) which is useful for cases when one wants to cluster solely using a distance metric.", "startOffset": 31, "endOffset": 52}, {"referenceID": 13, "context": "(Zhitomirsky-Geffet and Dagan, 2009) defined entailment in terms of substitution.", "startOffset": 0, "endOffset": 36}, {"referenceID": 3, "context": "They then go on to define entailment through the semantic relations in (Bejar et al., 1991).", "startOffset": 71, "endOffset": 91}, {"referenceID": 2, "context": "The first, known as BBDS (Baroni et al., 2012) is motivated by the first definition and is largely a collection of hyponymhypernym pairs.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "This approach, first described in (Kotlerman et al., 2010), aims to reward those situations when the first context vector argument is a subset of the second.", "startOffset": 34, "endOffset": 58}, {"referenceID": 6, "context": "This idea naturally comes from the distributional inclusion hypothesis (Geffet, 2005), which states that if word a occurs in a subset of the context of word b then a often entails b.", "startOffset": 71, "endOffset": 85}, {"referenceID": 2, "context": "ConVecs (Baroni et al., 2012) operates under the hypothsis that the entailment of words a and b is a", "startOffset": 8, "endOffset": 29}, {"referenceID": 9, "context": "In (Reisinger and Mooney, 2010b), the authors use the mixture of von Mises-Fisher distributions (movMF) clustering method (Banerjee et al.", "startOffset": 3, "endOffset": 32}, {"referenceID": 0, "context": "In (Reisinger and Mooney, 2010b), the authors use the mixture of von Mises-Fisher distributions (movMF) clustering method (Banerjee et al., 2005).", "startOffset": 122, "endOffset": 145}, {"referenceID": 8, "context": "To remove this restriction, (Reisinger and Mooney, 2010a) introduces tiered clustering, a nonparametric model, that does not require this parameter to be specified.", "startOffset": 28, "endOffset": 57}, {"referenceID": 8, "context": "Figure 1: Plate diagram for tiered clustering (Reisinger and Mooney, 2010a).", "startOffset": 46, "endOffset": 75}, {"referenceID": 4, "context": "nese Restaurant Process (Blei et al., 2004) where the depth is fixed at 2.", "startOffset": 24, "endOffset": 43}, {"referenceID": 1, "context": "I used a variation of correlation clustering (Bansal et al., 2002).", "startOffset": 45, "endOffset": 66}, {"referenceID": 5, "context": "The distance function used was the LLM measure used in (Do et al., 2009).", "startOffset": 55, "endOffset": 72}, {"referenceID": 12, "context": "To compute the distance between words, the Wu Palmer algorithm was used (Wu and Palmer, 1994).", "startOffset": 72, "endOffset": 93}, {"referenceID": 10, "context": "Support for this idea lies in (Turney and Mohammad, 2013) where the author claims that two words entail each other if any pair of their senses entail each other.", "startOffset": 30, "endOffset": 57}, {"referenceID": 9, "context": "It is also mentioned in (Reisinger and Mooney, 2010b).", "startOffset": 24, "endOffset": 53}, {"referenceID": 8, "context": "This approach is used in (Reisinger and Mooney, 2010a) and (Reisinger and Mooney,", "startOffset": 25, "endOffset": 54}, {"referenceID": 9, "context": "and used in (Reisinger and Mooney, 2010b) but are not used in the tiered clustering paper (Reisinger and Mooney, 2010a).", "startOffset": 12, "endOffset": 41}, {"referenceID": 8, "context": "and used in (Reisinger and Mooney, 2010b) but are not used in the tiered clustering paper (Reisinger and Mooney, 2010a).", "startOffset": 90, "endOffset": 119}, {"referenceID": 10, "context": "formed on two of the three data sets used in (Turney and Mohammad, 2013).", "startOffset": 45, "endOffset": 72}, {"referenceID": 2, "context": "The first, dubbed BBDS and was also used in (Baroni et al., 2012), consists of 1228 examples.", "startOffset": 44, "endOffset": 65}, {"referenceID": 10, "context": "The second data set with 720 examples, JMTH, was created from the SemEval-2012 Task 2 following the instruction in (Turney and Mohammad, 2013).", "startOffset": 115, "endOffset": 142}, {"referenceID": 8, "context": "Additionally, these features were also pruned as per (Reisinger and Mooney, 2010a) to only the most frequent 500 terms.", "startOffset": 53, "endOffset": 82}, {"referenceID": 11, "context": "The ococcurrence frequencies were transformed to positive pointwise mutual information values (PPMI) (Turney and Pantel, 2010) in order to better represent the importance of a context feature for a given word.", "startOffset": 101, "endOffset": 126}], "year": 2014, "abstractText": "Many tasks in Natural Language Processing involve recognizing lexical entailment. Two different approaches to this problem have been proposed recently that are quite different from each other. The first is an asymmetric similarity measure designed to give high scores when the contexts of the narrower term in the entailment are a subset of those of the broader term. The second is a supervised approach where a classifier is learned to predict entailment given a concatenated latent vector representation of the word. Both of these approaches are vector space models that use a single context vector as a representation of the word. In this work, I study the effects of clustering words into senses and using these multiple context vectors to infer entailment using extensions of these two algorithms. I find that this approach offers some improvement to these entailment algorithms.", "creator": "TeX"}}}