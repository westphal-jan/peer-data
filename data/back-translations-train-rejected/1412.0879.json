{"id": "1412.0879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2014", "title": "Watsonsim: Overview of a Question Answering Engine", "abstract": "The objective of the project is to design and run a system similar to Watson, designed to answer Jeopardy questions. In the course of a semester, we developed an open source question answering system using the Indri, Lucene, Bing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka among additional modules. By the end of the semester, we achieved 18% accuracy on Jeopardy questions, and work has not stopped since then.", "histories": [["v1", "Tue, 2 Dec 2014 12:15:18 GMT  (27kb)", "http://arxiv.org/abs/1412.0879v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["sean gallagher", "wlodek zadrozny", "walid shalaby", "adarsh avadhani"], "accepted": false, "id": "1412.0879"}, "pdf": {"name": "1412.0879.pdf", "metadata": {"source": "CRF", "title": "Watsonsim: Overview of a Question Answering Engine", "authors": ["Sean Gallagher", "Wlodek Zadrozny", "Walid Shalaby"], "emails": ["sgalla19@uncc.edu", "wzadrozn@uncc.edu", "wshalaby@uncc.edu", "amanhuna@uncc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.08 79v1 [cs.CL] 2 Dec 201 4"}, {"heading": "1. INTRODUCTION", "text": "Given the time constraints, we prioritized our efforts, starting with a minimal functioning subset of the functionality needed to answer a question, so the first demonstration was to index and query Wikipedia with Lucene, using titles as answers, and from there we added supportive passage search, question classification, additional sources, evidence assessment, scoring models, etc. As a team, we were divided into smaller groups, each group specializing in one facet of the pipeline. Overall, there were teams for source capture, search and query creation, machine learning and scoring, and group integration.Development began without Apache UIMA because we did not know it at the time. By the middle of the semester, developers were split on whether the project should be switched to UIMA, but we decided to keep our existing solution, and whether this was optimal remains an important question."}, {"heading": "2. DATA SOURCES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Offline sources", "text": "The main focus was on retrieving, cleaning, processing and indexing Wikipedia material. This included the full texts of all Wikipedia articles from the publicly available XML dump, where redirects were removed and indexed only as synonyms to the titles of their target articles. Page view statistics for 100 days of Wikipedia traffic were randomly selected by a PRNG and compiled into a histogram to determine the popularity of the page, for use as a score. Two corporations of Wikipedia texts were created, one of which is intended to run the machine on underpowered machines or for those with slow Internet connections who could not download the full details. Other collections were also made available, including the full text of Wikipedia quotes and the works of Shakespeare indexed to match titles with works and match the character's line with their respective names."}, {"heading": "2.2 Online sources", "text": "Unlike IBM Watson, our team is not limited to offline data sources. In particular, we use web search engines such as Google and Bing. Google query limits made mass searches of the order of magnitude needed to retrieve the passage impractical, even if the results were cached and each team member used their daily quota continuously.At some point, a sufficient number of results would have been achieved if the queries had not continuously changed to improve the quality of the results. As a result, the Google search plugin is available, but is not used in performance measurements.The situation at Bing is similar, but less extreme. At the time of this writing, the automated search rate at Bing is 5000 per month, as opposed to Google's search rate of 100 per day. The longer interval at Bing was only enough to allow the team to measure two performance during the semester, but required less team coordination. As a result, performance measurements are available for Bing."}, {"heading": "3. DATA USAGE", "text": "We used the reduced and complete Wikipedia texts as well as the full text of WikiQuotes and Shakespeare full texts as soon as they became available. Wikipedia redirects were easy to integrate, but added a lot of generally unhelpful candidate responses. Although they added 6% binary reminders (from 22% to 28%), they caused a 30% drop in average reciprocal rank (from 0.6469 to 0.3483), which reduced overall accuracy by 6% at that point (from 12% to 6%)."}, {"heading": "4. TEXT ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Query Generation", "text": "There are two major question analysis pipelines, and another outdated pipeline. The selection of the pipeline that would follow a question was made according to its category, of which there was a small predefined quantity. The category was decided at the time the question was constructed by running a series of manually developed regular expressions, after which the standard pipeline would choose a more specific way of analyzing. The first and most common pipeline analyzes questions as factoids or general trivialities. Requests along this pipeline are weighted to favor the question text found in the document, but not the question text in the title. By default, all asking engines except Google are used in the initial phase of the factoid pipeline, and Bing and Lucene query engines are available for the later query phase. No configuration of the Indri search engine was found that was too slow to enable passage search."}, {"heading": "4.2 Supporting Passage Scoring", "text": "Supporting passages are given a number of ratings to guide how much evidence the passage provides for the accuracy of the answer it was retrieved for. Some ratings come from the original search engines; this applies to both offline search engines, Lucene and Indri. Lucene bases its rating on TermFrequency-Inverse Document Frequency (TF-IDF) and the Vector SpaceModel, while Indri ratings are based on Bayesian inference networks. [4] [2] The team expected these models to generate substantially different results, supported by the resulting accuracy. With Indri, binary memory increased from 13% with Lucene alone to 21% for both machines, as measured by the first 10 results from each technique. Online search engines do not provide results for their search results in publicly available APIs, and no one knows how their ratings are calculated."}, {"heading": "4.3 Pluggable Ranking", "text": "Watsonsim was originally designed for use with Mahout, and it was thought that specialized machine learning would be necessary. In early experiments, Weka was used, for which some integrating scripts were created. Later, it became a kind of its own, a learner, and eventually it was rewritten as a researcher. Under all these names, the function was the same, simply transforming a large vector of fixed values into a floating-point value that represented general confidence in the candidate's response. Up until the first passage of the proof for a candidate response, the machine learning plugin was fed with the entire vector of results, and the only pre-processing that needed to be done was to classify the results in a consistent manner. In this revision, it was postulated that there were too many variables that did not really represent different concepts; the results of the first passage of a proof for a candidate response are conceptually related to the results of the second pass, which indicated that the maximum number of points needed to be made from both to one."}, {"heading": "5. DEPENDENCIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6. PERFORMANCE EVALUATION", "text": "Performance tests are run with a JUnit test that automates queries to the standard pipeline, so everything needed to operate the standard pipeline is also needed to evaluate performance. You also need a way to perform JUnit tests. You can use the gradle test or find Eclipse to your liking."}, {"heading": "7. CURRENT PERFORMANCE", "text": "The performance tests for Watsonsim are created by taking a series of Jeopardy! questions with known correct answers, guiding them through the standard pipeline and collecting statistics on the resulting rankings of candidates, which were tabulated and uploaded online. The following data comes from the summit that Watsonsim reached just before the end of the spring semester of 2014. Candidates \"responses were marked as right or wrong based on whether the candidate and known correct answers contained the same disordered sets of words after being filtered by the Lucene EnglishAnalyzer. After all candidates\" answers were marked as right or wrong, the \"Recall of Rank 1\" was calculated as: Number of correct answers in the ranking 1 \"Recall of Top 3\" is similar, with correct answers in ranks 1, 2 or 3, indexing of 1 was indexed. The number of answers varied significantly; some questions had no answers, and there was a limit of 20 out of 20, and Lucing."}, {"heading": "8. NEXT STEPS", "text": "The answers to many questions are not included in Wikipedia, even if the search was perfect. However, it is possible to generate important related phrases in this process. Work on the prism is ongoing, but not complete. Simple questions can be dealt with directly in the corpusse."}, {"heading": "9. CONCLUSIONS", "text": "Watsonsim demonstrates that it is feasible for a small team to create a functioning question-and-answer system based on existing search technologies, online sources, natural language processing tools, and readily available machine learning tools within a specified timeframe. As with many such projects, much of the time is spent collecting and cleaning target data. Wikipedia's easy-to-analyze format, as well as the accessible APIs offered by Google and Microsoft, are partly responsible for reducing the time to the first demonstration. Extensive documentation from IBM Watson has been helpful in developing the entire pipeline and has had a major impact on the design of the project."}, {"heading": "10. ACKNOWLEDGMENTS", "text": "We would like to thank IBM for providing lecture material and the significant contributions of the following students: Chris Gibson, Dhaval Patel, Elliot Mersch, Jagan Vujjini, Jonathan Shuman, Ken Overholt, Phani Rahul, Varsha Devadas"}, {"heading": "11. REFERENCES", "text": "[1] C. Biemann. Jobimtext project.http: / / sourceforge.net / projects / jobimtext /, 2014. [2] D. Fisher. Indri retrieval model. http: / / sourceforge.net / p / lemur / wiki / Indri retrieval model /, 2012. [3] T. Mikolav. Word2vec project. https: / / code.google.com / p / word2vec /, 2014. [4] R. Muir. Lucene similarity class reference. http: / / lucene.apache.org / core / 4 9 0 / core / org / apache / lucene / search / similarities / TFIDFSimilarity.html, 2014."}], "references": [{"title": "Jobimtext project", "author": ["C. Biemann"], "venue": "http://sourceforge.net/projects/jobimtext/", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Indri retrieval model", "author": ["D. Fisher"], "venue": "http://sourceforge.net/p/lemur/wiki/Indri Retrieval Model/", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Word2vec project", "author": ["T. Mikolav"], "venue": "https://code.google.com/p/word2vec/", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Lucene similarity class reference", "author": ["R. Muir"], "venue": "http://lucene.apache.org/core/4 9 0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "[4] [2] The team expected that these models would generate substantially different results, which is supported by resulting accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[4] [2] The team expected that these models would generate substantially different results, which is supported by resulting accuracy.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "Several projects have previously explored knowledge management through distributional semantic models, such as JoBimText which may be an excellent candidate for inclusion [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 2, "context": "In such a vector space, \u201dking\u201d - \u201dman\u201d + \u201dwoman\u201d is a very close approximation to the vector for \u201dqueen\u201d [3].", "startOffset": 105, "endOffset": 108}], "year": 2014, "abstractText": "The objective of the project is to design and run a system to answer Jeopardy questions, similar to Watson. In the course of a semester, we developed an open source question answering system using the Indri, Lucene, Bing and Google search engines, Apache UIMA, OpenNLP, and Weka among many additional modules. By the end of the semester, we achieved 18% accuracy on Jeopardy questions, and work has not stopped since then.", "creator": "LaTeX with hyperref package"}}}