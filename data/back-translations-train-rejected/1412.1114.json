{"id": "1412.1114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2014", "title": "Easy Hyperparameter Search Using Optunity", "abstract": "Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at", "histories": [["v1", "Tue, 2 Dec 2014 21:55:44 GMT  (89kb,D)", "http://arxiv.org/abs/1412.1114v1", "5 pages, 1 figure"]], "COMMENTS": "5 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marc claesen", "jaak simm", "dusan popovic", "yves moreau", "bart de moor"], "accepted": false, "id": "1412.1114"}, "pdf": {"name": "1412.1114.pdf", "metadata": {"source": "CRF", "title": "Easy Hyperparameter Search Using Optunity", "authors": ["Marc Claesen", "Jaak Simm", "Dusan Popovic", "Yves Moreau", "Bart De Moor"], "emails": ["marc.claesen@esat.kuleuven.be", "jaak.simm@esat.kuleuven.be", "dusan.popovic@esat.kuleuven.be", "yves.moreau@esat.kuleuven.be", "bart.demoor@esat.kuleuven.be"], "sections": [{"heading": null, "text": "Keywords: hyperparameter search, black box optimization, algorithm tuning, Python"}, {"heading": "1. Introduction", "text": "Many machine learning tasks aim to train a model M that minimizes some loss functions L (M | X (te)) to predefined test data X (te). A model is obtained via a learning algorithm A that uses a training set X (tr) and solves some optimization problems. Learning algorithm A can itself be parameterized by a set of hyperparameters \u03bb, e.g. M = A (tr). Hyperparameter search - also known as tuning - aims to find a set of hyperparameters so that the learning algorithm produces an optimal modelM that minimizes L (M | X (te)."}, {"heading": "2. Optunity", "text": "Our software is a Swiss Army knife for searching for hyperparameters. Optunity offers a range of configurable optimization methods and utility functions that enable efficient optimization of hyperparameters. Only a handful of lines of code are needed to perform tuning. Optunity should be used in conjunction with existing machine learning packages that implement learning algorithms. It uses a BSD license and is easy to use in any environment. Optunity has been tested in Python, R and MATLAB on Linux, OSX, and Windows."}, {"heading": "2.1 Functional overview", "text": "Optunity provides both simple routines for users and expert routines that allow fine-grained control of various aspects of the solution process = variety of variants. < Basic tuning can be performed with minimal configuration, requiring only an objective function, a cap on the number of evaluations, and box constraints for the hyperparameters to be optimized. < The objective function must be defined by the user. (It takes a hyperparameter tupel and typically comprises three steps: (i) training a model M with \u03bb, (ii) using M to predict a test set (ii), calculating a score or loss based on the predictions. (SVIi), based on the predictions. For unattended tasks, the separation between (i) and (ii) is not necessary."}, {"heading": "2.2 Available solvers", "text": "Optunity offers a wide variety of solvers, ranging from basic, undirected methods such as grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and covariance matrix adaptation strategy (CMA-ES) (Hansen and Ostermeier, 2001). Finally, we offer the Nelder-Mead Simplex (Nelder and Mead, 1965), which is useful for local search after a good region has been determined. Optunity's current standard solver is particle swarm optimization, as our experiments have shown that it is well suited for a wide variety of tuning tasks with different learning algorithms."}, {"heading": "2.3 Software design and implementation", "text": "Unlike typical Python packages, we avoid dependencies on large packages such as NumPy / SciPy and scikit-learn to make it easier for users to work in non-Python environments (sometimes at the expense of performance), and to avoid problems for users unfamiliar with Python, care is taken to ensure that all of the code in Optunity works out-of-the-box on any version of Python above 2.7, without tools such as 2to3 having to perform explicit conversions. Optunity has a single dependency on DEAP (Fortin et al., 2012) for the CMA-ES solution, and a key aspect of the Optunity design is interoperability with external environments. This requires bi-directional communication between Optunity's Python backend (O) and the external environment (E), and involves roughly three steps: (i) E \u2192 O solution that involves large data transmission (configuration) and E-configuration (E) environments."}, {"heading": "2.4 Documentation", "text": "The code is documented with Sphinx and contains many doctests that can serve as both unit tests and examples of the functions associated with it. Our website contains API documentation, user documentation and a variety of examples illustrating all aspects of the software. Examples include various packages, including scikit-learn (Pedregosa et al., 2011), OpenCV (Bradski, 2000) and Spark's MLlib (Zaharia et al., 2010)."}, {"heading": "2.5 Collaborative and future development", "text": "Collaborative development is organized through GitHub.3 The master branch of the project remains stable and will undergo continuous integration testing using Travis CI. We encourage potential users to clone the master branch for the latest stable version of the software. Bug reports and feature requests can be filed about issues on GitHub. Future development efforts will focus on wrappers for Java, Julia, and C / C + +. This will make Optunity readily available in all major machine learning environments, and we plan to integrate Bayesian optimization strategies (Jones et al., 1998)."}, {"heading": "3. Related work", "text": "There are a number of software solutions for searching for hyperparameters. HyperOpt offers random search and sequential model-based optimization (Bergstra et al., 2013). Packages dedicated to Bayean approaches include Spearmint (Snoek et al., 2012), DiceKriging (Roustant et al., 2012) and BayesOpt (Martinez-Cantin, 2014). Finally, ParamILS is a command-line-only tuning framework that provides iterated local search (Hutter et al., 2009). Optunity differs from existing packages in that it discloses a variety of fundamentally different solutions. This is important because the theorem of the free lunch suggests that no single approach is best in all environments (Wolpert and Macready, 1997). Furthermore, Optunity PI can be easily integrated into different environments and has a very simple API."}, {"heading": "Acknowledgments", "text": "This research was funded through the following channels: \u2022 KU Leuven Research Council: GOA / 10 / 09 MaNet, CoE PFV / 10 / 016 SymBioSys; \u2022 Flemish Government: FWO: projects: G.0871.12N (Neural circuits); IWT: TBMLogic Insulin (100793), TBM Rectal Cancer (100783), TBM IETA (130256), O & O ExaScience Life Pharma, ChemBioBridge, PhD grants (specifically 111065); Industrial Research Fund (IOF): IOF / HB / 13 / 027 Logic Insulin; iMinds Medical Information Technologies SBO 2014; VLK Stichting E. van der Schueren: Rectal Cancer \u2022 Federal Government: FOD: Cancer Plan 2012-2015 KPC-29-023 (prostate) \u2022 COST: Action: BM1104: Mass Spectrometry Imaging"}], "references": [{"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms", "author": ["James Bergstra", "Dan Yamins", "David D Cox"], "venue": "In Proceedings of the 12th Python in Science Conference,", "citeRegEx": "Bergstra et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2013}, {"title": "The OpenCV library", "author": ["G. Bradski"], "venue": "Dr. Dobb\u2019s Journal of Software Tools,", "citeRegEx": "Bradski.,? \\Q2000\\E", "shortCiteRegEx": "Bradski.", "year": 2000}, {"title": "DEAP: Evolutionary algorithms made easy", "author": ["F\u00e9lix-Antoine Fortin", "De Rainville", "Marc-Andr\u00e9 Gardner Gardner", "Marc Parizeau", "Christian Gagn\u00e9"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fortin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fortin et al\\.", "year": 2012}, {"title": "Completely derandomized self-adaptation in evolution strategies", "author": ["Nikolaus Hansen", "Andreas Ostermeier"], "venue": "Evolutionary computation,", "citeRegEx": "Hansen and Ostermeier.,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Ostermeier.", "year": 2001}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["Geoffrey E Hinton"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "A practical guide to support vector classification", "author": ["Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2003}, {"title": "ParamILS: an automatic algorithm configuration framework", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown", "Thomas St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hutter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2009}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["Donald R Jones", "Matthias Schonlau", "William J Welch"], "venue": "Journal of Global optimization,", "citeRegEx": "Jones et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1998}, {"title": "Particle swarm optimization", "author": ["James Kennedy"], "venue": "In Encyclopedia of Machine Learning,", "citeRegEx": "Kennedy.,? \\Q2010\\E", "shortCiteRegEx": "Kennedy.", "year": 2010}, {"title": "BayesOpt: A Bayesian optimization library for nonlinear optimization, experimental design and bandits", "author": ["Ruben Martinez-Cantin"], "venue": "arXiv preprint arXiv:1405.7430,", "citeRegEx": "Martinez.Cantin.,? \\Q2014\\E", "shortCiteRegEx": "Martinez.Cantin.", "year": 2014}, {"title": "A simplex method for function minimization", "author": ["John A Nelder", "Roger Mead"], "venue": "The computer journal,", "citeRegEx": "Nelder and Mead.,? \\Q1965\\E", "shortCiteRegEx": "Nelder and Mead.", "year": 1965}, {"title": "Scikit-learn: Machine learning in Python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "DiceKriging, DiceOptim: Two R packages for the analysis of computer experiments by kriging-based metamodeling and optimization", "author": ["Olivier Roustant", "David Ginsbourger", "Yves Deville"], "venue": null, "citeRegEx": "Roustant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roustant et al\\.", "year": 2012}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "No free lunch theorems for optimization", "author": ["David H Wolpert", "William G Macready"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "Wolpert and Macready.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert and Macready.", "year": 1997}, {"title": "Spark: cluster computing with working sets", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Michael J Franklin", "Scott Shenker", "Ion Stoica"], "venue": "In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing,", "citeRegEx": "Zaharia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "The most common tuning approaches are grid search and manual tuning (Hsu et al., 2003; Hinton, 2012).", "startOffset": 68, "endOffset": 100}, {"referenceID": 5, "context": "The most common tuning approaches are grid search and manual tuning (Hsu et al., 2003; Hinton, 2012).", "startOffset": 68, "endOffset": 100}, {"referenceID": 0, "context": "2 Available solvers Optunity provides a wide variety of solvers, ranging from basic, undirected methods like grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and the covariance matrix adaptation evolutionary strategy (CMA-ES) (Hansen and Ostermeier, 2001).", "startOffset": 139, "endOffset": 166}, {"referenceID": 9, "context": "2 Available solvers Optunity provides a wide variety of solvers, ranging from basic, undirected methods like grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and the covariance matrix adaptation evolutionary strategy (CMA-ES) (Hansen and Ostermeier, 2001).", "startOffset": 227, "endOffset": 242}, {"referenceID": 4, "context": "2 Available solvers Optunity provides a wide variety of solvers, ranging from basic, undirected methods like grid search and random search (Bergstra and Bengio, 2012) to evolutionary methods such as particle swarm optimization (Kennedy, 2010) and the covariance matrix adaptation evolutionary strategy (CMA-ES) (Hansen and Ostermeier, 2001).", "startOffset": 311, "endOffset": 340}, {"referenceID": 11, "context": "Finally, we provide the Nelder-Mead simplex (Nelder and Mead, 1965), which is useful for local search after a good region has been determined.", "startOffset": 44, "endOffset": 67}, {"referenceID": 3, "context": "Optunity has a single dependency on DEAP (Fortin et al., 2012) for the CMA-ES solver.", "startOffset": 41, "endOffset": 62}, {"referenceID": 12, "context": "The examples involve various packages, including scikit-learn (Pedregosa et al., 2011), OpenCV (Bradski, 2000) and Spark\u2019s MLlib (Zaharia et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 2, "context": ", 2011), OpenCV (Bradski, 2000) and Spark\u2019s MLlib (Zaharia et al.", "startOffset": 16, "endOffset": 31}, {"referenceID": 16, "context": ", 2011), OpenCV (Bradski, 2000) and Spark\u2019s MLlib (Zaharia et al., 2010).", "startOffset": 50, "endOffset": 72}, {"referenceID": 8, "context": "We additionally plan to incorporate Bayesian optimization strategies (Jones et al., 1998).", "startOffset": 69, "endOffset": 89}, {"referenceID": 1, "context": "HyperOpt offers random search and sequential model-based optimization (Bergstra et al., 2013).", "startOffset": 70, "endOffset": 93}, {"referenceID": 14, "context": "Some packages dedicated to Bayesian approaches include Spearmint (Snoek et al., 2012), DiceKriging (Roustant et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 13, "context": ", 2012), DiceKriging (Roustant et al., 2012) and BayesOpt (Martinez-Cantin, 2014).", "startOffset": 21, "endOffset": 44}, {"referenceID": 10, "context": ", 2012) and BayesOpt (Martinez-Cantin, 2014).", "startOffset": 21, "endOffset": 44}, {"referenceID": 7, "context": "Finally, ParamILS is a command-lineonly tuning framework providing iterated local search (Hutter et al., 2009).", "startOffset": 89, "endOffset": 110}, {"referenceID": 15, "context": "This matters because the no free lunch theorem suggests that no single approach is best in all settings (Wolpert and Macready, 1997).", "startOffset": 104, "endOffset": 132}], "year": 2014, "abstractText": "Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net.", "creator": "LaTeX with hyperref package"}}}