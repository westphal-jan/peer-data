{"id": "1412.1454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2014", "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation", "abstract": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM $n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as $n$-gram LMs do.", "histories": [["v1", "Wed, 3 Dec 2014 19:42:12 GMT  (13kb)", "http://arxiv.org/abs/1412.1454v1", null], ["v2", "Fri, 26 Jun 2015 20:35:52 GMT  (13kb)", "http://arxiv.org/abs/1412.1454v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["noam shazeer", "joris pelemans", "ciprian chelba"], "accepted": false, "id": "1412.1454"}, "pdf": {"name": "1412.1454.pdf", "metadata": {"source": "CRF", "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation", "authors": ["Noam Shazeer", "Joris Pelemans", "Ciprian Chelba"], "emails": ["noam@google.com", "jpeleman@google.com", "ciprianchelba@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.14 54v1 [cs.LA first series of experiments evaluating it empirically on the One Billion Word benchmark [Chelba et al., 2013] shows that SNM n-gram LMs are almost as powerful as the established Kneser-Ney (KN) models. When using Skip-gram features, the models are able to match the state-of-the-art recursive neural network (RNN) LMs; the combination of the two modeling techniques provides the best-known result on the benchmark. The computational advantages of SNM over maximum entropy and RNN-LM estimation are probably its greatest strength and promise an approach that offers the same flexibility in effectively combining arbitrary features, yet should scale to very large amounts of data as gracefully as N-gram LMs."}, {"heading": "1 Introduction", "text": "A statistical language model estimates the previous probability values P (W) for words W in a vocabulary V, the size of which is in the tens, hundreds, and sometimes even millions. Typically, the string W \u2212 \u2212 is divided into sentences or other segments such as expressions in automatic speech recognition, which are considered conditionally independent; we assume that W is such a segment or sequence. (Estimating complete language models is mathematically difficult if one observes a properly normalized word sequence of the limited word length in 1We note that the restriction on the use of a properly normalized language model is 1.) A simple and sufficient way to ensure proper normalization of the model is that the sentence probability is composed according to the chain rule and ensure that the end of the sentence symbol is predicted in any context."}, {"heading": "2 Skip-gram Language Modeling", "text": "Recently, the neural network (NN) has flattened out [Bengio et al., 2003], [Emami et al., 2006], [Schwenk, 2007] and in particular the recurring neural networks [Mikolov, 2012] (RNN) have shown excellent performance in speech modeling [Chelba et al., 2013]. Their excellent performance is attributed to a combination of the use of long-range contexts and the formation of a vector representation for words. Another simple way of using long-range contexts is the use of skip programs. In our approach, a skip-gram feature from the context Wk \u2212 1 is characterized by the tupel (r, s, a) in which the number of remote context words is densed \u2022 s denotes the number of skipped words \u2022 a denotes the number of adjacent context words relative to the target word wk."}, {"heading": "3 Sparse Non-negative Matrix Modeling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model definition", "text": "In the paradigm of sparse non-negative matrix (SNM), we represent the training data as a sequence of events E = e1, e2,..., where each event consists of a sparse non-negative feature vector f and a sparse non-negative target word vector t. Both vectors are evaluated binary, where Pos (f) indicates the number of positive elements in vector f. A language model is represented by a non-negative matrix M, which, when applied to a given feature vector f, creates a dense prediction vector y: y = Mf \u2248 t (4). After the evaluation, we normalize y so that we end up with a conditional probability distribution PM (t | f) for the word PM defined in this index, a dense prediction vector y: y = Mf \u2248 t (4)."}, {"heading": "3.2 Adjustment function and metafeatures", "text": "We let the input of M be a slightly modified version of the relative frequencies: Mij = e A (i, j) CijCi \u0445 (6), where C is a feature-target-counting matrix calculated over the entire training corpus and A (i, j) is a really evaluated function called an adjustment function. To limit memory usage, we use a flat hash table and allow collisions, although this has the potentially undesirable effect of merging the weights of different metafeatures. Calculation of the adjustment function for all (fi, tj) functions then boils down to summing the weights corresponding to their metafeatures."}, {"heading": "3.3 Loss function", "text": "The estimation of a model M corresponds to the determination of optimal weights \u03b8k for all metafeatures for all events in such a way that the average loss over all events between the target vector t and the predictive vector y is minimized, corresponding to a loss function L. The most natural choice of the loss function is one based on the multinomial distribution, that is, we consider t as multinomically distributed with | V | possible results. The loss function Lmulti is then: Lmulti (y, t) = \u2212 log (Pmulti (t | f))) = \u2212 log (yj \u2211 | V | u = 1 yu) = log (| V | \u2211 u = 1yu) \u2212 log (yj) (8) Another possibility is the loss function based on the Poisson distribution2: we consider each tj in t distributed as Poisson with the parameter yj. \u2212 The conditional probability of PPoisson (t | f) \u2212 ttson (t | f) is obvious, although this loss function (Poisson) is not suitable (Poisson)."}, {"heading": "3.4 Model Estimation", "text": "We learn the fit function by applying stochastic gradients to the loss function, that is, for each feature-target pair (fi, tj) we have to update the parameters of the metafeatures by calculating the gradient in relation to the fit function. However, for multinomial examples, this gradient is as follows (Lmulti (Mf, t))) byzantium (A (i, j) = byzantium (V | u = 1 (Mf) u = 1 (Mij) -Log (Mij) -examples (Mij) - examples (Mij) - (Mij) - examples (Mij) - (Mij) - examples (Mij) - examples (Mij) - examples (Mij) - examples (Mij) - examples (Mij) - examples (Mij) - examples (Mj) - examples (Mj) - (Mij) - examples (Mj) - (Mij) - examples (Mij) - (Mij) - examples (Mij) - (Mij) - examples (Mij) (Mij) - examples (Mij) - (Mij) - examples (Mij) - (Mij) - examples (Mij) - points (Mij - examples (Mj) - examples (Mj) (Mj) - examples (Mj) - examples (Mj - examples (Mj) - (Mj) - examples (Mj) (Mij - examples (Mj) - points (Mij - examples (Mj) - examples (Mij) - (Mj) - examples (Mij - points (Mij) - examples (Mij - (Mij) - examples (Mij) - points (Mij - (Mij) - examples (Mij) - examples (Mij) - points (Mij) (Mij - (Mij)"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Corpus: One Billion Benchmark", "text": "Our experimental setup used the One Billion Word Benchmark corpus4 provided by [Chelba et al., 2013]. For completeness, here is a brief description of the corpus containing only monolingual English data: \u2022 The total number of training marks is approximately 0.8 billion \u2022 The vocabulary provided consists of 793,471 words, including sentence boundary markers < S >, <\\ S >, and was created by discarding all words with a number below 34http: / / www.statmt.org / lm-benchmark \u2022 Words outside the vocabulary were assigned < UNK >, including part of the vocabulary \u2022 Sentence order was randomized \u2022 The test data consisted of 15,9658 words (excluding the sentence beginning < S >, which is never predicted by the language model) \u2022 The out-of-of-vocabulary rate (OoV) in the test was 0.28%."}, {"heading": "4.2 SNM for n-gram LMs", "text": "In fact, the fact is that most of them are able to put themselves at the top of society, in the way that they are able to put themselves at the top of the society in which they find themselves."}, {"heading": "4.4 Ablation Experiments", "text": "To find out how much, if any, each metafeature contributes to the adjustment function, we conducted a series of ablation experiments in which we each collected one metafeature. When we experimented with SNM5, we unsurprisingly found that the most important metafeature is the number of features. At first glance, it does not seem to matter much whether the number of features is stored in 1 or 2 buckets, but the second bucket really begins to pay off for models with a large number of singleton features, such as SNM10-skip5. This is not the case for the number of features where a single bucket is always better, although the number of features generally does not add much. In any case, the features are the least important for the model. The remaining metafeatures all contribute more or less evenly to what can be seen in Table 4."}, {"heading": "5 Related Work", "text": "The SNM estimation is closely related to all n-gram-LM smoothing techniques, which are based on mixing relative frequencies in different orders. Unlike most of these techniques, it combines the predictors in different orders without relying on hierarchical nesting of contexts, bringing them closer to the family of maximum entropy (ME) [Rosenfeld, 1994], 5 Ideally, we would also like to have SNM10 ablation results, but this takes a lot of time while other developments are hampered. We are not the first to highlight the effectiveness of skip n-programs in capturing dependencies in longer contexts, similar to RNN-LMs; previous such results have been reported in [Singh and Klakow, 2013]."}, {"heading": "6 Conclusions and Future Work", "text": "An initial empirical evaluation of the One Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs are almost as powerful as the established KN models. By using Skip-gram features, the models can keep up with the statistical RNN LMs; the combination of the two modeling techniques provides the best-known result on the benchmark. Future work will include model cropping, exploration of richer similar features such as [Goodman, 2001a], and richer metafeatures in the adaptation model, mixing SNM models trained on different data sources to work best on a given development set, and evaluation techniques that are more flexible in this respect."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al", "2003] Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Class-Based N-gram Models of Natural Language", "author": ["Brown et al", "1992] P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J. Della Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}, {"title": "Improved Backing-Off For M-Gram Language Modeling", "author": ["Kneser", "Ney", "1995] R. Kneser", "H. Ney"], "venue": "In Proceedings of ICASSP", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio", "2005] F. Morin", "Y. Bengio"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["Sundermeyer et al", "2012] M. Sundermeyer", "R. Schluter", "H. Ney"], "venue": "In Proceedings of Interspeech", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "author": ["Chelba et al", "2013] Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "Google Tech Report 41880", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Comparing RNNs and log-linear interpolation of improved skip-model on four Babel languages: Cantonese, Pashto, Tagalog, Turkish", "author": ["Singh", "Klakow", "2013] M. Singh", "D. Klakow"], "venue": "Proceedings of ICASSP", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Efficient Subsampling for Training Complex Language Models", "author": ["Xu et al", "2011] Puyang Xu", "A. Gunawardana", "S. Khudanpur"], "venue": "In Proceedings of EMNLP", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}], "referenceMentions": [], "year": 2017, "abstractText": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark [Chelba et al., 2013] shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.", "creator": "LaTeX with hyperref package"}}}