{"id": "1412.1619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2014", "title": "Fast Rates by Transferring from Auxiliary Hypotheses", "abstract": "In this work we consider the learning setting where in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. This paradigm, known as Hypothesis Transfer Learning (HTL), has been successfully exploited in empirical works, but only recently has received a theoretical attention. Here, we try to understand when HTL facilitates accelerated generalization -- the goal of the transfer learning paradigm. Thus, we study a broad class of algorithms, a Hypothesis Transfer Learning through Regularized ERM, that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that if the algorithm is fed with a good source hypotheses combination, generalization happens at the fast rate $\\mathcal{O}(1/m)$ instead of usual $\\mathcal{O}(1/\\sqrt{m})$. We also observe that if the combination is perfect, our theory formally backs up the intuition that learning is not necessary. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.", "histories": [["v1", "Thu, 4 Dec 2014 11:01:11 GMT  (44kb,D)", "http://arxiv.org/abs/1412.1619v1", null], ["v2", "Fri, 6 Feb 2015 10:32:37 GMT  (37kb,D)", "http://arxiv.org/abs/1412.1619v2", null], ["v3", "Fri, 17 Jul 2015 17:34:49 GMT  (39kb,D)", "http://arxiv.org/abs/1412.1619v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilja kuzborskij", "francesco orabona"], "accepted": false, "id": "1412.1619"}, "pdf": {"name": "1412.1619.pdf", "metadata": {"source": "CRF", "title": "Learning by Transferring from Auxiliary Hypotheses", "authors": ["Ilja Kuzborskij"], "emails": ["ilja.kuzborskij@idiap.ch", "francesco@orabona.com"], "sections": [{"heading": null, "text": "We also observe that when the combination is perfect, our theory formally supports the intuition that learning is not necessary. On the other hand, when the combination of the starting hypotheses is inappropriate for the target task, we regain the usual learning rate. As a by-product of our study, we also exhibit a new limit on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous work."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}, {"heading": "2 Related Work", "text": "The framework of the Transfer Learning (HTL) hypothesis that we address in this paper was first formally introduced and theoretically examined by Kuzborsky and Orabona [2013b]. It was shown that the generalization capability of the regulated read squares HTL algorithm improves when the source hypothesis provided is well targeted at the target task. Specifically, a key criterion was proposed that limits the risk of source usage to the target domain, which captures the relativity of the source and target domains. Later, Ben-David and Urner [2013] demonstrated a similarly bound but with different source criteria. Instead of considering the general source hypothesis, they limited their linear hypothesis to the class, allowing them to better generalize the target hypothesis when it comes close to the good source hypothesis. From this perspective, it is easy to interpret the source hypothesis as an initial theory."}, {"heading": "3 Definitions", "text": "In this section we present the definitions used in the rest of the paper. We refer to random variables by capital letters. The expected value of a random variable distributed according to a probability distribution is denoted by EX-D [X], and the variance is denoted by VarX-D [X]. The small and large bold letters respectively represent the vectors and matrices, e.g. x-x-x-x,. \u2212 xd] > and A-x-Rd1 \u00b7 d2. Denotation by X and Y or the input and output space of the learning problem, the training set is S = {(xi, yi)} mi = 1, drawn i.i.d. from the probability distribution D over X \u00b7 Y. Without the loss of generality, we have X = {x: x-x-x-x-projected function \u2264 1} and we will focus on the problems where Y = [\u2212 C, C].To measure the accuracy of a learning algorithm, we will not perform a loss algorithm."}, {"heading": "4 Transferring from Auxiliary Hypotheses", "text": "In the following, we will attempt to grasp and generalize the transmission problems, which include a collection of hypotheses as prior knowledge resulting from the different tasks. (These problems typically include some criteria for selecting the source hypotheses and combining them with the goal of increasing performance on the target task.) In fact, some source hypotheses will be based on tasks similar to the target task and the goal of a transfer learning algorithm in order to select only relevant ones. In this work, we will focus on the formulation of such transfer learning problems. As an example, we will consider a general transfer learning problem: the least quadratic transmissions with biased regulation. (2001). Least-squares with biased regularization. (In view of the target training amount S = {xi, yi} mi = 1, source hypotheses {wsrci} ni = 1). Least-squares with biased regularization."}, {"heading": "5 Main Results", "text": "In this section, we will present the main results of this work: generalization and excess risk limits for the transfer by regulated ERM = 1) hypothesis. In the next section, we will discuss in detail the implications of these results as we move the results to the following sections: the first sequence demonstrates the usefulness of the perfect combination of source hypotheses, while the second sequence allows us to observe the dependence on the arbitrary combination. In other words, whenRsrc = 0 We have the empirical risk that empirical risk becomes equal to probability one. Furthermore, we will present the second sequence, which implies a rapid rate of empirical risk convergence subject to the quality of the source hypotheses."}, {"heading": "5.1 Implications", "text": "We begin by discussing the effects of the source hypothesis on the target task, a good and a perfect balance. We begin by discussing the effects of the source hypothesis on the generalization capability of the HTL algorithms. (1) We assume that the source hypothesis for the target domain will be a good or bad source hypothesis. (2) In the previous sections of the theorems we have presented in the previous sections, it is presented by Rsrc, which is the risk of the source hypothesis combination on the target domain. (2) In this section, we will consider various rules of interest relating to Rsrc. (2) We will look at scenarios if it is a bad source hypothesis for the target domain."}, {"heading": "5.2 Comparison to Theories of Domain Adaptation and Transfer Learning", "text": "The attitude in DA differs from that we know of, but we will briefly discuss the theoretical relationship between the two domains. Typically, DA puts forward a hypothesis of a modified source training group trying to achieve good performance on the target domain. Here, the key question is how the domain relatedness can be estimated in some cases. To answer this question, the DA literature introduces the term domain relatedness from both source and target domains, which quantifies the differences between the distributions of the respective domains. DA theories [Ben-David et al., 2009, Ben-David and Urner, 2012, Mansour et al., 2008, Cortes and Mohri, 2014] have suggested a number of such domain relatedness criteria."}, {"heading": "6 Technical Results and Proofs", "text": "In this section, we will present general technical results used to prove our theories. First, we will present the Rademacher complexity generalization according to Theorem 7, which differs slightly from the usual one, based on the assumption that the variance of loss is uniformly limited across the hypotheses class, which allows us to define a generalization limit subject to the rapid empirical convergence rate subject to the low class complexity. Second, we will also focus on the Rademacher complexity of the smooth loss function class, which disappears when the complexity of the class is exactly zero. In other words, for the class without complexity, empirical risk is equated with risk with probability."}, {"heading": "6.1 Fast Rate Generalization Bound", "text": "The proof of the rapid rate and disappearance of the confidence limit, Theorem 7, results from the functional generalization of Bennett's inequality, which is due to Bousquet (2002, Theorem 2.11) and which we lead here to completeness. Theorem 4 (Bousquet [2002]) Let X1, X2,., Xm be idently distributed random variables for D-measurable, square-integrable g-G, with EX [g)] = 0, and supg G sup g G sup g g g \u2264 1, we denoteZ = sup g G m \u00b2, i = 1 g (Xi).Let us have a positive real number such that supg G VarX D [g)] \u2264 2, and sup G sup G sup G, and sup G G G, and p G G G G G, and p G G G G G S, and p G G G G G G P (Z E)."}, {"heading": "6.2 Rademacher Complexity of Smooth Loss Class", "text": "In this section, we will examine the Rademacher complexity of the hypothesis class L = (x) populated by functions of form (2), with the parameters w and \u03b2 being chosen by an algorithm with strongly convex regulators (2) (2). To this end, we will use the results of Kakade et al. (2008, 2012), which have studied strongly convex regulators in a more general environment. Furthermore, we will focus on the use of smooth loss functions associated with limited second derivatives, such as those of Srebro et al. (2010a). Evidence of the main result of this section, Theorem 10, essentially depends on the following definition, which describes the empirical wheel-maker complexity of an H-smooth loss class.Lemma 8 Let ': Y \u00d7 Y 7 \u2192 R + be the H-smooth loss function."}, {"heading": "6.3 Proofs of Main Results", "text": "To apply Theorem 7, we must obtain the complexity of the loss class L and also the quantity r = supf L E (x, y) [f (x, y) [f (x, y)]. We obtain the bound complexity by applying Theorem 10. First, we must define the loss class L: = (x, y): \"(h, y):\" (h, y): \"(h)\" (h), \"h,\" h, \"\" h, \"\" h, \"\" h, \"\" h, \"\" h, \"\" h, \"\" h, \"\" h, \"\" h, \"h,\" h, \"h,\" h, \"h,\" h, \"h,\" h, \"h,\" h, \"h,\" h, \"h."}, {"heading": "7 Conclusions", "text": "In this paper, we have formally identified and theoretically analyzed a general family of transfer learning algorithms applied by multiple supplied source hypotheses, the Transfer Learning through Regularized ERM hypothesis. In particular, our formulation stems from the principle of regulated empirical risk minimization with the choice of any non-negative frictionless loss function and a strongly convex regularizer. In theory, we have analyzed the generalization capability and excess risk of this family of HTL algorithms. Our analysis showed that a good source hypothesis combination enables faster generalization, especially in O (1 / m) instead of the usual O (1 / m). Furthermore, given a perfect source hypothesis combination, our analysis is consistent with the intuition that learning is not necessary. As a by-product of our investigation, we have found new results in Rademacher's smooth loss class complexity analysis, which may be of independent interests, the hypotheses are not directly available to us. Our conclusions on the huge importance of hypotheses in a hypothesis that are directly available."}, {"heading": "A Additional Proofs", "text": "Theorem 11 Let hw, \u03b2 be generated by HTL through Regularized ERM = \u03b2 > risk, given the m-sized training set S sampled i.\u03b2 d from the target domain, source hypotheses {hsrci} ni = 1, any source weights \u03b2 (\u03b2) \u2264. \"Suppose that\" (hw), \u03b2 \"(x), y.\" (hw). (hm). (hsrcm). (hsrcm). (hsrcm). (.). (.). (.). (.). (hw). (hw). (hw). (hw). (hw). (hw). (.). (hw). (.). (.). (.). (.). (...... (.). (.). (. (......). (.). (. (.). (.). (. (......). (.). (. (.). (.). (. (......). (.). (. (.). (.). (. (.). (.). (. (......).). (.). (.). (. (.). (.). (. (.). (. (......).). (. (.). (.). (. (.).). (. (.). (. (. (......). (.). (.). (. (.). (......). (.). (.).). (. (. (......). (. (.). (.). (......). (. (. (.). (.). (.).)...... (. (. (.). (.). (. (.). (......). (. (.). (.)....... (.). (. (. (.). (.).). (... (. (.). (.). (.).."}], "references": [{"title": "Tabula rasa: Model transfer for object category detection", "author": ["Y. Aytar", "A. Zisserman"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Aytar and Zisserman.,? \\Q2011\\E", "shortCiteRegEx": "Aytar and Zisserman.", "year": 2011}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2003}, {"title": "Local rademacher complexities", "author": ["P.L. Bartlett", "O. Bousquet", "S. Mendelson"], "venue": "Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "On the hardness of domain adaptation and the utility of unlabeled target samples", "author": ["S. Ben-David", "R. Urner"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Ben.David and Urner.,? \\Q2012\\E", "shortCiteRegEx": "Ben.David and Urner.", "year": 2012}, {"title": "Domain adaptation as learning with auxiliary information", "author": ["S. Ben-David", "R. Urner"], "venue": "In New Directions in Transfer and Multi-Task - Workshop @ Advances in Neural Information Processing Systems,", "citeRegEx": "Ben.David and Urner.,? \\Q2013\\E", "shortCiteRegEx": "Ben.David and Urner.", "year": 2013}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Classemes and other classifier-based features for efficient object categorization", "author": ["A. Bergamo", "L. Torresani"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bergamo and Torresani.,? \\Q2014\\E", "shortCiteRegEx": "Bergamo and Torresani.", "year": 2014}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Conference on Computational learning theory,", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Concentration inequalities and empirical processes theory applied to the analysis of learning algorithms", "author": ["O. Bousquet"], "venue": "PhD thesis, Ecole Polytechnique,", "citeRegEx": "Bousquet.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet.", "year": 2002}, {"title": "Stability and Generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff.", "year": 2002}, {"title": "Domain adaptation and sample bias correction theory and algorithm for regression", "author": ["C. Cortes", "M. Mohri"], "venue": "Theoretical Computer Science,", "citeRegEx": "Cortes and Mohri.,? \\Q2014\\E", "shortCiteRegEx": "Cortes and Mohri.", "year": 2014}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daum\u00e9 III"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "III.,? \\Q2007\\E", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Frustratingly easy semi-supervised domain adaptation", "author": ["H. Daum\u00e9 III", "A. Kumar", "A. Saha"], "venue": "In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,", "citeRegEx": "III et al\\.,? \\Q2010\\E", "shortCiteRegEx": "III et al\\.", "year": 2010}, {"title": "Domain adaptation from multiple sources via auxiliary classifiers", "author": ["L. Duan", "I.W. Tsang", "D. Xu", "T.-S. Chua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Duan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2009}, {"title": "One-shot adaptation of supervised deep convolutional models", "author": ["J. Hoffman", "E. Tzeng", "J. Donahue", "Y. Jia", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1312.6204,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Inequalities on the lambert w function and hyperpower function", "author": ["A. Hoorfar", "M. Hassani"], "venue": "J. Inequal. Pure and Appl. Math,", "citeRegEx": "Hoorfar and Hassani.,? \\Q2008\\E", "shortCiteRegEx": "Hoorfar and Hassani.", "year": 2008}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Regularization techniques for learning with matrices", "author": ["S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kakade et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2012}, {"title": "Personalized handwriting recognition via biased regularization", "author": ["W. Kienzle", "K. Chellapilla"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Kienzle and Chellapilla.,? \\Q2006\\E", "shortCiteRegEx": "Kienzle and Chellapilla.", "year": 2006}, {"title": "Correction to \u201dStability and Hypothesis Transfer Learning", "author": ["I. Kuzborskij", "F. Orabona"], "venue": null, "citeRegEx": "Kuzborskij and Orabona.,? \\Q2013\\E", "shortCiteRegEx": "Kuzborskij and Orabona.", "year": 2013}, {"title": "Stability and Hypothesis Transfer Learning", "author": ["I. Kuzborskij", "F. Orabona"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Kuzborskij and Orabona.,? \\Q2013\\E", "shortCiteRegEx": "Kuzborskij and Orabona.", "year": 2013}, {"title": "From N to N+1: multiclass transfer incremental learning", "author": ["I. Kuzborskij", "F. Orabona", "B. Caputo"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kuzborskij et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kuzborskij et al\\.", "year": 2013}, {"title": "Object bank: A high-level image representation for scene classification & semantic feature sparsification", "author": ["L.-J. Li", "H. Su", "E.P. Xing", "L. Fei-Fei"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "A bayesian divergence prior for classiffier adaptation", "author": ["X. Li", "J. Bilmes"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Li and Bilmes.,? \\Q2007\\E", "shortCiteRegEx": "Li and Bilmes.", "year": 2007}, {"title": "Multiclass transfer learning from unconstrained priors", "author": ["J. Luo", "T. Tommasi", "B. Caputo"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Luo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2011}, {"title": "Domain adaptation with multiple sources", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mansour et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2008}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In The Conference on Learning Theory,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Foundations of machine learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": null, "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "Model Adaptation with Least-Squares SVM for Adaptive Hand Prosthetics", "author": ["F. Orabona", "C. Castellini", "B. Caputo", "A.E. Fiorilla", "G. Sandini"], "venue": "In Robotics and Automation, IEEE International Conference on,", "citeRegEx": "Orabona et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "In Conference on Computational learning theory,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Smoothness, low noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Optimistic rates for learning with a smooth loss", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "eprint arXiv:1009.3896,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Transfer leraning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Safety in numbers: Learning categories from few examples with multi model knowledge transfer", "author": ["T. Tommasi", "F. Orabona", "B. Caputo"], "venue": "In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Tommasi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tommasi et al\\.", "year": 2010}, {"title": "Improving control of dexterous hand prostheses using adaptive learning", "author": ["T. Tommasi", "F. Orabona", "C. Castellini", "B. Caputo"], "venue": "Robotics, IEEE Transactions on,", "citeRegEx": "Tommasi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tommasi et al\\.", "year": 2013}, {"title": "Learning categories from few examples with multi model knowledge transfer", "author": ["T. Tommasi", "F. Orabona", "B. Caputo"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Tommasi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tommasi et al\\.", "year": 2014}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["V. Vapnik", "A. Vashist"], "venue": "Neural Networks,", "citeRegEx": "Vapnik and Vashist.,? \\Q2009\\E", "shortCiteRegEx": "Vapnik and Vashist.", "year": 2009}, {"title": "Model selection for regularized least-squares algorithm in learning theory", "author": ["E. De Vito", "A. Caponnetto", "L. Rosasco"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Vito et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vito et al\\.", "year": 2005}, {"title": "Cross-Domain Video Concept Detection Using Adaptive SVMs", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "In Proceedings of the 15th international conference on Multimedia,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}, {"title": "In order to apply it, we have to obtain an upper bound on the Rademacher complexity of the loss class L that is a sub-root function [Bousquet", "author": ["Bartlett"], "venue": "Definition", "citeRegEx": "Bartlett,? \\Q2002\\E", "shortCiteRegEx": "Bartlett", "year": 2002}, {"title": "In the first term we set fmax = Rsrc and in the second fmax = \u03c1. Now define function \u03c6m(r) = L", "author": ["Kakade"], "venue": null, "citeRegEx": "Kakade,? \\Q2012\\E", "shortCiteRegEx": "Kakade", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Among others, there is the use of structural information, such as taxonomy, or different views on the same data [Blum and Mitchell, 1998], or even a sort of privileged information [Vapnik and Vashist, 2009].", "startOffset": 112, "endOffset": 137}, {"referenceID": 39, "context": "Among others, there is the use of structural information, such as taxonomy, or different views on the same data [Blum and Mitchell, 1998], or even a sort of privileged information [Vapnik and Vashist, 2009].", "startOffset": 180, "endOffset": 206}, {"referenceID": 5, "context": "For example, in domain adaptation [Ben-David et al., 2010], one employs large unlabeled samples to estimate the relatedness of source and target domains to perform adaptation.", "startOffset": 34, "endOffset": 58}, {"referenceID": 18, "context": "A first formalization and theoretical treatment of transfer learning under the use of source hypothesis was given by Kuzborskij and Orabona [2013b] in the Hypothesis Transfer Learning (HTL) framework, albeit empirically it has already been extensively exploited in the past [Yang et al.", "startOffset": 117, "endOffset": 148}, {"referenceID": 5, "context": "For example, in domain adaptation [Ben-David et al., 2010], one employs large unlabeled samples to estimate the relatedness of source and target domains to perform adaptation. Even if unlabeled data are abundant, the estimation of adaptation parameters can be computationally prohibitive. A hypothetical example is a large number of domains involved, or, for instance, when one acquires new domains incrementally. There, keeping unlabeled data from all the domains and re-estimating the parameters is a necessity. In HTL we naturally alleviate these practical limitations through indirect access to the source domain by means of a source hypothesis. As was mentioned above, the first theoretical analysis of HTL was carried out by Kuzborskij and Orabona [2013b], where we analyzed HTL problem cast as a regularized least-squares with a single fixed, unweighted, source hypothesis.", "startOffset": 35, "endOffset": 762}, {"referenceID": 5, "context": "For example, in domain adaptation [Ben-David et al., 2010], one employs large unlabeled samples to estimate the relatedness of source and target domains to perform adaptation. Even if unlabeled data are abundant, the estimation of adaptation parameters can be computationally prohibitive. A hypothetical example is a large number of domains involved, or, for instance, when one acquires new domains incrementally. There, keeping unlabeled data from all the domains and re-estimating the parameters is a necessity. In HTL we naturally alleviate these practical limitations through indirect access to the source domain by means of a source hypothesis. As was mentioned above, the first theoretical analysis of HTL was carried out by Kuzborskij and Orabona [2013b], where we analyzed HTL problem cast as a regularized least-squares with a single fixed, unweighted, source hypothesis. There, we came up with polynomial generalization bound that depends on the performance of that fixed source hypothesis on the target task. In this work we considerably extend and generalize the theory of HTL. Our contributions. We formulate a general Hypothesis Transfer Learning problem through regularized Empirical Risk Minimization (ERM) with respect to any non-negative smooth loss function and any strongly convex regularizer. For its solution we prove high-probability generalization bounds that exhibit fast rate, i.e. O(1/m), of convergence whenever any weighted combination of multiple source hypotheses performs well on the target task. In other words, we show theoretically that HTL facilitates a faster generalization. In addition, we show that, if the combination is perfect, the error on the training set becomes the error we observe on the whole target distribution. Furthermore, we analyze an excess risk of our formulation, and conclude that a good source hypothesis also speeds up the convergence to the performance of the bestin-the-class. As a byproduct of our study, we prove an upper bound on the Rademacher complexity of a smooth loss class that provides extra information compared to that of Lipschitz loss classes. This is an alternative to the analysis of Srebro et al. [2010a] that holds under much weaker assumptions, and it might be of independent interest.", "startOffset": 35, "endOffset": 2186}, {"referenceID": 17, "context": "The framework of Hypothesis Transfer Learning (HTL) that we address in this paper was first formally introduced and studied theoretically by Kuzborskij and Orabona [2013b]. It was shown that the generalization ability of the regularized least-squares HTL algorithm improves if supplied source hypothesis performs well on the target task.", "startOffset": 141, "endOffset": 172}, {"referenceID": 3, "context": "Later, Ben-David and Urner [2013] showed a similar bound, but with different source performance criterion.", "startOffset": 7, "endOffset": 34}, {"referenceID": 31, "context": ", 2014], employs the principle of biased regularization [Sch\u00f6lkopf et al., 2001].", "startOffset": 56, "endOffset": 80}, {"referenceID": 13, "context": "Li and Bilmes [2007] have analyzed a Bayesian approach to HTL.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Li and Bilmes [2007] have analyzed a Bayesian approach to HTL. Employing a PAC-Bayes analysis they showed that given a prior on the hypothesis class, the generalization ability of logistic regression improves if the prior is informative on the target task. Mansour et al. [2008] analyzed a setting of multiple source hypotheses combination.", "startOffset": 0, "endOffset": 279}, {"referenceID": 1, "context": "We will quantify the complexity of a hypothesis class by the means of Rademacher complexity [Bartlett and Mendelson, 2003].", "startOffset": 92, "endOffset": 122}, {"referenceID": 31, "context": "As an example of such, consider a common transfer learning problem: the least squares with biased regularization [Sch\u00f6lkopf et al., 2001].", "startOffset": 113, "endOffset": 137}, {"referenceID": 2, "context": "This bound is similar in spirit to the results of localized complexities, as in works of Bartlett et al. [2005], Srebro et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 2, "context": "This bound is similar in spirit to the results of localized complexities, as in works of Bartlett et al. [2005], Srebro et al. [2010a], however we focus on the HTL scenario rather than a generic learning setting.", "startOffset": 89, "endOffset": 135}, {"referenceID": 8, "context": ", 2005, Bousquet and Elisseeff, 2002], that is a special case of the smooth loss function that the HTL through Regularized ERM employs. On the other hand, Srebro et al. [2010a] showed a better worst-case rate O(1/ \u221a m\u03bb).", "startOffset": 8, "endOffset": 177}, {"referenceID": 2, "context": "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2.", "startOffset": 61, "endOffset": 84}, {"referenceID": 2, "context": "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2.", "startOffset": 61, "endOffset": 104}, {"referenceID": 2, "context": "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2. Indeed, Theorem 7 shows a bound which is very similar to the localized ones, albeit with two differences. The r.h.s. of the first inequality in Theorem 7 vanishes when the loss class has zero variance. Though intuitively trivial, this allows to prove a considerable result in the theory of transfer learning as it quantifies the intuition that no learning is necessary if the source has perfect performance on the target task. Second, by applying the standard localized Rademacher complexity bounds of Bousquet [2002], and assuming the use of the Lipschitz loss function, we do not achieve a fast rate of convergence, as can be seen from Theorem 13, shown in the Appendix.", "startOffset": 61, "endOffset": 699}, {"referenceID": 2, "context": "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2. Indeed, Theorem 7 shows a bound which is very similar to the localized ones, albeit with two differences. The r.h.s. of the first inequality in Theorem 7 vanishes when the loss class has zero variance. Though intuitively trivial, this allows to prove a considerable result in the theory of transfer learning as it quantifies the intuition that no learning is necessary if the source has perfect performance on the target task. Second, by applying the standard localized Rademacher complexity bounds of Bousquet [2002], and assuming the use of the Lipschitz loss function, we do not achieve a fast rate of convergence, as can be seen from Theorem 13, shown in the Appendix. We suspect that assuming the smoothness of the loss function is crucial to prove fast rates in our formulation. Fast rates for ERM with the smooth loss have been thoroughly analyzed by Srebro et al. [2010a]. Yet, the analysis of our HTL algorithm within their framework would yield a bound that is inferior to ours in two respects.", "startOffset": 61, "endOffset": 1061}, {"referenceID": 2, "context": "In particular, the localized Rademacher complexity bounds of Bartlett et al. [2005] and Bousquet [2002] can be used to obtain results similar to the second inequality of Theorem 2. Indeed, Theorem 7 shows a bound which is very similar to the localized ones, albeit with two differences. The r.h.s. of the first inequality in Theorem 7 vanishes when the loss class has zero variance. Though intuitively trivial, this allows to prove a considerable result in the theory of transfer learning as it quantifies the intuition that no learning is necessary if the source has perfect performance on the target task. Second, by applying the standard localized Rademacher complexity bounds of Bousquet [2002], and assuming the use of the Lipschitz loss function, we do not achieve a fast rate of convergence, as can be seen from Theorem 13, shown in the Appendix. We suspect that assuming the smoothness of the loss function is crucial to prove fast rates in our formulation. Fast rates for ERM with the smooth loss have been thoroughly analyzed by Srebro et al. [2010a]. Yet, the analysis of our HTL algorithm within their framework would yield a bound that is inferior to ours in two respects. The first concerns the scenario when the combined source hypothesis is perfect, that is Rsrc = 0. The generalization bound of Srebro et al. [2010a] does not offer a way to show that the empirical risk converges to the risk with probability one \u2013 instead one can only get a fast rate of convergence.", "startOffset": 61, "endOffset": 1334}, {"referenceID": 5, "context": "Perhaps the most well known are the dH\u2206H-divergence [Ben-David et al., 2010] and its more general counterpart, the Discrepancy Distance [Mansour et al.", "startOffset": 52, "endOffset": 76}, {"referenceID": 26, "context": ", 2010] and its more general counterpart, the Discrepancy Distance [Mansour et al., 2009].", "startOffset": 67, "endOffset": 89}, {"referenceID": 3, "context": ", 2009, Ben-David and Urner, 2012, Mansour et al., 2008, Cortes and Mohri, 2014] have proposed a number of such domain relatedness criteria. Perhaps the most well known are the dH\u2206H-divergence [Ben-David et al., 2010] and its more general counterpart, the Discrepancy Distance [Mansour et al., 2009]. Typically, this divergence is explicitated in the generalization bound along with other terms controlling the generalization on the target domain. Let RDtrg(h) and RDsrc(h) denote the risks of the hypothesis h, measured w.r.t. the target and source distributions. Then a well-known result of Ben-David et al. [2010] suggests that for all h \u2208 H RDtrg(h) \u2264 RDsrc(h) + dH\u2206H(D,D) + \u03b5H, (6) where \u03b5H = minh\u2208H {RDtrg(h) +RDsrc(h)}.", "startOffset": 8, "endOffset": 617}, {"referenceID": 9, "context": "The following result, obtained through an algorithmic stability argument [Bousquet and Elisseeff, 2002], holds with probability at least 1\u2212 \u03b4", "startOffset": 73, "endOffset": 103}, {"referenceID": 21, "context": "Mansour et al. [2008] have addressed the problem of multiple source hypotheses combination, however, in a different HTL setting.", "startOffset": 0, "endOffset": 22}, {"referenceID": 33, "context": "Similarly as in previous works [Srebro et al., 2010a], we exploit additional information about the behavior of the hypothesis coming from the gradient of the loss function. This allows us to prove a bound on the empirical Rademacher complexity of a hypothesis class, Lemma 8, that depends on the point-wise bounds on the loss function. This contrasts with Srebro et al. [2010a], who consider smooth losses as well, but use a much more restrictive notion of Rademacher complexity.", "startOffset": 32, "endOffset": 378}, {"referenceID": 8, "context": "1 Fast Rate Generalization Bound The proof of fast-rate and vanishing-confidence-term bounds, Theorem 7, stems from the functional generalization of Bennett\u2019s inequality which is due to Bousquet [2002, Theorem 2.11] and that we report here for completeness. Theorem 4 (Bousquet [2002]) Let X1, X2, .", "startOffset": 186, "endOffset": 285}, {"referenceID": 15, "context": "3 in Hoorfar and Hassani [2008], that says that", "startOffset": 5, "endOffset": 32}, {"referenceID": 1, "context": "13)], [Bartlett and Mendelson, 2003], that is used to relate the expected uniform deviation of empirical risk over the hypothesis class to the Rademacher complexity of that class.", "startOffset": 6, "endOffset": 36}, {"referenceID": 16, "context": "For this purpose we employ the results of Kakade et al. [2008, 2012], who studied strongly convex regularizers in a more general setting. Furthermore, we will focus on the use of smooth loss functions, that is with bounded second derivative as done by Srebro et al. [2010a]. The proof of the main result of this section, Theorem 10, depends essentially on the following lemma, that bounds the empirical Rademacher complexity of a H-smooth loss class.", "startOffset": 42, "endOffset": 274}, {"referenceID": 27, "context": "Proof This proof follows a line of reasoning similar to the proof of Talagrand\u2019s lemma for Lipschitz functions, see for instance Mohri et al. [2012, p. 79]. We will also use Lemma B.1 by Srebro et al. [2010b], stating that for any H-smooth non-negative function \u03c6 : R 7\u2192 R+ and any x, z \u2208 R, |\u03c6(x)\u2212 \u03c6(z)| \u2264 \u221a 6H(\u03c6(x) + \u03c6(z))|x\u2212 z|.", "startOffset": 129, "endOffset": 209}, {"referenceID": 16, "context": "To prove Theorem 10 we will also use the following lemma in Kakade et al. [2012, Corollary 4]. Lemma 9 (Kakade et al. [2012]) If \u03a9 is \u03c3 strongly convex w.", "startOffset": 60, "endOffset": 125}, {"referenceID": 1, "context": "We apply Lemma 8 with the loss class L and the property of Rademacher complexities for the sums of function classes [Bartlett and Mendelson, 2003] to show that", "startOffset": 116, "endOffset": 146}, {"referenceID": 36, "context": "Indeed, when the HTL algorithm is provided with enormous pool of source hypotheses, how to select relevant ones on the basis of only few labeled examples? This might sound similar to the feature selection problem under the condition that n m, however, earlier empirical studies by Tommasi et al. [2014] with hundreds of sources did not find much corroboration for this hypothesis when applying L1 regularization.", "startOffset": 281, "endOffset": 303}], "year": 2017, "abstractText": "In this work we consider the learning setting where in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. This paradigm, known as Hypothesis Transfer Learning (HTL), has been successfully exploited in empirical works, but only recently has received a theoretical attention. Here, we try to understand when HTL facilitates accelerated generalization \u2013 the goal of the transfer learning paradigm. Thus, we study a broad class of algorithms, a Hypothesis Transfer Learning through Regularized ERM, that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that if the algorithm is fed with a good source hypotheses combination, generalization happens at the fast rate O(1/m) instead of usual O(1/ \u221a m). We also observe that if the combination is perfect, our theory formally backs up the intuition that learning is not necessary. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.", "creator": "LaTeX with hyperref package"}}}