{"id": "1412.1628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2014", "title": "Fisher Kernel for Deep Neural Activations", "abstract": "Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks.", "histories": [["v1", "Thu, 4 Dec 2014 11:30:57 GMT  (1994kb,D)", "https://arxiv.org/abs/1412.1628v1", null], ["v2", "Fri, 19 Dec 2014 07:16:18 GMT  (1994kb,D)", "http://arxiv.org/abs/1412.1628v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["donggeun yoo", "sunggyun park", "joon-young lee", "in so kweon"], "accepted": false, "id": "1412.1628"}, "pdf": {"name": "1412.1628.pdf", "metadata": {"source": "CRF", "title": "Fisher Kernel for Deep Neural Activations", "authors": ["Donggeun Yoo", "Sunggyun Park", "Joon-Young Lee"], "emails": ["dgyoo@rcv.kaist.ac.kr,", "sunggyun@kaist.ac.kr,", "jylee@rcv.kaist.ac.kr,", "iskweon77@kaist.ac.kr"], "sections": [{"heading": "1. Introduction", "text": "Barbu et al. [3] introduced an interesting experiment that a simple classifier, together with human brain scan data, is much better than the state-of-the-art model [29], which gathers rich local statistics that are captured by hand-crafted local descriptors. BOW representation is further improved with VLAD [14] and Fisher Kernel devotes itself to global image representation [24, 23] by adding higher order statistics. An important advantage of these global representations based on local descriptors is their inventory property to scale changes, localization changes, occlusions and background couplings. Recent computer visions are achieving dramatic advances in visual recognition through deep Convolutionary Neural Networks (CNNs) that learn the entire hierarchy from the outset."}, {"heading": "2. Multi-scale Pyramid Pooling", "text": "In this section, we first review the Fisher kernel framework and then introduce a multi-level pyramid pooling that adds a Fisher kernel-based pooling layer over a pre-trained CNN."}, {"heading": "2.1. Fisher Kernel Review", "text": "The Fisher kernel model based on a visual vocabulary is proposed by Perronnin et al. in [23]. It extends the traditional bag-of-words model by a probabilistic generative model. It models the distribution of low-threshold descriptors using a Gaussian mixture model (GMM) and represents an image by taking the gradient into account in terms of model parameters. Although the number of local descriptors varies from picture to picture, the resulting Fisher vector has a fixed length, so it is possible to use discriminatory classifiers such as a linear SVM. Let x have a d-dimensional local descriptor and gig = {gk, k = 1... K} a pre-tracted GMM with K-Gaussians, with \u03bb = {outlook, k, k = 1... K} a discriminatory classifier such as a linear SVM.Lasse} and a pre-tractable pyrak with a K-dimensional pyrak = 1 GMM..."}, {"heading": "2.2. Dense CNN Activations", "text": "To get multi-level activations from a CNN without modification, the previous approach truncated local patches and fed the patches into a network after the patches were adjusted to the fixed size of the CNN input. However, if we extract multi-level local activations densely, the approach is quite inefficient, as many redundant operations are performed in sinuous layers for overlapped regions. To extract dense CNN activations without redundant operations, we simply replace the fully connected layers of an existing CNN with equivalent multiple folding filters along spatial axes. If an image larger than the fixed size is fed in, the modified network issues multiple activation vectors, each vector being CNN activations from the corresponding local patch. The method is illustrated in Fig. 2. Using this method, thousands of dense local activations (4,410 per image) are extracted from multiple scale layers in a reasonable extraction time (0.46 seconds per image) as shown in Table 1."}, {"heading": "2.3. Multi-scale Pyramid Pooling (MPP)", "text": "To render an image, we first create a scale pyramid for the input image, in which the minimum scale image is a fixed size of a CNN, and each scale image has two times the resolution of the previous scale image. We feed all scaled images into a pre-formed CNN and extract dense CNN activation vectors. Then, all activation vectors are merged into a single vector through our multi-scale pyramid. (Fisher) Scale properties compared to local descriptors as explained in Sec. 3. To adopt the Fisher kernel for CNN activation features, we add all local activation vectors to a Fisher vector as described in Sec. 2.1. (Fisher) Scale properties as compared to local descriptors as explained in Sec. 3. To apply the Fisher kernel to CNN activation features, we introduce a multi-scale layer of vectors."}, {"heading": "3. Analysis of Multi-scale CNN Activations", "text": "It tells us that it is not appropriate to apply a Fisher kernel framework directly to a multi-scale of local CNN activations in order to present an image. To investigate the best way to aggregate CNN activations into a global representation, we conduct empirical studies and conclude that the application of scale normalization of Fisher vectors is very important. A naive way to obtain a Fisher G'S vector is a multi-scale of local activation vectors. (2) Here, any multi-scalable local activation vector is to a Fisher vector with equal weighting of 1 / 2. To combine a Fisher kernel with mid-level, the property of CNN activations should be considered."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "To evaluate our proposal as a generic image representation, we perform three different visual recognition tasks with the following datasets. MIT Indoor 67 [25] is used for a scene classification task. The dataset contains 15,620 images with a total of 67 indoor scene classes. It is a sophisticated dataset, as many indoor classes are characterized by the objects it contains (e.g. different types of shops) and not by their spatial characteristics. Performance is measured with top-1 accuracy. PASCAL VOC 2007 [9] is used for an object classification task. It consists of 9,963 images from a total of 20 object classes. The task is quite difficult as the scales of the objects vary and multiple objects of different classes are often included in the same picture.Performance is measured with (11-point interpolated) average accuracy. Oxford 102 flowers [21] is used for a fine-grained object classification task, which distinguishes the subclasses of the same object class of images, each consisting of the highest dataset of 258,102 images."}, {"heading": "4.2. Pre-trained CNNs", "text": "We use two CNNs pre-trained on the ILSVRC '12 dataset [6] to extract multi-scale local activations, one is the Caffe reference model [15], which consists of five Convolutionary layers and three fully connected layers. This model had 19.6% top-5 errors when a single section of each validation image is used for evaluation on the ILSVRC' 12 dataset. Henceforth, we refer to this model as \"Alex\" because it has almost the same architecture as Krizhevsky et al.'s CNN [17], the other is Chatfield et al.' s CNN-S model [4] (\"CNNS,\" henceforth). This model, a simplified version of OverFeat [27], also consists of five Convolutionary layers (three in [27]) and three fully connected layers. It shows 15.5% top-5 errors on the SVRILC '12 dataset with the same center Alex."}, {"heading": "4.3. Implementation Details", "text": "We use a seven-scale image pyramid by default, as the seven scales in all datasets can cover large variations and performance at sufficient scale, as shown in Fig. 4.The general approach of our image representation is as follows: For one image, we create an image pyramid with seven scaled images. Each image in the pyramid has double the resolution as the previous scale based on the standard size defined in each CNN (e.g. 227 x 227 for Alex). We then feed each scale image to CNN and receive 4,410 vectors with 4,096 dimensional dense CNN activations from the seventh level. The dimensionality of each activation vector is reduced by PCA to 128, where a projection is trained with 256,000 activation vectors sampled from training images. A visual vocabulary (GMM of 256 Gaussian distributions) is also trained with the same samples."}, {"heading": "4.4. Results and Analysis", "text": "We conduct extensive experiments to compare different methods to the three detection tasks. We first show the performance of our method and baseline methods, then compare our result with state-of-the-art methods for each dataset. To make it easier, we use an activation protocol \"A (B),\" where A denotes a pooling method, and B denotes descriptors summarized by A. The notations are summarized in Table 2.We compare our method with several baseline methods. The basic methods include intermediate CNN activations with a standard input, an average pooling with multiple jittered images, and modified versions of our method. The comparison results for each dataset are summarized in Table 3 (a), 4 (a), 6 (a). As expected, the most basic representation, Alex-FC7, performs the worst for all datasets. The average pooling performance in AP10 and AP50 improves performance by + 3%."}, {"heading": "4.5. Weakly-Supervised Object Confidence Map", "text": "An interesting feature of our method is that we can present object confidence maps for object classification tasks, even though we train the SVM classifiers in limited boxes with no annotations, but only with class-level labels. To restore confidence maps, we track how much weight is attached to each local patch, and accumulate all the weights of local activations. Tracking the weight of local activations is possible because our final representation can be created regardless of the number of scales and number of local activation vectors. To trace the weight of each patch, we calculate our final representation per patch using only the corresponding single activation vector and calculate the score from the SVM classifiers we have pre-tracted. Figure 5 and Figure 6 show several examples of object confidence maps on the VOC 2007 test images. In the illustrations, we can also verify our image quality within the object classification, as well as for further modifying small objects."}, {"heading": "5. Discussion", "text": "There are several conclusions we can deduce from our study. First, we should consider the scale characteristics of neural activations for the successful combination of a Fisher kernel and a CNN. Activations become uninformative as a patch size shrinks, but they can contribute to better scale invariance if they correspond to simple scale-by-scale normalization; second, dense deep neural activations are extracted from multiple levels of scale with reasonable calculations by replacing the full connection with equivalent multiple fold filters. It allows us to bundle the truly multi-scale activations and achieve significant performance improvements in the tasks of visual detection; third, reasonable confidence maps at object level can be derived from our image representation, although only class-level labels are given for monitoring that can be further applied to object detection or localization tasks. In the comprehensive experiments on three different detection tasks, the results indicate that our visual tasks can be used as different suggestions for detection."}], "references": [{"title": "Efficient object detection and segmentation for fine-grained recognition", "author": ["A. Angelova", "S. Zhu"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V. Lempitsky"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Seeing is worse than believing: Reading people\u2019s minds better than computer-vision methods recognize actions", "author": ["A. Barbu", "D.P. Barrett", "W. Chen", "S. Narayanaswamy", "C. Xiong", "J.J. Corso", "C.D. Fellbaum", "C. Hanson", "S.J. Hanson", "S. H\u00e9lie", "E. Malaia", "B.A. Pearlmutter", "J.M. Siskind", "T.M. Talavage", "R.B. Wilbur"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In Proceedings of British Machine Vision Conference (BMVC),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y.L. Cun", "B. Boser", "J.S. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L.F. Fei"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In Computing Research Repository (CoRR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "The PASCAL Visual Object Classes Challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Mining midlevel features for image classification", "author": ["B. Fernando", "\u00c9. Fromont", "T. Tuytelaars"], "venue": "International Journal on Computer Vision (IJCV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R.B. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activations features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. Jegou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Blocks that shout: Distinctive parts for scene classification", "author": ["M. Juneja", "A. Vedaldi", "C.V. Jawahar", "A. Zisserman"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal on Computer Vision (IJCV),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Generalized max pooling", "author": ["N. Murray", "F. Perronnin"], "venue": "Computing Research Repository (CoRR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Automated flower classification over a large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C.R. Dance"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A.Torralba"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "In Computing Research Repository (CoRR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In Proceedings of International Conference on Learning Representations (ICLR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["S. Singh", "A. Gupta", "A.A. Efros"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In Proceedings of IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www. vlfeat.org/,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "MatConvNet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "http://www.vlfeat.org/ matconvnet/,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Cnn: Single-label to multi-label", "author": ["Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan"], "venue": "In Computing Research Repository (CoRR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Part-based R-CNNs for fine-grained category detection", "author": ["N. Zhang", "J. Donahue", "R.B. Girshick", "T. Darrell"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "PANDA: Pose aligned networks for deep attribute modeling", "author": ["N. Zhang", "M. Paluri", "M. Ranzato", "T. Darrell", "L.D. Bourdev"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Learning discriminative and shareable features for scene classification", "author": ["Z. Zuo", "G. Wang", "B. Shuai", "L. Zhao", "Q. Yang", "X. Jiang"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "[3] introduced an interesting experiment that a simple classifier along with human brain-scan data substantially outperforms the state-of-the-art methods in recognizing action from video clips.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "With a success of local descriptors [19], many researches devoted deep study to global image representation based on a Bag-of-Word (BOW) model [29] that aggregates abundant local statistics captured by hand-designed local descriptors.", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "With a success of local descriptors [19], many researches devoted deep study to global image representation based on a Bag-of-Word (BOW) model [29] that aggregates abundant local statistics captured by hand-designed local descriptors.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "The BOW representation is further improved with VLAD [14] and Fisher kernel [24, 23] by adding higher order statistics.", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "The BOW representation is further improved with VLAD [14] and Fisher kernel [24, 23] by adding higher order statistics.", "startOffset": 76, "endOffset": 84}, {"referenceID": 22, "context": "The BOW representation is further improved with VLAD [14] and Fisher kernel [24, 23] by adding higher order statistics.", "startOffset": 76, "endOffset": 84}, {"referenceID": 4, "context": "In recent computer vision researches, drastic advances of visual recognition are achieved by deep convolutional neural networks (CNNs) [5], which jointly learn the whole feature hierarchies starting from image pixels to the final class posterior with stacked non-linear processing layers.", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "The recent presence of large scale ImageNet [6] database and the raise of parallel computing contribute to the breakthrough in visual recognition.", "startOffset": 44, "endOffset": 47}, {"referenceID": 16, "context": "[17] achieved an impressive result using a CNN in large-scale image classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 144, "endOffset": 162}, {"referenceID": 7, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 144, "endOffset": 162}, {"referenceID": 21, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 144, "endOffset": 162}, {"referenceID": 12, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 144, "endOffset": 162}, {"referenceID": 3, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 144, "endOffset": 162}, {"referenceID": 10, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 181, "endOffset": 189}, {"referenceID": 12, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 181, "endOffset": 189}, {"referenceID": 25, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 212, "endOffset": 224}, {"referenceID": 11, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 212, "endOffset": 224}, {"referenceID": 35, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 212, "endOffset": 224}, {"referenceID": 25, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 254, "endOffset": 262}, {"referenceID": 33, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 254, "endOffset": 262}, {"referenceID": 34, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 286, "endOffset": 290}, {"referenceID": 1, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 308, "endOffset": 311}, {"referenceID": 7, "context": "Combining the CNN activations with a classifier has shown impressive performance in wide visual recognition tasks such as object classification [26, 8, 22, 13, 4], object detection [11, 13], scene classification [26, 12, 36], fine-grained classification [26, 34], attribute recognition [35], image retrieval [2], and domain transfer [8].", "startOffset": 333, "endOffset": 336}, {"referenceID": 7, "context": "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].", "startOffset": 248, "endOffset": 262}, {"referenceID": 1, "context": "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].", "startOffset": 248, "endOffset": 262}, {"referenceID": 12, "context": "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].", "startOffset": 248, "endOffset": 262}, {"referenceID": 10, "context": "For utilizing CNN activations as a generic image representation, a straightforward way is to extract the responses from the first or second fully connected layer of a pretrained CNN by feeding an image and to represent the image with the responses [8, 2, 13, 11].", "startOffset": 248, "endOffset": 262}, {"referenceID": 16, "context": "Though the data augmentation has been used to prevent over-fitting [17], recent researches show that average pooling, augmenting data and averaging the multiple activation vectors in a test stage, also helps to achieve better geometric invariance of CNNs while improving classification performance by +2.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "92% in [4] and +3.", "startOffset": 7, "endOffset": 10}, {"referenceID": 25, "context": "in [26] on PASCAL VOC 2007.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "[12] proposed a method to exploit multi-scale CNN activations in order to achieve geometric invariance characteristic while improving recognition accuracy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The CNN activations are aggregated at finer scales via VLAD encoding which was introduced in [14], and then the encoded activations are concatenated as a single vector to obtain the final representation.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Similar to [12], we also utilize multi-scale CNN activations, but present a different pooling method that shows better performance in our experiments.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "Specifically, we suggest an efficient way to obtain abundant amount of multi-scale local activations from a CNN, and aggregate them using the state-of-the-art Fisher kernel [24, 23] with a simple but important scale-wise normalization, so called multi-scale pyramid pooling.", "startOffset": 173, "endOffset": 181}, {"referenceID": 22, "context": "Specifically, we suggest an efficient way to obtain abundant amount of multi-scale local activations from a CNN, and aggregate them using the state-of-the-art Fisher kernel [24, 23] with a simple but important scale-wise normalization, so called multi-scale pyramid pooling.", "startOffset": 173, "endOffset": 181}, {"referenceID": 25, "context": "Our proposal demonstrates substantial improvements on both scene and object classification tasks compared to the previous representations including a single activation, the average pooling [26, 4], and the VLAD of activations [12].", "startOffset": 189, "endOffset": 196}, {"referenceID": 3, "context": "Our proposal demonstrates substantial improvements on both scene and object classification tasks compared to the previous representations including a single activation, the average pooling [26, 4], and the VLAD of activations [12].", "startOffset": 189, "endOffset": 196}, {"referenceID": 11, "context": "Our proposal demonstrates substantial improvements on both scene and object classification tasks compared to the previous representations including a single activation, the average pooling [26, 4], and the VLAD of activations [12].", "startOffset": 226, "endOffset": 230}, {"referenceID": 22, "context": "in [23].", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "With Caffe reference model [15], FC7 activations are extracted from 100 random images of PASCAL VOC 2007.", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "The fisher kernel framework is further improved in [24] by the additional two-stage normalizations: power-normalization with the factor of 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "Refer to [24] for the theoretical proofs and details.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "Following the Improved Fisher Kernel framework [24], we finally apply power normalization and `2-normalization to the Fisher vector G .", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "In the traditional use of Fisher kernel on visual classification tasks, the hand-designed local descriptors such as SIFT [19] have been often densely computed in multi-scale.", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "FC6 or FC7 of [17]) represents higher level structure information which is closer to class posteriors.", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": "As shown in the CNN visualization proposed by Zeiler and Fergus in [33], image regions strongly activated by a certain CNN filter of the fifth layer usually capture a category-level entire object.", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "empirical analysis with scale-wise classification scores on PASCAL VOC 2007 [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 24, "context": "MIT Indoor 67 [25] is used for a scene classification task.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "PASCAL VOC 2007 [9] is used for an object classification task.", "startOffset": 16, "endOffset": 19}, {"referenceID": 20, "context": "Oxford 102 Flowers [21] is used for a fine-grained object classification task, which distinguishes the sub-classes of the same object class.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "We use two CNNs pre-trained on the ILSVRC\u201912 dataset [6] to extract multi-scale local activations.", "startOffset": 53, "endOffset": 56}, {"referenceID": 14, "context": "One is the Caffe reference model [15] composed of five convolutional layers and three fully connected layers.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "\u2019s CNN [17].", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "\u2019s CNN-S model [4] (\u201cCNNS\u201d, henceforth).", "startOffset": 15, "endOffset": 18}, {"referenceID": 26, "context": "This model, a simplified version of the OverFeat [27], is also composed of five convolutional layers (three in [27]) and three fully connected layers.", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "This model, a simplified version of the OverFeat [27], is also composed of five convolutional layers (three in [27]) and three fully connected layers.", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "The CNNS is used only for the PASCAL VOC 2007 dataset to compare our method with [4], which demonstrates excellent performance with the CNNS.", "startOffset": 81, "endOffset": 84}, {"referenceID": 30, "context": "Both of the two pre-trained models are available online [31].", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "Our system is mostly implemented using open source libraries including VLFeat [30] for a Fisher kernel framework and MatConvNet [31] for CNNs.", "startOffset": 78, "endOffset": 82}, {"referenceID": 30, "context": "Our system is mostly implemented using open source libraries including VLFeat [30] for a Fisher kernel framework and MatConvNet [31] for CNNs.", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "\u2019s method [12].", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "We also apply the spatial pyramid (SP) kernel [18] to our representation.", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "[12] proposed a pooling method for multi-scale CNN activations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Compared to [12], our representation largely outperforms the method with a gain of +7.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "[37] who combined the Alex-FC6 and their complementary features so called DSFL.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "There are two methods ([22] and [26]) that use the same Alex network.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "There are two methods ([22] and [26]) that use the same Alex network.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "[26] performed target data augmentation and Oquab et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] used a multi-layer perceptron (MLP) instead of a linear SVM with ground truth bounding boxes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "ImageNet classification) or the target task, such as Spatial Pyramid Pooling (SPP) network [13], Multi-label CNN [32] and the CNNS [4].", "startOffset": 91, "endOffset": 95}, {"referenceID": 31, "context": "ImageNet classification) or the target task, such as Spatial Pyramid Pooling (SPP) network [13], Multi-label CNN [32] and the CNNS [4].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "ImageNet classification) or the target task, such as Spatial Pyramid Pooling (SPP) network [13], Multi-label CNN [32] and the CNNS [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "When our representation is equipped with the superior CNNS [4], which is not fine-tuned on VOC 2007, our representation (81.", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "The performance is still lower than [4], who conduct target data augmentation and fine-tuning.", "startOffset": 36, "endOffset": 39}, {"referenceID": 25, "context": "28%) outperforms the previous state-of-the-art method [26] (86.", "startOffset": 54, "endOffset": 58}, {"referenceID": 23, "context": "[24] 10\u2019 No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] \u201914 No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] \u201914 No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] \u201914 Yes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] \u201914 Yes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "56 Ours MPP(Alex-FC7)+DSFL[37] Yes.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "[28] \u201912 Part+GIST+DPM+SP No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] \u201913 IFK+Bag-of-Parts No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] \u201913 IFK+MidlevelRepresent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[37] \u201914 DSFL No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] \u201914 DSFL+Alex-FC6 Yes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] \u201914 Alex-FC7 Yes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] \u201914 Alex-FC7 Yes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] \u201914 AP(Alex)+PT+TargetAug.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] \u201914 VLAD Concat.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] 10\u2019 IFK(SIFT+color) No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] \u201914 SPPNET-FC7 No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] \u201914 Multi-label CNN Yes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] \u201914 AP(Alex)+PT+TA No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] \u201914 Alex-FC7+MLP No.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] \u201914 AP(CNNS-FC7)+TA No.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] \u201914 AP(CNNS-FC7)+TA Yes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "Nilsback and Zisserman [21] \u201908 Multple kernel learning Yes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "70 Angelova and Zhu [1] \u201913 Seg+DenseHoG+LLC+MaxPooling Yes.", "startOffset": 20, "endOffset": 23}, {"referenceID": 19, "context": "70 Murray and Perronnin [20] \u201914 GMP of IFK(SIFT+color) No.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "[10] \u201914 Bag-of-FLH Yes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] \u201914 AP(Alex)+PT+TA No.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks.", "creator": "LaTeX with hyperref package"}}}