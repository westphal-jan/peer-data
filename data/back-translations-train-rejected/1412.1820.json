{"id": "1412.1820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2014", "title": "Context-Dependent Fine-Grained Entity Type Tagging", "abstract": "Entity type tagging is the task of assigning category labels to each mention of an entity in a document. While standard systems focus on a small set of types, recent work (Ling and Weld, 2012) suggests that using a large fine-grained label set can lead to dramatic improvements in downstream tasks. In the absence of labeled training data, existing fine-grained tagging systems obtain examples automatically, using resolved entities and their types extracted from a knowledge base. However, since the appropriate type often depends on context (e.g. Washington could be tagged either as city or government), this procedure can result in spurious labels, leading to poorer generalization. We propose the task of context-dependent fine type tagging, where the set of acceptable labels for a mention is restricted to only those deducible from the local context (e.g. sentence or document). We introduce new resources for this task: 11,304 mentions annotated with their context-dependent fine types, and we provide baseline experimental results on this data.", "histories": [["v1", "Wed, 3 Dec 2014 23:26:33 GMT  (266kb,D)", "http://arxiv.org/abs/1412.1820v1", null], ["v2", "Mon, 1 Aug 2016 20:14:36 GMT  (266kb,D)", "http://arxiv.org/abs/1412.1820v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dan gillick", "nevena lazic", "kuzman ganchev", "jesse kirchner", "david huynh"], "accepted": false, "id": "1412.1820"}, "pdf": {"name": "1412.1820.pdf", "metadata": {"source": "CRF", "title": "Context-Dependent Fine-Grained Entity Type Tagging", "authors": ["Dan Gillick", "Nevena Lazic", "Kuzman Ganchev", "Jesse Kirchner", "David Huynh"], "emails": [], "sections": [{"heading": null, "text": "We propose the task of contextual labeling of fine types, where the number of acceptable names for a mention is limited to those that can be derived from the local context (e.g. sentence or document). We present new resources for this task: 11,304 mentions that are commented with their context-dependent fine types, and we provide experimental baseline results on this data."}, {"heading": "1 Introduction", "text": "Dre rf\u00fc nde eeisrVnlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Fine-grained type labels and manual annotations", "text": "Our group of fine-grained type labels T, like FIGER, is derived from the freebase; however, we also organize the labels in a hierarchy. The motivation for this is that a hierarchy enables us to integrate simple domain knowledge (for example, that an athlete is also a person but not a place) and to ensure the consistency of labels. In addition, when the number of possible labels is very large, it allows us to draw faster conclusions by assigning labels in a top-down manner. The labels are organized into a tree-structured taxonomy, with each label relating to the asymmetric, anti-reflective, transitive \"IS-A\" relationship with its parent. The root of the tree is a standard node that includes all types. Labels at the first level of the tree are the commonly used coarse types person, location, organization and others. These labels are then grouped into more general, fine-grained categories such as the Yx hierarchy."}, {"heading": "2.1 Manual annotations", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own, \"he said in an interview with The New York Times,\" It's not as if they are able to survive on their own. \""}, {"heading": "1 0.98 0.93 0.96", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 0.92 0.76 0.83", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 0.89 0.69 0.78", "text": "Precision was low (50%), which indicates that many of the automatically generated types have nothing to do with mentions."}, {"heading": "3 Distant supervision for training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Assembling training data", "text": "Ling and Weld use the internal links in Wikipedia as training data: A linked entity inherits the freebase types associated with the landing page. We follow a similar strategy, but rely instead on an entity resolution system that assigns freebase types to resolved entities, which we then assign to our types. We use a set of 133,000 message documents as a training corpus. Each document is processed through a standard NLP pipeline, including a partial speech (POS) tagger and dependency saver comparable in accuracy to the current Stanford dependency saver (Klein and Manning, 2003), and a NP extractor that uses POS tags and dependency edges to identify a number of entity mentions. Therefore, we separate the type tagging task from the identification of entity mentions commonly performed by entity detection systems."}, {"heading": "3.2 Training heuristics", "text": "The first heuristic we use to refine training data removes sibling types that are associated with a single unit, leaving only the parent type. For example, a unit with type person / political figure and person / athlete would end up with a single person. The motivation for this heuristic is that it is unusual for several sibling types to be relevant in the same context. This can shift some correct labels again; for example, cases of Barack Obama are only tagged with person, although in many cases person / political figure is correct. However, rarer entities associated with a few freebase types are better for generating training data as they are usually associated with types that are relevant to the context. Thus, we learn about politicians and governors rather than presidents. Coarse type circumcision removes types that do not match the output of a standard coarse classification."}, {"heading": "4 Feature extraction, models, and inference", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Feature extraction", "text": "For each mention of a resolved entity with at least one type, we extract a training instance (x, y), where x is a vector of binary feature indicators and y {0, 1} | T | is the binary vector of label indicators. The feature set includes the lexical and syntactic characteristics described in Table 4, similar to those used in previous work. We also use a more semantic document topic attribute, the result of training a simple bag topic model with eight themes (art, economy, entertainment, health, chaos, politics, scitech, sport) to try to capture a larger context. Word clusters are derived from the class-based exchange cluster algorithm described by Uszkoreit and Brants (2008). Intuitively, the characteristics describing the mention itself are most relevant for the top level of the typecast taxomy, as they are weighted accordingly for each type during the work being deeper."}, {"heading": "4.2 Models and inference", "text": "A recent study (Silla and Freitas, 2011) categorizes existing approaches as: flat, with a single multi-class classifier, locally, with a binary classifier for each label and enforcing label consistency at testing point, locally per parent node, with a multi-class classifier for all children of a node, and globally, by training a single multi-class classifier, but replacing the standard zero-one loss with a function that reflects label similarity. We examine the flat and local approaches and acknowledge that the results may be improved by more complex models. In particular, we use maximum entropy discriminatory local and flat classifiers (i.e. logistic and softmax regression). We note that existing fine-type labeling systems are also based on simple linear classifiers; FIGER uses a flat multi-class receive generality classifier that supports multiple labels while we support multi-classifiable ones."}, {"heading": "4.2.1 Local classifiers", "text": "In the local approach, a binary classifier is independently trained for each label, and the consistency of the labels is enforced at the time of inference. For each label, we train a binary logistic regression classifier with L2 regularization. Defining the positive and negative training examples for each binary classifier is not easy due to the asymmetrical IS-A relationships between the labels. We set the positive examples for a type on itself and all its progeny in the hierarchy; for example, a mention of a designated person / artist is considered a positive example for a person. We experiment with setting the negative examples for a type like (1) all other types with the same parent, (2) all other types with the same depth, or (3) all other types in the inference period, because the learned parameters and a test trait vector x, we first independently evaluate the probability of each type."}, {"heading": "4.2.2 Flat classifier", "text": "In this approach, we train a flat Softmax regression classifier (Berger et al., 1996) to distinguish all possible types. This classifier expects a single type label for each instance, while our training examples are labeled with several types. To take this into account, we convert each multi-label instance into several single-label instances during the training. For example, an appearance of \"Canada\" could be both location and organization. Instead of constructing a learning goal suitable for such multi-label training data, we create two training examples, one with label location and the other with label organization."}, {"heading": "5 Experiments", "text": "Assessing the performance of a hierarchical classifier is not easy. Previous work has used a variety of loss measurements to evaluate hierarchical classification errors; see, for example, CesaBianchi et al. (2006) or Weinberger and Chapelle (2008). For simplicity, we evaluate performance by precision, recall, F-score, and area below the precision / recall curve. Since the performance ratios are dominated by the Level 1 types, we additionally provide precision, recall, and F-score at each level (see Table 6). We divide the gold data into a development set of 16 documents and a test set of 61 documents, and report on the test results. We evaluate only named and nominal mentions, as is common in the designated entity-recognition literature. For simplicity, we select a single threshold to maximize the overall evaluation of the development set."}, {"heading": "5.1 Classifiers and inference", "text": "We begin by evaluating the local classifier approach described in Section 4.2.1. We compare the three strategies for selecting negative examples and the three inference methods for assigning labels. For each training strategy, we report on the results of the best corresponding inference method and vice versa. Results are presented in Table 5; the best results are obtained using the same depth designations as negative training examples and marginalizing hierarchy constraints. Next, we compare the best local classifier results with the flat classifier described in Section 4.2.2. Note that the characteristics and total number of model parameters are identical for both approaches. Results are presented in Table 6 and suggest that the local classifier outperforms the flat classifier, especially at deeper levels. The range below the precision / recall curve (AUC) is 63.7% for the flat classifier and 69.3% for the local classifier."}, {"heading": "5.2 Distant supervision heuristics", "text": "In Table 7, we compare the effects of different heuristics for pruning training labels with the best settings for our models: use of local classifiers with equally deep negative examples and marginalization over limitations on conclusions. Table 7 also lists the number of training examples extracted from the data, as discussed in Section 3.2. It is evident that heuristics has a significant impact on system performance, with rough pruning being particularly important. Taken together, the heuristics improve general formula 1 by 11.3% and AUC by 7.2%."}, {"heading": "6 Discussion and conclusions", "text": "Evidence suggests that the performance of such systems can be dramatically improved by using fine-grained type labels instead of the standard limited set of coarse tags. In the absence of labeled training data, fine-type labeling systems typically automatically obtain training data by using resolved units and types extracted from a knowledge base. Since companies often have multiple assigned types, this process can lead to incorrect type labels that are neither visible from the local context nor considered common knowledge. This subtle problem is not addressed in existing systems, which are both trained and evaluated on auto-generated data. In this paper, we strive to make fine-type labels more meaningful by requiring context dependence; that is, that the assigned labels are derivable from the local context. To this end, we introduce several distant supervision euphemisms that aim to work out relevant training data."}], "references": [{"title": "A maximum entropy approach to natural language processing", "author": ["Vincent J. Della Pietra", "Stephen A. Della Pietra"], "venue": "Comput. Linguist.,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Incremental algorithms for hierarchical classification", "author": ["Claudio Gentile", "Luca Zaniboni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "The automatic content extraction (ace) programtasks, data, and evaluation", "author": ["Alexis Mitchell", "Mark A Przybocki", "Lance A Ramshaw", "Stephanie Strassel", "Ralph M Weischedel"], "venue": null, "citeRegEx": "Doddington et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Doddington et al\\.", "year": 2004}, {"title": "Muc-7 named entity task definition", "author": ["Hirschman", "Chinchor1997] L Hirschman", "N Chinchor"], "venue": "In Proceedings of the 7th Message Understanding Conference (MUC-7)", "citeRegEx": "Hirschman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hirschman et al\\.", "year": 1997}, {"title": "Accurate unlexicalized", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D Manning"], "venue": null, "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Named entity recognition with character-level models", "author": ["Klein et al.2003] Dan Klein", "Joseph Smarr", "Huy Nguyen", "Christopher D Manning"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "No noun phrase left behind: Detecting and typing unlinkable entities", "author": ["Lin et al.2012] Thomas Lin", "Mausam", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S Weld"], "venue": "In AAAI", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["Ritter et al.2011] Alan Ritter", "Sam Clark", "Oren Etzioni"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery, 22(1-2):31\u201372", "author": ["Silla", "Freitas2011] Jr. Silla", "Carlos N", "Alex A. Freitas"], "venue": null, "citeRegEx": "Silla et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silla et al\\.", "year": 2011}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of the seventh conference on Natural language", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Distributed word clustering for large scale class-based language modeling in machine translation", "author": ["Uszkoreit", "Brants2008] Jakob Uszkoreit", "Thorsten Brants"], "venue": "In ACL,", "citeRegEx": "Uszkoreit et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Uszkoreit et al\\.", "year": 2008}, {"title": "Large margin taxonomy embedding with an application to document categorization", "author": ["Weinberger", "Chapelle2008] Kilian Weinberger", "Olivier Chapelle"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Weinberger et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2008}, {"title": "Ontonotes: A large training corpus for enhanced processing", "author": ["Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Robert Belvin", "S Pradan", "Lance Ramshaw", "Nianwen Xue"], "venue": "Handbook of Natural", "citeRegEx": "Weischedel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weischedel et al\\.", "year": 2011}, {"title": "HYENA: Hierarchical Type Classification for Entity Names", "author": ["Sandro Bauer", "Johannes Hoffart Marc Spaniol", "Gerhard Weikum"], "venue": "In Proc. of the 24 Intl. Conference on Computational Linguistics", "citeRegEx": "Yosef et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yosef et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "Type tagging is useful in a variety of related natural language tasks like coreference resolution and relation extraction, as well as for downstream processing like question answering (Lin et al., 2012).", "startOffset": 184, "endOffset": 202}, {"referenceID": 2, "context": "Most tagging systems only consider a small set of 3-18 type labels (Hirschman and Chinchor, 1997; Tjong Kim Sang and De Meulder, 2003; Doddington et al., 2004).", "startOffset": 67, "endOffset": 159}, {"referenceID": 14, "context": "labels derived from knowledge bases; for example, FIGER (Ling and Weld, 2012) uses 112 Freebase types, and HYENA Yosef et al. (2012) uses 505 YAGO types, which are Wikipedia categories mapped to WordNet synsets.", "startOffset": 113, "endOffset": 133}, {"referenceID": 13, "context": "This includes 11,304 manually annotated mentions in the OntoNotes test corpus (Weischedel et al., 2011).", "startOffset": 78, "endOffset": 103}, {"referenceID": 14, "context": "used in (Yosef et al., 2012) is also hierarchical, with 5 top-level types and 100 labels in each subcategory.", "startOffset": 8, "endOffset": 28}, {"referenceID": 8, "context": "(Ritter et al., 2011) in relation to entity recognition (with 10 types) for Twitter messages, and is addressed", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "We use a softmax classifier trained on labeled data derived from ACE (Doddington et al., 2004).", "startOffset": 69, "endOffset": 94}, {"referenceID": 4, "context": "features similar to those described in Klein et al. (2003). The motivation here is to reduce ambiguity by encouraging type labels to correspond to a single subtree of a hierarchy.", "startOffset": 39, "endOffset": 59}, {"referenceID": 0, "context": "In this approach, we train a flat softmax regression classifier (Berger et al., 1996) to discriminate between all possible types.", "startOffset": 64, "endOffset": 85}], "year": 2017, "abstractText": "Entity type tagging is the task of assigning category labels to each mention of an entity in a document. While standard systems focus on a small set of types, recent work (Ling and Weld, 2012) suggests that using a large fine-grained label set can lead to dramatic improvements in downstream tasks. In the absence of labeled training data, existing fine-grained tagging systems obtain examples automatically, using resolved entities and their types extracted from a knowledge base. However, since the appropriate type often depends on context (e.g. Washington could be tagged either as city or government), this procedure can result in spurious labels, leading to poorer generalization. We propose the task of context-dependent fine type tagging, where the set of acceptable labels for a mention is restricted to only those deducible from the local context (e.g. sentence or document). We introduce new resources for this task: 11,304 mentions annotated with their context-dependent fine types, and we provide baseline experimental results on this data.", "creator": "LaTeX with hyperref package"}}}