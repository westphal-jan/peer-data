{"id": "1412.1947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2014", "title": "A parallel sampling based clustering", "abstract": "The problem of automatically clustering data is an age old problem. People have created numerous algorithms to tackle this problem. The execution time of any of this algorithm grows with the number of input points and the number of cluster centers required. To reduce the number of input points we could average the points locally and use the means or the local centers as the input for clustering. However since the required number of local centers is very high, running the clustering algorithm on the entire dataset to obtain these representational points is very time consuming. To remedy this problem, in this paper we are proposing two subclustering schemes where by we subdivide the dataset into smaller sets and run the clustering algorithm on the smaller datasets to obtain the required number of datapoints to run our clustering algorithm with. As we are subdividing the given dataset, we could run clustering algorithm on each smaller piece of the dataset in parallel. We found that both parallel and serial execution of this method to be much faster than the original clustering algorithm and error in running the clustering algorithm on a reduced set to be very less.", "histories": [["v1", "Fri, 5 Dec 2014 10:50:31 GMT  (68kb,D)", "http://arxiv.org/abs/1412.1947v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aditya av sastry", "kalyan netti"], "accepted": false, "id": "1412.1947"}, "pdf": {"name": "1412.1947.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "In fact, the number of clusters we create is far too large to be manually categorized, no matter how large they may be. An automated system that can classify all types of data is performed by Chavent et al. [1] Monothecary clustering is performed by Chavent et al. [2], which is a dividing cluster. Possible mergers of C-Means clusters are used by Pal et al. [3] to calculate cluster membership."}, {"heading": "II. EQUAL SIZED SUBCLUSTERING", "text": "Instead, we select a few points that we will call milestones, and based on each similarity to a particular landmark, we collect the points in a subgroup. The similarity measure could be a distance measure such as the Euclidean distance, the Manhattan distance, or something else. A recursive algorithm for this would involve selecting a point that is farthest from all the points as a milestone and grouping all the points that resemble it into a subgroup and repeating this process from the omitted points. An iterative version of the algorithm is given below. Algorithm 1 Equal Sized Subclustering 1: Procedure SUBCLUSTER 2: Scale the features on all attributes. 3: Make a new point L, with each attribute having the most eloquent value of all the points for this attribute. 4: Collect N-points that come closest to the number of points in a sub-cluster. 5: Collect the points that are required to reach the number of points, from the number of 47.1."}, {"heading": "III. UNEQUAL SIZED SUBCLUSTERING", "text": "The problem with the above method is that the dataset has far too many outliers, resulting in some of the sub-clusters being filled only by the outlier points. To prevent us ideally wanting the landmarks to be in the dense regions of the dataset. To make sure that we take a point whose attributes are the lowest values for that attribute in our dataset, and then take a point whose attributes are the highest values for that attribute in our dataset. Then, we connect these two points with a line and divide the line into the required number of landmarks. Due to the similarity of each point to these landmarks, they will be grouped into a subcluster. Algorithm 2 unequal size of the sub-clusters 1: Procedure SUBCLUSTER 2: Perform a scaling of the attributes on all attributes. 3: Make a new point L with each attribute that has the lowest value of all attributes for that attribute."}, {"heading": "IV. CUDA", "text": "CUDA stands for Compute Unified Device Architecture. CUDA is a parallel computing platform and a programming model invented by NVIDIA. It allows for dramatic increases in computing power by harnessing the power of the graphics processor unit (GPU). Due to the rapid growth in performance of graphics cards, their ability to perform the same operations on multiple pieces of data at the same time, and recent improvements in their programmability, GPUs have become very lucrative devices for a wide range of applications that contain a lot of data. Lately, a lot of research has been presented to program GPUs for general computation.There are two parts of a CUDA application. The device part running on the GPU is called the kernel. Kernel is implemented in the CUDA programming language, which is extended by a number of keywords. A kernel is executed by multiple threads."}, {"heading": "V. PARALLEL IMPLEMENTATION", "text": "As we mentioned in the previous section, each CUDA application consists of two parts: the host part and the device part. In the host part, we divide the given record into the required number of subclusters using one of our proposed algorithms. The device part of the code is the clustering algorithm with each thread running on one of the sub-clusters of the original record. Once each thread ends execution and returns the captured points, we return to the host part of the application, where we execute the clustering algorithm on the sampled points to derive the required number of cluster centers. The input to our application is an MxN 2-dimensional array. Where M represents the number of data points and N represents the number of attributes, we run this 2-dimensional array through one of the sub-tribusting algorithms to obtain multiple 2-dimensional devices that we must transfer to the sub-region in front of the array."}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "We performed this algorithm on Iris [7] and Seeds [8] datasets for accuracy comparisons.Iris Seeds Standard Kmeans 133 187 Equal Partitioning Best performance 138 (6 subclusters 6 times compression) 191 (6 subclusters 6 times compression) Unequal Partitioning 138 (6 subclusters 6 times compression) 191 (6 subclusters 6 times compression) The Iris dataset has 150 datapoints with 3 classes and 4 attributes. The Seeds dataset has 210 datapoints with 3 classes and 7 attributes. We executed these algorithms on an NVIDIA TESLA C2075 GPU. On a system running Intel Xeon E5-1410 cpu with 3.2 GHZ and 48 GB RAM. Our test dataset was a two-dimensional synthetic dataset consisting of 100k, 250k, 500k elements. Each of these synthetic dataset contains 25,500 points for 25.5 GB of RAM."}], "references": [{"title": "A monothetic clustering method.", "author": ["Chavent", "Marie"], "venue": "Pattern Recognition Letters", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "A possibilistic fuzzy c-means clustering algorithm.", "author": ["Pal", "Nikhil R"], "venue": "Fuzzy Systems, IEEE Transactions on 13.4", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Chameleon: Hierarchical clustering using dynamic modeling.", "author": ["Karypis", "George", "Eui-Hong Han", "Vipin Kumar"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "On the performance of bisecting K-means and PDDP.", "author": ["Savaresi", "Sergio M", "Daniel L. Boley"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "K-means on commodity gpus with cuda.", "author": ["Hong-Tao", "Bai"], "venue": "Computer Science and Information Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Hierarchical Clustering with CUDA/GPU.", "author": ["Chang", "Dar-Jen", "Mehmed M. Kantardzic", "Ming Ouyang"], "venue": "ISCA PDCCS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images", "author": ["M. Charytanowicz", "J. Niewczas", "P. Kulczycki", "P.A. Kowalski", "S. Lukasik", "S. Zak"], "venue": "in: Information Technologies in Biomedicine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Various clustering algorithms were examined in Xu et al[1] The monothetic clustering algorithm or MONA proposed by Chavent et al[2] which is a divisive clustering algorithm.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Various clustering algorithms were examined in Xu et al[1] The monothetic clustering algorithm or MONA proposed by Chavent et al[2] which is a divisive clustering algorithm.", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "The possibilistic fuzzy C-Means clustering by Pal et al[3] uses the fuzzy approach to compute membership of each data point.", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Karypis et al[4] considers each datapoint as a subcluster initially and merges two subclusters if the proximity between two subclusters is higher than the internal interconnectivity of the points in a subcluster.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Savaresi et al[5] examimes the two famous divisive bisecting K means algorithms.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "We ran this algorithm on Iris[7] and Seeds[8] dataset for accuracy comparision.", "startOffset": 42, "endOffset": 45}], "year": 2014, "abstractText": "The problem of automatically clustering data is an age old problem. People have created numerous algorithms to tackle this problem. The execution time of any of this algorithm grows with the number of input points and the number of cluster centers required. To reduce the number of input points we could average the points locally and use the means or the local centers as the input for clustering. However since the required number of local centers is very high, running the clustering algorithm on the entire dataset to obtain these representational points is very time consuming. To remedy this problem, in this paper we are proposing two subclustering schemes where by we subdivide the dataset into smaller sets and run the clustering algorithm on the smaller datasets to obtain the required number of datapoints to run our clustering algorithm with. As we are subdividing the given dataset, we could run clustering algorithm on each smaller piece of the dataset in parallel. We found that both parallel and serial execution of this method to be much faster than the original clustering algorithm and error in running the clustering algorithm on a reduced set to be very less.", "creator": "LaTeX with hyperref package"}}}