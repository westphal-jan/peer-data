{"id": "1412.2106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2014", "title": "Consistent optimization of AMS by logistic loss minimization", "abstract": "In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, a threshold is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting (thresholded) classifier measured with respect to the squared AMS, is upperbounded by the regret of the underlying real-valued function measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS.", "histories": [["v1", "Fri, 5 Dec 2014 19:28:15 GMT  (13kb)", "http://arxiv.org/abs/1412.2106v1", "9 pages, HEPML workshop at NIPS 2014"]], "COMMENTS": "9 pages, HEPML workshop at NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wojciech kot{\\l}owski"], "accepted": false, "id": "1412.2106"}, "pdf": {"name": "1412.2106.pdf", "metadata": {"source": "CRF", "title": "Consistent optimization of AMS by logistic loss minimization", "authors": ["Wojciech Kot lowski", "Wojciech Kot"], "emails": ["wkotlowski@cs.put.poznan.pl"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.21 06v1 [cs.LG] 5 Dec 201 4from threshold f on \u03b8) measured in terms of the square AMS, is exceeded by regretting the measured f in terms of logistical loss. Therefore, we prove that minimizing logistical spare parts is a consistent method for optimizing the AMS. Keywords: Approximate Mean Significance (AMS), Higgs Boson Machine Learning Challenge, Kaggle, logistical loss, regret bound, statistical consistency."}, {"heading": "1. Introduction", "text": "This paper addresses a problem of learning a classification to optimize the median meaning (AMS), which was the goal of the AMS classification. (However, the goal of the AMS classification is to achieve the goal of the AMS classification.) In particular, we are interested in an approach to optimizing the AMS classification based on the following two-step approach. (Firstly, a real evaluated function f is learned by minimizing a replacement loss for binary classification, such as logistic loss function, at the second stage, since f, a threshold is matched to a separate \"validation\" achieved by directly optimizing the AMS classification by classifying all observations with a value above the threshold f as a positive class (signal event), and all observations below the threshold as a negative class (background event)."}, {"heading": "2. Problem Setting", "text": "In binary classification, the goal is, given an input (feature vector) x-X (b), to accurately predict the output (label) x-1, 1). We assume that the input-output pairs (x, y), which we call observations, i.i.d. according to Pr (x, y).2 A classifier is a figure h: X-1, 1). Given h, we define the following two quantities: s (h) = Pr (x) = 1, y = 1), b (h) = Pr (h) = 1, y = \u2212 1), which are considered true positive and false positive rates of h.2. The original HiggsML problem also concerned observations \"weights,\" but without loss of generality, they can be included in the distribution Pr (x, y).AMS and regret."}, {"heading": "3. Main Result", "text": "RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAMS-RAM"}, {"heading": "4. Proof of Lemma 1", "text": "The proof consists of two steps. First, we have the AMS regret of each classifier h = sensitive regret h = sensitive regret h = sensitive regret b = sensitive regret b = sensitive regret b = sensitive regret b = sensitive regret b = sensitive regret b = sensitive regret b (s). Next, we show that there is a real number c (0, 1) which has a cost-sensitive classification loss c: {\u2212 1, 1} \u00b7 {\u2212 1, 1} \u00b7 R + as: \"c (y, h) = c1 [s], there is a real number c (0, 1), 1 [y = 1], where 1 [A] the indicator function is the same if predicate A applies, and 0 otherwise. The cost-sensitive loss assign has different costs of misclassification for positive and negative labels."}, {"heading": "5. Generalization beyond AMS and logistic loss", "text": "The results of this work can be generalized beyond the metric and logistical losses of the AMS. AMS can be replaced by any other evaluation quantity which has the following two properties: 1) increases in s and increases in b; 2) is convex in s and b. These were the only two properties of the AMS to be used in the proof of Lemma. Logistic losses 5. rc (\u03b7, hf, \u03b8) = | \u03b7 \u2212 c | if (\u03b7 \u2212 c) (\u03b7 \u2212 c) < 0 and can be either 0 or | \u03b7 \u2212 c | if (\u03b7f \u2212 c) (\u03b7 \u2212 c) = 0, surrogate can be replaced by other convex losses, so that the following property applies: There is a threshold which is a function of cost c, so that for all f, Rc (hf \u2212 c) = 0, surrogate losses can be replaced by other constant losses."}, {"heading": "Acknowledgments", "text": "The author was supported by the Homing Plus Foundation for Polish Science, which is co-financed by the European Regional Development Fund. Krzysztof Dembczyn \u0301 ski thanks the author for the interesting discussions and proofreading of the essay."}], "references": [{"title": "Learning to discover: the Higgs boson machine learning", "author": ["Claire Adam-Bourdarios", "Glen Cowan", "C\u00e9cile Germain", "Isabelle Guyon", "Bal\u00e1zs K\u00e9gl", "David Rousseau"], "venue": null, "citeRegEx": "Adam.Bourdarios et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Adam.Bourdarios et al\\.", "year": 2014}, {"title": "Surrogate regret bounds for bipartite ranking via strongly proper losses", "author": ["Shivani Agarwal"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Agarwal.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal.", "year": 2014}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Asymptotic formulae for likelihood-based tests of new physics", "author": ["Glen Cowan", "Kyle Cranmer", "Eilam Gross", "Ofer Vitells"], "venue": "The European Physical Journal C-Particles and Fields,", "citeRegEx": "Cowan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cowan et al\\.", "year": 2011}, {"title": "Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Weighted classification cascades for optimizing discovery significance in the HiggsML challenge", "author": ["Lester Mackey", "Jordan Bryan"], "venue": "CoRR, abs/1409.2655,", "citeRegEx": "Mackey and Bryan.,? \\Q2014\\E", "shortCiteRegEx": "Mackey and Bryan.", "year": 2014}, {"title": "On the statistical consistency of plug-in classifiers for non-decomposable performance measures", "author": ["Harikrishna Narasimhan", "Rohit Vaish", "Shivani Agarwal"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Narasimhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2014}, {"title": "Consistent binary classification with generalized performance metrics", "author": ["Nagarajan Natarajan", "Oluwasanmi Koyejo", "Pradeep K. Ravikumar", "Inderjit S. Dhillon"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Natarajan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2014}, {"title": "Beyond Fano\u2019s inequality: Bounds on the optimal F-score, BER, and cost-sensitive risk and their implications", "author": ["Ming-Jie Zhao", "Narayanan Edakunni", "Adam Pocock", "Gavin Brown"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Introduction This paper concerns a problem of learning a classifier to optimize approximate median significance (AMS), which was the goal of the Higgs Boson Machine Learning Challenge (HiggsML), hosted by Kaggle website (see Adam-Bourdarios et al. (2014) for details on this contest and description of the problem).", "startOffset": 225, "endOffset": 255}, {"referenceID": 0, "context": "Introduction This paper concerns a problem of learning a classifier to optimize approximate median significance (AMS), which was the goal of the Higgs Boson Machine Learning Challenge (HiggsML), hosted by Kaggle website (see Adam-Bourdarios et al. (2014) for details on this contest and description of the problem). In particular, we are interested in an approach to optimize AMS, based on the following two-stage procedure. First, a real-valued function f is learned by minimizing a surrogate loss for binary classification, such as logistic loss function, on the training sample. In the second stage, given f , a threshold is tuned on a separate \u201cvalidation\u201d sample, by direct optimization of AMS with respect to a classifier obtained from f by classifying all observations with value of f above the threshold as positive class (signal event), and all observations below the threshold as negative class (background event). This approach became very popular among HiggsML challenge participants, mainly due to the fact that its first stage, learning a classifier, does not exploit the task evaluation metric (AMS) in any way and thus can employ without modifications any standard classification tools such as logistic regression, LogitBoost, Stochastic Gradient Boosting, Random Forest, etc. (see, e.g., Hastie et al. (2009)).", "startOffset": 225, "endOffset": 1326}, {"referenceID": 7, "context": "The issue of consistent optimization of performance measures which are functions of true positive and true negative rates has received increasing attention recently in machine learning community (Narasimhan et al., 2014; Natarajan et al., 2014; Zhao et al., 2013).", "startOffset": 195, "endOffset": 263}, {"referenceID": 8, "context": "The issue of consistent optimization of performance measures which are functions of true positive and true negative rates has received increasing attention recently in machine learning community (Narasimhan et al., 2014; Natarajan et al., 2014; Zhao et al., 2013).", "startOffset": 195, "endOffset": 263}, {"referenceID": 9, "context": "The issue of consistent optimization of performance measures which are functions of true positive and true negative rates has received increasing attention recently in machine learning community (Narasimhan et al., 2014; Natarajan et al., 2014; Zhao et al., 2013).", "startOffset": 195, "endOffset": 263}, {"referenceID": 6, "context": "Recently, Mackey and Bryan (2014) proposed a classification cascade approach to optimize AMS.", "startOffset": 10, "endOffset": 34}, {"referenceID": 4, "context": "Given a classifier h, define its approximate median significance (AMS) score (Cowan et al., 2011) as AMS(h) = AMS(s(h), b(h)), where:3", "startOffset": 77, "endOffset": 97}, {"referenceID": 5, "context": ", Hastie et al. (2009)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": "Comparing to the definition in (Adam-Bourdarios et al., 2014), we skip the regularization term breg.", "startOffset": 31, "endOffset": 61}, {"referenceID": 3, "context": "Boyd and Vandenberghe (2004)), where \u015d and b\u0302 are empirical counterparts of s and b.", "startOffset": 0, "endOffset": 29}, {"referenceID": 3, "context": "Any convex and differentiable function g(x) satisfies g(x) \u2265 g(y) +\u2207g(y)\u22a4(x \u2212 y) for any x, y in its convex domain (Boyd and Vandenberghe, 2004).", "startOffset": 115, "endOffset": 144}, {"referenceID": 2, "context": "This part relies on the techniques used by Bartlett et al. (2006). Then, the final bound is obtained by taking expectation with respect to x, and applying Jensen\u2019s inequality.", "startOffset": 43, "endOffset": 66}, {"referenceID": 1, "context": "We conjecture that all strongly proper composite losses (Agarwal, 2014) hold this property.", "startOffset": 56, "endOffset": 71}], "year": 2014, "abstractText": "In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function f is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, given f , a threshold \u03b8\u0302 is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting classifier (obtained from thresholding f on \u03b8\u0302) measured with respect to the squared AMS, is upperbounded by the regret of f measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS.", "creator": "LaTeX with hyperref package"}}}