{"id": "1412.2197", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2014", "title": "Practice in Synonym Extraction at Large Scale", "abstract": "Synonym extraction is an important task in natural language processing and often used as a submodule in query expansion, question answering and other applications. Automatic synonym extractor is highly preferred for large scale applications. Previous studies in synonym extraction are most limited to datasets in small scales. In this paper, we build a large dataset with 3.5 million synonym/non-synonym pairs to capture the challenges in real world scenarios. To overcome these challenges in large scale in synonym extraction, we proposed (1) a new cost function to accommodate the unbalanced learning problem, and (2) a feature learning based deep neural network to model the complicated relationships in synonym pairs. We compare several different approaches based on SVMs and neural networks, and find out our feature learning based neural network outperforms the methods with hand-assigned features. Specifically, the best performance of our model surpluses the SVM baseline with a significant 97\\% relative improvement.", "histories": [["v1", "Sat, 6 Dec 2014 04:40:18 GMT  (294kb,D)", "http://arxiv.org/abs/1412.2197v1", null], ["v2", "Thu, 18 Dec 2014 16:49:44 GMT  (294kb,D)", "http://arxiv.org/abs/1412.2197v2", null], ["v3", "Mon, 1 Jun 2015 19:55:17 GMT  (0kb,I)", "http://arxiv.org/abs/1412.2197v3", "This paper has been withdrawn by the author since the experimental results are not good enough"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["liangliang cao", "chang wang"], "accepted": false, "id": "1412.2197"}, "pdf": {"name": "1412.2197.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Liangliang Cao", "Chang Wang"], "emails": ["liangliang.cao@us.ibm.edu", "wangchan@us.ibm.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 DATASET", "text": "Our synonym dataset is obtained by extending the synonyms and antonyms from WordNet (Miller, 1995). After excluding some extremely unpopular words, we first obtain 35K synonyms and antonyms pairs. The ratio between synonyms and antonyms is about 10: 1. They are used as positive and negative examples in our experiments.In practice, we have to distinguish synonyms from a large number of irrelevant pairs. To model this phenomenon, we introduce an enormous amount of unrelated terms as negative pairs. Table 2 shows an example of the expansion of negative pairs. We choose the ratio of synonyms across the entire dataset as 1: 100. Finally, we obtain 3.5 million term pairs, of which 1% are positive and 99% negative, including antonyms and irrelevant. The motivation for building such an unbalanced dataset vis-\u00e0-\u00e0-vis the market is twofold: 1For applications with high demands on accuracy, a follow-up step is applied when looking at the symonyms."}, {"heading": "3 BASELINE AND CHALLENGES", "text": "This year it is so far that it will be able to retaliate, \"he says.\" It is very important that we are able to find a solution, \"he says.\" It is very important that we are able to find a solution that is able to find a solution, \"he says.\" It is very important that we are able to find a solution that is able to find a solution. \""}, {"heading": "4 NEW APPROACHES AND RESULTS", "text": "To explain our approach, we first designate a word pair represented as x1 and x2, and the label of that pair is y. before discussing our model, we would like to introduce our cost function: C (y, y) = {(y, y) 2 if (y, y). This cost function prefers estimates with good F1 values. However, unlike traditional MSE values, our new costs do not require a y value that is exactly + 1 or \u2212 1. Given a training sample with y \u2212 0 for y = 1 or y \u2212 < 0 for hyperfunctions that are considered unbalanced."}, {"heading": "4.1 NEURAL NETWORK WITH HAND-DESIGNED FEATURES", "text": "As shown in the previous section, the linear SVM cannot model the nonlinear relationship and does not work effectively on a large scale. On the other hand, nonlinear SVMs in large datasets are too expensive because empirically the number of support vectors increases with the number of training samples. Our first attempt is to use multi-layer perceptron (MLP) to replace SVM. After the large amount of previous work, we use a two-layer MLP with hyperbolic tangent function as an activation function as a classification model. Input of MLP can be of many possibilities. Suppose the input is a pair of words represented by x1 and x2. Indeed, the simplest representation is to consider the difference x1 \u2212 x2, or to use the absolute value in each dimension abs (x1 \u2212 x2). If we only use the difference as input, then the XLP is a non-linear mahalization of generational distance (x2) to x2 \u2212 x1 and \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "4.2 DEEP NEURAL NETWORK WITH FEATURES LEARNING", "text": "In this work, we propose an automatic approach to learn the property of property representation, rather than classify it in a nonlinear space and then use it as input of a multilayered perceptronik. As shown in Figure 4.2, we first build a neural network that can accommodate a pair of input characteristics and automatically learn a set of relationships. These learned characteristics are classified in a nonlinear space and then used as input of a multilayered perceptronik. Our learned characteristics can be depicted in the following form: f (wax1 + wbx2), wx2 (x2, x2), x3), 4, 5, 5, 5, 6, 6, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "5 CONCLUSION", "text": "This paper describes our efforts to extract synonyms in large environments. We use distribution representation to represent pairs of words and propose a feature-learning neural network with a new cost function to recognize synonyms. Experimental results from a data set of 3.5 million synonyms / non-synonyms suggest that our approach exceeds the hand-assigned characteristics. Our best performance improves the SVM baseline by relative 97%. Our future work is to merge synonym extraction in QA systems in different areas."}], "references": [{"title": "A neural probabilistic language mode", "author": ["Bengio", "Yoshua", "Ducharme", "Rejean", "Vincent", "Pascal", "Janvin", "Christian"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Query expansion based on pseudo relevance feedback from definition clusters", "author": ["Bernhard", "Delphine"], "venue": "In COLING, pp", "citeRegEx": "Bernhard and Delphine.,? \\Q2010\\E", "shortCiteRegEx": "Bernhard and Delphine.", "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML, pp", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Building watson: An overview of the deepqa project", "author": ["Ferrucci", "David", "Brown", "Eric", "Chu-Carroll", "Jennifer", "Fan", "James", "Gondek", "Kalyanpur", "Aditya A", "Lally", "Adam", "Murdock", "J William", "Nyberg", "Prager", "John"], "venue": "AI magazine,", "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Introduction to \u201dthis is watson", "author": ["Ferrucci", "David A"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Ferrucci and A.,? \\Q2012\\E", "shortCiteRegEx": "Ferrucci and A.", "year": 2012}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Synonym extraction and abbreviation expansion with ensembles of semantic spaces", "author": ["Henriksson", "Aron", "Moen", "Hans", "Skeppstedt", "Maria", "Daudaravicius", "Vidas", "Duneld", "Martin"], "venue": "J. Biomedical Semantics,", "citeRegEx": "Henriksson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henriksson et al\\.", "year": 2014}, {"title": "Word embeddings through Hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In EACL,", "citeRegEx": "Lebret and Collobert,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2014}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning locally-adaptive decision functions for person verification", "author": ["Li", "Zhen", "Chang", "Shiyu", "Liang", "Feng", "Huang", "Thomas S", "Cao", "Liangliang", "Smith", "John R"], "venue": "In CVPR,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In ICLR,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "tau Yih", "Wen", "Zweig", "Geoffrey"], "venue": "NAACL HLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "In ICML,", "citeRegEx": "Mnih and Teh,? \\Q2012\\E", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Extraction of synonyms and semantically related words from chat logs", "author": ["Norlindh", "Fredrik"], "venue": null, "citeRegEx": "Norlindh and Fredrik.,? \\Q2012\\E", "shortCiteRegEx": "Norlindh and Fredrik.", "year": 2012}, {"title": "Learning representations by backpropagating", "author": ["Rumelhart", "David E", "Hintont", "Geoffrey E", "Williams", "Ronald J"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher", "Richard", "Lin", "Cliff C", "Ng", "Andrew Y", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Peter D", "Pantel", "Patrick"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Extracting synonyms from dictionary definitions", "author": ["Wang", "Tong", "Hirst", "Graeme"], "venue": "In RANLP, pp", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 9, "context": "\u2022 Clinical text: In health and related applications, we also need match different terms used in books and clinical realities (Henriksson et al., 2014).", "startOffset": 125, "endOffset": 150}, {"referenceID": 6, "context": "Ferrucci et al. (2010) showed it is possible to analyzes natural language questions and content well enough and fast enough to compete and win against champion players at Jeopardy! game.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "The experiments in (Henriksson et al., 2014) are limited to TOFEL synonym test with 80 questions.", "startOffset": 19, "endOffset": 44}, {"referenceID": 9, "context": "Henriksson et al. (2014) studied synonym in medical datasets.", "startOffset": 0, "endOffset": 25}, {"referenceID": 9, "context": "Henriksson et al. (2014) studied synonym in medical datasets. One shared limitation in these studies lies in the fact that the dataset is very small. The medical synonym datasets used by Wang & Hirst (2009) is limited 340 synonym pairs.", "startOffset": 0, "endOffset": 207}, {"referenceID": 9, "context": "Henriksson et al. (2014) studied synonym in medical datasets. One shared limitation in these studies lies in the fact that the dataset is very small. The medical synonym datasets used by Wang & Hirst (2009) is limited 340 synonym pairs. The experiments in (Henriksson et al., 2014) are limited to TOFEL synonym test with 80 questions. Collobert & Weston (2008) considered synonym, as a related task to improve the performance of semantic role labeling, but did not report the performance of synonym extraction in large scales.", "startOffset": 0, "endOffset": 361}, {"referenceID": 0, "context": "This idea has since been applied to statistical language modeling (Bengio et al., 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 8, "context": ", 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al., 2011) (Socher et al.", "startOffset": 170, "endOffset": 191}, {"referenceID": 19, "context": ", 2011) (Socher et al., 2011) (Mnih & Teh, 2012) (Lebret & Collobert, 2014).", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "A number of researcher (Bengio et al., 2003) have employed recurrent neural network to learn a distributed representations of words, which encode many linguistic regularities and patterns in a vector space (Mikolov et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 13, "context": "One of the earliest use of word representations dates back to 1986 due to Rumelhart et al. (1986). This idea has since been applied to statistical language modeling (Bengio et al.", "startOffset": 74, "endOffset": 98}, {"referenceID": 0, "context": "This idea has since been applied to statistical language modeling (Bengio et al., 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al., 2011) (Socher et al., 2011) (Mnih & Teh, 2012) (Lebret & Collobert, 2014). The idea of using distributed representation is to explore the co-occurrence of words and phrases to represent semantics (Turney & Pantel, 2010). A number of researcher (Bengio et al., 2003) have employed recurrent neural network to learn a distributed representations of words, which encode many linguistic regularities and patterns in a vector space (Mikolov et al., 2013b). For example, the result of a vector calculation vec(Madrid) - vec(Spain) + vec(France) is closer to vec(Paris) than to any other word vector (Mikolov et al., 2013b). Although the recurrent neural network can achieve good performance in natural language processing tasks, not all of them are scalable for very large corpus. We prefer an efficient method for arbitrary size of documents, so we employ the Skip-gram model proposed by (Mikolov et al., 2013a) which learn word vectors efficiently by a two layer network from the context words in a sentences. The implementation of Mikolov et al. (2013a) is extremely efficient: we can learn vector representations for billions of words from the whole Wikipedia corpus on a server with 5 CPUs in two hours.", "startOffset": 67, "endOffset": 1317}, {"referenceID": 0, "context": "This idea has since been applied to statistical language modeling (Bengio et al., 2003), which motivated a number of research in speech recognition and machine translation, as well as a wide range of NLP tasks, for example (Collobert & Weston, 2008) (Glorot et al., 2011) (Socher et al., 2011) (Mnih & Teh, 2012) (Lebret & Collobert, 2014). The idea of using distributed representation is to explore the co-occurrence of words and phrases to represent semantics (Turney & Pantel, 2010). A number of researcher (Bengio et al., 2003) have employed recurrent neural network to learn a distributed representations of words, which encode many linguistic regularities and patterns in a vector space (Mikolov et al., 2013b). For example, the result of a vector calculation vec(Madrid) - vec(Spain) + vec(France) is closer to vec(Paris) than to any other word vector (Mikolov et al., 2013b). Although the recurrent neural network can achieve good performance in natural language processing tasks, not all of them are scalable for very large corpus. We prefer an efficient method for arbitrary size of documents, so we employ the Skip-gram model proposed by (Mikolov et al., 2013a) which learn word vectors efficiently by a two layer network from the context words in a sentences. The implementation of Mikolov et al. (2013a) is extremely efficient: we can learn vector representations for billions of words from the whole Wikipedia corpus on a server with 5 CPUs in two hours. We choose dimension=100 for the word representation. The unsupervised learning of distributed representation helps us to develop a synonym classifier for either general purpose or a specified domain. We can first learn the vector representation from a large corpus, and use this presentation as features to learn a synonym extractor. Our baseline is to train a linear SVM with the distributed representation. In practice, we use the liblinear package by Fan et al. (2008). To handle the unbalanced problem of positive pairs, we use a sampling weight of 100 for positive class in the 3.", "startOffset": 67, "endOffset": 1941}, {"referenceID": 12, "context": "As suggested by Li et al. (2013), such models in fact impose a global decision threshold without considering the local characteristics of x1 or x2.", "startOffset": 16, "endOffset": 33}, {"referenceID": 3, "context": "We have seen a number of successful examples including unsupervised learning (Coates et al., 2011) or supervised convolutional network (LeCun et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 11, "context": ", 2011) or supervised convolutional network (LeCun et al., 1998).", "startOffset": 44, "endOffset": 64}, {"referenceID": 1, "context": "Our algorithm is implementation based on Theano (Bergstra et al., 2010), where the layer definition is illustrated as Algorithm 1.", "startOffset": 48, "endOffset": 71}], "year": 2014, "abstractText": "Synonym extraction is an important task in natural language processing and often used as a submodule in query expansion, question answering and other applications. Automatic synonym extractor is highly preferred for large scale applications. Previous studies in synonym extraction are most limited to datasets in small scales. In this paper, we build a large dataset with 3.5 million synonym/nonsynonym pairs to capture the challenges in real world scenarios. To overcome these challenges in large scale in synonym extraction, we proposed (1) a new cost function to accommodate the unbalanced learning problem, and (2) a feature learning based deep neural network to model the complicated relationships in synonym pairs. We compare several different approaches based on SVMs and neural networks, and find out our feature learning based neural network outperforms the methods with hand-assigned features. Specifically, the best performance of our model surpluses the SVM baseline with a significant 97% relative improvement.", "creator": "LaTeX with hyperref package"}}}