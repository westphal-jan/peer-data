{"id": "1412.2620", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2014", "title": "Cells in Multidimensional Recurrent Neural Networks", "abstract": "The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.", "histories": [["v1", "Mon, 8 Dec 2014 15:47:45 GMT  (51kb)", "https://arxiv.org/abs/1412.2620v1", null], ["v2", "Tue, 16 Feb 2016 12:26:37 GMT  (61kb,D)", "http://arxiv.org/abs/1412.2620v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["g leifert", "t strau{\\ss}", "t gr\\\"uning", "r labahn"], "accepted": false, "id": "1412.2620"}, "pdf": {"name": "1412.2620.pdf", "metadata": {"source": "CRF", "title": "Cells in Multidimensional Recurrent Neural Networks", "authors": ["Gundram Leifert", "Tobias Strau\u00df", "Tobias Gr\u00fcning", "Welf Wustlich", "Roger Labahn", "GUNDRAM LEIFERT"], "emails": ["GUNDRAM.LEIFERT@UNI-ROSTOCK.DE", "TOBIAS.STRAUSS@UNI-ROSTOCK.DE", "TOBIAS.GRUENING@UNI-ROSTOCK.DE", "WELF.WUSTLICH@UNI-ROSTOCK.DE", "ROGER.LABAHN@UNI-ROSTOCK.DE"], "sections": [{"heading": "Cells in Multidimensional Recurrent Neural Networks", "text": "Gundram Leifert GUNDRAM.LEIFERT @ UNI-ROSTOCK.DE Tobias Strauss TOBIAS.STRAUSS @ UNI-ROSTOCK.DE Tobias Gruening TOBIAS.GRUENING @ UNI-ROSTOCK.DE Welf Wustlich WELF.WUSTLICH @ UNI-ROSTOCK.DE Roger Labahn ROGER.LABAHN @ UNI-ROSTOCK.DE"}, {"heading": "University of Rostock Institute of Mathematics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "18051 Rostock, Germany", "text": "Editor: Yoshua BengioKeywords: LSTM, MDRNN, CTC, Handwriting Recognition, Neural Network AbstractThe transcription of handwritten text on images is a task in machine learning and one solution to its solution is the use of multidimensional recursive neural networks (MDRNN) with connectionistic time classification (CTC).The RNNNs can contain special units, the long-term memory cells (LSTM).They are able to learn long-term dependencies, but they become unstable when the dimension is greater than one. We have defined some useful and necessary properties for the one-dimensional LSTM cell and expand them in the multidimensional case. Thus, we introduce several new cells with better stability. We present a method to design cells with the help of the theory of linear displacement of invariant systems. The new cells will be compared with the LSTM cell on the FIR / RIT database, where we can compare the data base with the LIMN."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "2. Previous Work", "text": "In this section, we will briefly introduce a recursive neural network (RNN) and the evolution of the LSTM cell. In the previous literature, there are various notations to describe the updated equations of RNNNs and LSTMs. To unify the notations, we refer to their notation, \"(F. A. Gers and Cummins, 1999; S. Hochreiter, 1997; Graves and Schmidhuber, 2008). Therefore, we focus on a simple hierarchical RNN with an input layer containing the set of neurons I, a recursive hidden layer containing the set of neurons H and an output layer containing the set of neurons O. For each time step t-N, the layers will be asynchronous in the order I, H, O. In a specific layer, all neurons can be updated synchronously. In the hidden layer for a neuron c, c, at the time t-N, we calculate the network input c."}, {"heading": "2.1 The Long Short-Term Memory", "text": "A standard LSTM has two gates: the entrance gate (IG) and the exit gate (OG), both of which are calculated as one unit (see S. Hochreiter, 1997).The standard LSTM cell has two gates: the entrance gate (IG) and the exit gate (IG).The standard LSTM cell has two gates: the entrance gate (IG) and the exit gate (IG) and the exit gate (OG).These two gates are calculated as one unit (see S. Hochreiter, 1997).The standard LSTM cell has two gates: the entrance gate (IG) and the exit gate (OG)."}, {"heading": "2.2 Learning to Forget", "text": "For long time series, the internal state is unlimited (compare with F. A. Gers and Cummins, 1999, 2.1). Assuming a positive or negative input and a non-zero activation of the IG, the absolute activation of the internal state grows over time. Using the weightspace symmetries in a network with at least one hidden layer (Bishop, 2006, 5.1.1), we assume that without loss of generality ycin (t) \u2265 0, so that sc (t) t \u2192 is insoluble. Therefore, the activation function hc saturates and (3) can be simplified toyc (t) = hc (sc) explosively (t). The gradients coming out of production multiply by the activation of sc (t) the whole LSTM functions like a unit with a logistic activation function. A similar problem can be observed for the gradients."}, {"heading": "3. Cells and Their Properties", "text": "In this section we want to introduce a general cell update and find out the properties of these cells, which are likely to lead to the good performance observed by LSTM cells. (Definition 2 (Cell, cf.) A cell, c, which we know to have a certain input unit, cin, with sigmoid function fc (typically fc = tanh). (Definition 2) A cell that has no cell activation function is a cell. (Definition 2) A cell that does not have a cell activation function. (Typically logically fc = flew) a cell, unless there is an arbitrary function, and a cell activation function, gout. (Definition) Each unit of the cell receives the same amount of input activations. (The cell is carried out in three consecutive phases: 1. According to the classical scheme of neurons (see Section 2) we will activate all of this cell."}, {"heading": "4. Expanding to More Dimensions", "text": "In A: Graves and Schmidhuber (2007) the 1D LSTM cell is extended to any number of dimensions (= q = 1). This is solved by using one FG cell for each dimension. - In many publications that use the MD-LSTM cell in MDRNs, the absolute value of the cell can grow faster than linearly over time. - If we exceed the state of the detection systems, there are peephole connections (for the peephole connection see F. A. Gers and Schmidhuber, 2002). - The cells have an output of the ypc cell {\u2212 1, 1}: The internal state multiplied by the peephole weight of the other activation products and this leads to an activation of the OG cell."}, {"heading": "5. Reducing the MD LSTM Cell to One Dimension", "text": "In the last section, we showed that the MD LSTM cell can have an exploding gradient. To solve this problem, we tried different methods: For example, we divided the activation of the FG by the number of dimensions. Then, the gradient cannot explode over time, but the gradient disappears quickly along some paths. Another approach was to allow the cells to stabilize when the internal state begins to diverge. Therefore, we added an additional peephole connection between the query value of the previous internal states (s p \u2212 d c) 2 and the FG, so that the cell can learn to close the FG for large internal states. Again, this does not make a significant difference. Also, forcing the cell to stabilize itself by adding an error-free state = \u03b5, i.e. a spatial pwith p = {1, 2, 3, 4} and different learning rates. So we tried to change the layout of the MLD cell."}, {"heading": "5.1 MD LSTM Stable Cell", "text": "In section 3 we realized that 1D LSTM cells work well and the gradient does not explode, but in the MD case we do. Our idea is to combine the previous states s \u2212 d at the time p \u2212 to a previous state sp \u2212 c and choose the 1D form of the LSTM cell. Therefore, we call this cell LSTM-stable cell. Therefore, we need a function p \u2212 c = f (s p \u2212 1 c,., s p \u2212 D c) so that the following two advantages of the 1D LSTM cell remain: 1. The MD LSTM-stable cell has a NEG2. The MD LSTM-stable cell allows NVG.The convex combinations p \u2212 c = f (s p \u2212 1 c,.,.,. s p \u2212 D) gates remain: 1. The MD-LSTM-stable cell has a NEG2."}, {"heading": "6. Bounding the Internal State", "text": "In the last sections we discussed the growth of the EC over time and we found a solution to have an NGEC for higher dimensions. However, it is possible that the internal state grows linearly over time. If we take a look at definition 10, we see that the partial derivative for p = pout depends on h \u2032 c (spc). So it is possible that the inequality between p and p (spc) can be activated with h \u2032 c (spc). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 0the cell allows NVG defined in definition 11, but in fact we have y pout c."}, {"heading": "7. General Derivation of Leaky Cells", "text": "So far, we have proposed cells in the MD case that are able to provide long-term memory. Another way to view the cell as a kind of MD feature extractor is to view it as \"feature maps\" in Convolutionary Neural Networks (Bengio and LeCun, 1995), and then the goal is to construct an MD cell that is capable of generating useful features. A hierarchical neural network, as in Bengio and LeCun (1995) and Graves and Schmidhuber (2008), across the hierarchies, increases the number of features while decreasing the feature resolution. Features in a low-resolution layer can be viewed as low-frequency features compared to features in a high-resolution layer. Therefore, it would be useful to construct a cell as a feature extractor that generates a low frequency."}, {"heading": "8. Experiments", "text": "RNNs with 1D-LSTM cells are well studied. In some experiments, the activation of the gates and the internal state are observed and one can see that the cell can really learn when one \"forgets\" information and when the internal state should be accessible to the network (see F. A. Gers and Schmidhuber, 2002). However, we have not found experiments like this for the MD case and we do not want to transfer these experiments to the MD case. Instead, we compare the different cell types with each other in two scenarios where the MD-RNNs do very well with LSTM cells. In both benchmarks, the task is to transfer a handwritten text to an image, so we have a 2D-RNN. In this case, we compare the cells on the IFN / ENIT (Pechwitz et al., 2002) and the Rimes database (Augustin et al., 2006). Both tasks are solved with the MD-RNN layout."}, {"heading": "8.1 The IFN/ENIT Database", "text": "This database contains handwritten names of Tunisian towns and villages. The set is divided into 7 (a-f, s) sets, of which 5 (a-e) are available for training and validation (for details see Pechwitz et al., 2002). With all the information we received from A. Graves, we were able to obtain results comparable to Graves and Schmidhuber (2008). For this purpose, we divide the sets a-e into 30,000 training samples and 2493 validation samples. All networks will have 100 epochs with a fixed learning rate \u03b4 = 1 \u00b7 10 \u2212 4. The LER is calculated on the basis of the validation set."}, {"heading": "8.1.1 DIFFERENT CELLS IN THE LOWEST MD LAYER", "text": "In our first experiment, we replace the LSTM cell in the lowest MD layer. We take some of the cells described in this article. In Table 1, the results are shown. The first row is the same RNN layout as in Graves and Schmidhuber (2008). We can see that the LeakyLP cell performs best. Nevertheless, the worst RNN cell with LeakyLP cells in the lowest MD layer is more valuable than the best RNN cell with LSTM cells. Therefore, we cannot say that LeakyLP is always better. However, it can be observed that the deviation of RNN performance in LSTM cells in the lowest MD layer is very high. Our interpretation is that LSTM cells perform as well as the LeakyLP cells in the lowest layer if they are not saturated."}, {"heading": "8.1.2 DIFFERENT CELLS IN OTHER MD LAYERS", "text": "Now we want to compare the most developed cell - the LeakyLP cell - with the LSTM cell in the other MD layers. Therefore, we also replace the LSTM cell in the upper MD layers. We enumerate the 2D layers, as shown in Figure 4. In Table 2, we see that the replacement of the LSTM cells only works slightly better in the bottom layer or in the two bottom layers. We get the best results when we use LeakyLP cells in 2D layer 1 and LSTM cells in 2D layer 3. Using LSTM in the middle layer seems to work slightly better than using the LeakyLP cells. This corresponds to our already mentioned intuition that the LSTM cells function better when they do not have too long time series and when there are enough cells in a layer that are not saturated."}, {"heading": "8.1.3 PERFORMANCE OF CELLS REGARDING LEARNING-RATE", "text": "If we look at the updating equations and the evidence of the NEG, we can assume that the gradient that passes through the cells is lower for LeakyLP cells than for LSTM cells. Therefore, we think that the learning rate for LeakyLP cells must be higher. In Table 3, we compare the networks with either LSTM or LeakyLP cells. There, we see that the learning rate for LeakyLP cells must be much higher. In addition, the RNNs with LeakyLP cells are more robust in choosing the learning rate."}, {"heading": "8.2 The Rimes Database", "text": "One task of the Rimes database is handwritten word recognition (see E. Grosicki and Geoffrois, 2008; Grosicki and El-Abed, 2011), containing 59292 images of single French words. It is divided into different subsets; a training set of 44196 samples, a validation set of 7542 samples, and a test set of 7464 samples. We train the MD RNNNs by using the training set for training and calculating the LER via the validation set, so that the network is trained on 44196 training samples per epoch. The network used in this section differs only in the subsampling rate between two layers from the network used in Graves and Schmidhuber (2008)."}, {"heading": "8.2.1 DIFFERENT CELLS IN THE LOWEST MD LAYER", "text": "In Table 4 we can see that replacing the LSTM in the bottom layer with one of the three cells improves the performance of the network, even the leaky cell with one less gate."}, {"heading": "8.2.2 DIFFERENT CELLS IN OTHER MD LAYERS", "text": "We want to see the effect of replacing the LSTM cell with the LeakyLP cell in the upper MD layers. In Table 5, we can see that the use of LeakyLP cells in the bottom two layers works very well. Therefore, we also use this setup to try out different learning rates, the performance of cells in terms of the learning rate. Based on different learning rates, we can see that the RNN with LeakyLP cells in the bottom two layers and the LSTM cells in the top layer can significantly improve performance."}, {"heading": "9. Conclusion", "text": "In this article, we looked at the one-dimensional LSTM cell and discussed the advantages of this cell. We found two characteristics that probably make these cells so powerful in a one-dimensional case. In extending these characteristics to the multi-dimensional case, we saw that the LSTM cell no longer fulfills any of these characteristics. We solved this problem by changing the architecture of the cell. We also presented a more general idea of how to create one-dimensional or multi-dimensional cells. We compared some newly developed cells with the LSTM cell on two datasets and we can improve performance with the new cell types. Therefore, we believe that replacing the multi-dimensional LSTM cell with the multi-dimensional LeakyLP cell could improve the performance of many systems operating in multi-dimensional space."}, {"heading": "Appendix A. Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of 7", "text": "Proof Let c be a 1D LSTM cell. To obtain the derived results, we must take a closer look at the derived results (t2). (t1). (t1). (t1). (t1). (t2). (t2). (t1). (t1). (t2). (t1). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t1). (t1). (t1). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t1). (t1). (t1). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2). (t2)."}, {"heading": "A.2 Proof of 14", "text": "Proof Let c be an MD LSTM cell of dimension D, p, p1, p2, pin, pout, pout, pout, pout, pout, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pout, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pout, pin, pin, pin, pin, pin, pout, pin, pin, pin, pin, pout, pin, pin, pin, pin, pin, pout, pin, pin, pin, pin, pin"}, {"heading": "A.3 Proof of 15", "text": "Evidence that c is an MD cell of dimension D with the internal state sc and the pin, pk and ND, pin and pk two data. Let pk be a date k, which in each dimension goes further than a fixed date pin. So the distance between both is the set of all pin-pk paths, then there are paths (see definition 8). We assume that d-pin, d-pin, 1 \u2212 \u03b5 with the upper pin-pk path (0, 0.5) and we can estimate the partial derivation by using the severed gradient pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-value pin-pin-pin-pin-pin-pin-pin-value pin-pin-pin-k-pin-k). (So we can estimate the partial derivative by using the severed gradient pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-value pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-pin-k-pin-value pin-pin-pin-pin-pin-k)."}, {"heading": "A.4 Proof of 17", "text": "Proof Let c be an MD LSTM stable cell of dimension D \u2265 2 (for D = 1) the proof is equivalent to the 1D case of the LSTM cell), p \u2212 \u2212 p, p2, pin, pout, pout, pout, pout, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin, pin"}, {"heading": "A.5 Proof of 18", "text": "Proof Let c be an MD LSTM stable cell of dimension D with the internal state sc and pin, p, p, ND, pin \u2264 p two arbitrary data and pin \u2212 p, 1 = k. Let all gate activations in [0, 1] be arbitrary. We show that pin c, 0, 1, (33) is fulfilled by induction via k. For the base case k = 0 we get pin c = x, s pin c, s pin c, s pin c, s pin c = 1. Let (33) be fulfilled for k \u2212 1. That is, if p \u2212 d, s, s, p \u2212 d, s, p \u2212 pin, 1 = k \u2212 1 and this leads to s, p \u2212 d, s, s, s, s, s, s, s, s, c, tr [0, 1]."}, {"heading": "A.6 Proof of 20", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof", "text": "NEG: The cell has an NEG, because all gates have the same boundaries as the MD stable cell. NVG: To prove the NVG, we use the proof for theorem 17. The difference between the MD stable cell and the MD leaky cell is that the activations of the FG and IG for the Leaky cell are dependent on each other. Let the FG have an arbitrary activation, so that we have the p-y-p-p-cell as in theorem 17. The IG only has the restriction that it must hold for p = pin y p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "Appendix B. Theory to Create First Order MD Cells", "text": "If you want to take a closer look at the theory of linear shift invariants (LSI) systems and their frequency analysis and analyze a first-order LSI system with respect to its freely selectable parameters based on the F and Z transformation, it is highly recommended to be familiar with these theories (for a good overview and further details see Poularikas, 2000; Schlichth\u00e4rle, 2000). If you add the knowledge about the reduction of the MD case to the 1D case (see Section 5), new cell types arise for the MD case."}, {"heading": "B.1 Analysing a First Order LSI-System", "text": "The updated equations of a first order LSI system with an input u = 36 () (Y) (answer) (Z) (Z) (Z) are we (Z) = 37 (Z) (Z) (Z): The updated equations of a first order LSI system with an input u = 36 (Z) (Z) = 37 (Z) (Z) can we (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 1 (Z) = 0 (Z) = 0 (Z), Y (z) = 1) (Z), Z (Z). Then we can write the so-called transfer functions H1 (z): = X (z) U (z) = 0 (Z) = 1 \u2212 1z (Z) \u2212 z (Z): 1 (Z) \u2212 z \u2212 z \u2212 z \u2212 z \u2212 z."}, {"heading": "B.2 Creating a First Order Cell", "text": "With these parameter constraints, we can now define a new cell type. \u03b10, \u03b11, b0, b1 should be activations of gates as in LSTM cells. We must find the right activation functions to fulfill the above inequalities. To fulfill the boundaries for H1, we use \u03b11 as activation of a gate with activation function flog (Bishop, 2006, 5.1.1), without loss of generality, we use \u03b10, \u03b1p \u2212 \u2212 b0, b1 \u2265 0. To fulfill the boundaries for H1, we use \u03b11 as activation of a gate with activation function. Thus, we have \u03b11 (0, 1).This is comparable to the FG in the previous sections. To select the \u03b10 \u2212 b0, we choose \u03b10: = 1 \u2212 \u03b11 (see (38)). Thus, the value of the gate is comparable to the activation of the IG. For H1z, we set both values b0, b1 as activation of a gate with max activation function that flew."}, {"heading": "B.3 General First Order MD Cells", "text": "With the theory of this section, we can easily create new cell types. In general, a cell has a number of gates \u03b31, \u03b32,,,,,,,,,, for D = 1 a previous state sp \u2212 c is directly given. Otherwise, the previous state is calculated as a traceable convex combination of the previous states D, as in Section 5. Table 7 shows cell layouts, where type A is the cell developed in Section 7 (see (39)). For the other types, we will briefly describe the main ideas."}, {"heading": "B.3.1 THE MD BUTTERWORTH LOWPASS FILTER", "text": "If we set yp\u03c90 = y p \u03c91 = 0.5, there is a direct correlation between the cutoff frequency of a discrete Butterworth lowpass filter and the activation of yp\u03c6: Let fcutoff be the frequency at which the amplitude response is reduced to 1 \u221a 2 of the maximum gain. We can calculate fcutoff byfcutoff = 1\u03c0 arctan (1 \u2212 yp\u03c6 1 + yp\u03c6) (42) \u21d4 yp\u03c6 = 1 + tan (\u03c0fcutoff) 1 \u2212 tan (\u03c0fcutoff) using the limits fcutoff \u03b7 (0, 0.5) and ypvis (\u2212 1, 1) (for further details see Simplifying hardness, 2000, 2,2; 6,4.2). At yp\u03c6 (0, 1) we get fcutoff (0, 0,25).In Figure 7 (left) we see that even at a negative value of yp\u03c6 and a high pass, the H1 has the lowest characteristic for the reaction."}, {"heading": "B.3.2 ADDING AN ADDITIONAL STATE GATE", "text": "In B.2, we have fulfilled (38) for the MD LeakyLP cell by setting \u03b10: = 1 \u2212 \u03b11 so that \u03b10 is directly connected to \u03b11. Another solution would be to add an additional value \u03b10 (0, 1) and choose \u03b10: = \u03b3 (1 \u2212 \u03b11). Thus, we can extend the MD LeakyLP cell by adding an additional gate \u03b34 for the previous state (see type C). Unfortunately, this does not lead to better performance and another gate has to be calculated."}, {"heading": "B.3.3 ANOTHER SOLUTION FOR THE OUTPUT", "text": "The type D cell is another solution to select b0 and b1 in Section B.2. For the LeakyLP cell, we calculate the output as described in (41). Now, we set b0 = \u03b3 p 2\u03b3 p 3 and b1 = (1 \u2212 \u03b3p2) \u03b3p3 and getypc = \u03b3 p 3 (\u03b3p2s p c + (1 \u2212 \u03b3p2) sp \u2212 c). This cell actually works as well as the MD LeakyLP cell and has the same number of gates. In this case, we do not need a squeeze function hc because we already have y p c [\u2212 1, 1]."}, {"heading": "B.3.4 AN MD CELL AS MD PID-CONTROLLER", "text": "Type E has a completely different interpretation: In controlling the technique, a PID controller receives an error as input. In our case, the gate activations must decide whether the proportional (P), the integral (I) or the derivative (D) concept of the error is relevant for the output. When activating the gate, we have y p cin \u2248 s p c, so that the internal state is proportional to the input. Then, \u03b3p2 regulates the proportional part (P) of the input. The second gate \u03b3p3 regulates the difference between the last and the current input, which can be regarded as a discrete derivative (D). In the internal state change p 1 \u2248 1, the exponential sliding average of y p cin, which is an integral term. Thus, the second gate \u03b3p3 switches on a primarily integral part of the input (I), while conversely p 3 regulates a predominantly proportional part of the input (P). Depending on the respective type E controller, a PD (mainly a component of both types of input) may be a PD (7)."}], "references": [{"title": "Multi-dimensional recurrent neural networks", "author": ["S. Fernandez A. Graves", "J. Schmidhuber"], "venue": "Technical report,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2007\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2007}, {"title": "RIMES evaluation campaign for handwritten mail processing", "author": ["E. Augustin", "J.-M Brodin", "M. Carr\u00e9", "E. Geoffrois", "E. Grosicki", "F. Pr\u00eateux"], "venue": "In Proc. of the Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "Augustin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Augustin et al\\.", "year": 2006}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "CoRR, abs/1206.5533,", "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Y. Bengio", "Y. LeCun"], "venue": null, "citeRegEx": "Bengio and LeCun.,? \\Q1995\\E", "shortCiteRegEx": "Bengio and LeCun.", "year": 1995}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Pattern Recognition and Mashine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "RIMES evaluation campaign for handwritten mail processing", "author": ["J.-M. Brodin E. Grosicki", "M. Carr\u00e9", "E. Geoffrois"], "venue": "In Proc. of the Int. Conf. on Frontiers in Handwriting Recognition,", "citeRegEx": "Grosicki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grosicki et al\\.", "year": 2008}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["J. Schmidhuber F.A. Gers", "F. Cummins"], "venue": "Technical report,", "citeRegEx": "Gers and Cummins.,? \\Q1999\\E", "shortCiteRegEx": "Gers and Cummins.", "year": 1999}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["N. Schraudolph F.A. Gers", "J. Schmidhuber"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gers and Schmidhuber.,? \\Q2002\\E", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2002}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2008\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2008}, {"title": "ICDAR 2011: French handwriting recognition competition", "author": ["E. Grosicki", "H. El-Abed"], "venue": "In Proc. of the Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "Grosicki and El.Abed.,? \\Q2011\\E", "shortCiteRegEx": "Grosicki and El.Abed.", "year": 2011}, {"title": "Ifn/enit-database of handwritten arabic words", "author": ["M. Pechwitz", "S. Maddouri", "V. M\u00e4rgner", "N. Ellouze", "H. Amiri"], "venue": "In Proc. of CIFED,", "citeRegEx": "Pechwitz et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pechwitz et al\\.", "year": 2002}, {"title": "The Transforms and Applications Handbook", "author": ["A.D. Poularikas"], "venue": "CRC Press, 2. edition edition,", "citeRegEx": "Poularikas.,? \\Q2000\\E", "shortCiteRegEx": "Poularikas.", "year": 2000}, {"title": "Long short-term memory", "author": ["J. Schmidhuber S. Hochreiter"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter.", "year": 1997}], "referenceMentions": [{"referenceID": 10, "context": "Hochreiter (1997) the authors develop the long short-term memory (LSTM) which is able to have a long term dependency.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Graves and Schmidhuber (2007) to the multi-dimensional (MD) case and is used in a hierarchical multi-dimensional RNN (MDRNN) which performed best in three competitions at the International Conference on Document Analysis and Recognition (ICDAR) in 2009 without any feature extraction and knowledge of the recognized language model.", "startOffset": 0, "endOffset": 30}, {"referenceID": 9, "context": "To unify the notations we will refer to their notation using \u201c,\u201d (F. A. Gers and Cummins, 1999; S. Hochreiter, 1997; Graves and Schmidhuber, 2008).", "startOffset": 65, "endOffset": 146}, {"referenceID": 12, "context": "Hochreiter (1997) and F.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Gers and Cummins (1999). Definition 1 (truncated gradient) Let \u03b3 \u2208 {cin, \u03b9, \u03c9} be any input or gate unit and yc(t\u2212 1) any previous output activation.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Gers and Cummins (1999). They tried to stabilize the LSTM with a \u201cstate decay\u201d by multiplying the internal state in each time step with a value \u2208 (0, 1), which did not improve the performance.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "In this paper we denote the Extended LSTM as LSTM Another point of view was introduce in Bengio et al. (1994): To learn long-term dependencies a system must have an architecture to that an input can be saved over long time and does not suffer from the \u201cvanishing gradient\u201d problem.", "startOffset": 89, "endOffset": 110}, {"referenceID": 0, "context": "Graves and Schmidhuber (2007) the 1D LSTM cell is extended to an arbitrary number of dimensions; this is solved by using one FG for each dimension.", "startOffset": 0, "endOffset": 30}, {"referenceID": 3, "context": "Another way to interpret the cell is to consider them as kind of MD feature extractor like \u201cfeature maps\u201d in Convolutional Neural Networks (Bengio and LeCun, 1995).", "startOffset": 139, "endOffset": 163}, {"referenceID": 1, "context": "Another way to interpret the cell is to consider them as kind of MD feature extractor like \u201cfeature maps\u201d in Convolutional Neural Networks (Bengio and LeCun, 1995). Then the aim is to construct an MD cell which is able to generate useful features. Having a hierarchical Neural Network like in Bengio and LeCun (1995) and Graves and Schmidhuber (2008) over the hierarchies the number of features increases with a simultaneously decreasing feature resolution.", "startOffset": 140, "endOffset": 317}, {"referenceID": 0, "context": "Having a hierarchical Neural Network like in Bengio and LeCun (1995) and Graves and Schmidhuber (2008) over the hierarchies the number of features increases with a simultaneously decreasing feature resolution.", "startOffset": 73, "endOffset": 103}, {"referenceID": 11, "context": "In this case we compare the cells on the IFN/ENIT (Pechwitz et al., 2002) and the Rimes database (Augustin et al.", "startOffset": 50, "endOffset": 73}, {"referenceID": 1, "context": ", 2002) and the Rimes database (Augustin et al., 2006).", "startOffset": 31, "endOffset": 54}, {"referenceID": 0, "context": "Both tasks are solved with the MD RNN layout described in Graves and Schmidhuber (2008) and shown in Figure 4.", "startOffset": 58, "endOffset": 88}, {"referenceID": 0, "context": "Graves, we were able to get comparable results to Graves and Schmidhuber (2008). Therefor we divide the sets a-e into 30000 training samples and 2493 validation samples.", "startOffset": 50, "endOffset": 80}, {"referenceID": 0, "context": "The first row is the same RNN layout used in Graves and Schmidhuber (2008). We can see, that the LeakyLP cell performs the best.", "startOffset": 45, "endOffset": 75}, {"referenceID": 10, "context": "One task of the Rimes database is the handwritten word recognition (for more details see E. Grosicki and Geoffrois, 2008; Grosicki and El-Abed, 2011).", "startOffset": 67, "endOffset": 149}, {"referenceID": 0, "context": "The network used in this section differs only in the subsampling rate between two layers from the network used in Graves and Schmidhuber (2008). When there is a subsampling between layers, the factors are 3 \u00d7 2 instead of 4 \u00d7 3 or 4 \u00d7 2.", "startOffset": 114, "endOffset": 144}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}