{"id": "1412.2672", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2014", "title": "When Computer Vision Gazes at Cognition", "abstract": "Joint attention is a core, early-developing form of social interaction. It is based on our ability to discriminate the third party objects that other people are looking at. While it has been shown that people can accurately determine whether another person is looking directly at them versus away, little is known about human ability to discriminate a third person gaze directed towards objects that are further away, especially in unconstraint cases where the looker can move her head and eyes freely. In this paper we address this question by jointly exploring human psychophysics and a cognitively motivated computer vision model, which can detect the 3D direction of gaze from 2D face images. The synthesis of behavioral study and computer vision yields several interesting discoveries. (1) Human accuracy of discriminating targets 8{\\deg}-10{\\deg} of visual angle apart is around 40% in a free looking gaze task; (2) The ability to interpret gaze of different lookers vary dramatically; (3) This variance can be captured by the computational model; (4) Human outperforms the current model significantly. These results collectively show that the acuity of human joint attention is indeed highly impressive, given the computational challenge of the natural looking task. Moreover, the gap between human and model performance, as well as the variability of gaze interpretation across different lookers, require further understanding of the underlying mechanisms utilized by humans for this challenging task.", "histories": [["v1", "Mon, 8 Dec 2014 17:25:57 GMT  (3866kb)", "http://arxiv.org/abs/1412.2672v1", "Tao Gao and Daniel Harari contributed equally to this work"]], "COMMENTS": "Tao Gao and Daniel Harari contributed equally to this work", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["tao gao", "daniel harari", "joshua tenenbaum", "shimon ullman"], "accepted": false, "id": "1412.2672"}, "pdf": {"name": "1412.2672.pdf", "metadata": {"source": "CRF", "title": "When Computer Vision Gazes at Cognition", "authors": ["Tao Gao", "Daniel Harari", "Joshua Tenenbaum", "Shimon Ullman"], "emails": [], "sections": [{"heading": null, "text": "Shared attention is a central, early evolving form of social interaction, based on our ability to distinguish third-party objects that other people are looking at. Although it has been shown that people can accurately determine whether or not another person is looking at them directly, little is known about a person's ability to distinguish a third-person's gaze from objects that are farther away, especially in unrestricted cases where the viewer can move his head and eyes freely. In this paper, we explore this question by jointly exploring human psychophysics and a cognitively motivated computer vision model that can recognize the 3D direction of vision from 2D facial images. The synthesis of behavioral science and computer vision yields several interesting discoveries. (1) Human accuracy in distinguishing objectives between 8 \u00b0 -10 \u00b0 of the angle of view is about 40% for a free-looking eye task; (2) The ability to interpret the gaze of different viewers can be captured by the computer-based model."}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 .1 Unders ta nd ing so c ia l a g ency in rea l scenes", "text": "As a social species, people are remarkably good at understanding other people's mental states based solely on visual perception. Recent work on social perception has been done in the 2D toy world with cartoon-like characters (e.g. [1], [2]). Meanwhile, there is a growing interest in understanding non-verbal social interactions in real scenes. Many developmental studies have shown that even toddlers can understand other people's mental states by observing their nonverbal actions [3] - [5]. However, this kind of visual social understanding poses a challenge for both cognitive science and computer vision, because most human social interactions take place in real scenes where agents act freely. However, due to the complex interactions between the 3D environment and human actions, analyzing these scenes can be extremely challenging."}, {"heading": "1 .2 The percept io n o f \" free\" g a ze", "text": "In order to achieve the long-term goal of understanding social actions in real scenes, a first step is needed to interpret a person's perspective, which is a window on their mental states, and this ability is the foundation of common human attention and many other important social interactions. [8] However, there are several interesting opposites in terms of the development of communication and language, as early as 3-6 months of age. [9] The view of perception has been extensively studied in human perception."}, {"heading": "1 .3 Co ntra s t ing Hu ma n a nd Co mp uter Vis io n Per fo r ma nce", "text": "While it is certainly important to have a threshold for the sharpness of the human gaze that follows skill, we also hope to evaluate human performance by comparing it with modern computer vision models, which is a critical component of this study, as we would like to know how difficult the perception of the natural gaze is actually. Researchers do not usually have a good intuition about the difficulty of a cognitive process until they start to reverse it. For example, people on the street may feel that solving a differential equation is much smarter than acquiring common sense via a three-year-year-old girl. In fact, the latter is much harder for a machine to learn [21]. From the perspective of cognitive studies, we would like to focus more on aspects of social perception where the machine still cannot achieve performance on a human level."}, {"heading": "2 Experimental Sett ings", "text": "On the other side of the table (121cm), four human observers sit on chairs facing the observer. In each study, the observer looks at an object by following a command displayed on the screen of a laptop behind the observer. The action scene is recorded by a Microsoft Kinect sensor with RGB-D cameras (Figure 1). In this setup, the observer looks at the object array, the Kinect sensor and the command computer are all aligned in order so that the observer stands directly in front of the camera between the two experiments. The task of the observer is to follow the observer's gaze and write down his best guesses of the observer. The object field is created as a concentric configuration with two pillars consisting of 4 objects (width: 4.6cm)."}, {"heading": "3 Appearance-based computational model", "text": "In fact, you will be able to put yourself in a situation where you have to put yourself at the centre of attention."}, {"heading": "4 Results", "text": "The overall performance of people and models is shown in Figure 3 (left). Performance is primarily measured as the percentage of studies in which people or models get both the correct target row and the position of the column. Figure 3 (right) shows the percentage of studies in which the errors are no larger than a column or row, corresponding to about 12 \u00b0 viewing angles around the true target. Results reflect several interesting patterns: (1) people outperform all models, suggesting that people's perception is actually highly efficient. (2) There is a significant amount of discrepancy between viewers. Both people and models are much better at reading the gaze of JP than the rest of viewers, who show that the models actually capture some intrinsic discrepancies in human performance. (3) Most errors are within a 12 \u00b0 range."}, {"heading": "B ia s a nd Va r ia nce", "text": "Here, we examined the results further by analyzing human distributions and modeling responses to a particular goal. Two summarizing statistics are selected: bias and standard deviation (SD). For both humans and models, poor performance could be due to a large systematic bias or a large variance.Bias measures the mean of the response in relation to the true position. For the column dimension (left-right), a positive bias is defined, as the mean tends to the viewer's peripheral field of view.For the line dimension (up-down), a positive bias means that the perceived viewing direction moves downward (farther from the resting direction).Table 1 shows the bias and deviation along the left-right dimension. What we really notice is that the perceived viewing direction moves downward (farther from the resting direction of the viewer).Table 1 shows the bias and deviation along the left-right dimension, and the deviation along the left-right dimension are considered to be relatively impressionable."}, {"heading": "5 Discussion", "text": "In this paper, we examined the sharpness of the gaze of others in a \"free-looking\" task with both human participants and computer vision models. The guidelines for this study are to keep the ecological validity as high as possible while maintaining quantitative precision. Ecological validity is emphasized, since the results should serve to understand social interactions in real scenarios. A simple result is that the human accuracy of distinguishing between targets of 8 \u00b0 -10 \u00b0 of the viewing angle for a free-looking visual task is about 40%. However, the most difficult part of this project is not to obtain this accuracy, but to interpret it. By comparing with several computer vision models, it turns out that human visual perception achieves a remarkable level of performance, taking into account the associated computer-related challenge. More interesting is the comparison of the model performance with the eye-visible and the eye-invisible server, where the eyes-invisible information is represented by the invisible, while we found that the visual conditions are likely to be paradoxical."}, {"heading": "Ac kno w ledg me nts", "text": "This material is based on work supported by the Center for Brains, Minds and Machines (CBMM) and funded by the NSF STC award CCF-1231216 and NSF National Robotics Intuitive."}], "references": [{"title": "The psychophysics of chasing: A case study in the perception of animacy", "author": ["T. Gao", "G.E. Newman", "B.J. Scholl"], "venue": "Cogn. Psychol., vol. 59, no. 2, pp. 154\u201379, Sep. 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Action understanding as inverse planning", "author": ["C.L. Baker", "R. Saxe", "J.B. Tenenbaum"], "venue": "Cognition, vol. 113, no. 3, pp. 329\u201349, Dec. 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "The cultural origins of human cognition", "author": ["M. Tomasello"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "The capacity for joint visual attention in the infant", "author": ["M. Scaife", "J.S. Bruner"], "venue": "Nature, vol. 253, pp. 265\u2013266, 1975.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1975}, {"title": "A demonstration of gaze following in 3- to 6month-olds", "author": ["B. D\u2019Entremont", "S.M.J. Hains", "D.W. Muir"], "venue": "Infant Behav Dev, vol. 20, no. 4, pp. 569\u2013572, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Does the chimpanzee have a theory of mind", "author": ["D. Premack", "G. Woodruff"], "venue": "Behav. Brain Sci., 1978.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1978}, {"title": "Understanding other minds: linking developmental psychology and functional neuroimaging", "author": ["R. Saxe", "S. Carey", "N. Kanwisher"], "venue": "Annu. Rev. Psychol., vol. 55, pp. 87\u2013124, Jan. 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Gaze-following: It\u2019s development and significance", "author": ["R. Flom", "K. Lee", "D. Muir"], "venue": "Mahwah: Lawrence Erlbaum Associates,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Perception of another person\u2019s looking behavior", "author": ["J. Gibson", "A. Pick"], "venue": "Am. J. Psychol., 1963.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1963}, {"title": "The perception of where a person is looking", "author": ["M. Cline"], "venue": "Am. J. Psychol., 1967.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}, {"title": "What are you looking at? Acuity for triadic eye gaze", "author": ["L.A. Symons", "K. Lee", "C.C. Cedrone", "M. Nishimura"], "venue": "J. Gen. ..., vol. 131, no. 4, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Are you looking at me? Neural correlates of gaze adaptation", "author": ["S.R. Schweinberger", "N. Kloth", "R. Jenkins"], "venue": "Neuroreport, vol. 18, no. 7, pp. 693\u20136, May 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Geometrical basis of perception of gaze direction", "author": ["D. Todorovi\u0107"], "venue": "Vision Res., vol. 46, no. 21, pp. 3549\u201362, Oct. 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptation to vergent and averted eye gaze", "author": ["B. Stiel", "C. Clifford", "I. Mareschal"], "venue": "J. Vis., 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "How precise is gaze following in humans", "author": ["S.W. Bock", "P. Dicke", "P. Thier"], "venue": "Vision Res., vol. 48, no. 7, pp. 946\u201357, Mar. 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Head pose estimation in computer vision: A survey", "author": ["E. Murphy-Chutorian", "M.M. Trivedi"], "venue": "Pattern Anal. Mach. Intell. IEEE Trans., vol. 31, no. 4, pp. 607\u2013626, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "In the eye of the beholder: a survey of models for eyes and gaze", "author": ["D.W. Hansen", "Q. Ji"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 3, pp. 478\u2013500, Mar. 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Determining the gaze of faces in images", "author": ["A. Gee", "R. Cipolla"], "venue": "Image Vis. Comput., vol. 12, no. 10, pp. 639\u2013647, Dec. 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Person Independent 3D Gaze Estimation From Remote RGB-D Cameras", "author": ["J. Odobez", "K.F. Mora"], "venue": "Int. Conf. Image Process., 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "From simple innate biases to complex visual concepts", "author": ["S. Ullman", "D. Harari", "N. Dorfman"], "venue": "Proc. Natl. Acad. Sci., vol. 109, no. 44, pp. 18215\u201318220, Sep. 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1821}, {"title": "Histograms of Oriented Gradients for Human Detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proceedings of Computer Vision and Pattern Recognition, 2005, pp. 886\u2013893.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Person Independent 3D Gaze Estimation From Remote RGB-D Cameras", "author": ["J. Odobez", "K.F. Mora"], "venue": "Int. Conf. Image Process., 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", [1], [2]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": ", [1], [2]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Many developmental studies have demonstrated that even young infant can understand others' mental states by observing their non-verbal actions [3]\u2013[5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "Many developmental studies have demonstrated that even young infant can understand others' mental states by observing their non-verbal actions [3]\u2013[5].", "startOffset": 147, "endOffset": 150}, {"referenceID": 5, "context": "building a vision model that can track the \u2018false-belief\u2019 of an observed person, given what this person can or cannot see in the visual scene [6], [7]).", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "building a vision model that can track the \u2018false-belief\u2019 of an observed person, given what this person can or cannot see in the visual scene [6], [7]).", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "Infants begin to develop this ability [8], which plays an important role in development of communication and language [3], as early as at the age of 3-6 months [5].", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "Infants begin to develop this ability [8], which plays an important role in development of communication and language [3], as early as at the age of 3-6 months [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 4, "context": "Infants begin to develop this ability [8], which plays an important role in development of communication and language [3], as early as at the age of 3-6 months [5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "Gaze perception has been extensively studied in human perception since 1960s [9]\u2013[15].", "startOffset": 77, "endOffset": 80}, {"referenceID": 14, "context": "Gaze perception has been extensively studied in human perception since 1960s [9]\u2013[15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "First, most behavioral studies have been focusing on judging whether the gaze is directly towards the observers or not [9], [12].", "startOffset": 119, "endOffset": 122}, {"referenceID": 11, "context": "First, most behavioral studies have been focusing on judging whether the gaze is directly towards the observers or not [9], [12].", "startOffset": 124, "endOffset": 128}, {"referenceID": 8, "context": "The acuity of detecting direct eye-contact can be as high as 3\u00b0 of visual angle [9], [10].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "The acuity of detecting direct eye-contact can be as high as 3\u00b0 of visual angle [9], [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": ", in [10], a circle of 12\u00b0 of visual angle around the observer), which only covers a very small the space in a scene.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "However, in most research on discriminating gaze perception, the threshold is measured by asking the looker to look at an empty space [9], [10].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "However, in most research on discriminating gaze perception, the threshold is measured by asking the looker to look at an empty space [9], [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "In the field of computer vision, head pose and eye gaze estimations have been studied for a long time [17]\u2013[19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "In the field of computer vision, head pose and eye gaze estimations have been studied for a long time [17]\u2013[19].", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "A recent work [20] suggested to estimate gaze under free-head movements by combining depth and visual data from a Microsoft Kinect sensor.", "startOffset": 14, "endOffset": 18}, {"referenceID": 19, "context": "The Appear-Face-Eyes model is based on a gaze detection method used in [22], in which grayscale image patches of faces were associated with the 2D direction vector of the gaze (the vector connecting the face center to the regarded target position in the 2D image) .", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "We use Histograms of Gradients descriptors (HoG, [23]) for the representation of the face and eyes appearance, and quaternions for representing the 3D orientations.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "For models, this performance is very good compared with current reported accuracy on a similar task [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "[9], [10]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[9], [10]).", "startOffset": 5, "endOffset": 9}], "year": 2014, "abstractText": "Joint attention is a core, early-developing form of social interaction. It is based on our ability to discriminate the third party objects that other people are looking at. While it has been shown that people can accurately determine whether another person is looking directly at them versus away, little is known about human ability to discriminate a third person gaze directed towards objects that are further away, especially in unconstraint cases where the looker can move her head and eyes freely. In this paper we address this question by jointly exploring human psychophysics and a cognitively motivated computer vision model, which can detect the 3D direction of gaze from 2D face images. The synthesis of behavioral study and computer vision yields several interesting discoveries. (1) Human accuracy of discriminating targets 8\u00b0-10\u00b0 of visual angle apart is around 40% in a free looking gaze task; (2) The ability to interpret gaze of different lookers vary dramatically; (3) This variance can be captured by the computational model; (4) Human outperforms the current model significantly. These results collectively show that the acuity of human joint attention is indeed highly impressive, given the computational challenge of the natural looking task. Moreover, the gap between human and model performance, as well as the variability of gaze interpretation across different lookers, require further understanding of the underlying mechanisms utilized by humans for this challenging task. * Tao Gao and Daniel Harari contributed equally to this work. When Computer Vision Gazes at Cognition Tao Gao\u2217, Daniel Harari\u2217 \u2020, Joshua Tenenbaum\u2217, Shimon Ullman\u2217 \u2020 \u2217 Center for Brains, Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA \u2020 Weizmann Institute of Science, Rehovot, Israel", "creator": "pdfsam-console (Ver. 2.4.3e)"}}}