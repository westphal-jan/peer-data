{"id": "1412.2824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2014", "title": "Plan or not: Remote Human-robot Teaming with Incomplete Task Information", "abstract": "Human-robot interaction can be divided into two categories based on the physical distance between the human and robot: remote and proximal. In proximal interaction, the human and robot often engage in close coordination; in remote interaction, the human and robot are less coupled due to communication constraints. As a result, providing automation for the robot in remote interaction becomes more important. Thus far, human factor studies on automation in remote human-robot interaction have been restricted to various forms of supervision, in which the robot is essentially being used as a smart mobile manipulation platform with sensing capabilities. In this paper, we investigate the incorporation of general planning capability into the robot to facilitate peer-to-peer human-robot teaming, in which the human and robot are viewed as teammates that are physically separated. The human and robot share the same global goal and collaborate to achieve it. Note that humans may feel uncomfortable at such robot autonomy, which can potentially reduce teaming performance. One important difference between peer-to-peer teaming and supervised teaming is that an autonomous robot in peer-to-peer teaming can achieve the goal alone when the task information is completely specified. However, incompleteness often exists, which implies information asymmetry. While information asymmetry can be desirable sometimes, it may also lead to the robot choosing improper actions that negatively influence the teaming performance. We aim to investigate the various trade-offs, e.g., mental workload and situation awareness, between these two types of remote human-robot teaming.", "histories": [["v1", "Tue, 9 Dec 2014 01:05:59 GMT  (3162kb,D)", "http://arxiv.org/abs/1412.2824v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["vignesh narayanan", "yu zhang", "nathaniel mendoza", "subbarao kambhampati"], "accepted": false, "id": "1412.2824"}, "pdf": {"name": "1412.2824.pdf", "metadata": {"source": "CRF", "title": "Plan or not: Remote Human-robot Teaming with Incomplete Task Information", "authors": [], "emails": [], "sections": [{"heading": null, "text": "Keywords Robot design principles, Autonomous robot skills, User study / evaluation"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is so that most people who are able to determine themselves are not able to determine themselves, are able to determine themselves, \"he said.\" But it is not as if, \"he said.\" It is as if, \"he said.\" But it is as if, \"he said.\" But it is as if, \"he said.\" \"It is not as if.\" \"\". \"\" \"It is as if.\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\". \"\" \"\" \".\" \"\". \".\". \"\" \"\". \".\". \"\". \".\". \"\". \"\" \".\" \".\". \"\". \".\". \"\". \"\". \".\" \"\". \"\". \".\" \"\". \"\" \"\". \"\". \"\". \".\" \".\". \".\". \"\" \".\". \".\" \".\" \".\". \".\". \".\". \".\". \".\" \"\". \".\". \".\". \"\". \"\". \".\". \"\". \".\". \".\". \".\". \".\" \".\" \".\". \".\". \"\". \".\" \"\". \".\". \".\". \".\". \".\". \".\". \".\". \"\". \".\". \"\". \".\". \".\". \".\". \".\". \".\". \"\". \".\". \"\". \"\" \".\". \".\" \".\". \".\" \".\". \".\" \".\". \"\". \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\""}, {"heading": "2. RELATED WORK", "text": "Considering the proximity between humans and robots, proximal interaction tends to be rich and multimodal. For example, it has been studied how speech (e.g., [23]), looks (e.g., [26]) and gestures (e.g., [17]) are used, as well as their relationships during interaction (e.g., [27]). Furthermore, researchers have also investigated how humans can be viewed during proximal interaction for security reasons (e.g., [25]), how human preferences influence interaction (e.g., [29]). Meanwhile, in human-robot interaction, humans and robots are often less coupled. Considering communication limitations (e.g. communication bandwidth, reliability and modalities), we are more important in remote interaction due to the physical separation, automation of the robot."}, {"heading": "2.1 Background", "text": "When task information is a priori fully specified, a task can be compiled in a problem instance for an automated planner. Depending on the task, there are many extensions of the PDDL (e.g. [8, 9]) that include different modeling requirements. In this paper, we use the extension of the PDDL described in [8] to model a USAR task. While the use of an automated planner allows an automated agent to think directly about the global goal, the limitation stems from the assumption of completeness. In terms of this assumption, completeness often cannot be guaranteed (e.g. dynamic environmental changes cannot be modeled or predicted), but there are various types of incompatibility that can be taken into account in an automated planner."}, {"heading": "2.2 Hypotheses", "text": "We compare peer-to-peer teaming, in which robots have a general planning capability, with supervised teaming, which lacks this capability. In particular, we hypothesize that: H1) Planning-capable robots offer more natural teaming experience.H2) Planning-capable robots reduce the mental workload of human teammates; on the other hand, it also reduces awareness of situations. H3) Planning-capable robots gradually reduce the interaction between humans and robots."}, {"heading": "3. STUDY DESIGN", "text": "Our study focuses on the comparison of robot teams with (i.e. peer-to-peer) and without (i.e. supervised) planning capacities in human-robot teams, which help in the preparation of situation assessments during urban search and rescue (USAR) tasks."}, {"heading": "3.1 Environment", "text": "Fig. 1 is the simulated environment (created in webs) used in our USAR task, which represents the layout of an office building before a disaster (e.g., a fire) occurs. The environment is divided into segments, and each segment is marked with a unique marking (e.g., R11) in Fig. 1. In addition, the segments representing rooms are grouped into four regions, with each region containing four rooms. For example, one of the regions designated as R1 contains four rooms R11, R12, R13 and R14. Each region is accessible via a door connected to a flood segment, and the rooms in each region are also connected by doors. Initially, the doors are closed and can be opened by the robot, the doors remain open after opening. The robot companion works within this environment and the human teammate interacts with it remotely. The environment is designed so that no essential computing skills are required to eliminate the impact of this ability on performance."}, {"heading": "3.2 Task Specification", "text": "The global goal of this USAR task is to simulate the number of victims in as many rooms as possible, with a certain amount of time available. This means simulating that the human-robot team has only a limited amount of time to perform the situation assessment in a disaster response scenario. The robot starts in the position indicated by a red circle in Fig. 1. In this task, both the human and the robot have access to the floor plan prior to the disaster, and thus both can independently determine the order in which they plan the rooms to be reported. In the meantime, the incomplete task information may be the result of blocked doors and other environmental changes. To simulate this incompleteness, we design the doors within the regions (connecting adjacent rooms) so that some of them are blocked. Note that the room with a blocked door would be accessible through the other door, so it would still be of interest to report them."}, {"heading": "3.3 Interface Design", "text": "In order to create a more realistic USAR environment, the human teammate only has access to the visual feeds of the robot teammate. In other words, the human can only observe that part of the environment through the eyes of the robot teammate (i.e., cameras). The interaction interface with the robot is shown in Fig. 3. The interaction interface between human and robot teammate is the same for the robot with or without planning ability. In both cases, the robot teammate would display a list of applicable actions that he can perform given the current state. However, note that the degree of automation [20] that can be supported between these two cases varies. In the first case (robot with planning ability), the robot teammate knows exactly the sequence of actions to achieve the global goal; in the second case, the robot teammate can only filter actions that are not applicable given the current state."}, {"heading": "3.4 Experimental Setup Process", "text": "The experiment was conducted in our laboratory and participants were assigned to either a robot with planning capability or a robot without a USAR task. Each participant was only allowed to take part in one experiment to avoid performance fluctuations based on their experience, and all participants completed the consent form before participating in the study. Before each run, participants were asked to read the teaching materials, and were then confronted with the simulator and the user interface and asked to experiment with them in order to become a little familiar.During each of the actual experiments, participants were provided with a map of the simulation environment (as shown in Fig. 1) to create an awareness of the initial state (i.e. the initial position of the robot on the map).The participant was also given the secondary questions and a separate answer sheet to complete the answers. The participant was asked to report as many rooms with the robot as possible within 20 minutes. In addition to the interaction with the robot, the test participants were assigned to one secondary task and one secondary task at a time."}, {"heading": "4. RESULTS", "text": "The study was conducted over 4 weeks and involved 19 volunteers (11 men, 8 women), the volunteers having an age of M = 24.47 and SD = 1.07. These participants were recruited by students on campus. Due to the requirement to understand English instructions, participants must indicate that they are familiar with English communication skills before participating in the study. We also asked participants about their familiarity with computers (M = 6.68, SD = 0.48), robots (M = 2.74, SD = 0.73), puzzles for the secondary task (M = 3.58, SD = 1.02) and computer games (M = 3.79, SD = 1.18), in seven-point scales (with 1 being least familiar and 7 being most familiar) according to the study. Participants reported familiarity with computers but not so much with robots, puzzles for the secondary task or computer game."}, {"heading": "4.2 Objective Performance", "text": "This year it has come to the point that it will be able to erenie.n the aforementioned lcihsrteeSe rf\u00fc eid eerwtlrteeeirsrteeGe"}, {"heading": "4.3 Subjective Performance", "text": "This year, the time has come for us to be able to assert ourselves, to be able to change the world, and to be able to change the world in order to change it."}, {"heading": "4.4 Summary", "text": "In summary, our results are partially consistent with our hypotheses. In general, participants seemed to prefer to engage in peer-to-peer teamwork and task performance was higher than supervised teaming. However, our expectation is that peer-to-peer teamwork cannot reduce mental workload for short-term tasks, which is understandable since working with a proactive teammate requires more interaction at the outset. Once mutual understanding is established, mental workload could be reduced. In our USAR task, the reduction is not significant given the short time span of our task. The outcome in terms of situation awareness is also interesting. Given that situation awareness is not significantly reduced in peer-to-peer teaming, it seems to suggest that the provision of \"recommended actions\" can also be used as a way to reduce the negative impact of automation."}, {"heading": "5. LIMITATIONS AND FUTURE WORK", "text": "One of the most obvious limitations of this work is the simplicity of the peer-domain model of planning, which is a result of the simplification of the USAR task scenario that we considered. Specifically, the USAR task scenario that we test our hypotheses with includes only three types of action: Move, Push and Report. Although these actions can be considered abstract actions that involve complex action sequences, real-world USAR scenarios and other scenarios often involve significantly more action. This can lead to delays in the robot's planning capability that needs to be investigated. On the other hand, due to the increased complexity, we can expect the planning system to offer more benefits than supervised teaming, especially when partial goals cannot be easily identified (e.g. when deciding the best sequence of visits to locations to ensure timely coverage of the task in a complex environment). Meanwhile, an appropriate planning explanation and excuse methodology to incorporate automation is essential."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we examine the inclusion of general planning skills in robots to facilitate peer-to-peer teamwork, in which humans and robots are considered physically separate teammates. Humans and robots share the same global goal and work together to achieve it. One of the important differences between peer-to-peer teaming and supervised teamwork is that in peer-to-peer teaming, the robot can only achieve the goal if task information is fully specified. However, incompleteness often occurs, which implies information asymmetry. While information asymmetry can sometimes be desirable, it can also cause the robot to choose inappropriate actions that can negatively affect team performance. We show that human teammates generally prefer to work in peer-to-peer teaming. However, our results show that peerto-peer teaming cannot reduce the mental workload on short-term tasks. This is understandable because working with a proactive teammate requires more interactions. \"Once mutual understanding is established, we can speculate that peer-to-teaming is significant."}, {"heading": "7. REFERENCES", "text": "[1] M. V. Briel, R. S\u00e1nchez, M. B. Do, andS. Kambhampati Part Intelligence and Interface Systems issues. Effective approaches for partial satisfaction (over-subscription) planning. In AAAI, pp. 562-569. AAAAI Press, 2004. [2] R. Cantrell, K. Talamadupula, P. W. Schermerhorn, J. Benton, S. Kambhampati, and M. Scheutz. Tell me when and why to do it!: run-time planner model updates via natural language instruction. In HRI '12, pp. 471-478, 2012. [3] J. Casper and R. Murphy. Human-robot interactions during the robot-assisted urban search and rescue response at the world trade center. Systems, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 33 (3): 367-385, June 2003. [4] J. Chen, M. Barnes, and M. Harper-Sciarini. Supervisory of multiple Goobots: Human Interfaces."}], "references": [{"title": "Effective approaches for partial satisfaction (over-subscription) planning", "author": ["M.V.D. Briel", "R. Sanchez", "M.B. Do", "S. Kambhampati"], "venue": "In In AAAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Tell me when and why to do it!: run-time planner model updates via natural language instruction", "author": ["R. Cantrell", "K. Talamadupula", "P.W. Schermerhorn", "J. Benton", "S. Kambhampati", "M. Scheutz"], "venue": "In HRI\u201912,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Human-robot interactions during the robot-assisted urban search and rescue response at the world trade center. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["J. Casper", "R. Murphy"], "venue": "IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Supervisory control of multiple robots: Human-performance issues and user-interface design. Systems, Man, and Cybernetics, Part C: Applications and Reviews", "author": ["J. Chen", "M. Barnes", "M. Harper-Sciarini"], "venue": "IEEE Transactions on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "How to make automated systems team players", "author": ["K. Christoffersen", "D.D. Woods"], "venue": "Advances in human performance and cognitive engineering research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Characterizing efficiency of human robot interaction: a case study of shared-control teleoperation", "author": ["J. Crandall", "M. Goodrich"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Adaptive aiding of human-robot teaming: Effects of imperfect automation on performance, trust, and workload", "author": ["E. de Visser", "R. Parasuraman"], "venue": "Journal of Cognitive Engineering and Decision Making,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Pddl2.1: An extension to pddl for expressing temporal planning domains", "author": ["M. Fox", "D. Long"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Plan constraints and preferences in PDDL3", "author": ["A. Gerevini", "D. Long"], "venue": "In ICAPS Workshop on Soft Constraints and Preferences in Planning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Using a mini-uav to support wilderness search and rescue: Practices for human-robot teaming", "author": ["M. Goodrich", "J. Cooper", "J. Adams", "C. Humphrey", "R. Zeeman", "B. Buss"], "venue": "In Safety, Security and Rescue Robotics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Seven principles of efficient human robot interaction", "author": ["M. Goodrich", "D. Olsen"], "venue": "In Systems, Man and Cybernetics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Conformant planning via heuristic forward search: A new approach", "author": ["J. Hoffmann", "R.I. Brafman"], "venue": "Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Model-lite planning for the web age masses: The challenges of planning with incomplete and evolving domain", "author": ["S. Kambhampati"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Ten challenges for making automation a \u201dteam player\u201d in joint human-agent activity", "author": ["G. Klein", "D.D. Woods", "J.M. Bradshaw", "R.R. Hoffman", "P.J. Feltovich"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "PDDL - The Planning Domain Definition Language. Technical report, CVC TR-98-003/DCS TR-1165", "author": ["D. Mcdermott", "M. Ghallab", "A. Howe", "C. Knoblock", "A. Ram", "M. Veloso", "D. Weld", "D. Wilkins"], "venue": "Yale Center for Computational Vision and Control,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Generating diverse plans to handle unknown and partially known user preferences", "author": ["T.A. Nguyen", "M. Do", "A.E. Gerevini", "I. Serina", "B. Srivastava", "S. Kambhampati"], "venue": "Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Visual recognition of pointing gestures for human-robot interaction", "author": ["K. Nickel", "R. Stiefelhagen"], "venue": "Image and Vision Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Human-robot teaming for search and rescue", "author": ["I.R. Nourbakhsh", "K. Sycara", "M. Koes", "M. Yong", "M. Lewis", "S. Burion"], "venue": "IEEE Pervasive Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Designing automation for human use: empirical studies and quantitative models", "author": ["R. Parasuraman"], "venue": "PMID:", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "A model for types and levels of human interaction with automation. Systems, Man and Cybernetics, Part A: Systems and Humans", "author": ["R. Parasuraman", "T. Sheridan", "C.D. Wickens"], "venue": "IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Exploring automation issues in supervisory control of multiple uavs", "author": ["H.A. Ruff", "G.L. Calhoun", "M.H. Draper", "J.V. Fontejon", "B.J. Guilfoos"], "venue": "In Proceedings of the Human Performance, Situation Awareness, and Automation Technology Conference,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Human interaction with levels of automation and decision-aid fidelity in the supervisory control of multiple simulated unmanned air vehicles", "author": ["H.A. Ruff", "S. Narayanan", "M.H. Draper"], "venue": "Presence: Teleoper. Virtual Environ.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "The utility of affect expression in natural language interactions in joint human-robot tasks", "author": ["M. Scheutz", "P. Schermerhorn", "J. Kramer"], "venue": "In Proceedings of the 1st ACM SIGCHI/SIGART Conference on Human-robot Interaction,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Human-automation interaction", "author": ["T.B. Sheridan", "R. Parasuraman"], "venue": "Reviews of Human Factors and Ergonomics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "A human aware mobile robot motion planner", "author": ["E. Sisbot", "L. Marin-Urias", "R. Alami", "T. Simeon"], "venue": "Robotics, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Visual attention in spoken human-robot interaction", "author": ["M. Staudte", "M. Crocker"], "venue": "In Human-Robot Interaction (HRI),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Natural human-robot interaction using speech, head pose and gestures", "author": ["R. Stiefelhagen", "C. Fugen", "R. Gieselmann", "H. Holzapfel", "K. Nickel", "A. Waibel"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Planning for human-robot teaming in open worlds", "author": ["K. Talamadupula", "J. Benton", "S. Kambhampati", "P. Schermerhorn", "M. Scheutz"], "venue": "ACM Trans. Intell. Syst. Technol.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Comparative performance of human and mobile robotic assistants in collaborative fetch-and-deliver tasks", "author": ["V.V. Unhelkar", "H.C. Siu", "J.A. Shah"], "venue": "In Proceedings of the 2014 ACM/IEEE International Conference on Human-robot Interaction,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "During plan execution, the human can interact with the robot using a low level motion controller [3], or a high level task and plan manager [11].", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "During plan execution, the human can interact with the robot using a low level motion controller [3], or a high level task and plan manager [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "We choose the urban search and rescue (USAR) task [18] to conduct our investigation.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": ", [23]), gaze (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": ", [26]) and gesture (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": ", [17]) are used, as well as their relationships during the interaction (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 26, "context": ", [27]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": ", [25]), as well as how human preference influences interaction (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": ", [29]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": "However, so far, human factor studies on automation in human-robot teaming have been mainly restricted to various forms of supervision, in which the human decides the actions [22] or sub-goals [21] for the robot towards achieving the global goal.", "startOffset": 175, "endOffset": 179}, {"referenceID": 20, "context": "However, so far, human factor studies on automation in human-robot teaming have been mainly restricted to various forms of supervision, in which the human decides the actions [22] or sub-goals [21] for the robot towards achieving the global goal.", "startOffset": 193, "endOffset": 197}, {"referenceID": 2, "context": "During the missions, the human interacts with the robot either through motion controllers [3, 10], or task and plan managers [11].", "startOffset": 90, "endOffset": 97}, {"referenceID": 9, "context": "During the missions, the human interacts with the robot either through motion controllers [3, 10], or task and plan managers [11].", "startOffset": 90, "endOffset": 97}, {"referenceID": 10, "context": "During the missions, the human interacts with the robot either through motion controllers [3, 10], or task and plan managers [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "A comprehensive review of related works on supervisory human-robot teaming can be found at [4].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "While the human teammate may receive updates [2] about the environment (e.", "startOffset": 45, "endOffset": 48}, {"referenceID": 27, "context": ", [28]), there exists no empirical investigation on its influence on humanrobot teaming performance and the trade-offs.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "Empirical proofs have been provided in four main areas: mental workload, situation awareness, complacency and skill degradation [19].", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "Many earlier human factor studies have been on how automation should be designed to benefit humanmachine interaction [7].", "startOffset": 117, "endOffset": 120}, {"referenceID": 18, "context": "Researchers have also characterized the types [19] and levels of autonomy [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Researchers have also characterized the types [19] and levels of autonomy [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "Regarding efficient human-machine teaming, it is argued in [5] that automation should be provided in the context of interaction.", "startOffset": 59, "endOffset": 62}, {"referenceID": 13, "context": "Along this line, a substantial amount of work has investigated how efficient teaming can be achieved [14, 24], which is also applicable to human-robot teaming scenarios.", "startOffset": 101, "endOffset": 109}, {"referenceID": 23, "context": "Along this line, a substantial amount of work has investigated how efficient teaming can be achieved [14, 24], which is also applicable to human-robot teaming scenarios.", "startOffset": 101, "endOffset": 109}, {"referenceID": 14, "context": "A planning problem can be specified using a planning domain definition language (PDDL) [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": ", [8, 9]) that incorporate various modeling requirements.", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": ", [8, 9]) that incorporate various modeling requirements.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": "In this paper, we use the extension of PDDL described in [8] to model an USAR task.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "For example, partial satisfaction of the goal [1], incomplete initial state [12], incomplete action models (i.", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "For example, partial satisfaction of the goal [1], incomplete initial state [12], incomplete action models (i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": ", used to perform planning) [13], as well as incomplete preference of the human teammate [16].", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": ", used to perform planning) [13], as well as incomplete preference of the human teammate [16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "However, note that the level of automation [20] that can be supported varies between these two cases.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "To limit the performance difference due to the level of automation, we set the level of automation to be the lower bound that is applicable to the first case, while setting it to be the upper bound for the second case, according to [20].", "startOffset": 232, "endOffset": 236}, {"referenceID": 19, "context": "In our study, the two cases correspond coarsely to level 6 (management-by-exception) and 3 described in [20], respectively.", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "A post-study questionnaire is used to evaluate three of four areas that are often used to assess automated systems: mental workload, situation awareness, and complacency [19].", "startOffset": 170, "endOffset": 174}, {"referenceID": 5, "context": "Furthermore, we also evaluated the robot attention demand (RAD) [6], which is the percentage of the participant\u2019s time dedicated to human-robot interaction.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "Another note is about the relationship between RAD and Fan out [6].", "startOffset": 63, "endOffset": 66}], "year": 2014, "abstractText": "Human-robot interaction can be divided into two categories based on the physical distance between the human and robot: remote and proximal. In proximal interaction, the human and robot often engage in close coordination; in remote interaction, the human and robot are less coupled due to communication constraints. As a result, providing automation for the robot in remote interaction becomes more important. Thus far, human factor studies on automation in remote human-robot interaction have been restricted to various forms of supervision, in which the robot is essentially being used as a smart mobile manipulation platform with sensing capabilities. In this paper, we investigate the incorporation of general planning capability into the robot to facilitate peer-to-peer human-robot teaming, in which the human and robot are viewed as teammates that are physically separated. The human and robot share the same global goal and collaborate to achieve it. Note that humans may feel uncomfortable at such robot autonomy, which can potentially reduce teaming performance. One important difference between peer-to-peer teaming and supervised teaming is that an autonomous robot in peer-to-peer teaming can achieve the goal alone when the task information is completely specified. However, incompleteness often exists, which implies information asymmetry. While information asymmetry can be desirable sometimes, it may also lead to the robot choosing improper actions that negatively influence the teaming performance. We aim to investigate the various trade-offs, e.g., mental workload and situation awareness, between these two types of remote human-robot teaming.", "creator": "LaTeX with hyperref package"}}}