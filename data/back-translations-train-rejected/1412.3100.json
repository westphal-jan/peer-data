{"id": "1412.3100", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2014", "title": "Semi-Supervised Learning with Heterophily", "abstract": "We propose a novel linear semi-supervised learning formulation that is derived from a solid probabilistic framework: belief propagation. We show that our formulation generalizes a number of label propagation algorithms described in the literature by allowing them to propagate generalized assumptions about influences between classes of neighboring nodes. We call this formulation Semi-Supervised Learning with Heterophily (SSL-H). We also show how the affinity matrix can be learned from observed data with a simple convex optimization framework that is inspired by locally linear embedding. We call this approach Linear Heterophily Estimation (LHE). Experiments on synthetic data show that both approaches combined can learn heterophily of a graph with 1M nodes, 10M edges and few labels in under 1min, and give better labeling accuracies than a baseline method in the case of small fraction of explicitly labeled nodes.", "histories": [["v1", "Tue, 9 Dec 2014 20:58:45 GMT  (1059kb,D)", "http://arxiv.org/abs/1412.3100v1", "8 pages, 6 figures"], ["v2", "Wed, 28 Dec 2016 02:27:12 GMT  (644kb,D)", "http://arxiv.org/abs/1412.3100v2", "17 pages, 13 figures"]], "COMMENTS": "8 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["wolfgang gatterbauer"], "accepted": false, "id": "1412.3100"}, "pdf": {"name": "1412.3100.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning with Heterophily", "authors": ["Wolfgang Gatterbauer"], "emails": ["gatt@cmu.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to determine for themselves how they have behaved."}, {"heading": "2. BACKGROUND", "text": "Here we describe three closely related groups of works on which the methods described in this essay are based."}, {"heading": "2.1 Semi-Supervised Learning (SSL)", "text": "Semi-Monitored Learning Methods (SSL) usually derive their formalism by motivating a loss function consisting of (i) a suitable term for existing labels, e.g. (fi \u2212 xi) 2, where xi \u2212 n denotes the given label and fixes the learned label, and (ii) a smoothing term or regulator, e.g. (fi \u2212 fj) 2. We focus here only on the most important aspects of SSL and refer to [26] and [4] for two excellent surveys of SSL. Here, we follow the representation of [1] with two notable limitations: (i) we assume a symmetrical graph structure of W = W; (ii) we allow the recognition of already described information (fi \u2212 xi) 2 = 0 possible). We focus on three methods in particular: 1. The harmonic function method (HF) minimizes the loss function (f)."}, {"heading": "2.2 Locally Linear Embedding (LLE)", "text": "Local linear embedding (LLE) [15] is a method for deriving compact representations of high-dimensional data by establishing a linear relationship between adjacent points. It was originally an unattended learning algorithm. We will later use the formulation for our SSL scenario, namely the estimation of the heterophilic matrix H instead of embedding W from the data. We are following exactly the exposure in the original paper [11] using our notation.The LLE algorithm assumes that the data consists of n real-weighted vectors xi, each dimensionality k. LLE then reconstructs each data point from its neighbors by constructing a locally linear combination. Reconstruction errors are measured by the loss function: E (W) = \u2211 jWijxj | | 2 (1), which adds the square distances between the data points and their reconstructions."}, {"heading": "2.3 Linearized Belief Propagation (LinBP)", "text": "In fact, it is so that it is about a way and a way, in which it is about the question, in how far it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way and a way and a way, in which it is about a way and a way and a way, in which it is about a way and a way and a way and a way and a way, in which it is about a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way, a way and a way and a way and a way, a way and a way and a way and a way, a way and a way and a way and a way, a way and a way, a way and a way and a way, a way and a way and a way, a way and a way and a way, a way and a way and a way, a way and a way, a way and a way and a way, a way and a way and a way and a way, a way and a way and a way and a way, a way and a way and a way, a way and a way and a way, a way and a way and a way and a way and a way and a way and a way and a way and a way and a way, a way and a way and a way and a way and a way, a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way and a way, a way and a way and a way and a way, a way and a way and a way and a way and a way and a way and a way and a way and a way and a way, a way and a way and a way and a way and a way and a way and a way and a way and a way and"}, {"heading": "3. GENERALIZING SSL FOR HETEROPHILY", "text": "\"We can therefore assume that we write all nodes with,\" says Wlogall, \"that we write the diagonal matrix with Gi = 1, and Gi = 0, otherwise we write L = D \u2212 W for the basic idea.\" We can therefore assume that \"we write all nodes with,\" according to Wlogall, \"that we write the diagonal matrix with Gi = 1, and Gi = 0, otherwise we write L = D \u2212 W for the primer Laplacian matrix.\" So we can alternatively write: E (f) = 2, \"Gf \u2212 G for the diagonal matrix with Gi = 1, and Gi = 0, otherwise we write L = D \u2212 W for the primer Laplacian matrix.\""}, {"heading": "4. LEARNING HETEROPHILY FROM DATA", "text": "In this section we present our framework for heterophilic learning from partially marked data."}, {"heading": "4.1 Baseline approach: MHE", "text": "We first introduce a simple baseline method, which we call Myopic Heterophily Estimation (MHE) = 4mmx. We call it \"myopic\" because it attempts to derive the relative frequencies between different classes in the network by a simple frequency calculation, followed by a transformation into a symmetrical double stochastic matrix. Partly labeled with n \u00b7 k matrix X \u2032, with X \u2032 (i \u2032, c) = 1 if node i has a label c. Consider that some nodes have no label. Then, the matrix n \u00b7 k matrix N: = WX \u2032 contains the number of neighbors of a certain class X \u2032, i.e. N (i, c) determines the number of neighbors of i belonging to class c. Next, the matrix is the matrix."}, {"heading": "4.2 Improved approach: LHE", "text": "In fact, it is as if most of us are able to arm ourselves for the future. (...) It is not as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save themselves. (...) It is as if they are able to save the world. (...)"}, {"heading": "5. EXPERIMENTS", "text": "We evaluate our method with two questions in mind: (1) How well does our Linear Heterophily Estimation (LHE) work compared to the basic method? (2) How well do our combined methods scale with the size of the network?"}, {"heading": "5.1 Quality", "text": "We have chosen to limit our technique to carefully controlled fundamentals, as this allows us to change the basic truth and assess the accuracy of our techniques as a result of 6HHF. We have repeated the experiments with various synthetic data sets and will focus on a specific selection of parameters to illustrate the main conclusions. Figure 5h summarizes our entire experimental structure. We assume that we will use the three classes and the use of the heterophilic matrix from the introduction: H = 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.8. We will create a graph with n = 100 nodes of one of the three classes (we assumed that the same previous probabilities are of a particular class)."}, {"heading": "5.2 Scalability", "text": "For the scalability experiments, we have implemented our Python 2.7 learning framework. We use fmin slsqp in scipy.optimize to minimize our function using sequential least squares programming. Figure 6b shows the results for graphs with average m = 2 or m = 10 edges per node. The time for LHE for 1M nodes and 10M edges is 31sec and LinBP about 7sec for 10 iterations."}, {"heading": "6. RELATED WORK", "text": "Our work refers to several existing papers on semi-supervised learning. We have previously discussed in detail the method of harmonic function (HF) [27], linear neighbourhood propagation (LNP) [22], method of local and global consistency (LGC) [25], locally linear embedding (LLE) [15], rapid faith propagation (FaBP) [10], and linearized faith propagation (LinBP) [6]. In [27] a symmetrical weight matrix W is learned, but they assume simple homophobic relationships and do not take into account more complex heterophilic relationships. In [22] a problem of multi-class classification is considered. Although each of the classes is treated separately and they should never interact with other classes (mixing / modularization), the works [7] and [21] incorporate similarities into the label learning process: it can be pointed out that two adjacent nodes should have different labels."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we proposed a novel semi-supervised learning formulation based on two novel components: First, it allows us to use not only similarity and dissimilarity, but any kind of reciprocal coupling strengths between different classes of nodes (we abstract this with the double stochastic heterophilic matrix); second, we showed how to estimate the heterophilic matrix using partially labeled data; and finally, we offer experiments that demonstrate the effectiveness of our method, plus detailed insights into the effects of parameters of the problem on our ability to label data correctly; and, of course, we show how our framework generalizes a number of existing frameworks and expands them from homophobic to heterophobic; and finally, we offer experiments that demonstrate the effectiveness of our method, plus detailed insights into the effects of parameters of the problem on our ability to label data correctly. Thank you Christos Faloutsos for inspiring discussions, helpful comments, and for having me write a Linde algorithm at all (I am grateful to Fernando for having algorithmic problems)."}, {"heading": "8. REFERENCES", "text": "[1] Y. Bengio, O. Delalleau, and N. L. Roux. Label propagationand quadratic criterion. In O. Chapelle, B. Scho \ufffd lkopf, and A. Zien, Editors, Semi-supervised learning, pp. 193-216. MIT Press, 2006. [2] B. Bolloba's, C. Borgs, J. Chayes, and O. Riordan. Directed scale-free graphs. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA '03, pp. 132-139, Philadelphia, PA, USA, 2003. Society for Industrial and Applied Mathematics. [3] L. Breiman. Statistical modeling: The two cultures. Statistical Science, 16 (3): 199-215, 2001. [4] O. Chapelle, B. Scho \ufffd lkopf, and A. Zien. Semi-supervised learning."}, {"heading": "A. NOMENCLATURE", "text": "n Number of nodes k Number of classes W n \u00b7 n weighted symmetrical adjacence matrix P n \u00b7 n row stochastic adjacence matrix Wx, Px \"fixed\" variants: distant columns for explicit beliefs xi given label of node i (if labeled) fi derived label of node i X n \u00b7 k given label F n \u00b7 k derived label distribution L n \u00b7 n laplactic matrix Ln \u00b7 n normalized laplactic matrix H k \u00b7 k coupling matrix with Hi, j Indication of the influence of class i of a transmitter on class j of the receiver X, F, F, H: restrictive matrices centred by 1 k H, unscaled, centered coupling matrix: H = HH = H, o H: scaling factor Ik-dimensional identity matrix Xtranspose matrix of the X-X series X (X)"}], "references": [{"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N.L. Roux"], "venue": "O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, editors, Semi-supervised learning, pp. 193\u2013216. MIT Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Directed scale-free graphs", "author": ["B. Bollob\u00e1s", "C. Borgs", "J. Chayes", "O. Riordan"], "venue": "Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201903, pp. 132\u2013139, Philadelphia, PA, USA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Statistical modeling: The two cultures", "author": ["L. Breiman"], "venue": "Statistical Science, 16(3):199\u2013215,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-supervised learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": "MIT Press, Cambridge, Mass.,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Residual belief propagation: Informed scheduling for asynchronous message passing", "author": ["G. Elidan", "I. McGraw", "D. Koller"], "venue": "UAI,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Linearized and single-pass belief propagation", "author": ["W. Gatterbauer", "S. G\u00fcnnemann", "D. Koutra", "C. Faloutsos"], "venue": "PVLDB, 8(5),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Dissimilarity in graph-based semi-supervised classification", "author": ["A.B. Goldberg", "X. Zhu", "S.J. Wright"], "venue": "AISTATS, pp. 155\u2013162,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of statistical network models", "author": ["A. Goldenberg", "A.X. Zheng", "S.E. Fienberg", "E.M. Airoldi"], "venue": "Foundations and Trends in Machine Learning, 2(2):129\u2013233,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic blockmodels: First steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social Networks, 5(2):109 \u2013 137,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1983}, {"title": "Unifying guilt-by-association approaches: Theorems and fast algorithms", "author": ["D. Koutra", "T.-Y. Ke", "U. Kang", "D.H. Chau", "H.-K.K. Pao", "C. Faloutsos"], "venue": "ECML/PKDD (2), pp. 245\u2013260,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Link-based classification", "author": ["Q. Lu", "L. Getoor"], "venue": "ICML, pp. 496\u2013503,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Sufficient conditions for convergence of the sum-product algorithm", "author": ["J.M. Mooij", "H.J. Kappen"], "venue": "IEEE Transactions on Information Theory, 53(12):4422\u20134437,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "Adaptive computation and machine learning series. MIT Press, Cambridge, MA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20136, Dec", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparative study of algorithms for matrix balancing", "author": ["M.H. Schneider", "S.A. Zenios"], "venue": "Oper. Res., 38(3):439\u2013455, May", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Link-based classification", "author": ["P. Sen", "L. Getoor"], "venue": "Technical report, University of Maryland Technical Report CS-TR-4858, February", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine, 29(3):93\u2013106,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A relationship between arbitrary positive matrices and doubly stochastic matrices", "author": ["R. Sinkhorn"], "venue": "The Annals of Mathematical Statistics, 35(2):876\u2013879, 06", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1964}, {"title": "Planar 3-colorability is polynomial complete", "author": ["L. Stockmeyer"], "venue": "SIGACT News, 5(3):19\u201325, July", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1973}, {"title": "Semi-supervised learning by mixed label propagation", "author": ["W. Tong", "R. Jin"], "venue": "AAAI, pp. 651\u2013656. AAAI Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Label propagation through linear neighborhoods", "author": ["F. Wang", "C. Zhang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 20(1):55\u201367,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Transductive classification via local learning regularization", "author": ["M. Wu", "B. Sch\u00f6lkopf"], "venue": "AISTATS, pp. 628\u2013635,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Doubly stochastic normalization for spectral clustering", "author": ["R. Zass", "A. Shashua"], "venue": "B. Sch\u00f6lkopf, J. C. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006, pp. 1569\u20131576. MIT Press,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530, Computer Sciences, University of Wisconsin-Madison,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "ICML, pp. 912\u2013919,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 25, "context": "INTRODUCTION Graph-based Semi-Supervised Learning (SSL) methods define a graph where the nodes are labeled and unlabeled examples in the dataset, and where (potentially weighted) edges reflect the similarity of examples [26].", "startOffset": 220, "endOffset": 224}, {"referenceID": 6, "context": ", [7] and [21]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": ", [7] and [21]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": ", those based on Local and Blobal Consistency (LGC) [25] or Harmonic Function methods (HF) [27]) can be generalized in a natural way as to allow to propagate heterophily from labeled to unlabeled data.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": ", those based on Local and Blobal Consistency (LGC) [25] or Harmonic Function methods (HF) [27]) can be generalized in a natural way as to allow to propagate heterophily from labeled to unlabeled data.", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "In this regard, this work draws heavily upon and provides a generalization of our recent work on linearizing the update equations of belief propagation [6].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "H\u2192 \u2193x x\u2032\u2198 [ 1 0 0 0 1 0 0 0 1 ] [ 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 0, "context": "H\u2192 \u2193x x\u2032\u2198 [ 1 0 0 0 1 0 0 0 1 ] [ 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 0, "context": "H\u2192 \u2193x x\u2032\u2198 [ 1 0 0 0 1 0 0 0 1 ] [ 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 0, "context": "[ 1 0 0 ] [ 1 0 0 ] [ 0.", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[ 1 0 0 ] [ 1 0 0 ] [ 0.", "startOffset": 10, "endOffset": 19}, {"referenceID": 14, "context": "Here, we draw an interesting connection to the Locally Linear Embedding (LLE) [15] framework.", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "We focus our discussion here only on the most important aspects of SSL and refer to [26] and [4] for two excellent surveys on SSL.", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "We focus our discussion here only on the most important aspects of SSL and refer to [26] and [4] for two excellent surveys on SSL.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "We follow here the exposition of [1] with two notable restrictions: (i) we assume a symmetric graph structure W = WT; (ii) we allow relabeling of already labeled information ((fi \u2212 xi) 6= 0 possible).", "startOffset": 33, "endOffset": 36}, {"referenceID": 26, "context": "The harmonic function method (HF) [27] minimizes the loss function E(f) = \u2211 i(fi\u2212xi)2+ \u03bc2 \u2211 i,jWij(fi\u2212 fj) .", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "The linear neighborhood propagation (LNP) [22] is a variation in which the weights of the neighbors of a node i need to sum up to 1: \u2211 iWij = 1.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "The local and global consistency method (LGC) [25] is similar to HF except for normalizing the labeling function by the square root of the degrees of each node: E(f) = \u2211 i(fi \u2212 xi) + \u03bc2 \u2211 i,jWij( fi \u221a di \u2212 fj \u221a dj ) The", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "It is easy to extend existing label propagation algorithms to multiclass classification problems [22] by assigning with a vector to each node, where each entry represents the belief of a node in a particular labeling class.", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "Update equation Closed form Loss function Previous methods for binary case: HF [27] f \u2190 (I + \u03bcD)\u22121(x + \u03bcWf) f = (I + \u03bcL)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLf LNP [22] f \u2190 \u1fb1x + \u03b1Wf f = \u1fb1(I\u2212 \u03b1W)\u22121x \u201d \u201d \u201d \u201d \u201d \u201d LGC [25] f \u2190 \u1fb1x + \u03b1L\u2217f f = \u1fb1(I\u2212 \u03b1L\u2217)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLnf FaBP [10] f \u2190 x + 2hWf f = (I\u2212 2hW)\u22121x E(f) = ||f \u2212 x\u2212 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F\u2190 (I + \u03bcD)\u22121(X + \u03bcWF) F = (I + \u03bcL)\u22121X E(F) = ||F\u2212X||2 + \u03bc 2 \u2211 i,jWij \u2211 k(Fik \u2212 Fjk) LNP F\u2190 \u1fb1X + \u03b1WF F = \u1fb1(I\u2212 \u03b1W)\u22121X \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d LGC F\u2190 \u1fb1X + \u03b1L\u2217F F = \u1fb1(I\u2212 \u03b1L\u2217)\u22121X E(F) = ||F\u2212X||2 + \u03bc2 \u2211 i,jWij \u2211 k( Fik \u221a di \u2212 Fjk \u221a dj )2 LinBP [6] F\u2190 X + WFH vec ( F ) = (I\u2212H\u2297W)\u22121vec ( X ) E(F) = ||F\u2212X\u2212WFH||2 (shown in this paper)", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "Update equation Closed form Loss function Previous methods for binary case: HF [27] f \u2190 (I + \u03bcD)\u22121(x + \u03bcWf) f = (I + \u03bcL)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLf LNP [22] f \u2190 \u1fb1x + \u03b1Wf f = \u1fb1(I\u2212 \u03b1W)\u22121x \u201d \u201d \u201d \u201d \u201d \u201d LGC [25] f \u2190 \u1fb1x + \u03b1L\u2217f f = \u1fb1(I\u2212 \u03b1L\u2217)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLnf FaBP [10] f \u2190 x + 2hWf f = (I\u2212 2hW)\u22121x E(f) = ||f \u2212 x\u2212 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F\u2190 (I + \u03bcD)\u22121(X + \u03bcWF) F = (I + \u03bcL)\u22121X E(F) = ||F\u2212X||2 + \u03bc 2 \u2211 i,jWij \u2211 k(Fik \u2212 Fjk) LNP F\u2190 \u1fb1X + \u03b1WF F = \u1fb1(I\u2212 \u03b1W)\u22121X \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d LGC F\u2190 \u1fb1X + \u03b1L\u2217F F = \u1fb1(I\u2212 \u03b1L\u2217)\u22121X E(F) = ||F\u2212X||2 + \u03bc2 \u2211 i,jWij \u2211 k( Fik \u221a di \u2212 Fjk \u221a dj )2 LinBP [6] F\u2190 X + WFH vec ( F ) = (I\u2212H\u2297W)\u22121vec ( X ) E(F) = ||F\u2212X\u2212WFH||2 (shown in this paper)", "startOffset": 154, "endOffset": 158}, {"referenceID": 24, "context": "Update equation Closed form Loss function Previous methods for binary case: HF [27] f \u2190 (I + \u03bcD)\u22121(x + \u03bcWf) f = (I + \u03bcL)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLf LNP [22] f \u2190 \u1fb1x + \u03b1Wf f = \u1fb1(I\u2212 \u03b1W)\u22121x \u201d \u201d \u201d \u201d \u201d \u201d LGC [25] f \u2190 \u1fb1x + \u03b1L\u2217f f = \u1fb1(I\u2212 \u03b1L\u2217)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLnf FaBP [10] f \u2190 x + 2hWf f = (I\u2212 2hW)\u22121x E(f) = ||f \u2212 x\u2212 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F\u2190 (I + \u03bcD)\u22121(X + \u03bcWF) F = (I + \u03bcL)\u22121X E(F) = ||F\u2212X||2 + \u03bc 2 \u2211 i,jWij \u2211 k(Fik \u2212 Fjk) LNP F\u2190 \u1fb1X + \u03b1WF F = \u1fb1(I\u2212 \u03b1W)\u22121X \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d LGC F\u2190 \u1fb1X + \u03b1L\u2217F F = \u1fb1(I\u2212 \u03b1L\u2217)\u22121X E(F) = ||F\u2212X||2 + \u03bc2 \u2211 i,jWij \u2211 k( Fik \u221a di \u2212 Fjk \u221a dj )2 LinBP [6] F\u2190 X + WFH vec ( F ) = (I\u2212H\u2297W)\u22121vec ( X ) E(F) = ||F\u2212X\u2212WFH||2 (shown in this paper)", "startOffset": 204, "endOffset": 208}, {"referenceID": 9, "context": "Update equation Closed form Loss function Previous methods for binary case: HF [27] f \u2190 (I + \u03bcD)\u22121(x + \u03bcWf) f = (I + \u03bcL)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLf LNP [22] f \u2190 \u1fb1x + \u03b1Wf f = \u1fb1(I\u2212 \u03b1W)\u22121x \u201d \u201d \u201d \u201d \u201d \u201d LGC [25] f \u2190 \u1fb1x + \u03b1L\u2217f f = \u1fb1(I\u2212 \u03b1L\u2217)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLnf FaBP [10] f \u2190 x + 2hWf f = (I\u2212 2hW)\u22121x E(f) = ||f \u2212 x\u2212 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F\u2190 (I + \u03bcD)\u22121(X + \u03bcWF) F = (I + \u03bcL)\u22121X E(F) = ||F\u2212X||2 + \u03bc 2 \u2211 i,jWij \u2211 k(Fik \u2212 Fjk) LNP F\u2190 \u1fb1X + \u03b1WF F = \u1fb1(I\u2212 \u03b1W)\u22121X \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d LGC F\u2190 \u1fb1X + \u03b1L\u2217F F = \u1fb1(I\u2212 \u03b1L\u2217)\u22121X E(F) = ||F\u2212X||2 + \u03bc2 \u2211 i,jWij \u2211 k( Fik \u221a di \u2212 Fjk \u221a dj )2 LinBP [6] F\u2190 X + WFH vec ( F ) = (I\u2212H\u2297W)\u22121vec ( X ) E(F) = ||F\u2212X\u2212WFH||2 (shown in this paper)", "startOffset": 272, "endOffset": 276}, {"referenceID": 5, "context": "Update equation Closed form Loss function Previous methods for binary case: HF [27] f \u2190 (I + \u03bcD)\u22121(x + \u03bcWf) f = (I + \u03bcL)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLf LNP [22] f \u2190 \u1fb1x + \u03b1Wf f = \u1fb1(I\u2212 \u03b1W)\u22121x \u201d \u201d \u201d \u201d \u201d \u201d LGC [25] f \u2190 \u1fb1x + \u03b1L\u2217f f = \u1fb1(I\u2212 \u03b1L\u2217)\u22121x E(f) = ||f \u2212 x||2 + \u03bcfTLnf FaBP [10] f \u2190 x + 2hWf f = (I\u2212 2hW)\u22121x E(f) = ||f \u2212 x\u2212 2hWf ||2 (shown in this paper) Previous methods for multi-class case: HF F\u2190 (I + \u03bcD)\u22121(X + \u03bcWF) F = (I + \u03bcL)\u22121X E(F) = ||F\u2212X||2 + \u03bc 2 \u2211 i,jWij \u2211 k(Fik \u2212 Fjk) LNP F\u2190 \u1fb1X + \u03b1WF F = \u1fb1(I\u2212 \u03b1W)\u22121X \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d \u201d LGC F\u2190 \u1fb1X + \u03b1L\u2217F F = \u1fb1(I\u2212 \u03b1L\u2217)\u22121X E(F) = ||F\u2212X||2 + \u03bc2 \u2211 i,jWij \u2211 k( Fik \u221a di \u2212 Fjk \u221a dj )2 LinBP [6] F\u2190 X + WFH vec ( F ) = (I\u2212H\u2297W)\u22121vec ( X ) E(F) = ||F\u2212X\u2212WFH||2 (shown in this paper)", "startOffset": 632, "endOffset": 635}, {"referenceID": 0, "context": "Figure 3: HF: harmonic function method (according to [1]), LNP: linear neighborhood propagation, LGC: local and global consistency method, FaBP: fast belief propagation.", "startOffset": 53, "endOffset": 56}, {"referenceID": 14, "context": "2 Locally Linear Embedding (LLE) Locally linear embedding (LLE) [15] is a method to derive compact representations of high-dimensional data by building a linear relationship among neighboring points.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "We follow here exactly the exposition in the original paper [11] while using our notation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "3 Linearized Belief Propagation (LinBP) Another widely used methods for semi-supervised reasoning in networked data is Belief Propagation (BP) [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "For graphs with loops, however, BP has well-known convergence problems (see [18] for a detailed discussion from a practitioner\u2019s point of view).", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "While there is a lot of work on convergence of BP [5, 12], exact criteria for convergence are not known [13, Sec.", "startOffset": 50, "endOffset": 57}, {"referenceID": 11, "context": "While there is a lot of work on convergence of BP [5, 12], exact criteria for convergence are not known [13, Sec.", "startOffset": 50, "endOffset": 57}, {"referenceID": 17, "context": "22], and practical use of BP is still non-trivial [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "[10] proposed to linearize belief propagation for the case of two classes and proposed fast belief propagation (FaBP) as a method to propagate existing knowledge of homophily or heterophily to unlabeled data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "In [6], we have recently solved the problem for the multiclass case and proposed Linearized Belief Propagation (LinBP) as an efficient linearization of belief propagation on pairwise Markov random fields.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Notice that in [6], we distinguished between two versions of linearized belief propagation: LinBP and LinBP\u2217.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "Proposition 1 (Closed-form [6]).", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "Lemma 2 (Convergence [6]).", "startOffset": 21, "endOffset": 24}, {"referenceID": 26, "context": "GENERALIZING SSL FOR HETEROPHILY We illustrate here generalizing the update equations for the harmonic functions (HF) method [27] to Harmonic functions with heterophily (HFH).", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "The harmonic function method (HF) [27] then minimizes the loss function", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "We can now use the exact same mathematics we had derived in [6] in order to determine the closed-form as follows:", "startOffset": 60, "endOffset": 63}, {"referenceID": 23, "context": "This problem can be solved efficiently with Algorithm 1 in [24] that finds a Frobenius-norm optimum doubly stochastic approximation to a given matrix Notice that in the case of an incompletely labeled graph, it suffices to consider only the subgraph induced by the labeled nodes.", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "Then H\u2032=[ 0 2 2 2 ].", "startOffset": 8, "endOffset": 19}, {"referenceID": 1, "context": "Then H\u2032=[ 0 2 2 2 ].", "startOffset": 8, "endOffset": 19}, {"referenceID": 1, "context": "Then H\u2032=[ 0 2 2 2 ].", "startOffset": 8, "endOffset": 19}, {"referenceID": 0, "context": "And H \u2032\u2032= [ 0 1 1 2 1 2 ] .", "startOffset": 10, "endOffset": 25}, {"referenceID": 0, "context": "And H \u2032\u2032= [ 0 1 1 2 1 2 ] .", "startOffset": 10, "endOffset": 25}, {"referenceID": 1, "context": "And H \u2032\u2032= [ 0 1 1 2 1 2 ] .", "startOffset": 10, "endOffset": 25}, {"referenceID": 0, "context": "And H \u2032\u2032= [ 0 1 1 2 1 2 ] .", "startOffset": 10, "endOffset": 25}, {"referenceID": 1, "context": "And H \u2032\u2032= [ 0 1 1 2 1 2 ] .", "startOffset": 10, "endOffset": 25}, {"referenceID": 0, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 3, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 3, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 3, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 0, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 3, "context": "to H\u2217= [ 1 4 3 4 3 4 1 4 ] .", "startOffset": 7, "endOffset": 26}, {"referenceID": 0, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 1, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 1, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 0, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "In constrast, LHE leads to H= [ 1 3 2 3 2 3 1 3 ] .", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": "The proof follows immediately from our proof of convergence of LinBP [6].", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).", "startOffset": 110, "endOffset": 129}, {"referenceID": 2, "context": "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).", "startOffset": 110, "endOffset": 129}, {"referenceID": 0, "context": "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).", "startOffset": 110, "endOffset": 129}, {"referenceID": 2, "context": "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).", "startOffset": 110, "endOffset": 129}, {"referenceID": 0, "context": "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).", "startOffset": 110, "endOffset": 129}, {"referenceID": 2, "context": "sign each node one of the 3 classes (we assumed equal prior probabilities of a node being a particular class: [ 1 3 , 1 3 , 1 3 ]).", "startOffset": 110, "endOffset": 129}, {"referenceID": 0, "context": "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) \u2248 1.", "startOffset": 39, "endOffset": 58}, {"referenceID": 2, "context": "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) \u2248 1.", "startOffset": 39, "endOffset": 58}, {"referenceID": 0, "context": "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) \u2248 1.", "startOffset": 39, "endOffset": 58}, {"referenceID": 2, "context": "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) \u2248 1.", "startOffset": 39, "endOffset": 58}, {"referenceID": 0, "context": "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) \u2248 1.", "startOffset": 39, "endOffset": 58}, {"referenceID": 2, "context": "92 (as compared to the random vector H([ 1 3 , 1 3 , 1 3 ]) \u2248 1.", "startOffset": 39, "endOffset": 58}, {"referenceID": 26, "context": "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].", "startOffset": 68, "endOffset": 72}, {"referenceID": 21, "context": "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].", "startOffset": 197, "endOffset": 201}, {"referenceID": 9, "context": "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].", "startOffset": 234, "endOffset": 238}, {"referenceID": 5, "context": "We previously discussed in detail the harmonic function method (HF) [27], linear neighborhood propagation (LNP) [22], local and global consistency method (LGC) [25], locally linear embedding (LLE) [15], fast belief propagation (FaBP) [10], and Linearized Belief Propagation (LinBP) [6].", "startOffset": 282, "endOffset": 285}, {"referenceID": 26, "context": "In [27] a symmetric weight matrix W is learned 0 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [22] a multi-class classification problems is considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "The works [7] and [21] integrate dissimilarity into the label learning process: one can indicate that two neighboring nodes should have different labels.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "The works [7] and [21] integrate dissimilarity into the label learning process: one can indicate that two neighboring nodes should have different labels.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "Also, [23] formulate a local learning regularizer (LL-Reg): learned to predict the label of each data point based on the neighbors and their labels.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "only been described before in our recent work on LinBP [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "We refer to [11], [26], [4], and [1] for excellent exposition and comparison of various methods.", "startOffset": 33, "endOffset": 36}], "year": 2017, "abstractText": "We propose a novel linear semi-supervised learning formulation that is derived from a solid probabilistic framework: belief propagation. We show that our formulation generalizes a number of label propagation algorithms described in the literature by allowing them to propagate generalized assumptions about influences between classes of neighboring nodes. We call this formulation Semi-Supervised Learning with Heterophily (SSL-H). We also show how the affinity matrix can be learned from observed data with a simple convex optimization framework that is inspired by locally linear embedding. We call this approach Linear Heterophily Estimation (LHE). Experiments on synthetic data show that both approaches combined can learn heterophily of a graph with 1M nodes, 10M edges and few labels in under 1min, and give better labeling accuracies than a baseline method in the case of small fraction of explicitly labeled nodes.", "creator": "LaTeX with hyperref package"}}}