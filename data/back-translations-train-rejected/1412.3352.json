{"id": "1412.3352", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2014", "title": "Web image annotation by diffusion maps manifold learning algorithm", "abstract": "Automatic image annotation is one of the most challenging problems in machine vision areas. The goal of this task is to predict number of keywords automatically for images captured in real data. Many methods are based on visual features in order to calculate similarities between image samples. But the computation cost of these approaches is very high. These methods require many training samples to be stored in memory. To lessen this burden, a number of techniques have been developed to reduce the number of features in a dataset. Manifold learning is a popular approach to nonlinear dimensionality reduction. In this paper, we investigate Diffusion maps manifold learning method for web image auto-annotation task. Diffusion maps manifold learning method is used to reduce the dimension of some visual features. Extensive experiments and analysis on NUS-WIDE-LITE web image dataset with different visual features show how this manifold learning dimensionality reduction method can be applied effectively to image annotation.", "histories": [["v1", "Mon, 8 Dec 2014 10:38:28 GMT  (333kb)", "http://arxiv.org/abs/1412.3352v1", "11 pages, 8 figures"]], "COMMENTS": "11 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG", "authors": ["neda pourali"], "accepted": false, "id": "1412.3352"}, "pdf": {"name": "1412.3352.pdf", "metadata": {"source": "CRF", "title": "WEB IMAGE ANNOTATION BY DIFFUSION MAPS MANIFOLD LEARNING ALGORITHM", "authors": ["Neda Pourali"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of us are able to play by the rules we have set ourselves in order to make them a reality."}, {"heading": "2. DIMENSIONALITY REDUCTION", "text": "The problem of nonlinear dimensionality reduction can be defined as follows: Suppose we have a dataset representation in an nD matrix X consisting of n datavectors (1, 2,..., n}) with dimensionality D. Suppose that this dataset has an intrinsic dimensionality of d (where d < D, and often we have d \u00b2 D). Intrinsic dimensionality means that the points in dataset X are located on or near a manifold with dimensionality of d embedded in D-dimensional space. Dimensionality reduction problems represent the dataset X with dimensionality D, while the geometry of the data is maintained as far as possible. Neither the geometry of data diversity, nor the intrinsic dimensionality d of the dataset Xdimensionality d are known."}, {"heading": "3. DIFFUSION MAPS MANIFOLD LEARNING DIMENSIONALITY REDUCTION", "text": "The diffusion chart (DM) algorithm [21, 22] is based on dynamic systems. Maaten [20] introduced LEM as follows. Diffusion chart techniques are based on a Markov random path on the graph of the data. If the random path takes place for a number of time steps, a measure of the proximity of the data points is obtained. By using this measure, the diffusion distance is defined. In the low-dimensional representation of the data, the paired diffusion distances are kept as good as possible. In the diffusion charts, the graph of the data is constructed first. The weights of the edges in this constructed graph are calculated using the Gaussian core function, which results in a matrix W with the following entries = 1), where the variance of the Gaussian matrix is shown. Afterwards, the normalization process of the matrix is carried out."}, {"heading": "3.1. Diffusion maps on Synthetic Data", "text": "To investigate the performance of algorithms, we first evaluate algorithms only for dimension reduction tasks. We test on two different standard data sets:"}, {"heading": "3.1.1. Swiss Roll Example", "text": "A good method for reducing the nonlinear dimensionality should be to unroll this example into a plane. If the number of the nearest neighbors is set too large, the embedding will take place across folds. This dataset was suggested by Fannenbaum [24]. Figure (2) shows this dataset with N = 2000 data points."}, {"heading": "2.1.2. Punctured Sphere Example", "text": "The 2D projection should have concentric circles, allowing the user to control the height of the sphere. Saul wrote this dataset [25]. Figure (3) shows this dataset with N = 2000 datasets."}, {"heading": "3.1.3. Dimensionality reduction results on two datasets", "text": "We tested the DM method on two previously described datasets. Figure (4) and Figure (5) show the embedding results of this method. We set sigma = {1, 4, 10} in both datasets. The performance of DM to sigma = 10 is best in both datasets."}, {"heading": "4. EXPERIMENT WITH REAL DATA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Feature Vectors", "text": "To evaluate our diverse learning methods, we have tested three characteristics:"}, {"heading": "4.1.1. 73-D edge direction histogram:", "text": "The edge direction histogram encrypts the distribution of the edge directions. The edge direction histogram has a total of 73 edges, in which the first 72 edges are the number of edges whose edge direction is quantified five degrees apart, and the last edge direction histogram is the number of pixels that do not contribute to an edge. To return the different image sizes, they normalized the entries in the histogram as follows: iH = (), [0,..., 71] eH i if i M (), 72H i if i M (6), where () H i is the number of edge points i in the edge direction histogram; Me the total number of edge points that are detected in the subblock of an image, and M the total number of pixels in the subblock. It uses canny filters to detect edge points and bobbin operator to calculate the direction based on the gradient of each edge point [23]."}, {"heading": "4.1.2. 144-D color auto-correlogram (HSV):", "text": "The color autocorlogram is proposed to describe the color distributions and the spatial correlation of color pairs together. The histogram has three dimensions, the first two dimensions of which are the colors of any pixel pair, and the third dimension is their spatial distance. If I show the entire set of image pixels and () c iI represent the subset of pixels with the color () c i, then the color autocorlogram is defined as: 1 () 2 2 () 1 2 (), [P] P c i c i k ij P I I P dr, where, {1, 2,...,} i K, {1, 2,...,} d L and 1 2p p p p p p p p is the distance between 1p and 2p pixels. The color autocorlogram reduces the dimension of 2 () O (N) d because it captures only the spatial correlation between identical colors."}, {"heading": "4.1.3. 225-D block-wise color moments (LAB):", "text": "The first three moments are defined as: 11 N i ijj f N 12 211 ((() Ni ij i j f N 13 311 (()) Ni ij i j s f N, where ijf is the value of the i'th color component of the image pixel j, and N is the total number of pixels in the image. Color moments provide a very accurate representation of the image content compared to other color characteristics. Using three color moments as described above, only nine components (three color moments with three color components each) are used. Due to this density, it cannot have good discrimination force. for NUS _ WIDE datasets, they extract the block-based color moments over 5 x 5 fixed grid partitions, resulting in block-based color moments with a dimension of 225 [23]."}, {"heading": "4.2. Dataset", "text": "We conducted experiments with the NUS-WIDE-LITE dataset, which is represented by Tat-Seng Chua [23]. This is a large web image dataset downloaded from Flickr. All samples of the dataset are monitored and labeled with 81 concepts, and it has about 2 captions per image. The NUS-WIDE-LITE dataset contains a set of 55,615 images and associated tags randomly selected from the full NUS-WIDE dataset. It covers the full 81 concepts of NUS-WIDE. We crop images with less than five captions and then edit the partial dataset. We randomly divide the images into training and test part sets. Half of these are used as training data and the rest for test data. We present each image as three feature vectors."}, {"heading": "4.3. Annotation Method and Evaluation Protocol", "text": "For the NUS WIDE LITE dataset, we tested k = {4, 8,16, 32} next neighbors and d = {10, 20, 30, 40, 50} dimensions for dimension reduction methods. To evaluate the annotations, we used the methodology of previous work proposed by Hideki Nakayama [26]. The system comments test images with 5 words each. These words are then compared with the truth of the ground. To facilitate a quantitative comparison, we use publicly available feature files in the experiment. We test on three features: 1) 225-D block-by-block color moments, 2) 144-D color tree correction diagram, 3) 73-D edge directionist. These features are provided by the authors [23]."}, {"heading": "4.4. Experimental Results", "text": "We report on the results of the NUS-WIDE-LITE dataset. In this dataset, the dimension of the visual characteristics is too large for the k nearest neighbourhood method, so we reduce the dimensions of these three characteristic vectors to d = {10, 20, 30, 40, 50} dimensions. First, the dimensionality reduction time for each of the four methods is summarized in Table (1). PCA's dimensionality reduction time is set to d = 30, and we use a 5-core Intel 2.30 GHz system for the calculation. We compare the performance of the DM method with three other dimensionality reduction algorithms; PCA, LLE, and LEM. PCA's dimensionality reduction time is the lowest of all methods, and DM's dimensionality reduction time is the highest of all methods. Next, we summarize the results for the image annotation task."}, {"heading": "5. CONCLUSION", "text": "In this article, we examined and compared a diverse learning Dimensionality Reduction Algorithm (DM) for caption tasks. Maintaining powerful small algorithms in a scalable manner is an important issue in implementing large-area caption systems. Linear methods such as PCA allow training in linear time and are suitable for this purpose. Overall, we can balance the trade-off between calculation efficiency and annotation accuracy by selecting the dimensions of feature vectors. We can obtain a small-dimensional latent subspace that reflects the semantic removal of instances. To this end, we used the DM method and achieved the best performance in the value of average precision. Our future work could be extended to other compression formats and other advanced multiple learning algorithms."}], "references": [{"title": "Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary", "author": ["P.Duygulu", "K.Barnard", "N. de Freitas", "D. Forsyth"], "venue": "In Proc. ECCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Multiple Bernoulli relevance models for image and video annotation", "author": ["S.Feng", "R. Manmatha", "V. Lavrenko"], "venue": "In proc. IEEE CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A model for learning the semantics of pictures", "author": ["V.Larvenko", "R. Manmatha", "J. Jeon"], "venue": "In proc. NIPS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Image annotation via graph learning", "author": ["J.Liu", "M. Li", "Q. Liu", "H. Lu", "S. Ma"], "venue": "Pattern Recognition,42(2),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "An adaptive graph model for automatic image annotation", "author": ["J.Liu", "M. Li", "W.-Y. Ma", "Q. Liu", "H. Lu"], "venue": "In proc. ACM MIR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Modeling annotated data", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "In Proc. ACM SIGIR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "PLSA-based image auto-annotation: constraining the latent space", "author": ["F.Monay", "D. Gatica-Perez"], "venue": "In Proc. ACM multimedia", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Supervised learning of semantic classes for image annotation and retrieval", "author": ["G.Carneiro", "A.B. Chan", "P.J. Moreno", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M.Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "In Proc. IEEE ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Subspace manifold learning with sample weights", "author": ["Nathan Mekuz", "Christian Bauckhage", "John K. Tsotsos"], "venue": "Image and Vision Computing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A comparative study of nonlinear manifold learning methods for cancer microarray data classification", "author": ["Carlotta Orsenigo", "Carlo Vercellis"], "venue": "Expert Systems with Applications 40,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Cross-media manifold learning for image retrieval and annotation", "author": ["Xianming Liu", "Rongrong Ji", "Hongxun Yao", "Pengfei Xu", "Xiaoshuai Sun", "Tianqiang Liu"], "venue": "Proceedings of the 1st ACM international conference on Multimedia information retrieval,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Dimensionality Reduction: A Comparative Review", "author": ["L.J.P. van der Maaten", "E.O. Postma", "H.J. van den Herik"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Incremental nonlinear dimensionality reduction by manifold learning", "author": ["M.H. Law", "A.K. Jain"], "venue": "IEEE Transactions of Pattern Analysis and Machine Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Ng", "M. Jordan", "Y. Weiss"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "NUS-WIDE: a real-word web image database from National University of Sangapore", "author": ["T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y. -T. Zheng"], "venue": "In proc. ACM CIVR", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction", "author": ["Joshua B. Tenenbaum", "Vin de Silva", "John C. Langford"], "venue": "Science", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds", "author": ["Lawrence K. Saul", "Sam T. Roweis"], "venue": "Journal of Machine Learning Research", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Evaluation of dimensionality reduction methods for image auto-annotation", "author": ["H.Nakayama", "T. Harada", "Y. Kuniyoshi"], "venue": "British Machine Vision Association,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "ML-KNN: A lazy learning approach to multi-label learning", "author": ["Min-Ling Zhang", "Zhi-Hue Zhou"], "venue": "Pattern Recognition", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 126, "endOffset": 135}, {"referenceID": 1, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 126, "endOffset": 135}, {"referenceID": 2, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 126, "endOffset": 135}, {"referenceID": 3, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 149, "endOffset": 155}, {"referenceID": 4, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 149, "endOffset": 155}, {"referenceID": 5, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 169, "endOffset": 175}, {"referenceID": 6, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 169, "endOffset": 175}, {"referenceID": 7, "context": "Many approaches have been proposed to solve the image annotation problem, including classification-based [1, 2], region-based [3, 4, 5], graph-based [6, 7], topic model [8, 9], and generative image patch modelling [10] techniques.", "startOffset": 214, "endOffset": 218}, {"referenceID": 8, "context": "For example Guillaumin [12] worked on 37,000 dimensional data.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "Nathan Mekuz [14] worked on Local Linear Embedding (LLE) method for face recognition problem.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Carlotta Orsenigo [16] proposed manifold learning methods for cancer microarray data classification.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "Xianming Liu [19] worked on cross-media manifold learning for image retrieval and annotation task.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Taxonomy of dimensionality reduction techniques [20].", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "DIFFUSION MAPS MANIFOLD LEARNING DIMENSIONALITY REDUCTION The diffusion maps (DM) algorithm [21, 22] originates from the dynamical systems.", "startOffset": 92, "endOffset": 100}, {"referenceID": 14, "context": "DIFFUSION MAPS MANIFOLD LEARNING DIMENSIONALITY REDUCTION The diffusion maps (DM) algorithm [21, 22] originates from the dynamical systems.", "startOffset": 92, "endOffset": 100}, {"referenceID": 12, "context": "Maaten [20] introduced LEM as follows.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "This dataset was proposed by Tannenbaum [24].", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "This dataset was written by Saul [25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "It uses Canny filter to detect edge points and Sobel operator to compute the direction by the gradient of each edge point [23].", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "So the color autocorrelogram has a dimension of144(36 4) \uf0b4 [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "for NUS_WIDE dataset, they extract the block-wise color moments over 5\u00d75 fixed grid partitions, giving rise to a block-wise color moments with a dimension of 225 [23].", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "Dataset We performed experiment on NUS-WIDE-LITE dataset that represented by Tat-Seng Chua [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "For evaluating of annotation, we follow the methodology of previous work that Hideki Nakayama [26] proposed it.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "These features are provided by the authors of [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Average Precision is originally used in information retrieval systems to evaluate the document ranking performance for query retrieval [27].", "startOffset": 135, "endOffset": 139}], "year": 2014, "abstractText": "Automatic image annotation is one of the most challenging problems in machine vision areas. The goal of this task is to predict number of keywords automatically for images captured in real data. Many methods are based on visual features in order to calculate similarities between image samples. But the computation cost of these approaches is very high. These methods require many training samples to be stored in memory. To lessen this burden, a number of techniques have been developed to reduce the number of features in a dataset. Manifold learning is a popular approach to nonlinear dimensionality reduction. In this paper, we investigate Diffusion maps manifold learning method for web image auto-annotation task. Diffusion maps manifold learning method is used to reduce the dimension of some visual features. Extensive experiments and analysis on NUS-WIDE-LITE web image dataset with different visual features show how this manifold learning dimensionality reduction method can be applied effectively to image annotation.", "creator": null}}}