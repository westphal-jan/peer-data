{"id": "1412.3518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Appropriate Causal Models and the Stability of Causation", "abstract": "Causal models defined in terms of structural equations have proved to be quite a powerful way of representing knowledge regarding causality. However, a number of authors have given examples that seem to show that the Halpern-Pearl (HP) definition of causality gives intuitively unreasonable answers. Here it is shown that, for each of these examples, we can give two stories consistent with the description in the example, such that intuitions regarding causality are quite different for each story. By adding additional variables, we can disambiguate the stories. Moreover, in the resulting causal models, the HP definition of causality gives the intuitively correct answer. It is also shown that, by adding extra variables, a modification to the original HP definition made to deal with an example of Hopkins and Pearl may not be necessary. Given how much can be done by adding extra variables, there might be a concern that the notion of causality is somewhat unstable. Can adding extra variables in a \"conservative\" way (i.e., maintaining all the relations between the variables in the original model) cause the answer to the question \"Is X=x a cause of Y=y\" to alternate between \"yes\" and \"no\"? It is shown that we can have such alternation infinitely often, but if we take normality into consideration, we cannot. Indeed, under appropriate normality assumptions. adding an extra variable can change the answer from \"yes\" to \"no\", but after that, it cannot cannot change back to \"yes\".", "histories": [["v1", "Thu, 11 Dec 2014 02:16:39 GMT  (52kb)", "https://arxiv.org/abs/1412.3518v1", "A preliminary version of this paper appears in the Proceedings of the Fourteenth International Conference on Principles of Knowledge Representation and Reasoning (KR 2014)}, 2014"], ["v2", "Mon, 3 Aug 2015 17:14:55 GMT  (50kb)", "http://arxiv.org/abs/1412.3518v2", "A preliminary version of this paper appears in the Proceedings of the Fourteenth International Conference on Principles of Knowledge Representation and Reasoning (KR 2014)}, 2014. To appear, Review of Symbolic Logic"]], "COMMENTS": "A preliminary version of this paper appears in the Proceedings of the Fourteenth International Conference on Principles of Knowledge Representation and Reasoning (KR 2014)}, 2014", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["joseph y halpern"], "accepted": false, "id": "1412.3518"}, "pdf": {"name": "1412.3518.pdf", "metadata": {"source": "CRF", "title": "Appropriate Causal Models and the Stability of Causation", "authors": ["Joseph Y. Halpern"], "emails": ["halpern@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.35 18v2 [cs.AI] 3A ug"}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Review", "text": "In this section, I will briefly discuss the definitions of causality, the HP definition (s) of causality, and the extension that takes into account the normality of Halpern and Hitchcock. Exposure is largely derived from [Halpern 2008]. Readers are encouraged to consult [Halpern and Pearl 2005] and [Halpern and Hitchcock 2015] for further details and intuition."}, {"heading": "2.1 Causal models", "text": "The HP approach assumes that the world is described in terms of random variables and their values. Some random variables can have a causal influence on others (V.) This influence is modelled by a series of structural equations. It makes conceptual sense to divide the random variables into two sets: the exogenous variables, whose values are determined by factors outside the model, and the endogenous variables, whose values are ultimately determined by the exogenous variables. For example, we could have endogenous variables that describe what voters actually do (i.e., which candidate they choose), exogenous variables that describe the factors that determine how voters vote, and a variable that describes the result (who wins). The structural equations describe how the result is determined (majority rules; a candidate wins if A and at least two of B, C, D, and E vote for him; and so on. Formally, a sufficient model is a pair M (S is a signature where S is a signature)."}, {"heading": "2.2 A language for reasoning about causality", "text": "In order to define causality carefully, it is useful to have a language that thinks about causality. In the face of a signature S = (U, V, R), a primitive event is a formula of the form X = x, for X-V and x-R (X). A causal formula (above S) is one of the form [Y1 \u2190 y1,.., Yk \u2190 yk], where \u2022 x is a Boolean combination of primitive events, \u2022 Y1,..., Yk are unique variables in V, and \u2022 yi-R (Yi). Such a formula is abbreviated as [~ Y-y]. The special case in which k = 0 is abbreviated as a conjunction."}, {"heading": "2.3 The definition(s) of causality", "text": "The definition of causality, like many others, is based on counterfactors. The idea is that A is a cause of B, if A did not occur (although it did happen), then B would not have occurred. But there are many examples that show that this naive definition will not quite work. To take just one example, consider the following story, which is discussed due to Ned Hall and the already discussed manner of Pearl 2005, of which the following version is not taken up. Suzy and Billy both fetch rocks and throw them onto a bottle. Suzy's skirt gets there first, which smashes the bottle. Since both throws are perfectly accurate, Billy's would smash the bottle if it was not anticipated by Suzy's throw. We would like to say that Suzy's throw is a cause of bottle fragmentation, and Billy's is not. But if Suzy's had not thrown, Billy's would smash the bottle and smash it. \""}, {"heading": "3 The Examples", "text": "In this section I will look at examples based on Spohn [2008], Weslake [2015], Hall [2007], Glymour et al. [2010] and Livengood [2013]. I will go through these examples in order."}, {"heading": "3.1 Throwing rocks at bottles", "text": "A na\u00efve model of rock thrower history has only three binary variables ST, BT = BT (for \"Suzy,\" \"Billy throws\" and \"bottle shatters\"); the fact that the variables are binary means that they take values in the BT bottle; the values of ST and BT are determined by context; the value of BS = BT \"s is given by the equation BS = BT.\" The bottle breaks when Suzy throws or Billy throws. (Although I have included the exogenous variables here, let's assume there is only one exogenous variable; let's be the context that in ST = BT = 1: Suzy and Billy's \"s\" s \"in both Figure 1 and Figure 1.\""}, {"heading": "3.2 Spohn\u2019s example", "text": "The next example is Spohn [2008]. Example 3.1: There are four endogenous binary variables, A, B, C and S, which take values 1 (up) and 0 (down). Intuitively, A and B are supposed to be alternative causes of C, and S acts as a switch. If S = 0, the causal route from A to C is active and the one from B to C is dead; and if S = 1, the causal route from A to C is dead and the one from B to C is active, there are no causal relationships between A, B and S; their values are determined by the context. The equation for C is C = (on S to A) and B to A. Suppose that the context is like A = B = S on A = 1, designed in this way."}, {"heading": "3.3 Weslake\u2019s example", "text": "The next example is attributed to Weslake [2015, Example 10].Example 3.2: A lamp L is controlled by three switches, A, B, and C, each of which has three possible positions, \u2212 1, 0, and 1. The lamp switches iff two or more of the switches are in the same position. (So, L = 1 iff (A = B) - (B = C) - (A = C) - (A = C). Suppose that in the actual context A = 1, B = \u2212 1, and C = \u2212 1. Intuition suggests that while B = \u2212 1 iff (A = 1) causes L = 1, A = 1 should not be - (A = C) - (A = C) - causal effects of B or C do not match, it has no causal effects on the result."}, {"heading": "3.4 Hall\u2019s example", "text": "Hall's [2007] gives an example that is meant to illustrate how a poor choice of variables leads to unreasonable answers = = D. I repeat here because, although I agree with his main point (this is actually one of the main points of this paper!), I actually disagree with one of his conclusions. Actually, what I am presenting is a slightly simplified version of his example that retains all the necessary characteristics. Consider a model M with four endogenous variables, A, B, D and E. The values of A and D are determined by the context. The values of B and E are determined by the equations B = A and E = D.7 Let us suppose that the context u is such that A = D = 1. In the context (M, u), A = 1 is a cause of B = 1 and not a cause of E. The values of B and E are given by the equations B."}, {"heading": "3.5 Glymour et al.\u2019s example", "text": "The next example is due to Glymour et al. = 2010].Example 3.3: A ranch has five individuals: a1,., a5. You must vote on two possible outcomes: stay by the campfire (O = 0) or go on a round (O = 1). Let Ai be the random variable denating ai's vote, so Ai = j if ai votes for result j. There is a complicated rule for deciding the result. If a1 and a2 agree (i.e. if A1 = A2), then that is the result. If a2,., a5 agree, and a1 votes otherwise, then the result is given by a1's vote (i.e., O = A1). In the actual situation, A1 = A2 and A3 it is the result."}, {"heading": "3.6 Livengood\u2019s voting examples", "text": "In this case, most of them are unable to abide by the rules they have imposed on themselves."}, {"heading": "4.1 The Hopkins-Pearl example", "text": "Let me begin by examining the Hopkins-Pearl example, which was intended to show that AC2 (b \u2032) was inappropriate; the following description comes from [Halpern and Pearl 2005].Example 4.1: Suppose a prisoner dies either when A loads B his weapon and B shoots, or when C loads and shoots his weapon. Let's take D to represent the prisoner's death and make the obvious assumptions about the meaning of the variables, we have D = (A, B), C. Let's assume that in the actual context u, A, B doesn't shoot, but C loads and shoots his weapon so the prisoner dies. That is, A = 1, B = 0, and C = 1. Clearly, C = 1 is a cause of D = 1. We wouldn't want to say that A = 1 is a cause of D = 1 because B didn't shoot (i.e., B = 0)."}, {"heading": "4.2 Conservative extensions", "text": "Similarly, adding additional variables in all the other examples above is causal. Of course, without any limitations, it is easy to add variables to get any desired result. Consider, for example, the rock throw model M \u00b2 RT. Suppose we add a variable BH1 model with equations that specify BH1 = BT and BS = SH-BT-BH1. This leads to a new \"causal path\" from BT to BS that goes through BH1, regardless of any other paths. Unsurprisingly, in this model BT = 1 is actually a cause of BS = 1. But this seems like cheating. Adding this new causal path fundamentally changes the scenario; Billy's throw has a new way of influencing whether the bottle is smashed or not. While it seems reasonable to improve a model by adding new information, we do not want to affect what we know about the old variables that we would be better at."}, {"heading": "4.3 Avoiding AC2(b)", "text": "I now show that we can always use AC2 (b) instead of AC2 (b) (b) if we have additional variables.Theorem 4.4: If X = x is not a cause for Y = y in (M, ~ ~ ~ ~ ~ ~ ~) using AC2 (b) but a cause using AC2 (b), then there is a model M \u2032 that is a conservative extension of M so that X = x is not a cause for Y = y using AC2 (b). Proof: Let us assume that (~ w, x) is a witness that X = x x is a cause for Y = y in (M, ~ u) using AC2 (b). Let us (M, ~ u) assume that (W = ~ w) must have a cause, because otherwise it is simply that X = x x x is a cause for Y."}, {"heading": "4.4 Discussion", "text": "Theorem 4.4 suggests that by adding additional variables accordingly, we can go back to the definition of causality by using AC2 (b) and not AC2 (b), which has some technical advantages. For example, AC2 (b) causes always have individual conjunctions [Pus and Lukasiewicz 2002; Hopkins 2001]. As shown in [Halpern 2008], this is generally not the case for AC2 (b); it may be that X1 = x1 x X2 = x2 is a cause of Y = y without X1 = x1 or X2 = x2 being causes (see also Example 6.6). It also seems that tests for causality are more difficult by adding AC2 (b). Pus and Lukasiewicz [2002] show that by using AC2 (b), tests for causality are not complete for binary models (where all random variables are binary) and for AC2 in general."}, {"heading": "5 Normality", "text": "In fact, most of them are able to play by the rules they have established in the past, and they are able to play by the rules they have established in the past."}, {"heading": "6 The Stability of (Non-)Causality", "text": "The examples in Section 3 raise a potential problem. Let's look again at the stone-throwing example. Adding additional variables changes BT = 1 from being as the cause of BS = 1 to being as the cause. Could one add more variables that convert BT = 1 back to being as the cause? Could it go further? These questions of stability have been raised before. Strevens [2008] provides an example where what Strevens calls a cause can become a non-cause if additional variables are added according to Woodward's [2003] definition of causality; 10 Eberhardt [2014] shows that this can also happen for the causality type (\"Smoking causes cancer\" instead of \"Mr. T's 20 years of causing cancer\") with Woodward's definition. Here, I look at the situation in more detail for the HP definition and show that it can get much worse. In general, we can convert an event from a cause to a non-cause and then again infinitely. Let's look at an arbitrary context with an M and a B (with such contexts as A and B)."}, {"heading": "7 Conclusions", "text": "This paper has shown that the HP definition of causality is remarkably resilient, but it emphasizes how sensitive the attribution of causality can be to the choice of model. The focus was on showing that the choice of variables is a powerful modeling tool, but it is a bottle that can be abused. One lesson that clearly emerges is the need to have variables that describe the mechanism of causality, especially when there is more than one mechanism. However, this is not a general prescription. Rather, it is a heuristic one for the construction of a \"good\" model. As Halpern and Hitchcock [2010] point out, the construction of a good model is still more art than science. The importance of the choice of variables for attributing causality leads to an obvious question: to what extent is the choice of variables determined by history. Certainly some variables are explicitly thrown in a trigger-oriented Suzy story, and Billy will be quite clear about it."}], "references": [{"title": "The computational complexity of structure-based causality", "author": ["G. Aleksandrowicz", "H. Chockler", "J.Y. Halpern", "A. Ivrii"], "venue": "Proc. Twenty-Eighth National Conference on Artificial Intelligence (AAAI \u201914), pp. 974\u2013980.", "citeRegEx": "Aleksandrowicz et al\\.,? 2014", "shortCiteRegEx": "Aleksandrowicz et al\\.", "year": 2014}, {"title": "Explaining counterexamples using causality", "author": ["I. Beer", "S. Ben-David", "H. Chockler", "A. Orni", "R.J. Trefler"], "venue": "Formal Methods in System Design 40(1), 20\u201340.", "citeRegEx": "Beer et al\\.,? 2012", "shortCiteRegEx": "Beer et al\\.", "year": 2012}, {"title": "Cause without default", "author": ["T. Blanchard", "J. Schaffer"], "venue": "unpublished manuscript.", "citeRegEx": "Blanchard and Schaffer,? 2013", "shortCiteRegEx": "Blanchard and Schaffer", "year": 2013}, {"title": "Moral appraisals affect doing/allowing judgments", "author": ["F. Cushman", "J. Knobe", "W. Sinnott-Armstrong"], "venue": "Cognition 108(1), 281\u2013289.", "citeRegEx": "Cushman et al\\.,? 2008", "shortCiteRegEx": "Cushman et al\\.", "year": 2008}, {"title": "Direct causes and the trouble with soft intervention", "author": ["F. Eberhardt"], "venue": "Erkenntnis 79(4), 755\u2013777.", "citeRegEx": "Eberhardt,? 2014", "shortCiteRegEx": "Eberhardt", "year": 2014}, {"title": "Complexity results for structure-based causality", "author": ["T. Eiter", "T. Lukasiewicz"], "venue": "Artificial Intelligence 142(1), 53\u201389.", "citeRegEx": "Eiter and Lukasiewicz,? 2002", "shortCiteRegEx": "Eiter and Lukasiewicz", "year": 2002}, {"title": "Spreading the blame: the allocation of responsibility amongst multiple agents", "author": ["T. Gerstenberg", "D. Lagnado"], "venue": "Cognition 115, 166\u2013171.", "citeRegEx": "Gerstenberg and Lagnado,? 2010", "shortCiteRegEx": "Gerstenberg and Lagnado", "year": 2010}, {"title": "Actual causation: a stone soup essay", "author": ["C. Glymour", "D. Danks", "B. Glymour", "F. Eberhardt", "J. Ramsey", "R. Scheines", "P. Spirtes", "C.M. Teng", "J. Zhang"], "venue": "Synthese 175, 169\u2013192.", "citeRegEx": "Glymour et al\\.,? 2010", "shortCiteRegEx": "Glymour et al\\.", "year": 2010}, {"title": "Structural equations and causation", "author": ["N. Hall"], "venue": "Philosophical Studies 132, 109\u2013136.", "citeRegEx": "Hall,? 2007", "shortCiteRegEx": "Hall", "year": 2007}, {"title": "Defaults and normality in causal structures", "author": ["J.Y. Halpern"], "venue": "Principles of Knowledge Representation and Reasoning: Proc. Eleventh International Conference (KR \u201908), pp. 198\u2013208.", "citeRegEx": "Halpern,? 2008", "shortCiteRegEx": "Halpern", "year": 2008}, {"title": "Actual causation and the art of modeling", "author": ["J.Y. Halpern", "C. Hitchcock"], "venue": "R. Dechter, H. Geffner, and J. Halpern (Eds.), Causality, Probability, and Heuristics: A Tribute to Judea Pearl, pp. 383\u2013406. London: College Publications.", "citeRegEx": "Halpern and Hitchcock,? 2010", "shortCiteRegEx": "Halpern and Hitchcock", "year": 2010}, {"title": "Graded causation and defaults", "author": ["J.Y. Halpern", "C. Hitchcock"], "venue": "British Journal for the Philosophy of Science 66(2), 413\u2013457.", "citeRegEx": "Halpern and Hitchcock,? 2015", "shortCiteRegEx": "Halpern and Hitchcock", "year": 2015}, {"title": "Causes and explanations: A structural-model approach", "author": ["J.Y. Halpern", "J. Pearl"], "venue": "Part I: Causes. In Proc. Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI 2001), pp. 194\u2013202.", "citeRegEx": "Halpern and Pearl,? 2001", "shortCiteRegEx": "Halpern and Pearl", "year": 2001}, {"title": "Causes and explanations: A structural-model approach", "author": ["J.Y. Halpern", "J. Pearl"], "venue": "Part I: Causes. British Journal for Philosophy of Science 56(4), 843\u2013887.", "citeRegEx": "Halpern and Pearl,? 2005", "shortCiteRegEx": "Halpern and Pearl", "year": 2005}, {"title": "Causal powers", "author": ["E. Hiddleston"], "venue": "British Journal for Philosophy of Science 56, 27\u201359.", "citeRegEx": "Hiddleston,? 2005", "shortCiteRegEx": "Hiddleston", "year": 2005}, {"title": "The intransitivity of causation revealed in equations and graphs", "author": ["C. Hitchcock"], "venue": "Journal of Philosophy XCVIII(6), 273\u2013299.", "citeRegEx": "Hitchcock,? 2001", "shortCiteRegEx": "Hitchcock", "year": 2001}, {"title": "Prevention, preemption, and the principle of sufficient reason", "author": ["C. Hitchcock"], "venue": "Philosophical Review 116, 495\u2013532.", "citeRegEx": "Hitchcock,? 2007", "shortCiteRegEx": "Hitchcock", "year": 2007}, {"title": "Cause and norm", "author": ["C. Hitchcock", "J. Knobe"], "venue": "Journal of Philosophy 106, 587\u2013612.", "citeRegEx": "Hitchcock and Knobe,? 2009", "shortCiteRegEx": "Hitchcock and Knobe", "year": 2009}, {"title": "A proof of the conjunctive cause conjecture", "author": ["M. Hopkins"], "venue": "Unpublished manuscript.", "citeRegEx": "Hopkins,? 2001", "shortCiteRegEx": "Hopkins", "year": 2001}, {"title": "Clarifying the usage of structural models for commonsense causal reasoning", "author": ["M. Hopkins", "J. Pearl"], "venue": "Proc. AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning.", "citeRegEx": "Hopkins and Pearl,? 2003", "shortCiteRegEx": "Hopkins and Pearl", "year": 2003}, {"title": "Norm theory: comparing reality to its alternatives", "author": ["D. Kahneman", "D.T. Miller"], "venue": "Psychological Review 94(2), 136\u2013153.", "citeRegEx": "Kahneman and Miller,? 1986", "shortCiteRegEx": "Kahneman and Miller", "year": 1986}, {"title": "Causal judgment and moral judgment: two experiments", "author": ["J. Knobe", "B. Fraser"], "venue": "W. SinnottArmstrong (Ed.), Moral Psychology, Volume 2: The Cognitive Science of Morality, pp. 441\u2013447. Cambridge, MA: MIT Press.", "citeRegEx": "Knobe and Fraser,? 2008", "shortCiteRegEx": "Knobe and Fraser", "year": 2008}, {"title": "Causal responsibility and counterfactuals", "author": ["D.A. Lagnado", "T. Gerstenberg", "R. Zultan"], "venue": "Cognitive Science 37, 1036\u20131073.", "citeRegEx": "Lagnado et al\\.,? 2013", "shortCiteRegEx": "Lagnado et al\\.", "year": 2013}, {"title": "Actual causation in simple voting scenarios", "author": ["J. Livengood"], "venue": "Nous 47(2), 316\u2013345.", "citeRegEx": "Livengood,? 2013", "shortCiteRegEx": "Livengood", "year": 2013}, {"title": "Comments on woodward, Making Things Happen", "author": ["M. Strevens"], "venue": "Philosophy and Phenomenology 77(1), 171\u2013192.", "citeRegEx": "Strevens,? 2008", "shortCiteRegEx": "Strevens", "year": 2008}, {"title": "A partial theory of actual causation", "author": ["B. Weslake"], "venue": "British Journal for the Philosophy of Science. To appear.", "citeRegEx": "Weslake,? 2015", "shortCiteRegEx": "Weslake", "year": 2015}, {"title": "Making Things Happen: A Theory of Causal Explanation", "author": ["J. Woodward"], "venue": "Oxford, U.K.: Oxford University Press.", "citeRegEx": "Woodward,? 2003", "shortCiteRegEx": "Woodward", "year": 2003}], "referenceMentions": [{"referenceID": 13, "context": "However, a number of authors have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers.", "startOffset": 119, "endOffset": 143}, {"referenceID": 9, "context": "However, a number of authors have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers. Here it is shown that, for each of these examples, we can give two stories consistent with the description in the example, such that intuitions regarding causality are quite different for each story. By adding additional variables, we can disambiguate the stories. Moreover, in the resulting causal models, the HP definition of causality gives the intuitively correct answer. It is also shown that, by adding extra variables, a modification to the original HP definition made to deal with an example of Hopkins and Pearl [2003] may not be necessary.", "startOffset": 76, "endOffset": 712}, {"referenceID": 1, "context": "For example, they have been used to find causes of errors in software [Beer et al. 2012] and have been shown to be useful in predicting human attributions of responsibility [Gerstenberg and Lagnado 2010; Lagnado, Gerstenberg, and Zultan 2013].", "startOffset": 70, "endOffset": 88}, {"referenceID": 6, "context": "2012] and have been shown to be useful in predicting human attributions of responsibility [Gerstenberg and Lagnado 2010; Lagnado, Gerstenberg, and Zultan 2013].", "startOffset": 90, "endOffset": 159}, {"referenceID": 7, "context": "However, a number of authors [Glymour et al. 2010; Hall 2007; Livengood 2013; Spohn 2008; Weslake 2015] have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers.", "startOffset": 29, "endOffset": 103}, {"referenceID": 8, "context": "However, a number of authors [Glymour et al. 2010; Hall 2007; Livengood 2013; Spohn 2008; Weslake 2015] have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers.", "startOffset": 29, "endOffset": 103}, {"referenceID": 23, "context": "However, a number of authors [Glymour et al. 2010; Hall 2007; Livengood 2013; Spohn 2008; Weslake 2015] have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers.", "startOffset": 29, "endOffset": 103}, {"referenceID": 25, "context": "However, a number of authors [Glymour et al. 2010; Hall 2007; Livengood 2013; Spohn 2008; Weslake 2015] have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers.", "startOffset": 29, "endOffset": 103}, {"referenceID": 13, "context": "2010; Hall 2007; Livengood 2013; Spohn 2008; Weslake 2015] have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers.", "startOffset": 149, "endOffset": 173}, {"referenceID": 2, "context": "The need to choose the causal model carefully has been pointed out frequently [Blanchard and Schaffer 2013; Hall 2007; Halpern and Pearl 2005; Halpern and Hitchcock 2010; Hitchcock 2001; Hitchcock 2007].", "startOffset": 78, "endOffset": 202}, {"referenceID": 8, "context": "The need to choose the causal model carefully has been pointed out frequently [Blanchard and Schaffer 2013; Hall 2007; Halpern and Pearl 2005; Halpern and Hitchcock 2010; Hitchcock 2001; Hitchcock 2007].", "startOffset": 78, "endOffset": 202}, {"referenceID": 13, "context": "The need to choose the causal model carefully has been pointed out frequently [Blanchard and Schaffer 2013; Hall 2007; Halpern and Pearl 2005; Halpern and Hitchcock 2010; Hitchcock 2001; Hitchcock 2007].", "startOffset": 78, "endOffset": 202}, {"referenceID": 10, "context": "The need to choose the causal model carefully has been pointed out frequently [Blanchard and Schaffer 2013; Hall 2007; Halpern and Pearl 2005; Halpern and Hitchcock 2010; Hitchcock 2001; Hitchcock 2007].", "startOffset": 78, "endOffset": 202}, {"referenceID": 15, "context": "The need to choose the causal model carefully has been pointed out frequently [Blanchard and Schaffer 2013; Hall 2007; Halpern and Pearl 2005; Halpern and Hitchcock 2010; Hitchcock 2001; Hitchcock 2007].", "startOffset": 78, "endOffset": 202}, {"referenceID": 16, "context": "The need to choose the causal model carefully has been pointed out frequently [Blanchard and Schaffer 2013; Hall 2007; Halpern and Pearl 2005; Halpern and Hitchcock 2010; Hitchcock 2001; Hitchcock 2007].", "startOffset": 78, "endOffset": 202}, {"referenceID": 10, "context": ", in [Halpern and Hitchcock 2010]), they are certainly not definitive.", "startOffset": 5, "endOffset": 33}, {"referenceID": 9, "context": "As already observed by Halpern and Hitchcock [2015], adding extra variables also lets us deal with two other concerns that resulted in changes to the original HP definition.", "startOffset": 23, "endOffset": 52}, {"referenceID": 9, "context": "As already observed by Halpern and Hitchcock [2015], adding extra variables also lets us deal with two other concerns that resulted in changes to the original HP definition. In Section 4, I consider an example due to Hopkins and Pearl [2003] that motivated one of the changes.", "startOffset": 23, "endOffset": 242}, {"referenceID": 9, "context": "As already observed by Halpern and Hitchcock [2015], adding extra variables also lets us deal with two other concerns that resulted in changes to the original HP definition. In Section 4, I consider an example due to Hopkins and Pearl [2003] that motivated one of the changes. After showing how this example can be dealt with by adding an extra variable in a natural way (without modifying the original HP definition), I show that this approach generalizes: we can always add extra variables so as to get a model where the original HP definition can be used. In Section 5, I discuss an example due to Hiddleston [2005] that motivated the addition of normality considerations to the basic HP framework (see Section 2).", "startOffset": 23, "endOffset": 619}, {"referenceID": 11, "context": "In the next section, I review the HP definition (and the original definition) and its extension to deal with normality, as discussed in [Halpern and Hitchcock 2015].", "startOffset": 136, "endOffset": 164}, {"referenceID": 9, "context": "The exposition is largely taken from [Halpern 2008].", "startOffset": 37, "endOffset": 51}, {"referenceID": 13, "context": "The reader is encouraged to consult [Halpern and Pearl 2005], and [Halpern and Hitchcock 2015] for more details and intuition.", "startOffset": 36, "endOffset": 60}, {"referenceID": 11, "context": "The reader is encouraged to consult [Halpern and Pearl 2005], and [Halpern and Hitchcock 2015] for more details and intuition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 13, "context": "Following [Halpern and Pearl 2005], I restrict attention here to what are called recursive (or acyclic) models.", "startOffset": 10, "endOffset": 34}, {"referenceID": 13, "context": "To take just one example, consider the following story, due to Ned Hall and already discussed in [Halpern and Pearl 2005], from where the following version is taken.", "startOffset": 97, "endOffset": 121}, {"referenceID": 12, "context": "The original HP paper [Halpern and Pearl 2001] used a weaker version of AC2(b).", "startOffset": 22, "endOffset": 46}, {"referenceID": 11, "context": "I now briefly sketch one way that this can be done, following the approach in [Halpern and Hitchcock 2015].", "startOffset": 78, "endOffset": 106}, {"referenceID": 9, "context": "I now briefly sketch one way that this can be done, following the approach in [Halpern and Hitchcock 2015]. (See Section 5 for some discussion of the need for normality.) Take a world (in a model M ) to be a complete assignment of values to the endogenous variables in M .3 (See the discussion after Corollary 6.4 for why it is conimportant that a world is an assignment only to the endogenous variables, and not all the variables, including the exogenous variables.) We assume a partial preorder on worlds, that is, a reflexive transitive relation.4 Intuitively, if s s, then s is at least as normal, or typical, as s. We can use normality in the definition of causality in two ways. Say that a world s is a witness world for ~ X = ~x being a cause of \u03c6 in (M,~u) if there is a witness ( ~ W, ~ w, ~x) to ~ X = ~x being a cause of \u03c6 and s = s ~ X=~x, ~ W=~ w,~ u, where s ~ X=~x, ~ W=~ w,~ u is the world that results by setting ~ X to ~x and ~ W to ~ w in context ~u. We can then modify AC2(a) so as to require that we consider ~ X = ~x to be a cause of \u03c6 in (M,~u) only if the witness world s for ~ X = ~x being a cause is such that s s~ u, where s~ u is the world determined by context ~u; call this modified version AC2(a). AC2(a) says that, in determining causality, we consider only possibilities that result from altering atypical features of a world to make them more typical, rather than vice versa. This captures an observation made by Kahneman and Miller [1986] regarding human ascriptions of causality.", "startOffset": 79, "endOffset": 1474}, {"referenceID": 11, "context": "3In [Halpern and Hitchcock 2015], a world is defined as a complete assignment of values to the exogenous variables, but this is a typo.", "startOffset": 4, "endOffset": 32}, {"referenceID": 17, "context": ", [Cushman, Knobe, and Sinnott-Armstrong 2008; Hitchcock and Knobe 2009; Knobe and Fraser 2008].", "startOffset": 2, "endOffset": 95}, {"referenceID": 21, "context": ", [Cushman, Knobe, and Sinnott-Armstrong 2008; Hitchcock and Knobe 2009; Knobe and Fraser 2008].", "startOffset": 2, "endOffset": 95}, {"referenceID": 22, "context": "In this section, I consider examples due to Spohn [2008], Weslake [2015], Hall [2007], Glymour et al.", "startOffset": 58, "endOffset": 73}, {"referenceID": 7, "context": "In this section, I consider examples due to Spohn [2008], Weslake [2015], Hall [2007], Glymour et al.", "startOffset": 74, "endOffset": 86}, {"referenceID": 7, "context": "In this section, I consider examples due to Spohn [2008], Weslake [2015], Hall [2007], Glymour et al. [2010], and Livengood [2013].", "startOffset": 87, "endOffset": 109}, {"referenceID": 7, "context": "In this section, I consider examples due to Spohn [2008], Weslake [2015], Hall [2007], Glymour et al. [2010], and Livengood [2013]. I go through these examples in turn.", "startOffset": 87, "endOffset": 131}, {"referenceID": 9, "context": "As already pointed out by Halpern and Pearl [2005], in MRT Suzy and Billy play completely symmetric roles.", "startOffset": 26, "endOffset": 51}, {"referenceID": 9, "context": "Following Halpern and Pearl [2005], we extend MRT so that it can express the fact that Suzy\u2019s rock hit first by adding two more variables:", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "The next example is due to Glymour et al. [2010].", "startOffset": 27, "endOffset": 49}, {"referenceID": 8, "context": "7Hall [2007] also has variables C and F such that C = B and F = E; adding them does not affect any of the discussion here (or in Hall\u2019s paper).", "startOffset": 1, "endOffset": 13}, {"referenceID": 7, "context": "But all this talk of mechanisms (which is also implicit in Glymour et al. [2010]; in footnote 11, they say that setting A2 back to its original value of 1 \u201cbrings out the original result, but in a different way\u201d) suggests that the mechanism should be part of the model.", "startOffset": 59, "endOffset": 81}, {"referenceID": 23, "context": "As Livengood [2013] points out, voting can lead to some apparently unreasonable causal outcomes (at least, if we model things naively).", "startOffset": 3, "endOffset": 20}, {"referenceID": 23, "context": "Now, following Livengood [2013], consider a vote where everyone can either vote for one of three candidates.", "startOffset": 15, "endOffset": 32}, {"referenceID": 23, "context": "But, as Livengood [2013] points out, we don\u2019t speak of Gore costing Nader a victory, although in a naive HP model of the situation, all the voters for Gore are causes of Nader not winning as much as the voters for Nader are causes of Gore not winning.", "startOffset": 8, "endOffset": 25}, {"referenceID": 13, "context": "The following description is taken from [Halpern and Pearl 2005].", "startOffset": 40, "endOffset": 64}, {"referenceID": 9, "context": "Nevertheless, as pointed out by Halpern and Hitchcock [2015], we can use AC2(b) if we have the \u201cright\u201d model.", "startOffset": 32, "endOffset": 61}, {"referenceID": 5, "context": "For example, with AC2(b), causes are always single conjuncts [Eiter and Lukasiewicz 2002; Hopkins 2001].", "startOffset": 61, "endOffset": 103}, {"referenceID": 18, "context": "For example, with AC2(b), causes are always single conjuncts [Eiter and Lukasiewicz 2002; Hopkins 2001].", "startOffset": 61, "endOffset": 103}, {"referenceID": 9, "context": "As shown in [Halpern 2008], this is not in general the case with AC2(b); it may be that X1 = x1 \u2227 X2 = x2 is a cause of Y = y with neither X1 = x1 nor X2 = x2 being causes (see also Example 6.", "startOffset": 12, "endOffset": 26}, {"referenceID": 5, "context": "For example, with AC2(b), causes are always single conjuncts [Eiter and Lukasiewicz 2002; Hopkins 2001]. As shown in [Halpern 2008], this is not in general the case with AC2(b); it may be that X1 = x1 \u2227 X2 = x2 is a cause of Y = y with neither X1 = x1 nor X2 = x2 being causes (see also Example 6.6). It also seems that testing for causality is harder using AC2(b). Eiter and Lukasiewicz [2002] show that, using AC2(b), testing for causality is NP-complete for binary models (where all random variables are binary) and \u03a32-complete in general; with AC2(b), it seems to be \u03a32-complete in the binary case and \u03a03-complete in the general case [Aleksandrowicz, Chockler, Halpern, and Ivrii 2014].", "startOffset": 62, "endOffset": 395}, {"referenceID": 1, "context": "For example, in Beer et al.\u2019s [2012] analysis of software errors using causality, the variables chosen for the analysis are determined by the program specification.", "startOffset": 16, "endOffset": 37}, {"referenceID": 11, "context": "As was already observed in [Halpern and Hitchcock 2015], the example that motivated the use of normality considerations can also be dealt with by adding variables to the model in an arguably reasonable way.", "startOffset": 27, "endOffset": 55}, {"referenceID": 9, "context": "As was already observed in [Halpern and Hitchcock 2015], the example that motivated the use of normality considerations can also be dealt with by adding variables to the model in an arguably reasonable way. Consider the following example, given by Hitchcock [2007], based on an example due to Hiddleston [2005].", "startOffset": 28, "endOffset": 265}, {"referenceID": 9, "context": "As was already observed in [Halpern and Hitchcock 2015], the example that motivated the use of normality considerations can also be dealt with by adding variables to the model in an arguably reasonable way. Consider the following example, given by Hitchcock [2007], based on an example due to Hiddleston [2005].", "startOffset": 28, "endOffset": 311}, {"referenceID": 11, "context": "Arguably a better solution to this problem, already suggested in [Halpern and Hitchcock 2015], is to add an additional variable.", "startOffset": 65, "endOffset": 93}, {"referenceID": 11, "context": "As we have seen, thinking in terms of normality helps in the Livengood voting example; there are many other examples given in [Halpern and Hitchcock 2015] where the use of normality, and in particular the ability to use normality to allow for gradation of causality, seems to be helpful.", "startOffset": 126, "endOffset": 154}, {"referenceID": 24, "context": "Strevens [2008] provides an example where what Strevens calls a cause can become a non-cause if extra variables are added according to Woodward\u2019s [2003]", "startOffset": 0, "endOffset": 16}, {"referenceID": 24, "context": "Strevens [2008] provides an example where what Strevens calls a cause can become a non-cause if extra variables are added according to Woodward\u2019s [2003]", "startOffset": 0, "endOffset": 153}, {"referenceID": 4, "context": "definition of causality;10 Eberhardt [2014] shows that this can also happen for type causality (\u201csmoking causes cancer\u201d rather than \u201cMr T.", "startOffset": 27, "endOffset": 44}, {"referenceID": 9, "context": "The following example, which is a variant of the example in [Halpern 2008] showing that a cause may involve more than one conjunct, shows that Corollary 6.", "startOffset": 60, "endOffset": 74}, {"referenceID": 9, "context": "As Halpern and Hitchcock [2010] point out, constructing a good model is still more of an art than a science.", "startOffset": 3, "endOffset": 32}, {"referenceID": 13, "context": "Here I used the variables SH and BH, as was done in [Halpern and Pearl 2005].", "startOffset": 52, "endOffset": 76}, {"referenceID": 13, "context": "But in [Halpern and Pearl 2005], another model was also presented, where the there are time-indexed variables (e.", "startOffset": 7, "endOffset": 31}, {"referenceID": 10, "context": "(A similar point is made in [Halpern and Hitchcock 2010].", "startOffset": 28, "endOffset": 56}], "year": 2015, "abstractText": "Causal models defined in terms of structural equations have proved to be quite a powerful way of representing knowledge regarding causality. However, a number of authors have given examples that seem to show that the Halpern-Pearl (HP) definition of causality [Halpern and Pearl 2005] gives intuitively unreasonable answers. Here it is shown that, for each of these examples, we can give two stories consistent with the description in the example, such that intuitions regarding causality are quite different for each story. By adding additional variables, we can disambiguate the stories. Moreover, in the resulting causal models, the HP definition of causality gives the intuitively correct answer. It is also shown that, by adding extra variables, a modification to the original HP definition made to deal with an example of Hopkins and Pearl [2003] may not be necessary. Given how much can be done by adding extra variables, there might be a concern that the notion of causality is somewhat unstable. Can adding extra variables in a \u201cconservative\u201d way (i.e., maintaining all the relations between the variables in the original model) cause the answer to the question \u201cIs X = x a cause of Y = y?\u201d to alternate between \u201cyes\u201d and \u201cno\u201d? It is shown that we can have such alternation infinitely often, but if we take normality into consideration, we cannot. Indeed, under appropriate normality assumptions. Adding an extra variable can change the answer from \u201cyes\u2019 to \u201cno\u201d, but after that, it cannot change back to \u201cyes\u201d.", "creator": "LaTeX with hyperref package"}}}