{"id": "1412.3555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "histories": [["v1", "Thu, 11 Dec 2014 06:46:53 GMT  (856kb,D)", "http://arxiv.org/abs/1412.3555v1", "Presented in NIPS 2014 Deep Learning and Representation Learning Workshop"]], "COMMENTS": "Presented in NIPS 2014 Deep Learning and Representation Learning Workshop", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["junyoung chung", "caglar gulcehre", "kyunghyun cho", "yoshua bengio"], "accepted": false, "id": "1412.3555"}, "pdf": {"name": "1412.3555.pdf", "metadata": {"source": "CRF", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "authors": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recurrent neural networks have recently shown promising results in many machine learning tasks, especially when input and / or output units are of variable length [see e.g. Graves, 2012]. More recently, Sutskever et al. [2014] and Bahdanau et al. [2014] reported that recurrent neural networks are able to function as well as existing, well-developed systems in a demanding machine translation task. An interesting observation we make from these recent successes is that almost none of these successes has been achieved with a recursive neural network. Rather, it is a recursive neural network with sophisticated recursive hidden units, such as long-term memory units [Hochreiter and Schmidhuber, 1997] that has been used in these successful areas. Among these sophisticated recursive units, we are interested in evaluating two closely related variants in this paper. One is a long-term memory of data parameters (LM) and the other is a controller unit."}, {"heading": "2 Background: Recurrent Neural Network", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3 Gated Recurrent Neural Networks", "text": "In this paper, we are interested in evaluating the performance of these recently proposed recurring units (LSTM unit and GRU) in sequence modelling. Before empirical evaluation, we will first describe each of these recurring units in this section."}, {"heading": "3.1 Long Short-Term Memory Unit", "text": "The unit Long Short-Term Memory (LSTM) was originally proposed by Hochreiter and Schmidhuber (1997). Unlike the recursive unit, which simply calculates a weighted sum of the input signal and uses a nonlinear function, each j-th LSTM unit retains a memory content. Output h j t, or activation of the LSTM unit, is thenhjt = o j t tanh (cjt), where ojt is an output gate that holds the amount of memory content. (Woxt + Uoht \u2212 1 + Voct) j, where the unit is a logistic sigmoid function. (Vo is a diagonal matrix.) The memory cell cjt is updated by partially formatting the existing memory. (Woxt + Uoht \u2212 1 + Voct) j, where the unit is a new unit. (Vo is a logistic sigmoid function.) Vonale is a diagrix."}, {"heading": "3.2 Gated Recurrent Unit", "text": "A gated recurrent unit (GRU) was proposed by Cho et al. [2014] to design each recursive unit so that dependencies of different time scales can be captured adaptively. Similar to the LSTM unit, the GRU has gating units that modulate the information flow within the unit, but without having separate memory cells. Activation of the GRU at the time of activation is a linear interpolation between the previous activation h j \u2212 1 and the candidate activation.The update gate is calculated byzjt = (Wzxt + Uzht \u2212 1) j t \u2212 1 + z j t, (5) where an update gate zjt determines how much the unit updates its activation or its content.The update gate is calculated byzjt = \u03c3 (Wzxt + Uzht \u2212 1) j. This method of taking a linear sum between the existing state and the recalculated state is similar to the LM unit."}, {"heading": "3.3 Discussion", "text": "It is easy to notice similarities between the LSTM units and the GRU units; the most important features of these units are the additive components of their updating from t to t + 1 that are missing in the traditional recursive unit; the traditional recursive unit always replaces the activation or content of a unit with a new value calculated from the current input and the previous hidden state; on the other hand, the LSTM units and the GRU stick to the existing content and add the new content. (4) and (5)"}, {"heading": "4 Experiments Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Tasks and Datasets", "text": "We compare the LSTM unit, the GRU and the Tanh unit in the task of sequence modeling. Sequence modeling aims to learn a probability distribution across sequences, as in Eq. (3), by maximizing the log probability of a model that specifies a set of training parameters: max \u03b81N N N * n = 1 Tn \u00b2 t = 1 log p (xnt | xn1,.., xnt \u2212 1; \u03b8), which is a set of model parameters. More precisely, these units are evaluated in the tasks of polyphonic music modeling and voice signal modeling. For polyphonic music modeling, we use three polyphonic music datasets from [BoulangerLewandowski et al., 2012]: Nottingham, JSB Chorales, MuseData and Piano-midi. These datasets contain sequences, each of which uses a 93, 96, 105 and 108-dimensional signal lengths, respectively."}, {"heading": "4.2 Models", "text": "For each task, we train three different recursive neural networks that have either LSTM units (LSTM-RNN, see paragraph 3.1), GRU-RNN (GRU-RNN, see paragraph 3.2) or tanh units (tanh-RNN, see paragraph 2). Since the primary goal of these experiments is to compare all three units fairly, we select the size of each model so that each model has approximately the same number of parameters. See Table 1 for the details of the model sizes. We train each model with RMSProp [see e.g. Hinton, 2012] and use weight noise with standard deviation set to 0.075 [Graves, 2011]."}, {"heading": "5 Results and Analysis", "text": "Table 2 lists all the results of our experiments. In the case of polyphonic music data sets, the GRU-RNN outperformed all others (LSTM-RNN and tanh-RNN) for all data sets except Nottingham. However, we can see that in these music data sets, all three models were close together. On the other hand, the RNN with the gating units (GRU-RNN and LSTM-RNN) performed significantly better than the more traditional Tanh RNN for both Ubisoft data sets. LSTM-RNN was best for Ubisoft A and for Ubisoft B, the GRU-RNN averaged best. In Figures 2-3, we show the learning curves of the best validation runs. In the case of music data sets (Fig. 2), we show that the GRU-RNN progresses much faster in updating individual data sets when both the number of data sets and the actual units (PU) are not taken into account."}, {"heading": "6 Conclusion", "text": "In this paper, we examined empirically relapsing neural networks (RNN) with three widely used relapsing units: (1) a traditional Tanh unit, (2) a long-term short-term memory (LSTM), and (3) a recently proposed gated recurrent unit (GRU). Our evaluation focused on the task of sequence modelling on a number of datasets, including polyphonic music data and raw speech signal data. The evaluation clearly showed the superiority of the gated units; both the LSTM unit and the GRU over the traditional Tanh unit. This became clearer in the more demanding task of raw modelling speech signals. However, we could not draw any concrete conclusions about which of the two gating units was better. To better understand how a gated unit contributes to learning and the contribution of the individual components, for example gating units in the LSTM unit or the GRU, the gating units, the experiments will be more thorough in the future."}, {"heading": "Acknowledgments", "text": "The authors thank Ubisoft for providing the data sets and for their support. The authors thank the developers of Theano [Bergstra et al., 2010, Bastien et al., 2012] and Pylearn2 [Goodfellow et al., 2013]. We thank the following research funding and computer support agencies: NSERC, Calcul Que \u0301 bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "creator": "LaTeX with hyperref package"}}}