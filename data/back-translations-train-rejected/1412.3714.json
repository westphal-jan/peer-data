{"id": "1412.3714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Feature Weight Tuning for Recursive Neural Networks", "abstract": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \"weight tuning\" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.", "histories": [["v1", "Thu, 11 Dec 2014 16:35:27 GMT  (207kb,D)", "http://arxiv.org/abs/1412.3714v1", null], ["v2", "Sat, 13 Dec 2014 00:57:57 GMT  (207kb,D)", "http://arxiv.org/abs/1412.3714v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG", "authors": ["jiwei li"], "accepted": false, "id": "1412.3714"}, "pdf": {"name": "1412.3714.pdf", "metadata": {"source": "CRF", "title": "Feature Weight Tuning for Recursive Neural Networks", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 Related Work", "text": "Distributed representations calculated on the basis of neural frameworks are extended beyond token levels to represent N-grams [15], phrases [2], sentences (e.g. [3, 16]), discourses [17, 13], paragraphs [18], or documents [19]. Recursive and recursive [20, 21] models represent two types of commonly used frameworks for embedding sentences at sentence level. Various variations of recursive / recursive models are proposed to be suitable for different scenarios (e.g. [3, 2]). Other recently proposed approaches include the sentence compositional approach proposed in [22] or the paragraph / sentence vector [18], in which representations are optimized by predicting words within the sentence. Neural network architecture sometimes requires a vector representation of each input tag. Various deep learning architectures have been explored to extract these embedding from a large-scale correction."}, {"heading": "3 \u201cWeight Tuning\u201d for Neural Network", "text": "Let us specify a sequence of characters s = {w1, w2,..., wns}. They could be phrases, sentences, etc. Each word w is associated with a specific vector embedding ew = {e1w, e2w,..., eKw}, where K denotes the dimension of the word embedding. We would like to calculate the vector representation for sentences s, which are called hs = {h1s, h 2 s,..., h K s. Analysis tree for each sentence is obtained from Stanford Parser [31]."}, {"heading": "3.1 WNN for Recursive Neural Network", "text": "The basic idea of WNN is to associate each node C with an additional weight variable MC, which is in the range (0.1) to indicate the meaning of the current node. Technically, MC is used to shift the representation of the non-useful node towards 0 and to maintain relatively important information. We expect information about the meaning of the current node (e.g. whether it is relevant for positive / negative feelings) to be embedded in its representation hC. So we use a convolution func-5http: / en.wikipedia.org / wikipedia.org / wiki _ short _ term _ memorytion to generate this type of information from the following compositional functions: RC = f (WM \u00b7 hC + bM) (2) MC = sigmod (U T \u00b7 RC), where WM is a D \u00d7 K dimensional matrix and bB is the 1 \u00d7 K effector vector RC."}, {"heading": "3.2 BENN for Recursive Neural Network", "text": "BENN associates each node with a binary variable BC, which is queried from a binary distribution = NP = p Film (NP). LC is a scalar fixed to the range [0,1], indicating the possibility that the current node should pass information to its ancestors. LC is achieved in a similar way to WNN by using a nesting to project the current representation hC onto a scalar within [0,1]. RC = f (WB \u00b7 hC + bB) (10) LC = sigmod (U T B \u00b7 RC) (11) BC \u0445 binary (LC) (12) For smoothing purposes, the current node C indicates the expectation of embedding hC to its ancestors as expressed by: Output (C) = E [hC] = E [hC] = E [hC] (13) Take the case in Figure 1 as an example hNP, hNP follows the following distribution: hp) NP."}, {"heading": "4 Experiment", "text": "We conduct experiments to better understand the behavior of the proposed models compared to standard neural models (and other variations). To achieve this, we implement our model for problems requiring fixed-length vector representations of phrases or sentences."}, {"heading": "4.1 Sentiment Analysis", "text": "In fact, most of them are able to assert themselves in public, and they are able to outdo themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to outdo ourselves. \""}, {"heading": "4.2 Document-level Sentiment Analysis on IMDB dataset", "text": "We use the IMDB data set proposed by Maas et al. [34]. The data set consists of 100,000 IMDB film reviews, and each film review contains several sentences. We follow the experimental protocols described in [34]. We first train word vectors from word2vect using the 75,000 training materials. Next, we train the compositional functions using the 25,000 labeled documents by embedding the word firmly. First, we obtain sentence representations using WNN / BENN (recursive). Since each review contains several sentences, we combine sentence representations into a single vector using the WNN / BENN recurrent network. We tick parameters based on the named documents and test the models based on the 25,000 test ratings. The results of our approach and other baselines are reported in Table 5."}, {"heading": "4.3 Sentence Representations for Coherence Evaluation", "text": "Next, we turn to a more syntactically oriented task, in which we obtain representations at the sentence level based on the proposed model to determine the coherence of a given sequence of sentences. We use corporas, which are often used for predicting coherence [35, 36], one containing reports of National Transportation Safety Board aircraft accidents, and the other containing reports of earthquakes from the Associated Press. By default, we follow the protocols introduced in [35, 37, 14] by looking at a window approach and using the concatenation of representations of neighboring sentences as positive examples, and the other a random permutation of sentences from the same document, which are treated as incoherent examples. We follow the protocols set out in [35, 37, 14] by looking at a window approach and feeding the concatenation of representations of neighboring sentences into a logical regression model that is either coherent or incoherent."}, {"heading": "5 Conclusion", "text": "In this paper, we propose two revised versions of neural models, WNN and BENN, to obtain higher-level feature representations for a sequence of symbols. The proposed framework automatically integrates the concept of SVM weight tuning into DL architectures, resulting in better higher-level representations and significantly better performance over standard neural models for multiple tasks. Although it still works inadequately in some cases, and the newly proposed paragraph vector approach, it represents an alternative to existing recursive neural models for representation learning."}], "references": [{"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Bidirectional recursive neural networks for token-level labeling with structure", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "arXiv preprint arXiv:1312.0493,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Rnnlmrecurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "J Cernocky"], "venue": "In Proc. of the 2011 ASRU Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Subsequence kernels for relation extraction", "author": ["Raymond J Mooney", "Razvan C Bunescu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy"], "venue": "In Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Empirical Methods in Natural Language Processing, A model of coherence based on distributed sentence", "author": ["Jiwei Li", "Eduard Hovy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "A convolutional neural network for modelling sentences", "author": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network", "author": ["Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1406.3830,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Translation modeling with bidirectional recurrent neural networks", "author": ["Martin Sundermeyer", "Tamer Alkhouli", "Joern Wuebker", "Hermann Ney"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Regina Barzilay", "Lillian Lee"], "venue": "In HLT-NAACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "Modeling local coherence: An entity-based approach", "author": ["Regina Barzilay", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "A coherence model based on syntactic patterns", "author": ["Annie Louis", "Ani Nenkova"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recursive neural network models [1] constitute one type of neural structure for obtaining higherlevel representations beyond word-level such as phrases or sentences.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": ", [2, 3]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": ", [2, 3]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 3, "context": "Table 1: A brief comparison between SVM and standard neural network models for sentence-level sentiment classification using date set from [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "Neural network models are trained with L2 regularization, using AdaGrad [5] with minibatches (for details about implementations of recursive networks, please see Section 2).", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Word embeddings are borrowed from Glove [6] with dimensionality of 300, which generates better performance than word2vect, SENNA [7] and RNNLM [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "Word embeddings are borrowed from Glove [6] with dimensionality of 300, which generates better performance than word2vect, SENNA [7] and RNNLM [8].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Such problem, which could also be referred to as gradient vanishing [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "When we compare neural models with SVM, one notable weakness of big-of-word based SVM is its inability of considering how words are combined to form meanings (or order information in other words) [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "\u2019s dataset [4], and as can be seen, in this specific task, standard neural network models underperform SVM2.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": ", word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data4.", "startOffset": 18, "endOffset": 25}, {"referenceID": 9, "context": ", word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data4.", "startOffset": 18, "endOffset": 25}, {"referenceID": 5, "context": ", word2vec, RNNLM [8, 11], SENNA [7]), rather than training embeddings from task-specific objective functions as neural models can be easily over fitted given the small amount of training data4.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "Recent proposed approaches include, for example, MatrixVector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "Recent proposed approaches include, for example, MatrixVector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which", "startOffset": 136, "endOffset": 139}, {"referenceID": 11, "context": "Recent proposed approaches include, for example, MatrixVector RNN [12], which represents every word as both a vector and a matrix, RNTN [2] which allows greater interactions between the input vectors, and the algorithm presented in [13] which", "startOffset": 232, "endOffset": 236}, {"referenceID": 1, "context": "\u2019s work [2] which obtains state-of-art performance for sentiment classification, as here labels at sentence-level constitute only sort of supervision for both SVM and neural network models (for details, see footnote 7).", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "4There are cases, for example, [2], where task-specific word embeddings are learned.", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "Distributed representations, calculated based on neural frameworks, are extended beyond tokenlevel, to represent N-grams [15], phrases [2], sentences (e.", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Distributed representations, calculated based on neural frameworks, are extended beyond tokenlevel, to represent N-grams [15], phrases [2], sentences (e.", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 2, "endOffset": 9}, {"referenceID": 15, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 22, "endOffset": 30}, {"referenceID": 11, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 22, "endOffset": 30}, {"referenceID": 16, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": ", [3, 16]), discourse [17, 13], paragraphs [18] or documents [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Recursive and recurrent [20, 21] models constitute two types of commonly used frameworks for sentence-level embedding acquisition.", "startOffset": 24, "endOffset": 32}, {"referenceID": 19, "context": "Recursive and recurrent [20, 21] models constitute two types of commonly used frameworks for sentence-level embedding acquisition.", "startOffset": 24, "endOffset": 32}, {"referenceID": 2, "context": ", [3, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": ", [3, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 20, "context": "Other recently proposed approaches included sentence compositional approach proposed in [22], or paragraph/sentence vector [18] where representations are optimized through predicting words within the sentence.", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "Other recently proposed approaches included sentence compositional approach proposed in [22], or paragraph/sentence vector [18] where representations are optimized through predicting words within the sentence.", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 22, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 23, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 24, "context": "Various deep learning architectures have been explored to learn these embeddings in an unsupervised manner from a large corpus [23, 24, 25, 26], which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand.", "startOffset": 127, "endOffset": 143}, {"referenceID": 25, "context": "Both of the proposed architectures are in this work inspired by the long short-term memory (LSTM) model, first proposed by Hochreiter and Schmidhuber back in 1990s [27, 28] to process time sequence data where there are very long time lags of unknown size between important events5.", "startOffset": 164, "endOffset": 172}, {"referenceID": 26, "context": "Both of the proposed architectures are in this work inspired by the long short-term memory (LSTM) model, first proposed by Hochreiter and Schmidhuber back in 1990s [27, 28] to process time sequence data where there are very long time lags of unknown size between important events5.", "startOffset": 164, "endOffset": 172}, {"referenceID": 26, "context": "LSTM associates each time with a series of \u201cgates\u201d to determine whether the information from early timesequence should be forgotten [28] and when current information should be allowed to flow into or out of the memory.", "startOffset": 132, "endOffset": 136}, {"referenceID": 27, "context": "LSTM could partially address gradient vanishing problem in recurrent neural models and have been widely used in machine translation [29, 30]", "startOffset": 132, "endOffset": 140}, {"referenceID": 28, "context": "LSTM could partially address gradient vanishing problem in recurrent neural models and have been widely used in machine translation [29, 30]", "startOffset": 132, "endOffset": 140}, {"referenceID": 29, "context": "Parse tree for each sentence is obtained from Stanford Parser [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Such implementation can be viewed as using a three-layer neural model with D latent neurons for an output projected to a [0,1] space.", "startOffset": 121, "endOffset": 126}, {"referenceID": 30, "context": "As all components in Equ 9 are continuous, the gradient can be efficiently obtained from standard backpropagation [32, 33].", "startOffset": 114, "endOffset": 122}, {"referenceID": 31, "context": "As all components in Equ 9 are continuous, the gradient can be efficiently obtained from standard backpropagation [32, 33].", "startOffset": 114, "endOffset": 122}, {"referenceID": 0, "context": "LC is a scalar fixed to the range of [0,1], indicating the possibility that current node should pass information to its ancestors.", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "LC is obtained in the similar ways as in WNN by using a convolution to project the current representation hC to a scalar lying within [0,1].", "startOffset": 134, "endOffset": 139}, {"referenceID": 3, "context": "Sentence-level Labels We first perform experiments on dateset from [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "In this setting, binary labels at the top of sentence constitute the only resource of supervision (to note, it is different from setting in [2]).", "startOffset": 140, "endOffset": 143}, {"referenceID": 10, "context": "\u2022 MV-RNN (Matrix-Vector RNN): which was proposed in [12] which represents every node in a parse tree as both a vector and a matrix.", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "\u2022 RNTN (Recursive Neural Tensor Network): proposed in [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 32, "context": ", [34]) can generate better performance what the proposed models achieve.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "Socher et al\u2019s setting We now consider Socher et al\u2019s setting [2] for sentiment analysis, where contains gold-standard labels at every phrase node in the parse tree.", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "We follow the experimental protocols described in [2] (word embeddings are treated as parameters to learn rather than fixed to externally borrowed embeddings).", "startOffset": 50, "endOffset": 53}, {"referenceID": 16, "context": "In addition to varieties of neural models mentioned in Socher et al\u2019s work, we also report the performance of recently proposed paragraph vector model [18], which first obtains sentence embeddings in an unsupervised manner by predicting words within the context and then feeds the pre-obtained embeddings into a logistic regression model.", "startOffset": 151, "endOffset": 155}, {"referenceID": 32, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "We follow the experimental protocols described in [34].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "Baseline performances are reported from [2, 18].", "startOffset": 40, "endOffset": 47}, {"referenceID": 16, "context": "Baseline performances are reported from [2, 18].", "startOffset": 40, "endOffset": 47}, {"referenceID": 32, "context": "The results are reported from [34]", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Although WNN and BENN still underperform current state-of-art model Paragraph Vector [18], they produces better performance than bag-of-word models.", "startOffset": 85, "endOffset": 89}, {"referenceID": 33, "context": "We use corpora widely employed for coherence prediction [35, 36].", "startOffset": 56, "endOffset": 64}, {"referenceID": 34, "context": "We use corpora widely employed for coherence prediction [35, 36].", "startOffset": 56, "endOffset": 64}, {"referenceID": 33, "context": "We follow the protocols introduced in [35, 37, 14] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent.", "startOffset": 38, "endOffset": 50}, {"referenceID": 35, "context": "We follow the protocols introduced in [35, 37, 14] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent.", "startOffset": 38, "endOffset": 50}, {"referenceID": 12, "context": "We follow the protocols introduced in [35, 37, 14] by considering a window approach and feeding the concatenation of representations of adjacent sentences into a logistic regression model, to be classified as either coherent or non-coherent.", "startOffset": 38, "endOffset": 50}, {"referenceID": 12, "context": "Current state-of-art performance regarding this task is obtained by using standard recursive network as described in [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 33, "context": "Entity-grid model [35] generates state-of-art performance among all non-neural network models.", "startOffset": 18, "endOffset": 22}, {"referenceID": 33, "context": "Reported baseline results are reprinted from [35].", "startOffset": 45, "endOffset": 49}], "year": 2014, "abstractText": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \u201cweight tuning\u201d for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.", "creator": "LaTeX with hyperref package"}}}