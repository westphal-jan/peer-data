{"id": "1412.4160", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2014", "title": "Ripple Down Rules for Question Answering", "abstract": "Recent years have witnessed a new trend on building ontology-based question answering systems, that is to use semantic web information to provide more precise answers to users' queries. However, these systems are mostly designed for English, therefore, we introduce in this paper such a system for Vietnamese, that is, to the best of our knowledge, the first one made for Vietnamese. Different from most of previous works, we propose an approach that systematically builds a knowledge base of grammar rules for processing each input question into an intermediate representation element. Then we take this element with respect to a target ontology by applying concept-matching techniques for returning an answer. Experimental results show that the performance of the system on a wide range of Vietnamese questions is promising with accuracies of 84.1% and 82.4% for analyzing question and retrieving answer, respectively. Furthermore, our approach to the question analysis can easily be applied to new domains and new languages, thus saving time and human effort.", "histories": [["v1", "Fri, 12 Dec 2014 23:30:06 GMT  (360kb,D)", "https://arxiv.org/abs/1412.4160v1", "21 pages, 7 figures, 10 tables. This article extends the work described in our publications at RANLP2011 and ISWC2013 conferences"], ["v2", "Mon, 15 Jun 2015 14:20:55 GMT  (388kb,D)", "http://arxiv.org/abs/1412.4160v2", "v1: 21 pages, 7 figures, 10 tables. This article extends the works described in our conference publications at KSE2009, RANLP2011 and ISWC2013 conferences. v2: 8 figures, 10 tables; shorten section 2; majorly change sections 4.3 and 5.1.2"], ["v3", "Thu, 29 Oct 2015 14:14:09 GMT  (398kb,D)", "http://arxiv.org/abs/1412.4160v3", "V1: 21 pages, 7 figures, 10 tables. V2: 8 figures, 10 tables; shorten section 2; change sections 4.3 and 5.1.2. V3 (Author's manuscript): Accepted for publication in the Semantic Web journal, available fromthis http URL"], ["v4", "Wed, 4 Nov 2015 23:39:58 GMT  (398kb,D)", "http://arxiv.org/abs/1412.4160v4", "V1: 21 pages, 7 figures, 10 tables. V2: 8 figures, 10 tables; shorten section 2; change sections 4.3 and 5.1.2. V3: Accepted for publication in the Semantic Web journal. V4 (Author's manuscript): camera ready version, available from the Semantic Web journal atthis http URL"]], "COMMENTS": "21 pages, 7 figures, 10 tables. This article extends the work described in our publications at RANLP2011 and ISWC2013 conferences", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["dat quoc nguyen", "dai quoc nguyen", "son bao pham"], "accepted": false, "id": "1412.4160"}, "pdf": {"name": "1412.4160.pdf", "metadata": {"source": "CRF", "title": "Ripple Down Rules for Question Answering", "authors": ["Editor(s): Christina Unger", "Shizhu He", "Dat Quoc Nguyen", "Dai Quoc Nguyen", "Son Bao Pham"], "emails": ["dat.nguyen@students.mq.edu.au", "daiquocn@coli.uni-saarland.de", "sonpb@vnu.edu.vn", "dat.nguyen@students.mq.edu.au."], "sections": [{"heading": null, "text": "Keywords: answer questions, question analysis, single classification Ripple Down Rules, knowledge acquisition, ontology, Vietnamese, English, DBpedia, biomedicine"}, {"heading": "1. Introduction", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "2. Short overview of question answering", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Open-domain question answering", "text": "The goal of an open domain QA system is to automatically return an answer to each question in natural language [21.63,31]. For example, systems such as START [23], FAQFinder [8] and AnswerBus [68] are considered important tasks in QA. Many suggested approaches to this task are based on machine learning, knowledge representation and reasoning [7.22.48.67,16,5]. Since the QA track awakened the text retrieval conference [59] and the multilingual QA track of the CLEF conference [42], many open domain QA systems have been introduced from the perspective of information retrieval [24]."}, {"heading": "2.2. Traditional restricted-domain question answering", "text": "A natural language interface to a database (NLIDB) is a system that allows the user to access information stored in a database by typing questions using natural language expressions [2]. Generally, NLIDB systems focus on converting the input query into an expression in the corresponding database query language. For example, the LUNAR system [64] transfers the input query into an analyzed tree, and the tree is then converted directly into an expression in a database query language. However, it is difficult to create conversion rules that transform the tree directly into the query expression. Later NLIDBs, such as Planes [61], Eufid [51], PRECISE [46], C-Phrase [32], and the systems represented in the database [50.34] use semantic grammars to analyze questions."}, {"heading": "2.3. Ontology-based question answering", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2.4. Question answering and question analysis for Vietnamese", "text": "In terms of answering Vietnamese questions, Nguyen and Le [34] introduced a Vietnamese NLIDB system that uses semantic grammars, and their system includes two main modules: the Query Translator (QTRAN) and the Text Generator (TGEN). QTRAN maps an input query from the natural language into an SQL query, while TGEN generates an answer based on the table result of the SQL query. QTRAN uses limited context-free grammars to convert the input query into a syntax tree using the CYK algorithm [65]. The TGEN module then combines pattern-based models and data sets to make sense of pattern-based relationships and metadata sets."}, {"heading": "3. Our KbQAS question answering system", "text": "This section gives an overview of KbQAS. The architecture of KbQAS, as illustrated in Figure 1, consists of two components: the engine for analyzing natural questions and the component for retrieving answers. The component of question analysis consists of three modules: preprocessing, syntactical analysis and semantic analysis. This component takes the user question as input and returns an intermediate element that represents the input question in a compact form. The role of the intermediate representation element is to provide the structured information about the input query for the later process of retrieving answers. The component for retrieving answers consists of two modules: ontology mapping and answer extraction. It takes the intermediate representation element generated by the question analysis component and an ontology as input to generate the answer."}, {"heading": "3.1. Intermediate representation of an input question", "text": "In contrast to AquaLog [26], the intermediate representation element in KbQAS covers a greater variety of question types. This element consists of a question structure and one or more query dots in the following format: (substructure, question category, term 1, term2, term 3), where term 1 represents a concept (i.e. an object class), without the cases of affirm, affirm _ 3term and affirm _ moretuple question structures. In addition, term 2 and term 3 represent entities (i.e. objects or instances), without the cases of definition and comparison question structures. In addition, relation is a semantic constraint between the terms. We define the following question structures: Normal, UnknTerm, UnknRel, question, interpretation, Compare, ThreeTerm, Clause, Combine, Appham _ MoreTuples."}, {"heading": "3.2. An illustrative example", "text": "For demonstration and evaluation purposes, we use an ontology that models the organizational system of the VNU University of Engineering and Technology, Vietnam. Ontology contains 15 concepts such as \"tri-ngschool,\" \"gib-ng-vi\u00eanlecturer\" and \"sinh-vi\u00eanstudent,\" 17 relationships or properties such as \"hb-cenroll,\" \"gib-ng-yteach\" and \"l\u00e0-sinh-vi\u00ean-c.a1The KbQAS is available at http: / / 150.65.242,39: 8080 / KbQAS / with an intro video on YouTube at http: / / youtu.be / M1PHvJv1Z8.is student of\" and 78 instances as described in our first version of KbQAS."}, {"heading": "3.3. Natural language question analysis component", "text": "When a question is asked, the task of this component is to convert the input query into an intermediate representation, which is then used in the rest of the system. KbQAS uses the JAPE grammars in the GATE framework [11] to specify semantic comment-based regular expression patterns for question analysis, in which existing linguistic processing modules for Vietnamese, including word segmentation and part-of-the-word tagging [43], are packaged as GATE plug-ins. The result of the packaged plug-ins are annotations for sentences and segmented words. Each annotation has a number of feature-value pairs. For example, a word has a category attribute that stores its part-of-speech tag, and this information can then be reused for further processing in subsequent modules."}, {"heading": "3.3.1. Preprocessing module", "text": "The pre-processing module generates TokenVn annotations that represent a Vietnamese word with features such as part-of-speech, as shown in Figure 3. Vietnamese is a monosyllabic language; therefore, a word may contain more than one token. Thus, there are words or phrases that indicate the question categories, such as \"Phrase i kh\u00f4ngis that / are there,\" \"l\u00e0 baonhi\u00eauhow many,\" \"khi n\u00e0owhen\" and \"l\u00e0 c\u00e1i g\u00ecwhat.\" However, the Vietnamese word segmentation module has not been trained on the question domain. In this module, we therefore identify these words or phrases and designate them as individual TokenVn annotations with the question-word characteristic and its semantic category, such as Howwhwhycause / method, YesNotrest or false, Whytime / date, Wherabyain, annotrules with the question-word-cause, \"Whywhyson-Whycause, Whywhyson, Whycause-Whyson, Whycause, Whyson, Whycause-Whyson, Whyyycause-category, Whyson."}, {"heading": "3.3.2. Syntactic analysis", "text": "The syntactic analysis module is responsible for the identification of concepts, entities and the relationships between them in the input query. This module uses the TokenVn annotations, which are the output of the preprocessing module. Concepts and entities are usually expressed in annotations as shown in Table 1. Therefore, it is of crucial importance to identify noun phrases in order to generate the query result. (Based on Vietnamese linguistic grammar [14], we use the JAPE grammar to define pattern over annotation as shown in Table 1.) When a noun phrase matches, a NounPhrase annotation is created to denote the noun-phrase-phrase-phrase-phrase-phrase-phrase-phrase-relationship-Se-Srae-Srase-Srase-phrase-Srae is used to determine whether term or entity of the noun-phrase-Srase-Srase-Srase-phrase-SHeun-phrase-Srase-phrase-Srastic-Srase-phrase-Srae-Srae-phrase-Srastic-Se is used when the Srastic-Se-phrase-Se-phrase-Srase-Srase-Srase-phrase-Se is covered:"}, {"heading": "3.3.3. Semantic analysis module", "text": "The semantic analysis module aims to identify the question structure and produce the question patterns (question pattern, question category, term 1, relation, term 3) as an intermediate representation element of the initial question, using the TokenVn, NounPhrase, relation and question phrase annotations returned by the two previous modules. In the first KbQAS version [35], following AquaLog [26], we developed an ad hoc approach to detect structural patterns of questions and then use these patterns to generate the intermedia representations."}, {"heading": "3.4. Answer retrieval component", "text": "In the first KbQAS version [35], the answer to the question of the structure of the goal is used as an instance to generate an answer. \"The answer to the question of the goal is to generate an answer to the question of the goal.\" \"The answer to the question of the goal.\" \"The answer to the question of the goal.\" \"The answer to the question of the goal.\" \"\" The answer to the question of the goal. \"\" \"\" The answer to the question of the goal. \"\" \"\" \"\" The answer to the question of the goal. \"\" \"\" \"\" The answer to the question of the goal. \"\" \"\" \"\" The answer to the question of the goal. \"\" \"\" \"\" \"\" \"The answer to the question of the goal.\" \"\" \"\" \"The answer to the question of the goal.\" \"\" The answer to the question of the goal. \"The answer to the question of the goal.\" The answer to the question of the goal. \"The answer to the question of the goal.\" The answer to the question of the goal. \"The answer to the question of the question of the goal.\" The answer to the question of the question of the question of the question of the goal."}, {"heading": "4. Single Classification Ripple Down Rules for question analysis", "text": "As mentioned in Section 3.3.3, creating grammar rules manually in an ad hoc manner is very expensive and error-prone due to the complexity and variety of question structures. For example, such rule-based approaches as presented in [26,35,45] manually defined a list of sequence pattern structures for analyzing questions. As rules are created ad hoc, these approaches share common difficulties in managing the interaction between rules and maintaining consistency with each other. Our contribution focuses on the semantic analysis module by proposing a JAPE-like rule language and systematic processing to create rules in a way that controls the interaction between rules and maintains consistency."}, {"heading": "4.1. Single Classification Ripple Down Rules", "text": "In fact, it is so that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is so that they are able to abide by the rules. (...) In fact, it is so that they are able to abide by the rules. (...) In fact, it is so that they are able to abide by the rules. (...) In the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world, the world in the world, the world of the world, the world in the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world of the world, the world of the world, the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world in the world, the world of the world of the world of the world in the world, in the world of the world of the world in the world, in the world of the world of the world of the world, in the world of the world of the world in the world, in the world of the world of the world in the world, in the world of the world in the world of the world in the world, in the world of the world in the world of the world in the world of the world in the world, in the world of the world of the world in the world of the world in the world of the world, in the world of the world in the world of the world in the world"}, {"heading": "4.2. Rule language", "text": "A rule consists of a conditional part and a closing part. A condition is a regular expression that contains annotations with JAPE grammars in GATE [11]. It can also add new annotations over matching phrases of the sub-components of the pattern. Since annotations have feature-value pairs, we can impose constraints on the annotations in the pattern by specifying that a feature of a comment must have a certain value. The following example shows posting a comment over the matching phrase: ({TokenVn.string = = \"li.t k\u00ealist\"} {TokenVn.string = \"ch.rashow\") {NounPhrase.type = = \"qp99K: qp.QuestionPhrase = {category =\" Any complete pattern followed by a label must be enclosed by round brackets."}, {"heading": "4.3. Knowledge acquisition process", "text": "Our approach is language-independent, as the emphasis is on the process of creating the rules-based system, and the language-specific part is contained in the rules themselves. Therefore, in this section we will illustrate the process of building a SCRDR knowledge base for analyzing English phrases. Figure 7 shows the graphical user interface for creating SCRDR knowledge bases. We used the JAPE grammars developed to identify noun phrases, question phrases, and relationship phrases in AquaLog [26]. Based on token annotations created as an output of the English tokenizer, sentence splitter, and part-of-speech tagger within the GATE framework [11], the JAPE grammars successfully produce NounPhrase4, QuestionPhrase and Relation annotations, and other annotations such as noun, verb, or preps annotations for covering nouns, or prepositions."}, {"heading": "4.3.1. Reusing detected question structures", "text": "In view of the fact that this issue is a problem, one must ask oneself whether there is any solution at all. (...) In view of the fact that this problem is a problem, one can only from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset. (...) In view of the fact that it is from the outset only a question from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset \"from the outset\" from the outset."}, {"heading": "4.3.2. Solving question structure ambiguities", "text": "rE \"s rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "4.3.3. Porting to other domains", "text": "This year we are dealing with strong growth, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "5. Experiments", "text": "In KbQAS, the question analysis component uses our language-independent approach to knowledge acquisition, while the answer component produces answers from a domain-specific Vietnamese ontology. Therefore, we evaluate the question analysis and the answer component separately in Section 5.1 and Section 5.2, respectively."}, {"heading": "5.1. Experiments on analyzing questions", "text": "This section demonstrates the capabilities of our question analysis approach to quickly build a new knowledge base and adapt easily to a new area and language. We evaluate both our ad hoc approaches (see Section 3.3.3) and knowledge acquisition (see Section 4) to Vietnamese question analysis, and then present the experiment of building a knowledge base to handle English questions."}, {"heading": "5.1.1. Question analysis for Vietnamese", "text": "In this experiment, we also compare our ad hoc and knowledge acquisition approaches to question analysis, using the same training set of 400 questions and a test set of 88 questions.With our first approach, it took about 75 hours to create rules in an ad hoc manner as in Table 2. In contrast, it took 13 hours to build a Vietnamese knowledge base of rules for question analysis. However, most of the time was spent studying questions to determine the question structures and phrases that would be extracted to create intermediate representation elements; the actual time to create rules in the knowledge base was about 5 hours in total.The knowledge base consists of the standard rule and 91 exclusion rules. Table 3 describes the number of exclusion rules at each level, where each rule in layer n represents an exception to a rule in shift.The default structure is not just an exception, the rule."}, {"heading": "5.1.2. Question analysis for English", "text": "For the experiment in English, we first used a set of 170 English questions8, which AquaLog [26] successfully analyzed. These questions relate to the Knowledge Media Institute and its research area on the semantic Web. Using this set of questions, we constructed a knowledge base with 59 rules for question analysis. However, it took 7 hours to build the knowledge base, including 3 hours of actual time to create all the rules. We then evaluated the knowledge base using a set of 50 DBpedia test questions from the QALD-1 workshop and another set of 25 biomedical test questions from the QALD-4 workshop.9 from unknown pattern questions. Furthermore, as in Vietnamese, the existing linguistic processing modules within the GATE framework [11], including the English tokenizer and part-of-speech tagger, are also sources of error that lead to unrecognized structural patterns. For example, questions such as \"Which US states do we have gold minerals\" and 78mg drugs?"}, {"heading": "5.2. Experiment on answering Vietnamese questions", "text": "In order to evaluate the retrieval component of KbQAS, we used the ontology to model the organizational structure of the VNU University of Engineering and Technology, as mentioned in Section 3.2, as the target domain. This ontology was constructed manually using the Prot\u00e9g\u00e9 platform [19]. From the list of 88 questions, as mentioned in Section 5.1.1, we used 74 questions that were successfully analyzed by the retrieval component. The result of the retrieval component is shown in Table 9. The retrieval component provides correct answers to 61 of 74 questions, achieving a promising accuracy of 82.4%. The number of correctly answered questions corresponding to each retrieval component of the retrieval component can be found in the third column of Table 5. From these, 30 questions can be answered automatically, without interacting with users. In addition, 31 questions are correctly answered with the help of the users to handle the ambiguities."}, {"heading": "6. Conclusion and future work", "text": "In this paper, we described the first ontology-based question-answering system for Vietnamese, KbQAS. KbQAS contains two components: the analysis of natural language questions and the retrieval of answers. The two components are connected by an intermediate representation element that captures the semantic structure of each input question, which facilitates comparison with a target ontology to produce an answer. Experimental results from KbQAS on a wide range of questions are promising. In particular, the response-retrieval module achieves an accuracy of 82.4%. In addition, we proposed a question-analytical approach to systematically build a knowledge base of rules in order to transform the input request into an intermediate representation element. Our approach enables systematic control of interactions between rules and maintaining consistency between them. We believe that our approach is important, especially for under-resource languages where annotated data may not be available on a basis, our approach may be well suited to the process of combining questions from the QAS with the one of the company."}, {"heading": "Acknowledgments", "text": "This work is partially supported by the Vietnam National University, Hanoi (VNU) Research Scholarship # QG.14.04. Most of this work was done during the study period of the first two authors at the VNU University of Engineering and Technology. The first author is supported by an International Postgraduate Research Scholarship and a NICTA NRPA Top-Up Scholarship."}, {"heading": "A. Definitions of question structure types", "text": "\"We define the structures: Normal, UnknRel, Definition, Affirm, ThreeTerm, AffreeTerm, Affirm, Affirm _ 3Term, Affirm _ MoreTuples, Combine, Clause as follows:\" A normal structural question has only a single question mark in which Term3 is missing. \u2022 An unnRel structural question has only a single question mark in which Relation and Term3 are missing. \u2022 A list of all publications in the Knowledge Media Institute, \"according to UnknRel,\" Publications, Publications, Knowledge Media Institute,? \"A definition structure has only a single query missing Term1, Relation and Term3."}, {"heading": "B. Definitions of Vietnamese question categories", "text": "In KbQAS, a question is classified as one of the following categories: \"HowWhy, YesNo, What, When, Who, Many, ManyClass, List, and Entity.\" \u2022 A HowWhy category question refers to a cause or method that contains a TokenVn annotation that refers to strings such as \"t\u00f4i saowhy\" or \"v\u00ec saowhy\" or \"th\u00ean\u00e0ohow\" or \"l\u00e0 nhotg\u00e0 thotg\u00e0 ohow.\" \u2022 A YesNo category question requires a true or false answer that includes a TokenV\u00e1n annotation \"or\" th\u00ean\u00e0ohse. \""}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Recent years have witnessed a new trend of building ontology-based question answering systems. These systems use<lb>semantic web information to produce more precise answers to users\u2019 queries. However, these systems are mostly designed for<lb>English. In this paper, we introduce an ontology-based question answering system named KbQAS which, to the best of our<lb>knowledge, is the first one made for Vietnamese. KbQAS employs our question analysis approach that systematically constructs<lb>a knowledge base of grammar rules to convert each input question into an intermediate representation element. KbQAS then<lb>takes the intermediate representation element with respect to a target ontology and applies concept-matching techniques to return<lb>an answer. On a wide range of Vietnamese questions, experimental results show that the performance of KbQAS is promising<lb>with accuracies of 84.1% and 82.4% for analyzing input questions and retrieving output answers, respectively. Furthermore, our<lb>question analysis approach can easily be applied to new domains and new languages, thus saving time and human effort.", "creator": "LaTeX with hyperref package"}}}