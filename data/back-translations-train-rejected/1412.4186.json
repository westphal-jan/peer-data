{"id": "1412.4186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2014", "title": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool", "abstract": "The purpose of this report is in examining the generalization performance of Support Vector Machines (SVM) as a tool for pattern recognition and object classification. The work is motivated by the growing popularity of the method that is claimed to guarantee a good generalization performance for the task in hand. The method is implemented in MATLAB. SVMs based on various kernels are tested for classifying data from various domains.", "histories": [["v1", "Sat, 13 Dec 2014 03:33:13 GMT  (239kb)", "http://arxiv.org/abs/1412.4186v1", "A short (6 page) report on evaluation of the SVM as a pattern classification tool, as of 1999"]], "COMMENTS": "A short (6 page) report on evaluation of the SVM as a pattern classification tool, as of 1999", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eugene borovikov"], "accepted": false, "id": "1412.4186"}, "pdf": {"name": "1412.4186.pdf", "metadata": {"source": "CRF", "title": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool", "authors": ["Eugene A. Borovikov"], "emails": [], "sections": [{"heading": null, "text": "Keywords: statistical learning theory, pattern recognition, support vector machine, SVM, VC dimension"}, {"heading": "Introduction", "text": "The fundamentals of SVM were developed by Vapnik (1995), and these pattern classifiers are gaining popularity due to a number of attractive features, including a promising generalization performance. It is this formulation that embodies the principle of Structural Risk Minimization (SRM) as opposed to the Empirical Risk Minimization (ERM) approach commonly used within statistical learning methods. It is this difference that provides SVMs with excellent generalization potential. As many real-world objects can be represented as points in Rn, and multiple class classifiers can be built through the use of orders of SVMs, the technique is expected to be applicable to a variety of classification problems. Feasibility and computational costs of such applications are separate issues and should be considered on a case-by-case basis."}, {"heading": "SVM Background", "text": "The SVM theory arose from considerations of the circumstances under which and how quickly the mean of an empirical quantity converges uniformly to the true mean with increasing numbers of data points. To describe the idea of a support vector machine, we must first deal with the principle of structural risk minimization. First, let us raise a generic statistical learning problem."}, {"heading": "Statistical Learning", "text": "Let us suppose that we have a attribute space X of potential observations; each observation should be designated as one of the K classes or \"doubts\" D or \"outliers\" O; let us let Y = {1,..., K, D, O} be the space of possible decisions, then a classifier is a map from X to Y. Let the training set T be a series of n classified cases; statistical learning is then finding a classification map from X to Y based on data in T. Any future case (outside T) is classified by the map. For the purpose of reformulating a generic problem for an SVM, let us let X = Rn and Y = {-1.1}. Let us suppose there is an unknown probability distribution P (x, y) from which the described observations are derived. For each assignment f: X \u00d7 process \u2192 Y, where a parameter space is x, let us define the following quantity: (), (2), (We expect this to have several xfyties \u2212"}, {"heading": "Structural Risk Minimization", "text": "Define the empirical risk as the measured mean error rate on the training set: \u2211 = \u2212 = li iiemp xfyl R 1), (2 1) (\u03b1\u03b1Note that no probability distribution appears here; this amount is specified for certain \u03b1 and certain training sets {xi, yi}. Now we can specify the upper limit of the actual risk specified by Vapnik: l hlhRR emp) 4 / log () 1) / 2 (log () (\u03b7\u03b1\u03b1 \u2212 + + \u2264 Here applies: Pr {y = 1} and h is the VC dimension of {f (x, \u03b1)}. The second term on the right is VC confidence. Finding a learning machine with the minimum upper limit of the actual risk leads us to a method for selecting an optimal machine for a given task. This is the essential idea of structural risk minimization (SRM).Let us suppose that we are determined a sequence of interlocking functional minimum target families < This is < H3; H3;"}, {"heading": "Linear SVM", "text": "In view of the fact that the training data is referred to as a sequence of points marked l in Rn = problem, the task is to find an \"optimal\" hyperplane that separates them. Here, there is a factor that we must take into account: linear separability of the training data. In the linear separation case, the support vector algorithm simply looks for the separation plane with the largest margin, where margin = 2d and d is the distance from the hyperplane to the next positive or negative example. It turns out that the margin is inversely proportional to the absolute value of the hyperplane, which reduces the original problem to the following optimization problem: minimize | be subject to the hyperplane xi + b - 1 \u2265 0, where (xi, yi) are the training examples and b are the typical solution for the bias term. A typical solution to the 2D case is shown in Figure 1. The solid line is the solution of the hyperplane, the margin is the circles between the two examples, the examples being positive and the examples."}, {"heading": "Non-linear SVM", "text": "The treatment of the most interesting non-linear case is based on the idea of injecting the data points into a higher-dimensional Hilbert space by means of a non-linear mapping and using the linear support vector algorithm there to separate the training examples, taking advantage of the fact that the data in the training problem only occurs in the form of Dot products. Suppose there were a map \u03a6: Rd \u2192 H, from our \"data room\" into a higher-dimensional Hilbert room H. If there were a function that evaluates Dot products in H cost-effectively, K (xi, xj) = (xi).Note: Then we could train our machine in H and would never need an explicit form of \u03a6.In our linear SVM algorithm, we replace all Dot products xi-xj with K (xi, xj); the kernel function, and we will end up with a machine that performs a linear separation, but helpfully in another space."}, {"heading": "SRM by SVM", "text": "Within the framework of Structural Risk Minimization (SRM), one gets a series of nested function families whose VC dimensions meet the SRM requirements. The support vector algorithm selects an optimal classifier from each family of classifiers, and the next step would be to identify the SVM with the lowest upper limit for the actual risk. This process can be illustrated using the SV algorithm with the polynomial core function. Let Hi have the space of all polynomials of the degree no larger than i. It should be fairly obvious that Hi-Hk and hi < hk for i < k. For each Hi apply the support vector algorithm with the polynomial core function, the p = i. This will form for each i from 1 to some n. From these n machines a minimize the upper limit of the actual risk. Name this SVM the optimal one for the respective task."}, {"heading": "Applications", "text": "In this section we look at the results of two SVM applications. One deals with face recognition in black and white images, the other serves as a test for comparing two classifiers and deals with the recognition of handwritten digits. We also discuss a possible symbiosis of SVM with decision trees."}, {"heading": "Face Detection", "text": "Edgar Osuna et al. [3] have demonstrated how to use a support vector machine to detect vertically oriented and unlocked frontal views of human faces in grayscale images. They claim that their system handles faces across a wide range of scales and operates under different lighting conditions, even under moderately strong shadows. They train an SVM using a database of face and non-face pixel patterns. The training phase uses an iterative decomposition algorithm. The system detects faces by exhaustively scanning an image for face-like patterns at many possible scales by splitting the original image into overlapping sub-images and classifying them with the trained SVM to determine the appropriate class (face / non-face). Multiple scales are detected by examining windows from scaled versions of the original image. The system was tested on two sets of images: Set A with 313 high quality images per image and Set B with 23 mixed quality images containing a total of 154.1 images with 664.1 patterns were reported during the set."}, {"heading": "Handwritten Digit Recognition", "text": "Sch\u00f6lkopf at al [5] compared SVM with Gaussian kernels with Radial Basis Function Classifiers. For one of the tests, they used the US Postal Service Database with 9300 handwritten digits (7300 training, 2000 for testing purposes.) An SV algorithm using standard square programming techniques (conjugated gradient descent) was used. For the 10-class classifiers, the following results were reported: test error rate cluster centers 6.7% SV centers 4.9% SVM 4.2%"}, {"heading": "SVM and Decision Trees", "text": "Bennett and Blue have tried to develop a method for generating logically simple decision trees with multivariate linear or non-linear decisions [2], the key idea being to reconcile the complexity of the tree with the complexity of the decisions made. This kind of compromise is achieved by using a simple SVM for every decision in the tree. A simple example of such a classifier is shown in Figure 2."}, {"heading": "Experiments", "text": "In this section we present the results of experimental runs of SVM classifiers, first on a synthetic dataset and then on a real dataset."}, {"heading": "Synthetic Problems", "text": "This section describes a number of SVMs that classify data in 2D. All data is synthetic and serves the purpose of demonstrating the SV algorithm's ability to find an optimal hypersurface for the given training data.The experiment is carried out as follows: Three different SVMs (linear, poly and RBF) are tested on three different training sets (linear separable, linear non-separable and hardware).Linear SVM uses a linear core, Poly uses a level 3 polynomic core, and RBF uses an RBF kernel with a sigma equal to 10.As Table 1 shows, the linear separable case is easily handled by all SVMs. Linear non-separable case is handled well by Poly and RBF; linear attempts to achieve the best possible result are the most expensive. The hard-to-separate case is only well accepted by RBF, but does not completely separate the data."}, {"heading": "Real-world Problems", "text": "In this section we will look at two databases with real data. Both databases come from the UCI Machine Learning Repository. Both databases contain marked examples that fall into two classes, which makes the use of the SV algorithm straightforward."}, {"heading": "Breast Cancer Classifier", "text": "The class distribution is 65% / 35%. The total number of data sets is 699. Each example has 9 attributes. Missing values are replaced by some numerical values outside the attribute range.A number of SVM classifiers are trained and tested using different core functions. Classifiers are trained using randomly selected examples of sizes from 100 to 500. Noteworthy is the fact that the straight linear classifier performs reasonably well with a 3.9% error rate in the worst case of 100 elements (9 support vectors) and in the best case of 3.14% with a training set of 400 (82 support vectors).The performance of the square SVM kernel is even better than the linear one. Cubic cores cause the method to behave a wrong minimization problem."}, {"heading": "Mushroom Classifier", "text": "This database contains data on two types of mushrooms (edible and toxic), with a class distribution of 52% / 48%. The total number of data sets is 8124. Each example has 22 attributes. Since all attributes are nominal, their values, represented by character symbols, will be considered the ASCII codes of these characteristics. For this data set, we need to use a robust RBF kernel SVMs, as any polynomial SVM (including linear) would degenerate in the training phase. As for the Breast Cancer Classifier, SVM classifiers are trained here using randomly selected examples of sizes from 100 to 500. As the sample run shows, the error rate is initially significant (4.7%) and grows for training sets of sizes 100 and 200, but then drops steeply to 0.8% and continues to decline as the size of the training rate increases. This points to excellent generating performance of the SVM."}, {"heading": "Conclusions", "text": "The aim of this project was to evaluate the Support Vector Machines as a tool for pattern recognition in statistical learning, hypothesising that the Support Vector Machines have excellent generalisation performance and that they can be successfully applied to a wide range of pattern recognition problems. We gave an introduction to the principle of Structure Risk Minimization (SRM) and provided background information on SV learning techniques using SRM. We implemented a Support Vector learning system in the Matlab and conducted a series of experiments with it, which included both synthetic and real data. The results of the experiments showed that the Support Vector Machines do indeed have the characteristics of excellent generalisation performance and can clearly be used successfully in solving various pattern classification tasks."}], "references": [{"title": "A tutorial on Support Vector Machines for Pattern Recognition", "author": ["C.J.C. Burges"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Support Vector Machines: Training and Application, C.B.C.L", "author": ["E.E. Osuna", "R. Freund", "F. Girosi"], "venue": "Paper No. 144,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Flexible Non-linear Approaches to Classification", "author": ["B.D. Ripley"], "venue": "ASI Proceedings,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Comparing Support Vector Machines with Gaussian Kernels to Radial Basis Function Classifiers", "author": ["B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "The Nature of Statistical Learning Theory, Springer-Verlag", "author": ["V. Vapnik"], "venue": "New York,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "Face Detection Edgar Osuna et al [3] have shown how to use a support vector machine for detecting vertically oriented and unoccluded frontal views of human faces in grey level images.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Handwritten Digit Recognition Sch\u00f6lkopf at al [5] compared SVM with Gaussian kernel to Radial Basis Function Classifiers.", "startOffset": 46, "endOffset": 49}], "year": 2003, "abstractText": "The purpose of this report is in examining the generalization performance of Support Vector Machines (SVM) as a tool for pattern recognition and object classification. The work is motivated by the growing popularity of the method that is claimed to guarantee a good generalization performance for the task in hand. The method is implemented in MATLAB. SVMs based on various kernels are tested for classifying data from various domains.", "creator": "PScript5.dll Version 5.2"}}}