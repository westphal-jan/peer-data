{"id": "1412.4314", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2014", "title": "Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus", "abstract": "Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this \"code-switching\" phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction.", "histories": [["v1", "Sun, 14 Dec 2014 05:34:25 GMT  (314kb,D)", "http://arxiv.org/abs/1412.4314v1", null], ["v2", "Mon, 22 Dec 2014 15:53:22 GMT  (314kb,D)", "http://arxiv.org/abs/1412.4314v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["joseph chee chang", "chu-cheng lin"], "accepted": false, "id": "1412.4314"}, "pdf": {"name": "1412.4314.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["CODE-SWITCHING CORPUS", "Joseph Chee Chang"], "emails": ["chuchenglin@cs.cmu.edu"], "sections": [{"heading": null, "text": "Most research in areas such as machine translation or sentiment analysis is based on monolingual input, but people who are able to use more than one language often communicate with multiple languages at the same time. Sociolinguists believe that this \"code switching\" phenomenon is socially motivated, for example, to express solidarity or establish authority. In this paper, we train recursive neural networks with only raw functions and use word insertions to automatically learn meaningful representations. With the same mixed-language Twitter corpus, our system is able to surpass the best SVM-based systems reported in the EMNLP '14 Code Switching Workshop by 1% accuracy or 17% error rate."}, {"heading": "1 INTRODUCTION", "text": "Code switching refers to the phenomenon of a speaker switching between different languages in a single utterance or conversation. Sociolinguists believe that people switch code due to sociolinguistic motivations, e.g. to express solidarity and familiarity, and to establish authority or distance. (Gumperz, 1982) It is also shown that code switching has its own grammatical regularities, that switching points almost never occur at specific points. (Berk-Seligson, 1986) This implies the use of sophisticated models to identify structures in a mixed sentence, which can potentially be helpful in the task of speech recognition. Computer models of code switching can therefore also be used to verify linguistic theories. Nevertheless, it can also be used to guide downstream NLP applications to use correct language models, which is a practical problem for current social media processing. There has recently been a workshop and a common task of modeling code in social media."}, {"heading": "2 RELATED WORK", "text": "In this section, we will review previous methods of language identification, both in monolingual and multilingual texts used in 2001, and computer-based models of code change. We will also review previous work in neural networks on processing natural language and on reproducing words in a semantic vector space. Monolingual language identification is generally treated as a text categorization problem, often defined as text, assigning some of the most relevant work here, where L = {l1.. lN} is a predefined group of languages in this task. Baldwin & Lui (2010) is an excellent review. To ensure completeness, we include some of the most relevant work here. By characterizing each text f (t), this problem can be treated as a standard-reviewed learning problem. In fact, many well-known classics such as Naive Bayes (Lui & Baldwin), 2012, SVM, SVM (SVM), SVM and Teyald (SVM)."}, {"heading": "3 DATA", "text": "All of our experiments are based on the Twitter data from the EMNLP Code-switching Workshop (Solorio et al., 2014). Due to Twitter's privacy policies, the organizers were not allowed to provide the tweets themselves. Instead, participants were given the unique Tweet Ids and character offsets, and then the participants had to search the data themselves. In situations such as deletion and changes in privacy settings made by users after the initial crawl, we are able to search a total of 27, 255 tweets. Breakdown of language pairs is shown in Table 1. Although the main objective of the common task is to predict the code switches, the data is provided with a fine-grained scheme listed in Table 2. It should be noted that there is a large variation in the frequency of each label. mixed and ambiguous, despite their importance in the linguistic theory of bilingualism, the tweets do not seem to happen frequently on social media."}, {"heading": "4 METHODOLOGY", "text": "The proposed network architecture is based on the Elman type and the Jordan type Recurrent Neural Network. We follow the implementation of Mesnil et. al 2013 and use a 3-word window to capture the immediate context and form a 7-D context vector (1-hot). A supervised Word embedding layer is used to project each word onto a 100-D real value vector. The second layer is a 100-D hidden layer with sigmoid function. Finally, we use the Softmax function at the output level. Long-term dependencies are captured with either Elman type or Jordan type recurring structures, with the current output depending on the output of the previous 9 hidden layer or final layer. If we extend this architecture to integrate character gram functions, we use the aforementioned technique to pre-harvest a 12-D vector model, extract the current 7-D vector model, and use the hot-D contextor model to pre-harvest it from the current 7-D and hot-D contextor model."}, {"heading": "4.1 PRE-TRAINED WORD2VECTOR", "text": "Mikolov et al. (2013) is a bilinear log model that encourages words with similar contexts to be embedded in a similar way. We use the implementation of Gensim and trained on a large Twitter corpus of random samples from the live feed. We sampled 10,000 tweets per day, spanning about 2,000 days. We didn't filter by specific languages. The idea is that words of different languages tend to share different contexts, so embedding should provide a good separation between languages, and they have proven to be an improvement in the task of code change (Lin et al., 2014)."}, {"heading": "4.2 CHARACTER NGRAMS", "text": "For example, a word that begins with \"lle\" is more Spanish than English. Conversely, a word that ends with \"tion\" is more English. These features are extracted from existing programs such as cld21 and ldig2.In our approach, we extract a fixed number of character ngrams for each word. We use a 3-character window and extract character bigrams and trigrams from both the beginning and end of each word, resulting in a 12-character ngram vector. For example, the character name vector for the word architecture is [arc, rch, chi, ar, rc, ch, ure, tur, ctu, re, ur, tu].1https: / / code.google.com / p / cld2 / 2https: / / github.com / shuyo / ldig"}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 PRELIMINARY STUDY", "text": "To test the effectiveness of different neural network structures, we first use a smaller dataset for a preliminary study. In addition to testing both Elman-type RNN structures and Jordanian-type RNN structures, we also tested the effectiveness of our proposed extensions. Our preliminary dataset contains only a total of 2,734 tweets, of which we use 1,000 for training, 1,000 for validation and the rest for evaluation."}, {"heading": "5.1.1 EXPERIMENTAL RESULTS", "text": "In the preliminary study, we tested the five following configurations: \u2022 Jordan: The basic Jordanian RNN with input as 7 1-hot word-context vectors and a supervised embedding level for projecting words onto a real Euclidean space. \u2022 Jordan + ngram: The basic Elman model with an additional 12 1-hot character-context vector. \u2022 Elman + ngram: The Elman model with an additional 12 1-hot character-gram vector. \u2022 Elman + ngram: The Elman model with an additional 12 1-hot character-gram vector. \u2022 Elman + ngram + w2v: The Elman + ngram configuration with an additional pre-rehearsed word-2vec model that feeds the hidden layers directly."}, {"heading": "5.2 COMPARING TO PREVIOUS WORK", "text": "To evaluate our approach, we use the training data from 27,255 tweets from the EMNLP 2014 Code-Switching Workshop. As a rating set, we use 2,734 tweets and the rest for training and validation. We acknowledge that we use a different test set to evaluate our systems and compare the results with the systems from the workshop, but we also separate the tweets into training, validation and evaluation sets by using individual authors. Nevertheless, both the test data we use and the test data from the workshop were collected in exactly the same way, and neither comes from an overlapping set of authors as the training set."}, {"heading": "5.2.1 BASELINE SYSTEMS", "text": "The EMNLP Code-Switching Workshop offered a simple deterministic baseline for all categories. In one word, it is looked up in the training corpus and the more common language designation \"w\" is used to describe the training corpus \"w\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \""}, {"heading": "5.2.2 BEST PERFORMING SYSTEMS IN THE SHARED TASK", "text": "According to the review article of the workshop (Solorio et al., 2014), most of the teams in the joint task adapted the CRF model for language identification; the best Nepalese-English template Barman et al. (2014) used SVM as the classification framework; they used character n-grams, context, capitalization, word length and dictionaries as characteristics; the best Spanish-English template (Bar & Dershowitz, 2014) also used SVM as the main classifier; they used character n-grams, context and dictionaries; and they also took probabilities of language models as characteristics."}, {"heading": "5.2.3 EXPERIMENTAL RESULTS", "text": "In Table 3, we show the performance of our complete systems. To compare with the results reported in the workshop, we evaluate our system against two different categories in terms of accuracy compared to the most powerful systems reported in the EMNLP '14 Code-Switching Workshop. We used only 18,521 tweets to train the networks, as we need the rest of the training for validation and testing, using a significantly smaller training set than previous systems that have access to the full training data of 27,255 tweets provided by the workshop. Similar to the preliminary experiment results, our Jordanian-type RNNNs performed similarly to Elman-type and Jordanian-type. Compared to state-of-the-art systems, our networks are able to achieve a 1% higher accuracy for the Anglo-Spanish category, a 17% reduction in the error rate. In the Anglo-Nepalese category, our Jordanian network delivered a 0.2% higher accuracy, an 8% reduction in the error rate for the Anglo-Spanish category, the error rate for SVM-based features are used only, and this makes sense on the different attributes of SVM."}, {"heading": "6 CONCLUSION", "text": "In this paper, we addressed the important task of processing natural language in the recognition of language in a Twitter corpus with code switching using relapsing neural networks. This is a new application for RNN, as most previous research focused on the use of machine learning methods such as concatenated conditional random fields to solve these types of problems. In fact, most participants in the 2014 EMNLP Code Switching Workshop used CRF to build their models, and the most powerful two teams used SVM-based models. Previous work has already compared RNNNs with CRF-based models for processing natural language, and the results show that RNNs can achieve a 1% higher accuracy or 17% higher accuracy in a task for detecting named objects. We tested RNNNs and found that RNNNs with the two proposed extensions can also use state-of-the-art SVM-based systems by 1% or more than 17% in error data, while using smaller training characteristics."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was initiated by the deep learning course offered by Dr. Bhiksha Raj at the Language Technologies Institute of Carnegie Mellon University. The authors also thank lecturer Zhenzhong Lan for the insightful discussions."}], "references": [{"title": "Language identification: The long and the short of the matter", "author": ["Baldwin", "Timothy", "Lui", "Marco"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Baldwin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baldwin et al\\.", "year": 2010}, {"title": "Proceedings of the First Workshop on Computational Approaches to Code Switching, chapter The Tel Aviv University System for the Code-Switching Workshop Shared Task, pp. 139\u2013143", "author": ["Bar", "Kfir", "Dershowitz", "Nachum"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Bar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bar et al\\.", "year": 2014}, {"title": "Proceedings of the First Workshop on Computational Approaches to Code Switching, chapter DCU-UVT: Word-Level Language Classification with Code-Mixed Data, pp. 127\u2013132", "author": ["Barman", "Utsab", "Wagner", "Joachim", "Chrupa\u0142a", "Grzegorz", "Foster", "Jennifer"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Barman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barman et al\\.", "year": 2014}, {"title": "Code switching and x-bar theory: The functional head constraint", "author": ["Belazi", "Hedi M", "Rubin", "Edward J", "Toribio", "Almeida Jacqueline"], "venue": "Linguistic Inquiry,", "citeRegEx": "Belazi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Belazi et al\\.", "year": 1994}, {"title": "Linguistic constraints on intrasentential code-switching: A study of spanish/hebrew bilingualism", "author": ["Berk-Seligson", "Susan"], "venue": "Language in Society,", "citeRegEx": "Berk.Seligson and Susan.,? \\Q1986\\E", "shortCiteRegEx": "Berk.Seligson and Susan.", "year": 1986}, {"title": "i\u00bf un nase or una nase\u00a1/i\u00bf? what gender marking within switched dps reveals about the architecture of the bilingual language faculty", "author": ["Cantone", "Katja Francesca", "M\u00fcller", "Natascha"], "venue": "Lingua,", "citeRegEx": "Cantone et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cantone et al\\.", "year": 2008}, {"title": "N-gram-based text categorization", "author": ["Cavnar", "William B", "Trenkle", "John M"], "venue": "Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,", "citeRegEx": "Cavnar et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cavnar et al\\.", "year": 1994}, {"title": "Language identification of web pages based on improved n-gram algorithm", "author": ["Chew", "Yew Choong", "Mikami", "Yoshiki", "Nagano", "Robin Lee"], "venue": "International Journal of Computer Science Issues (IJCSI),", "citeRegEx": "Chew et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chew et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Discourse Strategies. Studies in Interactional Sociolinguistics", "author": ["Gumperz", "John J"], "venue": "URL http://books.google.com/books? id=aUJNgHWl_koC", "citeRegEx": "Gumperz and J.,? \\Q1982\\E", "shortCiteRegEx": "Gumperz and J.", "year": 1982}, {"title": "Kernel-based text categorisation", "author": ["R. Jalam", "O. Teytaud"], "venue": "In Neural Networks,", "citeRegEx": "Jalam and Teytaud,? \\Q2001\\E", "shortCiteRegEx": "Jalam and Teytaud", "year": 2001}, {"title": "Solving the problem of language identification", "author": ["Johnson", "Stephen"], "venue": "Technical report, University of Leeds,", "citeRegEx": "Johnson and Stephen.,? \\Q1993\\E", "shortCiteRegEx": "Johnson and Stephen.", "year": 1993}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Jordan", "Michael I"], "venue": "Advances in psychology,", "citeRegEx": "Jordan and I.,? \\Q1997\\E", "shortCiteRegEx": "Jordan and I.", "year": 1997}, {"title": "Labeling the languages of words in mixed-language documents using weakly supervised methods", "author": ["King", "Ben", "Abney", "Steven"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "King et al\\.,? \\Q2013\\E", "shortCiteRegEx": "King et al\\.", "year": 2013}, {"title": "The iucl+ system: Word-level language identification via extended markov models", "author": ["King", "Levi", "Baucom", "Eric", "Gilmanov", "Timur", "K\u00fcbler", "Sandra", "Whyatt", "Dan", "Maier", "Wolfgang", "Rodrigues", "Paul"], "venue": "In Proceedings of the First Workshop on Computational Approaches to Code Switching,", "citeRegEx": "King et al\\.,? \\Q2014\\E", "shortCiteRegEx": "King et al\\.", "year": 2014}, {"title": "Language identification based on string kernels", "author": ["C. Kruengkrai", "P. Srichaivattana", "V. Sornlertlamvanich", "H. Isahara"], "venue": "In Communications and Information Technology,", "citeRegEx": "Kruengkrai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kruengkrai et al\\.", "year": 2005}, {"title": "Chunking with support vector machines", "author": ["Kudo", "Taku", "Matsumoto", "Yuji"], "venue": "In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,", "citeRegEx": "Kudo et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kudo et al\\.", "year": 2001}, {"title": "Robust part-of-speech tagging using a hidden markov model", "author": ["Kupiec", "Julian"], "venue": "Computer Speech & Language,", "citeRegEx": "Kupiec and Julian.,? \\Q1992\\E", "shortCiteRegEx": "Kupiec and Julian.", "year": 1992}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Lafferty", "John", "McCallum", "Andrew", "Pereira", "Fernando CN"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Language modeling with functional head constraint for code switching speech recognition", "author": ["Li", "Ying", "Fung", "Pascale"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "The cmu submission for the shared task on language identification in code-switched data", "author": ["Lin", "Chu-Cheng", "Ammar", "Waleed", "Levin", "Lori", "Dyer", "Chris"], "venue": "In Proceedings of the First Workshop on Computational Approaches to Code Switching,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Microblogs as parallel corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 176\u2013186", "author": ["Ling", "Wang", "Xiang", "Guang", "Dyer", "Chris", "Black", "Alan", "Trancoso", "Isabel"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Ling et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2013}, {"title": "Langid.py: An off-the-shelf language identification tool", "author": ["Lui", "Marco", "Baldwin", "Timothy"], "venue": "In Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "Lui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2012}, {"title": "Automatic detection and language identification of multilingual documents", "author": ["Lui", "Marco", "Lau", "Jey Han", "Baldwin", "Timothy"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Lui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2014}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["McCallum", "Andrew", "Freitag", "Dayne", "Pereira", "Fernando CN"], "venue": "In ICML, pp", "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["Mesnil", "Gr\u00e9goire", "He", "Xiaodong", "Deng", "Li", "Bengio", "Yoshua"], "venue": "In INTERSPEECH,", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A formal production-based explanation of the facts of code-switching", "author": ["Sankoff", "David"], "venue": "Bilingualism: language and cognition,", "citeRegEx": "Sankoff and David.,? \\Q1998\\E", "shortCiteRegEx": "Sankoff and David.", "year": 1998}, {"title": "Overview for the first shared task on language identification in code-switched data", "author": ["Solorio", "Thamar", "Blair", "Elizabeth", "Maharjan", "Suraj", "Bethard", "Steven", "Diab", "Mona", "Gohneim", "Mahmoud", "Hawwari", "Abdelati", "AlGhamdi", "Fahad", "Hirschberg", "Julia", "Chang", "Alison"], "venue": "EMNLP", "citeRegEx": "Solorio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solorio et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "For example, previous work has found that the use of existing NER systems does not translate into improvement of named entities in one submission (King et al., 2014).", "startOffset": 146, "endOffset": 165}, {"referenceID": 15, "context": "Indeed, many well known classifiers such as Naive Bayes(Lui & Baldwin, 2012), SVM,(Jalam & Teytaud, 2001) and kernel methods(Kruengkrai et al., 2005) have been used in the literature.", "startOffset": 124, "endOffset": 149}, {"referenceID": 7, "context": "People have also achieved excellent accuracy with lower\u2013level features, such as byte sequence(Chew et al., 2011).", "startOffset": 93, "endOffset": 112}, {"referenceID": 21, "context": "For example, researchers has found that people have been posting tweets using multiple languages for some time(Ling et al., 2013).", "startOffset": 110, "endOffset": 129}, {"referenceID": 21, "context": "For example, researchers has found that people have been posting tweets using multiple languages for some time(Ling et al., 2013). Lui et al. (2014) used topic modeling to model the languages that occur in a document.", "startOffset": 111, "endOffset": 149}, {"referenceID": 21, "context": "For example, researchers has found that people have been posting tweets using multiple languages for some time(Ling et al., 2013). Lui et al. (2014) used topic modeling to model the languages that occur in a document. People have also used sequential labeling algorithms to identify language segments in a document, such as CRF by King & Abney (2013).", "startOffset": 111, "endOffset": 351}, {"referenceID": 3, "context": "Linguists have proposed syntactic theories that predicts the locations of code\u2013switching(Sankoff, 1998; Belazi et al., 1994; Cantone & M\u00fcller, 2008), which at the same time prohibits their happening at certain places.", "startOffset": 88, "endOffset": 148}, {"referenceID": 18, "context": "Other frequently used, state-of-the-art machine learning models include Chained-Conditional Random Fields (Lafferty et al., 2001), Maximum-Entropy Markov Model (McCallum et al.", "startOffset": 106, "endOffset": 129}, {"referenceID": 24, "context": ", 2001), Maximum-Entropy Markov Model (McCallum et al., 2000), and sometimes Support Vector Machines (Kudo & Matsumoto, 2001).", "startOffset": 38, "endOffset": 61}, {"referenceID": 26, "context": "(2013) project 1-hot word vectors using a task-specific, supervised embedding layer, and experimented using both Elman-type (Elman, 1990) (Mikolov et al., 2010) and Jordan-type (Jordan, 1997) RNNs.", "startOffset": 138, "endOffset": 160}, {"referenceID": 18, "context": "Other frequently used, state-of-the-art machine learning models include Chained-Conditional Random Fields (Lafferty et al., 2001), Maximum-Entropy Markov Model (McCallum et al., 2000), and sometimes Support Vector Machines (Kudo & Matsumoto, 2001). More recently, researchers has also begun to explore using RNN architectures to rival, or even outperform such machine learning approaches. Mesnil et al. (2013) project 1-hot word vectors using a task-specific, supervised embedding layer, and experimented using both Elman-type (Elman, 1990) (Mikolov et al.", "startOffset": 107, "endOffset": 410}, {"referenceID": 29, "context": "All our experiments are conducted on the Twitter data provided by the EMNLP Code\u2013switching Workshop (Solorio et al., 2014).", "startOffset": 100, "endOffset": 122}, {"referenceID": 20, "context": "And they proved to provide improvement in the code\u2013switching task (Lin et al., 2014).", "startOffset": 66, "endOffset": 84}, {"referenceID": 25, "context": "Skip\u2013gram word embeddings Mikolov et al. (2013) is a log bilinear model that encourages words with similar contexts to have similar embeddings.", "startOffset": 26, "endOffset": 48}, {"referenceID": 29, "context": "According to the overview paper of the workshop (Solorio et al., 2014), most teams in the shared task adapted the CRF model for language identification.", "startOffset": 48, "endOffset": 70}, {"referenceID": 2, "context": "The best Nepali\u2013English submission Barman et al. (2014) used SVM as the classification framework.", "startOffset": 35, "endOffset": 56}], "year": 2017, "abstractText": "Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this \u201dcode-switching\u201d phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP\u201914 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction.", "creator": "LaTeX with hyperref package"}}}