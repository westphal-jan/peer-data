{"id": "1412.4401", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2014", "title": "Tools for Terminology Processing", "abstract": "Automatic terminology processing appeared 10 years ago when electronic corpora became widely available. Such processing may be statistically or linguistically based and produces terminology resources that can be used in a number of applications : indexing, information retrieval, technology watch, etc. We present the tools that have been developed in the IRIN Institute. They all take as input texts (or collection of texts) and reflect different states of terminology processing: term acquisition, term recognition and term structuring.", "histories": [["v1", "Sun, 14 Dec 2014 20:03:36 GMT  (170kb)", "http://arxiv.org/abs/1412.4401v1", null]], "reviews": [], "SUBJECTS": "cs.CY cs.CL", "authors": ["c enguehard", "b daille", "e morin"], "accepted": false, "id": "1412.4401"}, "pdf": {"name": "1412.4401.pdf", "metadata": {"source": "META", "title": "Tools for Terminology Processing", "authors": ["C. Enguehard", "B. Daille", "E. Morin"], "emails": ["Morin}@irin.univ-nantes.fr"], "sections": [{"heading": null, "text": "Keywords: term acquisition, term recognition, term structuring, terminology processing"}, {"heading": "INTRODUCTION", "text": "The terminology of a field is an important resource for researchers in various fields: lexicographers, translators, and also domain specialists (engineers, technicians, editors of technical documentation, etc.). In addition, specialists in natural language processing have recognized that this knowledge could be useful for a variety of other applications, such as indexing, automatic or machine-assisted translation, text generation, semi-automatic abstraction, hypertext output, ontology construction, etc. However, terminology collections are rare because they are difficult to create: terminologists need to study large amounts of documentation about the target area in order to extract the terms, give their definition, and structure them according to a hierarchical classification. In the 1990s, assembling large electronic corporations enabled the construction of systems to automatically perform different tasks in terminology processing, in particular terminology collection, terminology recognition, and terminology structuring."}, {"heading": "1. TERM ACQUISITION", "text": "Many different professions are interested in the automatic acquisition of terminology through body use in order to achieve a number of objectives: building glossaries, vocabulary and terminology dictionaries, text indexes, automatic translation, building knowledge bases, issuing hypertext systems, etc. The emergence of large electronic corporations in the 1990s enabled the development of terminology extractors that can support terminology work. Various approaches emerged: linguistic (CLARIT (ref. 1), LEXTER (ref. 2), TERMS (ref. 3), heuristic or statistical (ANA (ref. 4) or hybrid linguistic-statistical (ACABIT (ref. 5), NEURAL (ref. 6). The systems we present analyze electronic texts originating from a specialized field and produce a list of candidate terms. These candidate terms must be validated by a terminologist or a specialist."}, {"heading": "Terminology mining", "text": "Terminology Mining refers to several functionalities: to propose a list of candidate terms ranked from the most representative of the corpus to the least. This list could be used to create specialized dictionaries relating to a new domain; to propose a list of candidate terms for which a morphological discrepancy has been detected, thus reflecting a more advanced lexicalization; this list could be added to update an existing reference listing. Automatic indexing Automatic indexing is a partial task of retrieving information that assigns an indexing language to texts (Ref. 7). This language consists of a set of descriptors that must reflect the text document in a discriminatory and unambiguous manner. There are two types of indexing: Controlled indexing, where indices occur from a controlled vocabulary; Free indexing, where indices are selected without consideration of an authority list."}, {"heading": "1.1. ACABIT", "text": "This year it is more than ever before."}, {"heading": "1.2. ANA", "text": "\"That's the question we have to ask ourselves,\" he says. \"We've got to play by the rules,\" he says. \"We've got to play by the rules,\" he says. \"We've got to play by the rules,\" he says. \"We've got to play by the rules,\" he says. \"We've got to play by the rules.\" \"We've got to play by the rules,\" he says. \"We've got to play by the rules,\" he says, \"but we've got to play by the rules.\" \"We've got to play by the rules.\" \"We've got to play by the rules.\" We've got to play by the rules. \""}, {"heading": "2. TERM RECOGNITION", "text": "In a corpus, terms do not always appear in the form of reference. For example, we may encounter slight deviations due to diacritical signs (accent, hyphenation) or in the plural, but also some deep deviations are not as rare as the conversion of a composite word into a verbal form. Today, it is admitted that deviations account for 25% of the occurrence of terms. It is particularly important to identify them for indexing or textual stamping (Ref. 12). Surprisingly, in the last 20 years, research specifically devoted to this area has been rare. We can cite Jacquemin, who worked specifically on the recognition of terms and their variations and built the FASTR system on the basis of a fine linguistic study (Ref. 12). In English, in addition to the usual linguistic deviations (singular / plural for example), four types of variations can be detected: coordinations (recognition of simple categories and peraphragms are the coordination and mutations of forms)."}, {"heading": "Flexible-equality of terms", "text": "The flexible equality of terms (ref. 13) is a mathematical operator that determines whether two terms should be considered equivalent. This operator uses a list of functional words (empty words) and depends on one parameter: k."}, {"heading": "1. Flexible-equality of strings", "text": "The flexible equality of two strings w and w 'is based on the calculation of the minimum processing distance (ref. 14). dist (w1, i, w '1, i) = min (dist (w1, i-1, w '1, i) + q, dist (w1, i, w '1, j-1) + q, dist (w1, i-1, w '1, j-1) + p. dist (wi, i, w'j, j)))) with wn, m is the substring starting with the harvested letter and ending after the mth letter of the word w. dist (x, y) = 1, if x = y = 0 otherwise q: cost of inserting / deleting one character, p: cost of replacing one letter with another, p: cost of replacing one letter with another. Generally, a replacement of the string is considered as a replacement of the string, i.e. 2p = q."}, {"heading": "2. Flexible-equality of complex terms", "text": "The flexible equality of strings cannot be applied directly to compound words due to the high complexity of the algorithm. Therefore, we define an operator for these compound words. This operator requires the sequence of 3 steps:"}, {"heading": "1. Segmentation of the strings into words", "text": "Segmentation of a string into words is defined by defining a list of symbols that can occur in a string. This list includes all the letters of the alphabet under consideration (minuscules, majuscles and accents), the digits and sometimes the minus signs \"-.\" A term X is defined as an ordered list of words xi: X = xi, n.B.: \"Color of a hammer\" is considered the ordered list (\"Color,\" \"of,\" an, \"\" hammer \")."}, {"heading": "2. Deletion of the functional words", "text": "Function words (empty words) are defined in the list. In English, this list contains articles (\"a,\" \"the,\" \"this,\" etc.), prepositions (\"of,\" \"through,\" etc.), pronouns (\"they,\" \"who,\" etc.), etc.), etc. We refer to the constraint of a string as X and write down R (X), the ordered list of words of X that are not functional words. Example: X = \"color of a hammer\" R (X) = (\"color,\" hammer \")\" of \"and\" a \"being functional words."}, {"heading": "3. Respective comparisons of the words", "text": "Two strings are flexible-equal (noted as \"| | ~\") if the words of their constraint are flexible-equal. Example: Here, there is an m in the string Y X = \"Colour of a hammer\" R (X) = (\"Colour,\" \"Hammer\") Y = \"Colour of any hammer\" R (Y) = (\"Colour,\" \"Hammer\") (\"Colour,\" Colour \"and\" Hammer \") (X | | ~ Y) We define the distance (D) between two terms X and Y as the average of the distances between their constraints, comparing words with their rank. This definition is useful if several candidates are flexible-equal. In such a case, the one with the smallest distance is chosen."}, {"heading": "3. TERM ORGANISATION", "text": "Classes are generated by clustering techniques based on similar word contexts (which describe which words are likely to be found in the immediate vicinity of a particular word) (Ref. 16) or similar distribution contexts (which show which words share the same syntactic environment) (Ref. 17). Links result from the automatic acquisition of relevant predictive or discursive patterns (Ref. 10, 18, 19). Predictive patterns lead to predictive relationships such as < < cause > > or < effect > (Ref. 20), while discursive patterns do not produce predictive relationships such as hypernym (Ref. 21) or synonymous links (Ref. 22). Finally, some tools use such automatic aggregated data for automatic indexing (Ref. 23) or information extraction (Ref. 24)."}, {"heading": "The Promethee system", "text": "The Promethee system for corpus-based information extraction has two functionalities: 1. A corpus-based collection of lexico-syntactic patterns in relation to a certain conceptual relationship. 2. The collection of pairs of concepts by a database of lexico-syntactic patterns. These functionalities are implemented in three main modules:"}, {"heading": "Lexical Preprocessor", "text": "The lexical preprocessor receives raw text as input. First, the text is tokenized (detection of word and sentence boundaries), marked and lemmatized. Noun phrases, acronyms and sequences of nouns are recognized by regular expressions. The output of the lexical preprocessor is an enriched text with SGML tags."}, {"heading": "Shallow Parser and Classifier", "text": "This module extracts lexico-syntactic patterns related to semantic relationships. This phase was inspired by the work of P. Hearst (ref. 18) and implemented by a shallow parser associated with a classifier."}, {"heading": "Information Extractor", "text": "This database can be the output of the shallow parser and classifier, or can be specified manually. A classifier is added to the shallow parser to discover new patterns by exploring the corpus. Inspired by P. Hearst (ref. 18), this procedure consists of the following seven steps: 1. Manual choose a representative conceptual relationship, e.g. the hypernyme relationship. 2. Collect a list of pairs of terms connected by the previous relationship. This list of terms can be extracted from a thesaurus or specified manually."}, {"heading": "4. FUTURE DEVELOPMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Multilinguality", "text": "ACABIT, ANA and Promethee were designed for French, and their adaptation to other languages mainly depends on the linguistic characteristics of the target language. Previous experiments showed that this adaptation should be easy for non-agglutinative Indo-European languages. For example, adaptation to English requires very few adaptations, such as the new definition of basic terms for ACABIT, new lists of function words and lexical schemes for ANA, promethee, and other adaptations, such as a tagger, a lemmatizer and a phrase extractor. It is also possible to treat languages that do not belong to the Roman languages: we have adapted ACABIT and ANA systems for Maldivian texts in previous work (Ref. 25). The Malagasy language is very close to the Indonesian language. It has been written with the Latin alphabet since the 19th century."}, {"heading": "4.2. Acceptation of new formats", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "XML format", "text": "ACABIT accepts text in XML format and also produces terms in XML format. This capability could be added to ANA and the supple _ equality operator."}, {"heading": "Unicode", "text": "Unicode offers the ability to encode any symbol used by any language in the world. Adapting our systems to this standard is crucial for their multilingual future."}, {"heading": "Conclusion", "text": "These linguistic resources represent an important contribution to increasing the educational level of a population. Nevertheless, terminology processing is gaining a real place in natural language processing technologies. Nevertheless, these linguistic efforts are too often left unprocessed, mainly due to their cost. In the last decade, new tools have appeared designed to collect, retrieve and structure concepts. Recent advances in normalization of linguistic symbols (with Unicode) and linguistic study of the characteristics of the target languages should allow the adaptation of these existing tools to different languages."}], "references": [{"title": "Noun-phrase analysis in unrestricted text for information retrieval", "author": ["D.A. Evans", "C. Zhai"], "venue": "proceedings of ACL, Santa Cruz, California,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "LEXTER, a Natural Language Processing tool for terminology extraction", "author": ["D. Bourigault", "I. Ganzalez-Mullier", "C. Gros"], "venue": "Proceedings of the 7 EURALEX International Congress, Go\u0308teborg,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Technical terminology: some linguistic properties and an algorithm for identification in text", "author": ["J. Justeson", "S. Katz"], "venue": "Natural Language Engineering,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Pant\u00e9ra, \"Automatic Natural Acquisition of a Terminology", "author": ["L.C. Enguehard"], "venue": "Journal of quantitative linguistics, vol.2,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Ananiadiou, \"Statistical measures for terminological extraction", "author": ["S.K.T. Frantzi"], "venue": "Proc. of the 3rd int. conf. on statistical analysis of textual data, Rome,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Automatic Text Processing", "author": ["G. Salton"], "venue": "Addison-Wesley Publishing Company,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "What is the role of NLP in Text Retrieval ?\", Natural Language Information Retrieval, Tomek Strzalkowski editor", "author": ["K.S. Jones"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Acquisition of Selectional Patterns in Sublanguage\", Machine Tranlation", "author": ["R. Basili", "M.T. Pazienza", "P. Velardi"], "venue": "Kluwer Academic Publishers, vol.8,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Apprentissage de sch\u00e9mas lexicaux pour l'acquisition de candidats-termes\", A3CTE (Applications", "author": ["C. Enguehard"], "venue": "Apprentissages et Acquisition de Connaissances a\u0300 partir de Textes Electroniques), RFIA'2001,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "A symbolic and surgical acquisition of terms through variation\", in Connectionist, statistical and symbolic approaches to learning for natural language", "author": ["C. Jacquemin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Flexible-equality of terms: definition and evaluation", "author": ["C. Enguehard"], "venue": "Proceedings of the International Conference on Flexible Query Answering Systems, ISBN 3-7908-1347-8,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["V.I. Levenshtein"], "venue": "Soviet PhysicsDoklady, vol.10,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1966}, {"title": "String searching algorithms", "author": ["G.A. Stephen"], "venue": "Lecture notes series on computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["G. Grefenstette"], "venue": "The Indo-European Conference on Multilingual Communications Technologies (IEMCT),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "author": ["M.A. Hearst"], "venue": "In Proceedings 14th International Conference on Computational Linguistics (COLING'92),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Automatically Constructing a Dictionary for Information Extraction Tasks", "author": ["E. Riloff"], "venue": "National Conference onArticial Intelligence (AAAI'93),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Structuration du lexique de la causalit\u00e9 et r\u00e9alisation d'un outil d'aide au rep\u00e9rage de l'action dans les textes", "author": ["D. Garcia"], "venue": "In Actes, 2e\u0300me rencontres Terminologies et Intelligence Artificielle", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Using Lexico-Syntactic Patterns to Extract Semantic Relations between terms from Technical Corpus", "author": ["E. Morin"], "venue": "TKE'99, Innsbruck,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Detection of synonymy links between terms. Experiment and results", "author": ["T. Hamon", "A. Nazarenko"], "venue": "Recent Advances in Computational Terminology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Spotting and Discovering Terms through NLP", "author": ["C. Jacquemin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Traitement automatique de la terminologie en langue malgache\", in Ressources et \u00e9valuation en ing\u00e9nierie des langues", "author": ["B. Daille", "C. Enguehard", "C. Jacquin", "R.L. Raharinirina", "B.S. Ralalaoherivony", "C. Lehman"], "venue": "ISBN 2-8011-1258-5,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}], "referenceMentions": [], "year": 2014, "abstractText": "Automatic terminology processing appeared 10 years ago when electronic corpora became widely available. Such processing may be statistically or linguistically based and produces terminology resources that can be used in a number of applications : indexing, information retrieval, technology watch, etc. We present the tools that have been developed in the IRIN Institute. They all take as input texts (or collection of texts) and reflect different states of terminology processing: term acquisition, term recognition and term structuring. Key-words : Terminology processing, term acquisition, term recognition, term structuring C. Enguehard, B. Daille, E. Morin, \u201cTools for Terminology Processing\u201d, The Indo-European Conference on Multilingual Communications Technologies (IEMCT), R. K. Arora, M. Kulkarni, H. Darbari, Tata McGrawHill, pp.218-229, Pune, India, June 2002.", "creator": "Writer"}}}