{"id": "1412.4564", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2014", "title": "MatConvNet - Convolutional Neural Networks for MATLAB", "abstract": "MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. In this manner, MatConvNet allows fast prototyping of new CNN architectures; at the same time, it supports efficient computation on CPU and GPU allowing to train complex models on large datasets such as ImageNet ILSVRC. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.", "histories": [["v1", "Mon, 15 Dec 2014 12:23:35 GMT  (14kb)", "http://arxiv.org/abs/1412.4564v1", null], ["v2", "Sun, 21 Jun 2015 15:35:25 GMT  (21kb)", "http://arxiv.org/abs/1412.4564v2", "Updated for release v1.0-beta12"], ["v3", "Thu, 5 May 2016 14:31:06 GMT  (753kb,D)", "http://arxiv.org/abs/1412.4564v3", "Updated for release v1.0-beta20"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MS cs.NE", "authors": ["andrea vedaldi", "karel lenc"], "accepted": false, "id": "1412.4564"}, "pdf": {"name": "1412.4564.pdf", "metadata": {"source": "CRF", "title": "Convolutional Neural Networks for MATLAB", "authors": ["Andrea Vedaldi", "Karel Lenc"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 141 2,45 64v1 [cs.CV] 1 5D ec2 01Table of Contents"}, {"heading": "1 Introduction 2", "text": "1.1 MatConvNet at a Glance................................. 3. 1.2 Structure and valuation of CNNs..................... 4. 1.3 CNN Derivatives.........................................................................................................................................................."}, {"heading": "2 Computational blocks 6", "text": "2.1. Merger..........................................................................................................................."}, {"heading": "3 Network wrappers and examples 12", "text": "3.1 Prefabricated models............................................................................................................................"}, {"heading": "4 About MatConvNet 14", "text": "4.1 Acknowledgements...................................... 15"}, {"heading": "1 Introduction", "text": "MatConvNet is a simple MATLAB toolbox that implements Convolutional Neural Networks (CNN) for computer vision applications. These documents begin with a brief overview of CNNs and how they are implemented in MatConvNet. Section 2 lists all the computational blocks implemented in MatConvNet that can be combined to create CNNs and gives the technical details of each one. Finally, Section 3 discusses more abstract CNN wrappers and sample code and models.A Convolutional Neural Network (CNN) can be viewed as a function that uses data x, for example, on an output vector y. Function f is a composition of a sequence (or directed acyclic graph) of the simpler functions f1,."}, {"heading": "1.1 MatConvNet on a glance", "text": "MatConvNet has a simple design philosophy. Instead of wrapping CNNs around complex layers of software, it provides simple functions for calculating CNN building blocks, such as linear folding and ReLU operators. These building blocks can easily be combined into complete CNNs and can be used to implement complex learning algorithms. While several examples of small and large CNN architectures and training routines are available, it is always possible to build your own by using the efficiency of MATLAB in prototyping. Often, no C coding is required at all to try out a new architecture. Therefore, MatConvNet is an ideal playground for computer vision research and CNNs.MatConvNet includes the following elements: \u2022 CNN computing blocks. A set of optimized routines that calculate the basic building blocks of a CNN building block is required to try out new architectures in the first place."}, {"heading": "1.2 The structure and evaluation of CNNs", "text": "Each block y = f (x, w) takes an image x and a set of parameters w as input and generates a new image y as output. An image is a real 4D array; the first two dimensions index spatial coordinates (picture rows or columns), the third dimension denotes channels (there can be any number) and the last dimensional image instances. A arithmetic block f is therefore represented as follows: x f ywFormally, x is a 4D tensor that stacks N 3D images (RH \u00b7 B \u00b7 D \u00b7 Nwhere H and W are the height and width of the images, D its depth and N the number of images. Below, all operations are applied identically to each image in the stack x; for simplicity, we will drop the last dimension in the discussion xxxx. l, but the ability to work on image stacks is very important."}, {"heading": "1.3 CNN derivatives", "text": "In the formation of a CNN, we are often interested in analyzing the derivatives of a loss of operating points: f (x, w) 7 \u2192 R in terms of parameters. This effectively amounts to expanding the network with a scalar block at the end: x0 f1 f2... fL w2 wLz, a process also known as back propagation. () The derivatives of f in terms of parameters can be calculated, but starting from the end of the chain (or DAG) and working backwards with the chain rule. () The derivatives w.r.t. wl is: dzd (vecwl) d d d d d d (vecxL) d d (vecxL \u2212 1)."}, {"heading": "1.4 CNN modularity", "text": "Sections 1.2 and 1.3 suggest a modular programming interface for implementing CNN modules. In abstract terms, we need two functionalities: \u2022 Forwarding: Evaluating the output y = f (x, w) of given input data x and parameters w (forwarding). \u2022 Feedback: Evaluating the CNN derivatives dz / dx and dz / dw for block input data x and parameters w for block input data x and parameters w, and the CNN derivative dx / dy for block output data y."}, {"heading": "2 Computational blocks", "text": "The block can be evaluated as a MATLAB function y = vl _ nn < block > (x, w), which represents the input data and parameters of the block as an input field and returns an array y as output. x and y are real 4D arrays packaging N cards or images, as discussed above, while\\ bw can have any form. To calculate the block derivatives, the same function can take a third optional argument dzdy, which represents the derivative of network output with respect to y and return the corresponding derivatives [dzdx, dzdw] = vl _ nn n n block > (x, w, dzdy)."}, {"heading": "2.1 Convolution", "text": "The convolutionary block is implemented by the function vl _ nnconv RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH-RH"}, {"heading": "2.2 Pooling", "text": "vl _ nnpool implements max and sum pooling. The max pooling operator calculates the maximum response of each characteristic channel in an H '\u00b7 W'patchyi \u2032 j'd = max 1 \u2264 i '\u2264 H ', 1 \u2264 j' \u2264 W'xi \u2032 + i ', j' \u2032 + j ', i.e. resulting in an output of size y'RH \u2032 \u00b7 W \u2032 \u00b7 \u00b7 \u00b7 \u00b7 D, similar to the convolution operator of Sectino 2.1. Sum-pooling instead calculates the average of the values: yi \u2032 j '\u2032 d = 1W \u2032 H \u0445 1 \u2264 i \u2032 H \u2032, 1 \u2264 J \u2032 W \u2032 xi \u2032 + i \u2032, j \u2032 + j \u2032, d.Padding and Stride. Similar to the convolution operator of Sect. 2.1, vl _ nnpool supports input padding; however, the effect differs from the padding in the revolutionary vxi \u2032 + i \u2032, j \u2032 and j \u2032, d.Padding and Stride like the convolution of Sj \u2032 d'ect."}, {"heading": "2.3 ReLU", "text": "vl _ nnrelu calculates the rectified linear unit (ReLU): yijd = max {0, xijd}.Matrix notation. With matrix notation we can express the ReLU asvecy = diag s vecx, dzd vecx = diag sdzd vecywhere s = [vecx > 0] \u044e {0, 1} HWD is an indicator vector."}, {"heading": "2.4 Normalization", "text": "vl _ nnnormalize implements a cross-channel normalization operator. The normalization is applied independently at any spatial location and at any group of channels to obtain the following values: yijk = xijk \u0445 + \u03b1 \u2211 t \u0445 G (k) x2ijt \u2212 \u03b2, where the operator for each output channel k, G (k) \u04411, 2,..., D} is a corresponding subset of input channels. Note that input channels x and output channels y have the same dimensions. Also note that the operator is applied in all spatial locations in wavy fashion across characteristic channels. Implementation details. The derivative is easily compiled as: dzdxijd = dzdyijd L (i, j, d | x) \u2212 \u03b2 \u2212 2\u03b1\u03b2xijd \u0445 k: d \u0445 G (k) dzdyijk L (i, k | x)."}, {"heading": "2.5 Softmax", "text": "vl _ softmax calculates the Softmax operator: yijk = exijd = D t = 1 exijt. Note that the operator is applied across all characteristic channels and in a convolutionary manner at all spatial locations. Details on implementation. Caution is required when evaluating the exponent to avoid underflow or overflow. The simplest method is to divide the numerator and denominator by the maximum value: yijk = exijk \u2212 maxd xijkL (x) \u2212 2), L (x) = D \u0445 t = 1exijt.Simplification: dzdxijd = kdzdyijd (exijdL (x) \u2212 1\u03b4 {k = d} \u2212 e xijkL (x) \u2212 2), L (x) = D \u0445 t = 1exijt.Simplification: dzdxijd = yijd (dzdyijd) K \u2212 jY = Y (11 dY) (Y) (dY)."}, {"heading": "2.6 Log-loss", "text": "vl _ logloss calculates the logarithmic power dissipation = (x, c) = \u2212 \u2211 ijlog xijcwhere c-1, 2,.., D} is the ground truth class. Note that the operator is applied over the input channels in a sinuous manner and summarizes the loss calculated at each spatial location in a single scale. Implementation details. The derivative isdzdxijd = \u2212 dzdy1xijc \u03b4 {d = c}."}, {"heading": "2.7 Softmax log-loss", "text": "vl _ softmaxloss combines the Softmax layer and the log loss in one step to improve numerical stability. It calculates (xijc \u2212 log D \u2211 d = 1exijd), where c is the ground truth class. Details of implementation. The derivative is given by dzdxijd = \u2212 dz dy (\u03b4d = c \u2212 yijc), where yijc is the output of the Softmax layer. In matrix form: dzdX = \u2212 dzdy (1 ec \u2212 Y), where X, Y and RHW \u00b7 D are the matrices obtained by transforming the arrays x and y and ec."}, {"heading": "3 Network wrappers and examples", "text": "Nevertheless, MatConvNet offers a simple wrapper for the common case of a linear chain. This is represented by the vl _ simplenn and vl _ simplenn _ move functions.vl _ simplenn takes as input a structural network representing the CNN as well as input x and potential output derivatives dzdy, depending on the mode of function.For details on the inline and output formats, please refer to the inline help of the vl _ simplenn function. In fact, the implementation of vl _ simplenn is a good example of how the basic neural network block can be shared and can serve as a basis for more complex implementations."}, {"heading": "3.1 Pre-trained models", "text": "vl _ simplenn is easy to use with pre-learned models (see homepage for some to download), for example, the following code downloads a model that is pre-trained on the ImageNet data and applies it to one of the MATLAB master images:% setup MatConvNet in MATLAB run matlab / vl _ setupnn% download an upstream note CNN from the web urlwrite (... \"http: / / www. v l f e a t. org / sandbox \u2212 matconvennet / models / imagenet \u2212 vgg \u2212 f. mat,\". \"imagenet \u2212 vgg \u2212 f. mat\"); net = load (\"imagenet \u2212 f. mat\");% obta in and preproces s s s an image im = imread (\"peppers. png.).png\" i."}, {"heading": "3.2 Learning models", "text": "Since MatConvNet can calculate derivatives of CNN using back propagation, it is easy to implement learning algorithms with it. Basic implementation of stochastic gradient lineage is therefore straightforward. Example code can be found in examples / cnn _ train. This code is flexible enough to allow training on NMINST, CIFAR, ImageNet and probably many other datasets. Examples can be found in the examples / directory."}, {"heading": "3.3 Running large scale experiments", "text": "For example, to train on ImageNet, we recommend the following: \u2022 Download the ImageNet data http: / / www.image-net.org / challenges / LSVRC. Install it somewhere and link it from data / imagenet12. \u2022 View the preprocessing of the data to convert all images to 256 pixels high, which can be done with the included utils / preprocess-imagenet.sh script. This way, the images do not need to be resized each time. \u2022 Do not forget to point the training code to the preprocessed data. \u2022 Consider copying the data set to a RAM disk (assuming you have enough memory!) for faster access. Do not forget to point the training code to this copy. \u2022 Compile MatConvtionsNet with GPU support."}, {"heading": "4 About MatConvNet", "text": "MatConvNet's main features are: \u2022 Flexibility. Neural network layers are easily implemented, often directly in the MATLAB code, so that they can be easily modified, extended or integrated with new ones. \u2022 Power. The implementation can run the latest models such as Krizhevsky et al. [6], including the DeCAF and Caffe variants and variants of the Oxford Visual Geometry Group. Learned functions for various tasks can be easily downloaded. \u2022 Efficiency. The implementation is quite efficient and supports both CPU and GPU compatibility (in the latest versions of MALTAB). \u2022 The implementation is fully self-contained, requires only MATLAB and a compatible C / C + compatible to work (GPU code requires the freely available CUDA-CUDA configuration of CUDA DevKit)."}, {"heading": "4.1 Acknowledgments", "text": "The implementation of several CNN calculations in this library is inspired by the Caffe library [4] (but Caffe is not a dependency), and some of the sample networks were trained by Karen Simonyan in [2]. We kindly thank NVIDIA for providing the GPUs used in creating this software."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In Proc. BMVC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1923}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Yangqing Jia"], "venue": "http://caffe.berkeleyvision.org/,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Integrals and derivatives for correlated gaussian fuctions using matrix differential calculus", "author": ["D.B. Kinghorn"], "venue": "International Journal of Quantum Chemestry,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Prediction as a candidate for learning deep hierarchical models of data", "author": ["R.B. Palm"], "venue": "Master\u2019s thesis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "This notation for the derivatives is taken from [5] and is used throughout this document.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "An application of grouping is implementing the Krizhevsky and Hinton network [6], which uses two such streams.", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "Hence we obtain the following expression for the vectorized output (see [5]):", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "[6], including the DeCAF and Caffe variants, and variants from the Oxford Visual Geometry Group.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Both MatConvNet and Caffe are predated by Cuda-Convnet [6], a C++ -based project that allows defining a CNN architectures using configuration files.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "The DeepLearningToolbox [7] is a MATLAB toolbox implementing, among others, CNNs, but it does not seem to have been tested on large scale problems.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "For example, the Torch7 toolbox [3] uses Lua and Theano [1] uses Python.", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "For example, the Torch7 toolbox [3] uses Lua and Theano [1] uses Python.", "startOffset": 56, "endOffset": 59}], "year": 2014, "abstractText": "MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for MATLAB. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. In this manner, MatConvNet allows fast prototyping of new CNN architectures; at the same time, it supports efficient computation on CPU and GPU allowing to train complex models on large datasets such as ImageNet ILSVRC. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.", "creator": "LaTeX with hyperref package"}}}