{"id": "1412.4863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Max-Margin based Discriminative Feature Learning", "abstract": "In this paper, we propose a new max-margin based discriminative feature learning method. Specifically, we aim at learning a low-dimensional feature representation, so as to maximize the global margin of the data and make the samples from the same class as close as possible. In order to enhance the robustness to noise, a $l_{2,1}$ norm constraint is introduced to make the transformation matrix in group sparsity. In addition, for multi-class classification tasks, we further intend to learn and leverage the correlation relationships among multiple class tasks for assisting in learning discriminative features. The experimental results demonstrate the power of the proposed method against the related state-of-the-art methods.", "histories": [["v1", "Tue, 16 Dec 2014 02:55:01 GMT  (412kb,D)", "https://arxiv.org/abs/1412.4863v1", null], ["v2", "Mon, 3 Apr 2017 02:43:47 GMT  (581kb,D)", "http://arxiv.org/abs/1412.4863v2", "Accepted by IEEE TNNLS"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "qingshan liu", "weishan dong", "xin zhang", "lin yang"], "accepted": false, "id": "1412.4863"}, "pdf": {"name": "1412.4863.pdf", "metadata": {"source": "CRF", "title": "Max-Margin based Discriminative Feature Learning", "authors": ["Changsheng Li", "Qingshan Liu", "Weishan Dong", "Fan Wei", "Xin Zhang", "Lin Yang"], "emails": ["ylyang@cn.ibm.com).", "fanwei@stanford.edu.", "qsliu@nuist.edu.cn)."], "sections": [{"heading": null, "text": "The idea is that it is a way in which people in the real world can rank themselves by projecting the data into a sub-dimensional space with the least information loss. [3] Generally speaking, the dimensions can be achieved either by selecting or transforming characteristics. [4] The selection of characteristics is aimed at selecting the most important characteristics. [5] A number of selection criteria have been proposed in recent years, such as the mutual selection criteria."}, {"heading": "II. PROPOSED METHOD", "text": "Let X = {(xi, yi)} ni = 1 denote a training data set, where xi-Rd is the i-th data point and yi-th data point, and yi-i {1,..., K} represents the corresponding class name. Our goal is to obtain a projection matrix P-Rd \u00b7 r that maps the d-dimensional input vector to an r-dimensional vector (r < d) by zi = PTxi. For the transformation matrix P, Pi represents its i-th row, and Pij its (i, j) th entry. As usual, we use tr (P) to denote the track of P, and VP-2,1 stands for l2,1 standard of P, which is defined as follows:"}, {"heading": "A. Binary Classification: K = 2", "text": "\"It's like we're able to hide,\" he says."}, {"heading": "B. Multi-class Classification: K > 2", "text": "In the case of multi-class classification, we can extend (2) to the following objective categories: min Jmc (W, b, P) = 12 K \u00b2 m = 1, wm \u00b2 2 + C \u00b2 n = 1, m6 = yi l (wm, wyi, byi, bm, P; xi, yi, yi, byi, bm, P; yi, yi) measures the loss when3 + 1, where W = [w1,., wK] is the set of learned weight vectors. l (wm, byi, byi, bm, P; yxi, yi) measures the loss when3 1, 4, 7, 8, 9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 fig."}, {"heading": "C. Optimization Procedure", "text": "The objective function (2) is not convex with respect to the variables w, b, and P simultaneously."}, {"heading": "D. Discussion", "text": "Varshney and Willsky [29] propose a method of linear dimensionality reduction that introduces learned mappings through matrices on the Boot manifold and on margin-based classifiers. Nikitidis et al. [30] present the Maximum Margin Projection Algorithm (MMPP), which simultaneously learns the optimal data embedding and SVM classifier. However, our approach constructs the Laplacian graph through local learning to capture the intrinsic structure of the data and incorporates the minimization of the l2,1 standard into our framework to mitigate the effects of noisy features. Furthermore, our method can minimize correlations between multiple categories, benefiting both functional transformation and classification training."}, {"heading": "E. Time Complexity Analysis", "text": "The temporal complexity of algorithm 2 consists of three parts: the initialization in line 1, the laplac matrix construction in line 2, and the iterative update of the three variables in line 3-8. The complexity of the first two parts can be ignored compared to the third part. In the third part, we must update w, b, or P. For the update w, the worst complexity is O (ndr + r3). The update b costs O (ndr). For the update P, O (t1% mdc) is needed for the two-loop recursion scheme, where t1 denotes the total number of iterations. According to (8), the worst case of the calculation of a partial gradient is w.r.t. P O (t2% (nd2 + n2d)), where t2 is the total number of the calculation of the gradient. The complexity of the evaluation of the objective function values is O (t3% (t2% rnr + total number of the time + D2), and the Md2 (Md2) is the value of the DZ."}, {"heading": "F. Evaluation of Convergence Rate", "text": "Although the convergence of the MMLDF algorithm cannot be proven theoretically, we note that it converges asymptotically in our experiments. Fig. 3 shows the convergence curve5 algorithm 2 Max margin based Discriminative Feature Learning Input: Training dataset X = {xi, yi} ni = 1; The parameters: C, \u03bb, \u0440; Reduced dimension rMethod 1. Initiate iteration step t = 0; Random initialize wt, bt, Pt; 2. Construct the laplac matrix L; 3. Repeat 4. Fix Pt and bt, update wt + 1 by equation (9); 5. Fix Pt and wt + 1, update bt + 1 by equation (10); 6. Fix wt + 1 and bt + 1, update Pt + 1 by equation; until convergence and wt + 1, update + 1 by equation."}, {"heading": "III. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets and Experimental Settings", "text": "We evaluate the performance of MMMLDF on eight real datasets, including an aerial dataset Urban Land Cover [38], two biomedical domain datasets DNA and Glioma, a large-format learning effect method that deals with the way it is used by LSVT Voice Rehabilitation [36], a business domain dataset CNAE-9, a face recognition dataset Yale Face, and a scene classification dataset 15 scene. The datasets DNA and Epsilon are downloaded from LIBSVM's official Web-Page2, and the dataset CNAE9 is downloaded from UCI Machine Learning Repository3. Datasets from different areas serve as a good test bed for a comprehensive evaluation. Table 1 summarizes the details of the datasets used in the experiments. To verify the effectiveness of MMMLDF, we compare them with the following seven related linear methods of transformation."}, {"heading": "B. General Performance", "text": "Table II reports the experimental results of each algorithm with the optimal dimension. The optimal dimensions are listed in parentheses of Table II. It can be seen that MMLDF consistently outperforms the other seven algorithms in all eight datasets. Compared to the second best result on each dataset, our method achieves 6.5%, 4.8%, 2.0%, 6.9%, 6.5%, 3.3%, 5.3% and 8.1% relative improvement in terms of Urban Land Cover, CNAE9, DNA, Glioma, LSVT Voice Rehabilitation, Epsilon, Yale Face and 15 scene datasets, respectively. Some baselines achieve significantly poor performance on certain datasets (e.g. FSSL on the glioma and 15 scene datasets, LSDA on the 15 scene dataset), which may be due to the limited generalizability of these algorithms so that they can be reduced to different areas of the DA database."}, {"heading": "C. Analyses on Components\u2019 Roles", "text": "We verify the effectiveness of the components in the objective functions (2) and (7), individually. If the parameters \u03bb, \u03b7, and \u03c1 are set to zeros, MMLDF is reduced to MMPP, so we use MDF as the starting point. We conduct the experiments on the binary classification dataset LSVT Voice Rehabilitation and the multi-level classification dataset Urban Land Cover. The experimental setting is as follows: we set to zero in (2), and set to zero in (7) to demonstrate the effectiveness of module filtering. We call it MLDF-I for a short time."}, {"heading": "D. Sensitivity Analysis", "text": "Figure 6 shows the results. With the fixed characteristic dimensions, our method is not sensitive to \u03bb, \u03b7 and \u03c1 with wide ranges. As for the parameter C, the performance gradually improves when C rises. If C > 10 \u2212 3 rises, the performance gradually deteriorates when C rises. If C is set to 10 \u2212 3, the performance is the best."}, {"heading": "IV. CONCLUSION", "text": "The proposed methodology aimed to find a low-dimensional attribute space to maximize the classification span of the data while minimizing dispersion within the class. In addition, we added a regularization term to eliminate noisy or redundant characteristics. Finally, a further regularization term was introduced to capture the correlations between multiple categories to contribute to the learning of discriminatory characteristics. Extensive experiments on publicly available benchmarks demonstrated the effectiveness of the proposed methodology compared to several related methods."}], "references": [{"title": "Evaluation of gender classification methods with automatically detected and aligned faces", "author": ["E. Makinen", "R. Raisamo"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 30, no. 3, pp. 541\u2013547, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A geometric nearest point algorithm for the efficient solution of the svm classification task", "author": ["M.E. Mavroforakis", "M. Sdralis", "S. Theodoridis"], "venue": "IEEE Trans. on Neural Networks, vol. 18, no. 5, pp. 1545\u20131549, 2007.  8", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral regression: A unified approach for sparse subspace learning", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE International Conference on Data Mining (ICDM), 2007, pp. 73\u201382.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Normalized mutual information feature selection", "author": ["P.A. Est\u00e9vez", "M. Tesmer", "C.A. Perez", "J.M. Zurada"], "venue": "IEEE Trans. on Neural Networks, vol. 20, no. 2, pp. 189\u2013201, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "On kernel target alignment", "author": ["N. Shawe-Taylor", "A. Kandola"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 14, 2002, pp. 367\u2013373.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient and robust feature selection via joint l2,1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C.H. Ding"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2010, pp. 1813\u20131821.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["C. Hou", "F. Nie", "X. Li", "D. Yi", "Y. Wu"], "venue": "IEEE Trans. on Cybernetics, vol. 44, no. 6, pp. 793\u2013804, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, no. 5500, pp. 2323\u20132326, 2000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), 2010, pp. 663\u2013670.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust lowrank subspace segmentation with semidefinite guarantees", "author": ["Y. Ni", "J. Sun", "X. Yuan", "S. Yan", "L.-F. Cheong"], "venue": "IEEE International Conference on Data Mining Workshops (ICDMW), 2010, pp. 1179\u20131188.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Dimensionality reduction by using sparse reconstruction embedding", "author": ["S. Huang", "C. Cai", "Y. Zhang"], "venue": "Advances in Multimedia Information Processing-PCM, 2010, pp. 167\u2013178.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Dimensionality reduction via kernel sparse representation", "author": ["Z. Pan", "Z. Deng", "Y. Wang", "Y. Zhang"], "venue": "Frontiers of Computer Science, vol. 8, no. 5, pp. 807\u2013815, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Locality preserving projections", "author": ["X. He", "P. Niyogi"], "venue": "Neural Information Processing Systems (NIPS), vol. 16, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Extraction of feature subspace for contentbased retrieval using relevance feedback", "author": ["Z. Su", "S. Li", "H. Zhang"], "venue": "Proceedings of ACM Multimedia (MM), 2001, pp. 98\u2013106.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Neighborhood preserving embedding", "author": ["X. He", "D. Cai", "S. Yan", "H. Zhang"], "venue": "Proceedings of International Conference on Computer Vision (ICCV), 2005, pp. 1208\u20131213.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Stable orthogonal local discriminant embedding for linear dimensionality reduction", "author": ["Q. Gao", "J. Ma", "H. Zhang", "X. Gao", "Y. Liu"], "venue": "IEEE Trans. on Image Processing, vol. 22, no. 7, pp. 2521\u20132531, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural computation, vol. 10, no. 5, pp. 1299\u20131319, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Stable local dimensionality reduction approaches", "author": ["C. Hou", "C. Zhang", "Y. Wu", "Y. Jiao"], "venue": "Pattern Recognition, vol. 42, no. 9, pp. 2054\u2013 2066, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Pca-based feature transformation for classification: issues in medical diagnostics", "author": ["M. Pechenizkiy", "A. Tsymbal", "S. Puuronen"], "venue": "IEEE Symposium on Computer-Based Medical Systems, 2004, pp. 535\u2013540.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Locality sensitive discriminant analysis", "author": ["D. Cai", "X. He", "K. Zhou", "J. Han", "H. Bao"], "venue": "Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI), 2007, pp. 708\u2013713.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Locality-preserved maximum information projection", "author": ["H. Wang", "S. Chen", "Z. HU", "W. Zheng"], "venue": "IEEE Trans. on Neural Networks., vol. 19, no. 4, pp. 571\u2013585, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Integrating global and local structures: a least squares framework for dimensionality reduction", "author": ["J. Chen", "J. Ye", "Q. Li"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2007, pp. 1\u20138.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised large margin discriminative projection", "author": ["F. Wang", "B. Zhao", "C. Zhang"], "venue": "IEEE Trans. on Neural Networks, vol. 22, no. 9, pp. 1446\u20131456, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "springer,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Robust classifiers for data reduced via random projections", "author": ["A. Majumdar", "R.K. Ward"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 40, no. 5, pp. 1359\u20131371, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Random projections for support vector machines", "author": ["S. Paul", "C. Boutsidis", "M. Magdon-Ismail", "P. Drineas"], "venue": "Proceedings of International Conference on Artificial Intelligence and Statistics, 2013, pp. 498\u2013506.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear dimensionality reduction for margin-based classification: High-dimensional data and sensor networks", "author": ["K.R. Varshney", "A.S. Willsky"], "venue": "IEEE Trans. on Signal Processing, vol. 59, no. 6, pp. 2496\u2013 2512, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum margin projection subspace learning for visual data analysis", "author": ["S. Nikitidis", "A. Tefas", "I. Pitas"], "venue": "IEEE Trans. on Image Processing, vol. 23, no. 10, pp. 4413\u20134425, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Dimensionality reduction by local discriminative gaussians", "author": ["N. Parrish", "M. Gupta"], "venue": "Proceedings of International Conference on Machine Learning (ICML), 2012, pp. 559\u2013566.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint feature selection and subspace learning", "author": ["Q. Gu", "Z. Li", "J. Han"], "venue": "Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), vol. 22, no. 1, 2011, pp. 1294\u20131299.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-task feature learning", "author": ["A. Evgeniou", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 19, 2007, pp. 41\u201348.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C. Burges"], "venue": "Data Mining and Knowledge Discovery, vol. 2, pp. 121\u2013167, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Objective automatic assessment of rehabilitative speech treatment in parkinson\u2019s disease", "author": ["A. Tsanas", "M. Little", "C. Fox", "L. Ramig"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 22, pp. 181\u2013190, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI), 2010, pp. 733\u2013742.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifying a high resolution image of an urban area using super-object information", "author": ["B. Johnson", "Z. Xie"], "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 83, pp. 40\u201349, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust unsupervised feature selection", "author": ["M. Qian", "C. Zhai"], "venue": "Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI), 2013, pp. 1621\u20131627.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D. Liu", "J. Nocedal"], "venue": "Mathematical Programming, vol. 45, pp. 503\u2013528, 1989.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1989}, {"title": "Discriminative embedded clustering: A framework for grouping high-dimensional data", "author": ["C. Hou", "F. Nie", "D. Yi", "D. Tao"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative semi-supervised feature selection via manifold regularization", "author": ["Z. Xu", "I. King", "M.-T. Lyu", "R. Jin"], "venue": "IEEE Trans. on Neural Networks, vol. 21, no. 7, pp. 1033\u20131047, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Data classification plays a key role in many practical applications [1], [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "Data classification plays a key role in many practical applications [1], [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "To cope with this issue, a popular way is to do dimensionality reduction, which is to project the data into a low-dimensional subspace with the least information loss [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 170, "endOffset": 173}, {"referenceID": 6, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 175, "endOffset": 178}, {"referenceID": 7, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 11, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 189, "endOffset": 193}, {"referenceID": 12, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 195, "endOffset": 199}, {"referenceID": 13, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 160, "endOffset": 164}, {"referenceID": 7, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 166, "endOffset": 169}, {"referenceID": 8, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 171, "endOffset": 174}, {"referenceID": 18, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "In this paper, we focus on the linear feature transformation methods due to its simplicity and effectiveness [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "In literature, principal component analysis (PCA) [15] and linear discriminant analysis (LDA) [21] are two classical linear algorithms, both capturing global Euclidean structure of the data.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 179, "endOffset": 183}, {"referenceID": 16, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 244, "endOffset": 248}, {"referenceID": 21, "context": "Meanwhile, some work integrates global and local information during feature transformation, such as LPMIP [23], LapLDA [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "Meanwhile, some work integrates global and local information during feature transformation, such as LPMIP [23], LapLDA [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "In order to alleviate this limitation, the maximum margin projection (MMP) algorithm [25] takes advantage of a binary support vector machine (SVM) [26] classifier to obtain some hyperplanes that separate data points in different clusters with the maximum margin.", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "In order to alleviate this limitation, the maximum margin projection (MMP) algorithm [25] takes advantage of a binary support vector machine (SVM) [26] classifier to obtain some hyperplanes that separate data points in different clusters with the maximum margin.", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "The random projection algorithms [27], [28] aim to find some Gaussian random projection matrices to preserve the pairwise distances between data points in the projected subspace, which can be effectively combined with some classifiers, such as SVM.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "The random projection algorithms [27], [28] aim to find some Gaussian random projection matrices to preserve the pairwise distances between data points in the projected subspace, which can be effectively combined with some classifiers, such as SVM.", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "Varshney and Willsky [29] propose a framework to simultaneously learn a linear dimensionality reduction projection matrix and a margin-based classifier defined in the reduced space.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "The main idea of the maximum margin projection pursuit (MMPP) algorithm is to integrate optimal data embedding and SVM classification in a single framework in both bi-class and multi-class classification [30].", "startOffset": 204, "endOffset": 208}, {"referenceID": 29, "context": "In addition, local discriminant gaussian (LDG) [31] is a feature transformation method which exploits a smooth approximation of the leave-one-out cross validation error of a classifier.", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "[32] propose a framework for joint subspace learning and feature selection, called FSSL, which can alleviate the effect of the noisy features for feature transformation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "discriminative features and designing classifiers [33].", "startOffset": 50, "endOffset": 54}, {"referenceID": 32, "context": "Margin maximization has been demonstrated to be a good principle applied by various learning methods [34], [35].", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "Margin maximization has been demonstrated to be a good principle applied by various learning methods [34], [35].", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "We take the LSVT Voice Rehabilitation dataset [36], a biclass classification dataset, as an example to illustrate the effectiveness of the l2,1 norm constraint on P in the objective function (2).", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "In real-world applications, one classification task is often correlated with other classification tasks, and mining the correlations among multiple categories can be good for feature learning [33].", "startOffset": 192, "endOffset": 196}, {"referenceID": 35, "context": "\u0393 plays the role of the inverse covariance matrix that encodes the correlations among the weight vectors wi [37].", "startOffset": 108, "endOffset": 112}, {"referenceID": 36, "context": "We use the Urban Land Cover dataset [38], a multi-class classification dataset, to visualize the correlation coefficient matrix of the weight vectors, which can be obtained based on the learned \u0393.", "startOffset": 36, "endOffset": 40}, {"referenceID": 37, "context": "Here we choose the limited-memory BFGS (L-BFGS) algorithm for its efficiency [39], [40], which is summarized in Algorithm 1.", "startOffset": 77, "endOffset": 81}, {"referenceID": 38, "context": "Here we choose the limited-memory BFGS (L-BFGS) algorithm for its efficiency [39], [40], which is summarized in Algorithm 1.", "startOffset": 83, "endOffset": 87}, {"referenceID": 39, "context": "We randomly initialize the parameters, and adopt the second updating rule in [41] for deriving the local optimal solution.", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": "Details of the proof on (11) can be found in [37].", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "Varshney and Willsky [29] propose a linear dimensionality reduction method, which represents the learned mappings by matrices on the Stiefel manifold and on margin-based classifiers.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "[30] present the maximum margin projection pursuit (MMPP) algorithm, which learns the optimal data embedding and the SVM classifier simultaneously.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "When \u03b7 \u2192 0, \u03bb\u2192 0, \u03c1\u2192 0 in (7), our objective function has similar effects to those of [29] and [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "When \u03b7 \u2192 0, \u03bb\u2192 0, \u03c1\u2192 0 in (7), our objective function has similar effects to those of [29] and [30].", "startOffset": 95, "endOffset": 99}, {"referenceID": 30, "context": "[32] propose a framework for joint subspace learning and feature selection, where subspace learning is reformulated as solving a linear system equation, and feature selection is achieved by utilizing l2,1-norm on the projection matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "When C \u2192 0, \u03c1\u2192 0, and adding a constraint on the projection matrix in (7), the formulation of our method is reduced to that of [32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "[42] propose a semi-supervised feature selection method called FS-Manifold, where the feature selection process is embedded with a manifold regularized SVM classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "According to [35], we know the original max-margin objective function of SVM can be transformed into its dual version as:", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "We evaluate the performance of MMLDF on eight realworld datasets, including one aerial image dataset Urban Land Cover [38], two biomedical area datasets DNA and Glioma, one large scale learning competition dataset Epsilon, one speech signal processing area dataset LSVT Voice Rehabilitation [36], one business area dataset CNAE-9, one face recognition dataset Yale Face, and one scene classification dataset 15 scene.", "startOffset": 118, "endOffset": 122}, {"referenceID": 34, "context": "We evaluate the performance of MMLDF on eight realworld datasets, including one aerial image dataset Urban Land Cover [38], two biomedical area datasets DNA and Glioma, one large scale learning competition dataset Epsilon, one speech signal processing area dataset LSVT Voice Rehabilitation [36], one business area dataset CNAE-9, one face recognition dataset Yale Face, and one scene classification dataset 15 scene.", "startOffset": 291, "endOffset": 295}, {"referenceID": 16, "context": "\u2022 SOLDE: Stable Orthogonal Local Discriminant Embedding [17] reduces the dimensions by considering both the diversity and similarity.", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "\u2022 LDG: Local Discriminant Gaussian [31] exploits a smooth approximation of the leave-one-out cross validation error of a quadratic discriminant analysis classifier4.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "\u2022 LPMIP: Locality-Preserved Maximum Information Projection [23] aims to preserve the local structure while maximizing the global information simultaneously.", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "\u2022 LapLDA: Laplacian Linear Discriminant Analysis [24] presents a least squares formulation for LDA, which intends to preserve both of the global and local structures.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "\u2022 LSDA: Locality Sensitive Discriminant Analysis [22] aims to seek a projection which maximizes the margin between data points from different classes at local areas.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "\u2022 FSSL: This method proposes a framework for joint feature selection and subspace learning [32].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "\u2022 MMPP: Maximum Margin Projection Pursuit [30] aims to find a subspace based on maximum margin principle.", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "edu/ml/datasets/CNAE-9 4The MATLAB code for LDG was obtained from the authors of [31]", "startOffset": 81, "endOffset": 85}], "year": 2017, "abstractText": "In this paper, we propose a new max-margin based discriminative feature learning method. Specifically, we aim at learning a low-dimensional feature representation, so as to maximize the global margin of the data and make the samples from the same class as close as possible. In order to enhance the robustness to noise, we leverage a regularization term to make the transformation matrix sparse in rows. In addition, we further learn and leverage the correlations among multiple categories for assisting in learning discriminative features. The experimental results demonstrate the power of the proposed method against the related state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}