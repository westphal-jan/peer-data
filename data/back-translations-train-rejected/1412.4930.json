{"id": "1412.4930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Rehabilitation of Count-based Models for Word Vector Representations", "abstract": "Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarties as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words.", "histories": [["v1", "Tue, 16 Dec 2014 09:43:56 GMT  (283kb,D)", "https://arxiv.org/abs/1412.4930v1", null], ["v2", "Wed, 8 Apr 2015 18:35:17 GMT  (39kb,D)", "http://arxiv.org/abs/1412.4930v2", "A. Gelbukh (Ed.), Springer International Publishing Switzerland"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\\'emi lebret", "ronan collobert"], "accepted": false, "id": "1412.4930"}, "pdf": {"name": "1412.4930.pdf", "metadata": {"source": "CRF", "title": "Rehabilitation of Count-based Models for Word Vector Representations", "authors": ["R\u00e9mi Lebret", "Ronan Collobert"], "emails": ["remi@lebret.ch,", "ronan@collobert.com"], "sections": [{"heading": null, "text": "Distributed word representations (also known as word embedding) are trained to optimally predict the context in which the corresponding words tend to occur. Such models have managed to capture word similarities as well as semantic and syntactic regularities. Instead, we aim to revive interest in a counting-based model. We present a systematic study of the use of Hellinger distance to extract semantic representations from the word coordination statistics of large corpora texts. We show that this distance performs well in word similarities and analogy tasks, with appropriate type and size of context, and dimensionality reduction based on a stochastic low-ranking approach. This method is not only simple and intuitive, but also provides a coding function to derive invisible words or phrases, which becomes a clear advantage over predictable models that need to train these new words."}, {"heading": "1 INTRODUCTION", "text": "In this year, it has come to the point where one sees oneself in a position to live in a country in which most people are able to live in a country in which they are able to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live"}, {"heading": "2 HELLINGER-BASED WORD VECTOR REPRESENTATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Word Co-Occurrence Probabilities", "text": "\"You will know a word from the company that holds it.\" (Firth, 1957) Keeping this famous quote, the probabilities of word coincidences are calculated by counting the number of times each contextual word c (c, w) occurs by one word w (c, w): p (c, w) = p (c, w) p (w) = n (c, w), (1) where n (c, w) is the number of times a contextual word c occurs in the vicinity of the word w. (2) By repeating this process over all the words of W, the word interaction (words) is thus obtained for each word w: Pw = p (c1 | w),."}, {"heading": "2.2 Hellinger Distance", "text": "Similarities between words can be derived by calculating a distance between their corresponding word distributions. There exist several distances (or metrics) via discrete distributions, such as the Bhattacharyya distance, the Hellinger distance or the Kullback-Leibler divergence. We chose the Hellinger distance here because of its simplicity and symmetry property (since it is a true distance). Taking into account two discrete probability distributions P = (p1,..., pk) and Q = (q1,., qk), the Hellinger distance is formally defined as: H (P, Q) = 1, 2, 2, 3, 4, 5, which is directly related to the Euclidean standard of the difference of square root vectors: H (p1,..., pk) and Q = (q1,., 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, Q."}, {"heading": "2.3 Dimensionality Reduction", "text": "Since discrete distributions depend on the size of the dictionary, the direct use of distribution as a word representation for large dictionaries is generally incomprehensible, especially for a large number of context words, with distributions becoming less sparse. We examine two approaches to embed these representations in a low-dimensional space: (1) a major component analysis (PCA) of the word co-occurrence matrix C, (2) a stochastic approach to coding distributions \u221a Pw."}, {"heading": "2.3.1 Principal Component Analysis (PCA)", "text": "We perform a principal component analysis (PCA) of the square root of the word Co-Incience probability matrix to represent words in a lower dimensional space while minimizing the reconstruction error according to the Hellinger distance. This PCA can be performed by eigenvalue analysis of the covariance matrix C-T-C-B. With a limited size of the context dictionary D (tens of thousands of words), this operation is very quick to perform (see Lebret and Collobert (2014) essay for details). With a larger size for D, a shortened singular value analysis of C-A could be an alternative, even if it is time consuming and memory-hungry."}, {"heading": "2.3.2 Stochastic Low-Rank Approximation (SLRA)", "text": "To solve this problem and still fit into the memory, we propose a low-level stochastic approach to represent words in a lower dimensional space. It takes a distribution \u221a Pw as input, encodes it in a more compact representation, and is trained to reconstruct its own input from this representation: | | V UT \u221a Pw | | 2, (7) where U and V \u0432 R | D | \u00b7 d. U is a slight approximation of the co-occurrence matrix C, which maps distributions in a d dimension (with d | D |), and V is the reconstruction matrix. UT \u221a Pw is a distributed representation that captures the most important variation factors in the data, as does the Hellinger PCA. U and V are trained by back propagation using stoic gradation."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Building Word Representation over Large Corpora", "text": "Our English corpus consists of the entire English Wikipedia1 (where all labels in the MediaWiki have been removed). We consider lowercase letters to be the limit of the number of words in the dictionary. Additionally, all occurrences of number strings within a word are replaced by the string \"NUMBER.\" The resulting text is symbolized by the Stanford tokenizer2. The data set contains about 1.6 billion words. As a dictionary W, we consider all words within this dictionary that occur at least a hundred times. This results in a dictionary of 191,268 words. Five scenarios assume that the word can form simultaneous probabilities with the context words D: (1) Only the 10,000 most common words within this dictionary. (2) All dictionaries. Mikolov et al. (2013b) have shown that better word representations can be achieved by subbing the frequent words. \u2212 We therefore define the following scenarios: (3), the number of which is greater than the number of words (5)."}, {"heading": "3.2 Evaluating Word Representations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Word analogies", "text": "The word analogy task consists of questions such as \"a is to b as c is to?.\" It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset. The 8,869 semantic questions are analogies to places such as \"Bern is to Switzerland like Paris?\" or family relationships such as \"Uncle is to aunt like boy?\" The 10,675 syntactic questions are grammatical analogies that include plural and adjective forms, superlatives, verbs, etc. To answer the question correctly, the model should clearly identify the missing term, with only an exact match as the correct match.1Available at http: / / downad.wikimedia.org."}, {"heading": "3.2.2 Word Similarities", "text": "We also evaluate our model using a variety of word similarity tasks, including the WordSimilarity353 Test Collection (WS-353) (Finkelstein et al., 2002), the Rubenstein and Goodenough datasets (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (Luong et al., 2013), all of which contain sentences of English word pairs along with human-assigned similarity judgments. WS-353 and RG-65 datasets contain 353 and 65 word pairs, respectively. These are relatively common word pairs, such as computers: Internet or football: tennis. The RW dataset differs from these two datasets because it contains 2034 pairs in which one of the words is rare or morphologically complex, such as brigadier: general or cognitive: knowledge."}, {"heading": "3.3 Analysis of the Context", "text": "With respect to context, two main parameters play a role: (1) The context window size to consider, i.e. the number of context words c to count for a given word w. Either we can count only context words occurring after w (asymmetrical context window), or we can count words surrounding w (symmetrical context window). (2) The type of context to use, i.e. which words to choose to define the context dictionary D. Do we need all words, the most common or, on the contrary, the rare? Figure 1 represents the performance achieved on the benchmark datasets for all five scenarios described in Section 3.1 with different context sizes. In this analysis, no dimensionality reduction was applied. Similarities between words are calculated using the Hellinger distance between the word probability distributions. \"For the word task, we used the Gold School (2014), the 3Object Word Explanation."}, {"heading": "3.3.1 Window Size", "text": "Apart from semantic analogy questions, the best results are always achieved with a symmetrical context window of size 1. However, the best results are achieved with a symmetrical window of 10 words to the semantic analogy question. This intuition is confirmed by looking at the closest neighbours of certain rare words with different context sizes. In Table 1, we can observe that a window of a context word merges words that occur in the same syntactic structure, while a window of ten context words goes beyond that and adds semantic information. Thus, with only one word of the context, Lake Baikal is a neighbour to other lakes, and the word special needs is close to other words composed of two words. With ten words of context asymmetry, the closest neighbours of the word give the word in direct relation to that place, and the word special needs cannot always be related to a similar context."}, {"heading": "3.3.2 Type of Context", "text": "First, the use of all words as context does not mean achieving the best performance. Second, the performance of the 10,000 most common words is quite similar to that of all words. An intermediate situation with words whose frequency is greater than 10 \u2212 6 also results in a fairly similar performance. Second, discarding the most common words from context distributions generally helps to increase performance. In fact, the best performance is achieved with scenarios (3) and (4). But all rare words are not necessarily essential to achieving good performance, as the results are not significantly lower for words whose frequency is less than 10 \u2212 5 and greater than 10 \u2212 6. These two observations could be explained by the low frequency of probability distributions. Cencies in Table 2 show significant differences in scarcity depending on context type. Similarities between words seem to be easier to find with sparse distributions."}, {"heading": "3.4 Dimensionality Reduction Models", "text": "Context analysis shows that word similarities can be found even in extremely sparse word vector representations. However, these representations lack semantic information because they perform poorly in the word analogy task with semantic questions. A symmetrical window of five or ten context words seems to be the best way to capture both syntactic and semantic information about words. Average number of context words is much higher within these parameters, which justifies the need for dimensionality reduction. Furthermore, this analysis shows that a large number of context words is not necessary to achieve significant improvements. Good performance in syntactic and similarity tasks can be achieved by rehabilitating counter models for word vector representations that use 10,000 common words as context. Instead, the distribution of a limited number of rare words increases performance in the semantic task while reducing performance in tactical and similarity tasks (we focus on the number of both scenarios with 5 words)."}, {"heading": "3.4.1 Number of Dimensions", "text": "This number must be large enough to maintain maximum variability, and it must be small enough for the dimensionality reduction to be truly significant and effective. Thus, we analyze the impact of the number of dimensions using the Hellinger PCA of the coexistence matrix from scenario (1) with a symmetrical context of five and ten words. Figure 2 reports on the performance of the benchmark datasets for different dimensions described in Section 3.2. The PCA's ability to summarize the information compactly results in improved results in word similarity tasks where performance is better than without dimensionality reduction. However, for the WS-353 and RG-65 datasets, we observe that the increase in performance tends to stabilize between 300 and 1,200 dimensions, and the increase in dimensions results in a small decrease after 100 dimensions on the RW dataset."}, {"heading": "3.4.2 Stochastic Low-Rank Approximation vs Covariance-based PCA", "text": "In this section, we compare the performance of both word evaluation tasks with the two methods for reducing dimensionality described in Section 2.3. All results are in Table 3. Apart from some isolated results, the performance is always much better with the stochastic approach of low-rank approximation than with a Hellinger approach of low-rank approximation. Calculation of the reconstruction error of both approaches confirms that the PCA somehow does not adequately reduce dimensionality. At a reduction from 10,000 to 100 dimensions, the reconstruction error of the PCA is 532.2 compared to 440.3 for low-rank stochastic approximation. This result is not really surprising, since the standard PCA is known to be exceptionally fragile, and the quality of its results can suffer dramatically in the face of only a few grossly corrupt points (Jolliffe, 1986)."}, {"heading": "3.5 Comparison with Other Models", "text": "We compare our word representations with other available models for calculating vector representations of words: (1) the GloVe model, which is also based on corpora coexistence statistics (Pennington et al., 2014) 3, (2) the Continuous Word Wallet (CBOW) and the Skip-gram (SG) architectures, which learn representations from predictive models (Mikolov et al., 2013b) 4. The same corpus and dictionary W as those described in Section 3.1 are used to train 200-dimensional word vector representations. We use a3Code, which is available at http: / / www-nlp.stanford.edu / software / glove.tar.gz.4Code, which is available at http: / word2vec.googlecode.com / svn / trunk /."}, {"heading": "3.6 Inference", "text": "This is a clear advantage compared to methods that focus on learning word embedding, where the entire system needs to be retrained to learn representations for these new words. To derive a representation for a new word, you only need to count your contextual words over a large body of text to make the distribution. This nice feature can be extrapolated to phrases. Table 5 presents some interesting examples of invisible phrases where the meaning clearly depends on the composition of their words. For example, words from the Chicago Bulls unit differ in meaning separately. Chicago will be close to other American cities, and bulls will be close to other horned animals. However, Table 5 shows that our model contains a representation for this new phrase that is similar to other NBA teams, such as the Lakers or the Celtics. This also works with longer phrases, such as New York City or the President of the United States of America."}, {"heading": "4 CONCLUSION", "text": "We presented a systematic study of a method based on counting and the Hellinger distance to build word vector representations, the main results of which are: (1) a large window of contextual words is crucial to capture both syntactic and semantic information; (2) a contextual dictionary of rare words helps capture semantic words, but by using only a fraction of the most common words, a high level of performance is already ensured; (3) a reduction in size through a low-level stochastic approach exceeds the PCA approach. The aim of the paper was to rehabilitate Countvector-based models, while all attention is now focused on context-dependent models. We show that such a simple model can deliver good results on both similarity and analogy tasks. It is even better to infer invisible words or phrases when relying on counting."}, {"heading": "Acknowledgements", "text": "This work was supported by the HASLER Foundation with the support of \"Information and Communication Technology for a Better World 2020\" (SmartWorld)."}], "references": [{"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy.,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behav Res Methods,", "citeRegEx": "Bullinaria and Levy.,? \\Q2012\\E", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Finkelstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1952\\E", "shortCiteRegEx": "Firth.", "year": 1952}, {"title": "Distributional Representations for Handling Sparsity in Supervised SequenceLabeling. In Proceedings of the Association for Computational Linguistics (ACL), pages 495\u2013503", "author": ["F. Huang", "A. Yates"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Huang and Yates.,? \\Q2009\\E", "shortCiteRegEx": "Huang and Yates.", "year": 2009}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "Jolliffe.,? \\Q1986\\E", "shortCiteRegEx": "Jolliffe.", "year": 1986}, {"title": "A solution to Plato\u2019s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer and Dumais.,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Word Embeddings through Hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the EACL,", "citeRegEx": "Lebret and Collobert.,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Towards a theory of semantic space", "author": ["W. Lowe"], "venue": null, "citeRegEx": "Lowe.,? \\Q2001\\E", "shortCiteRegEx": "Lowe.", "year": 2001}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["M. Luong", "R. Socher", "C.D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshp,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In NIPS", "citeRegEx": "Mnih and Kavukcuoglu.,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein and Goodenough.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P. Turney", "P. Pantel"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Turney and Pantel.,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Word Category Maps based on Emergent Features Created by ICA", "author": ["J.J. V\u00e4yrynen", "T. Honkela"], "venue": "Proceedings of the STeP\u20192004", "citeRegEx": "V\u00e4yrynen and Honkela.,? \\Q2004\\E", "shortCiteRegEx": "V\u00e4yrynen and Honkela.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities (Lowe, 2001).", "startOffset": 145, "endOffset": 157}, {"referenceID": 7, "context": "LSA (Landauer and Dumais, 1997), ICA (V\u00e4yrynen and Honkela, 2004)).", "startOffset": 4, "endOffset": 31}, {"referenceID": 19, "context": "LSA (Landauer and Dumais, 1997), ICA (V\u00e4yrynen and Honkela, 2004)).", "startOffset": 37, "endOffset": 65}, {"referenceID": 0, "context": "In Bullinaria and Levy (2007, 2012), the authors provide a full range of factors to use for properly extracting semantic representations from the word cooccurrence statistics of large text corpora. While word co-occurrence statistics are discrete distributions, an information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean distance over a discrete distribution space. In this respect, Lebret and Collobert (2014) propose to perform a principal component analysis (PCA) of the word cooccurrence probability matrix to represent words in a lower dimensional space, while minimizing the reconstruction error according to the Hellinger distance.", "startOffset": 3, "endOffset": 461}, {"referenceID": 8, "context": "This paper proposes an extension of the work of Lebret and Collobert (2014) by investigating the impact of different factors.", "startOffset": 48, "endOffset": 76}, {"referenceID": 8, "context": "This paper proposes an extension of the work of Lebret and Collobert (2014) by investigating the impact of different factors. Mikolov et al. (2013b) show that a subsampling approach to imbalance between the rare and frequent words improves the performance.", "startOffset": 48, "endOffset": 149}, {"referenceID": 8, "context": "This paper proposes an extension of the work of Lebret and Collobert (2014) by investigating the impact of different factors. Mikolov et al. (2013b) show that a subsampling approach to imbalance between the rare and frequent words improves the performance. Recent approaches for word representation have also shown that large windows of context are helpful to capture semantic information (Mikolov et al., 2013b, Pennington et al., 2014). While, in Lebret and Collobert (2014), only the 10,000 most frequent words from the dictionary W are considered as context dictionary D, we ar X iv :1 41 2.", "startOffset": 48, "endOffset": 477}, {"referenceID": 2, "context": "Recently, distributed approaches based on neural network language models have revived the field of learning word embeddings (Collobert and Weston, 2008, Huang and Yates, 2009, Turian et al., 2010, Mnih and Kavukcuoglu, 2013, Mikolov et al., 2013a). Such approaches are trained to optimally predict the contexts in which words from W tend to appear. Baroni et al. (2014) present a systematic comparison of these predictive models with the models based on co-occurrence counts, which suggests that context-predicting models should be chosen over their count-based counterparts.", "startOffset": 125, "endOffset": 370}, {"referenceID": 8, "context": "With a limited size of context word dictionary D (tens of thousands of words), this operation is performed very quickly (See Lebret and Collobert (2014) paper for details).", "startOffset": 125, "endOffset": 153}, {"referenceID": 12, "context": "Mikolov et al. (2013b) have shown that better word representations can be obtained by subsampling of the frequent words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset.", "startOffset": 21, "endOffset": 44}, {"referenceID": 3, "context": "These include the WordSimilarity353 Test Collection (WS-353) (Finkelstein et al., 2002), the Rubenstein and Goodenough dataset (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al.", "startOffset": 61, "endOffset": 87}, {"referenceID": 16, "context": ", 2002), the Rubenstein and Goodenough dataset (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al.", "startOffset": 54, "endOffset": 87}, {"referenceID": 11, "context": ", 2002), the Rubenstein and Goodenough dataset (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al., 2013).", "startOffset": 121, "endOffset": 141}, {"referenceID": 9, "context": "For the word analogy task, we used the objective function 3CosMul defined by Levy and Goldberg (2014), as we are dealing with explicit word representations in this case.", "startOffset": 77, "endOffset": 102}, {"referenceID": 6, "context": "the quality of its output can suffer dramatically in the face of only a few grossly corrupted points (Jolliffe, 1986).", "startOffset": 101, "endOffset": 117}, {"referenceID": 6, "context": "the quality of its output can suffer dramatically in the face of only a few grossly corrupted points (Jolliffe, 1986). Covariance-based PCA as proposed in Lebret and Collobert (2014) is thus not an approach offering a complete guarantee of success.", "startOffset": 102, "endOffset": 183}], "year": 2015, "abstractText": "Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarities as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurrence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words.", "creator": "LaTeX with hyperref package"}}}