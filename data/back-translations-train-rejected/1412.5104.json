{"id": "1412.5104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Locally Scale-Invariant Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting.", "histories": [["v1", "Tue, 16 Dec 2014 18:09:34 GMT  (321kb,D)", "http://arxiv.org/abs/1412.5104v1", "Deep Learning and Representation Learning Workshop: NIPS 2014"]], "COMMENTS": "Deep Learning and Representation Learning Workshop: NIPS 2014", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["angjoo kanazawa", "abhishek sharma", "david jacobs"], "accepted": false, "id": "1412.5104"}, "pdf": {"name": "1412.5104.pdf", "metadata": {"source": "CRF", "title": "Locally Scale-Invariant Convolutional Neural Networks", "authors": ["Angjoo Kanazawa", "Abhishek Sharma"], "emails": ["kanazawa@umiacs.umd.edu", "bohkaal@umiacs.umd.edu", "djacobs@umiacs.umd.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to feel as if they are able to survive on their own."}, {"heading": "2 Background", "text": "This year, it is only a matter of time before a solution is found, until an agreement is reached."}, {"heading": "2.1 Convolutional Neural Networks", "text": "Convolutionary Neural Networks (ConvNets) are a supervised, multi-layered network architecture in which each layer learns feature detectors of increasing complexity, the last layer being a classifier or regressor with a cost function so that the network can be trained in a supervised manner. The entire network is jointly optimized by stochastic gradient pedigree with gradients calculated by backpropagation [2] A single layer in a ConvNet usually consists of feature extraction and non-linear activation steps, alternatively followed by spatial pooling and feature normalization. [11] The hallmark of ConvNets is the idea of interweaving local feature detectors throughout the image. This idea is motivated by the fact that similar patterns can occur anywhere in the 2D layout of pixels and that nearby values represent strong dependencies in natural images. [10] Local feature detectors are presented as the receptor of the kernel and its spatial extension function."}, {"heading": "3 Scale-Invariant Convolutional Neural Network", "text": "In this section we describe a scale invariant ConvNet (SI-ConvNet). Our formulation also allows the output of ConvNet to be locally scale invariant, whereby the representation of the same patterns on different scales will be similar 1. Figure 1 shows the adjacent comparison of the overall structure of these two layers."}, {"heading": "3.1 Forward Propagation", "text": "In fact, it is so that most of us are in a position to embark upon a search for a solution which is in a position in which they are in a position to survive themselves. (...) In the second half of the last decade, in which they are in a position in the second half of the twentieth decade, in the second half of the twentieth decade, in the second half of the twentieth decade, in which they are in a position to survive themselves in the second half of the twentieth century, in the second half of the twentieth century, in the second half of the twentieth decade, in the second half of the twentieth decade, in the second half of the twentieth century, and in themselves, in the second half of the twentieth century, and in the second half of the twentieth century, they are in themselves, and in themselves, and in the twentieth century, and in the second half of the twentieth century, they are in themselves, and in themselves, and in the twentieth century, and in the second half of the twentieth century, they are they, and in themselves, and in themselves, in the twentieth century, and in the second half of the twentieth century."}, {"heading": "3.2 Backward Propagation", "text": "Since the scale-invariant folding layer consists only of linear and maximum operations, its gradients can be calculated by a simple modification of the back-propagation algorithm. Backprop to maximize scale oversize is implemented by using the Argmax indices, similar to how backprop is performed for spatial max pooling. In the case of scale transformations, the error signal is propagated by the forming coefficients used to calculate the transformation. Please refer to the supplementary materials for the detailed derivatives."}, {"heading": "4 Experiments", "text": "We first compare the performance of the proposed method, known as \"SI-ConvNet,\" with other baseline methods, including traditional ConvNets. For all the experiments we have conducted, all networks share exactly the same hyperparameters and architecture, except for the folding layers that are replaced by scale-invariant folding layers. We are implementing our method using the opensource Caffe framework [12], and our code will be available online. To evaluate the effectiveness of SI-ConvNets, we need to experiment with a data set in which objects come at different scales, as there is no great gain that can be gained from learning in a scale-invariant manner if there is no scale variation in the data. Unfortunately, most benchmark data sets are not suitable for evaluating ConvNets."}, {"heading": "4.1 MNIST-Scale", "text": "This year, the number of unemployed has tripled compared to the previous year, and the number of unemployed has increased by 2.2 percent compared to the previous year."}, {"heading": "4.1.2 Effect of training data and number of parameters", "text": "For these experiments, we report the test error on 10,000 images. As discussed in Section 3.1, one of the major disadvantages of ConvNets is that the use of n scales in a scale-invariant convolution layer with m cores resembles a network that has nm cores without actually increasing the number of parameters n times. One of the major disadvantages of ConvNets is that it requires more training data as the number of parameters increases. By keeping the number of parameters fixed, SIConvNets can train a network that is n times larger and therefore more powerful to a less demanding amount of training data. The ability to exchange information between the same patterns on multiple scales allows SI-ConvNets to learn better properties with less data. In contrast, ConvNets learn several filters for the same pattern than Convots, and learning these filters requires many examples of this pattern on each scale."}, {"heading": "4.1.3 Robustness to Unfamiliar Scales and Scale Variation", "text": "In the following two experiments, we consistently increase the image sizes of the MNIST dataset from 28x28 to 40x40, so that we can experiment with a wider range of scale variations. To account for the larger scale range, we change the scales used in SI-ConvNet from 5 scales in [0.6-2] to 5 scales in [0.5-2.7] for these two experiments. We train and test on 10,000 images. First, we evaluate the ability to correctly classify images that are less common in the training data. Here, the training data is scaled using factors sampled by a Gaussian (1, 0.24) rather than a uniform distribution. The digits in the test data are scaled to a specific scale factor, and we vary the test scale between [0.4, 1.6] which corresponds to factors that depend on a Gaussian."}, {"heading": "5 Conclusion", "text": "By distributing the weights across multiple scales and locations, a single feature detector can capture this feature at any scale and location. By bundling the detector reactions across multiple scales, we achieve a locally scalable feature representation. Our architecture differs from previous approaches in that scale invariance is built into each fold layer independently. By maintaining the same number of parameters as conventional ConvNets while incorporating the previous balance, we can learn functions more efficiently, with lower revision probability. Our experiments show that SI-ConvNets outperform ConvNets in several aspects."}, {"heading": "A Back-propagation", "text": "To align the notation of backward propagation with that of an ordinary multilayer neural network, we write each step in the forward propagation of a scale-invariant convolution layer as a matrix vector multiplication. Let xl be a vectorized input at the layer l of length n (n = hwc for an h through w through c image). The spatial transformation T (x) can be written as a matrix vector multiplication of n by m matrix T encoding the interpolation coefficients, where n and m are the dimensions of the original and transformed input respectively T. With bilinear interpolation, each series of T has a non-zero coefficient. The convolution operation can be written as a matrix vector multiplication on the equation layer i, using W as a Toeplitz matrix matrix matrix."}, {"heading": "B Time Analysis", "text": "We discuss the increase in the number of folding operations required in a scale-invariant folding layer compared to a traditional folding layer. In the case of an n \u00b7 n \u00b7 m input image and a kernel of size k \u00b7 k \u00b7 m, a traditional folding layer calculates the linear combination of the core and a local region (n \u2212 k + 1) 2 = O (n2) times (using \"valid\" folding at the edges). Thus, for a scale-invariant folding layer that uses t scales in a step size of s > 1, the largest scale of which is sk, the input image is scaled to t-different images of size [skn, \u00b7 \u00b7, sk \u2212 tn]. Thus, the number of linear combinations to be calculated on all scaled inputs is (skn \u2212 k + 1) 2 + \u00b7 \u00b7 geometric (n \u2212 k + 1) 2 = 1k + (1k) (1 \u00b7 2k + 12)."}, {"heading": "C Experimental Details", "text": "All input is pre-processed by subtracting the training mean value and the pixel values are scaled to the [0, 1] range. Basic setup is a three-layer network, all experiments have this architecture unless otherwise specified; the first layer is a (SI) folding layer of 7 x 7 kernels at step speed with 36 function boards with ReLu activation function, followed by 2 x 2 max pooling of step two; the second layer is another (SI) folding layer of 5 x 5 kernels with 64 function boards with ReLu and 3 x 3 max pooling of step three; the third layer is a fully connected layer with 150 hidden variables with ReLu and this final output is sent to the size 10 logistic regression layer; the network is optimized by stochastic gradient descent of step three."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Lon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Yann LeCun", "Fu Jie Huang", "L\u00e9on Bottou"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Traffic sign recognition with multi-scale convolutional networks", "author": ["Pierre Sermanet", "Yann LeCun"], "venue": "In Proceedings of International Joint Conference on Neural Networks", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Learning invariant representations with local transformations", "author": ["Kihyuk Sohn", "Honglak Lee"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Transformation equivariant boltzmann machines", "author": ["Jyri J. Kivinen", "Christopher K.I. Williams"], "venue": "In Artificial Neural Networks and Machine Learning (ICANN),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Tiled convolutional neural networks", "author": ["Quoc V. Le", "Jiquan Ngiam", "Zhenghao Chen", "Daniel Jin hao Chia", "Pang Wei Koh", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Semantic road segmentation via multi-scale ensembles of learned features", "author": ["Jose M. Alvarez", "Yann LeCun", "Theo Gevers", "Antonio M. Lopez"], "venue": "In ECCV Workshop on Computer Vision in Vehicle Technology: From Earth to Mars,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning hierarchical features for scene labeling", "author": ["Cl\u00e9ment Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Yangqing Jia"], "venue": "http://caffe.berkeleyvision.org/,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew D. Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1].", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 161, "endOffset": 164}, {"referenceID": 0, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 217, "endOffset": 220}, {"referenceID": 5, "context": "[6] and [7] introduced transformation-invariant Restricted Boltzmann Machines (RBMs) where linear transformations of a filter are applied to each input to infer its highest activation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] and [7] introduced transformation-invariant Restricted Boltzmann Machines (RBMs) where linear transformations of a filter are applied to each input to infer its highest activation.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "and our goal is to incorporate scale-invariant feature learning into the extremely successful ConvNet models [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "Tiled convolutional neural networks [8] learn invariances implicitly by square-root pooling hidden units that are computed by partially un-tied weights.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "[9] fuses outputs of multiple ConvNets applied over multiple scales for semantic segmentation, but each ConvNet is learned independently without weight-sharing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] proposes a multi-scale ConvNet where outputs of all convolutional layers are fed to the classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], who train ConvNets over the Laplacian pyramid of images with tied weights for scene parsing problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The entire network is optimized jointly via stochastic gradient descent with gradients computed by backpropagation [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 10, "context": "This idea is motivated by the fact that similar patterns can appear anywhere in the 2D layout of pixels and that nearby values present strong dependencies in natural images [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "The convolution operation effectively ties the learned weights at multiple locations, which radically reduces the number of trainable parameters as compared to having different weights at each location [2].", "startOffset": 202, "endOffset": 205}, {"referenceID": 11, "context": "We implement our method using the opensource Caffe framework [12], and our code will be available online.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "Therefore, we experiment with the modified MNIST handwritten digit classification dataset introduced in [6] called MNIST-scale.", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "The network architecture is modeled after the ConvNets of [13] that achieve state-of-the-art on the original MNIST dataset, and we use the same pre-processing method and hyper-parameters unless otherwise noted.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "[10], Restricted Boltzmann Machine (RBM) and its scale-invariant version of Sohn et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Following the experimental protocol of [6], we train and test each network on 10,000 and 50,000 images respectively.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Method Test Error (%) on 6 train/test fold Restricted Boltzman Machine (RBM)[6] 6.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "1 Scale-invariant RBM [6] 5.", "startOffset": 22, "endOffset": 25}, {"referenceID": 12, "context": "5 Convolutional Neural Network [13] 3.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "23 Hierarchical ConvNets [10] 3.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "Note that in [13], ConvNet without dropout achieves state-of-the-art performance along with ConvNet with dropconnect.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "[1, 1] [0.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1, 1] [0.", "startOffset": 0, "endOffset": 6}], "year": 2014, "abstractText": "Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting.", "creator": "LaTeX with hyperref package"}}}