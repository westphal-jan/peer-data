{"id": "1412.5404", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts", "abstract": "The short text has been the prevalent format for information of Internet in recent decades, especially with the development of online social media, whose millions of users generate a vast number of short messages everyday. Although sophisticated signals delivered by the short text make it a promising source for topic modeling, its extreme sparsity and imbalance brings unprecedented challenges to conventional topic models like LDA and its variants. Aiming at presenting a simple but general solution for topic modeling in short texts, we present a word co-occurrence network based model named WNTM to tackle the sparsity and imbalance simultaneously. Different from previous approaches, WNTM models the distribution over topics for each word instead of learning topics for each document, which successfully enhance the semantic density of data space without importing too much time or space complexity. Meanwhile, the rich contextual information preserved in the word-word space also guarantees its sensitivity in identifying rare topics with convincing quality. Furthermore, employing the same Gibbs sampling with LDA makes WNTM easily to be extended to various application scenarios. Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods. And finally we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages.", "histories": [["v1", "Wed, 17 Dec 2014 14:18:52 GMT  (108kb,D)", "http://arxiv.org/abs/1412.5404v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yuan zuo", "jichang zhao", "ke xu"], "accepted": false, "id": "1412.5404"}, "pdf": {"name": "1412.5404.pdf", "metadata": {"source": "CRF", "title": "Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts", "authors": ["Yuan Zuo", "Jichang Zhao", "Ke Xu"], "emails": ["skywatcher.buaa@gmail.com", "jichang@buaa.edu.cn", "kexu@nlsde.buaa.edu.cn"], "sections": [{"heading": null, "text": "Keywords Word Co-occurrence Network \u00b7 Topic Modeling \u00b7 Short TextsYuan Zuo State Key Lab of Software Development Environment, Beihang University E-mail: skywatcher.buaa @ gmail.comJichang Zhao School of Economics and Management, Beihang University E-mail: jichang @ buaa.edu.cnKe Xu State Key Lab of Software Development Environment, Beihang University E-mail: kexu @ nlsde.buaa.edu.cnar Xiv: 141 2.54 04v1 [cs.CL] 1 7D ec2 01"}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2 Related works", "text": "Probabilistic topic models such as PLSA [16] and LDA [4] have been widely applied in the study of text corpora. Specifically, LDA is a more complete generative model, since it expands PLSA to topic distributions by adding Dirichlet Priors. Due to their expandability, many complicated variants of LDA and PLSA have been proposed over the last decade, such as the dynamic topic model [3], the social topic model [5], the author's topic-theme community model [22], and so on. While most of them are designed to treat normal texts with specific additional characteristics, such as time, social relationships, and authority, the sparse short texts also have much research interest in the existing literature, and most early studies focus on increasing data density through the use of auxiliary information. For example, Hong et al. [17] Theme models are based on aggregated tweets that share the same word, and these models work better than those that are trained directly on tweets."}, {"heading": "3 Word network topic model", "text": "As a result, the direct application of contextual conceptual models to short or unbalanced text models may not work as well as normal, balanced texts. To solve the above problem by a simple but general method, we propose a new framework that uses the same words as sampling [14] with LDA to detect latent word groups in a word co-occurrence. Here, latent word groups from the network are taken as theme components of a corpus. Furthermore, distribution over latent groups for each word is learned from our model. Details of the new framework are presented in the following subsections."}, {"heading": "4 Complexity analysis and word network re-weighting", "text": "Although our conclusion on WNTM is the same pseudo-document length. As the maximum number of sampling windows in the LDA is approximately identical, the ongoing time and space complexities are different from them. We will compare the time complexity in detail and then give a brief discussion on the spatial complexity of the two models. Finally, to reduce the number of documents, Kz is the number of topics and Ld is the average document length. Likewise, the time complexity for WNTM is O (NpKgLp), where Nd is the number of documents, Kg is the number of topics and Ld is the average document length. Likewise, the time complexity for WNTM is O (NpKgLp), where the number of pseudo-documents is equal, i.e., the size of vocabularies, Kg is the number of latent word groups (topics) and Lp is the average length of pseudo-documents."}, {"heading": "5 Experiments", "text": "In this paper, we evaluate our approach in three metrics, including topic quality, word similarity, and document classification. For each metric, we conduct extensive experiments on short texts or normal texts from the real world. For short texts, we use LDA and Biterm Topic Model (BTM) [41] as basic methodologies. For normal texts, we omit comparison with BTM due to its intense time complexity when applied to normal texts. it is worth noting that the comparison between WNTM and LDA can reveal the strengths and weaknesses of learning topics from document collection and word interaction. Most of the experiments in this section are conducted on a Windows Server with an Intel Xeon 2.40GHz CPU and 12G memory except for experiments using Wikipedia datasets. Due to the large volume of the Wikipedia dataset and the word cooperation network, the corresponding experiments are conducted in a Linux cluster with 13 nodes. Each node contains Open Source JeK 2.12 Intel CeX1 and Intel H27GB."}, {"heading": "5.1.1 Topic coherence", "text": "Topic coherence (also called UMass measurement [25]) is a comprehensive and automated assessment measure for topic models that measures the value of an individual topic by calculating the degree of semantic similarity between words with a high probability in the topic. Higher topic coherence often indicates a better quality of the topic, i.e. a better ability to interpret the topic. Topic coherence is defined asC (z; M (z)) = T \u2211 t = 2t \u2212 1 \u2211 l = 1 log D (m (z) t, m (z) l) + \u03b5 D (m (z) l), (4), where M (z) = (z) 1,..., m (z) T) is the list of T most likely words in topic z, D (m) counts the number of documents containing the word m, D (m, m \u2032) counts the number of documents containing both m and m \u00b2."}, {"heading": "5.1.2 Topic coherence on short texts", "text": "To explore the ability to learn high-quality topics from real-world short texts, we conduct experiments on micro-blogs sampled by Weibo. As a Twitter-like service in China, it also imposes a limited length for each tweet, i.e., no more than 140 Chinese characters. As the textual content of micro-blogs is non-formal, meticulous, and pre-processional, it is made publicly available by http: / / jgibblda.net / 2 http: / code.google.com / p / 3 http: / code.google.com / p / btm / 4. http: / ipv6.nlsde.buaa.edu.cn / zhaojichang / paper / wntm.raris rather necessary in pre-processing, we take the following steps to wash the collected corpus."}, {"heading": "5.1.3 Topic coherence on normal texts", "text": "We conduct experiments on the walls of Phan et al. [27] to explore the ability to find out what the number of topics is in the real world, and what the number of topics is in the real world, is listed in Table 2, where the number of words in the real world increases from 5 to 20. The number of topics in the real world is slightly higher than in the real world, while it is lower in the real world than in the real world."}, {"heading": "5.2.1 Semantic representation of a word", "text": "For LDA and BTM, the conditional topic distribution for a word w can be defined as its semantic representation: p (zk | w) = [p (z1 | w), p (z2 | w),..., p (zk | w)], where k is the number of topics. p (zk | w) is easy to obtain after Gibbs sampling phase has been completed, p (zk | w) = nw | zk nw, (5) where nw | zk for how many times w was assigned to the topic k during the sampling phase, and nw means the total event of w in the given corpus. With respect to WNTM, we do not need to calculate p (zk | w)."}, {"heading": "5.2.2 Word similarity tasks", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5.3.1 Evaluation on news corpus", "text": "To explore the effectiveness of WNTM in terms of ephemerality and normality, we must first assess its performance in terms of news titles and news content derived from the news corpus. However, we obtain 508,554 news titles listed in the table and 59,348 unique words. The average number of tokens in news broadcasts is 5.5."}, {"heading": "5.3.2 Document classification on imbalanced texts", "text": "To avoid the impact of randomness on documents that contain rare topics, we study the variation in their classification results by continuously improving the imbalance of texts. First, we build a balanced set of message content that was introduced previously, which includes messages from five classes, namely \"Education,\" \"Finance,\" and \"Unt.\" Within each class, there are 1,000 documents that are equally assigned. Second, we take different numbers of documents away from \"Auto,\" which are randomly selected to build rows with different levels of imbalance. Specifically, we build 8 groups of unbalanced data, each group containing 1,000 documents belonging to classes that contain dc documents, in which we leave dc = 800, 400, 200, 100, 80, 60, and 40. As dc, the imbalance ranges from 800 to 40, the imbalance of the data is improved."}, {"heading": "6 Conclusions", "text": "A simple but general approach called WNTM 63 is presented in this paper to facilitate the modeling of the topic in short and unbalanced texts at an acceptable cost. In contrast to traditional LDA-like solutions, it investigates topics from Word co-occurence networks and successfully reduces the data sparsity and heterogeneity of the topic document in the word-for-document area. Thorough experiments on short and normal texts suggest that WNTM may be an effective model for recognizing emerging topics or unexpected events in social media at relatively early stages. Nevertheless, there are promising new research guidelines for further studies. For example, we would like to investigate the influence that could have on topic outcomes as different means of establishing co-occurrence networks."}], "references": [{"title": "Incorporating domain knowledge into topic modeling via dirichlet forest priors", "author": ["D. Andrzejewski", "X. Zhu", "M. Craven"], "venue": "ICML, pp. 25\u201332", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D.M. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "ICML, vol. 28 (2), pp. 280\u2013288. JMLR: W&CP", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "ICML, pp. 113\u2013120", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res. 3, 993\u20131022", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Social-network analysis using topic models", "author": ["Y. Cha", "J. Cho"], "venue": "SIGIR, pp. 565\u2013574", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "S. Gerrish", "C. Wang", "J.L. Boyd-graber", "D.M. Blei"], "venue": "Advances in Neural Information Processing Systems 22, pp. 288\u2013296", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Short text classification improved by learning multi-granularity topics", "author": ["M. Chen", "X. Jin", "D. Shen"], "venue": "IJCAI, pp. 1776\u20131781", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Emerging topic detection for organizations from microblogs", "author": ["Y. Chen", "H. Amiri", "Z. Li", "T.S. Chua"], "venue": "SIGIR, pp. 43\u201352", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering coherent topics using general knowledge", "author": ["Z. Chen", "A. Mukherjee", "B. Liu", "M. Hsu", "M. Castellanos", "R. Ghosh"], "venue": "CIKM, pp. 209\u2013218", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic summarization of events from social media", "author": ["F.C.T. Chua", "S. Asur"], "venue": "ICWSM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science 41(6), 391\u2013407", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1990}, {"title": "Topic dynamics in weibo: Happy entertainment dominates but angry finance is more periodic", "author": ["R. Fan", "J. Zhao", "X. Feng", "K. Xu"], "venue": "Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2014 IEEE/ACM International, pp. 230\u2013233", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Trans. Inf. Syst. 20(1), 116\u2013131", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Web:http://www.arbylon.net/publications/textest.pdf", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Applying latent dirichlet allocation to group discovery in large graphs", "author": ["K. Henderson", "T. Eliassi-Rad"], "venue": "SAC, pp. 1456\u20131461", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR, pp. 50\u201357", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Empirical study of topic modeling in twitter", "author": ["L. Hong", "B.D. Davison"], "venue": "SOMA, pp. 80\u201388", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Incorporating lexical priors into topic models", "author": ["J. Jagarlamudi", "H. Daum\u00e9 III", "R. Udupa"], "venue": "EACL, pp. 204\u2013213", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Personalized query suggestion with diversity awareness", "author": ["D. Jiang", "K.T. Leung", "J. Vosecky", "W. Ng"], "venue": "ICDE, pp. 400\u2013411", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast topic discovery from web search streams", "author": ["D. Jiang", "K.W.T. Leung", "W. Ng"], "venue": "Proceedings of the 23rd International Conference on World Wide Web, WWW, pp. 949\u2013960. ACM, New York, NY, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Transferring topical knowledge from auxiliary long texts for short text clustering", "author": ["O. Jin", "N.N. Liu", "K. Zhao", "Y. Yu", "Q. Yang"], "venue": "CIKM, pp. 775\u2013784", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "The author-topic-community model for author interest profiling and community discovery", "author": ["C. Li", "W. Cheung", "Y. Ye", "X. Zhang", "D. Chu", "X. Li"], "venue": "Knowledge and Information Systems pp. 1\u201325", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised topic models", "author": ["J.D. Mcauliffe", "D.M. Blei"], "venue": "J. Platt, D. Koller, Y. Singer, S. Roweis (eds.) Advances in Neural Information Processing Systems 20, pp. 121\u2013128", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Rethinking lda: Why priors matter", "author": ["A. McCallum", "D.M. Mimno", "H.M. Wallach"], "venue": "Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, A. Culotta (eds.) NIPS, pp. 1973\u20131981. Curran Associates, Inc.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimizing semantic coherence in topic models", "author": ["D. Mimno", "H.M. Wallach", "E. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP, pp. 262\u2013272", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Text classification from labeled and unlabeled documents using em", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Mach. Learn. 39(2-3), 103\u2013134", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning to classify short and sparse text & web with hidden topics from large-scale data collections", "author": ["X.H. Phan", "L.M. Nguyen", "S. Horiguchi"], "venue": "WWW, pp. 91\u2013100", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Short text similarity based on probabilistic topics", "author": ["X. Quan", "G. Liu", "Z. Lu", "X. Ni", "L. Wenyin"], "venue": "Knowledge and Information Systems 25(3), 473\u2013491", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Characterizing microblogs with topic models", "author": ["D. Ramage", "S. Dumais", "D. Liebling"], "venue": "ICWSM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "EMNLP, pp. 248\u2013256", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "UAI, pp. 487\u2013494", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM 8(10), 627\u2013633", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1965}, {"title": "Statistical topic models for multi-label document classification", "author": ["T. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Machine Learning 88(1-2), 157\u2013208", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "A web-based kernel function for measuring the similarity of short text snippets", "author": ["M. Sahami", "T.D. Heilman"], "venue": "WWW, pp. 377\u2013386", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Exploring topic coherence over many models and many topics", "author": ["K. Stevens", "P. Kegelmeyer", "D. Andrzejewski", "D. Buttler"], "venue": "EMNLP-CoNLL, pp. 952\u2013961", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the limiting factors of topic modeling via posterior contraction analysis", "author": ["J. Tang", "Z. Meng", "X. Nguyen", "Q. Mei", "M. Zhang"], "venue": "ICML, pp. 190\u2013198", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Tcs: Efficient topic discovery over crowd-oriented service data", "author": ["Y. Tong", "C.C. Cao", "L. Chen"], "venue": "KDD, pp. 861\u2013870. ACM, New York, NY, USA", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing semantic relatedness using chinese wikipedia links and taxonomy", "author": ["X. Wang", "Y. Jia", "B. Zhou", "Z. Ding", "L. Zheng"], "venue": "Journal of Chinese Computer Systems 32(11), 2237\u20132242", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Topics over time: A non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "KDD, pp. 424\u2013433", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Twitterrank: Finding topic-sensitive influential twitterers", "author": ["J. Weng", "E.P. Lim", "J. Jiang", "Q. He"], "venue": "WSDM, pp. 261\u2013270", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "A biterm topic model for short texts", "author": ["X. Yan", "J. Guo", "Y. Lan", "X. Cheng"], "venue": "WWW, pp. 1445\u20131456", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "What trends in chinese social media", "author": ["L. Yu", "S. Asur", "B.A. Huberman"], "venue": "arXiv preprint arXiv:1107.3522", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamics of trends and attention in chinese social media", "author": ["L.L. Yu", "S. Asur", "B.A. Huberman"], "venue": "arXiv preprint arXiv:1312.0649", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparing twitter and traditional media using topic models", "author": ["W.X. Zhao", "J. Jiang", "J. Weng", "J. He", "E.P. Lim", "H. Yan", "X. Li"], "venue": "ECIR, pp. 338\u2013349", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to suggest questions in social media", "author": ["T. Zhou", "M.T. Lyu", "I. King", "J. Lou"], "venue": "Knowledge and Information Systems pp. 1\u201328", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 124, "endOffset": 134}, {"referenceID": 36, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 124, "endOffset": 134}, {"referenceID": 19, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 124, "endOffset": 134}, {"referenceID": 18, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 153, "endOffset": 160}, {"referenceID": 44, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 153, "endOffset": 160}, {"referenceID": 6, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 186, "endOffset": 189}, {"referenceID": 20, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 210, "endOffset": 217}, {"referenceID": 27, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 210, "endOffset": 217}, {"referenceID": 41, "context": "For example, in social media like Weibo, the amount of entertainment tweets is much larger than the number of ones in other categories [42,43,12].", "startOffset": 135, "endOffset": 145}, {"referenceID": 42, "context": "For example, in social media like Weibo, the amount of entertainment tweets is much larger than the number of ones in other categories [42,43,12].", "startOffset": 135, "endOffset": 145}, {"referenceID": 11, "context": "For example, in social media like Weibo, the amount of entertainment tweets is much larger than the number of ones in other categories [42,43,12].", "startOffset": 135, "endOffset": 145}, {"referenceID": 17, "context": "Since the objective of most commonly used topic models is to maximize the probability of the observed data, they tend to sacrifice the performance on rare topics [18].", "startOffset": 162, "endOffset": 166}, {"referenceID": 5, "context": "Consequently, those topic models may not perform well in extrinsic tasks [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "As a canonical form of existing topic models, LDA [4] is a hierarchical parametric Bayesian approach for topic discovery in a large corpus.", "startOffset": 50, "endOffset": 53}, {"referenceID": 38, "context": "Generally speaking, LDA-like models group semantically related words into a single topic by utilizing document-level word co-occurrence information [39], which makes them extremely sensitive to the document length and the number of documents related to each topic.", "startOffset": 148, "endOffset": 152}, {"referenceID": 35, "context": "[36] suggests that if the distribution over documents for each topic is heavily skewed, identifying topics from a small number of documents will be extremely difficult for LDA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "While in fact rare topics might be essential for newly emerging events discovery or real-time hot trends detection in online social media [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 39, "context": "For example, related short texts can be aggregated into lengthy pseudodocuments before training the topic model [40] or models trained from external data (e.", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "Wikipedia) can be used to help the topic inference in short texts [27].", "startOffset": 66, "endOffset": 70}, {"referenceID": 43, "context": "Besides, many arbitrary manipulations of LDA have also been introduced to satisfy the demands of specific short texts analyses [44,10,8].", "startOffset": 127, "endOffset": 136}, {"referenceID": 9, "context": "Besides, many arbitrary manipulations of LDA have also been introduced to satisfy the demands of specific short texts analyses [44,10,8].", "startOffset": 127, "endOffset": 136}, {"referenceID": 7, "context": "Besides, many arbitrary manipulations of LDA have also been introduced to satisfy the demands of specific short texts analyses [44,10,8].", "startOffset": 127, "endOffset": 136}, {"referenceID": 40, "context": "A typical example is the biterm topic model [41], which works well on short texts.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "On the other hand, with respect to the topic imbalance, performance improvement of LDA is mainly obtained by adding prior information to guide the topic learning progress [18, 1] or using asymmetric Dirichlet prior over the document-topic distribution [24].", "startOffset": 171, "endOffset": 178}, {"referenceID": 0, "context": "On the other hand, with respect to the topic imbalance, performance improvement of LDA is mainly obtained by adding prior information to guide the topic learning progress [18, 1] or using asymmetric Dirichlet prior over the document-topic distribution [24].", "startOffset": 171, "endOffset": 178}, {"referenceID": 23, "context": "On the other hand, with respect to the topic imbalance, performance improvement of LDA is mainly obtained by adding prior information to guide the topic learning progress [18, 1] or using asymmetric Dirichlet prior over the document-topic distribution [24].", "startOffset": 252, "endOffset": 256}, {"referenceID": 1, "context": "Since the topic quality can be guaranteed in the dense wordword space [2], we conjecture that learning topic components from word co-occurrence network rather than document collection is more reliable.", "startOffset": 70, "endOffset": 73}, {"referenceID": 14, "context": "topics) [15] and learns distribution over topics for words rather than topics for documents.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "Probabilistic topic models such as PLSA [16] and LDA [4] have been extensively applied in exploring text corpora.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Probabilistic topic models such as PLSA [16] and LDA [4] have been extensively applied in exploring text corpora.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Due to their extensibility, many complicated variants of LDA and PLSA have been proposed in the last decade, such as the dynamic topic model [3], social topic model [5], author-topic model [31] and author-", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Due to their extensibility, many complicated variants of LDA and PLSA have been proposed in the last decade, such as the dynamic topic model [3], social topic model [5], author-topic model [31] and author-", "startOffset": 165, "endOffset": 168}, {"referenceID": 30, "context": "Due to their extensibility, many complicated variants of LDA and PLSA have been proposed in the last decade, such as the dynamic topic model [3], social topic model [5], author-topic model [31] and author-", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": "topic-community model [22] etc.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "[17] train topic models on aggregated tweets that sharing the same word, and find those models work better than those being directly trained on original tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] propose a search-snippet-based similarity measure for short texts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "learn topics on short texts via transfer learning from auxiliary long text data [21].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "assume each tweet only covers a single topic [44].", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "[41] propose a special form of mixture of unigrams [26], which is called biterm topic model to improve topic modeling on short texts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[41] propose a special form of mixture of unigrams [26], which is called biterm topic model to improve topic modeling on short texts.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "propose Dirichlet forest priors to incorporate must-links and cannot-links constraints into topic models [1].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "use general lexical knowledge to help discovering coherent topics [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 29, "context": "bring labels into the generative process of the corpora [30,33] and Blei et al propose a supervised topic model [23] to predict the category labels for input labeled documents.", "startOffset": 56, "endOffset": 63}, {"referenceID": 32, "context": "bring labels into the generative process of the corpora [30,33] and Blei et al propose a supervised topic model [23] to predict the category labels for input labeled documents.", "startOffset": 56, "endOffset": 63}, {"referenceID": 22, "context": "bring labels into the generative process of the corpora [30,33] and Blei et al propose a supervised topic model [23] to predict the category labels for input labeled documents.", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "In order to solve the problem mentioned above through a simple but general method, we propose a new framework, which applies the same Gibbs sampling [14] with LDA to discover latent word groups in a word co-occurrence network.", "startOffset": 149, "endOffset": 153}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "did in [15] to discover latent word groups in word network.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "To sum up, when texts are short and sparse, learning topics in word-by-document space will suffer from the severe sparsity problem, while learning topics in a word-word space has a theoretical guarantee for topic coherence, which has been proved in [2].", "startOffset": 249, "endOffset": 252}, {"referenceID": 40, "context": "For short texts, We take LDA and biterm topic model (BTM) [41] as baseline methods.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "Furthermore, recent research shows that the perplexity does not always correlate with semantically interpretable topics [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 24, "context": "Topic coherence (also called UMass measure [25]) is a comprehensive and automated evaluation measure for topic models, which measures the score of a single topic by computing the semantic similarity degree between high probability words in the topic.", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "[27] to investigate WNTM\u2019s ability of learning high quality topics from real-world normal texts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] revealed that LDA is better than LSA (Latent Semantic Analysis) [11] at learning descriptive topics, while LSA is better than LDA at creating a compact semantic representation of words and documents and outperforms LDA in extrinsic tasks [35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[15] revealed that LDA is better than LSA (Latent Semantic Analysis) [11] at learning descriptive topics, while LSA is better than LDA at creating a compact semantic representation of words and documents and outperforms LDA in extrinsic tasks [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "[15] revealed that LDA is better than LSA (Latent Semantic Analysis) [11] at learning descriptive topics, while LSA is better than LDA at creating a compact semantic representation of words and documents and outperforms LDA in extrinsic tasks [35].", "startOffset": 243, "endOffset": 247}, {"referenceID": 37, "context": "[38] to evaluate the ability of word semantic modeling of two models on Weibo data set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Regarding to normal texts, we use word similarity tasks designed by Rubenstein and Goodenough in [32] and Finkelstein et al.", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "in [13] on Wikipedia data set.", "startOffset": 3, "endOffset": 7}], "year": 2014, "abstractText": "The short text has been the prevalent format for information of Internet in recent decades, especially with the development of online social media, whose millions of users generate a vast number of short messages everyday. Although sophisticated signals delivered by the short text make it a promising source for topic modeling, its extreme sparsity and imbalance brings unprecedented challenges to conventional topic models like LDA and its variants. Aiming at presenting a simple but general solution for topic modeling in short texts, we present a word co-occurrence network based model named WNTM to tackle the sparsity and imbalance simultaneously. Different from previous approaches, WNTM models the distribution over topics for each word instead of learning topics for each document, which successfully enhance the semantic density of data space without importing too much time or space complexity. Meanwhile, the rich contextual information preserved in the wordword space also guarantees its sensitivity in identifying rare topics with convincing quality. Furthermore, employing the same Gibbs sampling with LDA makes WNTM easily to be extended to various application scenarios. Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods. And finally we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages.", "creator": "LaTeX with hyperref package"}}}