{"id": "1412.5513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Towards a constructive multilayer perceptron for regression task using non-parametric clustering. A case study of Photo-Z redshift reconstruction", "abstract": "The choice of architecture of artificial neuron network (ANN) is still a challenging task that users face every time. It greatly affects the accuracy of the built network. In fact there is no optimal method that is applicable to various implementations at the same time. In this paper we propose a method to construct ANN based on clustering, that resolves the problems of random and ad hoc approaches for multilayer ANN architecture. Our method can be applied to regression problems. Experimental results obtained with different datasets, reveals the efficiency of our method.", "histories": [["v1", "Wed, 17 Dec 2014 18:36:23 GMT  (300kb)", "http://arxiv.org/abs/1412.5513v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["cyrine arouri", "engelbert mephu nguifo", "sabeur aridhi", "c\\'ecile roucelle", "gaelle bonnet-loosli", "norbert tsopz\\'e"], "accepted": false, "id": "1412.5513"}, "pdf": {"name": "1412.5513.pdf", "metadata": {"source": "CRF", "title": "Towards a constructive multilayer perceptron for regression task using non-parametric clustering. A case study of Photo-Z redshift reconstruction", "authors": ["C. Arouri", "E. Mephu Nguifo", "S. Aridhi"], "emails": [], "sections": [{"heading": null, "text": "This year, the time has come for us to be able to go in search of a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2.2 Neural Networks", "text": "Multilayer perceptron (MLP) is a static neural structure consisting of successive layers that communicate with each other and exchange information through synaptic connections represented by an adaptive weight.The structure of the multilayer network contains an input layer that consists of the number of perceptions corresponding to the number of data attributes, on the other hand, the output layer includes a perceptron in the case of regression or more if it is a task of classification and in this case the number of perceptrons is equal to the number of layers to be predicted as hidden.Figure1: Neural networks with 3couches, 3 inputs, 2 outputs and 4 hidden nodesMLPQNA is a learning rule algorithm, it is an optimization of the Newton method, it seeks local maxima and minima and includes several techniques."}, {"heading": "PHAT:", "text": "The PHAT (PHoto-z Accuracy Testing) project [10] provides a standard testing environment for estimating redshift and comparing the results obtained. We distinguish PHAT1 from a real data catalog for redshift estimation, which contains 1984 objects using only the fourth spectroscopic values for training, because all other values are set to z-spec = -9.999, so we need an extraction of useful data for training. Figure 3: Histogram of data (515 galaxies) At the attribute level, PHAT1 contains 18 attributes, including irrelevant values at 99 and -99, and these values are due to sampling errors and may affect the training step. Figure 2 also shows that 316 images of 515 are retained for training, do not contain false values, so we test these two data sets separately, so that we keep the one that gives us the best and worst percentage of RMS (Root error values in Figure 4) of the distribution in Figure 4."}, {"heading": "Deep2 D4:", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "4 Conclusion and remarks", "text": "The method described here extends the constructive MLP DistAl method and enables the handling of regression tasks. This method relies on non-parametric clustering to repair the 3-layer network, thus avoiding the provision of this parameter by the user. Experiments show that the ANN construction method with a first step of clustering is suitable for applications that process large amounts of data, in fact we find that the cluster time is still negligible compared to the ANN training time, so that a cluster algorithm for determining the number of hidden neurons is preferable to the repetitive process of training neural networks to find the optimal architecture. Furthermore, the method provides good results compared to the ad'hoc methods where a repetitive process is running to find the optimal architecture. The results of the method presented in this paper correspond to those performed with the Xmeans algorithm, but other algorithms can also be used, such as BAN or DAN."}, {"heading": "Acknowledgment", "text": "This work was supported by the PETASKY project for astrophysics."}], "references": [{"title": "Knowledge-based artificial neural networks", "author": ["G.G. Towell", "J.W. Shavlik"], "venue": "Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "DistAl : An inter-pattern distance-based constructive learning algorithm", "author": ["J.Yang", "R.Parekh", "V.Honavar"], "venue": "Intelligence Data Analysis", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "X-means : Extending K-means with Efficient Estimation of the Number of clusters", "author": ["D.Pelleg", "A.Moore"], "venue": "School of computer science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "Proceedings of the 2nd International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Mean shift : A robust approach toward feature space analysis", "author": ["D. Comaniciu", "P. Meer"], "venue": "IEEE Trans. Pattern Anal. Machine Intell.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The annals of statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1978}, {"title": "Numerical Methods For Unconstrained Optimization and Nonlinear Equations. Englewood Cliffs, New Jersey,1983", "author": ["J.E", "Jr.Dennis", "R.B.Schabel"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1983}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "In Parallel Distributed Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1986}, {"title": "ANNz : estimating photometric redshifts using artificial neural networks", "author": ["A.A.Collister", "O.Lahav"], "venue": "The Astrophysical Journal,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Subsequently a variety of methods have emerged, we mention: the ad\u2019hoc method, static methods among them those using prior knowledge about the learning as KBANN [1], as well as other static constructive methods based on concept lattice or deduced from a decision tree and finally dynamic approaches where neurons are created at the same time as learning without generating random weights in advance.", "startOffset": 161, "endOffset": 164}, {"referenceID": 1, "context": "Among the dynamic approaches, stands out the Distal (Inter-pattern distance-based constructive learning algorithm) method [2], based on a 3-layer architecture, with a single hidden layer, Distal [2] built the ANN, by grouping training data in regions bounded by thresholds without using a clustering algorithm.", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Among the dynamic approaches, stands out the Distal (Inter-pattern distance-based constructive learning algorithm) method [2], based on a 3-layer architecture, with a single hidden layer, Distal [2] built the ANN, by grouping training data in regions bounded by thresholds without using a clustering algorithm.", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "In order to find a new solution to choose the architecture of ANN and that can be applied in the case of a regression to approximate real values, we proposed a method that generalize other constructive approaches like DistAl [2] to deal with regression tasks.", "startOffset": 225, "endOffset": 228}, {"referenceID": 2, "context": "Among these non-parametric algorithms, we are interested in Xmeans [3], DBscan [4] and Meanshift [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Among these non-parametric algorithms, we are interested in Xmeans [3], DBscan [4] and Meanshift [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "Among these non-parametric algorithms, we are interested in Xmeans [3], DBscan [4] and Meanshift [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "Xmeans proceeds by successive application of 2-means and needs the comparison criterion BIC [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Among them stands out the technique of Broyden-Fletcher-GoldfarbShanno [7] also known as BFGS algorithm tested by Watrous and compared to the backpropagation algorithm and Fletcher algorithm Powell [8], it estimates the inverse of the symmetric positive definite the Hessian matrix of the cost function in O(n2) compared with the method of Newton O(n3), without having to calculate it automatically.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "In the third and final step, the ANN is trained with a learning algorithm, such as MLPQNA algorithm (Multi layers Perceptron Quasi-Newton algorithm) [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 8, "context": "To properly carry out the experiments, we used two reported and available toolbox: ANNZ [12] and Skynet[13], where the choice of multilayer architecture remains a difficult task.", "startOffset": 88, "endOffset": 92}], "year": 2014, "abstractText": "The choice of architecture of artificial neuron network (ANN) is still a challenging task that users face every time. It greatly affects the accuracy of the built network. In fact there is no optimal method that is applicable to various implementations at the same time. In this paper we propose a method to construct ANN based on clustering, that resolves the problems of random and ad\u2019hoc approaches for multilayer ANN architecture. Our method can be applied to regression problems. Experimental results obtained with different datasets, reveals the efficiency of our method.", "creator": "Word"}}}