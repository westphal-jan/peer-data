{"id": "1412.5567", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Deep Speech: Scaling up end-to-end speech recognition", "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called DeepSpeech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.5% error on the full test set. DeepSpeech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "histories": [["v1", "Wed, 17 Dec 2014 20:39:45 GMT  (333kb,D)", "http://arxiv.org/abs/1412.5567v1", null], ["v2", "Fri, 19 Dec 2014 21:36:13 GMT  (333kb,D)", "http://arxiv.org/abs/1412.5567v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["awni hannun", "carl case", "jared casper", "bryan catanzaro", "greg diamos", "erich elsen", "ryan prenger", "sanjeev satheesh", "shubho sengupta", "adam coates", "andrew y ng"], "accepted": false, "id": "1412.5567"}, "pdf": {"name": "1412.5567.pdf", "metadata": {"source": "CRF", "title": "DeepSpeech: Scaling up end-to-end speech recognition", "authors": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "Andrew Y. Ng"], "emails": ["awnihannun@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we describe a complete speech and speech recognition system that is much simpler than it is the way we imagine it to be. Results are made possible by the formation of a large recursive neural network (RNN) that uses multiple GPUs and thousands of hours of data. As this system learns directly from data, we do not need special components for speaker adaptation or speech noise. In fact, in settings where the robustness of speaker variations and noise are critical, our system has excesses previously published methods on the Hub5 \"00 corpus switch board, which achieves 16.5% error and performs better than commercial speech recognition systems."}, {"heading": "2 RNN Training Setup", "text": "The core of our work is a recurrent neural network (RNN), in which we are in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position in a position to be in a position in a position to be in a position in a position to be in a position to be in a position in a position to be in a position in a position to be in a position in a position to be in a position in a position to be in a position in a position to be in a position in a position to be in a position to be in a position in a position to be in a position in a position in a position to be in a position in a position to be in a position in a position to be in a position in a position to be in a position in a position in a position to be in a position in a position to be in a position in a position in a position to be in a position to be in a position in a position in a position in a position to be in a position to be in a position in a position in a position to be in a position in a position to be in a position in a position in a position in a position to be in a position to be in a in a position in a position to be in a position to be in a in a position in a position in a position to be in a to be in a position in a in a position in a position to be in a position to be in a to be in a position to be in a in a position in a position in a position in a position in a"}, {"heading": "2.1 Regularization", "text": "During the training, we apply a failure rate of between 5% and 10%. We apply dropouts in the feedback layers, but not to the recurring hidden activations. A common technique in computer vision during network evaluation is to randomly jitter input through translations or reflections, feed each blurred version through the network and tune or intersect the results [23]. Such jittering is not common in ASR, but we found it advantageous to translate the raw audio files by 5ms (half the size of the filter bank) to the left and right, then forward the recompiled features and determine the output probabilities on average."}, {"heading": "2.2 Language Model", "text": "In fact, the most likely string predicted by the RNN is just right for many of the transcriptions without external language limitations. In practice, this is hard to avoid: training enough speech data to hear all the words or speech constructions we might need to know is impractical. Therefore, we integrate our system with an N-gram language model, as these models are easily trained by huge unlabeled corpora of text. By comparison, while our speech records typically contain up to 3 million expressions, the N-gram language model used for the experiments in Section 5.2 is an N-gram language model, where the N-word is microscoped. While our speech records typically contain up to 3 million expressions, the N-gram language model used for the experiments in Section 5.2 is an N-word microscopic."}, {"heading": "3 Optimizations", "text": "As mentioned above, we have made several design decisions to make our networks usable for high-speed execution (and thus for fast training). For example, we have opted for homogeneous, linear networks that are easy to implement and rely only on a few highly optimized BLAS calls. When our networks are fully rolled out, they comprise nearly 5 billion connections for a typical utterance 4We use the KenLM toolkit [17] to train the N-gram language models in our experiments. Therefore, efficient computing is crucial to make our experiments feasible. We use multi-GPU training [7, 23] to speed up our experiments, but this effectively requires some additional work, as we explain."}, {"heading": "3.1 Data parallelism", "text": "To process data efficiently, we use two levels of data parallelism. First, each GPU processes many examples in parallel, in the usual way, by combining many examples into a single matrix. For example, rather than performing a single matrix vector multiplication Wrht at the recursive level, we prefer to calculate many in parallel by computing WrHt where Ht = [h (i) t, h (i + 1) t,.] (where h (i) t corresponds to the i'th example x (i) in due course). The GPU is most efficient when Ht is relatively wide (e.g. 1000 examples or more), and therefore we prefer to process as many examples on a GPU as possible (up to the limit of GPU memory). If we want to use larger minibatches than a single GPU, we cannot support data parallelism on our own."}, {"heading": "3.2 Model parallelism", "text": "Data parallelism leads to training accelerations for modest multiplications of minibatch size (e.g. 2 to 4), but faces decreasing returns because stacking more examples in a single gradient update does not improve the training convergence rate. That is, processing 2 x as many examples on 2 x as many GPUs does not result in 2 x acceleration. Furthermore, it is inefficient to fix the entire minibatch size, but to distribute the examples over 2 x as many GPUs: as the minibatch shrinks within each GPU, most operations are limited by memory bandwidth. To scale further, we parallelise by partitioning the model (\"model parallelism\" [7, 10]). Our model is parallelized due to the sequential nature of the recursive layers, most operations are limited by memory bandwidth."}, {"heading": "3.3 Striding", "text": "We have worked to minimize the runtime of the recurring layers of our RNN, as they are the most difficult to parallelise. As a final optimization, we shorten the recurring layers by performing \"steps\" (or steps) of size 2 in the original input, so that the rolled-out RNN has half as many steps. This is similar to a Convolutionary Network [25] with a step size of 2 in the first layer. We use the cuDNN library [2] to efficiently implement this first layer of folding."}, {"heading": "4 Training Data", "text": "Large deep learning systems require a wealth of marked data. We require a lot of recorded utterances and corresponding English transcriptions for our system, but there are few public records of sufficient size. Therefore, in order to train our largest models, we have collected an extensive dataset of 5000 hours of read speech from 9,600 speakers. For comparison, we have summarized the marked datasets available to us in Table 2."}, {"heading": "4.1 Synthesis by superposition", "text": "To expand our potential training data even further, we use data synthesis, which has been successfully applied in other contexts, to increase the effective number of training samples [37, 26, 6]. In our work, the primary goal is to improve performance in noisy environments where existing systems are collapsing. We can use this fact to synthesize noisy training data. For example, if we have a voice track x (i) and a \"sound track\" (i), we can generate the audio signals through a process of overlapping source signals. We can use this fact to synthesize noisy training data."}, {"heading": "4.2 Capturing Lombard Effect", "text": "One challenging effect of speech recognition systems in noisy environments is the \"Lombard effect\" [20]: speakers actively alter the pitch or diffraction of their voice to overcome sounds around them. This (involuntary) effect does not appear in recorded speech records, as they are collected in quiet environments. To ensure that the effect is represented in our training data, we intentionally induce the Lombard effect during data collection by playing loud background noise through headphones worn by a person while recording a utterance, which causes them to bend their voice, allowing us to capture the Lombard effect in our training data.5"}, {"heading": "5 Experiments", "text": "In both cases, we use the model described in Section 2, which was trained from a selection of data sets in Table 2, to predict character-level transcriptions, and the predicted probability vectors and language models are then fed into our decoder to obtain a word-level transcription that is compared with the basic-level transcription to obtain the word error rate (WHO)."}, {"heading": "5.1 Conversational speech: Switchboard Hub5\u201900 (full)", "text": "In fact, it is so that we are able to assert ourselves, that we are able, that we are able, that we are able to achieve our goals, and that we are able to achieve our goals, \"he said."}, {"heading": "5.2 Noisy speech", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6 Related Work", "text": "These systems, similar to DNN acoustic models [30, 18, 9], replace only one level of speech recognition techniques. Mechanically, our system is similar to other efforts to build a comprehensive speech system from deep learning algorithms. For example, Graves et al. [13] previously introduced the Connectionist Temporal Classification (CTC) loss function for transcriptions produced by RNNs and, with LSTM networks, previously applied this approach to language. We absorb CTC loss for part of our training process, but use much simpler recursive networks with rectified linear activations [12, 29, 31]. Our recursive network is similar to the bidirectional RNN used by Hannun et al."}, {"heading": "7 Conclusion", "text": "We have presented an end-to-end, deep learning language system that is capable of surpassing existing state-of-the-art recognition pipelines in two challenging scenarios: clear, conversation-based language and speech in loud environments. Our approach is enabled in particular by multi-GPU training and data acquisition and synthesis strategies to build large sets of training that address the distortions our system has to deal with (such as background noise and Lombard effect). Together, these solutions allow us to build a data-driven speech system that is both more powerful than existing methods, while no longer relying on the complex processing steps that had impeded further progress."}, {"heading": "Acknowledgments", "text": "We would like to thank Jia Lei, whose work on DL for speech at Baidu has propelled us, for his advice and support during this project. We would also like to thank Ian Lane, Dan Povey, Dan Jurafsky, Dario Amodei, Andrew Maas, Calisa Cole and Li Wei for helpful conversations."}], "references": [{"title": "Connectionist Speech Recognition: A Hybrid Approach", "author": ["H. Bourlard", "N. Morgan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "The Fisher corpus: a resource for the next generations of speech-to-text", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "In LREC,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Multi-column deep neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Text detection and character recognition in scene images with unsupervised feature learning", "author": ["A. Coates", "B. Carpenter", "C. Case", "S. Satheesh", "B. Suresh", "T. Wang", "D.J. Wu", "A.Y. Ng"], "venue": "In International Conference on Document Analysis and Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Deep learning with COTS HPC", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In 14th International Conference on AI and Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Size matters: An empirical study of neural network training for large vocabulary continuous speech recognition", "author": ["D. Ellis", "N. Morgan"], "venue": "In ICASSP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In 14th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Shift-invariance sparse coding for audio classification", "author": ["R. Grosse", "R. Raina", "H. Kwong", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1206.5241,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs", "author": ["A.Y. Hannun", "A.L. Maas", "D. Jurafsky", "A.Y. Ng"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The Lombard reflex and its role on human listeners and automatic speech recognizers", "author": ["J.-C. Junqua"], "venue": "Journal of the Acoustical Society of America,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed hessian-free optimization", "author": ["B. Kingsbury", "T. Sainath", "H. Soltau"], "venue": "In Interspeech,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1989}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Increasing deep neural network acoustic model size for large vocabulary continuous speech recognition", "author": ["A.L. Maas", "A.Y. Hannun", "C.T. Lengerich", "P. Qi", "D. Jurafsky", "A.Y. Ng"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "In ICML Workshop on Deep Learning for Audio, Speech, and Language Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In 27th International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "K. Vesel\u00fd", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer"], "venue": "ASRU,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["R. Raina", "A. Madhavan", "A. Ng"], "venue": "In 26th International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["S. Renals", "N. Morgan", "H. Bourlard", "M. Cohen", "H. Franco"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1994}, {"title": "Improvements to deep convolutional neural networks for LVCSR", "author": ["T. Sainath", "B. Kingsbury", "A. Mohamed", "G. Dahl", "G. Saon", "H. Soltau", "T. Beran", "A. Aravkin", "B. Ramabhadran"], "venue": "In ASRU,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "A. rahman Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "In ICASSP,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "A fast data collection and augmentation procedure for object recognition", "author": ["B. Sapp", "A. Saxena", "A.Y. Ng"], "venue": "In AAAI Twenty-Third Conference on Artificial Intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "In ASRU,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "On the importance of momentum and initialization in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In 30th International Conference on Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesely", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "In Interspeech,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}], "referenceMentions": [{"referenceID": 24, "context": "The introduction of deep learning algorithms [27, 30, 15, 18, 9] has improved speech system performance, usually by improving acoustic models.", "startOffset": 45, "endOffset": 64}, {"referenceID": 27, "context": "The introduction of deep learning algorithms [27, 30, 15, 18, 9] has improved speech system performance, usually by improving acoustic models.", "startOffset": 45, "endOffset": 64}, {"referenceID": 13, "context": "The introduction of deep learning algorithms [27, 30, 15, 18, 9] has improved speech system performance, usually by improving acoustic models.", "startOffset": 45, "endOffset": 64}, {"referenceID": 16, "context": "The introduction of deep learning algorithms [27, 30, 15, 18, 9] has improved speech system performance, usually by improving acoustic models.", "startOffset": 45, "endOffset": 64}, {"referenceID": 7, "context": "The introduction of deep learning algorithms [27, 30, 15, 18, 9] has improved speech system performance, usually by improving acoustic models.", "startOffset": 45, "endOffset": 64}, {"referenceID": 11, "context": "This problem has been addressed by Graves, Fern\u00e1ndez, Gomez and Schmidhuber [13], thus enabling neural networks to easily consume unaligned, transcribed audio during training.", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "[7], demonstrating the speed advantages of multi-GPU computation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[27] who applied early unsupervised feature learning techniques to replace hand-built speech features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Using a combination of collected and synthesized data, our system learns robustness to realistic noise and speaker variation (including Lombard Effect [20]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 35, "context": "2 The fourth layer is a bi-directional recurrent layer [38].", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "Once we have computed a prediction for P(ct|x), we compute the CTC loss [13] L(\u0177, y) to measure the error in prediction.", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "We use Nesterov\u2019s Accelerated gradient method for training [40].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "Note that its structure is considerably simpler than related models from the literature [14]\u2014we have limited ourselves to a single recurrent layer (which is the hardest to parallelize) and we do not use Long-Short-Term-Memory (LSTM) circuits.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "During training we apply a dropout [19] rate between 5% - 10%.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "A commonly employed technique in computer vision during network evaluation is to randomly jitter inputs by translations or reflections, feed each jittered version through the network, and vote or average the results [23].", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We use the KenLM toolkit [17] to train the N-gram language models in our experiments.", "startOffset": 25, "endOffset": 29}, {"referenceID": 5, "context": "We use multi-GPU training [7, 23] to accelerate our experiments, but doing this effectively requires some additional work, as we explain.", "startOffset": 26, "endOffset": 33}, {"referenceID": 20, "context": "We use multi-GPU training [7, 23] to accelerate our experiments, but doing this effectively requires some additional work, as we explain.", "startOffset": 26, "endOffset": 33}, {"referenceID": 38, "context": "[41] to accelerate RNNs for text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "To scale further, we parallelize by partitioning the model (\u201cmodel parallelism\u201d [7, 10]).", "startOffset": 80, "endOffset": 87}, {"referenceID": 8, "context": "To scale further, we parallelize by partitioning the model (\u201cmodel parallelism\u201d [7, 10]).", "startOffset": 80, "endOffset": 87}, {"referenceID": 22, "context": "This is similar to a convolutional network [25] with a step-size of 2 in the first layer.", "startOffset": 43, "endOffset": 47}, {"referenceID": 1, "context": "The Wall Street Journal, Switchboard and Fisher [3] corpora are all published by the Linguistic Data Consortium.", "startOffset": 48, "endOffset": 51}, {"referenceID": 34, "context": "To expand our potential training data even further we use data synthesis, which has been successfully applied in other contexts to amplify the effective number of training samples [37, 26, 6].", "startOffset": 180, "endOffset": 191}, {"referenceID": 23, "context": "To expand our potential training data even further we use data synthesis, which has been successfully applied in other contexts to amplify the effective number of training samples [37, 26, 6].", "startOffset": 180, "endOffset": 191}, {"referenceID": 4, "context": "To expand our potential training data even further we use data synthesis, which has been successfully applied in other contexts to amplify the effective number of training samples [37, 26, 6].", "startOffset": 180, "endOffset": 191}, {"referenceID": 18, "context": "One challenging effect encountered by speech recognition systems in noisy environments is the \u201cLombard Effect\u201d [20]: speakers actively change the pitch or inflections of their voice to overcome noise around them.", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "We evaluate our system trained on only the 300 hour Switchboard conversational telephone speech dataset and trained on both Switchboard (SWB) and Fisher (FSH) [3], a 2000 hour corpus collected in a similar manner as Switchboard.", "startOffset": 159, "endOffset": 162}, {"referenceID": 39, "context": "Speaker adaptation is critical to the success of current ASR systems [43, 36], particularly when trained on 300 hour Switchboard.", "startOffset": 69, "endOffset": 77}, {"referenceID": 33, "context": "Speaker adaptation is critical to the success of current ASR systems [43, 36], particularly when trained on 300 hour Switchboard.", "startOffset": 69, "endOffset": 77}, {"referenceID": 39, "context": "(DNN-GMM sMBR) [43] uses a sequence based loss function on top of a DNN after using a typical hybrid DNN-HMM system to realign the training set.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "(DNN-HMM FSH) [28] achieves 19.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "That system was built using Kaldi [32], state-of-the-art open source speech recognition software.", "startOffset": 34, "endOffset": 38}, {"referenceID": 39, "context": "(GMM-HMM BMMI) [43] 18.", "startOffset": 15, "endOffset": 19}, {"referenceID": 39, "context": "(DNN-HMM sMBR) [43] 12.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "(DNN-HMM SWB) [28] 14.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "(DNN-HMM FSH) [28] 16.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "(CD-DNN) [39] 16.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "(DNN-HMM sMBR HF) [22] 13.", "startOffset": 18, "endOffset": 22}, {"referenceID": 33, "context": "(CNN-HMM) [36] 11.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "Neural network acoustic models and other connectionist approaches were first introduced to speech pipelines in the early 1990s [1, 34, 11].", "startOffset": 127, "endOffset": 138}, {"referenceID": 31, "context": "Neural network acoustic models and other connectionist approaches were first introduced to speech pipelines in the early 1990s [1, 34, 11].", "startOffset": 127, "endOffset": 138}, {"referenceID": 9, "context": "Neural network acoustic models and other connectionist approaches were first introduced to speech pipelines in the early 1990s [1, 34, 11].", "startOffset": 127, "endOffset": 138}, {"referenceID": 27, "context": "These systems, similar to DNN acoustic models [30, 18, 9], replace only one stage of the speech recognition pipeline.", "startOffset": 46, "endOffset": 57}, {"referenceID": 16, "context": "These systems, similar to DNN acoustic models [30, 18, 9], replace only one stage of the speech recognition pipeline.", "startOffset": 46, "endOffset": 57}, {"referenceID": 7, "context": "These systems, similar to DNN acoustic models [30, 18, 9], replace only one stage of the speech recognition pipeline.", "startOffset": 46, "endOffset": 57}, {"referenceID": 11, "context": "[13] have previously introduced the \u201cConnectionist Temporal Classification\u201d (CTC) loss function for scoring transcriptions produced by RNNs and, with LSTM networks, have previously applied this approach to speech [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] have previously introduced the \u201cConnectionist Temporal Classification\u201d (CTC) loss function for scoring transcriptions produced by RNNs and, with LSTM networks, have previously applied this approach to speech [14].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "We similarly adopt the CTC loss for part of our training procedure but use much simpler recurrent networks with rectified-linear activations [12, 29, 31].", "startOffset": 141, "endOffset": 153}, {"referenceID": 26, "context": "We similarly adopt the CTC loss for part of our training procedure but use much simpler recurrent networks with rectified-linear activations [12, 29, 31].", "startOffset": 141, "endOffset": 153}, {"referenceID": 28, "context": "We similarly adopt the CTC loss for part of our training procedure but use much simpler recurrent networks with rectified-linear activations [12, 29, 31].", "startOffset": 141, "endOffset": 153}, {"referenceID": 14, "context": "[16], but with multiple changes to enhance its scalability.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The value of scalability in deep learning is well-studied [8, 24] and the use of parallel processors (including GPUs) has been instrumental to recent large-scale DL results [42, 24].", "startOffset": 58, "endOffset": 65}, {"referenceID": 21, "context": "The value of scalability in deep learning is well-studied [8, 24] and the use of parallel processors (including GPUs) has been instrumental to recent large-scale DL results [42, 24].", "startOffset": 58, "endOffset": 65}, {"referenceID": 21, "context": "The value of scalability in deep learning is well-studied [8, 24] and the use of parallel processors (including GPUs) has been instrumental to recent large-scale DL results [42, 24].", "startOffset": 173, "endOffset": 181}, {"referenceID": 30, "context": "Early ports of DL algorithms to GPUs revealed significant speed gains [33].", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "Researchers have also begun choosing designs that map well to GPU hardware to gain even more efficiency, including convolutional [23, 4, 35] and locally connected [7, 5] networks, especially when optimized libraries like cuDNN [2] and BLAS are available.", "startOffset": 129, "endOffset": 140}, {"referenceID": 2, "context": "Researchers have also begun choosing designs that map well to GPU hardware to gain even more efficiency, including convolutional [23, 4, 35] and locally connected [7, 5] networks, especially when optimized libraries like cuDNN [2] and BLAS are available.", "startOffset": 129, "endOffset": 140}, {"referenceID": 32, "context": "Researchers have also begun choosing designs that map well to GPU hardware to gain even more efficiency, including convolutional [23, 4, 35] and locally connected [7, 5] networks, especially when optimized libraries like cuDNN [2] and BLAS are available.", "startOffset": 129, "endOffset": 140}, {"referenceID": 5, "context": "Researchers have also begun choosing designs that map well to GPU hardware to gain even more efficiency, including convolutional [23, 4, 35] and locally connected [7, 5] networks, especially when optimized libraries like cuDNN [2] and BLAS are available.", "startOffset": 163, "endOffset": 169}, {"referenceID": 3, "context": "Researchers have also begun choosing designs that map well to GPU hardware to gain even more efficiency, including convolutional [23, 4, 35] and locally connected [7, 5] networks, especially when optimized libraries like cuDNN [2] and BLAS are available.", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "Indeed, using high-performance computing infrastructure, it is possible today to train neural networks with more than 10 billion connections [7] using clusters of GPUs.", "startOffset": 141, "endOffset": 144}, {"referenceID": 20, "context": "In other fields, such as computer vision, large labeled training sets have enabled significant leaps in performance as they are used to feed larger and larger DL systems [42, 23].", "startOffset": 170, "endOffset": 178}, {"referenceID": 1, "context": "Larger benchmark datasets, such as the Fisher corpus [3] with 2000 hours of transcribed speech, are rare and only recently being studied.", "startOffset": 53, "endOffset": 56}, {"referenceID": 34, "context": "This approach is well known in computer vision [37, 26, 6] but we have found this especially convenient and effective for speech when done properly.", "startOffset": 47, "endOffset": 58}, {"referenceID": 23, "context": "This approach is well known in computer vision [37, 26, 6] but we have found this especially convenient and effective for speech when done properly.", "startOffset": 47, "endOffset": 58}, {"referenceID": 4, "context": "This approach is well known in computer vision [37, 26, 6] but we have found this especially convenient and effective for speech when done properly.", "startOffset": 47, "endOffset": 58}], "year": 2014, "abstractText": "We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \u201cphoneme.\u201d Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called DeepSpeech, outperforms previously published results on the widely studied Switchboard Hub5\u201900, achieving 16.5% error on the full test set. DeepSpeech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "creator": "LaTeX with hyperref package"}}}