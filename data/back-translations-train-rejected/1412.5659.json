{"id": "1412.5659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Effective sampling for large-scale automated writing evaluation systems", "abstract": "Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students. It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts. Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them. This requirement limits large-scale adoption of AWE since human-scoring essays is costly. Here we evaluate algorithms for ensuring that AWE models are consistently trained using the most informative essays. Our results show how to minimize training set sizes while maximizing predictive performance, thereby reducing cost without unduly sacrificing accuracy. We conclude with a discussion of how to integrate this approach into large-scale AWE systems.", "histories": [["v1", "Wed, 17 Dec 2014 22:41:14 GMT  (390kb,D)", "http://arxiv.org/abs/1412.5659v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nicholas dronen", "peter w foltz", "kyle habermehl"], "accepted": false, "id": "1412.5659"}, "pdf": {"name": "1412.5659.pdf", "metadata": {"source": "CRF", "title": "Effective sampling for large-scale automated writing evaluation systems", "authors": ["Nicholas Dronen", "Peter W. Foltz\u20202andKyle"], "emails": ["dronen@colorado.edu", "peter.foltz@pearson.com", "kyle.habermehl@pearson.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "2 Data sets", "text": "An AWE dataset consists of a certain number of written answers and a set of results for each answer. In our experiments, we use track data from the Automated Student Assessment Prize (ASAP) Automated Essay Scoring (AES) competition sponsored by the Hewlett Foundation in 2012 [Kaga]. It contains training and test sets numbered 1 to 8. Key elements of the rubric of each sentence are summarized in Table 1. In sentences 1 and 2, a student is asked to write convincingly. Essay Set 1, the student has a position on the impact of computers on society and to write a letter to the editor of a newspaper arguing for that position. Sentences 3-6 are responses to source text input; they let the student read and analyze some source code and provide evidence from the text to support their analysis. The source of the essay Set 5 is a passage about the son of Cuban immigrants growing up in New Jersey. The student must describe the mood of the text."}, {"heading": "3 Regression modeling", "text": "The sequence inherent in the target variables implies that the scoring data of the essay can best be modelled using a regression algorithm. Here, we describe the approach we use in regression modelling in our experiments. If the length of the feature vectors in a design matrix exceeds the number of feature vectors, the ordinary solution of the smallest squares is badly placed. In this essay, we allow m for the number of feature vectors to be as small as ten. Since p is the number of features twenty-eight, the ordinary solution of the smallest squares would be suboptimal for some of the models we form. Therefore, we use regulated regression instead of the ordinary smallest squares we arise. The regularized linear regression solution for \u03b2 is the solution for ordinary smallest squares combined with a streamlining function."}, {"heading": "4 Optimal design algorithms", "text": "This year, it is so that it is able to retaliate, to retaliate."}, {"heading": "4.2 Kennard-Stone algorithm", "text": "The initialization step of the Kennard Stone algorithm [KS69] is similar to D optimality in that it selects two attribute vectors on the periphery of the attribute space, but then selects attribute vectors to distribute them evenly. Specifically, let d (u, v) set the distance between u and v. The first step of the Kennard Stone algorithm is to select the indices i, j like thatarg max i, j... n d (xi, xj), then initialize with xi, xj and set IB = {i, j}. The first step of the Kennard Stone algorithm is to select the indices i, j like thatarg max i, j... n d (xi, xj), then select the indices with xi, xj and set IB = {i, j}. The second step of the index j is added to IB and xj is added to the data: arg, xj max, and Ij (set)."}, {"heading": "4.4 Persistence of selections", "text": "An important question is the degree of persistence in which the characteristic vectors selected by these algorithms persist beyond sequential values of m. If the design of m \u2212 1 is always contained in ig at m, its selections remain perfectly intact. The degree of persistence can be measured as persistence = | \u0441m \u0441m + 1 | | m |. (2) The persistence of selections to increase values of m is shown in Figure 3. KennardStone starts with the same two maximally distant characteristic vectors and adds others sequentially using a deterministic process so that its selections are completely persistent. The Fedorov algorithm runs several times with different starting points so as not to get stuck in the local optima, resulting in a certain loss of persistence that seems to improve the abyss. However, in the 150 < m < 175 range, the persistence of the Fedorov algorithm is ratic, indicating the existence of several unlocalized optics."}, {"heading": "5 Evaluation", "text": "To evaluate the performance of the algorithms, it would be ideal to have access to an effectively infinite number of unrated sketches, as well as to an oracle that could provide results on request. Since we have finite data, we simulate a much larger amount of data by repeatedly scanning half of the entire training set (without replacement) before selecting feature vectors that are rated by people. As we already have the scores, the human scoring step of our assessment does not actually take place. However, the algorithms do not consider the scores, so no information unfairly trickles into the process. We let the desired training set sketches sizes from 10 to 100 in steps of 10. This allows us to simulate an operational environment in which new essays arrive over time. Once we have determined m and scanned half of the entire training set, we use each algorithm to select a subset of m feature vectors to sketch sketches, and then train the sketching model to do the corresponding work with each sketch as we say."}, {"heading": "6 Results", "text": "Table 3 shows the percentage change in the mean of the models of each algorithm above the baseline. To test the significance, we transform the correlations using the Fisher Ztransform before applying the two-sample t-test at the level 0.001. The test is an approximation, since the assumption of normality of the t-test does not apply, but the correlation coefficients are unimodal and the sample sizes are relatively large, so we assume that the approximation is quite good. Interpretability is calculated by the percentage change based on the Pearson correlation. Fedorov and Kennard-Stone mean results are significantly different from the baseline for most ASAP-AES essay sets and training set sizes Fedorov and Kennard-Stone, except for the anomalous essay set 4. The k-mean results are often indistinguishable from the baseline of the Kennov-Stone algorithm and the stone-stone-type algorithm must be selected with approximately the stone-50 parameter algorithm results."}, {"heading": "7 Conclusion", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to achieve their goals."}], "references": [{"title": "2", "author": ["Yigal Attali", "Jill Burstein. Automated essay scoring with e-rater v"], "venue": "The Journal of Technology, Learning and Assessment, 4(3),", "citeRegEx": "AB06", "shortCiteRegEx": null, "year": 2006}, {"title": "Active learning for misspecied generalized linear models", "author": ["F R Bach"], "venue": "Adv. Neural Inf. Process. Syst.", "citeRegEx": "Bac07", "shortCiteRegEx": null, "year": 2007}, {"title": "Theory Of Optimal Experiments", "author": ["V V Fedorov"], "venue": "Probability and mathematical statistics. Elsevier Science", "citeRegEx": "Fed72", "shortCiteRegEx": null, "year": 1972}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["Arthur E Hoerl", "Robert W Kennard"], "venue": "Technometrics, 12(1):55\u201367, 1 February", "citeRegEx": "HK70", "shortCiteRegEx": null, "year": 1970}, {"title": "D-Optimality for regression designs: A review", "author": ["R C St John", "N R Draper"], "venue": "Technometrics, 17(1):15\u201323", "citeRegEx": "JD75", "shortCiteRegEx": null, "year": 1975}, {"title": "11(1):137\u2013148", "author": ["R W Kennard", "L A Stone. Computer aided design of experiments. Technometrics"], "venue": "1 February", "citeRegEx": "KS69", "shortCiteRegEx": null, "year": 1969}, {"title": "Assessment in Education: Principles", "author": ["Thomas K Landauer", "Darrell Laham", "Peter W Foltz. Automatic essay assessment"], "venue": "Policy and Practice, 10(3):295\u2013308,", "citeRegEx": "LLF03", "shortCiteRegEx": null, "year": 2003}, {"title": "4(4):590\u2013604", "author": ["D MacKay. Information-Based objective functions for active data selection. Neural Computation"], "venue": "July", "citeRegEx": "Mac92", "shortCiteRegEx": null, "year": 1992}, {"title": "The analysis of essays by computer", "author": ["EB Page", "DH Paulus"], "venue": "final report.", "citeRegEx": "PP68", "shortCiteRegEx": null, "year": 1968}, {"title": "R: A Language and Environment for Statistical Computing", "author": ["R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna, Austria", "citeRegEx": "R C14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and Assessment", "author": ["Lawrence M Rudner", "Veronica Garcia", "Catherine Welch. An evaluation of IntelliMetric essay scoring system. The Journal of Technology"], "venue": "4(4),", "citeRegEx": "RGW06", "shortCiteRegEx": null, "year": 2006}, {"title": "Active learning literature survey", "author": ["B Settles"], "venue": "University of Wisconsin, Madison", "citeRegEx": "Set10", "shortCiteRegEx": null, "year": 2010}, {"title": "12(1/2):1\u201385", "author": ["Kirstine Smith. On the standard deviations of adjusted", "interpolated values of an observed polynomial function", "its constants", "the guidance they give towards a proper choice of the distribution of observations. Biometrika"], "venue": "1 November", "citeRegEx": "Smi18", "shortCiteRegEx": null, "year": 1918}, {"title": "An introduction to the prospectr package, 2013. R package version 0.1.3", "author": ["Antoine Stevens", "Leornardo Ramirez-Lopez"], "venue": null, "citeRegEx": "Stevens and Ramirez.Lopez.,? \\Q2013\\E", "shortCiteRegEx": "Stevens and Ramirez.Lopez.", "year": 2013}, {"title": "Active learning for misspecified models", "author": ["M Sugiyama"], "venue": "Adv. Neural Inf. Process. Syst.", "citeRegEx": "Sug05", "shortCiteRegEx": null, "year": 2005}, {"title": "AlgDesign: Algorithmic Experimental Design, 2014. R package version 1.1-7.2", "author": ["Bob Wheeler"], "venue": null, "citeRegEx": "Wheeler.,? \\Q2014\\E", "shortCiteRegEx": "Wheeler.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "The first model for automatically scoring essays was created on punch cards in the 1960s [PP68].", "startOffset": 89, "endOffset": 95}, {"referenceID": 2, "context": "The older is optimal experimental design [Fed72], a subfield of statistics that originated with the work of Kirstine Smith in the early 20th century [Smi18].", "startOffset": 41, "endOffset": 48}, {"referenceID": 12, "context": "The older is optimal experimental design [Fed72], a subfield of statistics that originated with the work of Kirstine Smith in the early 20th century [Smi18].", "startOffset": 149, "endOffset": 156}, {"referenceID": 11, "context": "In machine learning, active learning occupies much the same space as optimal design, although the literature tends to focus more on classification than regression [Set10].", "startOffset": 163, "endOffset": 170}, {"referenceID": 2, "context": "Here we use parametric linear regression and we employ an algorithm from the optimal experimental design literature [Fed72] that is derived from the same foundations as linear regression.", "startOffset": 116, "endOffset": 123}, {"referenceID": 3, "context": "We use ridge regression [HK70], which imposes a penalty on the L2 norm of \u03b2, as in: J(\u03b2;\u03bb) = \u03bb \u2211", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "The R statistical language [R C14] has implementations of the algorithms we discuss in this section.", "startOffset": 27, "endOffset": 34}, {"referenceID": 2, "context": "1 Fedorov exchange algorithm (with D-optimality) The Fedorov exchange algorithm [Fed72] is a greedy stepwise algorithm for finding \u03be.", "startOffset": 80, "endOffset": 87}, {"referenceID": 4, "context": "Its purpose is to optimize various optimal design criteria \u2013 D-optimality, A-optimality, I-optimality, or others [JD75].", "startOffset": 113, "endOffset": 119}, {"referenceID": 5, "context": "2 Kennard-Stone algorithm The initialization step of the Kennard-Stone algorithm [KS69] is similar to D-optimality in that it chooses two feature vectors at the periphery of the feature space.", "startOffset": 81, "endOffset": 87}], "year": 2014, "abstractText": "Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students. It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts. Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them. This requirement limits large-scale adoption of AWE since human-scoring essays is costly. Here we evaluate algorithms for ensuring that AWE models are consistently trained using the most informative essays. Our results show how to minimize training set sizes while maximizing predictive performance, thereby reducing cost without unduly sacrificing accuracy. We conclude with a discussion of how to integrate this approach into large-scale AWE systems.", "creator": "LaTeX with hyperref package"}}}