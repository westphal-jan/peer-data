{"id": "1412.5721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "An Algorithm for Online K-Means Clustering", "abstract": "This paper shows that one can be competitive with with the k-means objective while operating online. In this model, the algorithm receives vectors v1,...,vn one by one in arbitrary order. For each vector vi the algorithm outputs a cluster identifier before receiving vi+1. Our online algorithm generates ~O(k) clusters whose k-means cost is ~O(W*) where W* is the optimal k-means cost using k clusters and ~O suppresses poly logarithmic factors. We also show that, experimentally, it is not much worse than k-means++ while operating in a strictly more constrained computational model.", "histories": [["v1", "Thu, 18 Dec 2014 05:09:32 GMT  (216kb,D)", "https://arxiv.org/abs/1412.5721v1", null], ["v2", "Mon, 23 Feb 2015 17:30:23 GMT  (220kb,D)", "http://arxiv.org/abs/1412.5721v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["edo liberty", "ram sriharsha", "maxim sviridenko"], "accepted": false, "id": "1412.5721"}, "pdf": {"name": "1412.5721.pdf", "metadata": {"source": "CRF", "title": "An Algorithm for Online K-Means Clustering", "authors": ["Edo Liberty", "Ram Sriharsha", "Maxim Sviridenko"], "emails": ["edo@yahoo-inc.com,", "harshars@yahoo-inc.com,", "sviri@yahoo-inc.com,Yahoo"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that it is a matter of a way in which people put themselves at the centre of attention. (...) In fact, it is the case that people are able to put themselves at the centre of attention. (...) It is the case that they put themselves at the centre of attention. (...) It is the case that they put themselves at the centre of attention. (...) It is as if they put themselves at the centre of attention. (...) It is as if they were able to put themselves at the centre. (...) It is as if they put themselves at the centre. (...). \"(...).\" (...). \"(...). (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre.\" (...). \"(...).\" (...). \""}, {"heading": "1.1 Motivation", "text": "In the context of machine learning, the results of k-means have proven to be powerful, unattended features [11] that are sometimes comparable to neural networks. This is often referred to as (unattended) trait learning. If clustering captures most of the variability in the data, the mapping of a single label to an entire cluster should be fairly accurate. Therefore, it is not surprising that cluster labels are powerful features for classification. In the case of machine online learning, these cluster labels must also be mapped online. The importance of such an online k middle model has already been recognized in the machine learning community [10, 12]. In order to retrieve information, [9] examined the incremental k-center problem. They argue that cluster algorithms often have to be online in practice. We observe the same thing at Yahoo. For example, when we suggest news stories to users, we want to avoid close variants of those who are already reading a particular story, or vice versa that we want to follow to a particular scenario."}, {"heading": "1.2 Prior Art", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "4 Experimental Analysis of the Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Practical modifications to the algorithm", "text": "While experimenting with the algorithm, we discovered that some log factors were actually too pessimistic in practice. The value of ktarget is the number of clusters we want to output, while kactual is the actual number of clusters created. Internally, the algorithm works with a value of k = d (ktarget \u2212 15) / 5e. This is a heuristic (fully adhoc) conversion that compensates for the kactual being larger than k. algorithm 3 Online k mean algorithm input: V, ktarget k = d (ktarget \u2212 15) / 5e C: the first k + 10 vectors in V (for each of these, kactual is the center) w = half of the sum of the smallest quadristic distances of points in qr (ktarget \u2212 15) / 5e C: the first k + 10 vectors in c (for each of these results in itself being its center) w = 1 quadristic point in the probability of the deviation of adrical = 1."}, {"heading": "4.2 Datasets", "text": "In order to evaluate our algorithm, we performed it on 12 different datasets. All the data sets we used are conveniently summarized on the LibSvm website [14] and on the UCI dataset [7]. Some basic information about each dataset can be found in Table 1. Feature engineering for online learning is one of the motivations for this work. Therefore, we apply standard stochastic linear learning with the square loss of these data, once with the raw characteristics and once with the added k-mean characteristics. In some cases, we see a small decrease in accuracy due to a slower convergence of learning to a larger set of characteristics. Theoretically, this effect should be reversed when more data is available. In other cases, however, we see a marked increase in classification accuracy. This is consistent with previous observations [11]."}, {"heading": "4.3 The number of online clusters", "text": "However, as we see in Figure 1, the number of resulting clusters is fairly predictable and controllable. Figure 1 indicates the ratio between the number of clusters that the algorithm outputs, kactual, and the specified target ktarget. The reported results are averages of 3 passes for each parameter setting. The observed standard deviation from kactual is typically in the range [0, 3] and has not exceeded 0.1 \u00b7 ktarget in any experiment. Figure 1 clearly shows that the ratio kactual / ktarget is approximately constant and close to 1.0. Interestingly, the main distinguishing feature is the choice of the dataset."}, {"heading": "4.4 Online clustering cost", "text": "In Figure 2, the reader can see the online K mean cluster cost for the set of centers selected online by our algorithm for different values of Ktarget and different datasets. Note that some datasets are inherently non-clusterable. Even when using many cluster centers, the k mean target does not decrease significantly. Nevertheless, the k mean cost obtained by the online algorithm is similar to the theoretical K mean cost for a center at the origin. Note that some datasets are inherently non-clusterable, and the monotonicity of conline centers is not significantly lower than the K mean target achieved by the online algorithm."}, {"heading": "5 Aknowledgements", "text": "We would like to thank Anna Choromanska and Sergei Vassilvitskii for very helpful suggestions and Dean Foster for the help in proving the Lemma 1."}], "references": [{"title": "Streamkm++: A clustering algorithm for data streams", "author": ["Marcel R. Ackermann", "Marcus M\u00e4rtens", "Christoph Raupach", "Kamil Swierkot", "Christiane Lammersen", "Christian Sohler"], "venue": "Journal of Experimental Algorithmics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Adaptive sampling for k-means clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 12th International Workshop", "author": ["Ankit Aggarwal", "Amit Deshpande", "Ravi Kannan"], "venue": "APPROX 2009, and 13th International Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Streaming k-means approximation", "author": ["Nir Ailon", "Ragesh Jaiswal", "Claire Monteleoni"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "A simple and deterministic competitive algorithm for online facility location", "author": ["Aris Anagnostopoulos", "Russell Bent", "Eli Upfal", "Pascal Van Hentenryck"], "venue": "Inf. Comput.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM J. Comput.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Incremental clustering and dynamic information retrieval", "author": ["Moses Charikar", "Chandra Chekuri", "Tom\u00e1s Feder", "Rajeev Motwani"], "venue": "In Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Online clustering with experts", "author": ["Anna Choromanska", "Claire Monteleoni"], "venue": "In Proceedings of the 9  Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y. Ng", "Honglak Lee"], "venue": "JMLR Proceedings,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Topics in unsupervised learning", "author": ["Sanjoy Dasgupta"], "venue": "Class Notes CSE 291,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Facility location - applications and theory", "author": ["Zvi Drezner", "Horst W. Hamacher"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Libsvm data: Classification, regression, and multi-label", "author": ["Rong-En Fan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "On the competitive ratio for online facility", "author": ["Dimitris Fotakis"], "venue": "location. Algorithmica,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Online and incremental algorithms for facility location", "author": ["Dimitris Fotakis"], "venue": "SIGACT News,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Clustering data streams: Theory and practice", "author": ["Sudipto Guha", "Adam Meyerson", "Nina Mishra", "Rajeev Motwani", "Liadan O\u2019Callaghan"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "In Symposium on Computational Geometry,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Online EM for unsupervised models", "author": ["Percy Liang", "Dan Klein"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Least squares quantization in pcm", "author": ["Stuart P. Lloyd"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1982}, {"title": "Online facility location. In FOCS, pages 426\u2013431", "author": ["Adam Meyerson"], "venue": "IEEE Computer Society,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Fast and accurate k-means for large datasets", "author": ["Adam Meyerson", "Michael Shindler", "Alex Wong"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Approximation algorithms for facility location problems", "author": ["Jens Vygen"], "venue": "Lecture Notes, Technical Report No", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "In the context of machine learning, the results of k-means were shown to provide powerful unsupervised features [11] on par, sometimes, with neural nets for example.", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "The importance of such an online k-means model was already recognized in machine learning community [10, 12].", "startOffset": 100, "endOffset": 108}, {"referenceID": 8, "context": "The importance of such an online k-means model was already recognized in machine learning community [10, 12].", "startOffset": 100, "endOffset": 108}, {"referenceID": 5, "context": "For information retrieval, [9] investigated the incremental k-centers problem.", "startOffset": 27, "endOffset": 30}, {"referenceID": 16, "context": "In the offline setting where the set of all points is known in advance, Lloyd\u2019s algorithm [20] provides popular heuristics.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Yet, only recently some theoretical guaranties were proven for its performance on \u201cwell clusterable\u201d inputs [23].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": ", [6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "Recently, [2] improved the analysis of [5] and gave an adaptive sampling based algorithm with constant factor approximation to the optimal cost.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "The streaming model was considered by [3] and [22] and later by [1].", "startOffset": 38, "endOffset": 41}, {"referenceID": 18, "context": "The streaming model was considered by [3] and [22] and later by [1].", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "The streaming model was considered by [3] and [22] and later by [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "They build upon adaptive sampling ideas from [5, 8] and branch-and-bound techniques from [17].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "The first (to our knowledge) result in online clustering dates back the k-centers result of [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "For k-means an Expectation Maximization (EM) approach was investigated by [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "In contrast, the result of [10] provides provable results for the online setting in the presence of base-k-means algorithm as experts.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "These problems are well-studied both from computational and theoretic viewpoints (a book [13] and a survey [24] provide the background on some of the aspects in this area).", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "These problems are well-studied both from computational and theoretic viewpoints (a book [13] and a survey [24] provide the background on some of the aspects in this area).", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "Meyerson [21] suggested a simple and elegant algorithm", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "Fotakis [15] suggested a primal-dual algorithm with better performance guarantee of O(log n/ log log n).", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "[4] considered a different set of algorithms based on hierarchical partitioning of the space and obtained similar competitive ratios.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "The survey [16] summarizes the results in this area.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "As a remark, [9] already considered connections between facility location problems and clustering.", "startOffset": 13, "endOffset": 16}, {"referenceID": 17, "context": "The algorithm uses ideas from the online facility location algorithm of Meyerson [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "All the datasets that we used are conveniently aggregated on the LibSvm website [14] and on the UCI dataset collection [7].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "Much more information is provided on LibSvm website [14] and in the UCI dataset collection [7].", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "ment with prior observations [11].", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "The observed standard deviation of kactual is typically in the range [0, 3] and never exceeded 0.", "startOffset": 69, "endOffset": 75}, {"referenceID": 7, "context": "Table 2: Corroborating the observations of [11] we report that adding k-means feature, particularly to low dimensional datasets, is very beneficial for improving classification accuracy.", "startOffset": 43, "endOffset": 47}], "year": 2015, "abstractText": "This paper shows that one can be competitive with the kmeans objective while operating online. In this model, the algorithm receives vectors v1, . . . , vn one by one in an arbitrary order. For each vector vt the algorithm outputs a cluster identifier before receiving vt+1. Our online algorithm generates \u00d5(k) clusters whose k-means cost is \u00d5(W \u2217) where W \u2217 is the optimal k-means cost using k clusters.1 We also show that, experimentally, it is not much worse than k-means++ while operating in a strictly more constrained computational model.", "creator": "LaTeX with hyperref package"}}}