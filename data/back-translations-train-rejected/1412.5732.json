{"id": "1412.5732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Dynamic Structure Embedded Online Multiple-Output Regression for Stream Data", "abstract": "Online multiple-output regression is an important machine learning technique for modeling, predicting, and compressing multi-dimensional correlated data streams. In this paper, we propose a novel online multiple-output regression method, called MORES, for streaming data. MORES can \\emph{dynamically} learn the structure of the regression coefficients to facilitate the model's continuous refinement. We observe that limited expressive ability of the regression model, especially in the preliminary stage of online update, often leads to the variables in the residual errors being dependent. In light of this point, MORES intends to \\emph{dynamically} learn and leverage the structure of the residual errors to improve the prediction accuracy. Moreover, we define three statistical variables to \\emph{exactly} represent all the seen samples for \\emph{incrementally} calculating prediction loss in each online update round, which can avoid loading all the training data into memory for updating model, and also effectively prevent drastic fluctuation of the model in the presence of noise. Furthermore, we introduce a forgetting factor to set different weights on samples so as to track the data streams' evolving characteristics quickly from the latest samples. Experiments on three real-world datasets validate the effectiveness and efficiency of the proposed method.", "histories": [["v1", "Thu, 18 Dec 2014 06:37:50 GMT  (437kb,D)", "http://arxiv.org/abs/1412.5732v1", null], ["v2", "Tue, 8 Sep 2015 03:00:55 GMT  (1878kb,D)", "http://arxiv.org/abs/1412.5732v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "fan wei", "weishan dong", "qingshan liu", "xiangfeng wang", "xin zhang"], "accepted": false, "id": "1412.5732"}, "pdf": {"name": "1412.5732.pdf", "metadata": {"source": "CRF", "title": "MORES: Online Incremental Multiple-Output Regression for Data Streams", "authors": ["Changsheng Li", "Weishan Dong", "Qingshan Liu", "Xin Zhang"], "emails": ["zxin}@cn.ibm.com", "qsliu@nuist.edu.cn"], "sections": [{"heading": null, "text": "The question of the way in which data is collected in individual countries is not only a question of expression, but also of the way in which it is used in the context of data flows, although many research questions such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [10], the selection of online features [12], multi-task learning [14], the recognition of indicators of change [16], etc., have been comprehensively investigated in the last ten years, little attention has been paid to separating multiple indicators."}, {"heading": "II. ONLINE MULTIPLE-OUTPUT REGRESSION FOR STREAMING DATA", "text": "Following the general discontinuation of online learning [26], we assume that the learner first observes an instance of xt-Rd on the t-th round and simultaneously observes several outputs y-t-Rm on the basis of the current model Pt-1-Rm-d. Afterwards, the learner receives the true answers yt-Rm for this instance. Finally, the learner updates the current model Pt-1 on the basis of the new data point (xt, yt). Our goal in this work is to update Pt-1 online so that the updated Pt-1 can predict the outputs for the incoming instance xt + 1 as accurately as possible. The prediction can be expressed as follows: yt + 1 = Ptxt + 1 + t + 1, (1) where Pt = [pt, 1,... pt, m] T denotes the learned regression coefficient on the t-th round and pt, i the regression coefficient vector of the putt + 1.1 consists of errors."}, {"heading": "A. Objective Function", "text": "To solve the optimization problem, we must first use a simple formulation such as: Pt = arg min P \u2212 Pt \u2212 Pt \u2212 Pt \u2212 Pt \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pr \u2212 Pt).Pt The core idea of the objective function (2) is as follows: On the one hand, it intends to minimize the distance between Pt and Pt \u2212 Pt \u2212 1 in order to make the Pt \u2212 Pt \u2212 1 as much as possible that can retain the information. On the other hand, it requires Pt to meet the condition: The total prediction error on the current data base (xt, yt) is less than or equal to the previous level."}, {"heading": "B. Optimization Procedure", "text": "The objective function (3) is non-convex in relation to all variables, but it is convex in relation to each variable when others are fixed. (3) We assume an alternative optimization strategy by which (3) local minidimensions can be found. (3) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -3 (P) -2 (P) -2 (P) -2 (P) -2 (P) -2 (P) -3 (P) -3 (P) -3 (P) -3 (3) -3 (P) -2 (P) -2 (P) (4) (P) (4) (P) (4) (4) (P) (4) (P) (4) (P) (4) (P) (4) (P) (4) (P) (4) (P) (4) (P (4) (P) (4) (P (4) (P) (3 (4) (P (4) P (4) (P (4) (P (4) (P (4) P (3 (4) P (3 (4) P (3 (3 (P) P (3 (4) P (3 (4) P (3 (4) P (3 (4) P (3 (4) P (4) P (4 (4 (P) P (4) P (4 (4 (4) P (4 (4) P (4 (P) P (4 (4 (4) P) P (4 (4 (4) P) P (4 (4 (4) P) P (4 (4) P (4 (4 (4) P) P (3 (3 (4) P (3 (4) P) P (3 (3 (3 (4) P (3 (4) P (4) P) P (3 (3 (3 (3 (4) P) P (3 (3 ("}, {"heading": "C. Time Complexity Analysis", "text": "In algorithm 1, the most time-consuming part of MORES is to update Pt and ignore the time costs of other parts. Here, we focus on analyzing the complexity of the case in which t is greater or equal to Tmin. To update Pt, we use two types of divergence metrics in this paper (quantum relative entropy and LogDet divergence). If we use quantum relative entropy as divergence metrics, updating MORES does not include spectral decomposition, the complexity of which is identical to self-decomposition, typically O (m3) in practice. Therefore, updating MORES O (m3 + dm2) requires O (m3 + dm2)."}, {"heading": "III. EXPERIMENTS", "text": "To evaluate the performance of MORES, we perform the experiments on three real datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34]."}, {"heading": "A. Experimental Setups", "text": "We compare our method with iS-PLS [19], which is most relevant for our work. We also compare with two variants of the PA algorithm in [26] called PA-I and PA-II, which are two classic online learning approaches for individual regression tasks. In the experiment, we use PA to train a regression model for each output. We call our simple formulation of online multiple output regression, which is proposed at the beginning of Section 2, SOMOR. In addition, there are some pre-defined parameters. In order to reduce the cost of setting parameters, we tune the parameters \u03b1, \u03b2, and \u03b7 in our algorithms with the help of LogDet as MORES-LD.There are some parameters that we need to specify in advance."}, {"heading": "B. Robot Inverse Dynamics", "text": "We will first investigate the problem of online learning the inverse dynamics of a 7 degree freedom of the robot arms on the Barrett WAM dataset. This dataset consists of a total of 16,200 samples, each sample being represented by 21 input characteristics, which in fact correspond to seven common positions, seven common velocities, and seven common accelerations. Seven common torques for the seven degrees of freedom (DOF) are used as a starting point. We will summarize the results of different methods in Table I. For each output, MORES-QE and MORES-LD achieve better prediction results than all other methods, and they both now achieve relative error reduction in terms of average MAMOR performance. The performance of MORES-QE is comparable to that of MORES-LD. It indicates that quantum relativity has relative divergence metrics similar effects on prediction results with the LogDet divergence metric."}, {"heading": "C. Stock Price Prediction", "text": "According to previous studies in [36] and [20], we also apply our algorithms to the share data of companies to forecast prices. We select the daily share price data of five companies, including IBM, Yahoo, Microsoft, Apple and Oracle, from 2010 to 2013. The learned model can predict stock prices in the future by using stock prices in the past as input factors. We use the autoregressive 1, aka AR (1) model yt + 1 = Ptyt + t, where the real share prices of the five companies at one time represent t + 1, and Pt denotes the regression coefficients learned at one time. Experimental results are shown in Table III. MORESLD and MORES-QE perform better than the other methods."}, {"heading": "D. Weather Forecast", "text": "In addition, we evaluate our algorithms on the weather forecast data set [34]. This data set consists of wind speed, wind direction, air pressure, water depth, maximum gust, maximum wave height, air temperature, water temperature and average wave height, collected every five minutes by a sensor network on the south coast of England. In the experiments, data of 143,034 samples are used for a year and a half. The first five variables are used as prediction variables, and the rest are the reaction variables. Experimental results are reported in Table IV. MORESLD and MORES-QE. The experimental setting is as follows: The model is updated when the results of our algorithms are cumulative N (= 1,)."}, {"heading": "E. Sensitivity Analysis", "text": "We also examine the sensitivity of the parameters \u03b1, \u03b2 and \u03b7 in our algorithm on the larger dataset, the weather dataset. As shown in Fig. 3, our method with the fixed \u03b7 is not sensitive to \u03b1 and \u03b2 with wide ranges. Concerning the parameter \u03b7, the performance when \u03b1 and \u03b2 are fixed gradually improves with increasing p-value."}, {"heading": "F. Efficiency", "text": "Experiments are performed on a desktop with Inter (R) Core (TM) i7-3770 CPU, and MORES is implemented with the MATLAB R2012b 64bit Edition without parallel operation. Update speeds of our algorithms are in Table VI. On the weather dataset, MORES-QE achieves 3566 updates per second. And on the Inverse Dynamics dataset with the largest dimensions, our algorithms perform more than 1000 updates per second. If we apply some parallel implementations or use a more efficient programming language, the update speed of MORES can be further improved."}, {"heading": "IV. RELATED WORK", "text": "In this section we review the related work from two aspects: Online Single Output Regression and Batch Multiplex Output Regression.8 0.11 10 100 1000 0.001 0.01 0.1 10 0 0.1 0.3 0.4 M A E (a) MORES-LD (\u03b7 = 0.1) 0.11 1001000 0.01 0.11 100 0.1 0.2 0.3 0.4 M A E (b) MORES-QE (\u03b7 = 0.1) 0.010.1 1 1 1 1 10100 0.01 0.1 0.3 M E (c) MORES-LD (\u03b2 = 10) 0.010.1 1 10100 0.4 M A E (e)."}, {"heading": "V. CONCLUSIONS", "text": "The proposed method can simultaneously and dynamically learn the structures of both regression coefficients and residual errors and use the learned structural information to continuously update the model. Meanwhile, we have gradually accumulated the prediction error for all the samples seen without loss of information and introduced a forgetfulness factor to weight the samples so that they fit into the evolution of the data streams.The experiments were carried out on three real data sets and the experimental results showed the effectiveness and efficiency of the proposed method."}], "references": [{"title": "Data Mining: Concepts and Techniques", "author": ["J. Han", "M. Kamber"], "venue": "Second Edition: Diane Cerra,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Mining high-speed data streams", "author": ["P. Domingos", "G. Hulten"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2000, pp. 71\u201380.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "A framework for on-demand classification of evolving data streams", "author": ["C.C. Aggarwal", "J. Han", "J. Wang", "P.S. Yu"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 18, no. 5, pp. 577\u2013589, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Learn ++.NC: Combining ensemble of classifiers with dynamically weighted consult-and-vote for efficient incremental learning of new classes", "author": ["M.D. Muhlbaier", "A. Topalis", "R. Polikar"], "venue": "IEEE Trans. on Neural Networks, vol. 20, no. 1, pp. 152\u2013168, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for clustering evolving data streams", "author": ["C.C. Aggarwal", "J. Han", "J. Wang", "P.S. Yu"], "venue": "International Conference on Very Large Data Bases (VLDB), 2003, pp. 81\u201392.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Density-based clustering for real-time stream data", "author": ["Y. Chen", "L. Tu"], "venue": "ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 2007, pp. 133\u2013142.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Svstream: A support vector-based algorithm for clustering data streams", "author": ["C.-D. Wang", "J.-H. Lai", "D. Huang", "W.-S. Zheng"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 25, no. 6, pp. 1410\u20131424, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning from data streams", "author": ["X. Zhu", "P. Zhang", "X. Lin", "Y. Shi"], "venue": "IEEE International Conference on Data Mining (ICDM), 2007, pp. 757\u2013762.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Unbiased online active learning in data streams", "author": ["W. Chu", "M. Zinkevich", "L. Li", "A. Thomas", "B. Tseng"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2011, pp. 195\u2013203.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Active learning with drifting streaming data.", "author": ["I. Zliobaite", "A. Bifet", "B. Pfahringer", "G. Holmes"], "venue": "IEEE Trans. on Neural Networks and Learning Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Online feature selection with streaming features", "author": ["X. Wu", "K. Yu", "W. Ding", "H. Wang", "X. Zhu"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, no. 5, pp. 1178\u20131192, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Online feature selection and its applications", "author": ["J. Wang", "P. Zhao", "S. Hoi", "R. Jin"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 3, pp. 698\u2013710, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Online learning of multiple tasks and their relationships", "author": ["A. Saha", "P. Rai", "H. Daum\u00e9 III", "S. Venkatasubramanian"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 643\u2013651.  9", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning of multiple tasks with a shared loss.", "author": ["O. Dekel", "P.M. Long", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "A martingale framework for detecting changes in data streams by testing exchangeability", "author": ["S.-S. Ho", "H. Wechsler"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 32, no. 12, pp. 2113\u2013 2127, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Change-point detection with feature selection in high-dimensional time-series data", "author": ["M. Yamada", "A. Kimura", "F. Naya", "H. Sawada"], "venue": "International joint conference on Artificial Intelligence (AAAI). AAAI Press, 2013, pp. 1827\u20131833.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A multivariate regression approach to association analysis of a quantitative trait network", "author": ["S. Kim", "K.-A. Sohn", "E.P. Xing"], "venue": "Bioinformatics, vol. 25, no. 12, pp. i204\u2013i212, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Svm multiregression for nonlinear channel estimation in multiple-input multiple-output systems", "author": ["M. S\u00e1nchez-Fern\u00e1ndez", "M. de Prado-Cumplido", "J. Arenas-Garc\u0131\u0301a", "F. P\u00e9rez-Cruz"], "venue": "IEEE Trans. on Signal Processing, vol. 52, no. 8, pp. 2298\u20132307, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse multivariate regression with covariance estimation", "author": ["A.J. Rothman", "E. Levina", "J. Zhu"], "venue": "Journal of Computational and Graphical Statistics, vol. 19, no. 4, pp. 947\u2013962, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint estimation of structured sparsity and output structure in multiple-output regression via inverse-covariance regularization", "author": ["K.-A. Sohn", "S. Kim"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS), 2012, pp. 1081\u20131089.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping", "author": ["S. Kim", "E.P. Xing"], "venue": "The Annals of Applied Statistics, vol. 6, no. 3, pp. 1095\u20131117, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernelized bayesian matrix factorization", "author": ["M. Gonen", "S. Kaski"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 36, no. 10, pp. 2047\u20132060, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Multivariate regression with calibration", "author": ["H. Liu", "L. Wang", "T. Zhao"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2014, pp. 127\u2013135.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Accurate on-line support vector regression", "author": ["J. Ma", "J. Theiler", "S. Perkins"], "venue": "Neural Computation, vol. 15, no. 11, pp. 2683\u20132703, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 551\u2013585, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to trade with incremental support vector regression experts", "author": ["G. Montana", "F. Parrella"], "venue": "Hybrid Artificial Intelligence Systems. Springer, 2008, pp. 591\u2013598.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse partial least squares regression for on-line variable selection with multivariate data streams", "author": ["B. McWilliams", "G. Montana"], "venue": "Statistical Analysis and Data Mining, vol. 3, no. 3, pp. 170\u2013193, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Information and Computation, vol. 132, no. 1, pp. 1\u201363, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "The Conference on Uncertainty in Artificial Intelligence (UAI), 2010, pp. 733\u2013742.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2005, pp. 1473\u20131480.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Matrix exponentiated gradient updates for on-line learning and bregman projection.", "author": ["K. Tsuda", "G. R\u00e4tsch", "M.K. Warmuth", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Model learning with local gaussian process regression", "author": ["D. Nguyen-Tuong", "M. Seeger", "J. Peters"], "venue": "no. 15, pp. 2015\u20132034, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse convolved gaussian processes for multi-output regression.", "author": ["M.A. Alvarez", "N.D. Lawrence"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Modeling slump flow of concrete using second-order regressions and artificial neural networks", "author": ["I. Yeh"], "venue": "Cement and Concrete Composites, vol. 29, no. 6, pp. 474\u2013480, 2007.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust sparse estimation of multiresponse regression and inverse covariance matrix via the l2 distance", "author": ["A.C. Lozano", "H. Jiang", "X. Deng"], "venue": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2013, pp. 293\u2013301.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Data streams arise in many scenarios, such as online transactions in the financial market, Internet traffic and so on [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 143, "endOffset": 146}, {"referenceID": 8, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 190, "endOffset": 194}, {"referenceID": 12, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 222, "endOffset": 226}, {"referenceID": 14, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 251, "endOffset": 255}, {"referenceID": 15, "context": "In the context of data streams, although many research issues, such as classification [2], [3], [4], clustering [5], [6], [7], active learning [8], [9], [10], online feature selection [11], [12], multi-task learning [13], [14], change point detection [15], [16], etc.", "startOffset": 257, "endOffset": 261}, {"referenceID": 16, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "Lots of batch multiple-output regression algorithms have been proposed [17], [18], [19], [20], [21], [22], [23], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "So far, many online regression algorithms for predicting single output variable have been proposed [25], [26], [27].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "The representative method is online passive-aggressive (PA) algorithm [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Since there are often correlations among outputs, mining the correlation relationships can improve the prediction accuracy of the model [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "Recently, McWilliams and Montana take advantage of partial least squares (PLS) to build a recursive regression model for online predicting multiple outputs, called iS-PLS [28].", "startOffset": 171, "endOffset": 175}, {"referenceID": 24, "context": "Following the general setting of online learning [26], we assume that the learner first observes an instance xt \u2208 R on the t-th round, and it simultaneously predicts multiple outputs \u0177t \u2208 R based on the current model Pt\u22121 \u2208 Rm\u00d7d.", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "Following [26], the optimization problem defined by (2) can be easily solved by the Lagrange multiplier method.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "\u2206\u03c6(\u03a9,\u03a9t\u22121) denotes the Bregman divergence [29] that measures the distance between the matrix \u03a9 and the matrix \u03a9t\u22121.", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "In (4), the term (P(i) \u2212 Pt\u22121(i))\u03a9(P(i) \u2212 Pt\u22121(i)) is actually the Mahalanobis distance between P(i) and Pt\u22121(i), where \u03a9 encodes the correlations between the variables of the i-th column of the regression coefficient matrix on round t [30].", "startOffset": 236, "endOffset": 240}, {"referenceID": 29, "context": "The term (yi \u2212 Pxi)\u0393(yi \u2212 Pxi) measures the Mahalnobis distance between the true value yi and the predicted value Pxi, which can remove the influence of the residual errors\u2019 correlations on distance calculation [31].", "startOffset": 211, "endOffset": 215}, {"referenceID": 30, "context": "In this paper, we employ two matrix divergence metrics, quantum relative entropy and LogDet divergence, because of their good properties stated in [32].", "startOffset": 147, "endOffset": 151}, {"referenceID": 30, "context": "According to [32], it is easily inferred that \u03a9 is positive definite.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "To evaluate the performance of MORES, we perform the experiments on three real-world datasets: the Barrett WAM dataset [33], the stock price dataset, and the weather dataset [34].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "We compare our method with iS-PLS [19] that is the most relevant work to ours.", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "We also compare with two variants of PA algorithm in [26] called PA-I and PA-II, which are two classical online learning approaches for single regression tasks.", "startOffset": 53, "endOffset": 57}, {"referenceID": 33, "context": ", 10, 10} on the Concrete Slump dataset [35], and choose the optimal parameters to directly apply to the above three datasets.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "Following previous studies in [36] and [20], we also apply our algorithms to the stock data of companies for price prediction.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "We also evaluate our algorithms on the weather dataset for weather forecast [34].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Online single-output regression: [25] presented an online version of support vector regression algorithm, called (AOSVR).", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "[26] proposed a margin based online regression algorithm, called passive-aggressive (PA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] proposed an incremental support vector regression algorithm, which evolved a pool of online SVR experts and learned to trade by dynamically weighting the experts\u2019 opinions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] presented MRCE, which jointly learned the output structure in the form of the noise covariance matrix and the regression", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Sohn and Kim [20] designed an algorithm to simultaneously estimate the regression coefficient vector for each output along with the covariance structure of the outputs with a shared sparsity assumption on the regression coefficient vectors.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "[22] proposed a tree-guided group lasso, or tree lasso, that directly combined statistical strength across multiple related outputs.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Online multiple-output regression is an important machine learning technique for modeling, predicting, and compressing multi-dimensional correlated data streams. In this paper, we propose a novel online multiple-output regression method, called MORES, for streaming data. MORES can dynamically learn the structure of the regression coefficients to facilitate the model\u2019s continuous refinement. We observe that limited expressive ability of the regression model, especially in the preliminary stage of online update, often leads to the variables in the residual errors being dependent. In light of this point, MORES intends to dynamically learn and leverage the structure of the residual errors to improve the prediction accuracy. Moreover, we define three statistical variables to exactly represent all the seen samples for incrementally calculating prediction loss in each online update round, which can avoid loading all the training data into memory for updating model, and also effectively prevent drastic fluctuation of the model in the presence of noise. Furthermore, we introduce a forgetting factor to set different weights on samples so as to track the data streams\u2019 evolving characteristics quickly from the latest samples. Experiments on three real-world datasets validate the effectiveness and efficiency of the proposed method.", "creator": "LaTeX with hyperref package"}}}