{"id": "1412.5949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Large Scale Distributed Distance Metric Learning", "abstract": "In large scale machine learning and data mining problems with high feature dimensionality, the Euclidean distance between data points can be uninformative, and Distance Metric Learning (DML) is often desired to learn a proper similarity measure (using side information such as example data pairs being similar or dissimilar). However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure. To our knowledge, this is the first distributed solution to DML, and we show that, on a system with 256 CPU cores, our program is able to complete a DML task on a dataset with 1 million data points, 22-thousand features, and 200 million labeled data pairs, in 15 hours; and the learned metric shows great effectiveness in properly measuring distances.", "histories": [["v1", "Thu, 18 Dec 2014 17:14:34 GMT  (638kb,D)", "http://arxiv.org/abs/1412.5949v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pengtao xie", "eric xing"], "accepted": false, "id": "1412.5949"}, "pdf": {"name": "1412.5949.pdf", "metadata": {"source": "CRF", "title": "LARGE SCALE DISTRIBUTED DISTANCE METRIC LEARNING", "authors": ["Pengtao Xie", "Eric Xing"], "emails": ["pengtaox@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is not as if this is a way in which a multitude of remote data collections and machine learning algorithms are required, such as data retrieval (2008), clustering by Xing et al. (2008), clustering by Xing et al. (2002), and the closest neighbor (kNN), whose classification Weinberger et al. (2005). One thing these algorithms have in common is that their accuracy depends crucially on a good distance between the points, especially when the dataset is highly dimensional.As first formulated in Xing et al. (2002), a mathematical problem (DML) that can be considered a semi-learning problem."}, {"heading": "2 RELATED WORKS", "text": "Most people who are able to explore and understand another world than those who are able to explore and understand another world will be able to explore another world. (2012) Most people who are able to explore and understand another world will be able to explore another world. (2012) Most people who are able to explore and understand another world will be able to explore another world. (2012) Most people who are able to explore and understand another world will be able to explore another world. (2012) Most people who are able to explore and explore another world will be able to explore another world. (2012) Most people who are able to explore and understand another world will be able to explore and understand another world. (2012) Most people who are able to explore and explore another world will be able to explore and explore another world. (2012) Most people who are able to explore and explore another world will be able to explore and explore another world. (2012) Most people who are able to explore and explore another world will be able to explore and explore another world will be able to explore another world. (2012) Most people who are able to explore and understand another world will be able to explore and understand another world. (2012) Most people who are able to explore and understand another world will be able to explore and understand another world will be able to explore."}, {"heading": "3 DISTANCE METRIC LEARNING", "text": "In the original formulations Xing et al. (2002); Globerson & Roweis (2005); Weinberger et al. (2005) of remote metric learning, a semi-defined programming problem (SDP) must be solved. SDP requires self-decomposition of the Mahalanobis distance matrix, which is very difficult, if not impossible, on high-dimensional data. In order to make DML scalable to large-scale problems, similar to Weinberger et al. (2005), we perform the reformulations in two ways: First, we factorize the Mahalanobis matrix M to M = LTL and perform optimization via L instead of M. Factorization ensures that M is still positively semi-defined, but avoids the self-decomposition of M. Second, we use flap variables to loosen the limitations vis-unequal pairs and use hinge loss to replace the constraints, but avoid the inherent decomposition of M. This makes the problem much easier to solve without the original limited distribution of the problem."}, {"heading": "3.1 REFORMULATION OF DISTANCE METRIC LEARNING", "text": "There are a number of pairs that are called similar S = {xi, yi)} and a number of pairs that are called dissimilar or dissimilar; 3, Objective: learn a distance metric to place similar points as close as possible and place dissimilar points as far as possible. (D) Given a number of pairs that are called similar S = {xi, yi), we can label a number of pairs that are dissimilar D = {xi, yi). (D | i = 1, DML = 1, learn the distance by optimizing the following problemminM (x, y)."}, {"heading": "4 DISTRIBUTED PARALLEL DISTANCE METRIC LEARNING", "text": "To solve this problem, we design a distributed framework that performs parallel DML. For optimization, we use an asynchronous stochastic gradient descent. We partition the similar pairs and unequal pairs on different machines. Each machine uses the data pairs it holds to update the model parameters in an asynchronous manner. Parameters on different machines are synchronized by a central parameter server. In terms of the form of distance monitoring, we focus in this work on pairs of constraints (e.g. i resembles j; j is not similar to k). Our framework can be easily expanded to support triple constraints Weinberger et al. (2005) (e.g. i resembles more j than k)."}, {"heading": "4.1 PARAMETER SYNCHRONIZATION", "text": "To learn L in a distributed environment with P-Worker machines, we divide the similarity pair S and the unequal pair D into P-pieces S1, S2, \u00b7 \u00b7, SP and D1, D2, \u00b7 \u00b7 \u00b7, DP and each machine holds one piece. Each machine p has a local copy Lp of global parameter L. And various parameter copies are synchronized across machines to ensure that they are as similar as possible. Similar to Ahmed et al. (2012); Dean & Ghemawat (2008); Ho et al. (2013); Lee et al. (2014), we use a central parameter server to synchronize these parameter copies. The central server keeps the global parameter L. Each worker communicates with the central server and the workers do not communicate with each other. At each iteration of the stochastic gradient descent algorithm, each worker p randomly sends a minibatch of data pairs from the similar pair and the dissimilar server with the P-4p server and local gradient pair."}, {"heading": "4.2 IMPLEMENTATION DETAILS", "text": "The central parameter server contains a central server and P-workers. Workers communicate with the central server and do not communicate with each other. The server manages the global parameter and each worker has a local copy of the parameter. On the server side there are two threads: 1) update thread; 2) communication thread. The server maintains two message queues to exchange messages with workers: 1) inbound message queue; 2) outbound message queue. The communication thread receives gradient updates from workers and inserts them into the incoming message queue. The update thread takes a stack of gradient updates from the incoming message queue and uses them to update the central parameter. Then the update thread inserts the updated central parameter into the outgoing message queue. The communication thread takes updated parameters from the outgoing message queue and sends them to all outgoing message quad messages. Then the update thread inserts the updated central parameter into the outgoing message queue. The communication thread takes updated parameters from the outgoing message queue and sends them to all outgoing message quad messages."}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 DATASETS", "text": "We use three data sets in our experiments. Table 1 summarizes the statistics of these data sets. Each image is of the size 32 x 32 and has a number label. The images are displayed with raw pixels, with a dimensionality of 780. We sample 100K \"similar\" pairs and 100K \"unequal\" pairs from the training images. If two images come from the same number, we label them as \"similar.\" If two images come from different numbers, we label them as \"unequal.\" The second data set we use is ImageNet-63K, in which 63K training images are randomly selected from ImageNet Deng et al. (2009). Each image is associated with a class label and the total number of classes is 1000. The images are used with Locality-constrained Linear Coding (LLC Deng et al.)."}, {"heading": "5.2 EXPERIMENTAL SETUP", "text": "In all experiments, the compromise parameter \u03bb is set to 1 and the threshold c to 1. For MNIST, we set k (the number of rows of L) to 600. For each iteration, we use a minibatch of 1000 pairs (500 similar pairs and 500 unequal pairs) to perform the update. For ImageNet-63K, we set k to 10000. The number of model parameters is about 220 million. For each iteration, we use a minibatch of 100 pairs (50 similar pairs and 50 unequal pairs) to perform the update. For ImageNet-1M, we set k to 1000 to make the calculation tractable. The minibatch size is 1000 (500 similar pairs and 500 unequal pairs). Experiments with MNIST are performed on machines, each of which has 16 CPUs and 64G main memory. Experiments with ImageNet-63K and ImageNet-1M are performed on machines, each of which has 64 CPUs and 12G main memory."}, {"heading": "5.3 SCALABILITY OF OUR FRAMEWORK", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able, that we are able to hide ourselves, and that we are able, that we will be able to achieve our goals, \"he said in an interview with the\" New York Times. \""}, {"heading": "5.4 QUALITY OF THE LEARNED DISTANCE METRIC", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we reformulate the original DML problem into an unrestricted optimization problem that can be paralleled, and use it as the basis for a distributed framework to support large-scale remote metric learning. Our framework uses asynchronous stochastic gradient lineage for distributed optimization, where computation is distributed among machines that use a centralized parameter server to synchronize parameters between them. Experiments on three sets of data show the efficiency and effectiveness of our framework on unprecedented problem scales with better accuracy than previous methods. We believe that our work is the first to address remote metric learning on such large scales in an intelligent, distributed manner."}], "references": [{"title": "Scalable inference in latent variable models", "author": ["J. der"], "venue": "In WSDM,", "citeRegEx": "der,? \\Q2012\\E", "shortCiteRegEx": "der", "year": 2012}, {"title": "Integrating constraints and metric learning in semi-supervised clustering", "author": ["Bilenko", "Mikhail", "Basu", "Sugato", "Mooney", "Raymond J"], "venue": "In ICML, pp", "citeRegEx": "Bilenko et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bilenko et al\\.", "year": 2004}, {"title": "Information-theoretic metric learning", "author": ["Davis", "Jason V", "Kulis", "Brian", "Jain", "Prateek", "Sra", "Suvrit", "Dhillon", "Inderjit S"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["Dean", "Jeffrey", "Ghemawat", "Sanjay"], "venue": "Communications of the ACM,", "citeRegEx": "Dean et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2008}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Metric learning by collapsing classes", "author": ["Globerson", "Amir", "Roweis", "Sam T"], "venue": "In NIPS, pp", "citeRegEx": "Globerson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2005}, {"title": "Is that you? metric learning approaches for face identification", "author": ["Guillaumin", "Matthieu", "Verbeek", "Jakob", "Schmid", "Cordelia"], "venue": "In ICCV,", "citeRegEx": "Guillaumin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guillaumin et al\\.", "year": 2009}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Ho", "Qirong", "Cipar", "James", "Cui", "Henggang", "Lee", "Seunghak", "Kim", "Jin Kyu", "Gibbons", "Phillip B", "Gibson", "Garth A", "Ganger", "Greg", "Xing", "Eric"], "venue": null, "citeRegEx": "Ho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2013}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["Hoi", "Steven CH", "Liu", "Wei", "Chang", "Shih-Fu"], "venue": "In CVPR,", "citeRegEx": "Hoi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2008}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M Kostinger", "Hirzer", "Martin", "Wohlhart", "Paul", "Roth", "Peter M", "Bischof", "Horst"], "venue": "In CVPR,", "citeRegEx": "Kostinger et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kostinger et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Primitives for dynamic big model parallelism", "author": ["Lee", "Seunghak", "Kim", "Jin Kyu", "Zheng", "Xun", "Ho", "Qirong", "Gibson", "Garth A", "Xing", "Eric P"], "venue": "arXiv preprint arXiv:1406.4580,", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["Li", "Mu", "Andersen", "David G", "Park", "Jun Woo", "Smola", "Alexander J", "Ahmed", "Amr", "Josifovski", "Vanja", "Long", "James", "Shekita", "Eugene J", "Su", "Bor-Yiing"], "venue": "In Proc. OSDI,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Large-scale image classification: fast feature extraction and svm training", "author": ["Lin", "Yuanqing", "Lv", "Fengjun", "Zhu", "Shenghuo", "Yang", "Ming", "Cour", "Timothee", "Yu", "Kai", "Cao", "Liangliang", "Huang", "Thomas"], "venue": "In CVPR,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["Mensink", "Thomas", "Verbeek", "Jakob", "Perronnin", "Florent", "Csurka", "Gabriela"], "venue": "In ECCV,", "citeRegEx": "Mensink et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mensink et al\\.", "year": 2012}, {"title": "Localityconstrained linear coding for image classification", "author": ["Wang", "Jinjun", "Yang", "Jianchao", "Yu", "Kai", "Lv", "Fengjun", "Huang", "Thomas", "Gong", "Yihong"], "venue": "In CVPR,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Weinberger", "Kilian Q", "Blitzer", "John", "Saul", "Lawrence K"], "venue": "In NIPS,", "citeRegEx": "Weinberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Xing", "Eric P", "Jordan", "Michael I", "Russell", "Stuart", "Ng", "Andrew Y"], "venue": "In NIPS, pp", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing", "author": ["Zaharia", "Matei", "Chowdhury", "Mosharaf", "Das", "Tathagata", "Dave", "Ankur", "Ma", "Justin", "McCauley", "Murphy", "Franklin", "Michael J", "Shenker", "Scott", "Stoica", "Ion"], "venue": "In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,", "citeRegEx": "Zaharia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions.", "startOffset": 75, "endOffset": 192}, {"referenceID": 0, "context": "However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure.", "startOffset": 75, "endOffset": 418}, {"referenceID": 10, "context": "Learning a proper distance metric Xing et al. (2002); Bilenko et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.", "startOffset": 8, "endOffset": 56}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.", "startOffset": 8, "endOffset": 83}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al.", "startOffset": 8, "endOffset": 109}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al.", "startOffset": 8, "endOffset": 130}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al.", "startOffset": 8, "endOffset": 156}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al.", "startOffset": 8, "endOffset": 181}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al.", "startOffset": 8, "endOffset": 204}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al.", "startOffset": 8, "endOffset": 322}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al.", "startOffset": 8, "endOffset": 361}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional.", "startOffset": 8, "endOffset": 430}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional. As first formulated in Xing et al. (2002), mathematically, a Distance Metric Learning (DML) problem can be formulated as semi-supervised learning problem where, given side information in the form of data pairs that are determined to be similar or dissimilar, we learn a metric M , which places similar data pairs close to each other, and dissimilar data pairs as far apart as possible.", "startOffset": 8, "endOffset": 642}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional. As first formulated in Xing et al. (2002), mathematically, a Distance Metric Learning (DML) problem can be formulated as semi-supervised learning problem where, given side information in the form of data pairs that are determined to be similar or dissimilar, we learn a metric M , which places similar data pairs close to each other, and dissimilar data pairs as far apart as possible. This leads to a quadratic program whose size grows super-linearly with the size of the data and of the side information. Specifically, let M defines a Mahalanobis distance (x \u2212 y)M(x \u2212 y), where x, y are d-dimensional feature vectors and M \u2208 Rd\u00d7d is a positive semidefinite matrix (to be learned). Because M is a d-by-d matrix, when the feature dimension d is huge \u2014 such as in web-scale problems where web pages are represented with bag-of-word (BOW) vectors that are millions of words long Ahmed et al. (2012), or in computer vision where hundreds of thousands of features are routinely extracted from images Lin et al.", "startOffset": 8, "endOffset": 1498}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) is essential for many distance based data mining and machine learning algorithms, such as retrieval Hoi et al. (2008), k-means clustering Xing et al. (2002) and k nearest neighbor (kNN) classification Weinberger et al. (2005). A commonality of these algorithms is that their accuracy depends critically on a good distance metric M between points, especially when the dataset is high-dimensional. As first formulated in Xing et al. (2002), mathematically, a Distance Metric Learning (DML) problem can be formulated as semi-supervised learning problem where, given side information in the form of data pairs that are determined to be similar or dissimilar, we learn a metric M , which places similar data pairs close to each other, and dissimilar data pairs as far apart as possible. This leads to a quadratic program whose size grows super-linearly with the size of the data and of the side information. Specifically, let M defines a Mahalanobis distance (x \u2212 y)M(x \u2212 y), where x, y are d-dimensional feature vectors and M \u2208 Rd\u00d7d is a positive semidefinite matrix (to be learned). Because M is a d-by-d matrix, when the feature dimension d is huge \u2014 such as in web-scale problems where web pages are represented with bag-of-word (BOW) vectors that are millions of words long Ahmed et al. (2012), or in computer vision where hundreds of thousands of features are routinely extracted from images Lin et al. (2011) \u2014 the size of M quickly becomes intractable for a single machine; e.", "startOffset": 8, "endOffset": 1615}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al.", "startOffset": 77, "endOffset": 176}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine.", "startOffset": 77, "endOffset": 207}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update.", "startOffset": 77, "endOffset": 754}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines\u2019 local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization \u2014 specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L.", "startOffset": 77, "endOffset": 1618}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines\u2019 local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization \u2014 specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al.", "startOffset": 77, "endOffset": 3053}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines\u2019 local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization \u2014 specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al. (2012); Ho et al.", "startOffset": 77, "endOffset": 3073}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines\u2019 local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization \u2014 specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al.", "startOffset": 77, "endOffset": 3091}, {"referenceID": 0, "context": "For problems involving big data volume, one would naturally speculate that modern distributed system should come for the rescue, and platforms such as Hadoop Foundation (2009) or Spark Zaharia et al. (2012) may straightforwardly offer to scale up the originally sequential algorithms intended for a single machine. But what makes DML on large-scale data nontrivial even with popular distributed platforms is that its basic formulation requires both substantial redesign of the original algorithm, and a more optimization-friendly parallel communication strategy not supported under the currently popular bulk synchronization parallelism (BSP) adopted by Hadoop and Spark. Specifically, DML is a semi-definite programming (SDP) problem Xing et al. (2002), which requires eigen-decomposition of the d-by-d Mahalanobis distance matrix M after each update. Because eigendecomposition requires O(d) computation, it is outright infeasible when the feature dimension d is high, no matter how many machines are available. Furthermore, DML is a optimization problem with hard constraints (where each similar/dissimilar data pair corresponds to one constraint); this makes the distribution and parallel learning of parameters M very difficult, as costly, frequent inter-machine synchronization must be employed to keep the machines\u2019 local views of M consistent with each other. A BSP model would make this operation very expensive. Motivated by these challenges, we explore and validate new approaches to performing distributed DML on large scale problems. On the algorithm side, inspired by ideas from Weinberger et al. (2005), we reformulate DML to make it tractable on high-dimensional data and amenable for distributed optimization \u2014 specifically, we re-represent M with an alternate decomposition LL, and perform optimization directly over L. This not only preserves the semi-definite property of M , but also avoids the costly O(d) eigen-decomposition. Moreover, to solve the inter-machine parameter synchronization challenge caused by the hard DML constraints, we use slack variables to relax those constraints, and then transform the relaxed constraints into a hinge loss function. This makes the distributed optimization much easier, yet does not hurt the effectiveness of the learned distance metric, as our validation will show. To solve this reformulated DML problem, we build a distributed system that uses asynchronous stochastic gradient descent to do parameter learning of M . Similar and dissimilar data pairs are partitioned onto different worker machines and each worker stores a local view of the parameter. At each iteration, each worker randomly picks up a mini-batch of data pairs, computes a stochastic gradient and uses the gradient to update the local parameter copy. Because the gradients computed at each worker need to be seen and utilized by other workers, we use a centralized parameter server to synchronize the parameters among workers, which has been proven to be effective both empirically and theoretically Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014). The major contributions of this work are summarized as follows:", "startOffset": 77, "endOffset": 3109}, {"referenceID": 12, "context": "Distance metric learning Xing et al. (2002); Bilenko et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.", "startOffset": 8, "endOffset": 56}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al.", "startOffset": 8, "endOffset": 83}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al.", "startOffset": 8, "endOffset": 109}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al.", "startOffset": 8, "endOffset": 130}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al.", "startOffset": 8, "endOffset": 156}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al.", "startOffset": 8, "endOffset": 181}, {"referenceID": 1, "context": "(2002); Bilenko et al. (2004); Bar-Hillel et al. (2005); Globerson & Roweis (2005); Weinberger et al. (2005); Davis et al. (2007); Guillaumin et al. (2009); Kostinger et al. (2012); Mensink et al. (2012) has been widely studied in many works.", "startOffset": 8, "endOffset": 204}, {"referenceID": 8, "context": "Xing et al Xing et al. (2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al.", "startOffset": 93, "endOffset": 289}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification.", "startOffset": 93, "endOffset": 329}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away.", "startOffset": 93, "endOffset": 627}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function.", "startOffset": 93, "endOffset": 806}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable.", "startOffset": 93, "endOffset": 1151}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al.", "startOffset": 93, "endOffset": 1703}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do.", "startOffset": 93, "endOffset": 1757}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al.", "startOffset": 93, "endOffset": 2090}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al.", "startOffset": 93, "endOffset": 2110}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al.", "startOffset": 93, "endOffset": 2128}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) .", "startOffset": 93, "endOffset": 2146}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al.", "startOffset": 93, "endOffset": 2706}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration.", "startOffset": 93, "endOffset": 2735}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration. In asynchronous parameter server Ahmed et al. (2012); Dean & Ghemawat (2008), all workers work on their own pace and never waits.", "startOffset": 93, "endOffset": 2853}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration. In asynchronous parameter server Ahmed et al. (2012); Dean & Ghemawat (2008), all workers work on their own pace and never waits.", "startOffset": 93, "endOffset": 2877}, {"referenceID": 0, "context": "(2002) used semidefinite programming to learn a Mahalanobis distance metric for clustering under similarity and dissimilarity constraints. They aim to minimize the distance of similar pairs while separating dissimilar pairs with a certain margin. Weinberger et al Weinberger et al. (2005) and Mensink et al Mensink et al. (2012) employed a similar semidefinite formulation for k-nearest neighbor classification. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Globerson and Roweis Globerson & Roweis (2005) proposed a formulation aiming to collapse all examples in the same class to a single point and push examples in other classes infinitely far away. Davis et al Davis et al. (2007) proposed information theoretic metric learning which minimizes the differential relative entropy between two multivariate Gaussians under the constraints on the distance function. All the above-mentioned works are designed and implemented on single machine and cannot handle large scale data efficiently. Kostinger et al Kostinger et al. (2012) proposed a fast DML method based on likelihood-ratio test, which does not require iterative optimization procedures and hence is very efficient and scalable. However, distance metrics learned by this method yield poor performance empirically and this method is less flexible (can only deal with equivalence constraints). Our method learns effective distance metrics on multiple datasets and can flexibly deal with not only pair-wise constraints but also triple-wise constraints. In terms of the size of the problem, Mensink et al Mensink et al. (2012) achieved similar scale on ImageNet Deng et al. (2009) dataset as we do. However, in their work, DML was performed on a single machine, which takes days to finish. In our work, with the distributed framework, we can finish learning within 15 hours. Designing and implementing parameter servers (PS) for distributed machine learning have been explored in several works Ahmed et al. (2012); Dean et al. (2012); Ho et al. (2013); Li et al. (2014) . Most of these designs contain a centralized server and a collection of workers. The central server maintains the global parameter and each worker has a local copy of the parameter. Workers compute local updates of the parameter and push the updates to the central server. The central server aggregates updates from workers, apply the update to the global parameter and push the fresh global parameter back to workers. The existing parameter servers differ in consistency control. In Bulk Synchronous Parallel (BSP) systems like Hadoop Dean & Ghemawat (2008), Spark Zaharia et al. (2012), workers must wait for each other at the end of every iteration. In asynchronous parameter server Ahmed et al. (2012); Dean & Ghemawat (2008), all workers work on their own pace and never waits. Staleness Synchronous Parallel Ho et al. (2013) seeks a balance between BSP and ASP.", "startOffset": 93, "endOffset": 2978}, {"referenceID": 17, "context": "In the original formulations Xing et al. (2002); Globerson & Roweis (2005); Weinberger et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 17, "context": "In the original formulations Xing et al. (2002); Globerson & Roweis (2005); Weinberger et al.", "startOffset": 29, "endOffset": 75}, {"referenceID": 17, "context": "(2002); Globerson & Roweis (2005); Weinberger et al. (2005) of distance metric learning, a semi-definite programming problem (SDP) needs to be solved.", "startOffset": 35, "endOffset": 60}, {"referenceID": 17, "context": "(2002); Globerson & Roweis (2005); Weinberger et al. (2005) of distance metric learning, a semi-definite programming problem (SDP) needs to be solved. SDP requires eigen-decomposition of the Mahalanobis distance matrix, which is very hard to do on high dimensional data, if not impossible. To make DML scalable on large scale problems, similar to Weinberger et al. (2005), we do the reformulations in two ways.", "startOffset": 35, "endOffset": 372}, {"referenceID": 17, "context": "Note that a symmetric and positive semidefinite matrix M can always be factorized into M = LL Weinberger et al. (2005), where L is a matrix of size k\u00d7 d and k \u2264 d.", "startOffset": 94, "endOffset": 119}, {"referenceID": 17, "context": "We adopt a strategy similar to Weinberger et al. (2005), which introduces slack variables \u03be to relax the hard constraint in Eq.", "startOffset": 31, "endOffset": 56}, {"referenceID": 17, "context": "Our framework can be easily extended to support triple-wise constraints Weinberger et al. (2005) (e.", "startOffset": 72, "endOffset": 97}, {"referenceID": 8, "context": "(2012); Dean & Ghemawat (2008); Ho et al. (2013); Lee et al.", "startOffset": 32, "endOffset": 49}, {"referenceID": 8, "context": "(2012); Dean & Ghemawat (2008); Ho et al. (2013); Lee et al. (2014), we use a centralized parameter servers to synchronize these parameter copies.", "startOffset": 32, "endOffset": 68}, {"referenceID": 10, "context": "The first one is the MNIST hidden written digits LeCun et al. (1998). It has 60K training images and 10K testing images.", "startOffset": 49, "endOffset": 69}, {"referenceID": 5, "context": "The second dataset we use is ImageNet-63K, which has 63K training images randomly selected from the ImageNet Deng et al. (2009) dataset.", "startOffset": 109, "endOffset": 128}, {"referenceID": 5, "context": "The second dataset we use is ImageNet-63K, which has 63K training images randomly selected from the ImageNet Deng et al. (2009) dataset. Each image is associated with a class label and the total number of classes is 1000. Images are represented with Locality-constrained Linear Coding (LLC) Wang et al. (2010), with a dimensionality of 21504.", "startOffset": 109, "endOffset": 310}, {"referenceID": 5, "context": "The second dataset we use is ImageNet-63K, which has 63K training images randomly selected from the ImageNet Deng et al. (2009) dataset. Each image is associated with a class label and the total number of classes is 1000. Images are represented with Locality-constrained Linear Coding (LLC) Wang et al. (2010), with a dimensionality of 21504. We randomly sample 100K \u201csimilar\u201d pairs and 100K \u201cdissimilar\u201d pairs from the training images. If two images are from the same class, we label them as \u201csimilar\u201d. If two images are from different classes, we label them as \u201cdissimilar\u201d. The third dataset is ImageNet-1M. It has 1 million training images and 63K testing images randomly selected from ImageNet Deng et al. (2009). The total number of classes is 1000.", "startOffset": 109, "endOffset": 718}, {"referenceID": 16, "context": "(4) with the original formulation Xing et al. (2002) in Eq.", "startOffset": 34, "endOffset": 53}, {"referenceID": 2, "context": "(1) (denoted by Xing2012) and with Information Theoretical Metric Learning (ITML) Davis et al. (2007) and the likelihood test based method (KISS) Kostinger et al.", "startOffset": 82, "endOffset": 102}, {"referenceID": 2, "context": "(1) (denoted by Xing2012) and with Information Theoretical Metric Learning (ITML) Davis et al. (2007) and the likelihood test based method (KISS) Kostinger et al. (2012). All methods (including our method) are implemented with MATLAB and runs on a single thread.", "startOffset": 82, "endOffset": 170}], "year": 2014, "abstractText": "In large scale machine learning and data mining problems with high feature dimensionality, the Euclidean distance between data points can be uninformative, and Distance Metric Learning (DML) is often desired to learn a proper similarity measure (using side information such as example data pairs being similar or dissimilar). However, high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al. (2002) and later extensions. In this paper, we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture. Our approach builds on a parallelizable reformulation of Xing et al. (2002), and an asynchronous stochastic gradient descent optimization procedure. To our knowledge, this is the first distributed solution to DML, and we show that, on a system with 256 CPU cores, our program is able to complete a DML task on a dataset with 1 million data points, 22-thousand features, and 200 million labeled data pairs, in 15 hours; and the learned metric shows great effectiveness in properly measuring distances.", "creator": "LaTeX with hyperref package"}}}