{"id": "1412.6093", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Learning Temporal Dependencies in Data Using a DBN-BLSTM", "abstract": "Since the advent of deep learning, it has been used to solve various problems using many different architectures. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate the generality of this new architecture by applying it to the generative task of music generation and obtain state-of-the-art results.", "histories": [["v1", "Thu, 18 Dec 2014 11:04:59 GMT  (251kb)", "https://arxiv.org/abs/1412.6093v1", "5 pages, 2 figures, 1 table, ICLR 2015 submission under review"], ["v2", "Tue, 23 Dec 2014 18:44:33 GMT  (400kb)", "http://arxiv.org/abs/1412.6093v2", "6 pages, 2 figures, 1 table, ICLR 2015 conference track submission under review"]], "COMMENTS": "5 pages, 2 figures, 1 table, ICLR 2015 submission under review", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["kratarth goel", "raunaq vohra"], "accepted": false, "id": "1412.6093"}, "pdf": {"name": "1412.6093.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kratarthgoel@gmail.com", "ronvohra@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.60 93v2 [cs.LG] 2 3Since the advent of deep learning, it has been used to solve various problems using many different architectures. Applying such deep architectures to auditory data is also not uncommon. However, these architectures do not always sufficiently take into account the temporal dependencies of data. We therefore propose a new generic architecture, the Deep Belief Network - Bidirectional Long ShortTerm Memory (DBN-BLSTM) network, which models sequences by tracking the temporal information while allowing deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and achieving state-of-the-art results."}, {"heading": "1 INTRODUCTION", "text": "Deep architectures have been an integral part of creating a paradigm shift in the way we address most pattern recognition problems today. Deep networks of faith (DBNs), for example, are designed to maximize the variational lower limit of log probability through hierarchical representation of data; they do not take into account the temporal information contained in speech signals; and various other approaches that have attempted to use deep architectures for speech recognition (as in Hinton et al. (2012); Deng & Yu (2011)). Given that music is inherently dynamic, it seems natural to consider recurring neural networks (RNNNs) as a good basic technology for producing music. However, these models do not work as well as deep networks (Vinyals et al al al al al al al al al al al al al al al. (2012))) Another alternative is to train the DNNs' end-time density rather than combine it with Hidden Markov models (MMs)."}, {"heading": "2 DBN", "text": "Restricted Boltzmann machines (RBMs) are energy-based models with their energy functionE (v, h), which are defined as follows: E (v, h) = \u2212 b \u2032 vv \u2212 b \u2032 hh \u2212 h \u2212 Wv (1), where W represents the weights connecting the units of the visible (v) and hidden (h) layers, and bv, bh are the distortions of the visible and hidden layers. Samples can be obtained from an RBM by performing Block-Gibbs sampling, in which visible units are sampled simultaneously with fixed values of the hidden layers. Likewise, hidden units are sampled simultaneously with the visible unit values. A single step in the Markov chain is taken as follows: h (n + 1) = \u03c3 (W \u2032 v) + bh) v (n + 1) = fixed values of the hidden layers of the hidden layers."}, {"heading": "3 RNN AND BLSTM", "text": "A recursive neural network (RNN) differs from a standard network in that it takes a sequence v = (v1, v2,..., vT) as input and iterates above it from t = 1 to T to produce the following sequence: qt = \u03a6 (bqt + \u2212 \u2192 Wvqvt + \u2212 \u2212 Wqqqt \u2212 1) (4), where q = (q1, q2,..., qT) is a vector representing the hidden unit. b terms are bias vectors (e.g. bq represents the bias of the hidden layer). Normally, the function is the application of elementary sigmoid (\u03c3), in the case of a general RNN \u2212 \u2212 a vector representing the hidden unit. (5) The b terms are bias vectors (e.g. bq represents the default of the hidden layer)."}, {"heading": "4 THE DBN-BLSTM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 THE ARCHITECTURE", "text": "The DBN-BLSTM is an extension of the generative model (RNN-DBN) proposed by Goel et al. (2014). Some significant improvements have been made to this model, in particular the replacement of the RNN by an LSTM, which represents a more powerful neural architecture capable of modelling time dependencies over large time steps, ensuring that the model retains information about the generated sequence over a longer period of time. As we discuss generative models, this property is exceptionally well suited for modeling creativity, for example for music generation, where choosing an LSTM over an RNN would result in less repetitions and more varied music due to better generalization, since the improved memory of the LSTM would have more information about previously produced music in the sequence than compared to an RNN. The network is shown in Figure 2.There are essentially two points for the interaction between the DBN and the STrN \u2212 qt."}, {"heading": "4.2 TRAINING THE NETWORK", "text": "The distortions to the layers of the DBN are calculated as specified in Equation (12) and (13), using the distortions to the layers of the DBN as specified in Equation (4) and (11). These distortions are then used to calculate the common probability distribution between the observed and hidden layers of the DBN as specified in Equation (3).For sampling from the DBN, we use Equation (2).It should be noted that the input vt to the DBN at each time step t is a binary vector, and therefore the model for the music generation task consists only of binary layers. However, this is not a limitation of our technique, and the model can be easily extended to handle real value data. Such a possible extension could include the visible layer of the DBN layers of the subsequent layers of LGU units."}, {"heading": "5 EXPERIMENT AND RESULTS", "text": "We demonstrate our technique by applying it to the task of polyphonic music generation. We used a DBN-BLSTM with 3 hidden DBN layers - each with 150 binary units - and 150 binary units in the BLSTM. The visible layer has 88 binary units, corresponding to the full range of the piano from A0 to C8. Each layer contains a dropout. We used our technique on four data sets - JSB Chorales, MuseData1, Nottingham2 and Piano-midi.de. Only raw MIDI data was given to the DBN-BLSTM as input. We evaluate our models qualitatively by generating sample sequences and quantitatively by using log probability (LL) as a measure of performance. Results (some of which are reproduced from Boulanger-Lewandowski et al. (2012) in Table 1. The results clearly show that our technology works much better than the current state of the art."}, {"heading": "6 CONCLUSIONS AND FUTURE WORK", "text": "We have proposed a generic technique called DBN-BLSTM for modeling sequences and demonstrated its successful application in polyphonic music generation. We have used four sets of data to evaluate our technique and achieved state-of-the-art results. In the future, we plan to work on other powerful architectures that help learn time dependencies of data without compromising the powerful hierarchical representations that DBNs provide."}, {"heading": "7 ACKNOWLEDGEMENTS", "text": "We would like to thank Yoshua Bengio for the helpful conversations and the developers of Theano (Bastien et al. (2012); Bergstra et al. (2010)), which we used for all experiments."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "In ICML,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Deep convex network: A scalable architecture for speech pattern classification", "author": ["Deng", "Li", "Yu", "Dong"], "venue": "In Interspeech. International Speech Communication Association,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "Polyphonic music generation by modeling temporal dependencies using a RNN-DBN", "author": ["Goel", "Kratarth", "Vohra", "Raunaq", "J.K. Sahoo"], "venue": "In ICANN,", "citeRegEx": "Goel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey E", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N", "Kingsbury", "Brian"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Revisiting recurrent neural networks for robust ASR", "author": ["Vinyals", "Oriol", "Ravuri", "Suman", "Povey", "Daniel"], "venue": "IEEE International Confrence on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Vinyals et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "The same goes for various other approcahes that have tried to use deep architectures for speech recognition (as explained in Hinton et al. (2012); Deng & Yu (2011)).", "startOffset": 125, "endOffset": 146}, {"referenceID": 5, "context": "The same goes for various other approcahes that have tried to use deep architectures for speech recognition (as explained in Hinton et al. (2012); Deng & Yu (2011)).", "startOffset": 125, "endOffset": 164}, {"referenceID": 5, "context": "The same goes for various other approcahes that have tried to use deep architectures for speech recognition (as explained in Hinton et al. (2012); Deng & Yu (2011)). Given that music is an inherently dynamic, it seems natural to consider recurrent neural networks (RNNs) as a good baseline technique for music generation. However, these models do not perform as well as deep networks (Vinyals et al. (2012)).", "startOffset": 125, "endOffset": 407}, {"referenceID": 4, "context": "The results achieved in Graves et al. (2013) clearly support our claim about the importance of modeling temporal dependencies in sequential data.", "startOffset": 24, "endOffset": 45}, {"referenceID": 3, "context": "1 THE ARCHITECTURE The DBN-BLSTM is an extension of the generative model (RNN-DBN) proposed by Goel et al. (2014). There are a few significant improvements made to this model, most notably the replacement of the RNN with a LSTM, which is a more powerful neural architecture capable of modeling temporal dependencies across large time steps.", "startOffset": 95, "endOffset": 114}], "year": 2014, "abstractText": "Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network Bidirectional Long ShortTerm Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.", "creator": "LaTeX with hyperref package"}}}