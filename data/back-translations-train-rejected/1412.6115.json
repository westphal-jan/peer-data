{"id": "1412.6115", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2014", "title": "Compressing Deep Convolutional Networks using Vector Quantization", "abstract": "Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.", "histories": [["v1", "Thu, 18 Dec 2014 21:09:01 GMT  (896kb,D)", "http://arxiv.org/abs/1412.6115v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["yunchao gong", "liu liu", "ming yang", "lubomir bourdev"], "accepted": false, "id": "1412.6115"}, "pdf": {"name": "1412.6115.pdf", "metadata": {"source": "CRF", "title": "COMPRESSING DEEP CONVOLUTIONAL NETWORKS USING VECTOR QUANTIZATION", "authors": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "emails": ["ycgong@fb.com", "lliu@fb.com", "mingyang@fb.com", "lubomir@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "It is particularly useful when the range of applications is limited or photos may not be sent to servers. However, the size of CNN models is usually very large (e.g. more than 200 bytes) in terms of application possibilities."}, {"heading": "2 RELATED WORK", "text": "In recent years it has been shown that with the great advances in object recognition (Girshick et al., 2014) and image retrieval (Razavian et al., 2014; Gong et al., 2014), the state-of-the-art image classifier can achieve 94% accuracy on the ILSVRC2014 dataset of 1000 object classes and is already very close to human performance. Such a great success ignites interest in adopting CNNs to applications in the real world. For example, CNN has already been applied to object classification, scene classification and indoor classification. It has also been applied to imaging results, with impressive predictions."}, {"heading": "3 COMPRESS DENSE CONNECTED LAYERS", "text": "In this section, we will consider two classes of methods for compressing the parameters in tightly connected layers: First, we will look at the matrix factorization methods, and then introduce the vector quantization methods."}, {"heading": "3.1 MATRIX FACTORIZATION METHODS", "text": "First, we will consider matrix factorization methods widely used to accelerate CNN (Denton et al., 2014) and to compress parameters in linear models (Denton et al., 2014). Specifically, we will consider the use of singular value decomposition (SVD) to factorize the parameter matrix. Given the parameter W-Rm-n in a tightly bonded layer, we will factorize it asW = UPS T, (1) where U-Rm-m and V-Rn are two dense orthogonal matrices and S-Rm-n is a diagonal matrix. To approximate W-Rm-n with two much smaller matrices, we can select the uppermost k-singular vectors in U and V with corresponding eigenvalues in S to reconstruct W-Rm-n values."}, {"heading": "3.2 VECTOR QUANTIZATION METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 BINARIZATION", "text": "We start with the simplest method of quantifying the parameter matrices. In view of parameter W, we take the character of the matrix: W, ij = {1 if Wij, 0, \u2212 1 if Wij < 0. (3) This method is mainly inspired by Dropconnect (Wan et al., 2013), which randomly sets a portion of the parameters (neurons) to 0 during training. Here, we take a more aggressive approach by turning on each neuron when it is positive and turning it off when it is negative. Geometrically, assuming that dense interconnected layers are a series of hyperplanes, we actually round each hyperplane to its next coordinate. This method will compress the data 32 times, since each neuron is represented by a bit. 3.2.2 SCALAR QUANTIZATION USING kMEANSAnother simple method is to perform scalare quantization to the parameters kn, for each of us can directly put RM values into the matrix."}, {"heading": "3.2.3 PRODUCT QUANTIZATION", "text": "Next, we will consider methods of structured vector quantization to compress the parameters. In particular, we will consider the use of product quantization (PQ) (Jegou et al., 2011), which investigates the redundancy of structures in vector space. PQ's basic idea is to divide the vector space into many fragmented sub-spaces and perform quantification in each sub-space. On the basis of its assumption that the vectors in each sub-space are highly redundant and are better able to research the redundancy structure through quantization in each sub-space. In particular, we will subdivide the matrix W colum-wise into several sub-matrices: W = [W 1, W 2,., W s], (6) with W i-Rm \u00d7 (n / s) assuming that n is divisible. We can perform kmeans clustering for each sub-matrix."}, {"heading": "3.2.4 RESIDUAL QUANTIZATION", "text": "The third quantization method we are considering is residual quantization (Chen et al., 2010), another form of structured quantization. The basic idea is to first quantify the vectors in k centers and then to quantify the residuals recursively. For example, if we obtain a series of vectors wi, i-1,..., m, we begin to quantify them in k-different vectors using kmeans clusters: min m \u00b2 z \u00b2 j \u00b2 j \u00b2 wz \u2212 c1j \u00b2 22, (10) Each vector wz is represented by its closest center c1j. Next, we calculate the residual center r \u00b2 z between wz and c1j for all data points and quantify the residual vectors r \u00b2 z in k different code words c2j recursively. Finally, a vector wz can be reconstructed by adding its corresponding centers at each stage: w \u00b2 c = j + j."}, {"heading": "3.2.5 OTHER METHODS AND DISCUSSION", "text": "The above mentioned (KM, PQ and RQ) are three different types of vector quantization methods for compressing matrices. KM only detects the redundancy of each neuron (single scalar); PQ examines some local redundancy structures; and RQ attempts to explore the global redundancy structure between weight vectors. It will be interesting to investigate what types of redundancy are present in the behavior of the learned parameters; there are many learning-based binarization or product quantization methods available, such as Spectral Hashing (Wei\u00df et al., 2008), Iterative Quantization (Gong et al., 2012) and Catesian kmeans (Norouzi & Fleet, 2013), which are not suitable for this particular task, however, because we need to store the learned parameter matrix (e.g. a rotation matrix), which is very large. Therefore, we do not consider the other methods in this paper."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 EXPERIMENTAL SETTING", "text": "This dataset contains more than 1 million training images from 1000 object categories. It also has a validation set of 20,000 images in categories of 20 images each. We trained on the standard training set, compressed to parameters and tested the validation set. The Zeiler & Fergus Convolutionary Neural Network (2013) we used contains 5 Convolutionary Layers and 3 tightly connected layers. All input images were initially reduced to a minimum size of 257, then we performed random cuts to 225 x 225 patches. Subsequently, the images were divided into 5 different Convolutionary Layers with respective filter sizes of 7, 5, 3 and 3. The first two Convolutionary Layers were reduced to a minimum size of 257, after which we performed random cuts to 225 x 225 patches. At that time, the images were grouped into 5 different Convolutionary Layers, each with different filter sizes of 7, 5, 3 and 3 layers."}, {"heading": "4.2 ANALYSIS OF PRODUCT QUANTIZATION", "text": "We first performed an analysis of the PQ-based compression of the parameters, since PQ has several different parameters. We used a different number of clusters k = 4, 8, 16 (equivalent to 2, 3, 4 bits) for each segment. For each fixed k, we showed results for different segment dimensions (column sizes) s = 1, 2, 3, 4, which changed the compression rate from lower to higher. As mentioned in Section 3.2.3, we were able to run PQ for either the x-axis or the y-axis, and thus show results for both cases. We will compare different PQ methods when aligning the segment size, and also compare them with the aligned compression rate.The results for the accuracy @ 1 are shown in Figure 1 and Figure 2 for different axis alignments, and from the results in Figure 1 we see that by using more centers k and smaller segment sizes, we were able to obtain smaller compression errors."}, {"heading": "4.3 OVERALL COMPARISON", "text": "Section 3 below provides a comparison of all the quantization methods we are introducing here. Similar to the above sections, we present the classification errors in terms of compression rate. For binary quantization, which has no parameter, the compression rate was 32. For the PQ, we used 8 centers per segment and varied the dimension (from 1 to 4) of each segment to achieve different compression rates. RQ performance was unsatisfactory; here, we only report the number of clusters from 1 to 32 to achieve a compression rate between 32 and 1. For SVD, we vary the output dimension to achieve different compression rates."}, {"heading": "4.4 ANALYSIS OF SINGLE LAYER ERROR", "text": "The results are shown in Figure 4 (for accuracy @ 1 only). We found that compressing the eighth and ninth hidden layer usually did not result in a significant reduction in performance, but compressing the tenth and last classification layer resulted in a much greater decrease in accuracy. Compressing all three layers together usually resulted in larger errors, especially when the compression rate was high. Finally, some sample results are shown in Figure 5."}, {"heading": "5 APPLICATION TO IMAGE RETRIEVAL", "text": "This section represents an application of compressed CNN to retrieve images in order to verify the generalization capability of compressed networks. In the real industry, many situations were not able to allow the uploading of a photo to the server, or where uploading a large number of photos to the server would be prohibitive. In fact, due to bandwidth limitations, we were only able to upload some processed data (e.g. features or hashes of the image) to the server. Given the above compressed CNN, we were able to process the images with compressed CNN on the mobile phone side and perform recovery on the database side by uploading only the processed characteristics. We conducted experiments on the Holidays dataset (Jegou et al., 2008). This is a widely used standard benchmark dataset for retrieving images containing 1491 images for 500 different instances; each instance contains duplicate images; the number of queries of the rest of the cosmopolitan database is set to 500; the rest of the number of the images are used to be used."}, {"heading": "6 DISCUSSION", "text": "Our work systematically dealt with the compression of the 108 parameters of a deep convolutionary neural network in order to save the storage of the models. In contrast to earlier approaches, which considered the use of matrix factoring methods, we proposed to study a number of methods of vector quantification to compress the parameters. Slightly surprisingly, we found that by simply performing a scalar quantization of the parameter values using kmeans, we were able to achieve a compression rate of 8-16 parameters without sacrificing the top-five accuracy at more than 0.5% of the compressions. Furthermore, by using structured quantization methods, we were able to compress the parameters up to 24 times, while maintaining the loss of the top-five accuracy of the parameters within 1%. By compressing the parameters over 20 times, we have confirmed the problem of applying modern CNs to the compression five times, when these parameters can be compressed to 24%, when the loss of the top-one is confirmed."}], "references": [{"title": "Approximate nearest neighbor search by residual vector quantization", "author": ["Chen", "Yongjian", "Guan", "Tao", "Wang", "Cheng"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Accelerating deep neural networks on mobile processor with embedded programmable logic", "author": ["Culurciello", "Eugenio", "Dundar", "Aysegul", "Jin", "Jonghoon", "Gokhale", "Vinayak", "Martini", "Berin"], "venue": "In NIPS", "citeRegEx": "Culurciello et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Culurciello et al\\.", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "Ranzato", "Marc\u2019aurelio", "Freitas", "Nando D"], "venue": "K.q. (eds.), Advances in Neural Information Processing Systems", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In NIPS", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"], "venue": "CoRR, abs/1310.1531,", "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "A 240 g-ops/s mobile coprocessor for deep neural networks", "author": ["V. Gokhale", "Jin", "Jonghoon", "A. Dundar", "B. Martini", "E. Culurciello"], "venue": "In CVPR Workshops", "citeRegEx": "Gokhale et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gokhale et al\\.", "year": 2013}, {"title": "Iterative quantization: A Procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Gong", "Yunchao", "Lazebnik", "Svetlana", "Gordo", "Albert", "Perronnin", "Florent"], "venue": null, "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Gong", "Yunchao", "Wang", "Liwei", "Guo", "Ruiqi", "Lazebnik", "Svetlana"], "venue": "In ECCV", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "In BMVC", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Hamming embedding and weak geometric consistency for large-scale image", "author": ["Jegou", "Herve", "Douze", "Matthijs", "Schmid", "Cordelia"], "venue": null, "citeRegEx": "Jegou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2008}, {"title": "Product quantization for nearest neighbor search", "author": ["Jegou", "Herve", "Douze", "Matthijs", "Schmid", "Cordelia"], "venue": "IEEE TPAMI,", "citeRegEx": "Jegou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2011}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing"], "venue": "http: //caffe.berkeleyvision.org/,", "citeRegEx": "Jia and Yangqing.,? \\Q2013\\E", "shortCiteRegEx": "Jia and Yangqing.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1990\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1990}, {"title": "Fast training of convolutional networks through ffts", "author": ["M Mathieu", "M. Henaff", "Y. LeCun"], "venue": "In Arxiv,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Razavian", "Ali Sharif", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "CoRR, abs/1403.6382,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Michael", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Deep fisher networks for large-scale image classification", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "In Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matt", "Zhang", "Sixin", "Lecun", "Yann", "Fergus", "Rob"], "venue": null, "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "Deep convolutional neural networks (Krizhevsky et al., 2012; LeCun et al., 1990; Szegedy et al., 2014; Simonyan & Zisserman, 2014) has recently achieved significant progress and have become the gold standard for object recognition, image classification, and retrieval.", "startOffset": 35, "endOffset": 130}, {"referenceID": 14, "context": "Deep convolutional neural networks (Krizhevsky et al., 2012; LeCun et al., 1990; Szegedy et al., 2014; Simonyan & Zisserman, 2014) has recently achieved significant progress and have become the gold standard for object recognition, image classification, and retrieval.", "startOffset": 35, "endOffset": 130}, {"referenceID": 4, "context": "Almost all of the recent successful recognition systems (Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013; Gong et al., 2014) are built on top of this architecture.", "startOffset": 56, "endOffset": 177}, {"referenceID": 19, "context": "Almost all of the recent successful recognition systems (Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013; Gong et al., 2014) are built on top of this architecture.", "startOffset": 56, "endOffset": 177}, {"referenceID": 17, "context": "Almost all of the recent successful recognition systems (Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013; Gong et al., 2014) are built on top of this architecture.", "startOffset": 56, "endOffset": 177}, {"referenceID": 8, "context": "Almost all of the recent successful recognition systems (Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013; Gong et al., 2014) are built on top of this architecture.", "startOffset": 56, "endOffset": 177}, {"referenceID": 6, "context": "Importing CNN onto embedded platforms (Gokhale et al., 2013; Culurciello et al., 2013), the recent trend toward mobile computing, has a wide range of application impacts.", "startOffset": 38, "endOffset": 86}, {"referenceID": 1, "context": "Importing CNN onto embedded platforms (Gokhale et al., 2013; Culurciello et al., 2013), the recent trend toward mobile computing, has a wide range of application impacts.", "startOffset": 38, "endOffset": 86}, {"referenceID": 17, "context": "For example, a typical CNN (Jia, 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013) that works well for object recognition contains eight layers (five convolutional layers and three dense connected layers) and a huge number of parameters (e.", "startOffset": 27, "endOffset": 84}, {"referenceID": 2, "context": "Because as is widely known, the parameters are heavily over-parameterized (Denil et al., 2013), it is very interesting to investigate whether we can compress these parameters by exploring their structure.", "startOffset": 74, "endOffset": 94}, {"referenceID": 3, "context": "Here, we are mainly interested in compressing the parameters to reduce storage instead of speeding up the testing time (Denton et al., 2014).", "startOffset": 119, "endOffset": 140}, {"referenceID": 9, "context": "Another similar work on speeding up CNN is (Jaderberg et al., 2014), in which the authors described several reconstruction methods for approximating the filers in convolutional layers.", "startOffset": 43, "endOffset": 67}, {"referenceID": 3, "context": "In this work, instead of the traditional matrix factorization methods considered in (Denton et al., 2014; Jaderberg et al., 2014), we mainly consider a series of information theoretical vector quantization methods (Jegou et al.", "startOffset": 84, "endOffset": 129}, {"referenceID": 9, "context": "In this work, instead of the traditional matrix factorization methods considered in (Denton et al., 2014; Jaderberg et al., 2014), we mainly consider a series of information theoretical vector quantization methods (Jegou et al.", "startOffset": 84, "endOffset": 129}, {"referenceID": 11, "context": ", 2014), we mainly consider a series of information theoretical vector quantization methods (Jegou et al., 2011; Chen et al., 2010) for compressing dense connected layers.", "startOffset": 92, "endOffset": 131}, {"referenceID": 0, "context": ", 2014), we mainly consider a series of information theoretical vector quantization methods (Jegou et al., 2011; Chen et al., 2010) for compressing dense connected layers.", "startOffset": 92, "endOffset": 131}, {"referenceID": 2, "context": "The most closely related one, Denton et al. (2014), explored matrix factorization methods for speeding up CNN testing time.", "startOffset": 30, "endOffset": 51}, {"referenceID": 13, "context": "Deep convolutional neural network has achieved great successes in image classification (Krizhevsky et al., 2012; Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013), object detection (Girshick et al.", "startOffset": 87, "endOffset": 214}, {"referenceID": 4, "context": "Deep convolutional neural network has achieved great successes in image classification (Krizhevsky et al., 2012; Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013), object detection (Girshick et al.", "startOffset": 87, "endOffset": 214}, {"referenceID": 19, "context": "Deep convolutional neural network has achieved great successes in image classification (Krizhevsky et al., 2012; Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013), object detection (Girshick et al.", "startOffset": 87, "endOffset": 214}, {"referenceID": 17, "context": "Deep convolutional neural network has achieved great successes in image classification (Krizhevsky et al., 2012; Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013), object detection (Girshick et al.", "startOffset": 87, "endOffset": 214}, {"referenceID": 5, "context": ", 2013; Zeiler & Fergus, 2013), object detection (Girshick et al., 2014), and image retrieval (Razavian et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 16, "context": ", 2014), and image retrieval (Razavian et al., 2014; Gong et al., 2014).", "startOffset": 29, "endOffset": 71}, {"referenceID": 8, "context": ", 2014), and image retrieval (Razavian et al., 2014; Gong et al., 2014).", "startOffset": 29, "endOffset": 71}, {"referenceID": 2, "context": ", 2012; Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013), object detection (Girshick et al., 2014), and image retrieval (Razavian et al., 2014; Gong et al., 2014). With the great progress in this area, the state-of-the-art image classifier can achieve 94% top five accuracy on the ILSVRC2014 dataset with 1000 object classes, and is already very close human performance. Such great success ignites interest in adopting CNNs to real world applications. For example, CNN has already been applied to object classification, scene classification, and indoor scene classification. It has also been applied to image retrieval, with impressive results. As discussed in the above section, a state-of-the-art CNN usually involves hundreds of millions of parameters, which require huge storage for the model that is difficult to achieve. The bottleneck comes from model storage and testing speed. Several works have been published on speeding up CNN prediction speed. Vanhoucke et al. (2011), who explored the properties of CPU to speed up the execution of CNN, particularly focused on the aligning of memory and SIMD operations to boost matrix operations.", "startOffset": 19, "endOffset": 1034}, {"referenceID": 2, "context": ", 2012; Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler & Fergus, 2013), object detection (Girshick et al., 2014), and image retrieval (Razavian et al., 2014; Gong et al., 2014). With the great progress in this area, the state-of-the-art image classifier can achieve 94% top five accuracy on the ILSVRC2014 dataset with 1000 object classes, and is already very close human performance. Such great success ignites interest in adopting CNNs to real world applications. For example, CNN has already been applied to object classification, scene classification, and indoor scene classification. It has also been applied to image retrieval, with impressive results. As discussed in the above section, a state-of-the-art CNN usually involves hundreds of millions of parameters, which require huge storage for the model that is difficult to achieve. The bottleneck comes from model storage and testing speed. Several works have been published on speeding up CNN prediction speed. Vanhoucke et al. (2011), who explored the properties of CPU to speed up the execution of CNN, particularly focused on the aligning of memory and SIMD operations to boost matrix operations. Mathieu et al. (2013) showed that the convolutional operation can be efficiently carried out in the Fourier domain, which leads to a speed-up of 200%.", "startOffset": 19, "endOffset": 1221}, {"referenceID": 2, "context": "Two very recent works by Denton et al. (2014) and Jaderberg et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 2, "context": "Two very recent works by Denton et al. (2014) and Jaderberg et al. (2014), which explored the use of linear matrix factorization methods for speeding up convolutional layers, showed a 200% speed-up with little compromise of classification performance.", "startOffset": 25, "endOffset": 74}, {"referenceID": 2, "context": "The use of vector quantization methods to compress CNN parameters is mainly inspired by the work of Denil et al. (2013), who demonstrate the redundancies in neural network parameters.", "startOffset": 100, "endOffset": 120}, {"referenceID": 2, "context": "The use of vector quantization methods to compress CNN parameters is mainly inspired by the work of Denil et al. (2013), who demonstrate the redundancies in neural network parameters. They show that the weights within one layer can be accurately predicted by a small ( 5%) subset of the parameters, which indicates that the neural network is over-parameterized. These results motivate us to apply vector quantization methods to explore the redundancy in parameter space. In particular, our paper can be viewed as a compression realization of the parameter prediction results reported in Denil et al. (2013). Somewhat surprisingly, we have found very similar results to those of Denil", "startOffset": 100, "endOffset": 607}, {"referenceID": 2, "context": "This result further confirms the interesting empirical findings in Denil et al. (2013).", "startOffset": 67, "endOffset": 87}, {"referenceID": 3, "context": "We first consider matrix factorization methods, which have been widely used to speed up CNN (Denton et al., 2014) as well as for compressing parameters in linear models (Denton et al.", "startOffset": 92, "endOffset": 113}, {"referenceID": 3, "context": ", 2014) as well as for compressing parameters in linear models (Denton et al., 2014).", "startOffset": 63, "endOffset": 84}, {"referenceID": 21, "context": "This method is mainly inspired by Dropconnect (Wan et al., 2013), which randomly sets part of the parameters (neurons) to 0 during training.", "startOffset": 46, "endOffset": 64}, {"referenceID": 11, "context": "In particular, we consider the use of product quantization (PQ) (Jegou et al., 2011), which explores the redundancy of structures in vector space.", "startOffset": 64, "endOffset": 84}, {"referenceID": 0, "context": "The third quantization method we consider is residual quantization (Chen et al., 2010), which is another form of structured quantization.", "startOffset": 67, "endOffset": 86}, {"referenceID": 7, "context": ", 2008), Iterative Quantization (Gong et al., 2012), and Catesian kmeans (Norouzi & Fleet, 2013), among others.", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": "In particular, SVD achieves impressive results for compressing and speeding up convolutional layers (Denton et al., 2014) but does not work well for compressing dense connected layers.", "startOffset": 100, "endOffset": 121}, {"referenceID": 10, "context": "We performed experiments on the Holidays dataset (Jegou et al., 2008).", "startOffset": 49, "endOffset": 69}], "year": 2014, "abstractText": "Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.", "creator": "LaTeX with hyperref package"}}}