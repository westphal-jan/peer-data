{"id": "1412.6257", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Gradual training of deep denoising auto encoders", "abstract": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets.", "histories": [["v1", "Fri, 19 Dec 2014 09:30:33 GMT  (1698kb,D)", "http://arxiv.org/abs/1412.6257v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexander kalmanovich", "gal chechik"], "accepted": false, "id": "1412.6257"}, "pdf": {"name": "1412.6257.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Alexander Kalmanovich"], "emails": ["sashakal@gmail.com,", "gal.chechik@biu.ac.il"], "sections": [{"heading": "1 INTRODUCTION", "text": "A central approach to education is to build a deep network for the reconstruction of corrupt data."}, {"heading": "2 TRAINING DENOISING AUTOENCODERS", "text": "For the sake of completeness, reference is made here to the method for training stacked autoencoders described by Vincent et al. (2010). Fig. 1 describes the architecture and the most important training phases. For training the first layer with a training sample x, obfuscation noise is used to generate a corrupt, loud version x (Fig. 1a, corrupt arrow). A forward process is inserted in which the hidden representation h1 = sigmoid (w > 1 x) and the output y = sigmoid (w \u2032 > 2 h1) is calculated. Specifically, the loss function is often taken as the cross-sectional entropy between y and x (Fig. 1a, dotted arrow). All weights are updated by propagating the error gradient back through the network. This is repeated in other samples in a stochastic gradient descenation (SGD), and combined with dynamics and weight drop to form a velocity layer, which is stretched to a layer of x1."}, {"heading": "2.1 GRADUAL TRAINING OF DEEP DAES", "text": "The basic idea is to train the deep autoencoder layer by layer, but to continuously adapt the lower layers. Noise injections are applied only at the input level (Fig. 2). The motivation for this method has two aspects. Specifically, it allows lower weights to take into account the higher representations during training, thereby reducing the greedy nature of the stacked training. Secondly, denoizing is applied to the input, rather than to a hidden representation that has been learned greedily. Specifically, the first layer is trained in the same way as in stacked training, generating the weights w1. Then, when the second layer of autoencoder is added, its weights w2 are matched with w1. This is done by using the weights w1 to initialize the first layer and calculate the weights w1 and initialize the fall zip weights after the second layer."}, {"heading": "3 EXPERIMENTS", "text": "We compare the performance of step-by-step training of DAEs with that of stacked training in two learning situations: first in an unattended denocialization task and then by using it to initialize a deep network in a supervised classification task. We conduct all experiments with \"MEDAL,\" a MATLAB implementation of DNNs and auto-encoders (Stansbury, 2012)."}, {"heading": "3.1 DATASETS", "text": "We tested the step-by-step training on three benchmark datasets: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). MNIST contains 70,000 grayscale images in 28 by 28 format, each with a single handwritten digit. CIFAR-10 and CIFAR-100 contain 60,000 natural RGB images with 32 by 32 pixels in 10 and 100 categories, respectively."}, {"heading": "3.2 TRAINING PROCEDURE", "text": "When quantifying performance in relation to the size of the data sets, we create training parts of different sizes while maintaining a uniform class distribution as in the original training data. Hyperparameters were selected on the basis of a second level of cross-validation (10x CV for MNIST, 5x CIFAR value), setting the following hyperparameters: number of units in hidden layers (equal for all layers: 1000,1500,2000,2500), learning rate (10 \u2212 1, 10 \u2212 2, 10 \u2212 3, 5 \u2212 4, 5 \u2212 5 \u2212 5) batch size for SGD (10, 20), seed for random initialization, dynamics (0.9, 0.7) (polyak, 1964) and weight decay (10 \u2212 4, 10 \u2212 5 \u2212 5)."}, {"heading": "4 RESULTS", "text": "We evaluate gradual and stacked training in unattended education denocialization activity. We then test these training methods as initialization for supervised learning and quantify their performance depending on the size of the dataset."}, {"heading": "4.1 UNSUPERVISED LEARNING FOR DENOISING", "text": "We begin by evaluating the step-by-step training in an unsupervised task of education denotation, where the network is trained to minimize loss of cross-sectional entropy from corrupt images. In addition to stacked and step-by-step training, we also tested a hybrid method that spends some epochs tuning only the second layer (as in stacked training) and then spends the rest of the training budget on both layers (as in gradual training). We define the stacked vs gradual fraction 0 \u2264 f \u2264 1 as a fraction of the weight updates that occur during stacked training. f = 1 corresponds to stacked training, while f = 0 corresponds to pure graduation epochs. Given a budget of n training epochs, we train the second hidden layer with gradual training for n (1 \u2212 f) epochs, and with stacked training for 2f epoch, the IAR-epoch training is lower than NAR-epoch training."}, {"heading": "4.2 GRADUAL-TRAINING DAE FOR INITIALIZING A NETWORK IN A SUPERVISED TASK", "text": "The network architecture is the same as the SDAE architecture, except for the top layer. The first two hidden layers are initialized with the first two layer weights of the SDAE (w1 and w2 in Fig. 2b). We then add a top classification layer that matches the classes in the dataset, with randomly initialized weights. We train these networks on several subsets of each dataset to quantify the benefit of unattended pretraining as a function of the train set size. Fig. 4 tracks the classification error as a function of the training set size and shows the percentage of relative improvement in the text. These results suggest that initialization with step-by-step trained DAEs yields better classification accuracy than initialization with stacked, trained DAEs, and that this effect is mainly relevant for datasets with less than 50K samplers. The gradation method described above leads to a reduction in the grading accuracy of the first two layers of this grading."}, {"heading": "5 CONCLUSION", "text": "We have described a step-by-step training program for the denocialization of auto-encoders that improves the reconstruction error within a fixed training budget compared to stacked training. It also resulted in a small but consistent improvement in the classification error in the system of medium-sized training sets. The comparison between stacked and tiered training can be considered as the two extreme adjustment programs: with stacked learning reflecting a learning rate of zero for the lower layer, and step-by-step training reflecting a full learning rate. It remains to be seen how the learning rate between training sessions is gradually reduced when a shift is presented using examples."}], "references": [{"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan", "Dumitru", "Manzagol", "Pierre-Antoine", "Bengio", "Yoshua", "Samy", "Vincent", "Pascal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Deep learning using robust interdependent codes", "author": ["Larochelle", "Hugo", "Erhan", "Dumitru", "Vincent", "Pascal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Switching of nmda receptor 2a and 2b subunits at thalamic and cortical synapses during early postnatal development", "author": ["Liu", "Xiao-Bo", "Murray", "Karl D", "Jones", "Edward G"], "venue": "The Journal of neuroscience,", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "A simple weight decay can improve generalization", "author": ["JE Moody", "SJ Hanson", "Krogh", "Anders", "Hertz", "John A"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Moody et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Moody et al\\.", "year": 1995}, {"title": "Teodorovich. Some methods of speeding up the convergence of iteration methods", "author": ["Polyak", "Boris"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak and Boris,? \\Q1964\\E", "shortCiteRegEx": "Polyak and Boris", "year": 1964}, {"title": "Matlab environment for deep architecture learning, 2012. URL https: //github.com/dustinstansbury/medal", "author": ["Stansbury", "Dustin E"], "venue": null, "citeRegEx": "Stansbury and E.,? \\Q2012\\E", "shortCiteRegEx": "Stansbury and E.", "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["Xie", "Junyuan", "Xu", "Linli", "Chen", "Enhong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xie et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "Indeed, denoising autoencoders (DAE) (Vincent et al., 2008) have been shown to extract meaningful features which allow to correct corrupted input data (Xie et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 11, "context": ", 2008) have been shown to extract meaningful features which allow to correct corrupted input data (Xie et al., 2012).", "startOffset": 99, "endOffset": 117}, {"referenceID": 10, "context": "It has been shown that in the small-data regime, good initializations can cut down the training time and improve the classification accuracy of the supervised task (Vincent et al., 2010; 2008; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 164, "endOffset": 237}, {"referenceID": 3, "context": "It has been shown that in the small-data regime, good initializations can cut down the training time and improve the classification accuracy of the supervised task (Vincent et al., 2010; 2008; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 164, "endOffset": 237}, {"referenceID": 1, "context": "It has been shown that in the small-data regime, good initializations can cut down the training time and improve the classification accuracy of the supervised task (Vincent et al., 2010; 2008; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 164, "endOffset": 237}, {"referenceID": 10, "context": "Going beyond a single layer, it has been shown that training a multi-layer (deep) DAE can be achieved efficiently by stacking single-layer DAEs and training them layer-by-layer (Vincent et al., 2010).", "startOffset": 177, "endOffset": 199}, {"referenceID": 0, "context": "Stacked training has been shown to outperform training de-novo of a full deep network, presumably because it provides better error signals to lower layers of the network (Erhan et al., 2009).", "startOffset": 170, "endOffset": 190}, {"referenceID": 5, "context": "Comparing this with the process of reduced plasticity in natural neural systems, early layers in mammalian visual system keep adapt for prolonged periods, and their synapses remain plastic long after representations have been formed in high brain areas (Liu et al., 2004).", "startOffset": 253, "endOffset": 271}, {"referenceID": 10, "context": "To train a deep network, multiple DAEs are stacked using greedy layer-wise training (Vincent et al., 2010).", "startOffset": 84, "endOffset": 106}, {"referenceID": 9, "context": "2 TRAINING DENOISING AUTOENCODERS For completeness, we detail here the procedure for training stacked denoising autoencoders described by Vincent et al. (2010). Fig.", "startOffset": 138, "endOffset": 160}, {"referenceID": 4, "context": "1 DATASETS We tested gradual training on three benchmark datasets: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009).", "startOffset": 73, "endOffset": 93}, {"referenceID": 6, "context": "7) (Polyak, 1964) and weight decay (10\u22123, 10\u22124, 10\u22125) (Moody et al., 1995).", "startOffset": 54, "endOffset": 74}, {"referenceID": 9, "context": "the validation set was sought in a semi-automatic fashion (as in Vincent et al. (2010)) by running experiments in parallel on a large computation cluster with manual guidance to avoid wasting resources on unnecessary parts of the configuration space.", "startOffset": 65, "endOffset": 87}], "year": 2014, "abstractText": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets.", "creator": "LaTeX with hyperref package"}}}