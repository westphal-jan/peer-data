{"id": "1412.6451", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer", "abstract": "Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although \"flat\" connectionist methods have already been used for model-based RL, up to now, only model-free variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment.", "histories": [["v1", "Fri, 19 Dec 2014 17:41:59 GMT  (54kb,D)", "http://arxiv.org/abs/1412.6451v1", "14 pages, 4 figures"]], "COMMENTS": "14 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["mark wernsdorfer", "ute schmid"], "accepted": false, "id": "1412.6451"}, "pdf": {"name": "1412.6451.pdf", "metadata": {"source": "META", "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer", "authors": ["Mark Wernsdorfer", "Ute Schmid"], "emails": ["ute.schmid}@uni-bamberg.de"], "sections": [{"heading": "1 INTRODUCTION", "text": "In most cases it is interpreted as the sensory perception of the agent. The second element A describes the amount of possible actions. The transition function determines transitions in the state area. It has the general form T: S \u00d7 A \u2192 S. A certain action in a certain state causes a certain succession state. Finally, the reward function gives a scalar value which acts as a reward. It shows whether a certain action must be repeated or avoided in a certain state. It has the general form T: S \u00d7 A \u2192 S. A certain action in a certain state causes a certain succession state. The reward function gives a scalar value which acts as a reward."}, {"heading": "1.1 SEMANTIC LOAD OF REPRESENTATIONS", "text": "The first problem concerns the semantic burden of predefined representations. The totality of world states, and thus the beliefs derived from them, can be considered representations of the environment. However, equipping an agent with such a fixed set of representations means injecting considerable amounts of knowledge into the system. It implicitly implies that these representations are suitable for solving the POMDP. Appropriate representations imply knowledge about how these representations are to be used. Knowing how to use representations, on the other hand, means already knowing how to handle the task. Bengio et al. (2013) mention the necessary trade-off between the \"wealth\" of representations and the effort required to process them. Diuk et al. (2008) show that the selection of representations has immense influence on the performance of RL algorithms. More specifically, they show that semantically rich representations (e.g. representations of objects) can considerably simplify a given task in comparison to sparse representations (e.g. of space)."}, {"heading": "1.2 TRANSFER OF KNOWLEDGE", "text": "This year it has reached the point where it will be able to leave the country it is in, unless it is able, it is able to take it in its hands, and it is able to take it in its hands to take it in its hands. \""}, {"heading": "2 RELATED WORK", "text": "Normally, the contrast between objective and subjective interaction is reduced to the difference between MDPs and POMDPs. However, in order to develop a machine learning algorithm from a suitable model of the problem to be solved, we consider it appropriate not to take the perspective of an observer but the agent's perspective on the problem. From the agent's perspective, there are no world states to begin with. First and foremost, beliefs are identical to the world perceived and influenced by the agent. Deviating from his views is a performance of highly developed animals and by no means a matter of course. POMDPs already imply a distinction between faith and world states by modelling a probability distribution (i.e. a belief) and its goals (i.e. world states) separately. POMDPs could therefore be the perfect model for the RL problem from the observer's perspective (i.e. for developing algorithms for objective interaction)."}, {"heading": "2.1 DEEP REPRESENTATION LEARNING", "text": "This insight goes back to the recent past, when the problem of grounding symbols was brought to the fore (Harnad, 1990). The semantic implications of representations have been recognized and explored in the field of representation, but the most famous application of representational experiences in the present is deep learning. Deep learning is usually done using connecting methods (e.g. limited machines, encoders, or artificial neural networks)."}, {"heading": "2.2 REINFORCEMENT LEARNING", "text": "RL is about optimizing behavior in unknown environments. Optimism is quantified as a reward function, but it provides an attractive follow-up function for the agent, indicating whether an action should be considered good or bad. To achieve optimal behavior over time, the agent optimizes not the immediate reward, but the cumulative reward he expects by following the current policy. This cumulative reward is deducted from future states, because immediate rewards may amount to more than those far in the future. Generally, such an assessment can be appreciated by remembering what action was followed in which sensory state, disregarding the discounted cumulative reward for future states."}, {"heading": "3 OVERVIEW", "text": "In the following, we present a rough outline of our approach to grounding deep models for RL. We present the general form of the functions we use. We implemented several agents according to this architecture. However, in this section, we only present the general structure in such a way that the individual methods remain interchangeable.We have shown that subjective interaction requires the disambiguation of seemingly identical states. To distinguish identical observations, additional information is necessary. However, in order to avoid semantic loads, they must be \"uncovered\" or \"generated\" in observable data. This hidden information can be modeled as latent states. In the belief of MDPs, the current state of the world is a latent state. However, in order to avoid semantic loads, we have determined that representations (i.e. the set of latent states) cannot be predefined, but must be derived from the dynamics of the interaction between agent and environment."}, {"heading": "3.1 LEARNING QUERY PROCESSES", "text": "The transitions between abstract representations can be described by the query function. The query function is analogous to the transition function T in MDPs. Note, however, that MDPs require a perceptual action tuple to determine a single succession function, while in an abstract representation various queries from abstract representations can succeed and / or fail. Q: L \u00b7 L \u00b7 L \u00b7 L \u2192 S = {>, VP} (3) The choice of an abstract representation, and thus the corresponding model, is not an MDP. The reason for this is that inducing a policy is not as easy as performing an action. Whether the model function of a queried latent state coincides with the subsequent story is not entirely under the control of the agent. Among abstract representations, there is no clear division of responsibilities between agent and environment as it is under perceptions and actions. Generally, transitions of representations follow query processes (QPs) that are defined by the query processes (QPs) (A)."}, {"heading": "3.2 ABSTRACTING QUERY PROCESSES", "text": "The linking of history with models enables abstract representations of concrete observations in the form of latent states q. These latent states model the abstract perception of the agent's environment: they represent the environment vis-\u00e0-vis the agent. A function for distinguishing a perceived history by the latent state, whose model best corresponds to the general FormA: H \u2192 L (6), where H = [et \u2212 n,.., et] is the current step in time and n is the length of history. This function realizes an abstract representation of a history that is referenced by the form of a latent state over the model contained therein. Therefore, we call it the \"abstraction function.\" The environment responds to queries by either accepting or rejecting the query, depending on whether the interaction history observed after a representation query actually corresponds to the corresponding model function. If it does so, the query succeeds."}, {"heading": "3.3 STACKING QUERY PROCESSES", "text": "In both cases, it is the case that they see themselves as being able to move and that they are able to move."}, {"heading": "4 IMPLEMENTATION", "text": "The value function estimates the discounted cumulative reward; the model function estimates which sensorimotor states can be queried; the Q query function in QPs replaces the transitional function T in MDPs; and finally, the abstraction function generalizes experience histories of sensorimotor states to high-level representations; the application function merely remembers which value and which model functions are associated with which abstract state. Therefore, we will not elaborate on it. We will present practical methods to approximate these functions.8"}, {"heading": "4.1 VALUE FUNCTION", "text": "Two of the most prominent are Q-Learning (originally Watkins, 1989) and SARSA-Learning (interestingly, originally in a connectionist context: Rummery & Niranjan, 1994); the former generates a value function that describes the optimal interaction policy (i.e., offline learning); the latter, however, generates a value function that describes the current interaction policy (i.e., online learning); consider an agent that performs his (arbitrarily chosen) actions only with a certain probability; in some cases, it performs a random action instead; the task is to learn to walk the shortest possible path along a straight cliff; Q-Agents learn to walk close to the edge because it is the shortest path with the most cumulative reward; however, if a random action occurs, they could fall and suffer large amounts of negative rewards. Q-Learning leads to learning.8In current research, we rate QPs as a means of hierarchical research."}, {"heading": "4.2 MODEL FUNCTION", "text": "The environment can be modelled by approximating the transition function for observable states. Transition probabilities are increased for actually observed transitions (see Gl. (12) and decreased for all other transitions, see Gl. (13). \u2212 T (st \u2212 1, at \u2212 1, st) \u00b7 T (st \u2212 1, at \u2212 1, st) + \u03b1 (1 \u2212 T (st \u2212 1, at \u2212 1, st))). \u2212 S (st \u2212 1, at \u2212 6 = st.T (st \u2212 1, at \u2212 1, s \u00b2) \u00b7 T (st \u2212 1, at \u2212 1, st \u2212)."}, {"heading": "4.3 ACTION SELECTION", "text": "If the value function makes it possible to evaluate perceptual-effect tuples, then the action selection simply returns the action a from a tupel, which maximizes the estimated value among all perceptual-effect tuples. Disadvantages are costs that are linear to the sum of perceptions and the number of actions. Equation (17) shows the decision function in conventional model-based and model-free RL during the perception phase st.D (st) = a V (st, a) = S \u00b7 A max st, a \u2032 V (st, a \u2032) (17) Instead of the probability of succession states, the query selection must take into account the \"inductability\" of potential succession states. The approach also performs a simple maximization. However, its specific form differs significantly from the conventional action selection in RL. The decision function implements the query selection, which is determined by the value function and the approximation of the model function. (D)"}, {"heading": "5 RESULTS", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they are able to"}, {"heading": "6 FUTURE WORK", "text": "The main challenge of extruding a grounded query policy into a hierarchical architecture, therefore, is to find an appropriate abstraction function A that reliably and repeatedly provides the same models that contain only linear histories of them. However, the fact that the dimensionality of history is increasingly smaller or equal to the dimensionality of the model makes reliable comparisons difficult. History may not contain the information relevant to deciding whether it corresponds to a model or another. Additional information must be acquired to resolve ambiguities. However, it is clear that this information cannot be extracted from the structure of history or the model. Learning to represent may be able to generalize stories, but any generalization that takes into account only the structure of history and the model is confronted with sub-specific examples: it cannot be decided which structurally similar set of models a history belongs to. In cognitive science, a similar situation poses a problem in explaining the universality of human objectivity."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Brainstorms: Philosophical essays on mind and psychology", "author": ["Dennett", "Daniel Clement"], "venue": "Number 8. MIT press,", "citeRegEx": "Dennett and Clement.,? \\Q1981\\E", "shortCiteRegEx": "Dennett and Clement.", "year": 1981}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "CoRR, cs.LG/9905014,", "citeRegEx": "Dietterich and G.,? \\Q1999\\E", "shortCiteRegEx": "Dietterich and G.", "year": 1999}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["Diuk", "Carlos", "Cohen", "Andre", "Littman", "Michael L"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Diuk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Diuk et al\\.", "year": 2008}, {"title": "A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification", "author": ["Feldkamp", "Lee A", "Puskorius", "Gintaras V"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Feldkamp et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Feldkamp et al\\.", "year": 1998}, {"title": "Using reinforcement learning to adapt an imitation task", "author": ["Guenter", "Florent", "Billard", "Aude G"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Guenter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Guenter et al\\.", "year": 2007}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "The symbol grounding problem", "author": ["Harnad", "Stevan"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad and Stevan.,? \\Q1990\\E", "shortCiteRegEx": "Harnad and Stevan.", "year": 1990}, {"title": "Discovering hierarchy in reinforcement learning with hexq", "author": ["Hengst", "Bernhard"], "venue": "In ICML,", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Kaelbling", "Leslie Pack", "Littman", "Michael L", "Cassandra", "Anthony R"], "venue": "Artificial intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Lin", "Long-Ji"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin and Long.Ji.,? \\Q1993\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1993}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "Computer Science Department Faculty Publication Series, pp", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "Roles of macro-actions in accelerating reinforcement learning", "author": ["McGovern", "Amy", "Sutton", "Richard S", "Fagg", "Andrew H"], "venue": "In Grace Hopper celebration of women in computing,", "citeRegEx": "McGovern et al\\.,? \\Q1997\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 1997}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Identifying hierarchical structure in sequences: A linear-time algorithm", "author": ["Nevill-Manning", "Craig G", "Witten", "Ian H"], "venue": "CoRR, cs.AI/9709102,", "citeRegEx": "Nevill.Manning et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Nevill.Manning et al\\.", "year": 1997}, {"title": "On-line Q-learning using connectionist systems", "author": ["Rummery", "Gavin A", "Niranjan", "Mahesan"], "venue": "University of Cambridge, Department of Engineering,", "citeRegEx": "Rummery et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Rummery et al\\.", "year": 1994}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1990\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1990}, {"title": "Deep learning in neural networks: An overview", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1404.7828,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2014\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2014}, {"title": "Minds, brains, and programs", "author": ["Searle", "John R"], "venue": "Behavioral and brain sciences,", "citeRegEx": "Searle and R.,? \\Q1980\\E", "shortCiteRegEx": "Searle and R.", "year": 1980}, {"title": "Convergence results for single-step on-policy reinforcement-learning algorithms", "author": ["Singh", "Satinder", "Jaakkola", "Tommi", "Littman", "Michael L", "Szepesv\u00e1ri", "Csaba"], "venue": "Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Finding structure in reinforcement learning", "author": ["Thrun", "Sebastian", "Schwartz", "Anton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Thrun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1995}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "Christopher John Cornish Hellaby"], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "Watkins and Hellaby.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and Hellaby.", "year": 1989}], "referenceMentions": [{"referenceID": 9, "context": "(Kaelbling et al., 1998) POMDPs can describe a plethora of tasks for RL agents.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Bengio et al. (2013) mention the necessary trade-off between the \u201crichness\u201d of representations and the effort necessary to process them.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2013) mention the necessary trade-off between the \u201crichness\u201d of representations and the effort necessary to process them. Diuk et al. (2008) show that the selection of representations has immense influence on the performance of RL algorithms.", "startOffset": 0, "endOffset": 156}, {"referenceID": 13, "context": ", a policy) go back at least to the early 90ies (Schmidhuber, 1990; Schmidhuber, 1991; Feldkamp & Puskorius, 1998), but only recently, RL methods have been presented that are efficiently able to generate deep policies from scratch (Mnih et al., 2013; Guo et al., 2014).", "startOffset": 231, "endOffset": 268}, {"referenceID": 6, "context": ", a policy) go back at least to the early 90ies (Schmidhuber, 1990; Schmidhuber, 1991; Feldkamp & Puskorius, 1998), but only recently, RL methods have been presented that are efficiently able to generate deep policies from scratch (Mnih et al., 2013; Guo et al., 2014).", "startOffset": 231, "endOffset": 268}, {"referenceID": 12, "context": "Therefore, semi MDPs are frequently used to model such time-sensitivity (Thrun & Schwartz, 1995; McGovern et al., 1997; Dietterich, 1999; McGovern & Barto, 2001; Hengst, 2002).", "startOffset": 72, "endOffset": 175}, {"referenceID": 20, "context": "This generalization of MDPs extends the model of state transitions by duration as proposed by Sutton et al. (1999). The structure of hierarchical models, however, is mostly trained: stochastic information is integrated into a fixed hierarchical structure.", "startOffset": 94, "endOffset": 115}], "year": 2014, "abstractText": "Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although \u201cflat\u201d connectionist methods have already been used for model-based RL, up to now, only modelfree variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment.", "creator": "LaTeX with hyperref package"}}}