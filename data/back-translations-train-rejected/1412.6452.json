{"id": "1412.6452", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Algorithmic Robustness for Learning via $(\\epsilon, \\gamma, \\tau)$-Good Similarity Functions", "abstract": "The importance of metrics in machine learning has attracted a growing interest for distance and similarity learning, and especially the Mahalanobis distance. However, it is worth noting that this research field lacks theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a learned metric. The theoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend this theory to a setting where the metric and the separator are jointly learned in a semi-supervised way. We furthermore provide a generalization bound for the associated classifier based on the algorithmic robustness framework. The behavior of our method is illustrated via some experimental results.", "histories": [["v1", "Fri, 19 Dec 2014 17:43:26 GMT  (106kb,D)", "http://arxiv.org/abs/1412.6452v1", "ICLR 2015 conference submission - under review"], ["v2", "Fri, 27 Feb 2015 15:36:14 GMT  (196kb,D)", "http://arxiv.org/abs/1412.6452v2", "ICLR 2015 conference submission - under review"], ["v3", "Tue, 31 Mar 2015 11:10:43 GMT  (27kb)", "http://arxiv.org/abs/1412.6452v3", "ICLR 2015 Workshop - accepted"]], "COMMENTS": "ICLR 2015 conference submission - under review", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maria-irina nicolae", "marc sebban", "amaury habrard", "\\'eric gaussier", "massih-reza amini"], "accepted": false, "id": "1412.6452"}, "pdf": {"name": "1412.6452.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Amaury Habrard", "Hubert Curien"], "emails": ["Maria.Irina.Nicolae@univ-st-etienne.fr", "Marc.Sebban@univ-st-etienne.fr", "Amaury.Habrard@univ-st-etienne.fr", "Eric.Gaussier@imag.fr", "Massih-Reza.Amini@imag.fr"], "sections": [{"heading": "1 INTRODUCTION", "text": "Many researchers have used the underlying geometry of the data to improve classification algorithms, e.g. by learning Mahanalobis distances instead of the usual Euclidean distance, paving the way for a new field of research called metric learning (Bellet et al., 2013). However, most of these studies have based their approaches on distance learning (Baoli et al., 2004; Davis et al., 2007; Diligenti et al., 2003; Shalev-Shwartz et al., 2004; Weinberger & Saul, 2009), although similarity learning has also aroused a growing interest (Bao et al., 2003); Grabowski & Sza\u0142as (2005); Hust (2004); Qamar & Gaussier (2009)), the rationality that cosmic similarity should be preferred in some cases."}, {"heading": "2 NOTATIONS AND RELATED WORK", "text": "We define vectors by lowercase letters (x) and matrices by upper case bold symbols (A). Consider the following learning problem: We gain access to marked examples z = (x, l (x)) drawn by an unknown distribution P over X \u00d7 Y, where X Rd and Y = {\u2212 1} are each instance and output spaces. We denounce the L1 norm of | | 1, the L2 norm of | 2, and the Frobenius norm of | \u00b7 F. Metric learning aims to find the parameters of a distance or similarity that best satisfies the underlying geometry of the data. This information is usually expressed as a pair (x and x) or similar."}, {"heading": "3 LEARNING CONSISTENT GOOD SIMILARITY FUNCTIONS", "text": "In this section, we present our semi-supervised framework for the joint learning of a similarity function and a linear separator of data, and present a generalization of our approach based on the algorithmic robustness framework recently proposed by Xu & Mannor (2010; 2012). We conclude this section by introducing some specific similarity functions that can be used in our environment."}, {"heading": "3.1 OPTIMIZATION PROBLEM", "text": "Let S be an example set of examples labeled dl (x, l (x)). Let KA (x, x). Let KA (x, x). Indeed, the empirical loss of a finite sample is a reasonable (, \u03b3, \u03c4) -good similarity function, parameterized by the matrix A, Rd \u00b7 d. We want to optimize the quality of KA w.r.t. the empirical loss of a finite sample. To achieve this, we need to find the matrix A and the global separator \u03b1 Rdu, which minimize the loss function (in our case, the hinge loss) compared to the training set S. Our learning algorithm takes the form of the following limited optimization problem. min \u03b1, A1dl dl dl dl dl (i = 1 \u2212 du j = 1) l (xi, xj)."}, {"heading": "3.2 CONSISTENCY GUARANTEES", "text": "We now present a theoretical analysis of our approach. To discuss the algorithmic robustness of the method (KA = KA = KA), let us describe the minimization problem (4) with a generalized notation of the loss function. (A, \u03b1, zi = (xi, l (xi)))), where \"A, \u03b1, zi = (xi, l (xi),\" \"\" X, \"\" X, \"\" Z, \"\" \"\" X, \"\" \"X,\" \"\" \"X,\" \"\" X, \"\" \"X,\" \"\" X, \"\" \"X,\" \"\" X, \"\" \"X\" and \"X,\" \"\" X, \"\" \"X,\" \"\" Z, \"\" \"that,\" \"and,\" \"we,\" \"and\" that. (A, xi, l \"),\" \"X,\" \"and,\" that, \"and,\" and \"we,\" and \"that,\" and \"X,\" and \"that,\" and \"we,\" and \"that."}, {"heading": "3.3 ROBUSTNESS ANALYSIS FOR DIFFERENT SIMILARITY FUNCTIONS", "text": "Our main theorems depend heavily on the l-lipschitzness of the similarity function. In this section, we focus on some specific similarities that can be used in our environment. Evidence for the following functions is detailed in the appendix. Similarity function 1. Let K1A be the bilinear form K1A (x, x \u2032) = xTAx \u2032. K1A (x, x \u2032) is 1-lipschitz w.r.t. its first argument. Similarity function 2. Let us define K2A (x, x \u2032) = 1 \u2212 (x \u2032) TA (x \u2212 x \u2032), a similarity derived from the Mahalanobis distance. K2A (x, x \u2032) is 4-lipschitz w.r.t. its first reasoning. Let K3A (x, x \u2032) = exp (\u2212 x) = exp (\u2212 x \u00b2), exp = exp."}, {"heading": "4 EXPERIMENTS", "text": "Most of them optimize a metric adapted to the k-NN classification (e.g. LMNN, ITML), while our work is focused on the search for a global linear delimiter. Therefore, it is difficult to propose a fair comparative study. Therefore, we propose to compare our method with the algorithm of Balcan et al. (2008), which will play the role of a baseline. We are conducting the experimental study on 7 classic data sets from the UCI Machine Learning Repository, which are both binary and multi-level. Their properties are presented in Table 1. These data sets are frequently used for metric learning evaluation."}, {"heading": "4.1 EXPERIMENTAL SETUP", "text": "We compare the following methods: the algorithm of Balcan et al. (2008) as a baseline (henceforth referred to as BBS) and the problem (4) with constraints (5), (6) and (7) that we will name JSL (for Joint Similarity Learning). In these methods we include the three similarity functions previously examined (see Section 3.3), giving us six settings. Since setting BBS does not contain a metric learning step, the metric A is set to the identity matrix, which is equivalent to the use of standardized, non-parameterized similarity functions. All attributes are centered and scaled zero to ensure that | x | | 2 \u2264 1, as this constraint is necessary for both algorithms. We randomly select 15% of the data for validation purposes, and another 15% as a test set. The training set and the blank data are selected from the remaining 70% of the examples not used in the previous sets."}, {"heading": "4.2 RESULTS", "text": "We report on the results of the linear classification in Tables 2 and 3. The best result for the pairwise comparison between the two methods that use the same similarity function is marked in bold. If we use all available data as (unlabeled) boundary stones (Table 2), our method achieves the best results in 15 of 21 settings and a tie in one configuration. Table 3 summarizes the results that are achieved when the unlabeled data set contains only 15 points. In this case, our technique delivers the best accuracy in 11 of 21 settings and similar results in two other cases. If you want to compare the similarity in the same settings, we note that in most cases K2A delivers the best results for both tables. This is surprising considering that the cosine similarity is K1A 1 lipstick and the resulting generalization is limited because K 2A is 4 lipstick."}, {"heading": "5 CONCLUSIONS", "text": "We show that our common approach is theoretically based on results from Balcan et al. (2008) and new results based on algorithmic robustness. This setting is designed to learn well with a limited amount of labeled data and works well in practice on different UCI datasets. Future work could cover a kernel-based version of our technique to learn more efficient similarities and classifiers in a non-linear feature pace, as well as local metrics and their combination in a coherent framework."}], "references": [{"title": "Improved guarantees for learning via similarity functions", "author": ["Balcan", "M.-F", "A. Blum", "N. Srebro"], "venue": "Omnipress,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Quick asymmetric text similarity measures", "author": ["Bao", "J.-P", "Shen", "J.-Y", "Liu", "X.-D", "H.-Y"], "venue": "ICMLC,", "citeRegEx": "Bao et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bao et al\\.", "year": 2003}, {"title": "An adaptive k-nearest neighbor text categorization strategy", "author": ["L. Baoli", "L. Qin", "Y. Shiwen"], "venue": "ACM TALIP,", "citeRegEx": "Baoli et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baoli et al\\.", "year": 2004}, {"title": "Similarity learning for provably accurate sparse linear classification", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "In ICML, pp", "citeRegEx": "Bellet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2012}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709,", "citeRegEx": "Bellet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2013}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "Zien", "A. (eds"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Learning similarities for text documents using neural networks", "author": ["M. Diligenti", "M. Maggini", "L. Rigutini"], "venue": "In ANNPR,", "citeRegEx": "Diligenti et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Diligenti et al\\.", "year": 2003}, {"title": "A technique for learning similarities on complex structures with applications to extracting ontologies. In AWIC, LNAI", "author": ["M. Grabowski", "A. Sza\u0142as"], "venue": null, "citeRegEx": "Grabowski and Sza\u0142as,? \\Q2005\\E", "shortCiteRegEx": "Grabowski and Sza\u0142as", "year": 2005}, {"title": "Guaranteed classification via regularized similarity learning", "author": ["Guo", "Z.-C", "Y. Ying"], "venue": "CoRR, abs/1306.3108,", "citeRegEx": "Guo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2013}, {"title": "Learning Similarities for Collaborative Information Retrieval", "author": ["A. Hust"], "venue": "In Proceedings of KI 2004 workshop \u201dMachine Learning and Interaction for Text-Based Information Retrieval\u201d,", "citeRegEx": "Hust,? \\Q2004\\E", "shortCiteRegEx": "Hust", "year": 2004}, {"title": "entropy and -capacity of sets in functional spaces", "author": ["A. Kolmogorov", "V. Tikhomirov"], "venue": "American Mathematical Society Translations,", "citeRegEx": "Kolmogorov and Tikhomirov,? \\Q1961\\E", "shortCiteRegEx": "Kolmogorov and Tikhomirov", "year": 1961}, {"title": "Online and batch learning of generalized cosine similarities", "author": ["A.M. Qamar", "\u00c9. Gaussier"], "venue": "In ICDM, pp", "citeRegEx": "Qamar and Gaussier,? \\Q2009\\E", "shortCiteRegEx": "Qamar and Gaussier", "year": 2009}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2004}, {"title": "Weak Convergence and Empirical Processes", "author": ["A. van der Vaart", "J. Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner,? \\Q1996\\E", "shortCiteRegEx": "Vaart and Wellner", "year": 1996}, {"title": "Fast solvers and efficient implementations for distance metric learning", "author": ["K. Weinberger", "L. Saul"], "venue": "In ICML,", "citeRegEx": "Weinberger and Saul,? \\Q2008\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2008}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "JMLR, 10:207\u2013244,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Robustness and generalization", "author": ["H. Xu", "S. Mannor"], "venue": "In COLT, pp", "citeRegEx": "Xu and Mannor,? \\Q2010\\E", "shortCiteRegEx": "Xu and Mannor", "year": 2010}, {"title": "Robustness and generalization", "author": ["H. Xu", "S. Mannor"], "venue": "Machine Learning,", "citeRegEx": "Xu and Mannor,? \\Q2012\\E", "shortCiteRegEx": "Xu and Mannor", "year": 2012}, {"title": "1-norm support vector machines", "author": ["J. Zhu", "S. Rosset", "T. Hastie", "R. Tibshirani"], "venue": "In NIPS, pp", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "The theoretical framework of ( , \u03b3, \u03c4)-good similarity functions (Balcan et al., 2008) has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it.", "startOffset": 65, "endOffset": 86}, {"referenceID": 4, "context": "by learning Mahanalobis distances instead of the standard Euclidean distance, thus paving the way for a new research area termed metric learning (Bellet et al., 2013).", "startOffset": 145, "endOffset": 166}, {"referenceID": 2, "context": "Most of these studies have based their approaches on distance learning (Baoli et al., 2004; Davis et al., 2007; Diligenti et al., 2003; Shalev-Shwartz et al., 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al.", "startOffset": 71, "endOffset": 189}, {"referenceID": 6, "context": "Most of these studies have based their approaches on distance learning (Baoli et al., 2004; Davis et al., 2007; Diligenti et al., 2003; Shalev-Shwartz et al., 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al.", "startOffset": 71, "endOffset": 189}, {"referenceID": 7, "context": "Most of these studies have based their approaches on distance learning (Baoli et al., 2004; Davis et al., 2007; Diligenti et al., 2003; Shalev-Shwartz et al., 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al.", "startOffset": 71, "endOffset": 189}, {"referenceID": 13, "context": "Most of these studies have based their approaches on distance learning (Baoli et al., 2004; Davis et al., 2007; Diligenti et al., 2003; Shalev-Shwartz et al., 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al.", "startOffset": 71, "endOffset": 189}, {"referenceID": 19, "context": "Their algorithm, whose formulation is equivalent to a relaxed L1-norm SVM (Zhu et al., 2003), does not enforce the positive definiteness constraint of the similarity.", "startOffset": 74, "endOffset": 92}, {"referenceID": 0, "context": "(2012) have explored the possibility of independently learning an ( , \u03b3, \u03c4)-good similarity that they plug into the initial algorithm (Balcan et al., 2008) to learn the linear separator.", "startOffset": 134, "endOffset": 155}, {"referenceID": 0, "context": ", 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al. (2003); Grabowski & Sza\u0142as (2005); Hust (2004); Qamar & Gaussier (2009)), the rationale being that the cosine similarity should in some cases be preferred over the Euclidean distance.", "startOffset": 105, "endOffset": 123}, {"referenceID": 0, "context": ", 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al. (2003); Grabowski & Sza\u0142as (2005); Hust (2004); Qamar & Gaussier (2009)), the rationale being that the cosine similarity should in some cases be preferred over the Euclidean distance.", "startOffset": 105, "endOffset": 150}, {"referenceID": 0, "context": ", 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al. (2003); Grabowski & Sza\u0142as (2005); Hust (2004); Qamar & Gaussier (2009)), the rationale being that the cosine similarity should in some cases be preferred over the Euclidean distance.", "startOffset": 105, "endOffset": 163}, {"referenceID": 0, "context": ", 2004; Weinberger & Saul, 2009), even though similarity learning has also attracted a growing interest (Bao et al. (2003); Grabowski & Sza\u0142as (2005); Hust (2004); Qamar & Gaussier (2009)), the rationale being that the cosine similarity should in some cases be preferred over the Euclidean distance.", "startOffset": 105, "endOffset": 188}, {"referenceID": 0, "context": "More recently, Balcan et al. (2008) have proposed the first framework that allows one to relate similarities with a classification algorithm making use of them.", "startOffset": 15, "endOffset": 36}, {"referenceID": 0, "context": "More recently, Balcan et al. (2008) have proposed the first framework that allows one to relate similarities with a classification algorithm making use of them. This framework, that is very general as it can be used with any bounded similarity function (potentially derived from a distance), provides generalization guarantees on a linear classifier learned from the similarity. Their algorithm, whose formulation is equivalent to a relaxed L1-norm SVM (Zhu et al., 2003), does not enforce the positive definiteness constraint of the similarity. However, to enjoy such generalization guarantees, the similarity function is assumed to be known beforehand and to satisfy ( , \u03b3, \u03c4)-goodness properties. Unfortunately, Balcan et al. (2008) do not provide any algorithm for learning such similarities.", "startOffset": 15, "endOffset": 736}, {"referenceID": 0, "context": "More recently, Balcan et al. (2008) have proposed the first framework that allows one to relate similarities with a classification algorithm making use of them. This framework, that is very general as it can be used with any bounded similarity function (potentially derived from a distance), provides generalization guarantees on a linear classifier learned from the similarity. Their algorithm, whose formulation is equivalent to a relaxed L1-norm SVM (Zhu et al., 2003), does not enforce the positive definiteness constraint of the similarity. However, to enjoy such generalization guarantees, the similarity function is assumed to be known beforehand and to satisfy ( , \u03b3, \u03c4)-goodness properties. Unfortunately, Balcan et al. (2008) do not provide any algorithm for learning such similarities. In order to overcome these limitations, Bellet et al. (2012) have explored the possibility of independently learning an ( , \u03b3, \u03c4)-good similarity that they plug into the initial algorithm (Balcan et al.", "startOffset": 15, "endOffset": 858}, {"referenceID": 0, "context": "More recently, Balcan et al. (2008) have proposed the first framework that allows one to relate similarities with a classification algorithm making use of them. This framework, that is very general as it can be used with any bounded similarity function (potentially derived from a distance), provides generalization guarantees on a linear classifier learned from the similarity. Their algorithm, whose formulation is equivalent to a relaxed L1-norm SVM (Zhu et al., 2003), does not enforce the positive definiteness constraint of the similarity. However, to enjoy such generalization guarantees, the similarity function is assumed to be known beforehand and to satisfy ( , \u03b3, \u03c4)-goodness properties. Unfortunately, Balcan et al. (2008) do not provide any algorithm for learning such similarities. In order to overcome these limitations, Bellet et al. (2012) have explored the possibility of independently learning an ( , \u03b3, \u03c4)-good similarity that they plug into the initial algorithm (Balcan et al., 2008) to learn the linear separator. Generalization bounds for the learned similarity (a bilinear similarity) were derived via uniform stability arguments (Bousquet & Elisseeff, 2002). However, despite good results in practice, one limitation of this framework is that it imposes to deal with strongly convex objective functions. More importantly, the similarity learning step is done in a completely supervised way while Balcan et al. (2008)\u2019s framework opens the door to the use of unlabeled data.", "startOffset": 15, "endOffset": 1444}, {"referenceID": 0, "context": "Enforcing ( , \u03b3, \u03c4)-goodness allows us to preserve (Balcan et al., 2008)\u2019s theoretical guarantees.", "startOffset": 51, "endOffset": 72}, {"referenceID": 0, "context": "In this paper, we aim at better exploiting the semi-supervised setting underlying the theoretical framework of Balcan et al. (2008), which is based on similarities between labeled data and unlabeled reasonable points (roughly speaking, the reasonable points play the same role as that of support vectors in SVMs).", "startOffset": 111, "endOffset": 132}, {"referenceID": 0, "context": "In this paper, we aim at better exploiting the semi-supervised setting underlying the theoretical framework of Balcan et al. (2008), which is based on similarities between labeled data and unlabeled reasonable points (roughly speaking, the reasonable points play the same role as that of support vectors in SVMs). Furthermore, and unlike Bellet et al. (2012), we propose here to jointly learn the metric and the classifier, so that both the metric and the separator are learned in a semisupervised way.", "startOffset": 111, "endOffset": 359}, {"referenceID": 6, "context": "One can partly get around this latter shortcoming by making use of specific solvers or using information-theoretic approaches, such as ITML (Davis et al., 2007).", "startOffset": 140, "endOffset": 160}, {"referenceID": 0, "context": "In this context, Balcan et al. (2008) introduced a theory for learning with so called ( , \u03b3, \u03c4)-good similarity functions based on non PSD matrices.", "startOffset": 17, "endOffset": 38}, {"referenceID": 0, "context": "(Balcan et al., 2008) K is a ( , \u03b3, \u03c4)-good similarity function in hinge loss for a learning problem P if there exists a random indicator function R(x) defining a probabilistic set of \u201dreasonable points\u201d such that the following conditions hold:", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Balcan et al., 2008) Let K be an ( , \u03b3, \u03c4)-good similarity function in hinge loss for a learning problem P.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "The main limitation of this approach is that the similarity function K is predefined and Balcan et al. (2008) did not provide any learning algorithm to design ( , \u03b3, \u03c4)-good similarities.", "startOffset": 89, "endOffset": 110}, {"referenceID": 0, "context": "The main limitation of this approach is that the similarity function K is predefined and Balcan et al. (2008) did not provide any learning algorithm to design ( , \u03b3, \u03c4)-good similarities. This problem has been fixed by Bellet et al. (2012) who optimized the ( , \u03b3, \u03c4)-goodness of a bilinear similarity function under Frobenius norm regularization.", "startOffset": 89, "endOffset": 240}, {"referenceID": 0, "context": "It is worth noticing that Equation (3) optimizes an empirical version of the notion of ( , \u03b3, \u03c4)goodness of Balcan et al. (2008). Moreover, since the random indicator function R(x) defining the probabilistic set of reasonable points (as defined in Equation (1)) is unknown, Bellet et al.", "startOffset": 108, "endOffset": 129}, {"referenceID": 0, "context": "It is worth noticing that Equation (3) optimizes an empirical version of the notion of ( , \u03b3, \u03c4)goodness of Balcan et al. (2008). Moreover, since the random indicator function R(x) defining the probabilistic set of reasonable points (as defined in Equation (1)) is unknown, Bellet et al. (2012) resort to an additional set of dr labeled points that are also used during the classifier learning step.", "startOffset": 108, "endOffset": 295}, {"referenceID": 2, "context": "More recently, Guo & Ying (2013) extended the theoretical results of Bellet et al. (2012). Using the Rademacher complexity (instead of the uniform stability) and Khinchin-type inequalities, they derive generalization bounds for similarity learning formulations that are regularized w.", "startOffset": 69, "endOffset": 90}, {"referenceID": 0, "context": "This allows us to make use of the semi-supervised setting presented by Balcan et al. (2008) to learn well with only a small amount of labeled data.", "startOffset": 71, "endOffset": 92}, {"referenceID": 0, "context": "Constraint (5) takes into account the desired margin \u03b3 and is the same as in Balcan et al. (2008). Constraints (6) and (7) come to restrict the similarity KA, as it is a generic form and its bounds are not known.", "startOffset": 77, "endOffset": 98}, {"referenceID": 5, "context": "Note that with this construction the \u03c1-covers verify the cluster assumption used in semi-supervised learning (Chapelle et al., 2006).", "startOffset": 109, "endOffset": 132}, {"referenceID": 0, "context": "Therefore, we propose to compare our method to Balcan et al. (2008)\u2019s algorithm which will play the role of a baseline.", "startOffset": 47, "endOffset": 68}, {"referenceID": 0, "context": "We compare the following methods: the algorithm from Balcan et al. (2008) as a baseline (referred to as BBS from now on) and problem (4) with constraints (5), (6) and (7), that we will name JSL (for Joint Similarity Learning).", "startOffset": 53, "endOffset": 74}], "year": 2017, "abstractText": "The importance of metrics in machine learning has attracted a growing interest for distance and similarity learning, and especially the Mahalanobis distance. However, it is worth noting that this research field lacks theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a learned metric. The theoretical framework of ( , \u03b3, \u03c4)-good similarity functions (Balcan et al., 2008) has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend this theory to a setting where the metric and the separator are jointly learned in a semi-supervised way. We furthermore provide a generalization bound for the associated classifier based on the algorithmic robustness framework. The behavior of our method is illustrated via some experimental results.", "creator": "LaTeX with hyperref package"}}}