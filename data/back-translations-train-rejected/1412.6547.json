{"id": "1412.6547", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Fast Label Embeddings via Randomized Linear Algebra", "abstract": "Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers fast label embedding algorithms and provides us with a unifying view of label embeddings in the multiclass and multilabel settings. Leveraging techniques from randomized linear algebra, the running time of our label embedding algorithm is exponentially faster than naive algorithms. Finally, we demonstrate our techniques on two datasets from the Large Scale Hierarchical Text Challenge and the Open Directory Project where we obtain state of the art results.", "histories": [["v1", "Fri, 19 Dec 2014 22:09:35 GMT  (81kb,D)", "https://arxiv.org/abs/1412.6547v1", "In submission for ICLR 2015"], ["v2", "Fri, 27 Feb 2015 23:29:44 GMT  (78kb,D)", "http://arxiv.org/abs/1412.6547v2", "In submission for ICLR 2015"], ["v3", "Mon, 23 Mar 2015 16:11:14 GMT  (78kb,D)", "http://arxiv.org/abs/1412.6547v3", "Accepted as a workshop contribution at ICLR 2015"], ["v4", "Mon, 30 Mar 2015 23:24:53 GMT  (78kb,D)", "http://arxiv.org/abs/1412.6547v4", null], ["v5", "Mon, 13 Apr 2015 00:29:44 GMT  (78kb,D)", "http://arxiv.org/abs/1412.6547v5", null], ["v6", "Mon, 15 Jun 2015 18:07:20 GMT  (67kb,D)", "http://arxiv.org/abs/1412.6547v6", "To appear in the proceedings of the ECML/PKDD 2015 conference"], ["v7", "Sun, 5 Jul 2015 15:38:11 GMT  (67kb,D)", "http://arxiv.org/abs/1412.6547v7", "To appear in the proceedings of the ECML/PKDD 2015 conference. Reference implementation available atthis https URL"]], "COMMENTS": "In submission for ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["paul mineiro", "nikos karampatziakis"], "accepted": false, "id": "1412.6547"}, "pdf": {"name": "1412.6547.pdf", "metadata": {"source": "CRF", "title": "Fast Label Embeddings via Randomized Linear Algebra", "authors": ["Paul Mineiro", "Nikos Karampatziakis"], "emails": ["pmineiro@microsoft.com", "nikosk@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, many multi-level and multi-level datasets have emerged with an increasing number of possible labels, such as ImageNet [12] and the Large Scale Hierarchical Text Classification (LSHTC) datasets [25]. It could be argued that all problems of vision and language in the wild have extremely large output spaces. If the number of possible output points is modest, multi-level and multi-level problems can be addressed directly (via a maximum or softmax layer) or with a reduction to binary classification. However, if the output space is large, these strategies are too general and do not fully exploit some of the common characteristics that these problems have. For example, the alternatives in output space exhibit different similarities between them, so typical examples from similar classes tend to be closer to each other than from unequal classes."}, {"heading": "1.1 Contributions", "text": "We motivate a particular label embedding, which is defined by the slight approximation of a particular matrix, based on a correspondence between the label embedding and the optimal ranking estimator of the smallest squares. Assuming feasibility and infinite data, the matrix to be decomposed is the expected outer product of the conditional label probabilities. In particular, this indicates that two labels are similar if their conditional probabilities are linearly dependent on the dataset. This unifies previous work that used the confusion matrix for multiclass [5] and empirical label covariance for multi-label [42]. We apply techniques from randomized linear algebra [19] to develop an efficient and scalgorithm for constructing the embedding, essentially using a novel randomized algorithm for ranged quadrified losses, to comply with the first steps of our intuitive decompression technique."}, {"heading": "2 Algorithm Derivation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "We denote vectors by lowercase letters x, y, etc. and matrices by uppercase letters W, Z, etc. The input dimension is denoted by d, the output dimension by c, and the embedding dimension by k. In case of multi-class problems, y is a uniform (line) vector (i.e. a vertex of the c \u2212 1 unit simplex), while in case of multi-label problems y is a binary vector (i.e. a vertex of the unit c cube). For a m \u00b7 n matrix X-Rm \u00b7 n, we use | X | | F for its Frobenius standard, X \u2020 for the pseudo-inverse, \u0445X, L for the projection onto the left singular subspace of X, and X1: k for the matrix that results by denoting the first k columns of X. We use X \u0445 to denote a varirix that is denoted by solving an optimization problem over the matrix random expectation of XE."}, {"heading": "2.2 Background", "text": "In this section we offer an informal discussion of randomized algorithms to approach the main components analysis of a data matrix X > Rn \u00b7 dAlgorithm 1 Randomized PCA1: Function RPCA (k, X, Rn \u00b7 d) 2: (p, q) \u2190 (20, 1) These hyperparameters rarely need to be adjusted. 3: Q \u2190 randomized (d, k + p) 4: for i, q} do. Randomized Range Finder for X > X > 5: X > XQ."}, {"heading": "2.3 Rank-Constrained Estimation and Embedding", "text": "Suppose we are looking for an optimal square loss predictor for a high-profile target matrix. (...) Suppose we are looking for an optimal square loss forecast for a high-profile target matrix. (...) Suppose we are looking for an optimal square loss forecast for a high-profile target matrix. (...) Suppose we have a low rate for a low rate. (...) Suppose we have a low rate for a low rate. (...) Suppose we have a low rate for a low rate. (...) Suppose we have a low rate for a low rate. (...) Suppose we have a low rate for a low rate. (...) Suppose we have a low rate for a low rate. (...) Suppose we have a low rate for a low rate."}, {"heading": "2.4 Rembrandt", "text": "Our proposal is Rembrandt, which is described in Algorithm 2. In the previous section we have the use of the top-right singular space of hecX, LY embedded as a label, or equivalent, the top principle components of Y > i, LY (the use of the projection is ideal).Let us use this matrix without explicitly forming it, because we can calculate the product of A > X, LY with another matrix Q via Equation 2. Algorithm 2 is a specialization of the randomized PCA on this particular form of matrix multiplication."}, {"heading": "3 Related Work", "text": "In fact, most of the people who have chosen to defend the rights of women and men over the past few years are able to defend themselves and their rights, most of them are able to defend themselves, most of them are able to defend themselves, most of them are able to defend themselves, most of them are able to take themselves and themselves into their own hands, most of them are able to save themselves, most of them are able to save themselves, most of them are able to save themselves, most of them are able to save themselves, most of them are able to save themselves, most of them are not able to save themselves, most of them are able to save themselves."}, {"heading": "4 Experiments", "text": "The aim of these experiments is to demonstrate the computational capability and statistical utility of the embedding algorithm, not to advocate a particular classification algorithm per se. We use classification tasks for demonstration and use our embedding strategy as part of Algorithm 3, but focus our attention on the impact of embedding on the result. In Table 1 we present some statistics on the data sets used in this section, as well as the times required to calculate an embedding of the data set. Unless otherwise specified, all timings presented in the Experiments section refer to a Matlab implementation running on a standard desktop that has two 3.2 Ghz Xeon E5-1650 CPUs and 48 GB of RAM."}, {"heading": "4.1 ALOI", "text": "In fact, it is a matter of a way in which people move in the world in a way in which they put themselves and themselves at the centre of it, as they do. (...) In fact, it is a matter of people being able to put themselves and themselves at the centre of it. (...) It is not a matter of them being able to put themselves at the centre of it. (...) It is not a matter of them being able to afford it. \"(...)\" It is as if they feel able to change the world. \"(...) It is as if they are able to change the world of the world themselves.\" (...) It is a matter of them being able to change the world of the world. \""}, {"heading": "4.2 ODP", "text": "The Open Directory Project [13] is a public, man-edited directory of the web processed by [6] into a multi-stage dataset. For these experiments, we consider test classification errors using the same trait-test split, attributes and labels from [8]. Specifically, there is a fixed trait-test split of 2: 1 for all experiments, the presentation of the document is a bag of words, and the unique class assignment for each document is the most specific category associated with the documentation.The procedures are the same as in the previous experiment, except that we do not compare with OAA or full logistic regression due to intractability on a single machine. To the best of our knowledge, the combination of rebrandt and logistic regression is the best published result on this dataset. PCA logistic regression exhibits a performance gap compared to Rembrandt and logistic regression."}, {"heading": "4.3 LSHTC", "text": "In fact, it is a way in which the quality of the different types of embedding strategies can be assessed independently of each other. In particular, we have calculated the proportion of embedding systems that are located near embedding systems, and the number of embedding systems that are located near embedding systems is very high."}, {"heading": "5 Discussion", "text": "In this paper, we identify a match between rank regression and label embedding and use this match with randomized matrix decomposition techniques to develop a fast label embedding algorithm. To simplify analysis and implementation, we focused on linear predictions, which is a simple architecture of neural networks with a single, hidden layer bottleneck. As linear predictors are well suited for text classification, we achieved excellent experimental results, but for tasks where deep architectures are state-of-the-art, higher complexity is required. Although the analysis presented here would not be strictly applicable, it is plausible that substituting line 5 in algorithm 2 by optimizing over a deep architecture could yield good embedding. This would be beneficial in terms of computation, as the number of outputs would be reduced (i.e. predicting of embedding instead of such labels)."}], "references": [{"title": "Least squares revisited: Scalable approaches for multi-class prediction", "author": ["A. Agarwal", "S.M. Kakade", "N. Karampatziakis", "L. Song", "G. Valiant"], "venue": "Proceedings of The 31st International Conference on Machine Learning. pp. 541\u2013549", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Partial least squares for discrimination", "author": ["M. Barker", "W. Rayens"], "venue": "Journal of chemometrics 17(3), 166\u2013173", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Further aspects of the theory of multiple regression", "author": ["M.S. Bartlett"], "venue": "Mathematical Proceedings of the Cambridge Philosophical Society. vol. 34, pp. 33\u201340. Cambridge Univ Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1938}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences 2(1), 183\u2013202", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "Advances in Neural Information Processing Systems. pp. 163\u2013171", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Refined experts: improving classification in large taxonomies", "author": ["P.N. Bennett", "N. Nguyen"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. pp. 11\u201318. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting multivariate responses in multiple linear regression", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 59(1), 3\u201354", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Logarithmic time online multiclass prediction", "author": ["A. Choromanska", "J. Langford"], "venue": "arXiv preprint arXiv:1406.1822", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning compact class codes for fast inference in large multi class classification", "author": ["M. Ciss\u00e9", "T. Arti\u00e8res", "P. Gallinari"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 506\u2013520. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian aggregation for hierarchical genre classification", "author": ["C. DeCoro", "Z. Barutcuoglu", "R. Fiebrink"], "venue": "ISMIR. pp. 77\u201380", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Large margin hierarchical classification", "author": ["O. Dekel", "J. Keshet", "Y. Singer"], "venue": "Proceedings of the twenty-first international conference on Machine learning. p. 27. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. pp. 248\u2013255. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalized rank-constrained matrix approximations", "author": ["S. Friedland", "A. Torokhti"], "venue": "SIAM Journal on Matrix Analysis and Applications 29(2), 656\u2013659", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T Mikolov"], "venue": "Advances in Neural Information Processing Systems. pp. 2121\u20132129", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Partial least-squares regression: a tutorial", "author": ["P. Geladi", "B.R. Kowalski"], "venue": "Analytica chimica acta 185, 1\u201317", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "The Amsterdam library of object images", "author": ["J.M. Geusebroek", "G.J. Burghouts", "A.W. Smeulders"], "venue": "International Journal of Computer Vision 61(1), 103\u2013112", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Recursive regularization for large-scale classification with hierarchical and graphical dependencies", "author": ["S. Gopal", "Y. Yang"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 257\u2013265. ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "SIAM review 53(2), 217\u2013288", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika pp. 321\u2013377", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1936}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S. Kakade", "J. Langford", "T. Zhang"], "venue": "NIPS. vol. 22, pp. 772\u2013780", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["A.J. Izenman"], "venue": "Journal of multivariate analysis 5(2), 248\u2013264", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1975}, {"title": "On the burstiness of visual elements", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. pp. 1169\u20131176. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "The ECIR 2010 large scale hierarchical classification workshop", "author": ["A. Kosmopoulos", "E. Gaussier", "G. Paliouras", "S. Aseervatham"], "venue": "ACM SIGIR Forum. vol. 44, pp. 23\u201332. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Vowpal Wabbit", "author": ["J. Langford"], "venue": "https://github.com/JohnLangford/vowpal_ wabbit/wiki", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Word emdeddings through hellinger pca", "author": ["R. Lebret", "R. Collobert"], "venue": "arXiv preprint arXiv:1312.5542", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Simple and deterministic matrix sketching", "author": ["E. Liberty"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 581\u2013588. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "The lasso and generalised linear models", "author": ["J. Lokhorst"], "venue": "Tech. rep., University of Adelaide, Adelaide", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Y. Lu", "D.P. Foster"], "venue": "arXiv preprint arXiv:1407.4508", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "randembed", "author": ["P. Mineiro"], "venue": "https://github.com/pmineiro/randembed", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "A randomized algorithm for CCA", "author": ["P. Mineiro", "N. Karampatziakis"], "venue": "arXiv preprint arXiv:1411.3409", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Y. Nesterov"], "venue": "Dokl. Akad. Nauk SSSR 269, 543\u2013547", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1983}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "Advances in neural information processing systems. pp. 1410\u20131418", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Y. Prabhu", "M. Varma"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 263\u2013272. ACM", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in neural information processing systems. pp. 1177\u20131184", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "The utilization of multiple measurements in problems of biological classification", "author": ["C.R. Rao"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological)", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1948}, {"title": "Predicting gene function using hierarchical multi-label decision tree ensembles", "author": ["L. Schietgat", "C. Vens", "J. Struyf", "H. Blockeel", "D. Kocev", "S. D\u017eeroski"], "venue": "BMC Bioinformatics 11, 2", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Zero-shot learning through crossmodal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems. pp. 935\u2013 943", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "On the equivalence between canonical correlation analysis and orthonormalized partial least squares", "author": ["L. Sun", "S. Ji", "S. Yu", "J. Ye"], "venue": "IJCAI. vol. 9, pp. 1230\u20131235", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Multilabel classification with principal label space transformation", "author": ["F. Tai", "H.T. Lin"], "venue": "Neural Computation 24(9), 2508\u20132542", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label linear discriminant analysis", "author": ["H. Wang", "C. Ding", "H. Huang"], "venue": "Computer Vision\u2013ECCV 2010, pp. 126\u2013139. Springer", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Large margin taxonomy embedding for document categorization", "author": ["K.Q. Weinberger", "O. Chapelle"], "venue": "Advances in Neural Information Processing Systems. pp. 1737\u2013 1744", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "IJCAI. vol. 11, pp. 2764\u20132770", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Label partitioning for sublinear ranking", "author": ["J. Weston", "A. Makadia", "H. Yee"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML13). pp. 181\u2013189", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Recent years have witnessed the emergence of many multiclass and multilabel datasets with increasing number of possible labels, such as ImageNet [12] and the Large Scale Hierarchical Text Classification (LSHTC) datasets [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 22, "context": "Recent years have witnessed the emergence of many multiclass and multilabel datasets with increasing number of possible labels, such as ImageNet [12] and the Large Scale Hierarchical Text Classification (LSHTC) datasets [25].", "startOffset": 220, "endOffset": 224}, {"referenceID": 4, "context": "This unifies prior work utilizing the confusion matrix for multiclass [5] and the empirical label covariance for multilabel [42].", "startOffset": 70, "endOffset": 73}, {"referenceID": 39, "context": "This unifies prior work utilizing the confusion matrix for multiclass [5] and the empirical label covariance for multilabel [42].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "We apply techniques from randomized linear algebra [19] to develop an efficient and scalable algorithm for constructing the embeddings, essentially via a novel randomized algorithm for rank-constrained squared loss regression.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "The first step of our algorithm resembles compressed sensing approaches to extreme classification that use random matrices [21].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "For a very thorough and more formal discussion see [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "Since principal eigenvectors can be thought as \u201cfrequent directions\u201d [28], the range of \u03a8 will tend to be more aligned with the space spanned by the top eigenvectors of X>X.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "This is a special case of a more general problem studied by [14]; specializing", "startOffset": 60, "endOffset": 64}, {"referenceID": 29, "context": "[32]", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "For example, the principal label space transformation of [42] is an eigendecomposition of the empirical label covariance Y >Y .", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": ", regularization) benefits to label embeddings, corresponding to the rich literature of low-rank regression regularization [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 32, "context": "Finally, embeddings can be part of a strategy for zero-shot learning [35], i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "[21], motivated by advances in compressed sensing, utilized a random embedding of the labels along with greedy sparse decoding strategy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "For the multilabel case, [42] construct a low-dimensional embedding using principal components on the empirical label covariance, which they utilize along with a greedy sparse decoding strategy.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "For multivariate regression, [7] use the principal components of the empirical label covariance to define a shrinkage estimator which exploits correlations between the labels to improve accuracy.", "startOffset": 29, "endOffset": 32}, {"referenceID": 42, "context": "Conversely, [45] motivate their ranking-loss optimized embeddings solely by computational considerations of inference time and space complexity.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].", "startOffset": 89, "endOffset": 92}, {"referenceID": 36, "context": "when composed with online learning [11]; Bayesian learning [10]; support vector machines [6]; and decision tree ensembles [39].", "startOffset": 122, "endOffset": 126}, {"referenceID": 8, "context": ", the sub-linear inference approach of [9] and the large margin approach of [44].", "startOffset": 39, "endOffset": 42}, {"referenceID": 41, "context": ", the sub-linear inference approach of [9] and the large margin approach of [44].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Analogously, [5] utilize a surrogate classifier rather than side information to define a similarity matrix between classes; our procedure can efficiently produce a similarity matrix which can ease the computational burden of this portion of their procedure.", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": ", [15] and [40].", "startOffset": 2, "endOffset": 6}, {"referenceID": 37, "context": ", [15] and [40].", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "[18] focus nearly exclusively on the statistical benefit of incorporating label structure by overcoming the space and time complexity of large-scale oneagainst-all classification via distributed training and inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Our objective in equation (1) is highly related to that of partial least squares [16], as Algorithm 2 corresponds to a randomized algorithm for PLS if the features have been whitened.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "Unsurprisingly, supervised dimensionality reduction techniques such as PLS can be much better than unsupervised dimensionality reduction techniques such as PCA regression in the discriminative setting if the features vary in ways irrelevant to the classification task [2].", "startOffset": 268, "endOffset": 271}, {"referenceID": 35, "context": "Two other classical procedures for supervised dimensionality reduction are Fisher Linear Discriminant [38] and Canonical Correlation Analysis [20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "Two other classical procedures for supervised dimensionality reduction are Fisher Linear Discriminant [38] and Canonical Correlation Analysis [20].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": "For multiclass problems these two techniques yield the same result [3,2], although for multilabel problems they are distinct.", "startOffset": 67, "endOffset": 72}, {"referenceID": 1, "context": "For multiclass problems these two techniques yield the same result [3,2], although for multilabel problems they are distinct.", "startOffset": 67, "endOffset": 72}, {"referenceID": 40, "context": "Indeed, extension of FLD to the multilabel case is a relatively recent development [43] whose straightforward implementation does not appear to be computationally viable for large number of classes.", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "CCA and PLS are highly related, as CCA maximizes latent correlation and PLS maximizes latent covariance [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 38, "context": "Furthermore, CCA produces equivalent results to PLS if the features are whitened [41].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Regarding computational considerations, scalable CCA algorithms are available [30,33], but it remains open how to specialize them to this context to leverage the equivalent of equation (2); whereas, if CCA is desired, Algorithm 2 can be utilized in conjunction with whitening pre-processing.", "startOffset": 78, "endOffset": 85}, {"referenceID": 30, "context": "Regarding computational considerations, scalable CCA algorithms are available [30,33], but it remains open how to specialize them to this context to leverage the equivalent of equation (2); whereas, if CCA is desired, Algorithm 2 can be utilized in conjunction with whitening pre-processing.", "startOffset": 78, "endOffset": 85}, {"referenceID": 28, "context": ", word2vec [31], which (empirically) provide analogous statistical and computational benefits despite being unsupervised.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "The text embedding technique of [27] is a particularly interesting comparison because it is a variant", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "This suggests that unsupervised dimensionality reduction approaches can work well when additional structure of the input domain is incorporated, in this case by modeling word burstiness with the square root nonlinearity [23] and word order via decomposing neighborhood statistics.", "startOffset": 220, "endOffset": 224}, {"referenceID": 24, "context": "Nonetheless [27] note that when maximum statistical performance is desired, the embeddings must be fine-tuned to the particular task, i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "Another plausible regularization technique which mitigates inference space and time complexity is L1 regularization [29].", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "ALOI is a color image collection of one-thousand small objects recorded for scientific purposes [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "For these experiments we will consider test classification accuracy utilizing the same train-test split and features from [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": ", embedding dimensionality) for this logistic regression is modest so the second order techniques of [1] are applicable (in particular, their Algorithm 1 with a simple modification to include acceleration [34,4]).", "startOffset": 101, "endOffset": 104}, {"referenceID": 31, "context": ", embedding dimensionality) for this logistic regression is modest so the second order techniques of [1] are applicable (in particular, their Algorithm 1 with a simple modification to include acceleration [34,4]).", "startOffset": 205, "endOffset": 211}, {"referenceID": 3, "context": ", embedding dimensionality) for this logistic regression is modest so the second order techniques of [1] are applicable (in particular, their Algorithm 1 with a simple modification to include acceleration [34,4]).", "startOffset": 205, "endOffset": 211}, {"referenceID": 7, "context": "We also compare against Lomtree (LT), which has training and test time complexity logarithmic in the number of classes [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 23, "context": "Both OAA and LT are provided by the Vowpal Wabbit [26] machine learning tool.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "The Open Directory Project [13] is a public human-edited directory of the web which was processed by [6] into a multiclass data set.", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "For these experiments we will consider test classification error utilizing the same train-test split, features, and labels from [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 39, "context": "PLST [42] has performance close to Rembrandt according to this metric, so the 3.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "Classification Performance We built an end-to-end classifier using an approximate kernelized variant of Algorithm 3, where we processed the embeddings with Random Fourier Features [37], i.", "startOffset": 180, "endOffset": 184}, {"referenceID": 33, "context": "Therefore we compare with published results of [36], who report example-averaged precision-at-k on the label ordering induced for each example.", "startOffset": 47, "endOffset": 51}, {"referenceID": 43, "context": "LPSR-NB is the Label Partitioning by Sub-linear Ranking algorithm of [46] composed with a Naive Bayes base learner, as reported in [36], where they also introduce and report precision for the multilabel tree learning algorithm FastXML.", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "LPSR-NB is the Label Partitioning by Sub-linear Ranking algorithm of [46] composed with a Naive Bayes base learner, as reported in [36], where they also introduce and report precision for the multilabel tree learning algorithm FastXML.", "startOffset": 131, "endOffset": 135}], "year": 2015, "abstractText": "Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.", "creator": "LaTeX with hyperref package"}}}