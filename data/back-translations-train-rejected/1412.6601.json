{"id": "1412.6601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Using Neural Networks for Click Prediction of Sponsored Search", "abstract": "Sponsored search is a multi-billion dollar industry and makes up a major source of revenue for search engines (SE). click-through-rate (CTR) estimation plays a crucial role for ads selection, and greatly affects the SE revenue, advertiser traffic and user experience. We propose a novel architecture for solving CTR prediction problem by combining artificial neural networks (ANN) with decision trees. First we compare ANN with respect to other popular machine learning models being used for this task. Then we go on to combine ANN with MatrixNet (proprietary implementation of boosted trees) and evaluate the performance of the system as a whole. The results show that our approach provides significant improvement over existing models.", "histories": [["v1", "Sat, 20 Dec 2014 04:44:00 GMT  (55kb,D)", "http://arxiv.org/abs/1412.6601v1", "6 pages, 3 figures, for submission to workshop proceedings of ICLR 2015"], ["v2", "Sat, 28 Feb 2015 08:15:40 GMT  (57kb,D)", "http://arxiv.org/abs/1412.6601v2", "7 pages, 3 figures, for submission to workshop proceedings of ICLR 2015 Added more info about training the model in section 3.3 Updated Tables 2 and 3 Added some brief clarifications in section 3.1 and 4.2"], ["v3", "Sat, 19 Sep 2015 23:30:51 GMT  (75kb,D)", "http://arxiv.org/abs/1412.6601v3", "updated typos, and removed conference header"]], "COMMENTS": "6 pages, 3 figures, for submission to workshop proceedings of ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["afroze ibrahim baqapuri", "ilya trofimov"], "accepted": false, "id": "1412.6601"}, "pdf": {"name": "1412.6601.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["SPONSORED SEARCH", "Afroze I. Baqapuri"], "emails": ["afroze.baqapuri@epfl.ch", "trofim@yandex-team.ru"], "sections": [{"heading": "1 INTRODUCTION", "text": "Before displaying the ads, the advertisements are sorted by SE according to two aspects: the relevance of the ads to the context, so that more users click on the ad, and a certain amount of the expected payment the advertiser receives. We limit ourselves to the most common cost-per-click (CPC) models: The advertiser is charged with the offer price each time a user clicks on the ad. Therefore, we propose a two-step click prediction system that recognizes ANN as estimated revenue. Therefore, the CTR estimate plays an essential role in sponsored search and SE. In this study, we use ANNs to model the click rate of sponsored search. We propose a two-step click prediction system that incorporates ANN into the existing framework of the decision trees currently used at Yandex. To our knowledge, only Zhang et al. (2014) we have used ANN's click prediction in the prediction area."}, {"heading": "2 SPONSORED SEARCH FRAMEWORK IN YANDEX", "text": "The mechanism of sponsored search is based on a keyword auction: Advertisers bid on selected keywords. When a user enters a search query, the SE matches it with all keywords and selects suitable ads to display. A simplified ad selection algorithm is described as follows: First, all ads that match the user's search query are selected and sorted in descending order according to expected revenue. Then, the leading ads - no more than three - are selected and sorted according to their bids. CTR is indispensable to this process as it is used in calculating expected revenue. Yandex MatrixNet is the machine learning algorithm used to estimate the CTR. MatrixNet is a proprietary implementation of enhanced decision trees and is successfully applied to numerous classification and regression problems within the company. In sponsored search, we use MatrixNet with click data-derived keywords to describe the following statistical features:"}, {"heading": "3 EXPERIMENTAL SETTING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 INPUT FEATURES", "text": "We use click logs from the Yandex search engine as a dataset containing approximately 6.6 million training examples, collected over a week between July 1 and July 7, 2014. For this study, we focus only on ID-based features, which belong to 14 namespaces, which can be divided into the following categories: \u2022 Users: User ID, Domain ID, Region ID. \u2022 Display: Ad ID, Campaign ID, Word Title, Ad Body, Ad Position, Ad Keywords. \u2022 Query: User Query Words These features are encoded in 1-of-c encoding, which results in very high dimensionality and extremely sparse memory space. As it would be impossible to enter the data directly into the neural network, we first try to reduce its dimensionality in two steps: 1. Frequent removal of features: If a function occurs less than a certain threshold, we simply discard it."}, {"heading": "3.2 PROPOSED MODEL", "text": "We design the click prediction system as a two-stage process. In the first stage, ANN models the sparse high-dimensional ID-based features; the second stage is MatrixNet, which models the real-valued features; for each display impression, there is a real probability that this display will be clicked; this value serves as one of the input features of the MatrixNet; the other input factors are those currently used as input features at Yandex and are described by Trofimov et al. (2012) The output of MatrixNet provides the final CTR of the advertising impression that can be used to estimate the expected revenue. This two-stage system is used for two reasons: first, it provides a way to efficiently combine the two types of features (real and ID-based) for the CTR prediction; second, since Yandex already uses the matrix networks for the task, this is the simplest way to integrate the existing framework into the ANN model."}, {"heading": "4 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 INDIVIDUAL ANN", "text": "The Softmax output of ANN can be interpreted as a CTR used to evaluate the performance of neural networks. We select the best six ANN architectures, with the best network architecture showing an improvement in log probability by 0.88% and auPRC metric by 5.57%. Experiments also showed that the use of rectified linear units (relu) was critical to this performance improvement, with the use of Relu resulting in an improvement in log probability by up to 0.87% and an improvement in auPRC by up to 5.7%, as well as the use of dropout functionality in the hidden units of the network resulting in an improvement in performance."}, {"heading": "4.2 ENSEMBLE OF ANNS", "text": "In ensemble methods, the results of many models are averaged to achieve performance improvement. The principle is that when different models agree on very different local optimum values, these lead to false predictions for different groups of instances. So, if we combine and average the predictions of these models before comparing them to the target of the test instance, this can lead to better performance than the results of each individual model. We experiment with this method and use the results of the best six ANN models in our ensemble. Figure 1 (a) shows that performance is steadily improving as we increase the number of ANNs in our ensemble. ANN models are added to the ensemble in the same order as they are listed in Table 1 (from top to bottom).In Table 2, we compare the performance of the ensemble and the highest performing individual ANN with the LR results. We can find that using ensemble models improves the probability of the protocol by 0.24% augmenting the PRC by 1.15% for the PRANN model and clearly shows the PRANC by 1.15% for the individual PRANN model."}, {"heading": "4.3 SIZE OF TRAINING SET", "text": "To this end, we train the ANN models on subsets of our training data (without changing the test and validation sets). Figure 2 shows the result of this experiment, in which different models are evaluated and compared using the auPRC metric. As expected, increasing the training size improves the performance of all models. However, we find that when training data is low, LR is more powerful than ANN. However, as training data is used, ANN is catching up with LR, and in fact, the performance gap is growing steadily in ANN's favor, which means that ANN's true potential in using large data sets is being exploited. From the charts, we can conclude that if we use even more training data, ANN would perform even better than LR compared to current results."}, {"heading": "4.4 MATRIXNET RESULTS", "text": "As already explained in Section 3, this consists of ANN followed by MatrixNet. For comparison, we also evaluate the existing framework: MatrixNet alone with basic features without ANN-Ooutput. The comparison results in Table 3 show that using ANN output as an additional input to MatrixNet leads to an improvement in the log probability by up to 0.22% and the auPRC by 0.48%. All results are calculated on a separate test set. From our experience, we know that results with a difference in the log probability of 0.1% are small but important and should not be neglected (Trofimov et al., 2012). Based on this observation, we assume that the use of neural networks in Yandex's click prediction system leads to a significant improvement."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "In this paper, we proposed the use of ANNs to model ID-based features for CTR prediction tasks. First, we showed that using non-linear models like ANN improves performance over linear models like LR. Then, we showed that using an ensemble of ANNs further improves performance and also further widens the performance gap between data. Finally, we performed a comparison and explained improvements in using ANN in combination with the existing click predictor MatrixNet. Further research may include testing real-time data and seeing the performance effects on a real-time ad selection system. However, since many of the ID-based features are in the form of words, it may be useful to initialize the neural network as RBM, which is trained at large data size, although it would be interesting to see the effects of using much larger training data."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank the Machine Learning Group and the Computer Vision Group of Yandex for helpful discussions."}], "references": [{"title": "A simple and scalable response prediction for display advertising", "author": ["Chapelle", "Olivier"], "venue": null, "citeRegEx": "Chapelle and Olivier,? \\Q2013\\E", "shortCiteRegEx": "Chapelle and Olivier", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Predicting ads click-through rate with decision rules", "author": ["Dembczynski", "Krzysztof", "W Kotlowski", "Weiss", "Dawid"], "venue": "In Workshop on Targeting and Ranking in Online Advertising,", "citeRegEx": "Dembczynski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Ad click prediction: a view from the trenches", "author": ["McMahan", "H Brendan", "Holt", "Gary", "Sculley", "David", "Young", "Michael", "Ebner", "Dietmar", "Grady", "Julian", "Nie", "Lan", "Phillips", "Todd", "Davydov", "Eugene", "Golovin", "Daniel"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "McMahan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2013}, {"title": "A comparison of numerical optimizers for logistic regression", "author": ["Minka", "Thomas P"], "venue": "Unpublished draft,", "citeRegEx": "Minka and P.,? \\Q2003\\E", "shortCiteRegEx": "Minka and P.", "year": 2003}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["Mohamed", "Abdel-rahman", "Sainath", "Tara N", "Dahl", "George", "Ramabhadran", "Bhuvana", "Hinton", "Geoffrey E", "Picheny", "Michael A"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "Updating quasi-newton matrices with limited storage", "author": ["Nocedal", "Jorge"], "venue": "Mathematics of computation,", "citeRegEx": "Nocedal and Jorge.,? \\Q1980\\E", "shortCiteRegEx": "Nocedal and Jorge.", "year": 1980}, {"title": "Predicting clicks: estimating the click-through rate for new ads", "author": ["Richardson", "Matthew", "Dominowska", "Ewa", "Ragno", "Robert"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "Using boosted trees for click-through rate prediction for sponsored search", "author": ["Trofimov", "Ilya", "Kornetova", "Anna", "Topinskiy", "Valery"], "venue": "In Proceedings of the Sixth International Workshop on Data Mining for Online Advertising and Internet Economy,", "citeRegEx": "Trofimov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Trofimov et al\\.", "year": 2012}, {"title": "Feature hashing for large scale multitask learning", "author": ["Weinberger", "Kilian", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alex", "Attenberg", "Josh"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Sequential click prediction for sponsored search with recurrent neural networks", "author": ["Zhang", "Yuyu", "Dai", "Hanjun", "Xu", "Chang", "Feng", "Jun", "Wang", "Taifeng", "Bian", "Jiang", "Bin", "Liu", "Tie-Yan"], "venue": "arXiv preprint arXiv:1404.5772,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Popular models include logistic regression (LR) (Richardson et al., 2007; McMahan et al., 2013; Chapelle et al., 2013) and boosted decision trees (Dembczynski et al.", "startOffset": 48, "endOffset": 118}, {"referenceID": 5, "context": "Popular models include logistic regression (LR) (Richardson et al., 2007; McMahan et al., 2013; Chapelle et al., 2013) and boosted decision trees (Dembczynski et al.", "startOffset": 48, "endOffset": 118}, {"referenceID": 3, "context": ", 2013) and boosted decision trees (Dembczynski et al., 2008; Trofimov et al., 2012).", "startOffset": 35, "endOffset": 84}, {"referenceID": 10, "context": ", 2013) and boosted decision trees (Dembczynski et al., 2008; Trofimov et al., 2012).", "startOffset": 35, "endOffset": 84}, {"referenceID": 8, "context": "Up to our knowledge, only Zhang et al. (2014) have used ANNs in the domain of click prediction.", "startOffset": 26, "endOffset": 46}, {"referenceID": 4, "context": "We consider ANNs to be promising models because they already show state-ofthe-art results in various other domains including computer vision (Krizhevsky et al., 2012), natural language processing (Collobert et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 7, "context": ", 2011b) and speech recognition (Mohamed et al., 2011).", "startOffset": 32, "endOffset": 54}, {"referenceID": 10, "context": "For more details about these input features refer to the paper by Trofimov et al. (2012).", "startOffset": 66, "endOffset": 89}, {"referenceID": 11, "context": "We used Vowpal Wabbit (Weinberger et al., 2009) to build and train these linear models.", "startOffset": 22, "endOffset": 47}, {"referenceID": 7, "context": "The other inputs are the ones being currently used as input features at Yandex and are described by Trofimov et al. (2012). The output of MatrixNet gives the final CTR of the ad impression which can be used to estimate the expected revenue.", "startOffset": 100, "endOffset": 123}, {"referenceID": 1, "context": "We used torch7 for building and training the ANN models (Collobert et al., 2011a). For comparison we also train a baseline LR model with L-BFGS (Nocedal, 1980), which is a state of the art second-order optimizer. BFGS is a popular choice for training logisitic regression and is used by Chapelle et al. (2013) and Richardson et al.", "startOffset": 57, "endOffset": 310}, {"referenceID": 1, "context": "We used torch7 for building and training the ANN models (Collobert et al., 2011a). For comparison we also train a baseline LR model with L-BFGS (Nocedal, 1980), which is a state of the art second-order optimizer. BFGS is a popular choice for training logisitic regression and is used by Chapelle et al. (2013) and Richardson et al. (2007). A review of different optimization methods is presented by Minka (2003) wich shows BFGS to be fast and perform well in practice on logistic regression.", "startOffset": 57, "endOffset": 339}, {"referenceID": 1, "context": "We used torch7 for building and training the ANN models (Collobert et al., 2011a). For comparison we also train a baseline LR model with L-BFGS (Nocedal, 1980), which is a state of the art second-order optimizer. BFGS is a popular choice for training logisitic regression and is used by Chapelle et al. (2013) and Richardson et al. (2007). A review of different optimization methods is presented by Minka (2003) wich shows BFGS to be fast and perform well in practice on logistic regression.", "startOffset": 57, "endOffset": 412}, {"referenceID": 10, "context": "1% is small but important, and should not be neglected (Trofimov et al., 2012).", "startOffset": 55, "endOffset": 78}], "year": 2014, "abstractText": "Sponsored search is a multi-billion dollar industry and makes up a major source of revenue for search engines (SE). click-through-rate (CTR) estimation plays a crucial role for ads selection, and greatly affects the SE revenue, advertiser traffic and user experience. We propose a novel architecture for solving CTR prediction problem by combining artificial neural networks (ANN) with decision trees. First we compare ANN with respect to other popular machine learning models being used for this task. Then we go on to combine ANN with MatrixNet (proprietary implementation of boosted trees) and evaluate the performance of the system as a whole. The results show that our approach provides significant improvement over existing models.", "creator": "LaTeX with hyperref package"}}}