{"id": "1412.6604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Video (language) modeling: a baseline for generative models of natural videos", "abstract": "We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary.", "histories": [["v1", "Sat, 20 Dec 2014 05:05:51 GMT  (5391kb)", "https://arxiv.org/abs/1412.6604v1", null], ["v2", "Wed, 24 Dec 2014 01:49:29 GMT  (5414kb)", "http://arxiv.org/abs/1412.6604v2", null], ["v3", "Tue, 17 Mar 2015 15:03:04 GMT  (5415kb)", "http://arxiv.org/abs/1412.6604v3", null], ["v4", "Tue, 21 Apr 2015 03:39:55 GMT  (5415kb)", "http://arxiv.org/abs/1412.6604v4", null], ["v5", "Wed, 4 May 2016 14:01:42 GMT  (5415kb)", "http://arxiv.org/abs/1412.6604v5", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["marcaurelio ranzato", "arthur szlam", "joan bruna", "michael mathieu", "ronan collobert", "sumit chopra"], "accepted": false, "id": "1412.6604"}, "pdf": {"name": "1412.6604.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sumit Chopra"], "emails": ["ranzato@fb.com", "aszlam@fb.com", "locronan@fb.com", "spchopra@fb.com", "mathieu}@cims.nyu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.66 04v5 [cs.LG] 4 May 201 6"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is not the case that one sees oneself in a position to behave in a way as one has done in the past years. (...) It is not the case that one behaves in a way as one has done in the past. (...) It is the case that one behaves in a way as one has done in the past. (...) It is not the case that one has been able to remain in the present. (...) It is as if one is in the present. (...) It is not the case that one is in the future in the future. (...) \"(...).\" (...) \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). \"It is.\" (...). (...). \"It is. (...).\" It is. (...). \"It is. (...).\" It. \"It is. (...).\" It is. \"It is. (...).\" It is. (...). \"It is.\" It is. (...). \"It is.\" It is. (...). \"It is. (...).\" It is. \"It is. (...).\" It is. (. \"It. (...).\" It is. \"It is. (. (...).\" It is. (...). \"It is.\" It. \"It. (). (. (...). It is. (). It is. (). (. It is. (). It is. (). (. (). It is. It is. (. It is. (). (. (). (). It is. (). It is. (). (. (. It is. (). It is. (). It is. It is. (). It is. (). (. (. (). It is. It is. (). It is. (. (). It is. (). It is. (. It is. It is. It is. (. It is. It is. (. (.). It is. (.). It is. It is. (."}, {"heading": "2 MODEL", "text": "Faced with a sequence of consecutive frames from a video denoted by (X1, X2,.., Xt), we could try to train a system to predict the next frame in the sequence, Xt + 1, in which the subscript denotes time. More broadly, we could try to predict some frames that have been left out of context. This is a simple and well-defined task that does not require labels, but accurate predictions can only be made by models that have learned motion primitives and understand the local deformation of objects. At the test date, we can validate our models on both generation and filling tasks (see Sections 3.3 and 3.4). To design a system that addresses these tasks, we draw inspiration from classical methods of processing natural language, namely n-grams, neural network language models (Bengio et al., 2003) and recursive neural networks (Mikolov al, 2010)."}, {"heading": "2.1 LANGUAGE MODELING", "text": "In language modeling, we get a sequence of separated symbols (e.g. words) from a finite (but very large) dictionary. Let a symbol in the sequence be denoted by Xk (symbol at position k in the sequence); if V is the dictionary size, Xk is an integer in the range [1, V]. In language modeling, we are interested in the probability of a word sequence p (X1, X2,..., XN) = p (XN | XN \u2212 1,.., X1) p (XN \u2212 1,...,., X1)... p (X2 | X1) p (X1). Therefore, everything is reduced to the calculation of the conditional distribution: p (Xk | Xk \u2212 1,.., X1). In the following, we will briefly go through three methods for estimating these sets."}, {"heading": "2.1.1 N-GRAM", "text": "The n-gram is a table of normalized frequency numbers under the Markovian assumption that p (Xt | Xt \u2212 1,... X1) = p (Xt | Xt \u2212 1,.., Xt \u2212 n + 1). In this thesis, these conditional probabilities are calculated by the counting ratio: p (Xt | Xt \u2212 1,.., Xt \u2212 n + 1) = count (Xt \u2212 n + 1,.., Xt) + 1count (Xt \u2212 n + 1,..., Xt \u2212 1) + V (1), whereby the constants in the counter and in the denominator are designed to smooth the distribution and improve the generalization to rare n-grams (Laplace smoothing). In this thesis, we have taken bigrams and trigrams (n = 2 and n = 3, respectively) into account."}, {"heading": "2.1.2 NEURAL NET LANGUAGE MODEL", "text": "The Neural Network Language Model (NN) (Bengio et al., 2003) is a parametric and nonlinear extension of n-grams. Let 1 (Xk) be the 1-hot vector representation of the integer Xk, that is, a vector where all entries are set to 0, except the Xk-th component, which is set to 1. In this model, the words in the context (the ones we are conditioning) are first transformed into their 1-hot vector representation, then linearly embedded using matrix Wx, the embedding is concatenated, and finally fed into a standard multilayered neural network. This network is trained using a cross-entropy loss to predict the next word in the sequence (usual multi-class classification task with V classes). In this model, the output probability is then covered by: p (Xt | 1, second \u2212 1, Xt + \u2212 1) in the first layer (Xt \u2212 1), and \u2212 1 (Xt \u2212 1) in the first layer."}, {"heading": "2.1.3 RECURRENT NEURAL NETWORK", "text": "The recurring neural network (rNN) (Mikolov et al., 2010) works similarly to the above model, except that: 1) it requires only one input at this point in time and 2) the hidden layer of the MLP also performs a linear transformation of the hidden state in the previous time step, allowing the rNN to potentially use a context of variable size without affecting computing power. Equations that regulate the rNN are: ht \u2212 1 = \u03c3 (Whht \u2212 2 + Bx1 (Xt \u2212 1))), p (Xt | ht \u2212 1) = SM (Where \u2212 1 + bo) (3) Training of the parameters of the model, {Wx, Wh, Wo, bo}, is done by minimizing the standard entropy loss on the next symbol by means of backpropagation by time (Rumelhart et al., 1986) and gradient section (Mikolov et al., 2010)."}, {"heading": "2.2 VIDEO (LANGUAGE) MODELING", "text": "The above methods work on a sequence of discrete input values; however, video frames are usually received as continuous vectors (to the extent that 8-bit numbers are continuous). If we want to use these methods to process video sequences, we can have two main strategies. This is difficult because the model has relatively low reconstruction errors by merely blurring the last frame. In our experiments, we found that this approach turns the classification problem into regression."}, {"heading": "2.2.1 RECURRENT CONVOLUTIONAL NEURAL NETWORK", "text": "The last model we propose is a simple extension of the rNN to better handle spatial correlations between nearby image fields. In the rNN, nearby image fields are treated independently of each other, while there are almost always very strong spatial correlations between nearby image fields. Therefore, in the recurring Convolutionary Neural Network (rCNN), we feed the system not only with a single image field, but also with the nearby image fields. The model uses not only temporal dependencies, but also spatial correlations to predict the central image point more accurately at the next time step (see Fig. 1 Center). The rCNN we use in our experiments takes as input an image field of size 9 x 9. Each integer in the network corresponds to a quantified image field of size 8 x 8 image fields. In this work, these image fields do not overlap, although everything we describe is applicable to overlapping image fields."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we empirically validate the language modeling techniques discussed in paragraph 2. Quantitatively, we evaluate models for their ability to predict patches one frame ahead of time. We also show examples of short video sequence generation and filling that capture non-trivial deformations. In our experiments, we used the following training protocol. Firstly, we do not process the data in any way except for grayscale conversion and division by standard deviation. Secondly, we use 10,000 centrioids for k-mean quantification. Thirdly, we tick all our hyperparameters on the validation set and report on the accuracy on the test set based on the model that yielded the best accuracy on the validation set. For the van Hateren dataset, we used 3 videos for validation and 3 videos for test (out of 56 available). For the UCF 101 dataset, we used the standard split (Soomal, data set) as the complement to the 2012 dataset."}, {"heading": "3.1 UCF-101 DATASET", "text": "The UCF-101 dataset (Soomro et al., 2012) is a standard benchmark dataset for the detection of human actions. It has 13320 videos of varying lengths belonging to 101 categories of human actions, and each frame has a size of 160 x 320 pixels. This dataset is also interesting for unattended learning because: a) it is much larger than the van Hateren dataset, and b) it is much more realistic because the movement and spatial scale of objects have not been normalized. This dataset is by no means ideal for learning motion patterns, as many videos have jpeg artifacts and duplicated frames due to compression, which further complicates learning. Tab. 1 (right) compares several models. Larger models generally worked better. In particular, the rCNN provides the best results, showing that not only the temporal but also the spatial context of pixels is important."}, {"heading": "3.2 ANALYZING THE MODEL", "text": "In this section, we will examine what the rCNN learned after training on the UCF 101 dataset. However, first, we will analyze the parameters in the embedding matrix and the first convolutionary layers. There is one embedding per k-middle centride and we will look at the centrioids corresponding to the closest embedding. Fig. 2 (left) shows that the rCNN, but also the rNN and NN, will learn to group similar centrioids. This means that despite the quantization step, the model will learn to become robust to small distortions. It does not matter which particular centric layer we select, the vector we select in space for similar-looking input fields. We will also visualize (a random subset) the first layer of revolutionary filters by looking at the input examples (from the validation group) that trigger the output most fire."}, {"heading": "3.3 GENERATION", "text": "After training on UCF-101, we used the rCNN to predict future frames, after conditioning 12 consecutive real frames, and the generation proceeds as follows: a) We roll out the rCNN regardless of the frame size we use at test time, and perform it forward on all frames on which we condition, b) we take the most likely predicted atom as next input and iterate. To reduce quantization errors, we perform the same procedure for all 64 spatial offsets (combination of 8 horizontal and 8 vertical shifts), and then calculate the predictions for each pixel. Figure 3 (right) shows that rCNN is pretty good at completing the movement, even capturing fairly complex rotations and deformations. However, predictions tend to slow down the movement fairly quickly, and finally, the model converges after a still image after a few frames in average video, even fairly complex rotations and deformations."}, {"heading": "3.4 FILLING", "text": "In our experiments, we update each spatio-temporal model before updating the problem, and finally, we determine the limits based on the specified limits. For convenience, the limits include the upper / lower / left / right edges of the entire video cube (15 pixels wide), in addition to the frames used for the temporal context (both for past and future frames). We use a model that uses two 3 x 3 atomic fields in the same place from frames j and j + 2 as input, and it is trained to predict the mean atom in the corresponding 3 x 3-3 range of the (j + 1) -th frame. At the test date, we use an iterative method to fill larger gaps. At each iteration and spatio-temporal position z, we take our current estimate of the spatio-temporal context C of z and use it as input into our language model to re-evaluate the atom at z. In our experiments, we update each spatio-temporal model."}, {"heading": "4 DISCUSSION", "text": "In fact, it is such that most people will be able to move to another world, in which they are able to move to another world, in which they are able to move, in which they are able, in which they are able to stay in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5 CONCLUSION", "text": "The method is simple, easily reproducible, and should serve as a stronger foundation for research on unattended learning from videos. It consists of a quantization step followed by a revolutionary extension of rNN. We evaluated the performance of this model using a relatively large video dataset, which shows that the model is capable of generating short sequences with non-trivial motion. This model shows that it is possible to learn the local spatio-temporal geometry of videos purely from data, without relying on explicit modelling of transformations. Time recidivity and spatial convolutions are key to regulating estimation by indirectly assuming standardisation and locality. However, there is still much to understand whether such generation results are valid only for short time intervals after which long interactions are lost."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Piotr Dollar for providing the optical flow estimator, Manohar Paluri for his help with the data, and the entire FAIR team for providing insightful commentary."}, {"heading": "6 SUPPLEMENTARY MATERIAL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 VAN HATEREN\u2019S DATASET", "text": "The van Hateren dataset of natural video was a standard dataset for studying temporal sequence models (Cadieu & Olshausen, 2009; Olshausen & Field, 1997), downloaded from the material provided by Cadieu & Olshausen (2009) at https: / / github.com / cadieu / twolayer. It consists of 56 videos, each with 64 frames, each with a size of 128 x 128 pixels. This is a very small dataset in which objects are heavily textured and move at a similar speed. Given the small dataset size, we could only evaluate patch-based models. Fig. 5 shows examples of video fields extracted from this dataset, along with the effect of quantification on the monitors.Table 1 (left) compares n-grams (Tri-grams functioned worse than bi-grams and are therefore omitted), neural networks and NN-models language tests. In view of the small dataset size, Tri-grams worked better than the larger ones, and the larger ones prevented the gram from performing correction."}, {"heading": "6.2 GENRATION EXAMPLES", "text": "For more examples, visit https: / / research.facebook.com / publications / video-language-modeling-a-baseline-for-generative-models-of-natural-videos /."}, {"heading": "6.3 FILLING EXAMPLES", "text": "The optical flow baseline in Fig. 4 was calculated by: a) estimating the flow from two images in the past, b) estimating the flow from two images in the future, c) linear flow interpolation for the missing images, and d) using the estimated flow to reconstruct the missing images. Below are some examples of filled images; more examples can be found at: https: / / research.facebook.com / publications / video-language-modeling-a-baseline-for-generative-models-of-natural-videos /."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "High accuracy optical flow estimation based on a theory for warping", "author": ["T. Brox", "A. Bruhn", "N. Papenberg", "J. Weickert"], "venue": "Vision-ECCV", "citeRegEx": "Brox et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Brox et al\\.", "year": 2004}, {"title": "Learning transformational invariants from natural movies", "author": ["C. Cadieu", "B. Olshausen"], "venue": "In NIPS,", "citeRegEx": "Cadieu and Olshausen,? \\Q2009\\E", "shortCiteRegEx": "Cadieu and Olshausen", "year": 2009}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["I Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Emergence of complex-like cells in a temporal product network with local receptive fields", "author": ["K. Gregor", "Y. LeCun"], "venue": null, "citeRegEx": "Gregor and LeCun,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun", "year": 2010}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "In ECCV,", "citeRegEx": "Hariharan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Simple-cell-like receptive fields maximize temporal coherence in natural video", "author": ["J. Hurri", "A. Hyv\u00e4rinen"], "venue": "Neural Computation,", "citeRegEx": "Hurri and Hyv\u00e4rinen,? \\Q2003\\E", "shortCiteRegEx": "Hurri and Hyv\u00e4rinen", "year": 2003}, {"title": "Learning flexible sprites in video layers", "author": ["N. Jojic", "B.J. Frey"], "venue": "In CVPR,", "citeRegEx": "Jojic and Frey,? \\Q2001\\E", "shortCiteRegEx": "Jojic and Frey", "year": 2001}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "ArXiv 1010.3467,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2008}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "E. Chaitanya", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["R. Memisevic", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Memisevic and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Memisevic and Hinton", "year": 2009}, {"title": "Learning the lie groups of visual invariance", "author": ["X. Miao", "R. Rao"], "venue": "Neural Computation,", "citeRegEx": "Miao and Rao,? \\Q2007\\E", "shortCiteRegEx": "Miao and Rao", "year": 2007}, {"title": "Modeling deep temporal dependencies with recurrent \u201dgrammar cells", "author": ["V. Michalski", "R. Memisevic", "K. Konda"], "venue": "In NIPS,", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "In Proc.Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by v1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "Visual parsing after recovery from blindness", "author": ["Y. Ostrovsky", "E. Meyers", "S. Ganesh", "U. Mathur", "P. Sinha"], "venue": "Psychological Science,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2009}, {"title": "Learning visual motion in recurrent neural networks", "author": ["M. Pachitariu", "M. Sahani"], "venue": "In NIPS,", "citeRegEx": "Pachitariu and Sahani,? \\Q2012\\E", "shortCiteRegEx": "Pachitariu and Sahani", "year": 2012}, {"title": "Modeling natural images using gated mrfs", "author": ["M. Ranzato", "V. Mnih", "J. Susskind", "G. Hinton"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2013}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In NIPS,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Ucf101: A dataset of 101 human action classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G.E. Hinton", "G.W. Taylor"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["G.W. Taylor", "G.E. Hinton", "S.T. Roweis"], "venue": null, "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "In all likelihood, deep belief is not enough", "author": ["L. Theis", "S. Gerwinn", "F. Sinz", "M. Bethge"], "venue": null, "citeRegEx": "Theis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2011}, {"title": "Mixtures of conditional gaussian scale mixtures applied to multiscale image representations", "author": ["L. Theis", "R. Hosseini", "M. Bethge"], "venue": "PLoS ONE,", "citeRegEx": "Theis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2012}, {"title": "Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex", "author": ["J.H. van Hateren", "D.L. Ruderman"], "venue": "Royal Society,", "citeRegEx": "Hateren and Ruderman,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Ruderman", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In ICML,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Slow feature analysis: unsupervised learning of invariances", "author": ["L. Wiskott", "T. Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "Wiskott and Sejnowski,? \\Q2002\\E", "shortCiteRegEx": "Wiskott and Sejnowski", "year": 2002}, {"title": "Natural images, gaussian mixtures and dead leaves", "author": ["D. Zoran", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Zoran and Weiss,? \\Q2012\\E", "shortCiteRegEx": "Zoran and Weiss", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Despite great advances in object recognition, detection, and parsing over the past few years (Krizhevsky et al., 2012; Hariharan et al., 2014; Simonyan & Zisserman, 2014; Girshick et al., 2014), none of the widely used methods for these tasks relies on unlabeled data.", "startOffset": 93, "endOffset": 193}, {"referenceID": 6, "context": "Despite great advances in object recognition, detection, and parsing over the past few years (Krizhevsky et al., 2012; Hariharan et al., 2014; Simonyan & Zisserman, 2014; Girshick et al., 2014), none of the widely used methods for these tasks relies on unlabeled data.", "startOffset": 93, "endOffset": 193}, {"referenceID": 3, "context": "Despite great advances in object recognition, detection, and parsing over the past few years (Krizhevsky et al., 2012; Hariharan et al., 2014; Simonyan & Zisserman, 2014; Girshick et al., 2014), none of the widely used methods for these tasks relies on unlabeled data.", "startOffset": 93, "endOffset": 193}, {"referenceID": 29, "context": "There is a vast body of literature on unsupervised learning for vision, for example (Hinton & Salakhutdinov, 2006; Vincent et al., 2008; Kavukcuoglu et al., 2008), but these methods have not found success in practical applications yet.", "startOffset": 84, "endOffset": 162}, {"referenceID": 10, "context": "There is a vast body of literature on unsupervised learning for vision, for example (Hinton & Salakhutdinov, 2006; Vincent et al., 2008; Kavukcuoglu et al., 2008), but these methods have not found success in practical applications yet.", "startOffset": 84, "endOffset": 162}, {"referenceID": 4, "context": "Although some recent works have shown some progress on smaller resolution images (Goodfellow et al., 2014; Zoran & Weiss, 2012; Ranzato et al., 2013; Theis et al., 2011) using generative models, it is unclear how these models can scale to handle full resolution inputs, due to the curse of dimensionality.", "startOffset": 81, "endOffset": 169}, {"referenceID": 20, "context": "Although some recent works have shown some progress on smaller resolution images (Goodfellow et al., 2014; Zoran & Weiss, 2012; Ranzato et al., 2013; Theis et al., 2011) using generative models, it is unclear how these models can scale to handle full resolution inputs, due to the curse of dimensionality.", "startOffset": 81, "endOffset": 169}, {"referenceID": 26, "context": "Although some recent works have shown some progress on smaller resolution images (Goodfellow et al., 2014; Zoran & Weiss, 2012; Ranzato et al., 2013; Theis et al., 2011) using generative models, it is unclear how these models can scale to handle full resolution inputs, due to the curse of dimensionality.", "startOffset": 81, "endOffset": 169}, {"referenceID": 10, "context": "Other models attempt to overcome this problem by using priors over the features, such as sparsity (Olshausen & Field, 1997; Kavukcuoglu et al., 2008; Lee et al., 2007).", "startOffset": 98, "endOffset": 167}, {"referenceID": 12, "context": "Other models attempt to overcome this problem by using priors over the features, such as sparsity (Olshausen & Field, 1997; Kavukcuoglu et al., 2008; Lee et al., 2007).", "startOffset": 98, "endOffset": 167}, {"referenceID": 27, "context": "While several authors have reported realistic generation of small image patches, fewer works have operated at the scale of high-resolution images (Theis et al., 2012; Ranzato et al., 2013) and success has been more limited.", "startOffset": 146, "endOffset": 188}, {"referenceID": 20, "context": "While several authors have reported realistic generation of small image patches, fewer works have operated at the scale of high-resolution images (Theis et al., 2012; Ranzato et al., 2013) and success has been more limited.", "startOffset": 146, "endOffset": 188}, {"referenceID": 18, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009).", "startOffset": 211, "endOffset": 235}, {"referenceID": 24, "context": "Perhaps with the only exception given by the layered model proposed by Jojic & Frey (2001), all the above mentioned models have been demonstrated on either small image patches or small synthetic datasets (Sutskever et al., 2009; Pachitariu & Sahani, 2012; Michalski et al., 2014).", "startOffset": 204, "endOffset": 279}, {"referenceID": 15, "context": "Perhaps with the only exception given by the layered model proposed by Jojic & Frey (2001), all the above mentioned models have been demonstrated on either small image patches or small synthetic datasets (Sutskever et al., 2009; Pachitariu & Sahani, 2012; Michalski et al., 2014).", "startOffset": 204, "endOffset": 279}, {"referenceID": 0, "context": "Inspired by work in the language modeling community (Bengio et al., 2003; Mikolov et al., 2010), we propose a method that is very simple yet very effective, as it can be applied to full resolution videos at a modest computational cost.", "startOffset": 52, "endOffset": 95}, {"referenceID": 16, "context": "Inspired by work in the language modeling community (Bengio et al., 2003; Mikolov et al., 2010), we propose a method that is very simple yet very effective, as it can be applied to full resolution videos at a modest computational cost.", "startOffset": 52, "endOffset": 95}, {"referenceID": 15, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful information about how objects deform, about occlusion, object boundaries, depth, and so on. By just looking at a patch at the same spatial location across consecutive time steps, the system can infer what the relevant invariances and local deformations are. Even more so when studying generative models of natural images, modeling becomes easier when conditioning on the previous frame as opposed to unconditional generation, yet this task is non-trivial and useful as the model has to understand how to propagate motion and cope with occlusion. Research on models that learn unsupervised from videos is still in its infancy. In their seminal work, van Hateren & Ruderman (1998) and Hurri & Hyv\u00e4rinen (2003) have applied ICA techniques to small video cubes of patches.", "startOffset": 212, "endOffset": 973}, {"referenceID": 15, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful information about how objects deform, about occlusion, object boundaries, depth, and so on. By just looking at a patch at the same spatial location across consecutive time steps, the system can infer what the relevant invariances and local deformations are. Even more so when studying generative models of natural images, modeling becomes easier when conditioning on the previous frame as opposed to unconditional generation, yet this task is non-trivial and useful as the model has to understand how to propagate motion and cope with occlusion. Research on models that learn unsupervised from videos is still in its infancy. In their seminal work, van Hateren & Ruderman (1998) and Hurri & Hyv\u00e4rinen (2003) have applied ICA techniques to small video cubes of patches.", "startOffset": 212, "endOffset": 1002}, {"referenceID": 15, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful information about how objects deform, about occlusion, object boundaries, depth, and so on. By just looking at a patch at the same spatial location across consecutive time steps, the system can infer what the relevant invariances and local deformations are. Even more so when studying generative models of natural images, modeling becomes easier when conditioning on the previous frame as opposed to unconditional generation, yet this task is non-trivial and useful as the model has to understand how to propagate motion and cope with occlusion. Research on models that learn unsupervised from videos is still in its infancy. In their seminal work, van Hateren & Ruderman (1998) and Hurri & Hyv\u00e4rinen (2003) have applied ICA techniques to small video cubes of patches. Wiskott & Sejnowski (2002) have proposed instead a method based on slowness of features through time, an idea later extended in a bilinear model by Gregor & LeCun (2010).", "startOffset": 212, "endOffset": 1090}, {"referenceID": 15, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful information about how objects deform, about occlusion, object boundaries, depth, and so on. By just looking at a patch at the same spatial location across consecutive time steps, the system can infer what the relevant invariances and local deformations are. Even more so when studying generative models of natural images, modeling becomes easier when conditioning on the previous frame as opposed to unconditional generation, yet this task is non-trivial and useful as the model has to understand how to propagate motion and cope with occlusion. Research on models that learn unsupervised from videos is still in its infancy. In their seminal work, van Hateren & Ruderman (1998) and Hurri & Hyv\u00e4rinen (2003) have applied ICA techniques to small video cubes of patches. Wiskott & Sejnowski (2002) have proposed instead a method based on slowness of features through time, an idea later extended in a bilinear model by Gregor & LeCun (2010). Bilinear models of transformations between nearby frames have also been investigated by Memisevic & Hinton (2009); Sutskever et al.", "startOffset": 212, "endOffset": 1233}, {"referenceID": 15, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful information about how objects deform, about occlusion, object boundaries, depth, and so on. By just looking at a patch at the same spatial location across consecutive time steps, the system can infer what the relevant invariances and local deformations are. Even more so when studying generative models of natural images, modeling becomes easier when conditioning on the previous frame as opposed to unconditional generation, yet this task is non-trivial and useful as the model has to understand how to propagate motion and cope with occlusion. Research on models that learn unsupervised from videos is still in its infancy. In their seminal work, van Hateren & Ruderman (1998) and Hurri & Hyv\u00e4rinen (2003) have applied ICA techniques to small video cubes of patches. Wiskott & Sejnowski (2002) have proposed instead a method based on slowness of features through time, an idea later extended in a bilinear model by Gregor & LeCun (2010). Bilinear models of transformations between nearby frames have also been investigated by Memisevic & Hinton (2009); Sutskever et al.", "startOffset": 212, "endOffset": 1348}, {"referenceID": 15, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful information about how objects deform, about occlusion, object boundaries, depth, and so on. By just looking at a patch at the same spatial location across consecutive time steps, the system can infer what the relevant invariances and local deformations are. Even more so when studying generative models of natural images, modeling becomes easier when conditioning on the previous frame as opposed to unconditional generation, yet this task is non-trivial and useful as the model has to understand how to propagate motion and cope with occlusion. Research on models that learn unsupervised from videos is still in its infancy. In their seminal work, van Hateren & Ruderman (1998) and Hurri & Hyv\u00e4rinen (2003) have applied ICA techniques to small video cubes of patches. Wiskott & Sejnowski (2002) have proposed instead a method based on slowness of features through time, an idea later extended in a bilinear model by Gregor & LeCun (2010). Bilinear models of transformations between nearby frames have also been investigated by Memisevic & Hinton (2009); Sutskever et al. (2009); Taylor et al.", "startOffset": 212, "endOffset": 1373}, {"referenceID": 15, "context": "On the other hand, many have argued that learning without (human) supervision can become much easier once we consider not just a collection of independently drawn natural images, but a dataset of natural videos (Ostrovsky et al., 2009). Then, spatial-temporal correlations can provide powerful information about how objects deform, about occlusion, object boundaries, depth, and so on. By just looking at a patch at the same spatial location across consecutive time steps, the system can infer what the relevant invariances and local deformations are. Even more so when studying generative models of natural images, modeling becomes easier when conditioning on the previous frame as opposed to unconditional generation, yet this task is non-trivial and useful as the model has to understand how to propagate motion and cope with occlusion. Research on models that learn unsupervised from videos is still in its infancy. In their seminal work, van Hateren & Ruderman (1998) and Hurri & Hyv\u00e4rinen (2003) have applied ICA techniques to small video cubes of patches. Wiskott & Sejnowski (2002) have proposed instead a method based on slowness of features through time, an idea later extended in a bilinear model by Gregor & LeCun (2010). Bilinear models of transformations between nearby frames have also been investigated by Memisevic & Hinton (2009); Sutskever et al. (2009); Taylor et al. (2011); Michalski et al.", "startOffset": 212, "endOffset": 1395}, {"referenceID": 14, "context": "(2011); Michalski et al. (2014) as well as Miao & Rao (2007) via Lie group theory.", "startOffset": 8, "endOffset": 32}, {"referenceID": 14, "context": "(2011); Michalski et al. (2014) as well as Miao & Rao (2007) via Lie group theory.", "startOffset": 8, "endOffset": 61}, {"referenceID": 14, "context": "(2011); Michalski et al. (2014) as well as Miao & Rao (2007) via Lie group theory. Related work by Cadieu & Olshausen (2009) uses a hierarchical model with a predefined decomposition between amplitude (which varies slowly) and phase (encoding actual transformations).", "startOffset": 8, "endOffset": 125}, {"referenceID": 14, "context": "(2011); Michalski et al. (2014) as well as Miao & Rao (2007) via Lie group theory. Related work by Cadieu & Olshausen (2009) uses a hierarchical model with a predefined decomposition between amplitude (which varies slowly) and phase (encoding actual transformations). Perhaps with the only exception given by the layered model proposed by Jojic & Frey (2001), all the above mentioned models have been demonstrated on either small image patches or small synthetic datasets (Sutskever et al.", "startOffset": 8, "endOffset": 359}, {"referenceID": 0, "context": "In order to design a system that tackles these tasks, we draw inspiration from classical methods from natural language processing, namely n-grams, neural net language models (Bengio et al., 2003) and recurrent neural networks (Mikolov et al.", "startOffset": 174, "endOffset": 195}, {"referenceID": 16, "context": ", 2003) and recurrent neural networks (Mikolov et al., 2010).", "startOffset": 38, "endOffset": 60}, {"referenceID": 0, "context": "2 NEURAL NET LANGUAGE MODEL The neural net language model (NN) (Bengio et al., 2003) is a parametric and non-linear extension of n-grams.", "startOffset": 63, "endOffset": 84}, {"referenceID": 16, "context": "3 RECURRENT NEURAL NETWORK The recurrent neural network (rNN) (Mikolov et al., 2010), works similarly to the model above except that: 1) it takes only one input at the time, and 2) the hidden layer of the MLP takes as input also a linear transformation of the hidden state at the previous time step.", "startOffset": 62, "endOffset": 84}, {"referenceID": 21, "context": "The equations that regulate the rNN are: ht\u22121 = \u03c3(Whht\u22122 +Wx1(Xt\u22121)), p(Xt|ht\u22121) = SM(Woht\u22121 + bo) (3) Training the parameters of the model, {Wx,Wh,Wo,bo}, proceeds by minimization of the standard cross-entropy loss on the next symbol using back-propagation through time (Rumelhart et al., 1986) and gradient clipping (Mikolov et al.", "startOffset": 271, "endOffset": 295}, {"referenceID": 16, "context": ", 1986) and gradient clipping (Mikolov et al., 2010).", "startOffset": 30, "endOffset": 52}, {"referenceID": 23, "context": "For the UCF 101 dataset, we used the standard split (Soomro et al., 2012).", "startOffset": 52, "endOffset": 73}, {"referenceID": 23, "context": "The UCF-101 dataset (Soomro et al., 2012) is a standard benchmark dataset for human action recognition.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": "Multi-Scale Prediction: Multiscale, coarse-to-fine approaches are classic in motion estimation models (Brox et al., 2004).", "startOffset": 102, "endOffset": 121}], "year": 2016, "abstractText": "We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences.", "creator": "LaTeX with hyperref package"}}}