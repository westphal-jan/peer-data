{"id": "1412.6610", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Scoring and Classifying with Gated Auto-encoders", "abstract": "Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modelling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to RBMs. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification.", "histories": [["v1", "Sat, 20 Dec 2014 05:46:05 GMT  (843kb,D)", "https://arxiv.org/abs/1412.6610v1", "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review"], ["v2", "Thu, 26 Feb 2015 18:05:21 GMT  (843kb,D)", "http://arxiv.org/abs/1412.6610v2", "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review"], ["v3", "Mon, 2 Mar 2015 16:35:39 GMT  (843kb,D)", "http://arxiv.org/abs/1412.6610v3", "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review"], ["v4", "Thu, 2 Apr 2015 18:15:16 GMT  (849kb,D)", "http://arxiv.org/abs/1412.6610v4", null], ["v5", "Mon, 15 Jun 2015 00:25:47 GMT  (849kb,D)", "http://arxiv.org/abs/1412.6610v5", null]], "COMMENTS": "Eight pages plus one page reference and four pages of appendix. Submit to the ICLR2015 conference track for review", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["daniel jiwoong im", "graham w taylor"], "accepted": false, "id": "1412.6610"}, "pdf": {"name": "1412.6610.pdf", "metadata": {"source": "CRF", "title": "Scoring and Classifying with Gated Auto-encoders", "authors": ["Daniel Jiwoong Im", "Graham W. Taylor"], "emails": ["imj@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "2 Gated Auto-encoders", "text": "In this section, we will review the gated auto-encoder (GAE) transformations. However, due to spatial constraints, we will not review the classic auto-encoder. Instead, we will direct the reader to the reviews in [15,8] with which we share the notation. Similar to the classic auto-encoder, GAE consists of an encoder h (\u00b7) and decoder r (\u00b7). While the standard auto-encoder is a datapoint x, the GAE processes input pairs (x, y). GAE is usually trained to reconstruct y given x (\u00b7), although it can also be trained symmetrically, i.e. to reconstruct both y from x and x from y. Intuitively, GAE learns relationships between the inputs rather than representations of the inputs themselves. If x 6 = y, for example, it represents sequential frames of a video."}, {"heading": "3 Gated Auto-Encoder Scoring", "text": "In contrast to probabilistic views based on score matching [21,24,6] and regularization, the dynamic systems approach allows scoring among models with linear (real data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions. The method is also agnostic for the learning procedure used to train the model, which means that it is suitable for the different types of regularized auto-encoders recently proposed. In this section, we will show how to extend the view of dynamic systems to GAE.4."}, {"heading": "3.1 Vector field representation", "text": "Similar to [7], we will consider GAE as a dynamic system with a vector field starting from F (y | x) = r (y | x) \u2212 y. The vector field represents the local transformation that y | x undergoes as a result of the application of the reconstruction function r (y | x). Repeated application of the reconstruction function to an input y | x \u2192 r (y | x) \u2192 r (y | x) | x) \u2192 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 r (r \u00b7 \u00b7 \u00b7 r (y | x) results in a path whose dynamics can be regarded as a force field from a physical perspective. At each point, the potential force acting on a point is the gradient of a potential energy (negative quality) at that point. In this light, the GAE reconstruction can be regarded as shifting pairs of inputs x, y towards a lower energy."}, {"heading": "3.2 Scoring the GAE", "text": "As mentioned in Section 3.1, our goal is to find an energy surface so that we can express the energy for a certain pair (x, y). Furthermore, we show that this vector field is a conservative field and this means that the vector field represents a gradient of a scalar function, which in this case is the energy function of a GAE: r (y | x) \u2212 y = E.Hence, by calculating the path of the GAE (x, y), we can measure the energy along a path. Furthermore, the line integral of a conservative vector field Y is independent, allowing us to take the anti-derivative of the scalar function ()."}, {"heading": "4 Relationship to Restricted Boltzmann Machines", "text": "In this section we refer GAEs via the scoring function to other types of Restricted Boltzmann machines, such as the Factored Gated Conditional RBM [23] and the Mean-Covariance RBM [19]."}, {"heading": "4.1 Gated Auto-encoder and Factored Gated Conditional Restricted Boltzmann Machines", "text": "Kamyshanska and Memisevic showed that several hidden activation functions defined gradient fields, including sigmoid, softmax, tanh, linear rectified linear function (ReLU), module, and quadrature of the circle. These activation functions also apply to GAEs. In the case of the sigmoid activation function, \u03c3 = h (u) = 11 + exp (\u2212 u), our energy function results in E\u03c3 = 2% (1 + exp \u2212 (u)) \u2212 1du \u2212 12 (x2 + y2) + const, = 2 \u0445 k log (1 + exp (WHk \u00b7 (W Xx WXy)))) \u2212 1 2 (x2 + y2) + const.6 Note the conditional GAE, let us reconstruct only x given (x2 + y2) + const, this yield-dependent unit (y | x) = \u2211 k log (WH (WYk \u00b7 y WXrik \u00b7 x) + const.6 Let us reconstruct only x given."}, {"heading": "4.2 Mean-Covariance Auto-encoder and Mean-covariance Restricted Boltzmann Machines", "text": "The auto-encoder (cAE) covariance was introduced in [15]. It is also a specific form of symmetrically trained auto-encoder with identical inputs: x = y and bound input weights: WX = WY. It maintains a series of relational mapping units to model the covariance between pixels. It is possible to present a separate set of mapping units connected in pairs to only one of the inputs that model the mean intensity. In this case, the model becomes a Mean covariance auto-encoder (mcAE).Theorem 1. Consider a cAE with encoder and decoder: h (x) = h (WX \u2212 x) + b) r (x | h) = (WX) T (WXx) and energy (WH) Th (x))) in which the cAE is connected to free encoders and decoders."}, {"heading": "5 Classification with Gated Auto-encoders", "text": "Kamyshanska and Memisevic showed that an application of the ability to assign energy or values to auto encoders consists in the construction of a classifier consisting of class-specific auto encoders. In this section we examine two different paradigms for classification. Similarly, we look at the usual multi-class problem by first training class-specific auto encoders and using their energy functions as confidence values. We also look at the more difficult structured output problem, in particular the case of multi-label prediction, where a data point may have more than one associated label and there may be correlations between the labels."}, {"heading": "5.1 Classification using class-specific gated auto-encoders", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.2 Multi-label classification via optimization in label space", "text": "The dominant application of deep learning approaches to \"post-classification\" is the allocation of images to discrete classes (e.g. GAxi labelling), but many applications include \"structured outputs\" where the output variable is high-dimensional and has a complex, multi-modal common distribution. Structured output procedures can include tasks such as multi-label classification, where there are regularities learned in the output, and segmentation, where the output is as high-dimensional as the input. A key challenge for such approaches is the development of models that are able to capture complex, high-level structures such as shape, while our proposed work is still based on a deterministic model, we have shown that the energy or scoring function of GAE is equivalent, up to a constant conditional RBM, a model that has already been seen in structured prediction problems [18,12]. \"Post-GAE classification can be applied as a structured output problem.\""}, {"heading": "6 Conclusion", "text": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], but theoretical research on gated auto-encoders is limited apart from [15,4]. GAE has several fascinating properties that a classic auto-encoder does not have, based on its ability to model relationships between pixel intensities, rather than just the intensities themselves. This opens up a broader range of applications. In this paper, we derive some theoretical results for GAE that allow us to gain more insight and understanding of how it works. We cast GAE as a dynamic system driven by a vector field to analyze the model. In the first part of the paper, we have shown that the GAE could be evaluated according to energy functionality."}, {"heading": "A Gated Auto-encoder Scoring", "text": "To check whether the vector field can be written as a derivation of a scalar field (Vector field representationTo check that the vector field of a scalar field (Vector field) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector) (Vector (V"}, {"heading": "B Relation to other types of Restricted Boltzmann Machines", "text": "B.1 Gated Auto-encoder and Factored Gated Gated Conditional + conditional + conditional = implicated Boltzmann MachinesSuppose that the hidden activation function is a sigmoid. In addition, we define our gated auto-encoder to consist of an encoder h (\u00b7) and decoder r (\u00b7) like thath (x, y) = h (WH (WXx) (WXx) (WY). Note that the weights are not bound in this case. The energy function for the gated auto-encoder 16will be: The energy (x, y) is assumed to be (1 + exp (WX, WY, b) the parameters of the model."}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "\u00c9. Thibodeau-Laufer"], "venue": "arXiv preprint arXiv:1306.1091", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luob", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition 37, 1757\u20131771", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Improved learning of Gaussian-Bernoulli restricted Boltzmann machines", "author": ["K. Cho", "A. Ilin", "T. Raiko"], "venue": "ICANN. pp. 10\u201317", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Gated autoencoders with tied input weights", "author": ["A. Droniou", "O. Sigaud"], "venue": "ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["A. Guillaume", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "On autoencoder scoring", "author": ["H. Kamyshanska", "R. Memisevic"], "venue": "ICML. pp. 720\u2013728", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "The potential energy of an auto-encoder", "author": ["H. Kamyshanska", "R. Memisevic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 37(6), 1261\u20131273", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. rep., Department of Computer Science, University of Toronto", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "ICML", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploring compositional high order pattern potentials for structured output learning", "author": ["Y. Li", "D. Tarlow", "R. Zemel"], "venue": "CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning tags that vary within a song", "author": ["M.I. Mandel", "D. Eck", "Y. Bengio"], "venue": "ISMIR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "A web-based game for collecting music metadata", "author": ["M.I. Mandel", "D.P.W. Ellis"], "venue": "Journal New of Music Research 37, 151\u2013165", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based learning of higher-order image features", "author": ["R. Memisevic"], "venue": "ICCV", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Gated softmax classification", "author": ["R. Memisevic", "C. Zach", "G. Hinton", "M. Pollefeys"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to detect roads in high-resolution aerial images", "author": ["V. Mnih", "G. Hinton"], "venue": "Proceedings of the 11th European Conference on Computer Vision (ECCV)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Conditional restricted Boltzmann machines for structured output prediction", "author": ["V. Mnih", "H. Larochelle", "G.E. Hinton"], "venue": "UAI", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "author": ["M. Ranzato", "G.E. Hinton"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "N.D. Freitas", "B.M. Marlin"], "venue": "ICML. pp. 1201\u20131208", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Factored conditional restricted Boltzmann machines for modeling motion style", "author": ["G.W. Taylor", "G.E. Hinton"], "venue": "ICML. pp. 1025\u20131032", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "A connection between score matching and denoising auto-encoders", "author": ["P. Vincent"], "venue": "Neural Computation 23(7), 1661\u20131674", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P. Manzagol"], "venue": "ICML", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Gaussian-binary restricted Boltzmann machines on modeling natural image statistics", "author": ["N. Wang", "J. Melchior", "L. Wiskott"], "venue": "Tech. rep., Institut fur Neuroinformatik Ruhr-Universitat Bochum, Bochum, 44780, Germany", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Deep learning techniques, which employ several layers of representation learning, have achieved much recent success in machine learning benchmarks and competitions, however, most of these successes have been achieved with purely supervised learning methods and have relied on large amounts of labeled data [10,22].", "startOffset": 306, "endOffset": 313}, {"referenceID": 21, "context": "Deep learning techniques, which employ several layers of representation learning, have achieved much recent success in machine learning benchmarks and competitions, however, most of these successes have been achieved with purely supervised learning methods and have relied on large amounts of labeled data [10,22].", "startOffset": 306, "endOffset": 313}, {"referenceID": 0, "context": "Though progress has been slower, it is likely that unsupervised learning will be important to future advances in deep learning [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 19, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 129, "endOffset": 139}, {"referenceID": 24, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 129, "endOffset": 139}, {"referenceID": 20, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 129, "endOffset": 139}, {"referenceID": 5, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 193, "endOffset": 199}, {"referenceID": 23, "context": "Conceptually simple and easy to train via backpropagation, various regularized variants of the model have recently been proposed [20,25,21] as well as theoretical insights into their operation [6,24].", "startOffset": 193, "endOffset": 199}, {"referenceID": 6, "context": "Kamyshanska and Memisevic have recently shown how scores can be computed from an auto-encoder by interpreting it as a dynamical system [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 14, "context": "In this paper we turn our interest towards a variant of auto-encoders which are capable of learning higher-order features from data [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "We adopt the framework of [7] both conceptually and formally in developing a theory which yields insights into the operation of gated auto-encoders.", "startOffset": 26, "endOffset": 29}, {"referenceID": 14, "context": "Instead, we direct the reader to the reviews in [15,8] with which we share notation.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "Instead, we direct the reader to the reviews in [15,8] with which we share notation.", "startOffset": 48, "endOffset": 54}, {"referenceID": 14, "context": "We can also constrain the GAE to be a symmetric model by training it to reconstruct both x given y and y given x [15]:", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "The symmetric objective can be thought of as the non-probabilistic analogue of modeling a joint distribution over x and y as opposed to a conditional [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 6, "context": "In [7], the authors showed that data could be scored under an auto-encoder by interpreting the model as a dynamical system.", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "In contrast to the probabilistic views based on score matching [21,24,6] and regularization, the dynamical systems approach permits scoring under models with either linear (real-valued data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions.", "startOffset": 63, "endOffset": 72}, {"referenceID": 23, "context": "In contrast to the probabilistic views based on score matching [21,24,6] and regularization, the dynamical systems approach permits scoring under models with either linear (real-valued data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions.", "startOffset": 63, "endOffset": 72}, {"referenceID": 5, "context": "In contrast to the probabilistic views based on score matching [21,24,6] and regularization, the dynamical systems approach permits scoring under models with either linear (real-valued data) or sigmoid (binary data) outputs, as well as arbitrary hidden unit activation functions.", "startOffset": 63, "endOffset": 72}, {"referenceID": 6, "context": "Similar to [7], we will view the GAE as a dynamical system with the vector field defined by F (y|x) = r(y|x)\u2212 y.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In order to find an expression for the potential energy, the vector field must be able to be written as the derivative of a scalar field [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "Identical to [7], if h(u) is an element-wise activation function and we know its anti-derivative, then it is very simple to compute E(x,y).", "startOffset": 13, "endOffset": 16}, {"referenceID": 22, "context": "In this section, we relate GAEs through the scoring function to other types of Restricted Boltzmann Machines, such as the Factored Gated Conditional RBM [23] and the Mean-covariance RBM [19].", "startOffset": 153, "endOffset": 157}, {"referenceID": 18, "context": "In this section, we relate GAEs through the scoring function to other types of Restricted Boltzmann Machines, such as the Factored Gated Conditional RBM [23] and the Mean-covariance RBM [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 14, "context": "2 Mean-Covariance Auto-encoder and Mean-covariance Restricted Boltzmann Machines The Covariance auto-encoder (cAE) was introduced in [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "Moreover, consider a Covariance RBM [19] with Gaussian-distributed visibles and Bernoulli-distributed hiddens, with an energy function defined by", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "We can extend this analysis to the mcAE by using the above theorem and the results from [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "We know from Theorem 1 that Ec is equivalent to the free energy of a covariance RBM, and the results from [7] show that that Em is equivalent to the free energy of mean (classical) RBM.", "startOffset": 106, "endOffset": 109}, {"referenceID": 18, "context": "As shown in [19], the free energy of a mcRBM is equal to summing the free energies of a mean RBM and a covariance RBM.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "The approach proposed in [7] is to train K class-specific auto-encoders, each of which assigns a non-normalized energy to the data Ei (x) , i = 1 .", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "Experimental results We followed the same experimental setup as [16] where we used a standard set of \u201cDeep Learning Benchmarks\u201d [11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Experimental results We followed the same experimental setup as [16] where we used a standard set of \u201cDeep Learning Benchmarks\u201d [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "SVM and RBM results are from [24], DEEP and GSM are results from [15], and AES is from [7].", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "SVM and RBM results are from [24], DEEP and GSM are results from [15], and AES is from [7].", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "SVM and RBM results are from [24], DEEP and GSM are results from [15], and AES is from [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 17, "context": "Though our proposed work is based on a deterministic model, we have shown that the energy, or scoring function of the GAE is equivalent, up to a constant, to that of a conditional RBM, a model that has already seen some use in structured prediction problems [18,12].", "startOffset": 258, "endOffset": 265}, {"referenceID": 11, "context": "Though our proposed work is based on a deterministic model, we have shown that the energy, or scoring function of the GAE is equivalent, up to a constant, to that of a conditional RBM, a model that has already seen some use in structured prediction problems [18,12].", "startOffset": 258, "endOffset": 265}, {"referenceID": 16, "context": "GAE scoring can be applied to structured output problems as a type of \u201cpost-classification\u201d [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "followed the same experimental set up as [18].", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 96, "endOffset": 99}, {"referenceID": 12, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "Four multi-labeled datasets were considered: Yeast [5] consists of biological attributes, Scene [2] is image-based, and MTurk [13] and MajMin [14] are targeted towards tagging music.", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "We compared our proposed approaches to logistic regression, a standard MLP, and the two structured CRBM training algorithms presented in [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 19, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 20, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 23, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 6, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 72, "endOffset": 89}, {"referenceID": 14, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 167, "endOffset": 173}, {"referenceID": 3, "context": "There have been many theoretical and empirical studies on auto-encoders [25,20,21,24,6,7], however, the theoretical study of gated auto-encoders is limited apart from [15,4].", "startOffset": 167, "endOffset": 173}, {"referenceID": 6, "context": "In the first part of the paper, by following the same procedure as [7], we showed that the GAE could be scored according to an energy function.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "One interesting observation is that Gaussian-Bernoulli RBMs have been reported to be difficult to train [9,3], and the success of training RBMs is highly dependent on the training setup [26].", "startOffset": 104, "endOffset": 109}, {"referenceID": 2, "context": "One interesting observation is that Gaussian-Bernoulli RBMs have been reported to be difficult to train [9,3], and the success of training RBMs is highly dependent on the training setup [26].", "startOffset": 104, "endOffset": 109}, {"referenceID": 25, "context": "One interesting observation is that Gaussian-Bernoulli RBMs have been reported to be difficult to train [9,3], and the success of training RBMs is highly dependent on the training setup [26].", "startOffset": 186, "endOffset": 190}], "year": 2015, "abstractText": "Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modeling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to Restricted Boltzmann Machines. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification.", "creator": "LaTeX with hyperref package"}}}