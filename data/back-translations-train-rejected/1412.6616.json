{"id": "1412.6616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Outperforming Word2Vec on Analogy Tasks with Random Projections", "abstract": "We present a distributed vector representation based on a simplification of the BEAGLE system, designed in the context of the Sigma cognitive architecture. Our method does not require gradient-based training of neural networks, matrix decompositions as with LSA, or convolutions as with BEAGLE. All that is involved is a sum of random vectors and their pointwise products. Despite the simplicity of this technique, it gives state-of-the-art results on analogy problems, in most cases better than Word2Vec. To explain this success, we interpret it as a dimension reduction via random projection.", "histories": [["v1", "Sat, 20 Dec 2014 07:07:29 GMT  (24kb)", "http://arxiv.org/abs/1412.6616v1", "Submission for ICLR 2015"], ["v2", "Tue, 17 Feb 2015 17:31:58 GMT  (0kb,I)", "http://arxiv.org/abs/1412.6616v2", "This paper has been withdrawn due to problems pointed out in review"]], "COMMENTS": "Submission for ICLR 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["abram demski", "volkan ustun", "paul rosenbloom", "cody kommers"], "accepted": false, "id": "1412.6616"}, "pdf": {"name": "1412.6616.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["demski@usc.edu", "ustun@ict.usc.edu", "rosenbloom@ict.usc.edu", "cydeko@ucla.edu"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.66 16v1 [We present a distributed vector representation based on a simplification of the BEAGLE system designed in the context of the cognitive architecture of Sigma. Our method does not require gradient-based training of neural networks, matrix decompositions as in LSA or coils as in BEAGLE. It is merely a sum of random vectors and their meaningless products. Despite the simplicity of this technique, it delivers state-of-the-art results on analogy problems, in most cases better than Word2Vec. To explain this success, we interpret it as a reduction in size by random projection."}, {"heading": "1 INTRODUCTION", "text": "DVRS is a distributed representation of words designed for the Sigma cognitive architecture (Ustun et al., 2014).Inspired by the design of BEAGLE (Jones & Mewhort, 2007), DVRS constructs vector representations for words by summarizing vectors that represent its experience.The details of DVRS have been strongly influenced by the Sigma cognitive architecture.Implementing distributed representations in a cognitive architecture such as Sigma offers design limitations due to the underlying cognitive theory that embodies the architecture.Unlike pure machine learning, performance is not the only goal; we also hope to find cognitive principles. Nevertheless, the performance has been quite good. DVRS does not rely on gradient-based training of neural embedding (Mikolov et al., 2013; Mnih & Kavukcuoglu, 2013)."}, {"heading": "2 DISTRIBUTED REPRESENTATIONS OF WORDS", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3 THE DVRS MODEL", "text": "It is not only the way in which people move in the world in the world, but also the way in which they move in the world, in which they move, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live"}, {"heading": "3.1 GOOGLE WORD ANALOGY TEST", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "100 15.5% 32.5%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "200 22.9% 41.7%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "512 28.3% 46.5%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1024 30.1% (30.5%) 47.1%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "100 25.4% 3.3% 32.5%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "200 32.4% 4.6% 41.7%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "512 35.2% 5.5% 46.5%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1024 36.2% 5.8% 47.1%", "text": "As a result, DVRS falls well short of Word2Vec. It seems possible that DVRS will hit a plateau that cannot improve with more data. Table 4 shows a breakdown of accuracy between DVRS and Word2Vec, who were trained on Enwik9 with a vector size of 512. DVRS model captures semantic information better, while the original Word2Vec captures syntactic information better. A simple implementation of DVRS mode in Common Lisp without optimized parallel processing takes 18.5 minutes to train on Enwik8 with a vector size of 100 on a MacBookAir4,2. It takes 4.5 minutes for Word2Vec to train with the same data on the same machine, with the default configuration using 20 threads."}, {"heading": "3.2 MILLER ANALOGIES TEST", "text": "The Miller Analogy Test (MAT) is an assessment of the analytical skills used to admit graduates, similar to the Graduate Record Examinations (GRE). Performance at the MAT, on average, predicts individual graduate grades better than graduate grades or graduate grades in the GPA. As a rigorous test of the cognitive abilities of individuals with above-average intelligence, competitive results at the MAT suggest sophisticated reasoning and representation similar to a well-educated person. We selected 150 sample MAT questions from a free online test preparation website to compare the performance of DVRS and Word2Vec. These analogies require sophisticated vocabulary and complex relational representation. Each analogy takes the form A: B:: C: (a, b, c, d)."}, {"heading": "4 RANDOM PROJECTIONS", "text": "It would be helpful to explain the performance of the DVRS with a mathematical model of what it is to compare arithmetic principles such as the Mannille Theory of 2001. Why does the addition of random vectors together give us some value? Why is it possible to solve analogy problems with vector additions and subtractions, and why is the cosinal similarity semantically meaningful? Analysis of this section cannot fully answer these questions, but useful things can be said by considering DVRS as a random projection method. Random projections are a very simple method for reducing the dimensionality of a randomly chosen linear transformation from a high-dimensional space to a low-dimensional space (a random matrix). It is justified by the Johnson-Lindenstrauss Lemma, which found that relative distances are most likely to be preserved under random projection (Indyk & Motwani, 1998). This shows that almost all of the dimensions are good enough to select a distance (so that almost all the dimensions are)."}, {"heading": "4.1 PROPERTIES OF RANDOM VECTORS", "text": "The most important property for the system is the importance of cosmic similarity. Why does this measure of similarity work so well? What can we say about the numbers that result from it? The argument of this section is that cosmic similarity counts the proportion of features that have two lexical vectors in common. (DVRS normalizes all vectors of length 1 so that there is no difference between the cosmic similarities and the Dot product.) The dot product of two independent random vectors of unit length is very likely very close to zero. In other words, random vectors are almost orthogonal. This has been known for some time, but was recently investigated in Cai et al. (2013) Due to this fact it is possible to sum up the sum of random vectors."}, {"heading": "4.2 DVRS AS A RANDOM PROJECTION", "text": "That is, it is not as if we can capture the matrix multiplication by the data and the summary of the vectors for each word we observe. To generate the lexical vector for one of the words, we count together in the data. This gives us a co-occurrence with words, for example. The numbers in the columns correspond to the surrounding vectors for each word. To generate the lexical vector for one of the words, we give a co-occurrence of vector with 5 elements. Table 5 is the projection matrix that transforms this co-occurrence vector into the lexical vector for the world."}, {"heading": "4.3 DISCUSSION", "text": "We introduced DVRS, a novel distributed representation that compares well with Word2Vec in analogy tests. DVRS seems to do well with particularly semantic analogies, while Word2Vec tends to do better with grammatical analogies. DVRS does not perform better with Miller analogies than Word2Vec, and more worryingly, it has not significantly improved when feeding in significantly more data, making it perform worse than Word2Vec in comparison to this dataset. This was a surprise, and DVRS does not require gradient descent training or other optimization processes, nor does it use a matrix factoring or a probable topic model. Instead, it constructs meaningful representations of sums and pointedly products of random vectors. The result can be described as a random projection that indicates that cosine similarity of the reading vectors can effectively measure the similarity of characteristics."}], "references": [{"title": "An algorithmic theory of learning: Robust concepts and random projection", "author": ["Arriaga", "Rosa I", "Vempala", "Santosh"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Arriaga et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Arriaga et al\\.", "year": 1999}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germ\u00e1n"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Bingham", "Ella", "Mannila", "Heikki"], "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Bingham et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bingham et al\\.", "year": 2001}, {"title": "Distributions of angles in random packing on spheres", "author": ["Cai", "Tony", "Fan", "Jianqing", "Jiang", "Tiefeng"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Experiments with random projection", "author": ["Dasgupta", "Sanjoy"], "venue": "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Dasgupta and Sanjoy.,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta and Sanjoy.", "year": 2000}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Indyk", "Piotr", "Motwani", "Rajeev"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "Indyk et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Indyk et al\\.", "year": 1998}, {"title": "Representing word meaning and order information in a composite holographic lexicon", "author": ["Jones", "Michael N", "Mewhort", "Douglas JK"], "venue": "Psychological review,", "citeRegEx": "Jones et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2007}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Omer", "Goldberg", "Yoav"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["Mnih", "Andriy", "Kavukcuoglu", "Koray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Holographic reduced representations", "author": ["Plate", "Tony A"], "venue": "Neural networks, IEEE transactions on,", "citeRegEx": "Plate and A.,? \\Q1995\\E", "shortCiteRegEx": "Plate and A.", "year": 1995}, {"title": "Distributed vector representations of words in the sigma cognitive architecture", "author": ["Ustun", "Volkan", "Rosenbloom", "Paul S", "Sagae", "Kenji", "Demski", "Abram"], "venue": "In Proceedings of the 7th conference on Artificial General Intelligence,", "citeRegEx": "Ustun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ustun et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "DVRS is a distributed representation of words designed for the Sigma cognitive architecture (Ustun et al., 2014).", "startOffset": 92, "endOffset": 112}, {"referenceID": 8, "context": "DVRS does not rely on the gradient-based training of neural embeddings (Mikolov et al., 2013; Mnih & Kavukcuoglu, 2013), or optimization of any other objective as with GloVe (Pennington et al.", "startOffset": 71, "endOffset": 119}, {"referenceID": 10, "context": ", 2013; Mnih & Kavukcuoglu, 2013), or optimization of any other objective as with GloVe (Pennington et al., 2014).", "startOffset": 88, "endOffset": 113}, {"referenceID": 1, "context": "It does not involve a matrix factorization as with latent semantic analysis and similar techniques (Baroni et al., 2014).", "startOffset": 99, "endOffset": 120}, {"referenceID": 1, "context": "Neural models are compared extensively to LSA-style approaches by Baroni et al. (2014). Neural models, and in particular the Word2Vec model, are declared the winner.", "startOffset": 66, "endOffset": 87}, {"referenceID": 12, "context": "This fit well with the existing operations in Sigma, which uses the summary-product algorithm Ustun et al. (2014), and is less expensive, taking O(n) time.", "startOffset": 94, "endOffset": 114}, {"referenceID": 8, "context": "The information stored in these features is more similar to the skip-gram architecture in Mikolov et al. (2013), rather than the n-gram architecture of BEAGLE.", "startOffset": 90, "endOffset": 112}, {"referenceID": 8, "context": "The information stored in these features is more similar to the skip-gram architecture in Mikolov et al. (2013), rather than the n-gram architecture of BEAGLE. DVRS uses the binding operation only to make pairwise bindings of words and word locations, whereas BEAGLE applies it successively to bind short strings of words together. Additionally, the choice of binding operation in DVRS is different. BEAGLE used circular convolution to bind two vectors together into a third. DVRS instead uses pointwise product: for two vectors ~v and ~ w, the pointwise product is ~v .\u2217 ~ w = \u3008v1w1, ..., vdwd\u3009. We performed limited tests showing no advantage to circular convolution, detailed in Ustun et al. (2014). A similar formula to ours is mention in Mnih & Kavukcuoglu (2013).", "startOffset": 90, "endOffset": 702}, {"referenceID": 8, "context": "The information stored in these features is more similar to the skip-gram architecture in Mikolov et al. (2013), rather than the n-gram architecture of BEAGLE. DVRS uses the binding operation only to make pairwise bindings of words and word locations, whereas BEAGLE applies it successively to bind short strings of words together. Additionally, the choice of binding operation in DVRS is different. BEAGLE used circular convolution to bind two vectors together into a third. DVRS instead uses pointwise product: for two vectors ~v and ~ w, the pointwise product is ~v .\u2217 ~ w = \u3008v1w1, ..., vdwd\u3009. We performed limited tests showing no advantage to circular convolution, detailed in Ustun et al. (2014). A similar formula to ours is mention in Mnih & Kavukcuoglu (2013). The co-occurrence features consist of the environment vectors of all other words in the current paragraph.", "startOffset": 90, "endOffset": 769}, {"referenceID": 3, "context": "This has been known for some time, but was studied in detail recently in Cai et al. (2013). Due to this, it is possible to sum random vectors ~s = \u2211", "startOffset": 73, "endOffset": 91}, {"referenceID": 3, "context": "Cai et al. (2013) examine the properties of a set of n random vectors of dimensionality d.", "startOffset": 0, "endOffset": 18}], "year": 2014, "abstractText": "We present a distributed vector representation based on a simplification of the BEAGLE system, designed in the context of the Sigma cognitive architecture. Our method does not require gradient-based training of neural networks, matrix decompositions as with LSA, or convolutions as with BEAGLE. All that is involved is a sum of random vectors and their pointwise products. Despite the simplicity of this technique, it gives state-of-the-art results on analogy problems, in most cases better than Word2Vec. To explain this success, we interpret it as a dimension reduction via random projection.", "creator": "LaTeX with hyperref package"}}}