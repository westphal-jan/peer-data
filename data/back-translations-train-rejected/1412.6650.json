{"id": "1412.6650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Incremental Adaptation Strategies for Neural Network Language Models", "abstract": "It is today acknowledged that neural network language models outperform back- off language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or rely on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT en- vironment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without over-fitting the small adaptation data.", "histories": [["v1", "Sat, 20 Dec 2014 13:06:05 GMT  (280kb,D)", "https://arxiv.org/abs/1412.6650v1", null], ["v2", "Tue, 23 Dec 2014 13:43:19 GMT  (280kb,D)", "http://arxiv.org/abs/1412.6650v2", "submitted as conference paper to ICLR 2015"], ["v3", "Tue, 23 Jun 2015 11:36:36 GMT  (39kb,D)", "http://arxiv.org/abs/1412.6650v3", "accepted as workshop paper at ACL-IJCNLP 2015"], ["v4", "Tue, 7 Jul 2015 14:54:51 GMT  (36kb,D)", "http://arxiv.org/abs/1412.6650v4", "accepted as workshop paper at ACL-IJCNLP 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CL cs.LG", "authors": ["aram ter-sarkisov", "holger schwenk", "loic barrault", "fethi bougares"], "accepted": false, "id": "1412.6650"}, "pdf": {"name": "1412.6650.pdf", "metadata": {"source": "CRF", "title": "Incremental Adaptation Strategies for Neural Network Language Models", "authors": ["Aram Ter-Sarkisov", "Holger Schwenk", "Fethi Bougares"], "emails": ["tersarkisov@lium.univ-lemans.fr", "holger.schwenk@lium.univ-lemans.fr", "loic.barrault@lium.univ-lemans.fr", "fethi.bougares@lium.univ-lemans.fr"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcnlrVo"}, {"heading": "2 Related work", "text": "In recent years it has been shown that in the USA, in the USA, in the USA, in Europe, in Europe, in Europe, in the world, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3 Statistical Machine Translation", "text": "In the statistical approach of machine translation, all models are automatically estimated on the basis of examples. Let's say that we want to translate a sentence in the source language s into a sentence in the target language t. Then, the basic SMT equation using the Bayes rule is: t \u0445 = argmax t P (t | s) = argmax t P (s | t) P (t) (1) (The translation model P (s | t) is estimated from requests, bilingual sentence alignment data, and the language model P (t) from monolingual data in the target language. A popular approach is phrase-based models that translate short sequences together (Koehn et al., 2003; Och undNey, 2003). The translation probabilities of these phrase pairs are usually estimated on the basis of simple relative frequency. LM is usually a 4 gram reverse model. The log-linear approach is commonly used to take into account more models (Och, 2003), rather than a better translation, and these are just one of the relative pairs of languages."}, {"heading": "4 Continuous Space Language Model", "text": "The basic architecture of a CSLM is illustrated in Figure 1. Words are initially projected onto a continuous representation, the remaining part of the network estimates the probabilities. Normally, a hidden output layer and a softmax output layer are used, but recent studies have shown that deeper architecture performs better (Schwenk et al., 2014). We will use three hidden tanh and a softmax output layer, as shown in Figure 1. This type of architecture is now well known and the reader is referred to the literature for further details, e.g. (Schwenk, 2007).All of our experiments were conducted with the open source CSLM toolkit4 (Schwenk, 2013), which has been extended for our purposes. A major challenge for neural network LMs is dealing with the words on the output layer, as a softmax normalization for large vocabularies would be very costly. Various solutions have been proposed: shortlists (Schwenk, 2007), a class decomposition (Mikoloet, we could apply the 2011 short scheme, or the 2011 hierarchy equally)."}, {"heading": "4.1 Adaptation schemes", "text": "As mentioned above, the most popular and successful adjustment schemes for standard backoff LMs are data selection and mixing models, both of which could also be applied to CSLMs. In practice, this would mean that we train a completely new CSLM on data selected through the adjustment process, or that we train several CSLMs, e.g. a generic and task specific, and combine them linearly or loglinear. However, full training of a CSLM usually requires a considerable amount of time, often several hours or even days depending on the size of available training data. The basic idea of both techniques is not to train new models, but to slightly modify the existing CSLMs and their combination in order to accommodate the new training data. In the first method, we implement CSLM adjustment schemes that can be performed in minutes. The basic idea of both techniques is not to train new models, but to slightly modify existing CSLMs to accommodate the new training data."}, {"heading": "5 Task and baselines", "text": "Our task is to improve an SMT system that is tightly integrated into an open source CAT tool for post-processing of professional human translators. This tool and the algorithms for updating standard phrases-based SMT systems were developed as part of the European project MateCat (Cettolo et al., 2014). We look at the translation of legal texts from English into German and French. The available resources for each language pair are in Table 1. Each SMT system is based on the Moses toolkit (Koehn et al., 2007) and is structured according to the following procedure: First, we perform data selection on the parallel and monolingual corpora in order to extract the data that is most representative of our development. In our case, we are interested in the translation of legal documents. Data selection is now a well-established method in the SMT community."}, {"heading": "5.1 Results for the English/German system", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "5.2 Results for the English/French system", "text": "In fact, it is true that most of them are a very limited group, capable of asserting their interests."}, {"heading": "6 Conclusions", "text": "In our case, we want to integrate user corrections so that a statistical machine translation system performs better on similar texts. Our task, which corresponds to the concrete needs in real-world applications, is the translation of a document by a human over several days. An SMT system is available to the human translator, which suggests translation hypotheses in order to speed up its work (post-editing). After one working day, we adapt the CSLM to the translations that have already been performed by the human translator and show that the SMT system performs better on the remaining part of the documentation. We examined two adaptation strategies: further training an existing neural network LM and the insertion of an adaptation layer, whereby the weight updates are limited to this level. In both cases, the network is trained on a combination of adaptation data (3-15k words) and a part of a similar English language."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Xiaodong He", "Jianfeng Gao"], "venue": "In EMNLP,", "citeRegEx": "Axelrod et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Incremental Adaptation of Speech-toSpeech Translation", "author": ["Bach et al.2009] Nguyen Bach", "Roger Hsiao", "Matthias Eck", "Paisarn Charoenpornsawat", "Stephan Vogel", "Tanja Schultz", "Ian Lane", "Alex Waibel", "Alan W. Black"], "venue": "In NAACL,", "citeRegEx": "Bach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2009}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In NIPS workshop on Modern Machine Learning and Natural Language Processing", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Evaluating the Learning Curve of Domain Adaptive Statistical MachineTranslation Systems", "author": ["Mauro Cettolo", "Marcello Federico", "Christian Buck"], "venue": "In Workshop on SMT,", "citeRegEx": "Bertoldi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2012}, {"title": "Translation project adaptation for MT-enhanced computer assisted translation", "author": ["Nicola Bertoldi", "Marcello Federico", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Christophe Servan"], "venue": "Machine Translation,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Adaptation data selection using neural language models: Experiments in machine translation", "author": ["Duh et al.2013] Kevin Duh", "Graham Neubig", "Katsuhito Sudoh", "Hajime Tsukada"], "venue": null, "citeRegEx": "Duh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2013}, {"title": "Mixture-model adaptation for SMT", "author": ["Foster", "Kuhn2007] George Foster", "Roland Kuhn"], "venue": "In EMNLP,", "citeRegEx": "Foster et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2007}, {"title": "Improving language model adaptation using automatic data selection and neural network", "author": ["Shahab Jalalvand"], "venue": "In RANLP,", "citeRegEx": "Jalalvand.,? \\Q2013\\E", "shortCiteRegEx": "Jalalvand.", "year": 2013}, {"title": "Experiments in domain adaptation for statistical machine translation", "author": ["Koehn", "Schroeder2007] Philipp Koehn", "Josh Schroeder"], "venue": "In Second Workshop on SMT,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based machine translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In HLT/NACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "ACL, demonstration session.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Recurrent neural network based language modeling in meeting recognition", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget"], "venue": "In INTERSPEECH,", "citeRegEx": "Kombrink et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kombrink et al\\.", "year": 2011}, {"title": "Structured output layer neural network language model", "author": ["Le et al.2011] Hai-Son Le", "I. Oparin", "A. Allauzen", "JL. Gauvain", "F. Yvon"], "venue": "In ICASSP,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Continuous space translation models with neural networks", "author": ["Le et al.2012] Hai-Son Le", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Intelligent selection of language model training data", "author": ["Moore", "Lewis2010] Robert C. Moore", "William Lewis"], "venue": "In ACL,", "citeRegEx": "Moore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2010}, {"title": "A systematic comparison of various statistical alignement models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In ACL,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Papineni et al.2002] K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improved neural network based language modelling and adaptation", "author": ["Park et al.2010] Junho Park", "Xunying Liu", "Mark J.F. Gales", "Phil C. Woodland"], "venue": "In Interspeech,", "citeRegEx": "Park et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Park et al\\.", "year": 2010}, {"title": "Efficient training strategies for deep neural network language models", "author": ["Fethi Bougares", "Lo\u0131\u0308c Barrault"], "venue": "In NIPS workshop on Deep Learning and Representation Learning", "citeRegEx": "Schwenk et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2014}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["Holger Schwenk"], "venue": "In Coling,", "citeRegEx": "Schwenk.,? \\Q2012\\E", "shortCiteRegEx": "Schwenk.", "year": 2012}, {"title": "CSLM a modular open-source continuous space language modeling toolkit", "author": ["Holger Schwenk"], "venue": "In Interspeech,", "citeRegEx": "Schwenk.,? \\Q2013\\E", "shortCiteRegEx": "Schwenk.", "year": 2013}, {"title": "Recurrent neural network language model adaptation with curriculum learning", "author": ["Shia et al.2014] Yangyang Shia", "Martha Larsona", "Catholijn M. Jonkera"], "venue": "Computer Speech & Language,", "citeRegEx": "Shia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shia et al\\.", "year": 2014}, {"title": "LSTM neural networks for language modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Interspeech", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Adaptation of a feedforward articifical neural network using a linear transform", "author": ["Trmal et al.2010] Jan Trmal", "Jan Zelinka", "Ludek M\u00fcller"], "venue": "In Text, Speech and Dialogue,", "citeRegEx": "Trmal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Trmal et al\\.", "year": 2010}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["Yao et al.2012] Kaisheng Yao", "Dong Yu", "Frank Seide", "Hang Su", "Li Deng", "Yifan Gong"], "venue": "In Spoken Language Technology Workshop (SLT), 2012 IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2012}, {"title": "How transferable are features in deep neural networks", "author": ["Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In NIPS,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "The projection and probability estimation can be jointly learned by a neural network (Bengio et al., 2003).", "startOffset": 85, "endOffset": 106}, {"referenceID": 16, "context": "the use of recurrent architectures (Mikolov et al., 2010) or LSTM (Sundermeyer et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 27, "context": ", 2010) or LSTM (Sundermeyer et al., 2012).", "startOffset": 16, "endOffset": 42}, {"referenceID": 15, "context": "tion model in an SMT system (Le et al., 2012; Schwenk, 2012; Cho et al., 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 24, "context": "tion model in an SMT system (Le et al., 2012; Schwenk, 2012; Cho et al., 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 6, "context": "tion model in an SMT system (Le et al., 2012; Schwenk, 2012; Cho et al., 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al.", "startOffset": 28, "endOffset": 78}, {"referenceID": 28, "context": ", 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 88, "endOffset": 135}, {"referenceID": 2, "context": ", 2014), and first translations systems entirely based on neural networks were proposed (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 88, "endOffset": 135}, {"referenceID": 1, "context": "(Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 4, "context": "(Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 5, "context": "(Bach et al., 2009; Bertoldi et al., 2012; Cettolo et al., 2014).", "startOffset": 0, "endOffset": 64}, {"referenceID": 31, "context": "One could for instance mention a recent work which investigated how to transfer features in convolutional networks (Yosinski et al., 2014), or research to perform speaker adaptation of a phoneme classifier based on TRAPS (Trmal et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 29, "context": ", 2014), or research to perform speaker adaptation of a phoneme classifier based on TRAPS (Trmal et al., 2010).", "startOffset": 90, "endOffset": 110}, {"referenceID": 21, "context": "(Park et al., 2010).", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "Earlier this idea was explored in (Yao et al., 2012) for speech recognition through an affine transform of the output layer.", "startOffset": 34, "endOffset": 52}, {"referenceID": 9, "context": "Adaptation through data selection was studied in (Jalalvand, 2013)", "startOffset": 49, "endOffset": 66}, {"referenceID": 7, "context": "(selection of sentences in out-of-domain corpora based on similarity between sentences) and (Duh et al., 2013) (training of three models: n-gram, RNN and interpolated LM on two SMT systems: in-domain data only and all-domain).", "startOffset": 92, "endOffset": 110}, {"referenceID": 26, "context": "to adapt a recurrent LM to a sub-domain, again in the area of speech recognition (Shia et al., 2014).", "startOffset": 81, "endOffset": 100}, {"referenceID": 13, "context": "Finally, one of the early applications of RNN was in (Kombrink et al., 2011): it was used", "startOffset": 53, "endOffset": 76}, {"referenceID": 19, "context": "monly used to consider more models (Och, 2003), instead of just a translation and language model:", "startOffset": 35, "endOffset": 46}, {"referenceID": 20, "context": "In this study we use the BLEU score which measures the n-gram precision between the translation and a human reference translation (Papineni et al., 2002).", "startOffset": 130, "endOffset": 153}, {"referenceID": 22, "context": "Usually one tanh hidden and a softmax output layer are used, but recent studies have shown that deeper architecture perform better (Schwenk et al., 2014).", "startOffset": 131, "endOffset": 153}, {"referenceID": 23, "context": "(Schwenk, 2007).", "startOffset": 0, "endOffset": 15}, {"referenceID": 25, "context": "All our experiments were performed with the open-source CSLM toolkit4 (Schwenk, 2013), which was extended for our purposes.", "startOffset": 70, "endOffset": 85}, {"referenceID": 23, "context": "Various solutions have been proposed: short-lists (Schwenk, 2007), a class decomposition (Mikolov et al.", "startOffset": 50, "endOffset": 65}, {"referenceID": 14, "context": ", 2011) or an hierarchical decomposition (Le et al., 2011).", "startOffset": 41, "endOffset": 58}, {"referenceID": 21, "context": "This idea was previously proposed in framework of a speech recognition system (Park et al., 2010).", "startOffset": 78, "endOffset": 97}, {"referenceID": 5, "context": "ing back-off language models, were developed in the framework of the European project MateCat (Cettolo et al., 2014).", "startOffset": 94, "endOffset": 116}, {"referenceID": 10, "context": "Each SMT system is based on the Moses toolkit (Koehn et al., 2007) and built according to the fol-", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "It is performed for the language and translation model using the methods described in (Moore and Lewis, 2010) and (Axelrod et al., 2011) respectively.", "startOffset": 114, "endOffset": 136}, {"referenceID": 5, "context": "This is achieved by adapting the translation and back-off LM (details of the algorithms can be found in (Cettolo et al., 2014)).", "startOffset": 104, "endOffset": 126}], "year": 2015, "abstractText": "It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without over-fitting the small adaptation data.", "creator": "LaTeX with hyperref package"}}}