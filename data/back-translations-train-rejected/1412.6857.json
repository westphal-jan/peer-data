{"id": "1412.6857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Contour Detection Using Cost-Sensitive Convolutional Neural Networks", "abstract": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNN), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500.", "histories": [["v1", "Mon, 22 Dec 2014 01:16:50 GMT  (1169kb,D)", "https://arxiv.org/abs/1412.6857v1", "9 pages, 3 figures"], ["v2", "Wed, 24 Dec 2014 14:37:27 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v2", "9 pages, 3 figures"], ["v3", "Thu, 15 Jan 2015 15:01:16 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v3", "9 pages, 3 figures"], ["v4", "Sat, 28 Feb 2015 07:37:54 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v4", "9 pages, 3 figures"], ["v5", "Tue, 12 May 2015 08:42:42 GMT  (1086kb,D)", "http://arxiv.org/abs/1412.6857v5", "9 pages, 3 figures"]], "COMMENTS": "9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jyh-jing hwang", "tyng-luh liu"], "accepted": false, "id": "1412.6857"}, "pdf": {"name": "1412.6857.pdf", "metadata": {"source": "CRF", "title": "CONTOUR DETECTION USING COST-SENSITIVE CON-", "authors": [], "emails": ["jyhjinghwang@iis.sinica.edu.tw", "liutyng@iis.sinica.edu.tw"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent years have shown that the number of people who are able to move, to move and to move, to move, to move and to move, to move and to move, to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move."}, {"heading": "2 RELATED WORK", "text": "As mentioned above, we focus on using a deep convolutionary neural network to achieve trait learning to enhance contour recognition, so the review of relevant work is presented to provide a revealing picture of recent advances in each of the two areas of focus."}, {"heading": "2.1 CONTOUR DETECTION", "text": "Early techniques for detecting contours (Fram & Deutsch, 1975; Canny, 1986; Perona & Malik, 1990) mainly concerned local gradients such as intensity and color gradients, among which the Canny detector (Canny, 1986) stands out for its simplicity and accuracy due to its investigation of the maximum gradient magnitude orthogonal to the contour direction. Detailed discussions of these approaches can be found in, e.g., Bowyer et al. (1999) Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Ganbelaez et al., 2011) also identify that textures are useful local cues for increasing detection accuracy. Part of the recognition of local cues, learning-based techniques form a remarkable group in accomplishing this complicated task (Dolla \u2012 r and al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiofeno & Bo."}, {"heading": "2.2 CONVOLUTIONAL NEURAL NETWORKS", "text": "Remarkably, CNNs are popularized by LeCun and colleagues who first apply CNNs to digit recognition (LeCun et al., 1989), OCR (LeCun et al., 1998), and generic object recognition (Jarrett et al., 2009). In contrast to using handmade features, CNNs learn differentiating features and display hierarchical semantic information along their deep architecture.The AlexNet by Krizhevsky et al. (2012) is perhaps the most popular implementation of CNNs for generic object classification. The model has been shown to outperform competing approaches based on traditional features in solving a number of mainstream computer vision problems. In Turaga et al. (2010) and Briggman et al. (2009), CNNs are used for image segmentation. To expand CNNs for object recognition (Farabet et al. (2013) we use segmentation for CNN."}, {"heading": "3 PER-PIXEL CNN FEATURES", "text": "It has been shown that most existing techniques focus on providing a feature vector for an input image (or an image spot), and such a design may not be suitable for vision applications that require the examination of image characteristics at the pixel level. If contour detection is a problem, the central task is to decide whether or not an underlying pixel is an edge point. Therefore, it would be handy for the deep network to produce one feature per pixel. We suggest constructing a multi-scale CNN model for contour detection. To this end, we extract Perpixel CNN features in AlexNet (Krizhevsky et al., 2012) with DenseNet (Iandola et al., 2014) and concatenate them pixel by pixel to feed them into a support vector machine (SVM) classifier."}, {"heading": "3.1 DENSENET FEATURE PYRAMIDS", "text": "We use DenseNet for CNN function extraction due to its efficiency, flexibility and availability. DenseNet is an open source system that calculates compact and multi-scale features from the coil layers of a Caffe CNN object classifier. DenseNet's feature extraction process proceeds as follows: When an input image is taken, DenseNet calculates its multi-scale versions and pricks them onto a large plane. After processing the entire layer by CNNs, DenseNet destifles the descriptor planes and then receives multi-resolution CNN descriptions. The dimensions of the coil features are ratios of the image size, e.g. a quarter for Conv1 and an eighth for Conv2. We scale the maps of all coil layers to the image size. That is, there is a feature vector in each pixel. As shown in Figure 1, the dimension of the resulting feature vector is 1376 x 1, the vector is concatenated by a conveector (96) x (1), Conx4 (1) and Conx1 (38x)."}, {"heading": "3.2 PER-PIXEL FINE-TUNING", "text": "To fine-tune the contour detection parameters per pixel, we exclude the two fully connected layers of the ImageNet model by limiting the input image size and thus the overall architecture. We only retain the five Convolutionary layers, and via Conv5 we add a new 2-way Softmax layer for edge classification. Specifically, the input image size of the ImageNet preschooled CNN model is 227 x 227, which is not suitable for our pro-pixel design, as each card in the Conv5 layer is still 13 x 13. In addition, we must remove the padding in CNN to take into account the fact that DenseNet does not use padding (except the input level). To fine-tune per pixel, we first create a series of edge and non-edge fields. The image (patch) is set to 163 x 163 and would decrease to 1 x 1 in the center of the pixel, with a loss of 16x truth."}, {"heading": "3.3 COST-SENSITIVE FINE-TUNING", "text": "Compared to the number of parameters in DenseNet, the size of the training set consisting of sensitive edges and insensitive edges is relatively small. To further learn these subtle properties from a small database, we adopt the concept of cost-sensitive learning. The original 2-way Softmax training costs are the negative log probability costs: \u2212 \u2211 i logP (yi | xi, \u03b8) = \u2212 \u2211 i yi logP (yi = 1 | xi, \u03b8) \u2212 \u2211 i (1 \u2212 yi) logP (yi = 0 | xi, \u03b8) (1), where xi is the input image field and vice versa the negative log parameters of CNN, and yi the binary (0 or 1) edge prediction. These costs are calculated using the 2-way log edges, where xi is the input image field."}, {"heading": "3.4 FINAL FUSION MODEL", "text": "The overall framework is an ensemble model. We combine an ImageNet model, a Perpixel finely tuned model, a positively cost-sensitive finely tuned model, and a negatively cost-sensitive finely tuned model. We use a heuristic branch-and-bound scheme to determine the fusion coefficients. The idea of merging different training models is to capture different aspects of characteristics. It is worth noting that the improvements resulting from model fusion indicate that the various fine tunings have their own merits in feature learning and are all useful in this respect."}, {"heading": "4 EXPERIMENT RESULTS", "text": "We test our method using the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011). To better assess the impact of the various fine-tuning techniques, we report on their respective contour detection performance. Comparisons with other competitive methods are also included to demonstrate the effectiveness of the proposed model. The BSDS500 dataset is the current de facto standard contour detection measurement. The dataset includes 200 training sessions, 100 validations and 200 test images. Limits in each image are labeled and averaged by multiple workers to form the basic truth.The accuracy of contour detection is evaluated using three metrics: the best F measure on the fixed threshold (ODS) dataset, the aggregate F measure on the dataset for the best threshold in each image (OIS) and the maximum precision (AP) on the whole (Recall) in 1986."}, {"heading": "4.1 ON FINE-TUNING", "text": "The Fine Tuning parameter is performed on a server with a GeForce GTX Titan Black GPU card. We set the general learning rate as one-tenth of the original ImageNet pre-trained learning rate, and the softmax learning rate as ten times higher than the general learning rate. Modifying the proposed fine tuning per pixel speeds up the fine tuning parameter. It takes 3 days to complete 100,000 iterations per pixel fine tuning, while we need more than 10 days for traditional fine tuning. For both traditional fine tuning and per pixel fine tuning, we try 500 border (edge) and 500 non-border (non-edge) patches per training image. For positive cost matching, we try 1000 boundary deviations and 500 non-boundary patches per training session."}, {"heading": "4.2 ON FEATURES IN DIFFERENT LAYERS", "text": "Next, we conduct experiments to show how characteristics from different wavelengths contribute to performance. In Table 1 (b), we see that characteristics in the second wave layer contribute the most, and then in the third and fourth layers. These indicate that characteristics at the low to medium level are most useful for contour detection, while characteristics at the lower and higher levels provide additional thrust. Although characteristics in the first and fifth wave layer alone are less effective, we achieve the best results by combining all five currents. It shows that local edge information in low characteristics and object contour information in higher characteristics are both necessary to achieve high performance in contour detection tasks."}, {"heading": "4.3 CONTOUR DETECTION RESULTS AND COMPARISONS", "text": "Finally, we present the experimental results of our pre-trained model and final fusion model. In Table 2, we report on contour recognition performance on BSDS500 using our methods and seven competitive techniques, including gPb (Arbelaez et al., 2011), Sketch Tokens (Lim et al., 2013), Sparse Code Gradients (Xiaofeng & Bo, 2012), DeepNet (Kivinen et al., 2014), Pointwise Mutual Information (Isola et al., 2014), N4-fields (Ganin & Lempitsky, 2014) and Structured Edges (Dolla \ufffd r & Zitnick, 2014). While our pre-trained 5-stream ImageNet model (using features from all five evolutionary layers) already achieves impressive results for contour ODS and OIS measurements, the proposed fine-tuning techniques can further improve performance."}, {"heading": "5 DISCUSSION", "text": "In this paper, we describe how to use the DenseNet architecture to adapt to pro-pixel image processing problems such as contour detection. We suggest fine-tuning techniques to more effectively perform parameter learning with a pro-pixel-based cost function and overcome the limitations of using a small training set. The resulting cost-sensitive model seems promising for generating useful pro-pixel feature vectors and should be useful for computer imaging applications that require the analysis of local image properties. An interesting future direction of research is to establish a suitable framework for reducing dimensionality for the resulting high-dimensional pro-pixel feature vectors and to investigate their impact on the performance of contour detection."}], "references": [{"title": "Contour detection and hierarchical image segmentation", "author": ["Arbelaez", "Pablo", "Maire", "Michael", "Fowlkes", "Charless", "Malik", "Jitendra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Arbelaez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arbelaez et al\\.", "year": 2011}, {"title": "Edge detector evaluation using empirical roc curves", "author": ["Bowyer", "Kevin", "Kranenburg", "Christine", "Dougherty", "Sean"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Bowyer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bowyer et al\\.", "year": 1999}, {"title": "Maximin affinity learning of image segmentation", "author": ["Briggman", "Kevin", "Denk", "Winfried", "Seung", "Sebastian", "Helmstaedter", "Moritz N", "Turaga", "Srinivas C"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Briggman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Briggman et al\\.", "year": 2009}, {"title": "A computational approach to edge detection", "author": ["Canny", "John"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Canny and John.,? \\Q1986\\E", "shortCiteRegEx": "Canny and John.", "year": 1986}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Fast edge detection using structured forests", "author": ["Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "arXiv preprint arXiv:1406.5549,", "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doll\u00e1r et al\\.", "year": 2014}, {"title": "Supervised learning of edges and object boundaries", "author": ["Doll\u00e1r", "Piotr", "Tu", "Zhuowen", "Belongie", "Serge"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Doll\u00e1r et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Doll\u00e1r et al\\.", "year": 2006}, {"title": "Learning hierarchical features for scene labeling", "author": ["Farabet", "Clement", "Couprie", "Camille", "Najman", "Laurent", "LeCun", "Yann"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Farabet et al\\.,? \\Q1915\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 1915}, {"title": "On the quantitative evaluation of edge detection schemes and their comparison with human performance", "author": ["Fram", "Jerry R", "Deutsch", "Edward S"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Fram et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Fram et al\\.", "year": 1975}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1311.2524,", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["Hariharan", "Bharath", "Arbel\u00e1ez", "Pablo", "Girshick", "Ross", "Malik", "Jitendra"], "venue": "arXiv preprint arXiv:1411.5752,", "citeRegEx": "Hariharan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2014}, {"title": "Densenet: Implementing efficient convnet descriptor pyramids", "author": ["Iandola", "Forrest", "Moskewicz", "Matt", "Karayev", "Sergey", "Girshick", "Ross", "Darrell", "Trevor", "Keutzer", "Kurt"], "venue": "arXiv preprint arXiv:1404.1869,", "citeRegEx": "Iandola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2014}, {"title": "Crisp boundary detection using pointwise mutual information", "author": ["Isola", "Phillip", "Zoran", "Daniel", "Krishnan", "Dilip", "Adelson", "Edward H"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Isola et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2014}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "M Ranzato", "LeCun", "Yann"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Visual boundary prediction: A deep neural prediction network and quality dissection", "author": ["Kivinen", "Jyri J", "Williams", "Christopher KI", "Heess", "Nicolas", "Technologies", "DeepMind"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kivinen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sketch tokens: A learned mid-level representation for contour and object detection", "author": ["Lim", "Joseph J", "Zitnick", "C Lawrence", "Doll\u00e1r", "Piotr"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1411.4038,", "citeRegEx": "Long et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "Discriminative sparse image models for class-specific edge detection and image interpretation", "author": ["Mairal", "Julien", "Leordeanu", "Marius", "Bach", "Francis", "Hebert", "Martial", "Ponce", "Jean"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Mairal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2008}, {"title": "Contour and texture analysis for image segmentation", "author": ["Malik", "Jitendra", "Belongie", "Serge", "Leung", "Thomas", "Shi", "Jianbo"], "venue": "International journal of computer vision,", "citeRegEx": "Malik et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Malik et al\\.", "year": 2001}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["Martin", "David", "Fowlkes", "Charless", "Tal", "Doron", "Malik", "Jitendra"], "venue": "In Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["Martin", "David R", "Fowlkes", "Charless C", "Malik", "Jitendra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Martin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2004}, {"title": "Scale-space and edge detection using anisotropic diffusion", "author": ["Perona", "Pietro", "Malik", "Jitendra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Perona et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Perona et al\\.", "year": 1990}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["Sermanet", "Pierre", "Kavukcuoglu", "Koray", "Chintala", "Soumith", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Multiscale categorical object recognition using contour fragments", "author": ["Shotton", "Jamie", "Blake", "Andrew", "Cipolla", "Roberto"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Shotton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shotton et al\\.", "year": 2008}, {"title": "Convolutional networks can learn to generate affinity graphs for image segmentation", "author": ["Turaga", "Srinivas C", "Murray", "Joseph F", "Jain", "Viren", "Roth", "Fabian", "Helmstaedter", "Moritz", "Briggman", "Kevin", "Denk", "Winfried", "Seung", "H Sebastian"], "venue": "Neural Computation,", "citeRegEx": "Turaga et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turaga et al\\.", "year": 2010}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["Xiaofeng", "Ren", "Bo", "Liefeng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Xiaofeng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiaofeng et al\\.", "year": 2012}, {"title": "Detecting object boundaries using low-, mid, and high-level information", "author": ["Zheng", "Songfeng", "Yuille", "Alan", "Tu", "Zhuowen"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Zheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2010}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["Zitnick", "C Lawrence", "Doll\u00e1r", "Piotr"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Zitnick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Contour detection is fundamental to a wide range of computer vision applications, including image segmentation (Malik et al., 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 0, "context": "Contour detection is fundamental to a wide range of computer vision applications, including image segmentation (Malik et al., 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 28, "context": ", 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008).", "startOffset": 67, "endOffset": 89}, {"referenceID": 24, "context": "The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014).", "startOffset": 125, "endOffset": 227}, {"referenceID": 21, "context": "The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014).", "startOffset": 125, "endOffset": 227}, {"referenceID": 0, "context": "The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014).", "startOffset": 125, "endOffset": 227}, {"referenceID": 16, "context": ", AlexNet (Krizhevsky et al., 2012), to generate features for each image pixel, not just a single feature vector for the whole input image.", "startOffset": 10, "endOffset": 35}, {"referenceID": 4, "context": "Such a distinction would call for a different perspective of parameter fine-tuning so that a pre-trained per-image CNN on ImageNet (Deng et al., 2009) can be adapted into a new model for per-pixel edge classifications.", "startOffset": 131, "endOffset": 150}, {"referenceID": 23, "context": "To further investigate the property of the features from different convolutional layers and from various ensembles, we carry out a number of experiments to evaluate their effectiveness in performing contour detection on the benchmark BSDS Segmentation dataset (Martin et al., 2001).", "startOffset": 260, "endOffset": 281}, {"referenceID": 0, "context": ", 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008). The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014). Take, for example, that Doll\u00e1r & Zitnick (2014) use structured random forests to learn local edge patterns, and report current state-of-the-art results with impressive computation efficiency.", "startOffset": 8, "endOffset": 391}, {"referenceID": 0, "context": ", 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008). The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014). Take, for example, that Doll\u00e1r & Zitnick (2014) use structured random forests to learn local edge patterns, and report current state-of-the-art results with impressive computation efficiency. More recently, object cues are also considered in Kivinen et al. (2014) and Ganin & Lempitsky (2014) to further boost the performance.", "startOffset": 8, "endOffset": 607}, {"referenceID": 0, "context": ", 2001; Arbelaez et al., 2011), object detection (Zitnick & Doll\u00e1r, 2014) and recognition (Shotton et al., 2008). The task is often carried out by exploring local image cues, such as intensity, color gradients, texture or local structures (Canny, 1986; Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011; Doll\u00e1r & Zitnick, 2014). Take, for example, that Doll\u00e1r & Zitnick (2014) use structured random forests to learn local edge patterns, and report current state-of-the-art results with impressive computation efficiency. More recently, object cues are also considered in Kivinen et al. (2014) and Ganin & Lempitsky (2014) to further boost the performance.", "startOffset": 8, "endOffset": 636}, {"referenceID": 24, "context": "Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy.", "startOffset": 32, "endOffset": 97}, {"referenceID": 21, "context": "Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy.", "startOffset": 32, "endOffset": 97}, {"referenceID": 0, "context": "Subsequent work along this line (Martin et al., 2004; Mairal et al., 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy.", "startOffset": 32, "endOffset": 97}, {"referenceID": 6, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 21, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 31, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 19, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 15, "context": "Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014).", "startOffset": 115, "endOffset": 287}, {"referenceID": 0, "context": ", Bowyer et al. (1999). Subsequent work along this line (Martin et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch.", "startOffset": 8, "endOffset": 432}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours.", "startOffset": 8, "endOffset": 554}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al.", "startOffset": 8, "endOffset": 690}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al.", "startOffset": 8, "endOffset": 779}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures.", "startOffset": 8, "endOffset": 798}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours.", "startOffset": 8, "endOffset": 933}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency.", "startOffset": 8, "endOffset": 1080}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency.", "startOffset": 8, "endOffset": 1105}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures.", "startOffset": 8, "endOffset": 1289}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures.", "startOffset": 8, "endOffset": 1318}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures. Kivinen et al. (2014) encode and decode contours using multilayer mean-and-covariance restricted Boltzmann machines.", "startOffset": 8, "endOffset": 1391}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures. Kivinen et al. (2014) encode and decode contours using multilayer mean-and-covariance restricted Boltzmann machines. Ganin & Lempitsky (2014) establish a deep architecture, which composes of convolutional neural networks and nearest neighbor search, and obtain convincing results.", "startOffset": 8, "endOffset": 1511}, {"referenceID": 0, "context": ", 2008; Arbelaez et al., 2011) also identifies that textures are useful local cues for increasing the detection accuracy. Apart from detecting local cues, learning-based techniques form a notable group in addressing this intriguing task (Doll\u00e1r et al., 2006; Mairal et al., 2008; Zheng et al., 2010; Xiaofeng & Bo, 2012; Lim et al., 2013; Doll\u00e1r & Zitnick, 2014; Kivinen et al., 2014; Ganin & Lempitsky, 2014). Doll\u00e1r et al. (2006) adopt a boosted classifier to independently label each pixel by learning its surrounding image patch. Zheng et al. (2010) analyze the combination of low-, mid-, and high-level information to detect object-specific contours. In addition, Xiaofeng & Bo (2012) propose to compute sparse code gradients and successfully improve Arbelaez et al. (2011). Lim et al. (2013) classify edge patches into sketch tokens using random forest classifiers, which can capture local edge structures. Isola et al. (2014) consider pointwise mutual information to extract global object contours. Their results display crisp and clean contours. Like in Lim et al. (2013), Doll\u00e1r & Zitnick (2014) use structured random forests to learn edge patches, and achieve current state-of-the-art results in both accuracy and efficiency. More relevant to our approach, Kivinen et al. (2014) and Ganin & Lempitsky (2014) learn contour information with deep architectures. Kivinen et al. (2014) encode and decode contours using multilayer mean-and-covariance restricted Boltzmann machines. Ganin & Lempitsky (2014) establish a deep architecture, which composes of convolutional neural networks and nearest neighbor search, and obtain convincing results. Different from Ganin & Lempitsky (2014), we strive for designing fine-tuning mechanisms with a small dataset for adapting an ImageNet pre-trained convolutional neural network for producing per-pixel image features.", "startOffset": 8, "endOffset": 1690}, {"referenceID": 17, "context": "Noticeably, CNNs are popularized by LeCun and colleagues who first apply CNNs to digit recognition (LeCun et al., 1989), OCR (LeCun et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 18, "context": ", 1989), OCR (LeCun et al., 1998) and generic object recognition (Jarrett et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 13, "context": ", 1998) and generic object recognition (Jarrett et al., 2009).", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": ", 1998) and generic object recognition (Jarrett et al., 2009). In contrast to using hand-crafted features, CNNs learn discriminative features and exhibit hierarchical semantic information along their deep architecture. The AlexNet by Krizhevsky et al. (2012) is perhaps the most popular implementation of CNNs for generic object classification.", "startOffset": 40, "endOffset": 259}, {"referenceID": 9, "context": ", 1998) and generic object recognition (Jarrett et al., 2009). In contrast to using hand-crafted features, CNNs learn discriminative features and exhibit hierarchical semantic information along their deep architecture. The AlexNet by Krizhevsky et al. (2012) is perhaps the most popular implementation of CNNs for generic object classification. The model has been shown to outperform competing approaches based on traditional features in solving a number of mainstream computer vision problems. In Turaga et al. (2010) and Briggman et al.", "startOffset": 40, "endOffset": 519}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation.", "startOffset": 11, "endOffset": 34}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation.", "startOffset": 11, "endOffset": 132}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al.", "startOffset": 11, "endOffset": 196}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al.", "startOffset": 11, "endOffset": 361}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al. (2013) also consider features from a deep CNN in a region proposal framework to achieve state-of-the-art object detection results on the PASCAL VOC dataset.", "startOffset": 11, "endOffset": 385}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al. (2013) also consider features from a deep CNN in a region proposal framework to achieve state-of-the-art object detection results on the PASCAL VOC dataset. While CNNs thrives in generic object recognition and detection, less attention is paid to applications demanding per-pixel processing, such as contour detection and segmentation. Our method exploits the AlexNet model for contour detection and explores its per-pixel fine-tuning with a small dataset. Recently and independently from our work, generating per-pixel features based on CNNs can also be found in Hariharan et al. (2014) and Long et al.", "startOffset": 11, "endOffset": 966}, {"referenceID": 2, "context": "(2010) and Briggman et al. (2009), CNNs are used for image segmentation. To extend CNNs for object detection, Farabet et al. (2013) utilize CNNs for semantic segmentation. Sermanet et al. (2013a) use CNNs to predict object locations via sliding window, while learning multi-stage features of CNNs for pedestrian detection is proposed in Sermanet et al. (2013b). Girshick et al. (2013) also consider features from a deep CNN in a region proposal framework to achieve state-of-the-art object detection results on the PASCAL VOC dataset. While CNNs thrives in generic object recognition and detection, less attention is paid to applications demanding per-pixel processing, such as contour detection and segmentation. Our method exploits the AlexNet model for contour detection and explores its per-pixel fine-tuning with a small dataset. Recently and independently from our work, generating per-pixel features based on CNNs can also be found in Hariharan et al. (2014) and Long et al. (2014).", "startOffset": 11, "endOffset": 989}, {"referenceID": 16, "context": "To this end, we extract perpixel CNN features in AlexNet (Krizhevsky et al., 2012) using DenseNet (Iandola et al.", "startOffset": 57, "endOffset": 82}, {"referenceID": 11, "context": ", 2012) using DenseNet (Iandola et al., 2014), and pixelwise concatenate them to feed into a support vector machine (SVM) classifier.", "startOffset": 23, "endOffset": 45}, {"referenceID": 14, "context": "In particular, DenseNet provides fast multiscale feature pyramid extraction of any Caffe convolutional neural networks (Jia et al., 2014) and the convenience of working with images of arbitrary size.", "startOffset": 119, "endOffset": 137}, {"referenceID": 23, "context": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 0, "context": "We test our method on the Berkeley Segmentation Dataset and Benchmark (BSDS500) (Martin et al., 2001; Arbelaez et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 0, "context": "The accuracy of contour detection is evaluated by three measures: the best F-measure on the dataset for a fixed threshold (ODS), the aggregate F-measure on the dataset for the best threshold in each image (OIS), and the average precision (AP) on the full recall range (Arbelaez et al., 2011).", "startOffset": 268, "endOffset": 291}, {"referenceID": 0, "context": "Method ODS OIS AP gPb-owt-ucm (Arbelaez et al., 2011) .", "startOffset": 30, "endOffset": 53}, {"referenceID": 19, "context": "73 Sketch tokens (Lim et al., 2013) .", "startOffset": 17, "endOffset": 35}, {"referenceID": 15, "context": "77 DeepNet (Kivinen et al., 2014) .", "startOffset": 11, "endOffset": 33}, {"referenceID": 12, "context": "76 PMI+sPb, MS (Isola et al., 2014) .", "startOffset": 15, "endOffset": 35}, {"referenceID": 0, "context": "In Table 2, we report the contour detection performances on BSDS500 by our methods and seven competitive techniques, including gPb (Arbelaez et al., 2011), Sketch Tokens (Lim et al.", "startOffset": 131, "endOffset": 154}, {"referenceID": 19, "context": ", 2011), Sketch Tokens (Lim et al., 2013), Sparse Code Gradients (Xiaofeng & Bo, 2012), DeepNet (Kivinen et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 15, "context": ", 2013), Sparse Code Gradients (Xiaofeng & Bo, 2012), DeepNet (Kivinen et al., 2014), Pointwise Mutual Information (Isola et al.", "startOffset": 62, "endOffset": 84}, {"referenceID": 12, "context": ", 2014), Pointwise Mutual Information (Isola et al., 2014), N-fields (Ganin & Lempitsky, 2014) and Structured Edges (Doll\u00e1r & Zitnick, 2014).", "startOffset": 38, "endOffset": 58}], "year": 2015, "abstractText": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500.", "creator": "LaTeX with hyperref package"}}}