{"id": "1412.7006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks", "abstract": "The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle's physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDAR-video inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.", "histories": [["v1", "Mon, 22 Dec 2014 14:54:53 GMT  (3190kb,D)", "https://arxiv.org/abs/1412.7006v1", "submitted, 9 pages"], ["v2", "Wed, 8 Jul 2015 01:14:14 GMT  (3557kb,D)", "http://arxiv.org/abs/1412.7006v2", "7 pages, double column, IEEE format, accepted at IEEE HPEC 2015"]], "COMMENTS": "submitted, 9 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["michael giering", "vivek venugopalan", "kishore reddy"], "accepted": false, "id": "1412.7006"}, "pdf": {"name": "1412.7006.pdf", "metadata": {"source": "CRF", "title": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks", "authors": ["Michael Giering", "Vivek Venugopalan"], "emails": ["reddykk}@utrc.utc.com"], "sections": [{"heading": "I. MOTIVATION", "text": "In fact, most people who are able to determine for themselves what they want and what they want have to take things into their own hands. It's not that they can take what they want into their own hands, but it's that they have to do it. It's not that they do it as if they do it. It's that they do it as if they do it. It's that they do it as if they do it, but it's that they do it as if they do it. It's not that they do it as if they do it, as if they do it, as if they do it as if they do it."}, {"heading": "II. PREVIOUS WORK", "text": "Most of the approaches we have taken generate an interest in each type of modality and create a decision-making mechanism that overlaps the characteristics of modalities, but these filtering methods are necessary to ensure proper intermodal registration. These filtering methods are often geometrically located in nature and allow the use of projections between the different data areas."}, {"heading": "III. PROBLEM STATEMENT", "text": "The ability to detect and correct misalignment (registration, calibration) between sensors of the same or different types is critical for decision support systems operating on their molten information streams. DCNNs have been implemented for our work to detect small spatial misalignments in LiDAR and video images, and the methodology is also directly applicable to time registration. LiDAR video data collected from a driverless car has been selected for multimodal fusion testing. LiDAR video is a common combination to provide perception capabilities to many types of ground and flight platforms, including driverless cars [8]."}, {"heading": "A. Ford LiDAR-video Dataset and Experimental Setup", "text": "The FORD LiDAR video dataset [3] is collected by an autonomous Ford F-250 vehicle equipped with the following sensors for perception and navigation: \u2022 Velodyne HDL-64E LiDAR with two laser blocks rotating at 10 Hz and a maximum reach of 120 m. \u2022 Point Grey Ladybug3 omnidirectional camera system with six 2-megapixel cameras that capture video data at 8 frames per second and a resolution of 1600 x 1600. \u2022 Two Riegl LMS-Q120 LIDAR sensors installed at the front of the vehicle generate reach and intensity data when the laser passes its 80 \u00b0 field of vision (FOV). \u2022 Applanix POS-LV420 INS with Trimble GPS system that provides the 6 degrees of freedom (DOF) estimate at 100 Hz. \u2022 Xsen's MTi-G sensor, consisting of accelerometer, gyroscope, GPS coordinator, and integrated pressure sensor in the vehicle, measures the GPS in the city center. \u2022 The FORD LiDAR video dataset [3] is collected by an autonomous Ford F-250 vehicle that is equipped with the following sensors for perception and navigation: \u2022 Velodyne HDL-64E LiDAR with two laser blocks rotating at 10 Hz and a maximum reach of 120 m. \u2022 Point Grey Ladybug3 omnidirectional camera system with six 2 mega-pixel cameras that capture video data at 8 frames per second and a resolution of 1600 x 1600 x 1600 x 1600 x 1600 x 1600 x 1600 x 1600 x. \u2022 Two Riegl LMS-Q120 LIDAR sensors installed at the front of the vehicle generates reach and intensity data when the laser crosses its 80 \u00b0 field of vision (FOV)."}, {"heading": "B. Optical Flow", "text": "In the field of mobile robot navigation, optical flow is often used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25]. The optical flow provides information about scene dynamics and is expressed as an estimate of the velocity in each pixel from two consecutive images, referred to as ~ u and ~ v. The motion field from these two images is measured by the motion of the pixel brightness pattern, with changes in image brightness attributable to camera or object motion. [26] describes an algorithm for calculating the optical flow from images used during the preprocessing step. Figure 2 shows an example of the optical flow calculated from two consecutive images from the Ford LiDAR video dataset. By incorporating the optical flow as input channels, we give CNN information about the dynamics observed over time steps."}, {"heading": "C. Preprocessing", "text": "The channels consist of grayscale size or (R, G, B) information from the video, horizontal and vertical components of the optical flow (U, V) and depth information L from LiDAR. The data from each modality is converted to a fixed size of 800 x 256 values, divided into p \u00b7 p fields in a prescribed step. Each patch p \u00b7 p is stacked over C channels and effectively generates a vector of the C dimensions. The different preprocessing parameters are indicated by the patch size p, stride s and the number of input channels. Pre-processing is repeated N times, with N being the number of offset classes. For each offset class, the video (R, G, B) and the optical flow (U, V) channels are kept static."}, {"heading": "IV. MODEL DESCRIPTION", "text": "Our automatic registration models are DCNNs designed to classify the current misalignment of LiDAR video streams 32 x 32 pixels into a predefined set of offsets. DCNNs are probably the most successful deep learning model to date. The fact that the algorithm splits weights in the training phase results in fewer model parameters and more efficient training. DCNNs are particularly useful for problems where the local structure is important, such as object recognition in images and time information for speech recognition. The changing steps of conversion and pooling generate properties at multiple scales, which in turn implement DCNNs with scale invariant properties. The model shown in Figure 4 consists of 3 pairs of convolution pooling layers that estimate the offset between the LiDAR video inputs at each time step. For each patch within a time period, there are variants with video inputs."}, {"heading": "V. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset using elliptically distributed offsets", "text": "In our experiments, elliptically distributed N = 9 offsets of the LiDAR video data were taken into account. LiDAR data is shifted clockwise along an ellipse with a main axis of 32 pixels and a secondary axis of 16 pixels rotated from the x-axis by 45 \u00b0, as shown in Figure 3a. Separate training and test sets were created from two different orbits, as shown in Figure 1 for all N = 9 offsets of LiDAR data. Training and test tracks have never seen regions and also have different lighting conditions. Our pre-processing step described in Section III-C leads to 223, 371 and 126, 513 fields for testing and training, which are extracted from 469 and 224 images respectively. In the test phase, a simple matching scheme is used for each image to aggregate the offset predictions of the patch plane in order to obtain a prediction of the patch plane. An example histogram is shown in the prediction of the 5."}, {"heading": "B. Experimental results", "text": "Table I lists the inputs and the CNN parameters studied ranked in order of increasing accuracy. We calculated the values over the diagonal of the confusion matrix to determine the image level and accuracy of the image level. Accuracy of the image level is the individual performance of all 32 \u00d7 32 image fields from the test images. Classification of the image fields belonging to a single time step is chosen to predict the shift in the accuracy of the image level. In Table I, the first 3 columns show the results for different number of filter combinations in the fixed number of filters and input channels RGBLUV folding layers. We observed that the accuracy of the image and image level decreased with the increase in the number of filters. In experiments shown in columns 4 and 5, the filter size was increased, with the number of filters at (32, 32, 64) being constant, we observed that the image size of the R6 and the R9 were the best."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we proposed a method for the registration of LiDAR video. We demonstrated the effect of filter size, number of filters and different channels. We also demonstrated the advantage of using temporal information, optical flow and grayscale. The next step in this work is to complete our development of a deep method for the automatic registration of ground and aviation platforms, which does not require a priori calibration of the ground. In particular, our aerospace applications present more noisy data with an increased number of degrees of freedom. Extending these methods to the simultaneous registration of information on multiple platforms and a greater number of modalities will present interesting challenges that we look forward to working on."}], "references": [{"title": "Real-time 2D Video 3D LiDAR Registration", "author": ["C. Bodensteiner", "M. Arens"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 2206\u20132209.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal Deep Learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Ford Campus Vision And Lidar Data Set", "author": ["G. Pandey", "J.R. McBride", "R.M. Eustice"], "venue": "The International Journal of Robotics Research, vol. 30, no. 13, pp. 1543\u20131552, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Information Fusion In Biometrics", "author": ["A. Ross", "A. Jain"], "venue": "Pattern recognition letters, vol. 24, no. 13, pp. 2115\u20132125, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning Representations By Maximizing Compression", "author": ["K. Gregor", "Y. LeCun"], "venue": "arXiv preprint arXiv:1108.1169, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal Multimodal Fusion For Multimedia Data Analysis", "author": ["Y. Wu", "E.Y. Chang", "K.C.-C. Chang", "J.R. Smith"], "venue": "Proceedings of the 12th annual ACM international conference on Multimedia. ACM, 2004, pp. 572\u2013579.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "The Challenge Problem For Automated Detection Of 101 Semantic Concepts In Multimedia", "author": ["C.G. Snoek", "M. Worring", "J.C. Van Gemert", "J.-M. Geusebroek", "A.W. Smeulders"], "venue": "Proceedings of the 14th annual ACM international conference on Multimedia. ACM, 2006, pp. 421\u2013430.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Google\u2019s driverless car", "author": ["S. Thrun"], "venue": "Ted Talk, Ed, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A Robust Approach For Automatic Registration Of Aerial Images With Untextured Aerial LIDAR Data", "author": ["L. Wang", "U. Neumann"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 2623\u20132630.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic registration of lidar and optical imagery using depth map stereo", "author": ["H. Kim", "C.D. Correa", "N. Max"], "venue": "Computational Photography (ICCP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1\u20138.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic Registration of LIDAR and Optical Images of Urban Scenes", "author": ["A. Mastin", "J. Kepner", "J. Fisher"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 2639\u20132646.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A systematic approach for 2d-image to 3d-range registration in urban environments", "author": ["L. Liu", "I. Stamos"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, Oct 2007, pp. 1\u20138.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic registration of aerial imagery with untextured 3d lidar models", "author": ["M. Ding", "K. Lyngbaek", "A. Zakhor"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, June 2008, pp. 1\u20138.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Automated texture mapping of 3d city models with oblique aerial imagery", "author": ["C. Frueh", "R. Sammon", "A. Zakhor"], "venue": "3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004. Proceedings. 2nd International Symposium on, Sept 2004, pp. 396\u2013403.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Integrating automated range registration with multiview geometry for the photorealistic modeling of large-scale scenes", "author": ["I. Stamos", "L. Liu", "C. Chen", "G. Wolberg", "G. Yu", "S. Zokai"], "venue": "International Journal of Computer Vision, vol. 78, no. 2-3, pp. 237\u2013260, 2008. [Online]. Available: http://dx.doi.org/10.1007/s11263-007-0089-1", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "A shadow based method for image to model registration", "author": ["A.J. Troccoli", "P.K. Allen"], "venue": "In IEEE Workshop on Image and Video Registration, Conf. on Comp. Vision and, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Alignment of continuous video onto 3d point clouds", "author": ["W. Zhao", "D. Nister", "S. Hsu"], "venue": "Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, vol. 2, June 2004, pp. II\u2013II.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiview geometry for texture mapping 2d images onto 3d range data", "author": ["L. Liu", "I. Stamos", "G. Yu", "G. Wolberg", "S. Zokai"], "venue": "Computer Vision  (a) Patch level confusion matrix (41.05% accuracy) (b) Image level confusion matrix (76.69% accuracy) Fig. 6: Confusion Matrix for elliptically distributed N = 9 classes using Greyscale, Optical Flow and LiDAR channels with a filter size of 9 and Pattern Recognition, 2006 IEEE Computer Society Conference on, vol. 2, June 2006, pp. 2293\u20132300.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep Learning for Robust Feature Generation in Audiovisual Emotion Recognition", "author": ["Y. Kim", "H. Lee", "E.M. Provost"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3687\u20133691.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal Learning With Deep Boltzmann Machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012, pp. 2222\u20132230.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep Learning for Detecting Robotic Grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "arXiv preprint arXiv:1301.3592, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Egomotion and relative depth map from optical flow", "author": ["K. Prazdny"], "venue": "Biological Cybernetics, vol. 36, no. 2, pp. 87\u2013102, 1980. [Online]. Available: http://dx.doi.org/10.1007/BF00361077", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1980}, {"title": "Robust depth estimation from optical flow", "author": ["B. Shahraray", "M. Brown"], "venue": "Computer Vision., Second International Conference on, Dec 1988, pp. 641\u2013650.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "Dynamic 3D Scene Depth Reconstruction via Optical Flow Field Rectification", "author": ["Y. Yang", "Q. Liu", "R. Ji", "Y. Gao"], "venue": "PLoS ONE, vol. 7, p. 47041, Nov. 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient moving object segmentation algorithm using background registration technique", "author": ["S.-Y. Chien", "S.-Y. Ma", "L.-G. Chen"], "venue": "Circuits  and Systems for Video Technology, IEEE Transactions on, vol. 12, no. 7, pp. 577\u2013586, Jul 2002.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "Beyond Pixels: Exploring New Representations and Applications for Motion Analysis", "author": ["C. Liu"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, USA, 2009, aAI0822221.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "NVIDIA\u2019s Next Generation CUDA Compute Architecture: Kepler TM GK110", "author": ["NVIDIA Inc."], "venue": "Whitepaper, May 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Accelerating real-time lidar data processing using gpus", "author": ["V. Venugopal", "S. Kannan"], "venue": "Circuits and Systems (MWSCAS), 2013 IEEE 56th International Midwest Symposium on, August 2013, pp. 1168\u20131171.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet Classification With Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Most approaches [1] in LiDAR-video for instance, build separate vision and LiDAR", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "For some challenges in which the modalities share significant mutual information, the features generated on the fused information can provide insight that neither input alone can [2].", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "The experiments were conducted using a publicly available dataset from FORD and the University of Michigan [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "[8] are required to ensure proper inter-modal registration.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 11, "context": "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Another approach used for video and LiDAR auto-registration is to reconstruct 3D point cloud from video sequences using structure from motion (SFM) and performing 3D-3D registration [17], [18].", "startOffset": 182, "endOffset": 186}, {"referenceID": 17, "context": "Another approach used for video and LiDAR auto-registration is to reconstruct 3D point cloud from video sequences using structure from motion (SFM) and performing 3D-3D registration [17], [18].", "startOffset": 188, "endOffset": 192}, {"referenceID": 1, "context": "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.", "startOffset": 141, "endOffset": 144}, {"referenceID": 18, "context": "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.", "startOffset": 181, "endOffset": 185}, {"referenceID": 7, "context": "LiDAR-video is a common combination for providing perception capabilities to many types of ground and airborne platforms including driverless cars [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "The FORD LiDAR-video dataset [3] is collected by an autonomous Ford F-250 vehicle integrated with the following perception and navigation sensors as follows:", "startOffset": 29, "endOffset": 32}, {"referenceID": 21, "context": "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 25, "context": "[26] describes an algorithm for computing optical flow from images, which is used during the preprocessing step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "The DCNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [27] The NVIDIA Kepler series K40 GPUs [28] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [29].", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "The DCNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [27] The NVIDIA Kepler series K40 GPUs [28] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 28, "context": "The DCNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [27] The NVIDIA Kepler series K40 GPUs [28] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [29].", "startOffset": 282, "endOffset": 286}, {"referenceID": 29, "context": "Deep Learning applications have been targeted on GPUs previously in [30] and these implementations are both compute and memory bound.", "startOffset": 68, "endOffset": 72}], "year": 2015, "abstractText": "The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle\u2019s physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDARvideo inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.", "creator": "LaTeX with hyperref package"}}}