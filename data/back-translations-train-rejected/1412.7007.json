{"id": "1412.7007", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Occlusion Edge Detection in RGB-D Frames using Deep Convolutional Networks", "abstract": "Occlusion edges in images which correspond to range discontinuity in the scene from the point of view of the observer are an important prerequisite for many vision and mobile robot tasks. Although they can be extracted from range data however extracting them from images and videos would be extremely beneficial. We trained a deep convolutional neural network (CNN) to identify occlusion edges in images and videos with both RGB-D and RGB inputs. The use of CNN avoids hand-crafting of features for automatically isolating occlusion edges and distinguishing them from appearance edges. Other than quantitative occlusion edge detection results, qualitative results are provided to demonstrate the trade-off between high resolution analysis and frame-level computation time which is critical for real-time robotics applications.", "histories": [["v1", "Mon, 22 Dec 2014 14:55:17 GMT  (3012kb,D)", "https://arxiv.org/abs/1412.7007v1", "9 pages, v1"], ["v2", "Wed, 24 Dec 2014 12:50:48 GMT  (3012kb,D)", "http://arxiv.org/abs/1412.7007v2", "9 pages, v2, spellcheck"], ["v3", "Wed, 8 Jul 2015 01:07:23 GMT  (3011kb,D)", "http://arxiv.org/abs/1412.7007v3", "7 pages, double column, IEEE HPEC 2015 Conference"]], "COMMENTS": "9 pages, v1", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["soumik sarkar", "vivek venugopalan", "kishore reddy", "michael giering", "julian ryde", "navdeep jaitly"], "accepted": false, "id": "1412.7007"}, "pdf": {"name": "1412.7007.pdf", "metadata": {"source": "CRF", "title": "Using Deep Convolutional Networks for Occlusion Edge Detection in RGB-D Frames", "authors": ["Soumik Sarkar", "Vivek Venugopalan", "Kishore Reddy", "Michael Giering", "Julian Ryde", "Navdeep Jaitly"], "emails": ["{soumiks@iastate.edu}", "rydejc}@utrc.utc.com", "{ndjaitly@google.com}"], "sections": [{"heading": null, "text": "In fact, most people are able to decide for themselves what they want and what they don't want."}, {"heading": "II. PROBLEM FORMULATION AND GENERATING THE TRAINING DATA", "text": "In general, it is difficult to define edge pixels rigorously. In an image, edges are manifested along high-contrast paths for four main reasons: (i) texture change, i.e., abrupt change in surface color, (ii) lighting change, i.e., sharp shadows, (iii) area discontinuity, i.e., abrupt change in distance from the observer, and (iv) normal surface change, e.g. intersection of two planes. It is important to estimate the distinction in the causes of the image edge. Texture change and illumination edges are not observed by 3D sensors. Therefore, the remaining geometric edge types are discontinuities and abrupt normal surface changes. Surface standard changes are invariant, but edges are due to distance differences between discontinuities that may vary with the position of the observer. These surface characteristics and discontinuities are illustrated in the last image."}, {"heading": "A. Training data", "text": "The information about the occlusion edge is mostly located in the depth (D) channel of an RGB-D frame. Therefore, the occlusion edge label is for one pixel, i.e. the truth of the ground can be determined automatically using the depth channel data. The process of label generation is shown in Fig. 2. From left to right, the three plates in the figure show an exemplary RGB frame, the corresponding D channel data and the classification frame, which are generated using a simple threshold that is permitted only on the depth data. Unlike gray (non-edge marking) and white (occlusion marking) colors, the black color can be seen in the classification frame. This means poor depth measurements due to the presence of an absorbing surface or greater than the maximum distance between the sensor and the surface. The RGB-D dataset was tested using a camera movement along a specific trajectory in a particular environment in a fog so that the trajectory can be divided into a trajectory that is more trained beforehand."}, {"heading": "III. CNN ARCHITECTURE AND MODEL LEARNING", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "IV. TESTING AND POST-PROCESSING", "text": "For quantitative results, classification errors are calculated based on the model's ability to predict the labeling of the center pixels of a test field obtained from a frame captured during the test section of camera motion. Qualitative observations and visualizations are made using a post-processing scheme as shown in Fig. 5. In this scheme, the classification security for pixels of a patch center is collected from the Softmax posterior distribution and extrapolated across the patch using a full-width Gaussian distribution at half-maximum (FWHM). Such Gaussian cores from overlapping patches are merged into a blending model to create smooth occlusion edges in the test field."}, {"heading": "V. EXPERIMENTS AND QUANTITATIVE RESULTS", "text": "Different experiments are performed with different sets of input data for a comparative evaluation. They are described below, along with corresponding quantitative performance of the CNN model: RGB-D frame The first set of experiments uses single temporal frames of RGB-D data (i.e., 4 channels). This task may seem quite simple, since the depth information is directly available as one of the channels in the input data. However, most of the edges in the current frames are appearance edges and RGB channels clearly provide this information. Therefore, the task for the CNN model is to detect edges by automatic feature extraction and to distinguish the edges of events from appearance edges. RGB frames The second set of experiments uses single time frames of RGB data (i.e., 3 channels).The goal here was to investigate whether discriminatory features exist and can be extracted by CNN from only RGB channels."}, {"heading": "VI. QUALITATIVE OBSERVATIONS", "text": "In this section, qualitative results are presented to understand the effectiveness of the deep learning tools for detecting occlusion edges and for robotic applications as a whole. Figures 7 and 8 show performance with RGB-D input with steps 4 and 8 (see section IV for details on steps) on a test frame. As expected, the generation of occlusion edges with a lower step value is better, as in this case more information per pixel is available. In the marked regions (bordered in red), the images show that the incorrect detection of occlusion edges with a lower step value decreases. Compensation is in computing speed. At a lower step value, the image processing time increases linearly with increasing number of test fields. Therefore, this target conflict must be chosen correctly for real-time robotic applications. Figures 9 and 10 show performance with RGB input with step 4 and 8 on the same test frame and very similar observations can be made in this case."}, {"heading": "VII. CONCLUSIONS AND FUTURE WORKS", "text": "In this study, we trained deep convolutionary neural networks in a supervised way to detect occlusion edges in RGB-D frames. It is formulated as a mid-pixel classification problem for an image patch extracted from a larger frame. Aside from RGB-D inputs, experiments have been conducted to investigate the performance deterioration associated with dropping the Depth (D) channel. It is noted that although the missed detection rate increases slightly without depth data, false alarm performance does not significantly deteriorate. To visualize test performance, a test and post-processing scheme is being developed. The goal conflict between high-resolution patch analysis and frame-level computation time is discussed, which is critical for real-time robotics applications. RGB-D and RGB frames are located at both ends of the spectrum of the input-data content spectrum, and therefore are currently being tracked by multiple investigations from RGB structure to input-input."}], "references": [{"title": "An online learning approach to occlusion boundary detection", "author": ["N. Jacobson", "Y. Freund", "T.Q. Nguyen"], "venue": "Image Processing, IEEE Transactions on, vol. 21, no. 1, pp. 252\u2013261, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Detachable object detection with efficient model selection", "author": ["A. Ayvaci", "S. Soatto"], "venue": "Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer, 2011, pp. 191\u2013204.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic occlusion boundary detection on spatio-temporal lattices", "author": ["M.E. Sargin", "L. Bertelli", "B.S. Manjunath", "K. Rose"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 560\u2013567.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Occlusion edge blur: a cue to relative visual depth", "author": ["J.A. Marshall", "C.A. Burbeck", "D. Ariely", "J.P. Rolland", "K.E. Martin"], "venue": "JOSA A, vol. 13, no. 4, pp. 681\u2013688, 1996.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Occlusion boundaries from motion: Low-level detection and mid-level reasoning", "author": ["A.N. Stein", "M. Hebert"], "venue": "International journal of computer vision, vol. 82, no. 3, pp. 325\u2013357, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A century of gestalt psychology in visual perception: I. perceptual grouping and figure\u2013ground organization.", "author": ["J. Wagemans", "J.H. Elder", "M. Kubovy", "S.E. Palmer", "M.A. Peterson", "M. Singh", "R. von der Heydt"], "venue": "Psychological bulletin,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Occlusion boundary detection and figure/ground assignment from optical flow", "author": ["P. Sundberg", "T. Brox", "M. Maire", "P. Arbel\u00e1ez", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011, pp. 2233\u20132240.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Layered motion segmentation and depth ordering by tracking edges", "author": ["P. Smith", "T. Drummond", "R. Cipolla"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 26, no. 4, pp. 479\u2013494, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "A comparative Fig. 10: Occlusion detection performance on a test frame with RGB input and stride 8; heat map description same as in Fig. 7; red circled region shows inferior performance compared to that of stride 4 evaluation of interest point detectors and local descriptors for visual slam", "author": ["A. Gil", "O.M. Mozos", "M. Ballesta", "O. Reinoso"], "venue": "Machine Vision and Applications, vol. 21, no. 6, pp. 905\u2013920, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "A bayesian treatment of the stereo correspondence problem using half-occluded regions", "author": ["P.N. Belhumeur", "D. Mumford"], "venue": "Computer Vision and Pattern Recognition, 1992. Proceedings CVPR\u201992., 1992 IEEE Computer Society Conference on. IEEE, 1992, pp. 506\u2013512.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "D. Olivier"], "venue": "Algorithmic Learning Theory. Springer Berlin/Heidelberg, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G.E. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "ICML, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Journal of Approximate Reasoning, vol. 50, pp. 969\u2013978, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 2949\u20132980, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Representational power of restricted boltzmann machines and deep belief networks", "author": ["N.L. Roux", "Y. Bengio"], "venue": "Neural Computation, vol. 20.6, pp. 1631\u20131649, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313.5786, pp. 504\u2013507, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning convolutional feature hierachies for visual recognition", "author": ["K. Kavukcuoglu", "Y.L. Sermanet", "P. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "NIPS, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A benchmark for the evaluation of rgb-d slam systems", "author": ["J. Sturm", "N. Engelhard", "F. Endres", "W. Burgard", "D. Cremers"], "venue": "Proc. of the International Conference on Intelligent Robot Systems (IROS), Oct. 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "3D is here: Point cloud library (pcl)", "author": ["R.B. Rusu", "S. Cousins"], "venue": "Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Shanghai, China, May 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Harris 3D: a robust extension of the harris operator for interest point detection on 3D meshes", "author": ["I. Sipiran", "B. Bustos"], "venue": "The Visual Computer, vol. 27, no. 11, pp. 963\u2013976, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Indoor semantic segmentation using depth information", "author": ["C. Couprie", "C. Farabet", "L. Najman", "Y. LeCun"], "venue": "ICLR, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Concurrent object recognition and segmentation by graph partitioning", "author": ["S.X. Yu", "R. Gross", "J. Shi"], "venue": "NIPS, 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Context-sensitive decision forests for object detection", "author": ["P. Kontschieder", "S.R. Bulo", "A. Criminisi", "P. Kohli", "M. Pelillo", "H. Bischof"], "venue": "NIPS, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Occlusion edge detection is a fundamental capability of computer vision systems as is evident from the number of applications and significant attention it has received [1], [2], [3], [4], [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "Occlusion edge detection is a fundamental capability of computer vision systems as is evident from the number of applications and significant attention it has received [1], [2], [3], [4], [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "Occlusion edge detection is a fundamental capability of computer vision systems as is evident from the number of applications and significant attention it has received [1], [2], [3], [4], [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "Occlusion edge detection is a fundamental capability of computer vision systems as is evident from the number of applications and significant attention it has received [1], [2], [3], [4], [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "Occlusion edge detection is a fundamental capability of computer vision systems as is evident from the number of applications and significant attention it has received [1], [2], [3], [4], [5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 5, "context": "In addition to numerous applications, the concept of occlusions edges is supported by the human visual perception research [6] where it is referred to as figure/ground determination.", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "Once occlusion boundaries have been established, depth order of regions become possible [7], [8] which aids navigation, simultaneous localization and mapping (SLAM) and path planning.", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "Once occlusion boundaries have been established, depth order of regions become possible [7], [8] which aids navigation, simultaneous localization and mapping (SLAM) and path planning.", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "on viewpoint position, removing these variant feature saves on further processing and increases recognition accuracy [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "Furthermore, knowledge of occlusion edges helps with stereo vision [10] and optic flow algorithms [7].", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "Furthermore, knowledge of occlusion edges helps with stereo vision [10] and optic flow algorithms [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "In this context, this paper evaluates the efficacy of Deep Learning tools [11] for the task of occlusion edge detection.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Recently, this class of techniques have emerged as the top performing machine learning tool for various tasks such as object recognition [12], speech recognition [13], denoising [14], hashing [15] and data fusion [16].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Recently, this class of techniques have emerged as the top performing machine learning tool for various tasks such as object recognition [12], speech recognition [13], denoising [14], hashing [15] and data fusion [16].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "Recently, this class of techniques have emerged as the top performing machine learning tool for various tasks such as object recognition [12], speech recognition [13], denoising [14], hashing [15] and data fusion [16].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "Recently, this class of techniques have emerged as the top performing machine learning tool for various tasks such as object recognition [12], speech recognition [13], denoising [14], hashing [15] and data fusion [16].", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "Recently, this class of techniques have emerged as the top performing machine learning tool for various tasks such as object recognition [12], speech recognition [13], denoising [14], hashing [15] and data fusion [16].", "startOffset": 213, "endOffset": 217}, {"referenceID": 16, "context": "While Deep Neural Networks (DNN) pre-trained using Deep Belief Networks (DBN) [17], [18] perform quite well in most data types, deep Convolutional Neural Networks [19] have been shown to be most suited for images.", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "While Deep Neural Networks (DNN) pre-trained using Deep Belief Networks (DBN) [17], [18] perform quite well in most data types, deep Convolutional Neural Networks [19] have been shown to be most suited for images.", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "While Deep Neural Networks (DNN) pre-trained using Deep Belief Networks (DBN) [17], [18] perform quite well in most data types, deep Convolutional Neural Networks [19] have been shown to be most suited for images.", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "Therefore, deep neural networks are particularly interesting for this problems as they extract hierarchical features (features of features) from data and visualization of intermediate optimized filters [12] show that edge type features are very common.", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "Due to availability of GPUs and recent advancements in the algorithmic/implementation side, large CNNs can be learnt without significant overfitting from high volume of data for complex problems [12].", "startOffset": 195, "endOffset": 199}, {"referenceID": 19, "context": "The study uses a publicly available benchmark RGB-D data set captured with moving camera in an indoor environment by the Computer Vision group at Technische Universitt Mnchen (TUM) [20].", "startOffset": 181, "endOffset": 185}, {"referenceID": 20, "context": "Traditional approaches for detect geometric edges in 3D data include a keypoint detector based on a 3D extension of the Harris corner operator in the Point Cloud Library [21].", "startOffset": 170, "endOffset": 174}, {"referenceID": 21, "context": "A related approach for selecting interest points on 3D meshes was introduced in [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "In principle, this study is similar to a recent work on indoor scene segmentation [23].", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "The training label for each patch is determined by the pixels located at the center [24], [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "The training label for each patch is determined by the pixels located at the center [24], [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "The CNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [[26]].", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "Deep Learning applications have been targeted on GPUs previously in [12] and these implementations are both compute and memory bound.", "startOffset": 68, "endOffset": 72}], "year": 2015, "abstractText": "Occlusion edges in images which correspond to range discontinuity in the scene from the point of view of the observer are an important prerequisite for many vision and mobile robot tasks. Although occlusion edges can be extracted from range data, extracting them from images and videos is challenging and would be extremely beneficial for a variety of robotics based applications. We trained a deep convolutional neural network (CNN) to identify occlusion edges in images and videos with both RGB-D and RGB inputs. The use of CNN avoids hand-crafting of features for automatically isolating occlusion edges and distinguishing them from appearance edges. Other than quantitative occlusion edge detection results, qualitative results are provided to demonstrate the trade-off between high resolution analysis and frame-level computation time which is critical for real-time robotics applications.", "creator": "LaTeX with hyperref package"}}}