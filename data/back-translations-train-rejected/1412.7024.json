{"id": "1412.7024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Training deep neural networks with low precision multiplications", "abstract": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with around 10 bits for computing activations and gradients, and 12 bits for storing updated parameters.", "histories": [["v1", "Mon, 22 Dec 2014 15:22:45 GMT  (183kb,D)", "http://arxiv.org/abs/1412.7024v1", "9 pages, 4 figures, ICLR 2015"], ["v2", "Thu, 25 Dec 2014 18:05:12 GMT  (183kb,D)", "http://arxiv.org/abs/1412.7024v2", "Few more details on our dynamic fixed point implementation compared with the previous version"], ["v3", "Thu, 26 Feb 2015 00:26:12 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7024v3", "9 pages, 4 figures, under review as conference paper at ICRL 2015"], ["v4", "Fri, 3 Apr 2015 22:52:43 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7024v4", "9 pages, 4 figures, Accepted as a workshop contribution at ICLR 2015"], ["v5", "Wed, 23 Sep 2015 01:00:44 GMT  (347kb,D)", "http://arxiv.org/abs/1412.7024v5", "10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015"]], "COMMENTS": "9 pages, 4 figures, ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["matthieu courbariaux", "yoshua bengio", "jean-pierre david"], "accepted": false, "id": "1412.7024"}, "pdf": {"name": "1412.7024.pdf", "metadata": {"source": "CRF", "title": "LOW PRECISION ARITHMETIC FOR DEEP LEARNING", "authors": ["Matthieu Courbariaux", "Yoshua Bengio"], "emails": ["matthieu.courbariaux@polymtl.ca", "jean-pierre.david@polymtl.ca", "yoshua.bengio@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Many previous papers have addressed the best exploitation of universal hardware, typically CPU clusters (Dean et al., 2012) and GPUs (Coates et al., 2009; Krizhevsky et al., 2012a). Faster implementations usually lead to state-of-the-art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).In fact, such approaches always consist in adapting the algorithm to the best state-of-the-art hardware. However, some dedicated hardware for deep learning, which is also based on state-of-the-art technology, also appears. FPGA implementations claim better energy efficiency than universal hardware (Farabet et al., 2011; Kim et al., 2009).The corresponding ASIC implementations are even more efficient (Pham et al., 2012)."}, {"heading": "2 MAXOUT NETWORKS", "text": "A maxout network is a multilayer neural network that uses maxout units in its hidden layers. A maxout unit prints the maximum of a set of k-point products between k-weight vectors and the input vector of the unit (e.g. the output of the previous layer): hli = kmax j = 1 (bli, j + w l i, j \u00b7 hl \u2212 1), where hl is the vector of activation of the layer l and the weight vectors wli, j and the distortions b l i, j the parameters of the j-th filter of the unit i on layer l.A maxout unit can be regarded as a generalization of the rectification units (Jarrett et al., 2009; Nair and Hinton et al., 2011; Krizhevsky et al al al al al al al al al al al al al al al., al., 2012b) hli = max (0, b l i \u00b7 hl \u2212 1) corresponding to a maxout unit when the other layer is connected to the 2b layer."}, {"heading": "3 FLOATING POINT", "text": "Floating-point formats are often used to represent real values. They consist of a character, an exponent and a mantissa. The exponent gives the floating-point formats a wide range, and the mantissa gives them good precision. You can calculate the value of a floating-point number using the following formula: value = (\u2212 1) character \u00d7 (1 + mantissa223) \u00d7 2 (exponent \u2212 127) Table 1 shows the exponent and mantissa widths associated with each floating-point format. In our experiments, we use the floating-point format as a reference format, as it is the most widely used format in the field of deep learning, especially for GPU calculation. We show that the use of the half-precision floating-point format has little to no effect on the formation of neural networks. At the time of writing this article, there is no IEEE standard below the half-precision format."}, {"heading": "4 FIXED POINT", "text": "Fixed point formats consist of a signed praying mantis and a global scaling factor divided between all fixed point variables. However, the scaling factor can be considered as the position of the natal point. Therefore, fixed point arithmetics can also be considered as floating point arithmetics with a common fixed exponent. However, the scaling factor is typically a strength of two for computational efficiency (the scaling multiplications are then replaced by shifts). As a result, fixed point arithmetics can also be considered as floating point arithmetics with a common fixed exponent. Fixed point arithmetics is often found on embedded systems without FPU (Floating Point Unit). It relies on more holistic arithmetic operations. It is more hardware-based as its floating point exponent."}, {"heading": "5 DYNAMIC FIXED POINT", "text": "Dynamic fixed point arithmetic (Williamson, 1991) is a variant of fixed point arithmetic in which there are multiple scaling factors instead of a single global one. As such, it can be used as a compromise between floating point arithmetic - where each scalar variable has its scaling factor that is updated during each operation - and fixed point arithmetic - where there is only one global scaling factor that is never updated. In dynamic fixed point arithmetic, some grouped variables share a scaling factor that is updated from time to time to reflect the statistics of the values in the group.In practice, we choose weights, bias, weighted sum, outputs (postlinearity) and the respective gradients vectors and matrices with a different scaling factor. These scaling factors are initialized with a global value at the beginning of the training so that the scaling factors are updated at the end of each training poche."}, {"heading": "6 TWO DIFFERENT BIT WIDTHS", "text": "We use two different bit widths in our fixed point and dynamic fixed point arithmetic experiments: one for the assignment of parameters and the other for the rest of the calculations. The idea behind this is to accumulate small changes in the parameters (which requires more precision) and thus save a few bits for the calculations that can be performed with less precision due to the implicit averaging that is performed during the training using stochastic gradient descent: \u03b8t + 1 = \u03b8t \u2212 \u2202 Ct (\u03b8t), where Ct (\u03b8t) is the cost to minimize the minibatch visited during iteration t and to use the learning rate. We see that the resulting parameter is the sum T = \u04450 \u2212 T \u2212 1 \u0445t = 1 \u0445 t = 1 \u0445 Ct (\u0445t) \u0445 \u0441\u0442t (\u0442t) \u0441\u0442t (\u0442t) \u0441\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442\u0442t. The terms of this sum are not statistically independent of the contribution, because the random value of the small number depends on the small number of the \u2212 1 is sufficient."}, {"heading": "7 SIMULATION", "text": "Since we do not have access to current low-precision hardware, all the results mentioned in this paper are obtained by simulating low-precision calculations. We use a floating-point format for calculation and storage, as it allows us to use existing GPU libraries, e.g. Theano (Bergstra et al., 2010; Bastien et al., 2012). Whenever an activation, gradient, or parameter is stored, we artificially reduce its precision. The only limitation of our simulation is that it does not take into account the size of the accumulators. We somehow hypothesize that the accumulators are as precise as the floating-point format. However, in practice, accumulators are often more precise than individual floats (Lee et al., 2008)."}, {"heading": "8 BASELINE RESULTS", "text": "We reproduce the results of Goodfellow et al. (2013a) on the MNIST, CIFAR10 and SVHN using floating-point arithmetic. In the next section, LOW PRECISION RESULTS, we evaluate the impact of low-precision arithmetic on the models of this section, BASeline RESULTS."}, {"heading": "8.1 MNIST", "text": "The data set of the MNIST (LeCun et al., 1998) is described in Table 2. We do not use data magnification (e.g. distortion) nor any unattended pre-training. We simply use a stochastic gradient descent according to Minibatch (SGD). We use a linear declining learning rate and a linear saturating momentum. We regulate the model with a dropout and a restriction of the structure of each weight vector, as in (Srebro and Shraibman, 2005).We train two different models on the MNIST. The first is a permutation invariant (PI) model that is unaware of the structure of the data. It consists of two fully connected maxout layers followed by a Softmax layer. The second model consists of three convolutionary maxout hidden layers (with spatial max pooling on top of the maxout layers). It consists of two fully connected maxout followed by a soft max layer."}, {"heading": "8.2 CIFAR10", "text": "The CIFAR10 dataset (Krizhevsky and Hinton, 2009) is described in Table 2. We process the data using global contrast normalization and ZCA whitening. The model consists of three convolutionary maxout layers, a fully bonded maxout layer, and a fully bonded softmax layer. Otherwise, we follow a similar approach to the MNIST dataset. This is the same approach as Goodfellow et al. (2013a), except that we do not base our model on the validation examples."}, {"heading": "8.3 STREET VIEW HOUSE NUMBERS", "text": "The SVHN dataset (Netzer et al., 2011) is described in Table 2. We applied the pre-processing of local contrast normalization in the same way as Zeiler and Fergus (2013).The model consists of three convolutionary maxout layers, a fully bonded maxout layer and a fully bonded softmax layer. Otherwise, we used the same approach as for the MNIST dataset. This is the same approach as for Goodfellow et al. (2013a)."}, {"heading": "9 LOW-PRECISION RESULTS", "text": "We evaluate the impact of low-precision arithmetic on the models of the previous section BASELINE RESULTS. We have provided our code 1."}, {"heading": "9.1 FLOATING POINT", "text": "Using the half-precision floating-point format has little or no impact on the error rate of test sets, as shown in Table 3.1. https: / / github.com / MatthieuCourbariaux / deep-learning-arithmetic"}, {"heading": "9.2 FIXED POINT", "text": "The optimal position of the radix point in the fixed point is according to the fifth most important bits, as shown in Figure 1. The corresponding range is approximately [-32,32]. The corresponding scaling factor depends on the bit width used. The minimum bit width for calculations in the fixed point is 19 (20 with the sign).Below this bit width, the error rate of the test set increases very strongly, as in Figure 2. The minimum bit width for parameter updates in the fixed point is 19 (20 with the sign).Below this bit width, the error rate of the test set increases very strongly, as in Figure 3. Finally, the use of 19 (20 with the sign) bits for both calculations and parameter updates has little effect on the final test error, as shown in Table 3. Doubling the number of hidden units does not allow for any further reduction of the bit widths on the permutation invariant MNIST."}, {"heading": "9.3 DYNAMIC FIXED POINT", "text": "Increasing the maximum overflow rate allows us to reduce the bit width of the calculations, but it also significantly increases the final test error rate, as shown in Figure 4. Consequently, we use a maximum 0% overflow rate for the rest of the experiments. The minimum bit width for calculations in the dynamic fix point is 9 (10 with the sign), and below this bit width the error rate of the test set increases very strongly, as shown in Figure 2. The minimum bit width for parameter updates in the dynamic fix point is 11 (12 with the sign), and below this bit width the error rate of the test set increases very strongly, as shown in Figure 3. Finally, the use of 9 (10 with the sign) bits for the calculations and 11 (12 with the sign) bits for the parameter updates has little effect on the final test error, except for the SVHN dataset, as shown in Table 3. Doubling the hidden units does not leave any further narrowing of the bit width of the IST calculation."}, {"heading": "10 DISCUSSION", "text": "We think that there are two major arithmetic difficulties in forming deep neural networks: 1. Activations, gradients and parameters typically have very different ranges 2. Gradients decrease during training, so their half-floating point format easily overcomes these two difficulties at the expense of its exponent bit width (5 bits).The fixed-point format saves the exponent bit width by splitting the exponent across all fixed-point variables. However, the global exponent cannot be adapted to the activations, gradients and parameters at the same time, and the resulting bit width (20 bits) is wider than half the precision of the sliding component.The dynamic fixed-point format is an interesting compromise between sliding component formats and fixed-point formats. Exponents are divided by groups of variables; the activations, gradients and parameters have different sliding components.The gradient exponents can be updated during training if the precision is reduced to 16 (if the precision is two)."}, {"heading": "11 CONCLUSION", "text": "We simulated the formation of a series of state-of-the-art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, using three different arithmetics: floating point, fixed point and dynamic fixed point. For each of these thosearithmetics, we assessed the impact of the precision of the calculations on the final error of the training. We found that very low precision calculations are sufficient not only for operating trained networks, but also for training them. This opens the door to new memory optimizations of deep learning algorithms on universal hardware, and, above all, this opens the door to very energy-efficient training of neural networks on dedicated hardware."}, {"heading": "12 ACKNOWLEDGEMENT", "text": "We thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library that enabled us to easily develop fast and optimized code for the GPU. We also thank the developers of Pylearn2 (Goodfellow et al., 2013b), a Python library built on top of Theano that enabled us to easily connect the data sets to our Theano code."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation.", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Scalable learning for object detection with gpu hardware", "author": ["A. Coates", "P. Baumstarck", "Q. Le", "A.Y. Ng"], "venue": "Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International Conference on, pages 4287\u20134293. IEEE.", "citeRegEx": "Coates et al\\.,? 2009", "shortCiteRegEx": "Coates et al\\.", "year": 2009}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "NIPS\u20192012.", "citeRegEx": "Dean et al\\.,? 2012", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "E. Culurciello", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on, pages 109\u2013116. IEEE.", "citeRegEx": "Farabet et al\\.,? 2011", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS\u20192011.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "Technical report, Universit\u00e9 de Montr\u00e9al.", "citeRegEx": "Goodfellow et al\\.,? 2013a", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Pylearn2: a machine learning research library", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "arXiv preprint arXiv:1308.4214.", "citeRegEx": "Goodfellow et al\\.,? 2013b", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Analysis of high-performance floatingpoint arithmetic on fpgas", "author": ["G. Govindu", "L. Zhuo", "S. Choi", "V. Prasanna"], "venue": "Parallel and Distributed Processing Symposium, 2004. Proceedings. 18th International, page 149. IEEE.", "citeRegEx": "Govindu et al\\.,? 2004", "shortCiteRegEx": "Govindu et al\\.", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical report, arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Back propagation simulations using limited precision calculations", "author": ["J.L. Holt", "T.E. Baker"], "venue": "Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, volume 2, pages 121\u2013126. IEEE.", "citeRegEx": "Holt and Baker,? 1991", "shortCiteRegEx": "Holt and Baker", "year": 1991}, {"title": "What is the best multi-stage architecture for object recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "International Conference on Computer Vision (ICCV\u201909), pages 2146\u20132153. IEEE.", "citeRegEx": "Jarrett et al\\.,? 2009", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "A highly scalable restricted boltzmann machine fpga implementation", "author": ["S.K. Kim", "L.C. McAfee", "P.L. McMahon", "K. Olukotun"], "venue": "Field Programmable Logic and Applications, 2009. FPL 2009. International Conference on, pages 367\u2013372. IEEE.", "citeRegEx": "Kim et al\\.,? 2009", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, University of Toronto.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25 (NIPS\u20192012).", "citeRegEx": "Krizhevsky et al\\.,? 2012a", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS\u20192012.", "citeRegEx": "Krizhevsky et al\\.,? 2012b", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11), 2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Design and implementation of 16-bit fixed point digital signal processor", "author": ["D. Lee", "C. Ryu", "J. Park", "K. Kwon", "W. Choi"], "venue": "SoC Design Conference, 2008. ISOCC\u201908. International, volume 2, pages II\u201361. IEEE.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML\u20192010.", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS.", "citeRegEx": "Netzer et al\\.,? 2011", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Neuflow: dataflow vision processing system-on-a-chip", "author": ["Pham", "P.-H.", "D. Jelaca", "C. Farabet", "B. Martini", "Y. LeCun", "E. Culurciello"], "venue": "Circuits and Systems (MWSCAS), 2012 IEEE 55th International Midwest Symposium on, pages 1044\u20131047. IEEE.", "citeRegEx": "Pham et al\\.,? 2012", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "A fixed point implementation of the backpropagation learning algorithm", "author": ["R.K. Presley", "R.L. Haggard"], "venue": "Southeastcon\u201994. Creative Technology Transfer-A Global Affair., Proceedings of the 1994 IEEE, pages 136\u2013138. IEEE.", "citeRegEx": "Presley and Haggard,? 1994", "shortCiteRegEx": "Presley and Haggard", "year": 1994}, {"title": "The impact of arithmetic representation on implementing mlp-bp on fpgas: A study", "author": ["A.W. Savich", "M. Moussa", "S. Areibi"], "venue": "Neural Networks, IEEE Transactions on, 18(1), 240\u2013 252.", "citeRegEx": "Savich et al\\.,? 2007", "shortCiteRegEx": "Savich et al\\.", "year": 2007}, {"title": "Rank, trace-norm and max-norm", "author": ["N. Srebro", "A. Shraibman"], "venue": "Proceedings of the 18th Annual Conference on Learning Theory, pages 545\u2013560. Springer-Verlag.", "citeRegEx": "Srebro and Shraibman,? 2005", "shortCiteRegEx": "Srebro and Shraibman", "year": 2005}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Technical report, arXiv preprint arXiv:1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Fpgas vs", "author": ["K. Underwood"], "venue": "cpus: trends in peak floating-point performance. In Proceedings of the 2004 ACM/SIGDA 12th international symposium on Field programmable gate arrays, pages 171\u2013180. ACM.", "citeRegEx": "Underwood,? 2004", "shortCiteRegEx": "Underwood", "year": 2004}, {"title": "Improving the speed of neural networks on cpus", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop.", "citeRegEx": "Vanhoucke et al\\.,? 2011", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Dynamic fixed-point arithmetic design of embedded svm-based speaker identification system", "author": ["Wang", "J.-F.", "Kuan", "T.-W.", "Wang", "J.-C.", "Sun", "T.-W."], "venue": "volume 6064 LNCS, pages 524 \u2013 531, Shanghai, China. Embedded environment;Fixed point arithmetic;Linear prediction cepstral coefficients;Point design;Sequential minimal optimization;Speaker identification;Speaker identification systems;.", "citeRegEx": "Wang et al\\.,? 2010", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Dynamically scaled fixed point arithmetic. pages 315 \u2013 18, New York, NY, USA. dynamic scaling;iteration stages;digital filters;overflow probability;fixed point arithmetic;fixed-point filter", "author": ["D. Williamson"], "venue": null, "citeRegEx": "Williamson,? \\Q1991\\E", "shortCiteRegEx": "Williamson", "year": 1991}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Zeiler and Fergus,? 2013", "shortCiteRegEx": "Zeiler and Fergus", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point.", "startOffset": 91, "endOffset": 117}, {"referenceID": 3, "context": "Lots of previous works address the best exploitation of general-purpose hardware, typically CPU clusters (Dean et al., 2012) and GPUs (Coates et al.", "startOffset": 105, "endOffset": 124}, {"referenceID": 2, "context": ", 2012) and GPUs (Coates et al., 2009; Krizhevsky et al., 2012a).", "startOffset": 17, "endOffset": 64}, {"referenceID": 14, "context": ", 2012) and GPUs (Coates et al., 2009; Krizhevsky et al., 2012a).", "startOffset": 17, "endOffset": 64}, {"referenceID": 3, "context": "Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).", "startOffset": 64, "endOffset": 133}, {"referenceID": 14, "context": "Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).", "startOffset": 64, "endOffset": 133}, {"referenceID": 24, "context": "Faster implementations usually lead to state of the art results (Dean et al., 2012; Krizhevsky et al., 2012a; Sutskever et al., 2014).", "startOffset": 64, "endOffset": 133}, {"referenceID": 4, "context": "FPGA implementations claim a better power efficiency than general-purpose hardware (Farabet et al., 2011; Kim et al., 2009).", "startOffset": 83, "endOffset": 123}, {"referenceID": 12, "context": "FPGA implementations claim a better power efficiency than general-purpose hardware (Farabet et al., 2011; Kim et al., 2009).", "startOffset": 83, "endOffset": 123}, {"referenceID": 20, "context": "The corresponding ASIC implementations are even more efficient (Pham et al., 2012).", "startOffset": 63, "endOffset": 82}, {"referenceID": 8, "context": "For instance, using single precision (32 bits) instead of double precision (64 bits) for a floating point multiplier reduces its area by four on modern FPGAs (Govindu et al., 2004; Underwood, 2004).", "startOffset": 158, "endOffset": 197}, {"referenceID": 25, "context": "For instance, using single precision (32 bits) instead of double precision (64 bits) for a floating point multiplier reduces its area by four on modern FPGAs (Govindu et al., 2004; Underwood, 2004).", "startOffset": 158, "endOffset": 197}, {"referenceID": 6, "context": "In this paper, we simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point.", "startOffset": 106, "endOffset": 132}, {"referenceID": 11, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 18, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 5, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 15, "context": "A maxout unit can be seen as a generalization of the rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al.", "startOffset": 70, "endOffset": 162}, {"referenceID": 6, "context": ", 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al., 2013a).", "startOffset": 126, "endOffset": 152}, {"referenceID": 9, "context": "Combined with dropout, a very effective regularization method (Hinton et al., 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 6, "context": ", 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al., 2013a), both as part of fully connected feedforward deep nets and as part of deep convolutional nets.", "startOffset": 85, "endOffset": 111}, {"referenceID": 5, "context": ", 2009; Nair and Hinton, 2010; Glorot et al., 2011; Krizhevsky et al., 2012b) hi = max(0, b l i + w l i \u00b7 hl\u22121) which correspond to a maxout unit when k = 2 and one of the filters is forced at 0 (Goodfellow et al., 2013a). Combined with dropout, a very effective regularization method (Hinton et al., 2012), maxout networks achieved state-of-the-art results on a number of benchmarks (Goodfellow et al., 2013a), both as part of fully connected feedforward deep nets and as part of deep convolutional nets. The dropout technique is a good approximation of model averaging with shared parameters across an exponentially large number of networks that are formed by subsets of the units of the original noise-free deep network. In our article, we reproduce the experiments of Goodfellow et al. (2013a) on the MNIST, CIFAR10 and SVHN data sets while changing the arithmetic operations and the numerical precision of the computations.", "startOffset": 31, "endOffset": 798}, {"referenceID": 10, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007).", "startOffset": 90, "endOffset": 160}, {"referenceID": 21, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007).", "startOffset": 90, "endOffset": 160}, {"referenceID": 22, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007).", "startOffset": 90, "endOffset": 160}, {"referenceID": 23, "context": "In Vanhoucke et al. (2011), the authors show that the use of 8 bits fixed point arithmetic speeds up over three times the application of a neural network on CPU, compared to floating point.", "startOffset": 3, "endOffset": 27}, {"referenceID": 10, "context": "Training neural networks with fixed point arithmetic has already been done in older works (Holt and Baker, 1991; Presley and Haggard, 1994; Savich et al., 2007). However, those works use very small models and datasets in today standards. For instance, in Savich et al. (2007), the model is a MultiLayer Perceptron with two hidden units and the task is to learn a XOR gate.", "startOffset": 91, "endOffset": 276}, {"referenceID": 28, "context": "Dynamic fixed point arithmetic (Williamson, 1991) is a variant of fixed point arithmetic in which there are several scaling factors instead of a single global one.", "startOffset": 31, "endOffset": 49}, {"referenceID": 28, "context": "Dynamic fixed point arithmetic was introduced several decades ago (Williamson, 1991) and it was recently used for an SVM implementation (Wang et al.", "startOffset": 66, "endOffset": 84}, {"referenceID": 27, "context": "Dynamic fixed point arithmetic was introduced several decades ago (Williamson, 1991) and it was recently used for an SVM implementation (Wang et al., 2010).", "startOffset": 136, "endOffset": 155}, {"referenceID": 1, "context": "We use single floating point format for computation and storage as it allows us to use existing GPU libraries, for instance Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 131, "endOffset": 176}, {"referenceID": 0, "context": "We use single floating point format for computation and storage as it allows us to use existing GPU libraries, for instance Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 131, "endOffset": 176}, {"referenceID": 17, "context": "That being said, in practice, accumulators are often more precise than single floats (Lee et al., 2008).", "startOffset": 85, "endOffset": 103}, {"referenceID": 6, "context": "We reproduce the results of Goodfellow et al. (2013a) on the MNIST, CIFAR10 and SVHN, using single precision floating point arithmetic.", "startOffset": 28, "endOffset": 54}, {"referenceID": 6, "context": "PI MNIST MNIST CIFAR10 SVHN Goodfellow et al. (2013a) 32 32 0.", "startOffset": 28, "endOffset": 54}, {"referenceID": 6, "context": "The single precision floating point line refers to the results of our experiments attempting to reproduce the results of Goodfellow et al. (2013a) without training on the validation samples.", "startOffset": 121, "endOffset": 147}, {"referenceID": 16, "context": "The MNIST (LeCun et al., 1998) data set is described in Table 2.", "startOffset": 10, "endOffset": 30}, {"referenceID": 23, "context": "We regularize the model with dropout and a constraint on the norm of each weight vector, as in (Srebro and Shraibman, 2005).", "startOffset": 95, "endOffset": 123}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples because it requires to stop the training at an arbitrary epoch, which adds some randomness to the final test error, which blurs the impact of low-precision arithmetic.", "startOffset": 33, "endOffset": 59}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples because it requires to stop the training at an arbitrary epoch, which adds some randomness to the final test error, which blurs the impact of low-precision arithmetic. As a result, our test error is slightly bigger that the one reported in Goodfellow et al. (2013a). The final test error is in Table 3.", "startOffset": 33, "endOffset": 390}, {"referenceID": 13, "context": "The CIFAR10 (Krizhevsky and Hinton, 2009) data set is described in Table 2.", "startOffset": 12, "endOffset": 41}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a) except that we do not train our model on the validation examples.", "startOffset": 33, "endOffset": 59}, {"referenceID": 19, "context": "The SVHN (Netzer et al., 2011) data set is described in Table 2.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "The SVHN (Netzer et al., 2011) data set is described in Table 2. We applied local contrast normalization preprocessing the same way as Zeiler and Fergus (2013). The model consists in three convolutional maxout layers, a fully connected maxout layer, and a fully connected softmax layer.", "startOffset": 10, "endOffset": 160}, {"referenceID": 6, "context": "This is the same procedure as in Goodfellow et al. (2013a). The final test error is in Table 3.", "startOffset": 33, "endOffset": 59}, {"referenceID": 6, "context": "We have simulated the training of a set of state-of-the-art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point.", "startOffset": 97, "endOffset": 123}], "year": 2014, "abstractText": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with around 10 bits for computing activations and gradients, and 12 bits for storing updated parameters.", "creator": "LaTeX with hyperref package"}}}