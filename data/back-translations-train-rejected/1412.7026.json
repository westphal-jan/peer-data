{"id": "1412.7026", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Language Recognition using Random Indexing", "abstract": "Random Indexing is a simple implementation of Random Projections with a wide range of applications. It can solve a variety of problems with good accuracy without introducing much complexity. Here we use it for identifying the language of text samples. We present a novel method of generating language representation vectors using letter blocks. Further, we show that the method is easily implemented and requires little computational power and space. Experiments on a number of model parameters illustrate certain properties about high dimensional sparse vector representations of data. Proof of statistically relevant language vectors are shown through the extremely high success of various language recognition tasks. On a difficult data set of 21,000 short sentences from 21 different languages, our model performs a language recognition task and achieves 97.8% accuracy, comparable to state-of-the-art methods.", "histories": [["v1", "Mon, 22 Dec 2014 15:34:43 GMT  (345kb,D)", "http://arxiv.org/abs/1412.7026v1", "9 pages, 3 figures, 5 tables, ICLR 2015"], ["v2", "Fri, 27 Feb 2015 08:02:49 GMT  (443kb,D)", "http://arxiv.org/abs/1412.7026v2", "7 pages, 1 figures, 2 tables, ICLR 2015"]], "COMMENTS": "9 pages, 3 figures, 5 tables, ICLR 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["aditya joshi", "johan halseth", "pentti kanerva"], "accepted": false, "id": "1412.7026"}, "pdf": {"name": "1412.7026.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RANDOM INDEXING", "Aditya Joshi", "Johan T. Halseth"], "emails": ["adityajoshi@berkeley.edu", "halseth@berkeley.edu", "pkanerva@cberkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "As people who communicate through language, we have the fascinating ability to recognize unknown languages in spoken or written form by using simple keywords to distinguish one language from another. Some unknown languages, of course, might sound very similar, especially if they come from the same language family, but we are often able to identify the language in question with very high accuracy. This is because within each language are embedded certain features that are distinctly different from each other, be it accent, rhythm or pitch patterns. The same can be said for written languages as they all have characteristics that are unique. Recognizing the language of a given text is the first step in all kinds of language processing, such as text analysis, categorization, translation and much more.As popularized by Shannon, most language models use distribution statistics to explain structural similarities in different specified languages."}, {"heading": "2 RANDOM INDEXING", "text": "Indexing stores information by projecting data onto vectors in such a vector; there are a huge number of different, almost orthogonal vectors in such a space (Kanerva, 1988, p. 19), allowing us to combine two such vectors into one new vector, using well-defined vector space operations while retaining the information of the two with a high probability; in our implementation of random indexing, we use a variant of the MAP (Multiplied, Added, Permute) encoding described in Levy & Gayler (2009) to define the hyperdimensional vector space. Vectors are first taken from a D-dimensional space in which the vector elements originate from the set {\u2212 1, 0, 1}; such vectors are used to represent the basic elements of the system, which in our case are letters of the alphabet; the number of \u2212 1 elements and 1 elements are both k, so the total number of the non-zero elements in such a vector is zero."}, {"heading": "2.1 RANDOM INDEXING FOR TEXTS OF DIFFERENT LANGUAGES", "text": "We use the properties of hyperdimensional vectors to extract certain properties of the text. Kanerva (2014) shows how random indexing can be used to efficiently store a word as it appears in a text. We show how to use a similar strategy to recognize the language of a text from the creation of a text vector for the text and compare the similarity of this vector to precomputed language vectors. Simple speech recognition can be done by comparing the letter frequency of a given text with known letter blocks of languages. Given enough text, the letter distribution of a text approaches the distribution of the language in which the text was written. It is called the \"ergodic\" process in Shannon (1948), as derived from similar ideas in physics and thermodynamics. This can be generalized with letter blocks of different sizes. By a block of size n, we mean n consecutive letters in the text, we ignore text so that the length is \u2212 n."}, {"heading": "2.2 COMPLEXITY", "text": "The outlined algorithm for creating text vectors can be implemented efficiently. To create a block vector for a block of size n, n \u2212 1, additions and permutations are performed, which takes time O (n \u00b7 D). To create text from m letters, O (m) block vectors must be created and merged. This clearly implies an implementation of O (n \u00b7 D \u00b7 m). This can be improved to O (D \u00b7 m) by pointing out that most of the information needed to create the block vector for the next block is already contained in the previous block vector and can be retrieved by removing the post from the letter that is now no longer in the block. Say the block vector A = n has (n \u2212 1) V1, the vector (n \u2212 2) V2 with which we do not multiply the bit vector (Vn = 1), the vector is (1), and the vector is safe."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "This year is the highest in the history of the country."}, {"heading": "4 FUTURE WORK", "text": "Inspired by the improved accuracy achieved by retaining spaces in the input text, one could try to keep even more characters in the input text. Another approach could be to encode words or pairs of words to capture characteristics of the text with the same idea as when creating block vectors. In addition, one could encode the sequential data using a combination of MAP operations. Due to the generality of random indexing of texts, one could try to generate text using voice vectors. Also, one does not have to stick to text. Time series data with well-distributed statistics can be well encoded using random indexing using this scheme. In this way, we propose that our method can be used with a well-crafted \"alphabet\" for speech recognition in speech data to solve our original problem."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Bruno Olshausen, Mayur Mudigonda and many others at the Redwood Center for Theoretical Neuroscience for their insightful discussions and feedback."}], "references": [{"title": "Github: Random indexing for languages python implementation, 2014. URL https://github.com/halseth/vs265_project_f14", "author": ["A. Joshi", "J.T. Halseth"], "venue": null, "citeRegEx": "Joshi and Halseth,? \\Q2014\\E", "shortCiteRegEx": "Joshi and Halseth", "year": 2014}, {"title": "Sparse Distributed Memory", "author": ["P. Kanerva"], "venue": null, "citeRegEx": "Kanerva,? \\Q1988\\E", "shortCiteRegEx": "Kanerva", "year": 1988}, {"title": "Computing with 10,000-bit", "author": ["P. Kanerva"], "venue": "words. Proc. 52nd Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "Kanerva,? \\Q2014\\E", "shortCiteRegEx": "Kanerva", "year": 2014}, {"title": "Random indexing of text samples for latent semantic analysis. pp. \u201d1036", "author": ["P. Kanerva", "J. Kristoferson", "A. Holst"], "venue": "Proc. 22nd Annual Conference of the Cognitive Science Society,", "citeRegEx": "Kanerva et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kanerva et al\\.", "year": 2000}, {"title": "Dimensionality reduction by random mapping: Fast similarity computation for clustering", "author": ["S. Kaski"], "venue": "Proc. IJCNN\u201998, International Joint Converence on Neural Networks,", "citeRegEx": "Kaski,? \\Q1998\\E", "shortCiteRegEx": "Kaski", "year": 1998}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge", "author": ["T. Landauer", "S. Dumais"], "venue": "Psychology Review,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "lateral inhibition in a fully distributed connectionist architecture", "author": ["S.D. Levy", "R.W. Gayler"], "venue": "Proceedings of the Ninth International Conference on Cognitive Modeling,", "citeRegEx": "Levy and Gayler,? \\Q2009\\E", "shortCiteRegEx": "Levy and Gayler", "year": 2009}, {"title": "Accuracy and performance of google\u2019s compact language detector", "author": ["M. McCandless"], "venue": null, "citeRegEx": "McCandless,? \\Q2011\\E", "shortCiteRegEx": "McCandless", "year": 2011}, {"title": "langdetect is updated(added profiles of Estonian / Lithuanian / Latvian / Slovene, and so on. http://shuyo.wordpress.com/2011/09/29/langdetect-is-updatedadded-profiles-of-estonianlithuanian-latvian-slovene-and-so-on/. [Online; accessed 16-December-2014", "author": ["S. Nakatani"], "venue": null, "citeRegEx": "Nakatani,? \\Q2014\\E", "shortCiteRegEx": "Nakatani", "year": 2014}, {"title": "Corpus portal for search in monolingual corpora", "author": ["U. Quastoff", "M. Richter", "C. Biemann"], "venue": "Proceedings of the fifth international conference on Language Resources and Evaluation,", "citeRegEx": "Quastoff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Quastoff et al\\.", "year": 2006}, {"title": "An introduction to random indexing", "author": ["M. Sahlgren"], "venue": "Methods and Applications of Semantic Indexing Workshop at the 7th international conference on Terminology and Knowledge Engineering,", "citeRegEx": "Sahlgren,? \\Q2005\\E", "shortCiteRegEx": "Sahlgren", "year": 2005}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal,", "citeRegEx": "Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["L. van der Maaten"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten,? \\Q2008\\E", "shortCiteRegEx": "Maaten", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "As popularized by Shannon (1948), most language models use distributional statistics to explain structural similarities in various specified languages.", "startOffset": 18, "endOffset": 33}, {"referenceID": 7, "context": "As a result, the accuracy of their detection, as seen through large-scale testing and in practice, is near perfect McCandless (2011).", "startOffset": 115, "endOffset": 133}, {"referenceID": 1, "context": "and Kaski (1998). Random Indexing Kanerva et al.", "startOffset": 4, "endOffset": 17}, {"referenceID": 1, "context": "Random Indexing Kanerva et al. (2000); Sahlgren (2005) is a simple and effective implementation of the idea.", "startOffset": 16, "endOffset": 38}, {"referenceID": 1, "context": "Random Indexing Kanerva et al. (2000); Sahlgren (2005) is a simple and effective implementation of the idea.", "startOffset": 16, "endOffset": 55}, {"referenceID": 1, "context": "There exist a huge number of different, nearly orthogonal vectors in such a space (Kanerva, 1988, p. 19). This lets us combine two such vectors into a new vector using well-defined vector space operations, while keeping the information of the two with high probability. In our implementation of Random Indexing, we use a variant of the MAP (Multiply, Add, Permute) coding described in Levy & Gayler (2009) to define the hyperdimensional vector space.", "startOffset": 83, "endOffset": 406}, {"referenceID": 1, "context": "Kanerva (2014) shows how Random Indexing can be used for efficiently storing a word as it appears in a text.", "startOffset": 0, "endOffset": 15}, {"referenceID": 11, "context": "The phenomenon is called an \u201dergodic\u201d process in Shannon (1948), as borrowed from similar ideas in physics and thermodynamics.", "startOffset": 49, "endOffset": 64}, {"referenceID": 9, "context": "Depending on the task, text samples were taken from Project Gutenberg Hart, where texts in a number of languages can be found, or the Wortschatz CorporaQuastoff et al. (2006), where large numbers of sentences in select languages can be easily downloaded.", "startOffset": 152, "endOffset": 175}, {"referenceID": 12, "context": "Hyperdimensional Language Vectors were projected onto a 2 dimensional space using t-sne van der Maaten (2008). Language Vectors were generated with random sentences from the Wortschatz Corpora.", "startOffset": 96, "endOffset": 110}, {"referenceID": 8, "context": "To get a rough estimate of how well the actual detection algorithm worked, we ran tests on the Europarl Parallel Corpus, described in Nakatani. This corpus has 1000 samples of text for each language (21 of them), and each such sample is a sentence. Our Language Vectors were built from sentences in the Wortschatz corpora by the University of Leipzig Quastoff et al. (2006). The results with different parameters are shown in Tables 1-4.", "startOffset": 134, "endOffset": 374}, {"referenceID": 12, "context": "This is better shown in Figures 2, where sparse and dense vectors were used to generate Language Vectors, and t-sne van der Maaten (2008) was used to project the data from the hyperdimensional space to a 2 dimensional image.", "startOffset": 124, "endOffset": 138}], "year": 2017, "abstractText": "Random Indexing is a simple implementation of Random Projections with a wide range of applications. It can solve a variety of problems with good accuracy without introducing much complexity. Here we use it for identifying the language of text samples. We present a novel method of generating language representation vectors using letter blocks. Further, we show that the method is easily implemented and requires little computational power and space. Experiments on a number of model parameters illustrate certain properties about high dimensional sparse vector representations of data. Proof of statistically relevant language vectors are shown through the extremely high success of various language recognition tasks. On a difficult data set of 21,000 short sentences from 21 different languages, our model performs a language recognition task and achieves 97.8% accuracy, comparable to state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}