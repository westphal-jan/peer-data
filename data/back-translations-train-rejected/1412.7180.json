{"id": "1412.7180", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Bayesian Optimisation for Machine Translation", "abstract": "This paper presents novel Bayesian optimisation algorithms for minimum error rate training of statistical machine translation systems. We explore two classes of algorithms for efficiently exploring the translation space, with the first based on N-best lists and the second based on a hypergraph representation that compactly represents an exponential number of translation options. Our algorithms exhibit faster convergence and are capable of obtaining lower error rates than the existing translation model specific approaches, all within a generic Bayesian optimisation framework. Further more, we also introduce a random embedding algorithm to scale our approach to sparse high dimensional feature sets.", "histories": [["v1", "Mon, 22 Dec 2014 21:44:00 GMT  (48kb,D)", "http://arxiv.org/abs/1412.7180v1", "Bayesian optimisation workshop, NIPS 2014"]], "COMMENTS": "Bayesian optimisation workshop, NIPS 2014", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yishu miao", "ziyu wang", "phil blunsom"], "accepted": false, "id": "1412.7180"}, "pdf": {"name": "1412.7180.pdf", "metadata": {"source": "CRF", "title": "Bayesian Optimisation for Machine Translation", "authors": ["Yishu Miao", "Ziyu Wang", "Phil Blunsom"], "emails": ["yishu.miao@cs.ox.ac.uk", "ziyu.wang@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before we get a result like that."}, {"heading": "2 Bayesian Optimisation Tuning Algorithms", "text": "This year, we will be able to put ourselves at the top, \"he said in an interview with the German Press Agency.\" We have to put ourselves at the top of the EU, \"he said.\" We have to put ourselves at the top of the EU, \"he said.\" We have to put ourselves at the top of the EU, \"he said.\" We have to put ourselves at the top of the EU, \"he said.\" We have to put ourselves at the top of the EU. \""}, {"heading": "3 Experiments", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4 Conclusion", "text": "We are introducing novel Bayesian Machine Translation Optimization Algorithms (BO). Our algorithms have faster convergence and achieve higher educational goals and better translation quality than existing translation model-specific approaches. We also demonstrate that by incorporating the random embedding method, it is practicable to use Bayesian Optimization to conduct large sales training with a high number of sparse features."}, {"heading": "5 Acknowledgements", "text": "This work was supported by the Xerox Foundation Award and EPSRC grant number EP / K036580 / 1. 5BO score is the best BLEU score achieved by Gaussian processes on fixed N leaderboards or hypergraphs."}], "references": [{"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pp. 311\u2013318, Association for Computational Linguistics, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pp. 295\u2013302, Association for Computational Linguistics, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pp. 160\u2013167, Association for Computational Linguistics, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 951\u2013991, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Hope and fear for discriminative training of statistical translation models", "author": ["D. Chiang"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 1159\u20131187, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. De Freitas"], "venue": "arXiv preprint arXiv:1012.2599, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems, pp. 2951\u20132959, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["W. Macherey", "F.J. Och", "I. Thayer", "J. Uszkoreit"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 725\u2013734, Association for Computational Linguistics, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices", "author": ["S. Kumar", "W. Macherey", "C. Dyer", "F. Och"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pp. 163\u2013171, Association for Computational Linguistics, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Advanced dynamic programming in semiring and hypergraph frameworks", "author": ["L. Huang"], "venue": "COL- ING, Manchester, UK, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian optimization in high dimensions via random embeddings", "author": ["Z. Wang", "M. Zoghi", "F. Hutter", "D. Matheson", "N. De Freitas"], "venue": "Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pp. 1778\u20131784, AAAI Press, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["C. Dyer", "J. Weese", "H. Setiawan", "A. Lopez", "F. Ture", "V. Eidelman", "J. Ganitkevitch", "P. Blunsom", "P. Resnik"], "venue": "Proceedings of the ACL 2010 System Demonstrations, pp. 7\u201312, Association for Computational Linguistics, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "11,001 new features for statistical machine translation", "author": ["D. Chiang", "K. Knight", "W. Wang"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 218\u2013226, Association for Computational Linguistics, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt", "author": ["P. Simianer", "S. Riezler", "C. Dyer"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pp. 11\u201321, Association for Computational Linguistics, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Contextual gaussian process bandit optimization", "author": ["A. Krause", "C.S. Ong"], "venue": "Advances in Neural Information Processing Systems, pp. 2447\u20132455, 2011. 5", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "BLEU) [1] using Minimum Error Rate Training (MERT) [2, 3] or a variant of the Margin Infused Relaxed Algorithm (MIRA) [4, 5].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "BLEU) [1] using Minimum Error Rate Training (MERT) [2, 3] or a variant of the Margin Infused Relaxed Algorithm (MIRA) [4, 5].", "startOffset": 51, "endOffset": 57}, {"referenceID": 2, "context": "BLEU) [1] using Minimum Error Rate Training (MERT) [2, 3] or a variant of the Margin Infused Relaxed Algorithm (MIRA) [4, 5].", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "BLEU) [1] using Minimum Error Rate Training (MERT) [2, 3] or a variant of the Margin Infused Relaxed Algorithm (MIRA) [4, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 4, "context": "BLEU) [1] using Minimum Error Rate Training (MERT) [2, 3] or a variant of the Margin Infused Relaxed Algorithm (MIRA) [4, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 5, "context": "In this paper we introduce generic, effective, and efficient Bayesian optimisation (BO) algorithms [6, 7] for training the weights of SMT systems for arbitrary metrics that outperform both MERT and MIRA.", "startOffset": 99, "endOffset": 105}, {"referenceID": 6, "context": "In this paper we introduce generic, effective, and efficient Bayesian optimisation (BO) algorithms [6, 7] for training the weights of SMT systems for arbitrary metrics that outperform both MERT and MIRA.", "startOffset": 99, "endOffset": 105}, {"referenceID": 1, "context": "The linear model popular for SMT systems [2] is parametrised in terms of a source sentence f , target translation e, feature weights wk and corresponding feature functions Hk(e,f) (including a language model, conditional translation probabilities, etc.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "The most common approach is an iterative algorithm MERT [3] which employs N-best lists (the best N translations decoded with a weight set from a previous iteration) as candidate translations C.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Hypergraph, or lattice, MERT [8, 9] aims to tackle problems caused by the lack of diversity in N-best lists.", "startOffset": 29, "endOffset": 35}, {"referenceID": 8, "context": "Hypergraph, or lattice, MERT [8, 9] aims to tackle problems caused by the lack of diversity in N-best lists.", "startOffset": 29, "endOffset": 35}, {"referenceID": 9, "context": "A hypergraph [10] efficiently encodes the exponential translation space explored by the beam-search translation decoder.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "Prior work [8, 9] showed that hypergraph MERT is superior to the original N-best algorithm both in speed of convergence and stability.", "startOffset": 11, "endOffset": 17}, {"referenceID": 8, "context": "Prior work [8, 9] showed that hypergraph MERT is superior to the original N-best algorithm both in speed of convergence and stability.", "startOffset": 11, "endOffset": 17}, {"referenceID": 10, "context": "Hence, we introduce a variant of random embedding Bayesian optimisation (REMBO) [11] into our hypergraph algorithm (HG-REMBO) to tackle the large scale training problem.", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "We implemented our models using spearmint [7]1 and the cdec SMT decoder [12]2.", "startOffset": 42, "endOffset": 45}, {"referenceID": 11, "context": "We implemented our models using spearmint [7]1 and the cdec SMT decoder [12]2.", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "The cdec implementations of hypergraph MERT [9] and MIRA[13] are used as benchmarks.", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "The cdec implementations of hypergraph MERT [9] and MIRA[13] are used as benchmarks.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "More details about the rules can be found in [14].", "startOffset": 45, "endOffset": 49}], "year": 2014, "abstractText": "This paper presents novel Bayesian optimisation algorithms for minimum error rate training of statistical machine translation systems. We explore two classes of algorithms for efficiently exploring the translation space, with the first based on N-best lists and the second based on a hypergraph representation that compactly represents an exponential number of translation options. Our algorithms exhibit faster convergence and are capable of obtaining lower error rates than the existing translation model specific approaches, all within a generic Bayesian optimisation framework. Further more, we also introduce a random embedding algorithm to scale our approach to sparse high dimensional feature sets.", "creator": "LaTeX with hyperref package"}}}