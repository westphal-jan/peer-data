{"id": "1412.7193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Audio Source Separation Using a Deep Autoencoder", "abstract": "This paper proposes a novel framework for unsupervised audio source separation using a deep autoencoder. The characteristics of unknown source signals mixed in the mixed input is automatically by properly configured autoencoders implemented by a network with many layers, and separated by clustering the coefficient vectors in the code layer. By investigating the weight vectors to the final target, representation layer, the primitive components of the audio signals in the frequency domain are observed. By clustering the activation coefficients in the code layer, the previously unknown source signals are segregated. The original source sounds are then separated and reconstructed by using code vectors which belong to different clusters. The restored sounds are not perfect but yield promising results for the possibility in the success of many practical applications.", "histories": [["v1", "Mon, 22 Dec 2014 22:38:06 GMT  (425kb)", "http://arxiv.org/abs/1412.7193v1", "3 pages, 4 figures, ICLR 2015"]], "COMMENTS": "3 pages, 4 figures, ICLR 2015", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["giljin jang", "han-gyu kim", "yung-hwan oh"], "accepted": false, "id": "1412.7193"}, "pdf": {"name": "1412.7193.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["gjang@ee.knu.ac.kr", "hgkim@cs.kaist.ac.kr", "yhoh@cs.kaist.ac.kr"], "sections": [{"heading": null, "text": "ar Xiv: 141 2.71 93v1 [cs.SD] 2 2This paper proposes a novel framework for unattended separation of audio sources using a deep autoencoder. Characteristics of unknown source signals mixed into the mixed input are automatically determined by correctly configured autoencoders implemented by a multi-layer network and separated by clusters of coefficient vectors in the code layer. By studying the weight vectors to the final target, the representation layer, the primitive components of the audio signals are observed in the frequency domain. Clustering the activation coefficients in the code layer separates the previously unknown source signals, and then separates and reconstructs the original source tones using code vectors belonging to different clusters. The recovered sounds are not perfect, but provide promising results for the possibility of many practical applications."}, {"heading": "1 INTRODUCTION", "text": "In a situation where we only have recordings from a single sensor, the problem is very difficult and therefore no general solution has been found. Spectral masking was proposed in Hu & Wang (2004), where the separator was constructed using estimated speech distances. Such a method worked well in extracting speech from the noisy environment, but performance is not guaranteed if the target source is not a speech signal. Several separation methods based on non-negative matrix factorization (NMF) were proposed to solve the monaural source separation problem (Raj et al., 2010). NMF used the redundancy of the sound spectrum for source separation."}, {"heading": "2 SOURCE SEPARATION USING AUTOENCODER", "text": "In order to learn an autoencoder for the mixtures of audio sources, we first apply a short-term Fourier analysis to the audio signal of the time domain, which results in a magnitude spectrum matrix denoted by Xc, m, where c is the frequency channel index and m is the time frame index. To enter the autoencoder, we construct a rectangular window with consecutive frequency channel and time frame index, such as Wi, j = {Xc, m | i \u2264 c < i + h, j \u2264 m < j + l}, (1) where Wi, j is the window that proceeds from frequency channel i and frame j of the size spectrum, where h and l are the height and length of the windows. The rectangular windows are then rolled up to supervectors for autoencoder training. The dataset is generated by shifting the window frame around a time clock."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "In order to evaluate the performance of the proposed method, source separation experiments were conducted on a mixture of 5 different sounds and 2 different language sources. Language sources are selected from the TIMIT speech corpus, and the sounds are jazz, drum, acoustic guitar, electric guitar and piano. These sounds and music were mixed together, resulting in a total of 2 x 5 = 10 mixes. Audio sounds are sampled at 8 kHz, and only 8 seconds were used to train the respective autoencoders. The spectrogram matrix of the mixes is obtained by short-term Fourier analysis with image length 40 ms and displacement size 10 ms. For window generation in Equation 1, h = 30 and l = 5 were used, which were decided empirically. Figure 3 shows 10 selected examples out of the total of 50 weight vectors that connect all nodes in the last hidden layer to one of the nodes in the final output layer."}, {"heading": "4 CONCLUSIONS", "text": "The spectral matrix obtained by the short-term Fourier analysis is used for the initial representation of the mixed source signal, and a deep autoencoder is used to represent the spatio-temporal local parts of the spectral matrix. The coefficients in the middle layer of the autoencoder serve as a feature for distinguishing audio sources. The main contribution of the proposed method is that the characteristics of unknown sources are extracted from the mixed signals. Although the experimental results are not yet complete, efforts are ongoing to improve the proposed method."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Basic Research Programme of the National Research Foundation of Korea (NRF), which is funded by the Ministry of Education, Science and Technology (No NRF-2010-0025642)."}], "references": [{"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Monaural speech segregation based on pitch tracking and amplitude modulation", "author": ["Hu", "Guoning", "Wang", "DeLiang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Non-negative matrix factorization based compensation of music for automatic speech recognition", "author": ["Raj", "Bhiksha", "Virtanen", "Tuomas", "Chaudhuri", "Sourish", "Singh", "Rita"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Raj et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raj et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Several separation methods based on non-negative matrix factorization (NMF) was proposed to solve the monaural source separation problem (Raj et al., 2010).", "startOffset": 137, "endOffset": 155}], "year": 2014, "abstractText": "This paper proposes a novel framework for unsupervised audio source separation using a deep autoencoder. The characteristics of unknown source signals mixed in the mixed input is automatically by properly configured autoencoders implemented by a network with many layers, and separated by clustering the coefficient vectors in the code layer. By investigating the weight vectors to the final target, representation layer, the primitive components of the audio signals in the frequency domain are observed. By clustering the activation coefficients in the code layer, the previously unknown source signals are segregated. The original source sounds are then separated and reconstructed by using code vectors which belong to different clusters. The restored sounds are not perfect but yield promising results for the possibility in the success of many practical applications.", "creator": "LaTeX with hyperref package"}}}