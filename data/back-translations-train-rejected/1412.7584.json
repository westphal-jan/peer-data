{"id": "1412.7584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Differential Privacy and Machine Learning: a Survey and Review", "abstract": "The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals.", "histories": [["v1", "Wed, 24 Dec 2014 01:51:06 GMT  (31kb)", "http://arxiv.org/abs/1412.7584v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.DB", "authors": ["zhanglong ji", "zachary c lipton", "charles elkan"], "accepted": false, "id": "1412.7584"}, "pdf": {"name": "1412.7584.pdf", "metadata": {"source": "CRF", "title": "Differential Privacy and Machine Learning: a Survey and Review", "authors": ["Zhanglong Ji", "Zachary C. Lipton", "Charles Elkan"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is such that most people who are in a position to survive themselves are also in a position to survive themselves. Most of them are in a position to survive themselves, namely in the form in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they are able to survive, are able to survive, in which they, in which they are able to survive, in which they are able to survive, in which they are able to survive, in which they are able to survive, in which they are able to survive, in which they are able to survive, in which they are able to survive"}, {"heading": "1 Differential Privacy", "text": "Differential privacy is now one of the most popular definitions of privacy. Intuitively, it requires that the mechanism that outputs information about an underlying data set be robust against any change in a sample, thus protecting privacy. The following sections define mathematically differential privacy and introduce some commonly used differential privacy methods."}, {"heading": "1.1 Definition of Differential Privacy", "text": "Definition 1: a mechanism f) is a random function that takes a data set D as input and prints a random variable f (D). (For example, if we assume that D is a medical data set, then the function that prints the number of patients in D plus noise from the normal standard distribution is a mechanism. (Definition 2: the removal of two data sets, d (D, D), means the minimum number of sample changes required to change D to D. (For example, ifD and D) differ in at most one individual, there is d (D): the removal of two data sets, d) = 1. We also call such a pair of data sets that are defined as different sets of data that \"differ in at most one individual.\" This formulation has led to two different understandings. Some interpret this as a replacement of a sample, while others also consider addition and deletion."}, {"heading": "1.2 Query", "text": "Definition 4: A query f is a function that uses a row as input, and the answer to query f is called f (D). For example, if D is a medical record, \"How many patients have been successfully cured?\" is a query because it takes D as input and output of a number. Outputting a query is not necessarily a number, but some mechanisms, notably the laplac mechanism, assume that responses to queries are numeric, or vectors f (D) and Rp, but not categorical. A more complex query can be \"a logistical regression model trained from the row\" that outputs a classification model."}, {"heading": "1.3 The Laplacian Mechanism", "text": "The laplac mechanism [15] is a popular, widely used private mechanism for queries f with answers f (D), in which the sensitivity s (f, E) plays an important role. Definition 5: a query f and a norm function are given. Normally, the norm function is either L1 or L2 standard. The laplac mechanism [15]: a query f and a norm function over the range f, the random function f (D) = f (D) +. Normally, the norm function is either L1 or L2 standard. The laplac mechanism [15]: a query f and a norm function over the range f, the random function f (D) = f (D) +. The random function satisfies the differential privacy. This is a random variable whose probability density is p (E) and a norm function over the range f (E) +."}, {"heading": "1.4 The Exponential Mechanism", "text": "The exponential mechanism [38] is a thoroughly private method for selecting an element from a set. Suppose the record to be selected is A, and there is a score function H, whose input is a record D and a potential response A, and whose output is a real number. In the case of a record D, the exponential mechanism selects the element a, which has a large score H (D, a). Definition 6: The sensitivity of the score function H is defined as ass (H, E) = max d (D, D) = 1, a, a, a, a, a, a, a, a, a, a, A, A, A, A, A, A, A, A, A, A, A, A, and the exponential mechanism: For a record D and a series of possible answers A, a random mechanism selects an answer based on the following probability, then the mechanism is distinctly private:"}, {"heading": "P (a \u2208 A is selected) \u221d e\u01ebH(D,a)/2s(H,\u2016.\u2016)", "text": "The laplac mechanism is related to the exponential mechanism. If f (D) is a vector in Rp, and vice versa, the output has exactly the same distribution as f (D) in the laplac mechanism with half the data protection budget."}, {"heading": "1.5 The Smooth Sensitivity Framework and the Sample and Aggregate Framework", "text": "The noise is determined not only by the query, but also by the database itself. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "1.6 Combination of Differentially Private Mechanisms", "text": "In this section, f & i differentiates between private algorithms, D is the dataset, and D is a partition of D. Notation g () represents any function. [37] returns the following two theories. Sequential theorems [15, 13,?]: if f & i (D) -differentiated is private, then f & i (D) = g (D), f & i (D), f & i (D), f & i (D) -differentially private, then f & i (D) = g (D), f & i (D), f & i (D), f & i (D), f & i (D), f & i (D), f & i (D), f & i (D), and f & i (D)."}, {"heading": "2 Machine Learning", "text": "Informally, a learning algorithm takes as input a series of samples, called a training set, and outputs a model that captures some knowledge of the underlying distribution. Samples are also called examples, and these variables can be either categorical or numeric. In the following sections, if there are variables whose values we want to predict in the context of machine learning, this is called sample space, and all samples in the sample space have the same set of variables. If we want to predict names for new examples, this task is called supervised learning; the task in which there are no labels known for future examples, this variable is called X. If we want to predict labels for new examples, this task is called superordinate learning; the task in which there are no labels, and we want to identify the structure in the data sets, is called unattended learning."}, {"heading": "2.1 Performance Measurement", "text": "Many papers have analyzed the performance of their mechanisms and proved that the private \u03b2 models they output are very close to the true models. However, the analyses in these papers differ in how they define the true model, how they define the distance between two models, and how they define such a distance measurement metric, how they define proximity. However, these differences can hamper our efforts to compare different mechanisms. To evaluate the performance of a differentiated private algorithm, it is necessary to have an idea of a \"true model\" with which comparisons can be made. Some papers [50, 45, 25, 26, 21, 31, 7, 53, 27] consider the \"true model\" as the output of a silent algorithm on training data. Others [14, 33, 40] consider the \"true model\" as the optimal model when the true distribution was known. They also differ from how the distance between two models is defined. Some papers [6, 25, 53] use the difference in the values of the target function."}, {"heading": "2.2 General Ideas of Differentially Private Machine Learning Algorithms", "text": "Some approaches first learn a clean data model and then use either the exponential mechanism or the laplactic mechanism to generate a noisy model. For example, [52, 40, 46, 28, 30] use the laplactic mechanism, while [7, 53] use the exponential mechanism. For some other approaches, which have many iterations or multiple steps, the laplactic mechanism and the exponential mechanism are applied to the initial parameters of each iteration / stage. Such approaches include [21, 31, 24, 25, 41, 55]. Some mechanisms add noise to the target function and use the minimum / maximum of the noisy function as the initial model. This technique is called objective interference. Some examples include [56, 5, 6, 45].Some mechanisms use the framework within which they are measured."}, {"heading": "3 Differentially Private Supervised Learning", "text": "Monitored Machine Learning describes the setting when labels for training data are known, and the task is to train a model to predict accurate labels using a new example. In this section, we will differentiate between private versions of commonly used monitored machine learning algorithms."}, {"heading": "3.1 Naive Bayes Model", "text": "The naive Bayes model is a classifier that predicts the label Y by properties in X. Given the properties X and a model, one can calculate the conditional probability P (Y | X) for all labels Y and predict the label with the most conditional probability. However, the naive Bayes model is based on two assumptions. The first assumption is that Xj's conditional capabilities of Y are conditionally independent, i.e., P (Xj | Y) is a normal distribution. Based on the first assumption and Bayes \"theorem, the conditional probability is that we can calculate the coefficient of each attribute independently. The second assumption is that for all numerical properties in X, P (X | Y) is a normal distribution.Based on the first assumption and Bayes\" theorem, \"the conditional probability is as follows: P (Y | X1,..., Xp).P (Y) p).P (Y) p = 1P (Xj | Y) To train the model, we must all estimate Y (P)."}, {"heading": "3.2 Linear Regression", "text": "Linear regression is a technique for predicting numerical values Y, in which the value is modeled as a linear combination wTX of characteristics X. Here, the vector w contains the weights associated with each characteristic and forms the set of parameters that must be optimized during the training. To train the model, w is calculated by minimizing the square loss \u2211 i (yi \u2212 wTxi) 2 over the training set. [56] Starting from a limited sample space, the mechanism proposes a differentiated private mechanism for linear regression. Since the loss function is analytical, the mechanism extends the function with Taylor expansion, approaches it with an approximation of low order and adds the coefficients of the terms noise. The mechanism then finds the w, which minimizes the approximate loss function. Since the sensitivities of the coefficients are easy to compile, the laplac mechanism can ensure the differential privacy of the noisy approximation."}, {"heading": "3.3 Linear SVM", "text": "A linear SVM model outputs a score wTX for features X in a sample and normally uses characters (wTX) as designation Y. The parameter w is calculated by limiting the sample space, the linear SVM model fulfills the following two conditions. Firstly, it calculates w by minimizing a strongly convex and differentiable loss function L (w). Secondly, a change in a sample results in a limited change in the sample space. Linear SVM and all other models that meet the two conditions [5, 6] result in an output disturbance mechanism and an objective disturbance mechanism."}, {"heading": "3.4 Logistic Regression", "text": "Logistic regression is a binary classification model. It predicts P (Y = 1 | X) = 1 / (1 + e \u2212 wTX) in view of characteristics X in a sample. Parameters w are trained by minimizing the negative log probability \u2211 i log (1 + exp (\u2212 yiwTxi)) via the training set. Regulated logistic regression differs from standard logistic regression in that the loss function includes a regulation term. Its w is calculated by minimizing the I-log (1 + exp (\u2212 yiwTxi) + \u03bbwTw via the training set {(xi, yi)}, while \u03bb > 0 is a hyperparameter that determines the strength of the regulation. Assuming that the sample space is limited, the mechanism can be applied in [56] (see section 3.2) (see section 3.3) to make both models competitive."}, {"heading": "3.5 Kernel SVM", "text": "The kernel SVM is a machine learning model that uses a kernel function K (,) that uses two examples as input and output mechanisms that already yield a real number. Different kernel functions lead to different SVM models. Kernel SVM can be used for both classification and regression. In both cases {(xi, yi)} are training samples and weights in the model for calculation. The model includes the kernel function K (), all training data and a vector of weights {wi} ni = 1. Although there are many algorithms to train SVM, I address only those that are relevant for current private versions.Unlike previous models, SVMs contain all training data."}, {"heading": "3.6 Decision Tree Learning", "text": "The training algorithm for a Decision Tree Classifier consists of a tree-building process and a pruning process. In the tree-building process, the entire sample space and all samples are first included in the root partition. Then, the algorithm iteratively selects an existing partition, selects a variable based on the samples in that partition and a score function such as information gain or Gini index, and partitions the sample space (and samples corresponding to that partition) according to the selected variable. If the selected variable is categorical, each value of that variable is numerical, then some thresholds are selected, and the partition is based on these thresholds. The partitioning process ends when the spaces corresponding to all partitions are small enough or the number of samples in each partition is too small."}, {"heading": "3.7 Online Convex Programming", "text": "Many machine learning techniques, such as logistic regression and SVM, specify optimization problems that must then be solved to find the optimal parameters. Online algorithms, such as gradient descent, which look at examples individually, are widely used for this purpose. To show that a machine learning algorithm is differentiated private, it is therefore important to show that the optimization algorithm does not contain any information.Online convex programming (OCP) solves convex programming problems in an online manner. Input to an OCP algorithm is a sequence of functions (f1,..., fT) and the output is a sequence of dots (w1,..., wT) from a convex sentence C. The algorithm is iterative and starts from a random point w0. In t-th iteration, the algorithm receives the function ft and gives a point wt \u00b2, wC, w1,... (and IGfunction is)."}, {"heading": "4 Differentially Private Unsupervised Learning", "text": "Without labels, unsupervised machine learning algorithms find structure in the data set. Clustering, for example, tries to find different groups to which each data point belongs. In many contexts, such as medical diagnosis, it can be useful to know that a person is a member of a group that has certain specific characteristics. However, publishing high-level information about a group can unintentionally reveal information about the people in the data set. Therefore, it is important to develop differentiated private, unsupervised machine learning algorithms."}, {"heading": "4.1 K-means clustering", "text": "To train the model, the algorithm starts with k randomly selected points representing the k groups, then iteratively clusters samples to the next point and updates the points by the mean of the samples bundled to the points. [42] It proposes a (B, E) -differentiated private k-mean cluster algorithm using the sample and aggregate frameworks. The mechanism is based on the assumption that the data are well separated. \"Well separated\" means that the clusters can easily be estimated with a small number of samples, which is a prerequisite for the sample and the aggregate frame. The mechanism randomly divides the training environment into many subsets, runs the non-private k-mean algorithm on each subset to obtain many results, and then uses the smooth sensitivity frame to publish the results from a dense region. This step preserves privacy while the underlying k-mean algorithm is unchanged."}, {"heading": "5 Differentially Private Dimensionality Reduc-", "text": "In the context of machine learning, learning low-dimensional representation is often desirable for high-dimensional data. Lower-dimensional data sets provide models with fewer degrees of freedom and tend to be less susceptible to overfits. From a differential privacy perspective, low-dimensional representations are desirable because they tend to have lower sensitivity. Feature Select is a dimensionality reduction technique that keeps a subset of features away from an original feature space. Principal Component Analysis (PCA), on the other hand, is a matrix factorization technique in which a linear projection of the original data set into a low-dimensional space is learned, so that the new representation explains as much as possible of the deviation from the original data set."}, {"heading": "5.1 Feature Selection", "text": "[53] proposes a selection of private characteristics, PrivateKD, to classify \u03b242. PrivateKD is based on the assumption that all characteristics are categorical and each characteristic has finite possible values. For each set of characteristics S, it defines a function F (S), which indicates how many pairs of samples from different classes can distinguish characteristics in S. The set of selected characteristics S \u00b2 is initialized to \u2205. Then, a greedy algorithm adds new characteristics one by one to S \u00b2. In selecting a characteristic to be added, the exponential mechanism uses the relationship m = d \u2212 1. In this case, except for probability O (1 / poly (m), a correct algorithm (s \u00b2) is provided for the specific case where the cardinality of the sample space m and the number of characteristics d mean the relationship m = d \u2212 1."}, {"heading": "5.2 Principal Component Analysis", "text": "So it is the self-composition of the data greatly reduced that this analysis is closely related to the self-decomposition: if we look at the self-promoters of the matrix A = V ar according to the corresponding eigenvalues of the eigenvalues1).It is thus known that the first eigenvalues1).There are two different private mechanisms to eigenvalues.The iterative methods are based on the spectral decomposition, which ensures that the components that correspond to the first i \u2212 1 eigenvalues.The components that correspond to the first i \u2212 1 eigenvalues.The components of A are subtracted, then becomes the largest eigenvalues.The iterative methods are based on the spectral decomposition, which ensures that the components correspond to the first i \u2212 1 eigenvalues.The components of A are subtracted, then becomes the largest eigenvalues.The iterative methods are based on the spectral decomposition."}, {"heading": "6 Statistical Estimators", "text": "Statistical estimators calculate approximations of interest based on the evidence in a given dataset. Simple examples include the population mean and variance. Although estimators are clearly useful, they can potentially reveal information about the individuals in the dataset, especially if the dataset is small or features are rare. Therefore, to protect privacy, it is necessary to develop differentiated private estimators."}, {"heading": "6.1 Robust Statistics Estimator", "text": "The definition of a robust estimator is based on the stability of estimates. An estimator is robust if for each element x in the sample space the following limit exists: limt \u2192 0 (T \u2212 t) P + t\u03b4x (P \u2212 t). The distribution (P \u2212 t).x means that the probability of the sample is 1."}, {"heading": "6.2 Point Estimator", "text": "[49, 50] give a differential private mechanism for estimating points. On the basis of the notation in 6.1, the mechanism D randomly divides into k subsets of the same size {D1..., Dk} and estimates the parameters {T (D1), T (D2),..., T (Dk) for each subset. Then it uses a differential private mean of all T (Di) to approximate T (D). The mean is calculated in two steps. First, if the space of the parameters is unlimited, it calculates two quantiles and truncates all estimates according to the quantities and sample size. Second, the mechanism adds laplactic noise to the mean of the truncated values and publishes the loud mean. If the space of the possible parameters is limited, then the mechanism performs only the second step and becomes differentially private in each case."}, {"heading": "6.3 M-estimator", "text": "In contrast to the robust estimator above, the definition of an M estimator depends on the function from which the estimates are derived. M estimation is based on a function m (,) which takes a sample and a parameter \u03b8 as input and outputs a real number. An M estimator estimates the parameter \u03b8 by calculating the number of samples in the cubes and calculates the density function in each cube by dividing the noisy number corresponding to the cube by the volume of the cube. Noisy density function results in a noisy target function in the minimizing problem and the noisy target function results in a noisy minimum. The noisy minimum can be divided by the volume of the cube. Noisy density function results in a noisy target function in the minimizing problem and the noisy target function results in a noisy minimum."}, {"heading": "7 Learning in Private Data Release", "text": "In this section, we focus on data-sharing mechanisms that are either useful for machine learning or based on machine learning algorithms. Many papers use partition-based algorithms to share data. [55], however, assumes that the density function in each partition is smooth, [8, 57] assumes that the format of the data allows it to be organized into a tree, and [41] assumes that partitions can retain most important information for further data mining. All of these assumptions motivate the partitioning of the sample space and the publication of data in each partition. In terms of mechanism design, the mechanisms can be divided into two groups. [41] first partitions that use the sample space using the exponential mechanism and then add noise to the counts. Some others ([8, 57, 55]) generate noisy counts with the laplacian mechanisms for each cell and then the partition."}, {"heading": "8 Theoretical Results", "text": "[32] examines the general characteristics of private classifiers rather than individual learning models, first defining a problem that can be learned if there is an algorithm that can output a high-precision model with a sufficiently high probability; accuracy is measured here as a percentage of correctly classified samples; they also claim that a problem, if it is learnable, can necessarily be learned privately; the mechanism they construct takes the number of correctly classified samples as a score function and uses the exponential mechanism to draw the best model that it calls the best hypothesis in a class; [39] formulates differentiated private learning within an information theory framework; the paper uses a concept in information theory, the PAC-Bayesian boundary; this concept refers to parametrized models that have limited loss functions; and it uses the Bayesian learning model that uses the Bayesian learning framework that has a limited PAC-Bayesian boundary."}, {"heading": "9 Discussion", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "Acknowledgments", "text": "The authors thank Kamalika Chaudhuri for her comments. The authors are partly funded by NLM (R00LM011392)."}], "references": [{"title": "Privacy, accuracy, and consistency too: a holistic solution to contingency table release", "author": ["Boaz Barak", "Kamalika Chaudhuri", "Cynthia Dwork", "Satyen Kale", "Frank McSherry", "Kunal Talwar"], "venue": "In ACM SIGACT-SIGMOD- SIGART Symposium on Principles of Database Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Practical privacy: the SuLQ framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A learning theory approach to non-interactive database privacy", "author": ["Avrim Blum", "Katrina Ligett", "Aaron Roth"], "venue": "In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Convergence rates for differentially private statistical estimation", "author": ["Kamalika Chaudhuri", "Daniel Hsu"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Near-optimal differentially private principal components", "author": ["Kamalika Chaudhuri", "Anand D. Sarwate", "Kaushik Sinha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Publishing set-valued data via differential privacy", "author": ["Rui Chen", "Noman Mohammed", "Benjamin C.M. Fung", "Bipin C. Desai", "Li Xiong"], "venue": "In International Conference on Very Large Data Bases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Personal privacy vs population privacy: learning to attack anonymization", "author": ["Graham Cormode"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Differentially private summaries for sparse data", "author": ["Graham Cormode", "Cecilia M. Procopiuc", "Divesh Srivastava", "Thanh T.L. Tran"], "venue": "In International Conference on Database Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Lower bounds in differential privacy", "author": ["Anindya De"], "venue": "In Theory of Cryptography,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Differential privacy. In Encyclopedia of Cryptography and Security (2nd Ed.)", "author": ["Cynthia Dwork"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor"], "venue": "In International Conference on the Theory and Applications of Cryptographic Techniques,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Differential privacy and robust statistics", "author": ["Cynthia Dwork", "Jing Lei"], "venue": "In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Boosting and differential privacy", "author": ["Cynthia Dwork", "Guy N. Rothblum", "Salil P. Vadhan"], "venue": "In FOCS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Differential privacy for statistics: What we know and what we want to learn", "author": ["Cynthia Dwork", "Adam Smith"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Adaptive differentially private histogram of low-dimensional data", "author": ["Chengfang Fang", "Ee-Chien Chang"], "venue": "In Privacy Enhancing Technologies,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Data mining with differential privacy", "author": ["Arik Friedman", "Assaf Schuster"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Composition attacks and auxiliary information in data privacy", "author": ["Srivatsava Ranjit Ganta", "Shiva Prasad Kasiviswanathan", "Adam Smith"], "venue": "In KDD,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Beyond worst-case analysis in private singular vector computation", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Computing Research Repository,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Privacy and data-based research", "author": ["Ori Heffetz", "Katrina Ligett"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays", "author": ["Nils Homer", "Szabolcs Szelinger", "Margot Redman", "David Duggan", "Waibhav Tembe", "Jill Muehling", "John Pearson", "Dietrich Stephan", "Stanley Nelson", "David Craig"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "A practical differentially private random decision tree classifier", "author": ["Geetha Jagannathan", "Krishnan Pillaipakkamnatt", "Rebecca N. Wright"], "venue": "In International Conference on Data Mining Workshops,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Differentially private online learning", "author": ["Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta"], "venue": "In Conference on Learning Theory,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Differentially private learning with kernels", "author": ["Prateek Jain", "Abhradeep Thakurta"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Application of rough sets in the presumptive diagnosis of urinary system diseases", "author": ["J.Czerniak", "H.Zarzycki"], "venue": "In Artifical Inteligence and Security in Computing Systems, ACS\u20192002 9th International Conference Proceedings,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Differential privacy based on importance weighting", "author": ["Zhanglong Ji", "Charles Elkan"], "venue": "In Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Differentially private distributed logistic regression using private and public data", "author": ["Zhanglong Ji", "Xiaoqian Jiang", "Shuang Wang", "Li Xiong", "Lucila Ohno- Machado"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Differential-private data publishing through component analysis", "author": ["Xiaoqian Jiang", "Zhanglong Ji", "Shuang Wang", "Noman Mohammed", "Samuel Cheng", "Lucila Ohno-Machado"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "On differentially private low rank approximation", "author": ["Michael Kapralov", "Kunal Talwar"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "What can we learn privately", "author": ["Shiva Prasad Kasiviswanathan", "Homin K. Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "In IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Differentially private M-estimators", "author": ["Jing Lei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["Ninghui Li", "Tiancheng Li", "Suresh Venkatasubramanian"], "venue": "In International Conference on Data Engineering,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Compressive mechanism: utilizing sparse representation in differential privacy", "author": ["Yang D. Li", "Zhenjie Zhang", "Marianne Winslett", "Yin Yang"], "venue": "In Workshop on Privacy in the Electronic Society,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "l-diversity: Privacy beyond kanonymity", "author": ["Ashwin Machanavajjhala", "Johannes Gehrke", "Daniel Kifer"], "venue": "In International Conference on Data Engineering,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["Frank McSherry"], "venue": "In SIGMOD Conference,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In FOCS, pages 94\u2013103,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Differentially-private learning and information theory", "author": ["Darakhshan J. Mir"], "venue": "In International Conference on Extending Database Technology Workshops,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "A differentially private graph estimator", "author": ["Darakhshan J. Mir", "Rebecca N. Wright"], "venue": "In International Conference on Data Mining Workshops,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Differentially private data release for data mining", "author": ["Noman Mohammed", "Rui Chen", "Benjamin C.M. Fung", "Philip S. Yu"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "In ACM SIGACT-SIGMOD- SIGART Symposium on Principles of Database Systems,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "The composition theorem for differential privacy", "author": ["Sewoong Oh", "Pramod Viswanath"], "venue": "In Computing Research Repository,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Learning in a large function space: Privacy-preservingmechanisms for SVM learning", "author": ["Benjamin I.P. Rubinstein", "Peter L. Bartlett", "Ling Huang", "Nina Taft"], "venue": "In Computing Research Repository,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Sharing graphs using differentially private graph models", "author": ["Alessandra Sala", "Xiaohan Zhao", "Christo Wilson", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In Internet Measurement Conference,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data", "author": ["Anand D. Sarwate", "Kamalika Chaudhuri"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Mining frequent graph patterns with differential privacy", "author": ["Entong Shen", "Ting Yu"], "venue": "In KDD,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Efficient, differentially private point estimators", "author": ["Adam Smith"], "venue": "In Computing Research Repository,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2008}, {"title": "Privacy-preserving statistical estimation with optimal convergence rates", "author": ["Adam Smith"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "k-anonymity: A model for protecting privacy", "author": ["Latanya Sweeney"], "venue": "In International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2002}, {"title": "Differentially private naive Bayes classification", "author": ["Jaideep Vaidya", "Basit Shafiq", "Anirban Basu", "Yuan Hong"], "venue": "InWeb Intelligence,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Differentially private projected histograms: Construction and use for prediction", "author": ["Staal A. Vinterbo"], "venue": "In European Conference on Machine Learning (ECML) and Conference on Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2012}, {"title": "Differentially private network data release via structural inference", "author": ["Qian Xiao", "Rui Chen", "Kian-Lee Tan"], "venue": "In KDD,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Differentially private data release through multidimensional partitioning", "author": ["Yonghui Xiao", "Li Xiong", "Chun Yuan"], "venue": "In Secure Data Management,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2010}, {"title": "Functional mechanism: Regression analysis under differential privacy", "author": ["Jun Zhang", "Zhenjie Zhang", "Xiaokui Xiao", "Yin Yang", "Marianne Winslett"], "venue": "In International Conference on Very Large Data Bases,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}, {"title": "Differentially private setvalued data release against incremental updates", "author": ["Xiaojian Zhang", "Xiaofeng Meng", "Rui Chen"], "venue": "In International Conference on Database Systems for Advanced Applications,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "1 [9] uses privacy to mean both population privacy and individual privacy.", "startOffset": 2, "endOffset": 5}, {"referenceID": 50, "context": "In a well-known case, the personal health information of Massachusetts governor William Weld was discovered in a supposedly anonymized public database [51].", "startOffset": 151, "endOffset": 155}, {"referenceID": 50, "context": "To combat such background attacks, some more robust definitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed.", "startOffset": 95, "endOffset": 99}, {"referenceID": 35, "context": "To combat such background attacks, some more robust definitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed.", "startOffset": 112, "endOffset": 116}, {"referenceID": 33, "context": "To combat such background attacks, some more robust definitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed.", "startOffset": 132, "endOffset": 136}, {"referenceID": 22, "context": "Recently [23], researchers demonstrated that an attacker could infer whether an individual had participated in a genome study using only publicly available aggregated genetic data.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "Differential privacy [12, 13], which will be introduced in the next section, uses random noise to ensure that the publicly visible information doesn\u2019t change much if one individual in the dataset changes.", "startOffset": 21, "endOffset": 29}, {"referenceID": 12, "context": "Differential privacy [12, 13], which will be introduced in the next section, uses random noise to ensure that the publicly visible information doesn\u2019t change much if one individual in the dataset changes.", "startOffset": 21, "endOffset": 29}, {"referenceID": 16, "context": "Several recent surveys address differential privacy and data science [17, 47, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 46, "context": "Several recent surveys address differential privacy and data science [17, 47, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 21, "context": "Several recent surveys address differential privacy and data science [17, 47, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 16, "context": "Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between differential privacy and machine learning.", "startOffset": 13, "endOffset": 21}, {"referenceID": 21, "context": "Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between differential privacy and machine learning.", "startOffset": 13, "endOffset": 21}, {"referenceID": 46, "context": "Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between differential privacy and machine learning.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Definition 3: a mechanism f\u0303 satisfies (\u01eb, \u03b4)-differential privacy [12, 13] for two non-negative numbers \u01eb and \u03b4 iff for all neighbors d(D,D) = 1, and all subset S of f\u0303 \u2019s range, as long as the following probabilities are well-defined, there holds P (f\u0303(D) \u2208 S) \u2264 \u03b4 + eP (f\u0303(D) \u2208 S)", "startOffset": 67, "endOffset": 75}, {"referenceID": 12, "context": "Definition 3: a mechanism f\u0303 satisfies (\u01eb, \u03b4)-differential privacy [12, 13] for two non-negative numbers \u01eb and \u03b4 iff for all neighbors d(D,D) = 1, and all subset S of f\u0303 \u2019s range, as long as the following probabilities are well-defined, there holds P (f\u0303(D) \u2208 S) \u2264 \u03b4 + eP (f\u0303(D) \u2208 S)", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "There is also a commonly used heuristic to choose \u03b4[20]: when there are n samples in the dataset, \u03b4 \u2208 o(1/n).", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "[11] shows that in terms of mtutal information, \u01eb-differential privacy is much stronger than (\u01eb, \u03b4)-differential privacy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The Laplacian mechanism[15] is a popular \u01eb-differentially private mechanism for queries f with answers f(D) \u2208 R, in which sensitivity (Definition 5) plays an important role.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "The Laplacian mechanism[15]: given a query f and a norm function over the range of f , the random function f\u0303(D) = f(D) + \u03b7 satisfies \u01eb-differential privacy.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "\u2016))2 log 2 \u03b4 ) [2].", "startOffset": 15, "endOffset": 18}, {"referenceID": 37, "context": "The exponential mechanism[38] is an \u01eb-differentially private method to select one element from a set.", "startOffset": 25, "endOffset": 29}, {"referenceID": 41, "context": "Smooth sensitivity [42] is a framework which allows one to publish an (\u01eb, \u03b4)differentially private numerical answer to a query.", "startOffset": 19, "endOffset": 23}, {"referenceID": 41, "context": "The sample and aggregate framework [42] is a mechanism to respond to queries whose answers can be approximated well with a small number of samples, while ensuring (\u01eb, \u03b4)-differential privacy.", "startOffset": 35, "endOffset": 39}, {"referenceID": 41, "context": "An efficient aggregation function is provided in the paper [42].", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "[37] provides the following two theorems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Some more sophisticated forms of this theorem can be find in [16, 43].", "startOffset": 61, "endOffset": 69}, {"referenceID": 42, "context": "Some more sophisticated forms of this theorem can be find in [16, 43].", "startOffset": 61, "endOffset": 69}, {"referenceID": 49, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 44, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 5, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 24, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 25, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 20, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 30, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 6, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 52, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 26, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 13, "context": "However, others [14, 33, 40] consider the \u2018true model\u2019 to mean the optimal model if the true distribution were known.", "startOffset": 16, "endOffset": 28}, {"referenceID": 32, "context": "However, others [14, 33, 40] consider the \u2018true model\u2019 to mean the optimal model if the true distribution were known.", "startOffset": 16, "endOffset": 28}, {"referenceID": 39, "context": "However, others [14, 33, 40] consider the \u2018true model\u2019 to mean the optimal model if the true distribution were known.", "startOffset": 16, "endOffset": 28}, {"referenceID": 5, "context": "Some papers [6, 25, 53] use the difference of values of the target function.", "startOffset": 12, "endOffset": 23}, {"referenceID": 24, "context": "Some papers [6, 25, 53] use the difference of values of the target function.", "startOffset": 12, "endOffset": 23}, {"referenceID": 52, "context": "Some papers [6, 25, 53] use the difference of values of the target function.", "startOffset": 12, "endOffset": 23}, {"referenceID": 13, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 49, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 32, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 39, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 20, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 30, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 6, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 26, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 44, "context": "Still others [45, 26] use the distance between the predictions made by private and non-private models at certain points in the sample space.", "startOffset": 13, "endOffset": 21}, {"referenceID": 25, "context": "Still others [45, 26] use the distance between the predictions made by private and non-private models at certain points in the sample space.", "startOffset": 13, "endOffset": 21}, {"referenceID": 13, "context": "Given a measure of distance between two models, some papers [14, 40] prove that as the number of training examples grows large, the output converges to the true model.", "startOffset": 60, "endOffset": 68}, {"referenceID": 39, "context": "Given a measure of distance between two models, some papers [14, 40] prove that as the number of training examples grows large, the output converges to the true model.", "startOffset": 60, "endOffset": 68}, {"referenceID": 44, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 49, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 5, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 25, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 20, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 30, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 6, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 52, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 26, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 2, "context": "For those which prove bounds on the speed of convergence, the convergence is usually measured by (\u03b1, \u03b2)-usefulness [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 32, "context": "A few papers [33] provide worst case guarantees on the distance, which is equivalent to (\u03b1, 0)-usefulness.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Yet another paper [25] uses the expectation of difference.", "startOffset": 18, "endOffset": 22}, {"referenceID": 51, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 39, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 45, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 27, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 29, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 6, "context": "[7, 53] use the exponential mechanism.", "startOffset": 0, "endOffset": 7}, {"referenceID": 52, "context": "[7, 53] use the exponential mechanism.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 30, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 23, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 18, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 24, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 40, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 54, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 55, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 4, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 5, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 44, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 41, "context": "Mechanisms that employ this idea include [42, 27].", "startOffset": 41, "endOffset": 49}, {"referenceID": 26, "context": "Mechanisms that employ this idea include [42, 27].", "startOffset": 41, "endOffset": 49}, {"referenceID": 13, "context": "The linear regression in [14] is partially based on this idea.", "startOffset": 25, "endOffset": 29}, {"referenceID": 32, "context": "For example, [33] partitions the sample space and uses counts in each partition to estimate the density function.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "[26] interprets a model as a function and uses another function to approximate it by iteratively minimizing the largest distance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "An \u01eb-differentially private naive Bayes model mechanism is introduced in [52].", "startOffset": 73, "endOffset": 77}, {"referenceID": 55, "context": "[56] assumes bounded sample space and proposes a differentially private mechanism for linear regression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "For linear SVM and all other models satisfying the two conditions, [5, 6] provide an output perturbation mechanism and an objective perturbation mechanism.", "startOffset": 67, "endOffset": 73}, {"referenceID": 5, "context": "For linear SVM and all other models satisfying the two conditions, [5, 6] provide an output perturbation mechanism and an objective perturbation mechanism.", "startOffset": 67, "endOffset": 73}, {"referenceID": 5, "context": "[6] also provides a performance analysis of the objective perturbation mechanism.", "startOffset": 0, "endOffset": 3}, {"referenceID": 55, "context": "Assuming that the sample space is bounded, the mechanism in [56] (see Section 3.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "Furthermore, the output perturbation and objective perturbation mechanism in [5, 6] (see Section 3.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "Furthermore, the output perturbation and objective perturbation mechanism in [5, 6] (see Section 3.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "In [6, 45], an idea for private kernel SVM is proposed.", "startOffset": 3, "endOffset": 10}, {"referenceID": 44, "context": "In [6, 45], an idea for private kernel SVM is proposed.", "startOffset": 3, "endOffset": 10}, {"referenceID": 43, "context": "According to [44], the kernel function of two samples in the original sample space can be approximated by the inner product of their projections in the new space.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "Therefore [26] proposes another private kernel SVM algorithm for all RKHS kernels.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "The Test Data-independent Learner (TTDP) mechanism in [26] publishes a private kernel SVM model satisfying (\u01eb, \u03b4)-differential privacy as follows.", "startOffset": 54, "endOffset": 58}, {"referenceID": 23, "context": "[24] proposes an \u01eb-differentially private mechanism.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposes another \u01eb-differentially private decision tree algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] provides (\u01eb, \u03b4)-differentially private versions for two of them: the Implicit Gradient Descent (IGD) and the Generalized Infinitesimal Gradient Ascent (GIGA) given all the functions are L-Lipschitz continuous for some", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The private mechanism in [25] adds Gaussian noise to every \u0175t before it is projected to wt to preserve privacy, and then use the noisy wt for the future computation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 41, "context": "[42] proposes an (\u01eb, \u03b4)-differentially private k-means clustering algorithm using the sample and aggregate framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[53] proposes an \u01eb-differentially private feature selection, PrivateKD, for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] proposes an (\u01eb, \u03b4)-differentially private algorithm for feature selection when the target function is stable.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "For the first kind of functions, the mechanism uses the smooth sensitivity framework in [42] to select features.", "startOffset": 88, "endOffset": 92}, {"referenceID": 41, "context": "For the second kind of functions, the mechanism uses an idea similar to the sample and aggregate framework in [42]: it creates some bootstrap sets from the private dataset, selects features non-privately on each set, and counts the frequencies of feature sets output by the algorithm.", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "An (\u01eb, \u03b4)-differentially private mechanism is proposed in [21].", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "[31] provides an \u01eb-differentially private mechanism for principal component analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] proposes an \u01eb-differentially private mechanism, PPCA, to compute k largest eigenvectors at the same time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] proposes an (\u01eb, \u03b4)-differentially private mechanism for robust statistical estimators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Based on the property, [14] comes up with a ProposeTest-Release framework.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "Based on this framework, [14] proposes three mechanisms for interquartile range estimation, trimmed mean and median, and linear regression, respectively.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "[4] explores robust estimators in another way.", "startOffset": 0, "endOffset": 3}, {"referenceID": 48, "context": "[49, 50] give a differentially private mechanism for point estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 49, "context": "[49, 50] give a differentially private mechanism for point estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[33] proposes an \u01eb-differentially private mechanism for M-estimator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "The mechanism in [33] first divides the sample space ([0, 1]) into many small cubes without using private data.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The mechanism in [33] first divides the sample space ([0, 1]) into many small cubes without using private data.", "startOffset": 54, "endOffset": 60}, {"referenceID": 2, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 0, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 17, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 9, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 54, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 50, "endOffset": 57}, {"referenceID": 56, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 50, "endOffset": 57}, {"referenceID": 40, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 130, "endOffset": 134}, {"referenceID": 40, "context": "[41] first partitions the sample space using the exponential mechanism and then adds noise to the counts using the Laplacian mechanism.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.", "startOffset": 13, "endOffset": 24}, {"referenceID": 56, "context": "Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.", "startOffset": 13, "endOffset": 24}, {"referenceID": 54, "context": "Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.", "startOffset": 13, "endOffset": 24}, {"referenceID": 39, "context": "[40, 46] publish a graph generator model based on the assumption that the private data is fit well by some parametrized generative model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 45, "context": "[40, 46] publish a graph generator model based on the assumption that the private data is fit well by some parametrized generative model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 53, "context": "[54] represents the network structure by using a statistical hierarchical random graph model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] makes use of a public dataset, assigning weights to its examples to embed information contained in the private data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] assumes that samples have binary labels and that they are fit by a linear discriminant analysis (LDA) model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] assumes that the data matrix is sparse, thus the mechanism can make use of results from compressive sensing research.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] first randomly projects the data matrix, then adds noise to the compressed information, and reconstructs data from the noisy compressed information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] studies the general properties of private classifiers, instead of individual learning models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] formulates differentially private learning in an information theoretic framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "According to [39], the output of the exponential mechanism follows the posterior distribution that minimizes", "startOffset": 13, "endOffset": 17}, {"referenceID": 38, "context": "Therefore the conclusion in [39] doesn\u2019t necessarily mean that the exponential mechanism is the best.", "startOffset": 28, "endOffset": 32}, {"referenceID": 51, "context": "For example, [52] adds noise to the counts that generate the naive Bayes model instead of conditional probabilities of the model directly.", "startOffset": 13, "endOffset": 17}, {"referenceID": 44, "context": "For example, [45] transforms kernel SVM to linear SVM, and [14] uses a robust linear regression model to replace the commonly used model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "For example, [45] transforms kernel SVM to linear SVM, and [14] uses a robust linear regression model to replace the commonly used model.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "According to [28, 29], such a public dataset can enhance the performance of differentially private mechanisms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 28, "context": "According to [28, 29], such a public dataset can enhance the performance of differentially private mechanisms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 47, "context": "For those models, one can consider trying the MCMC-based algorithm as in [48].", "startOffset": 73, "endOffset": 77}, {"referenceID": 49, "context": "For example, [50] proves that (\u01eb, \u03b4)-differential privacy is free for learning models satisfying a certain set of conditions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The mechanism in [6] ensures free \u01eb-differential privacy for regularized logistic regression models and linear SVM models, where noise from sample randomness is O(1/ \u221a n) and the noise to preserve privacy is O(1/n).", "startOffset": 17, "endOffset": 20}, {"referenceID": 27, "context": "The mechanism in [28] also proves that the effect of noise brought by differential privacy is O(1/n), while the effect from sample randomness is O(1/ \u221a n).", "startOffset": 17, "endOffset": 21}], "year": 2014, "abstractText": "The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals. In this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially", "creator": "LaTeX with hyperref package"}}}