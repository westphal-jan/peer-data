{"id": "1412.7725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Automatic Photo Adjustment Using Deep Neural Networks", "abstract": "Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work.", "histories": [["v1", "Wed, 24 Dec 2014 17:51:17 GMT  (7148kb,D)", "http://arxiv.org/abs/1412.7725v1", "referred to ACM Transactions on Graphics by Siggraph Asia 2014"], ["v2", "Sat, 16 May 2015 03:49:35 GMT  (7612kb,D)", "http://arxiv.org/abs/1412.7725v2", "TOG minor revision"]], "COMMENTS": "referred to ACM Transactions on Graphics by Siggraph Asia 2014", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["zhicheng yan", "hao zhang", "baoyuan wang", "sylvain paris", "yizhou yu"], "accepted": false, "id": "1412.7725"}, "pdf": {"name": "1412.7725.pdf", "metadata": {"source": "CRF", "title": "Automatic Photo Adjustment Using Deep Learning", "authors": ["Zhicheng Yan", "Hao Zhang", "Baoyuan Wang"], "emails": ["zyan3@illinois.edu,", "hao@cs.cmu.edu,", "baoyuanw@microsoft.com,", "sparis@adobe.com,", "yizhouy@acm.org.", "permissions@acm.org."], "sections": [{"heading": null, "text": "In fact, it is not as if this is a way for people who see themselves as able to deceive themselves and others, but it is also a time-consuming and challenging task that requires advanced skills beyond the skills of casual photographers. Using an automated algorithm is an attractive alternative to manual work, but such an algorithm poses many problems. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Moreover, these settings are often spatially different. As a result of these characteristics, existing automatic algorithms are still limited and cover only part of these challenges."}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in fact, in fact, are able to move, are able to"}, {"heading": "2. RELATED WORK", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3. A DEEP LEARNING MODEL", "text": "We will now discuss how to present copy-based photo matching as a regression problem, and how to set up a DNN to solve this regression problem. (D) We will ask ourselves how to select the images before and after magnification. (D) We have the premise that there is an intrinsic color mapping function that converts each pixel color in Ik to their respective pixel color in Jk for each pixel color. (D) Our goal is to train an approximate function F that can be applied to new images to improve image quality there. (D) The value of F) is simply the color of Jk to pixel pi, while the input of F) is more complex because F) depends not only on the color of pi to Ik, but also on local and global information extracted from Ik. (D) We are simply the color of Jk to pixel pi, while the input of F depends on not only the color of pi to Ik, but also on local and global information extracted from Ik."}, {"heading": "3.1 Neural Network Architecture and Training", "text": "In fact, most of us are able to move in a series of layers in which the individual layers are located, and in the other layer in which they are located. - The entry layer is directly tailored to the input vector, i.e., it is able to transform the elements of color in a layer in which the layers are located. - The initial layer is directly related to the input vector, i.e., it is able to transform the elements of color in a layer in which the layers are hidden. - Each layer absorbs the reactions to the neurons in the previous layer, i.e. it is in the next layer in which they are located. - The initial layers are able to transform the elements of color in a layer in which the layers are hidden. - Each layer absorbs the reactions to the neurons in the previous layer, i.e. it is in the next layer in which they are located. - The initial layers are able to transform the elements of color in the second layer, in the third layer, in the third layer, and in the third layer, and in the third layer, and in the third layer."}, {"heading": "4. FEATURE DESCRIPTORS", "text": "Our feature descriptor (xi) on an example pixel pi serves as the input layer in the neural network. It consists of three components, xi = (xpi, x c i, x g i), where x p i represents pixel-by-pixel attributes, x c i contextual attributes calculated for a local region surrounding pi, and xgi global attributes calculated for the entire image into which pi belongs."}, {"heading": "4.1 Pixelwise Features", "text": "Pixel-wise characteristics reflect high-resolution image variations at pixel level and are indispensable for learning spatially varying photoACM transactions on graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.enhancement models. They are defined as xpi = (ci, pi), where ci represents the average color in the CIELab color space within the 3x3 neighborhood and pi = (xi, yi) denotes the normalized sample position within the image."}, {"heading": "4.2 Global Features", "text": "In photographic practice, global attributes and overall impressions, such as the average intensity of an image, have at least a partial influence on artists when deciding how to improve an image. Therefore, we incorporate global image attributes into our representation. Specifically, we incorporate six types of global characteristics proposed in [Bychkovsky et al. 2011], including intensity distribution, scene brightness, equalization curves, detail-weighted equalization curves, highlight clipping, and spatial distribution, which together result in a 207-dimensional vector."}, {"heading": "4.3 Contextual Features", "text": "In fact, it is such that it is a matter of a way in which people move in the different regions of the world, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, they, they, in which they, in which they, in which they live, in which they, they, in which they, in which they, they, they, in which, they, they, in which, they, they, they, they, they, they, they, they, she, she, she, she, she, they, she, she, she, she, she, they, they, she, she, they, she, she, they, she, they, they, she, they, they, she, they, they, they, they, they, she, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they, they,"}, {"heading": "5. TRAINING DATA SAMPLING AND SELECTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Superpixel Based Sampling", "text": "When we train a mapping function on a set of images, we prefer not to use all of the pixels, as such dense scanning would result in unbalanced training data. For example, we may have too many pixels from large \"celestial regions,\" while relatively few from smaller \"person regions,\" which could ultimately lead to a serious bias in the trained mapping function. In addition, too dense scanning needlessly increases training costs, as we have to process millions of pixel samples. Therefore, we use a superpixel-based method to collect training samples. Note that a superpixel in a smooth region can be larger than a superpixel in a region with more high-frequency details. We require that the color transformation generated by our mapping function in the place of a superpixel be called a superpixel. Note that a superpixel in a smooth region can be larger than a superpixel in a region with more high-frequency details."}, {"heading": "5.2 Cross-Entropy Based Image Selection", "text": "For example-based image editing, sample images that have a specific image editing style often need to be created manually by human artists. It is a labor-intensive task to adapt many images, since each image has several attributes and regions that can be adapted. Therefore, it is highly desirable to select a small number of representative training images to reduce the workload for humans. On the other hand, to achieve a learned model with strong predictive power, it is necessary that the selected training images have a reasonable coverage of the play space. In this section, we present an entropy-based scheme for selecting a subset of representative training images from a large collection. First, we learn a codebook of feature film descriptors with K = 400 code words by running K-medium clusters on feature film descriptors that are collected from all training images. Then, each original algorithm 1: Small Training Selection Selection Input: A large collection of images, with a desired number of I: 1, 1)."}, {"heading": "6 end", "text": "The value in a histogram recycle bin is equal to the number of times the corresponding codeword appears in the image. Let Hk be the histogram for the image Ik. For each subset of images coming from an initial image collection, we calculate the cumulative histogram H\u0440 by simply summing up the individual histograms of the images in the image. We also evaluate the representative force of using the cross-entropy of the image Ik. That is, entropy (HB) = \u2212 j HB (j) logHB (j) logHB (j), where HB (j) implies the j \u2212 th element of HB (j)."}, {"heading": "6. OVERVIEW OF EXPERIMENTS", "text": "Our proposed method is well suited for learning complex and highly non-linear photo enhancement styles, especially when the style requires a challenge of spatially different local enhancements. Successful local enhancement styles can rely not only on the content in a particular local region, but also on content in its surroundings. In this sense, such operations could easily lead to complex effects that require stylistic or even exaggerated color transformations, making previous global methods (e.g. [Bychkovsky et al. 2011]) and local empirical methods (e.g. [Kaufman et al. 2012]) inapplicable. In contrast, our method was developed to address such challenges using powerful contextual features and the strong regression capability of deep neural networks. To fully evaluate our method, we hired a professional photographer who carefully retouched three different stylistic local effects by evaluating hundreds of photos we used in Section 7 of our experiments to evaluate the performance used."}, {"heading": "6.1 Experimental Setup", "text": "The number of neurons in the hidden layers has been empirically set at 192, and the number of neurons in the output layer has been set at the number of coefficients in the predicted color transformation. Our experiments have confirmed that square color transformations can render the colors in customized images more faithfully than affine color transformations. Therefore, there are 30 neurons in the output layer, 10 for each of the three color channels. Data sampling. Because we learn color mappings at the pixel level, each pixel within the image is a potential image sample. In practice, we segment each image into about 7,000 superpixels, of which we randomly select 10 superpixels. Therefore, even if we only have 70 image pairs for learning a particular local effect, we can segment a neural image into about 7,000 superpixels, of which we select each individual pixel as pixels."}, {"heading": "7. LEARNING LOCAL ADJUSTMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Three Stylistic Local Effects", "text": "This year we have reached the point where we are able to live in a country where it is a country, where it is a country where it is not a country, but a country where it is a country."}, {"heading": "7.2 Spatially Varying Color Mappings", "text": "To verify this, we collect pixels from each semantic region of an image. By drawing scatter diagrams for different semantic regions using pairs of pixel colors from input and retouched images, we are able to visualize the spatially different color transformations. See one such example in Figure 9, which clearly shows that color transformations differ in the sky, in the building, in the grass, and on the street. Furthermore, we see that our method can successfully learn such spatially varying complex color transformations. Furthermore, we performed a comparison with [Wang et al. 2011], which takes a local approach of piecemeal approximation. However, due to the lack of discriminatory contextual features, their learned adaptation parameters tend to be similar in different regions (Figure 8)."}, {"heading": "7.3 Generalization Capability", "text": "Here we test the generalizability of the DNN-based photo adaptation models we train using 70 pairs of images. As already mentioned, the actual number of training samples far exceeds the number of training image pairs because we use thousands of superpixels within each training image pair. As shown in Fig. 10, we apply our trained models to novel test images with significant visual differences from any images in the training set. The visual objects in these images either have a unique appearance or a unique spatial configuration. In Fig. 10top, the mountain in the input image has an appearance and a spatial arrangement that differs from the training images. In Fig. 10bottom, the appearance and spatial configuration of the car and the people also differ significantly from those of the training images. Despite these differences, our trained DNN models are still able to plausibly adjust the input images."}, {"heading": "7.4 Effectiveness of Contextual Features", "text": "We show the importance of contextual characteristics in learning local adjustments in this subsection. First, we calculate the L2 distance in the 3D-CIELab color space between input images and the truth of the soil that the photographer has shown for all local effect datasets as shown in the second column of Table I. They numerically reflect the magnitude of adjustments the photographer has made to the input images. Second, we numerically compare the test errors of our extended results with and without the contextual feature in the third and fourth columns of Table I. Our experiments show that the test errors of our extended results are relatively high. The mean L2 error in the 3D-CIELab color space is 9.27, 9.51, and 9.61 for the formground pop-out, local Xpro and watercolor effects, respectively. On the other hand, by including our proposed contextual feature, our experiments show that all errors are significantly multidimensional to 7.08, and 7.71, we have the need to combine the multidimensional characteristics with 7.20."}, {"heading": "7.5 Effectiveness of Learning Color Transforms", "text": "As shown in Figure 3, the use of color transformations helps to absorb high frequency color variations and allows DNN to undo the spatially smooth but otherwise highly nonlinear part of the color mapping. To emphasize the benefits of using color transformations, we train another DNN to directly undo the retouched colors. DNN architecture is similar to the one described in Section 6.1, except that there are only 3 neurons in the output layer representing the enhanced CIELab color. We compare test errors L2 on the Foregronud pop-out and Local Xpro datasets in Table II. For both datasets, the test error increases by more than 55%, indicating the use of color transformations, which is advantageous for our task."}, {"heading": "7.6 DNN Architecture", "text": "The complexity of our DNN-based model is primarily determined by the number of hidden layers and the number of neurons in each layer. Note that the complexity of the DNN architecture should be equal to the inherent complexity of the learning task. If the DNN does not have the complexity to complete the given task, the trained model would not even be able to learn all the samples in the study group. On the other hand, if the complexity of the DNN exceeds the inherent complexity of the given task, there is a risk of revision and the trained model would not be able to generalize well to novel test data, although it could make the training error very small. The nature of the learning task in this paper is a regression problem. It has been shown that a feeding neural network with a single hidden layer [Hornik et al. 1989] adjusts this layer as a universal regressor and the necessary number of neurons in the hidden layer."}, {"heading": "7.7 Comparison with Other Regression Methods", "text": "It is also of great interest to evaluate the performance of other regressors on our datasets. Specifically, we have decided to compare DNN with two popular regression methods: Lasso [Tibshirani 1996] and Random Forest [Breiman 2001]. Both Lasso and Random Forest are scalable to the large number of training samples used in DNN training. We use Lasso and Random Forest to regresse CIELab target colors directly using the same functional set as in DNN training, including pixel-wise features, global features and contextual features. Hyperparameters of both the lasso and the Random Forest are matched by cross-validation. A comparison of L2 errors is summarized in Table III. DNN significantly outperforms lasso in all three local effect datasets and receives lower test errors than the Random Forest on both sides."}, {"heading": "8. LEARNING GLOBAL ADJUSTMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 MIT-Adobe FiveK Dataset", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. (...) It is not so that people are able to determine for themselves what they want. (...) It is not so that they want it. (...) It is not so that they want it. (...) It is not so that they want it. (...) It is not so that they want it. (...) It is so. (...) It is so. (...) It is so that they do not want it. (...) It is so that they do not want it. (...). (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (.). (. (.). (.). (.). (. (.).). (.). (. (.). (.). (.). (.). (. (.). (.). (. (.). (.). (.). (. (.). (. (.).). (. (.). (.). (. (.). (. (.).). (. (.). (.). (.). (. (.). (.). (. (.). (.). (.). (.). (. (. (.).). (.). (. (.). (.). (.).). (. (. (.).). (.). (. (.).). (.). (.). (.).). (. (.).). (. (. (.). (.). (.).).). (.).). (. (.)."}, {"heading": "8.2 Instagram Dataset", "text": "Instagram has become one of the most popular apps on mobile phones. On Instagram, hundreds of filters can be applied to create different artistic color and sound effects; for example, the commonly used \"lo-fi\" filter enhances contrast and creates warm tones; the \"rise\" filter adds a golden glow while \"Hudson\" creates a cool light.ACM transactions on Graphics, Vol. VV, No. N, Article XXX, Release Date: Month YYYYY. For each specific effect, we randomly selected 50 images from MITAdobe FiveK and had Instagram enhance each of them. Of the resulting 50 pairs of images, half were used for training and the other half for testing. We checked whether images that were adjusted by the trained color mapping features resemble the truth produced by Instagram, which has the flavor of a reverse engineering task. Our experiments suggest that Instagram effects with our method are relatively easy to learn for two learning effects."}, {"heading": "8.3 User Studies", "text": "To make a visual comparison between our results and those of [Hwang et al. 2012] in an objective way, we collected all images from the two sets of data, \"Random 250\" and \"High Variance 50,\" and randomly selected 50, including 10 indoor and 40 outdoor images used in our user study. For each of these 50 test images, we also collected the basic truth images and the enhanced images produced with our method and [Hwang et al. 2012]. Then, we invited 33 participants, including 12 women and 21 men, aged 21 to 28. These participants had little experience using professional photo-adjustment tools, but had experience with photo-enhancement apps such as \"Instagram et al. 2012.\" The experiment was conducted by asking each participant to open a static website with a prepared computer and a 24-inch monitor with a resolution of H20x1080. For each test image, we first show the input and the basic image pair to let the participants know."}, {"heading": "9. CONCLUSIONS AND DISCUSSIONS", "text": "In this paper, we have demonstrated the effectiveness of deep learning in automatic photo adjustment. We are throwing this problem into a highly nonlinear map function by treating the bundled features as the entry layer of a deep neural network. Pooled features include a pixel descriptor, a global descriptor, and a novel contextual descriptor based on the surface of scenarios. We have conducted extensive experiments on a number of effects, including conventional and artistic. Our experiments show that the proposed approach is capable of learning computational models for automatic spatial adjustments."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Vladimir Bychkovsky and Sung Ju Hwang for fruitful discussions and suggestions. This work was partially supported by the Hong Kong Research Grants Council under General Research Funds (HKU17209714)."}], "references": [{"title": "Appprop: all-pairs appearance-space edit propagation", "author": ["X. AN", "F. PELLACINI"], "venue": "ACM Trans. Graph. 27, 3.", "citeRegEx": "AN and PELLACINI,? 2008", "shortCiteRegEx": "AN and PELLACINI", "year": 2008}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. ARBELAEZ", "M. MAIRE", "C. FOWLKES", "J. MALIK"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 5, 898\u2013916.", "citeRegEx": "ARBELAEZ et al\\.,? 2011", "shortCiteRegEx": "ARBELAEZ et al\\.", "year": 2011}, {"title": "Two-scale tone management for photographic look", "author": ["S. BAE", "S. PARIS", "F. DURAND"], "venue": "ACM Trans. Graph. 25, 3, 637\u2013645.", "citeRegEx": "BAE et al\\.,? 2006", "shortCiteRegEx": "BAE et al\\.", "year": 2006}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. BELONGIE", "J. MALIK", "J. PUZICHA"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 24, 4 (Apr.), 509\u2013522.", "citeRegEx": "BELONGIE et al\\.,? 2002", "shortCiteRegEx": "BELONGIE et al\\.", "year": 2002}, {"title": "Random forests", "author": ["L. BREIMAN"], "venue": "Machine learning 45, 1, 5\u201332.", "citeRegEx": "BREIMAN,? 2001", "shortCiteRegEx": "BREIMAN", "year": 2001}, {"title": "Learning photographic global tonal adjustment with a database of input/output image pairs", "author": ["V. BYCHKOVSKY", "S. PARIS", "E. CHAN", "F. DURAND"], "venue": "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition. CVPR \u201911. 97\u2013104.", "citeRegEx": "BYCHKOVSKY et al\\.,? 2011", "shortCiteRegEx": "BYCHKOVSKY et al\\.", "year": 2011}, {"title": "Collaborative personalization of image enhancement", "author": ["J. CAICEDO", "A. KAPOOR", "S.B. KANG"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. 249\u2013256.", "citeRegEx": "CAICEDO et al\\.,? 2011", "shortCiteRegEx": "CAICEDO et al\\.", "year": 2011}, {"title": "Color harmonization", "author": ["D. COHEN-OR", "O. SORKINE", "R. GAL", "T. LEYVAND", "XU", "Y.-Q."], "venue": "ACM Trans. Graph. 25, 3 (jul), 624\u2013630.", "citeRegEx": "COHEN.OR et al\\.,? 2006", "shortCiteRegEx": "COHEN.OR et al\\.", "year": 2006}, {"title": "Image restoration using online photo collections", "author": ["K. DALE", "M. JOHNSON", "K. SUNKAVALLI", "W. MATUSIK", "H. PFISTER"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. 2217\u20132224.", "citeRegEx": "DALE et al\\.,? 2009", "shortCiteRegEx": "DALE et al\\.", "year": 2009}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. FELZENSZWALB", "D. MCALLESTER", "D. RAMANAN"], "venue": "IEEE Conf. on Computer Vision and Pattern Recognition.", "citeRegEx": "FELZENSZWALB et al\\.,? 2008", "shortCiteRegEx": "FELZENSZWALB et al\\.", "year": 2008}, {"title": "Efficient graphbased image segmentation", "author": ["P.F. FELZENSZWALB", "D.P. HUTTENLOCHER"], "venue": "Int. J. Comput. Vision 59, 2 (Sept.), 167\u2013181.", "citeRegEx": "FELZENSZWALB and HUTTENLOCHER,? 2004", "shortCiteRegEx": "FELZENSZWALB and HUTTENLOCHER", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. HINTON", "N. SRIVASTAVA", "A. KRIZHEVSKY", "I. SUTSKEVER", "R. SALAKHUTDINOV"], "venue": "CoRR abs/1207.0580.", "citeRegEx": "HINTON et al\\.,? 2012", "shortCiteRegEx": "HINTON et al\\.", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. HORNIK", "M. STINCHCOMBE", "H. WHITE"], "venue": "Neural Netw. 2, 5 (July), 359\u2013366.", "citeRegEx": "HORNIK et al\\.,? 1989", "shortCiteRegEx": "HORNIK et al\\.", "year": 1989}, {"title": "Context-based automatic local image enhancement", "author": ["S.J. HWANG", "A. KAPOOR", "S.B. KANG"], "venue": "Proceedings of the 12th European Conference on Computer Vision - Volume Part I. ECCV\u201912. 569\u2013582.", "citeRegEx": "HWANG et al\\.,? 2012", "shortCiteRegEx": "HWANG et al\\.", "year": 2012}, {"title": "Personal photo enhancement using example images", "author": ["N. JOSHI", "W. MATUSIK", "E.H. ADELSON", "D.J. KRIEGMAN"], "venue": "ACM Trans. Graph. 29, 2 (Apr.), 12:1\u201312:15.", "citeRegEx": "JOSHI et al\\.,? 2010", "shortCiteRegEx": "JOSHI et al\\.", "year": 2010}, {"title": "Personalization of image enhancement", "author": ["S.B. KANG", "A. KAPOOR", "D. LISCHINSKI"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. 1799\u20131806.", "citeRegEx": "KANG et al\\.,? 2010", "shortCiteRegEx": "KANG et al\\.", "year": 2010}, {"title": "Content-aware automatic photo enhancement", "author": ["L. KAUFMAN", "D. LISCHINSKI", "M. WERMAN"], "venue": "Comp. Graph. Forum 31, 8 (Dec.), 2528\u2013 2540.", "citeRegEx": "KAUFMAN et al\\.,? 2012", "shortCiteRegEx": "KAUFMAN et al\\.", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds. 1106\u20131114.", "citeRegEx": "KRIZHEVSKY et al\\.,? 2012", "shortCiteRegEx": "KRIZHEVSKY et al\\.", "year": 2012}, {"title": "Interactive local adjustment of tonal values", "author": ["D. LISCHINSKI", "Z. FARBMAN", "M. UYTTENDAELE", "R. SZELISKI"], "venue": "ACM Trans. Graph. 25, 3, 646\u2013653.", "citeRegEx": "LISCHINSKI et al\\.,? 2006", "shortCiteRegEx": "LISCHINSKI et al\\.", "year": 2006}, {"title": "Nonparametric scene parsing via label transfer", "author": ["C. LIU", "J. YUEN", "A. TORRALBA"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 12, 2368?382.", "citeRegEx": "LIU et al\\.,? 2011", "shortCiteRegEx": "LIU et al\\.", "year": 2011}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ERHAN", "I. GOODFELLOW", "R. FERGUS"], "venue": "arXiv preprint arXiv:1312.6199.", "citeRegEx": "SZEGEDY et al\\.,? 2013", "shortCiteRegEx": "SZEGEDY et al\\.", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. TIBSHIRANI"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), 267\u2013 288.", "citeRegEx": "TIBSHIRANI,? 1996", "shortCiteRegEx": "TIBSHIRANI", "year": 1996}, {"title": "Superparsing: Scalable nonparametric image parsing with superpixels", "author": ["J. TIGHE", "S. LAZEBNIK"], "venue": "Proceedings of the 11th European Conference on Computer Vision: Part V. ECCV\u201910. 352\u2013365.", "citeRegEx": "TIGHE and LAZEBNIK,? 2010", "shortCiteRegEx": "TIGHE and LAZEBNIK", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. VINCENT", "H. LAROCHELLE", "Y. BENGIO", "MANZAGOL", "P.-A."], "venue": "International Conference on Machine Learning. 1096\u20131103.", "citeRegEx": "VINCENT et al\\.,? 2008", "shortCiteRegEx": "VINCENT et al\\.", "year": 2008}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. VIOLA", "M. JONES"], "venue": "IEEE Conf. on Computer Vision and Pattern Recognition.", "citeRegEx": "VIOLA and JONES,? 2001", "shortCiteRegEx": "VIOLA and JONES", "year": 2001}, {"title": "Example-based image color and tone style enhancement", "author": ["B. WANG", "Y. YU", "XU", "Y.-Q."], "venue": "ACM SIGGRAPH 2011 Papers. SIGGRAPH \u201911. 64:1\u201364:12.", "citeRegEx": "WANG et al\\.,? 2011", "shortCiteRegEx": "WANG et al\\.", "year": 2011}, {"title": "Regionlets for generic object detection", "author": ["WANG X.", "YANG M.", "ZHU S.", "LIN", "Y."], "venue": "ICCV\u201913: Proc. IEEE 14th International Conf. on Computer Vision.", "citeRegEx": "X. et al\\.,? 2013", "shortCiteRegEx": "X. et al\\.", "year": 2013}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. ZEILER", "R. FERGUS"], "venue": "arXiv preprint arXiv:1311.2901.", "citeRegEx": "ZEILER and FERGUS,? 2013", "shortCiteRegEx": "ZEILER and FERGUS", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": ", [Vincent et al. 2008; Krizhevsky et al. 2012].", "startOffset": 2, "endOffset": 47}, {"referenceID": 17, "context": ", [Vincent et al. 2008; Krizhevsky et al. 2012].", "startOffset": 2, "endOffset": 47}, {"referenceID": 12, "context": "A deep neural network (DNN) is a universal approximator that can represent arbitrarily complex continuous functions [Hornik et al. 1989].", "startOffset": 116, "endOffset": 136}, {"referenceID": 18, "context": "In addition to these tools, there exists much research on either interactive [Lischinski et al. 2006; An and Pellacini 2008] or automatic [Bae et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 0, "context": "In addition to these tools, there exists much research on either interactive [Lischinski et al. 2006; An and Pellacini 2008] or automatic [Bae et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 2, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment.", "startOffset": 42, "endOffset": 81}, {"referenceID": 7, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment.", "startOffset": 42, "endOffset": 81}, {"referenceID": 0, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment. Automatic methods typically operate on the entire image in a global manner without taking image content into consideration. To address this issue, Kaufman et al. [2012] introduces an automatic method that first detects semantic content, including faces, sky as well as shadowed salient regions, and then applies a sequence of empirically determined steps for saturation, contrast as well as exposure adjustment.", "startOffset": 6, "endOffset": 278}, {"referenceID": 0, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment. Automatic methods typically operate on the entire image in a global manner without taking image content into consideration. To address this issue, Kaufman et al. [2012] introduces an automatic method that first detects semantic content, including faces, sky as well as shadowed salient regions, and then applies a sequence of empirically determined steps for saturation, contrast as well as exposure adjustment. However, the limit of this approach is that output style is hard-coded in the algorithm and cannot be easily tuned to achieve a desired style. In comparison and as we shall see, our data-driven approach can easily be trained to produce a variety of styles. Further, these techniques rely on a fixed pipeline that is inherently limited in its ability to achieve user-preferred artistic enhancement effects, especially the exaggerated and dramatic ones. In practice, a fixedpipeline technique works well for a certain class of adjustments and only produces approximate results for effects outside this class. For instance, Bae et al. [2006] do well with tonal global transforms but", "startOffset": 6, "endOffset": 1160}, {"referenceID": 15, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 14, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 6, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 5, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 8, "context": "2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention.", "startOffset": 28, "endOffset": 46}, {"referenceID": 10, "context": "do not model local edits, and Kaufman et al. [2012] perform well on a predetermined set of semantic categories but does not handle elements outside this set.", "startOffset": 30, "endOffset": 52}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment.", "startOffset": 6, "endOffset": 158}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment. Bychkovsky et al. [2011] introduces a method based on Gaussian processes for learning tone mappings according to global image statistics.", "startOffset": 6, "endOffset": 356}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment. Bychkovsky et al. [2011] introduces a method based on Gaussian processes for learning tone mappings according to global image statistics. Since these methods were designed for global image adjustment, they do not consider local image contexts and cannot produce spatially varying local enhancements. Wang et al. [2011] proposes a method based on piecewise approximation for learning color mapping functions from exemplars.", "startOffset": 6, "endOffset": 650}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment. Bychkovsky et al. [2011] introduces a method based on Gaussian processes for learning tone mappings according to global image statistics. Since these methods were designed for global image adjustment, they do not consider local image contexts and cannot produce spatially varying local enhancements. Wang et al. [2011] proposes a method based on piecewise approximation for learning color mapping functions from exemplars. It does not consider semantic or contextual information either. In addition, it is not fully automatic, and relies on interactive soft segmentation. It is infeasible for this technique to automatically enhance a collection of images. In comparison, this paper proposes a scalable framework for learning user-defined complex enhancement effects from exemplars. It explicitly performs generic image semantic analysis, and its image enhancement models are trained using feature descriptors constructed from semantic analysis results. Hwang et al. [2012] proposes a context-aware local image enhancement technique.", "startOffset": 6, "endOffset": 1305}, {"referenceID": 12, "context": "Multi-layer deep neural networks have proven to be able to represent arbitrarily complex continuous functions [Hornik et al. 1989].", "startOffset": 110, "endOffset": 130}, {"referenceID": 17, "context": "We choose the rectified linear unit (ReLU) [Krizhevsky et al. 2012], g(z) = max(0, z), as the activation function in our networks.", "startOffset": 43, "endOffset": 67}, {"referenceID": 17, "context": "In addition, we apply the Dropout training strategy [Krizhevsky et al. 2012; Hinton et al. 2012], which has been shown very useful for improving the generalization capability.", "startOffset": 52, "endOffset": 96}, {"referenceID": 11, "context": "In addition, we apply the Dropout training strategy [Krizhevsky et al. 2012; Hinton et al. 2012], which has been shown very useful for improving the generalization capability.", "startOffset": 52, "endOffset": 96}, {"referenceID": 5, "context": "Specifically, we adopt six types of global features proposed in [Bychkovsky et al. 2011], including intensity distribution, scene brightness, equalization curves, detail-weighted equalization curves, highlight clipping, and spatial distribution, which altogether give rise to a 207-dimensional vector.", "startOffset": 64, "endOffset": 88}, {"referenceID": 22, "context": "Typical image semantic analysis algorithms include scene parsing [Tighe and Lazebnik 2010; Liu et al. 2011] and object detection [Viola and Jones 2001; Felzenszwalb et al.", "startOffset": 65, "endOffset": 107}, {"referenceID": 19, "context": "Typical image semantic analysis algorithms include scene parsing [Tighe and Lazebnik 2010; Liu et al. 2011] and object detection [Viola and Jones 2001; Felzenszwalb et al.", "startOffset": 65, "endOffset": 107}, {"referenceID": 24, "context": "2011] and object detection [Viola and Jones 2001; Felzenszwalb et al. 2008; Wang et al. 2013].", "startOffset": 27, "endOffset": 93}, {"referenceID": 9, "context": "2011] and object detection [Viola and Jones 2001; Felzenszwalb et al. 2008; Wang et al. 2013].", "startOffset": 27, "endOffset": 93}, {"referenceID": 22, "context": "During pixel annotation, we perform scene parsing using the state-of-theart algorithm in [Tighe and Lazebnik 2010].", "startOffset": 89, "endOffset": 114}, {"referenceID": 1, "context": "In our experiments, we adopt the image segmentation algorithm in [Arbelaez et al. 2011].", "startOffset": 65, "endOffset": 87}, {"referenceID": 3, "context": "Our multiscale context descriptor is partially inspired by shape contexts [Belongie et al. 2002].", "startOffset": 74, "endOffset": 96}, {"referenceID": 24, "context": "However, unlike the shape context descriptor, our regions and subregions are either rectangles or squares, which facilitate fast histogram computation based on integral images (originally called summed area tables) [Viola and Jones 2001].", "startOffset": 215, "endOffset": 237}, {"referenceID": 10, "context": "For each training image I , we first apply the graph-based segmentation [Felzenszwalb and Huttenlocher 2004] to divide the image into small homogeneous yet irregularly shaped patches, each of which is called a superpixel.", "startOffset": 72, "endOffset": 108}, {"referenceID": 5, "context": ", [Bychkovsky et al. 2011]) and local empirical methods (e.", "startOffset": 2, "endOffset": 26}, {"referenceID": 16, "context": ", [Kaufman et al. 2012]) inapplicable.", "startOffset": 2, "endOffset": 23}, {"referenceID": 25, "context": "We further conducted a comparison against [Wang et al. 2011], which adopts a local piecewise approximation approach.", "startOffset": 42, "endOffset": 60}, {"referenceID": 25, "context": "Comparison with [Wang et al. 2011] on the Local Xpro effect.", "startOffset": 16, "endOffset": 34}, {"referenceID": 25, "context": "Top Left: Input image; Top Right: enhanced image by [Wang et al. 2011]; Bottom Left: enhanced image by our approach ;Bottom Right:enhanced image by photographer.", "startOffset": 52, "endOffset": 70}, {"referenceID": 12, "context": "It has been shown that a feedforward neural network with a single hidden layer [Hornik et al. 1989] can be used as a universal regressor and the necessary number of neurons in the hidden layer varies with the inherent complexity of the given regression problem.", "startOffset": 79, "endOffset": 99}, {"referenceID": 21, "context": "Specifically, we chose to compare DNN against two popular regression methods, Lasso [Tibshirani 1996] and random forest [Breiman 2001].", "startOffset": 84, "endOffset": 101}, {"referenceID": 4, "context": "Specifically, we chose to compare DNN against two popular regression methods, Lasso [Tibshirani 1996] and random forest [Breiman 2001].", "startOffset": 120, "endOffset": 134}, {"referenceID": 5, "context": "The MIT-Adobe FiveK dataset [Bychkovsky et al. 2011] contains 5000 raw images, each of which was retouched by five well trained photographers, which results in five groups of global adjustment styles.", "startOffset": 28, "endOffset": 52}, {"referenceID": 13, "context": "We have compared our method with [Hwang et al. 2012] using the same experimental settings and testing datasets in that work.", "startOffset": 33, "endOffset": 52}, {"referenceID": 13, "context": "Two testing datasets were used in [Hwang et al. 2012].", "startOffset": 34, "endOffset": 53}, {"referenceID": 5, "context": "[Bychkovsky et al. 2011] 5.", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "[Hwang et al. 2012] N/A 15.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Figure 14 further shows the error histograms of our method and [Hwang et al. 2012] on these two testing datasets.", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "The technique in [Hwang et al. 2012] is based on nearest-neighbor search, which requires a fairly large training set that is slow to search.", "startOffset": 17, "endOffset": 36}, {"referenceID": 13, "context": "Thus our method has a stronger extrapolation capability than the nearest-neighbor based approach in [Hwang et al. 2012], which only exploits a limited number of nearest neighbors.", "startOffset": 100, "endOffset": 119}, {"referenceID": 13, "context": "For the same reason, the nearest-neighbor based approach in [Hwang et al. 2012] is also more sensitive to noisy and inconsistent adjustments in the training data.", "startOffset": 60, "endOffset": 79}, {"referenceID": 5, "context": "In another comparison with [Bychkovsky et al. 2011], we follow the same setting used in that work, which experimented on 2500 training images from group C and reported the mean error on the L channel (CIELAB color space) only.", "startOffset": 27, "endOffset": 51}, {"referenceID": 5, "context": "To validate the effectiveness of our cross-entropy based training set selection method (Algorithm 1), we have monitored the testing errors by varying the number of training images selected by our method, and compared them with both naive random selection and the sensor placement method used in [Bychkovsky et al. 2011] (Figure 16).", "startOffset": 295, "endOffset": 319}, {"referenceID": 13, "context": "Input Image Ground Truth Our Result [Hwang et al. 2012]", "startOffset": 36, "endOffset": 55}, {"referenceID": 13, "context": "Visual comparison with [Hwang et al. 2012].", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "Left: Input image; Middle Left: groundtruth enhanced image by expert C; Middle Right: enhanced image by our approach; Right: enhanced image by [Hwang et al. 2012].", "startOffset": 143, "endOffset": 162}, {"referenceID": 13, "context": "To perform a visual comparison between our results and those produced by [Hwang et al. 2012] in an objective way, we collected all the images from the two datasets, \u201cRandom 250\u201d and \u201cHigh variance 50\u201d, and randomly chose 50, including 10 indoor images and 40 outdoor images, to be used in our user study.", "startOffset": 73, "endOffset": 92}, {"referenceID": 13, "context": "For each of these 50 testing images, we also collected the groundtruth images and the enhanced images produced with our method and [Hwang et al. 2012].", "startOffset": 131, "endOffset": 150}, {"referenceID": 13, "context": "This comparison indicates that, from a visual perspective, our method can produce much better enhanced images than [Hwang et al. 2012].", "startOffset": 115, "endOffset": 134}, {"referenceID": 13, "context": "A comparison of user voting results between our approach and [Hwang et al. 2012]", "startOffset": 61, "endOffset": 80}, {"referenceID": 27, "context": "In fact, interpreting the internal representations of deep neural networks is still an ongoing research topic [Zeiler and Fergus 2013; Szegedy et al. 2013].", "startOffset": 110, "endOffset": 155}, {"referenceID": 20, "context": "In fact, interpreting the internal representations of deep neural networks is still an ongoing research topic [Zeiler and Fergus 2013; Szegedy et al. 2013].", "startOffset": 110, "endOffset": 155}], "year": 2014, "abstractText": "Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work.", "creator": "LaTeX with hyperref package"}}}