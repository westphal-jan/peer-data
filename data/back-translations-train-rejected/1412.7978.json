{"id": "1412.7978", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "The Computational Theory of Intelligence: Information Entropy", "abstract": "This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications.", "histories": [["v1", "Wed, 24 Dec 2014 07:41:45 GMT  (474kb,D)", "http://arxiv.org/abs/1412.7978v1", "Published atthis http URL"]], "COMMENTS": "Published atthis http URL", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["daniel kovach"], "accepted": false, "id": "1412.7978"}, "pdf": {"name": "1412.7978.pdf", "metadata": {"source": "CRF", "title": "The Computational Theory of Intelligence: Information Entropy", "authors": ["Daniel Kovach"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This approach takes into account data from psychology, neurology, artificial intelligence, machine learning and mathematics. A central part of this framework is the fact that the goal of any intelligent agent is to reduce randomness in its environment in a meaningful way. Of course, formal definitions in the context of this paper will follow for terms such as \"intelligence,\" \"environment\" and \"agent.\" The approach is based on multidisciplinary research and has many applications. We will use the construct in discussions at the end of the paper. Further applications will follow in future work. Implementations of this framework can be applied to many areas of study, including general artificial intelligence (GAI), machine learning, optimization, information gathering, clusters and big data, and extend beyond the field of applied mathematics and computer science to even more areas such as sociology, psychology and neurology and even philosophy."}, {"heading": "1.1 Definitions", "text": "One cannot begin a discussion about the philosophy of artificial intelligence without defining the word \"intelligence\" in the first place. Given the panopoly of available definitions, it is understandable that there may be disagreements, but typically each school of thought shares a common Xiv: 141 2,79 78v1 [cs.AI] 24 Dec 2thread. Below are three different definitions of intelligence from respectable sources: 1. The entirety or global capacity of the individual to act purposefully, to think rationally, and to deal effectively with his or her environment [19]. 2. A process that entails a set of problem-solving skills - which enable the individual to solve real problems or difficulties he or she encounters, and, if appropriate, to create an effective product - and bring with it the potential for finding or creating problems - and thus form the basis for acquiring new knowledge [5].3 Targeted adaptive behavior [18].Vernon's 1950s intelligence archive model."}, {"heading": "1.2 Structure", "text": "The paper is structured as follows: In Section 2 we give a brief summary of the concept of information entropy as used for our purposes; in Section 3 we provide a mathematical framework for intelligence and show its relationship to entropy; Section 4 covers the global impact of local entropy minimization; and in Section 5 we present a simple application of the framework to data analysis, which is available for free download."}, {"heading": "2 Entropy", "text": "A key concept of information theory is entropy, which amounts to uncertainty in a given random variable. [8] It is essentially a measure of unpredictability (among other interpretations).The concept of entropy is a much deeper principle of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models [17, 15, 6]."}, {"heading": "2.1 Mathematical Representation", "text": "Although terms such as Shannon entropy are widely used in the field of information theory, it will be instructive to examine the formulation in our context. To arrive at the definition of entropy, we must first remember what is meant by information content. The information content of a random variable, X referred to as I [X] = log [1P [X] = \u2212 log [P [X]] (1), where P [X] is the probability of X. The entropy of X, E [X] is then defined as the expectation value of the information content. E [X] = E [I [X] = \u2212 E [log [P [X]]]] (2) Extended to include the definition of the expectation value, we have E [X] = \u2212 N \u2211 i = 1 P [xi] log [P [xi]]] (3), where {xi} is the set of possible values that X can assume."}, {"heading": "2.1.1 Relationship of Shannon to Thermodynamic Entropy", "text": "The concept of entropy is deeply rooted in the core of physical reality. It is a central concept in thermodynamics that governs everything from chemical reactions to engines and refrigerators. However, the relationship of entropy as known in information theory is not mapped so easily for its use in thermodynamics. In statistical thermodynamics, the entropy S of a system is given by S = \u2212 kb \u2211 pi ln [pi] (4), where pi denotes the probability of any micro-state or system configuration, and kb is the Boltzmann constant that serves to map the value of the summation to physical reality in quantity and units. The link between thermodynamic and information-theoretical versions of entropy is related to the information needed to depict the exact state of the system in detail, especially the amount of other Shannon information needed to define the roscopic state of the system if it remains unclear."}, {"heading": "2.1.2 Renyi Entropy", "text": "We can extend the logic of the beginning of this section to a more general formulation called renyi entropy of the order \u03b1, where \u03b1 \u2265 0 and \u03b1 6 = 1 asH\u03b1 (X) = 11 \u2212 \u03b1 log [N \u2211 i = 1 P [xi] \u03b1]. (5) Within the framework of this convention, we can apply the concept of entropy more generally to extend the benefits of the concept to a variety of applications. It is important to note that this formulation approximates 1 in the limit as \u03b1 \u2192 1. Although the discussions of this paper were inspired by Shannon entropy, we would like to present a much more general definition and a bolder proposal."}, {"heading": "3 Intelligence: Definitions and Assumptions", "text": "Consider the sets S and O and let me show the following figure I: S \u2192 O. The function I represent represents the intelligence process, a member of I, the set of all these functions. It constitutes the input of sentence S to O. Firstly, letIt [si] = oit (6) reflects the fact that I map an element of S to an element in O, each characterized by the identifier i-N, which is limited by the cardinality of the input set. The cardinality of these two propositions does not have to coincide, nor does the mapping between me have to be bijective or even surjective. This is an iterative process, as indicated by the index. Finally, Ot is supposed to represent the collection of Oit.Over time, the mapping should converge to the intended element, oi-O, as reflected in I [si], oi-oi, oi-oi, oi-oi, oi-function (7), which in practice is an introduit (O) function (we)."}, {"heading": "3.1 Unsupervised and Supervised Learning", "text": "If O = P (S) is the power set of S, then we will say that Mapping I is an unattended mapping. Otherwise, the mapping is monitored, and the effects of this distinction are as follows: In supervised learning, the agent receives two different sets and is trained to form a mapping between them. In unsupervised learning, the agent is instructed to learn subtle relationships in a single data set, or, more succinctly, to develop the mapping between S and its power set discussed above [9, 16]."}, {"heading": "3.2 Overtraining", "text": "Furthermore, it should be noted that satisfying the definitions and assumptions of this section just because we have a function I: S \u2192 O does not necessarily mean that this figure must be meaningful. Finally, we could make a completely arbitrary but consistent representation of the above recipe, and although this would satisfy all definitions and assumptions, it would be a complete imprinting by the agent. In fact, this is exactly the definition of over-training of a frequent trap in the training level of machine learning that must be avoided very carefully."}, {"heading": "3.3 Entropy Minimization", "text": "A final part of the framework remains, and that is to show that entropy is minimized, as explained at the beginning of this section. To show this, we consider myself a probable figure, where Pt [sij] = P [sij] = oj] (12) indicates the probability that I will map sij [S] to some oj [O]. From this, we can calculate entropy in the figure from S to O, for each iteration. If the projection I [si] N has possible results, then the Shannon entropy of eachsi [S] is logged by Eit [si] = \u2212 N = 1 Pt [sij], for each iteration. (13) If | S | = M, then the total entropy is simply the sum of Eit [S], i [S], i [S], 2,..., N [S] as minimized. But in the sense of standardization, it is useful to talk about different cardinalities."}, {"heading": "3.4 Entropic Self Organization", "text": "In Section 3 we talked about the definitions of intelligence via mapping I: S \u2192 O. Here we try to apply the concept of entropy minimization to P (S) itself and not to a mapping. Let us explicitly allow it: \u03c3-P (S) \u03c3 = {s-P (S)}, (21) where for each s-S there is a unique s-\u03c3, so that s-s. That is, each element of S has one and only one element of it that contains it."}, {"heading": "4 Global Effects", "text": "In nature, whenever a system is brought from a state of higher entropy to a state of lower entropy, a certain amount of energy is involved in this transition and an increase in the entropy of the rest of the environment is greater or equal to the loss of entropy [17]. In other words, let us consider a system S consisting of two subsystems, s1 and s2. ThenS = s1 + s2 (23) Now let us consider this system in equilibrium at times t = 1 and t = 2, referred to as S1 and S2. Due to the second law of thermodynamics. S2 \u2265 S1 (24) ands21 + s 2 \u2265 s11 + s12 (25) Now let us suppose that one of the subsystems, say s1, decreases by a certain amount in entropy, during the transition of time t = 2, i.e. s21 = s 1 \u2212 \u00b2 s Then what can be said of s12 is the entropy of the rest of the system by a certain amount higher total energy than the total energy amount, or the amount of the tendency is greater than the sum of the total energy taken in physics."}, {"heading": "5 Application", "text": "Here we apply the discussion of this paper to practical examples. First, we look at a simple example of unsupervised learning: a cluster algorithm based on minimizing Shannon entropy. Next, we look at a simple behavior of an intelligent agent acting in its environment to maximize global entropy."}, {"heading": "5.1 Clustering by Entropy Minimization", "text": "It is indeed the case that most of us will be able to abide by the rules we have set ourselves in order to make them a reality, \"he said in an interview with the Deutsche Presse-Agentur.\" It is not as if we are able to hide, \"he told the Deutsche Presse-Agentur.\" But it is not as if we are able to hide, \"he said.\" But it is not as if we are able to hide, as if we are able to hide. \""}, {"heading": "5.2 Global Entropy Maximization", "text": "In our next series of examples, we will consider a virtual agent limited to moving across a \"terrain\" represented by a three-dimensional surface, each given by one of the following two equations: z = exp [\u2212 (x2 + y2)] (27) and z = 14 exp [\u2212 (((x 10) 2 + (y 10) 2)) (cos [1 2 \u03c0y] + sin [1 2 \u03c0x] + 2). (28) We will limit x, y so that (x, y) ([xmin, xmax], [ymin, ymax]) 2) and note that the range of each surface is z [0, 1]. The algorithm proceeds as follows. First, the agent is initialized with a starting position, s = (x0, y0). It updates s by increasing or decreasing its coordinates by a small value."}, {"heading": "6 Related Work", "text": "Although there are many approaches to intelligence from the perspective of cognitive science, so far only a few have been proposed from the side of computation. Recently, however, some major work is underway in this area. Many sources claim that there are computational theories on intelligence, but these \"theories\" mostly serve only to describe certain aspects of intelligence [2]. Thus, Meyer suggests in [14] that performance in several tasks depends on adaptive executive control, but makes no claim to the origin of such characteristics. Others discuss how data is aggregated. This type of analysis is particularly relevant to computer vision and image recognition [13]. The efforts in this paper aim to introduce a much more comprehensive theory of the origin of autonomous, targeted behavior. Similar efforts are currently underway. Inspired by physics and cosmology, Wissner-Gross claims that autonomous agents act to maximize entropy in their environment."}, {"heading": "7 Conclusion", "text": "The purpose of this paper was to lay the foundations for a generalization of the concept of intelligence in the computational sense. We discussed how minimizing entropy can be used to facilitate the intelligence process, and how the differences between the prediction of the agent and the reality of the training set can be used to optimize the performance of the agents. We also showed how such a concept could be used to produce a meaningful, albeit simplified, practical demonstration. Some future work will include applying the principles of this paper to data analysis, especially in the presence of noise or sparse data. Some of these applications will be discussed in the next paper.Further future work will include discussing the underlying principles under which data can be collected hierarchically, discussing how computer-aided processes can implement the discussions in this paper to develop and collaborate processes of greater complexity, and the relevance of these contributions to abstract concepts, and how to discuss the following concepts of self-awareness and structures, such as x."}], "references": [{"title": "Assessment of Intellectual Functioning", "author": ["Aiken", "Lewis R"], "venue": "New York: Plenum,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Engineering of Mind: An Introduction to the Science of Intelligent Systems", "author": ["Albus", "James Sacra", "A. Meystel"], "venue": "New York: Wiley,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Frames of the Mind: The Theory of Multiple Intelligences", "author": ["Gardner", "Howard"], "venue": "New York: Basic,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "A Brief History of Time", "author": ["S.W. Hawking"], "venue": "New York: Bantam,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Information Theory for Continuous Systems", "author": ["Ihara", "Shunsuke"], "venue": "Singapore: World Scientific,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Artificial Intelligence: A Systems Approach", "author": ["Jones", "M. Tim"], "venue": "Hingham, MA: Infinity Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Entropy Based Clustering.", "author": ["Kovach", "Daniel J", "Jr."], "venue": "Kovach Technologies. N.p.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Entropy Clustering Sample Data", "author": ["Kovach", "Daniel J", "Jr."], "venue": "Kovach Technologies. N.p.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Entropy Based Clustering.", "author": ["Kovach", "Daniel J", "Jr."], "venue": "Kovach Technologies. N.p.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A Computational Theory of Human Stereo Vision.", "author": ["D. Marr", "T. Poggio"], "venue": "Proceedings of the Royal Society B: Biological Sciences", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1979}, {"title": "A Computational Theory of Executive Cognitive Processes and Multiple-task Performance: I. Basic Mechanisms.", "author": ["Meyer", "David E", "David E. Kieras"], "venue": "Psychological Review", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Cycles of Time: An Extraordinary New View of the Universe", "author": ["Penrose", "Roger"], "venue": "New York: Alfred A. Knopf,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Russell", "Stuart J", "Peter Norvig"], "venue": "Upper Saddle River, NJ: Prentice Hall/Pearson Education,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "An Introduction to Thermal Physics", "author": ["Schroeder", "Daniel V"], "venue": "San Francisco, CA: Addison Wesley,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Wechsler\u2019s Measurement and Appraisal of Adult Intelligence", "author": ["Wechsler", "David", "Joseph D. Matarazzo"], "venue": "New York: Oxford UP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1972}, {"title": "Causal Entropic Forces.", "author": ["Wissner-Gross", "Alexander", "Cameron Freer"], "venue": "Physical Review Letters", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "The aggregate or global capacity of the individual to act purposefully, to think rationally, and to deal effectively with his environment[19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": "A process that entails a set of skills of problem solving \u2014 enabling the individual to resolve genuine problems or difficulties that he or she encounters and, when appropriate, to create an effective product \u2014 and must also entail the potential for finding or creating problems \u2014 and thereby providing the foundation for the acquisition of new knowledge[5].", "startOffset": 353, "endOffset": 356}, {"referenceID": 0, "context": "Vernon\u2019s hierarchical model of intelligence from the 1950\u2019s [1], and Hawkins\u2019 On Intelligence in 2004 [7] are some other great resources on this topic.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "A key concept of information theory is that of entropy, which amounts to the uncertainty in a given random variable, [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 13, "context": "deeper principal of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models, [17, 15, 6].", "startOffset": 134, "endOffset": 145}, {"referenceID": 11, "context": "deeper principal of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models, [17, 15, 6].", "startOffset": 134, "endOffset": 145}, {"referenceID": 3, "context": "deeper principal of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models, [17, 15, 6].", "startOffset": 134, "endOffset": 145}, {"referenceID": 5, "context": "With unsupervised learning, the agent is tasked with learning subtle relationships in a single data set or, put more succinctly, to develop the mapping between S and its power set discussed above [9, 16].", "startOffset": 196, "endOffset": 203}, {"referenceID": 12, "context": "With unsupervised learning, the agent is tasked with learning subtle relationships in a single data set or, put more succinctly, to develop the mapping between S and its power set discussed above [9, 16].", "startOffset": 196, "endOffset": 203}, {"referenceID": 13, "context": "In nature, whenever a system is taken from a state of higher entropy to a state of lower entropy, there is always some amount of energy involved in this transition, and an increase in the entropy of the rest of the environment greater than or equal to that of the entropy loss[17].", "startOffset": 276, "endOffset": 280}, {"referenceID": 7, "context": "Take for example the data that can be found at [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "The clustering source code is freely available at [10].", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We will confine x, y such that (x, y) \u2208 ([xmin, xmax], [ymin, ymax]) and note that the range of each respective surface is z \u2208 [0, 1].", "startOffset": 127, "endOffset": 133}, {"referenceID": 8, "context": "To run the simulation and obtain the above data, simply download the source code freely available at [12] and enter: > chmod 777 h i l l c l i m b e r .", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Many sources claim to have computational theories of intelligence, but for the most part these \u201ctheories\u201d merely act to describe certain aspects of intelligence [2].", "startOffset": 161, "endOffset": 164}, {"referenceID": 10, "context": "For example, Meyer in [14] suggests that performance on multiple tasks is dependent on adaptive executive control, but makes no claim on the emergence of such characteristics.", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "This type of analysis is especially relevant in computer vision and image recognition [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Inspired by physics and cosmology, Wissner-Gross asserts autonomous agents act to maximize the entropy in their environment [20].", "startOffset": 124, "endOffset": 128}], "year": 2014, "abstractText": "This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications.", "creator": "TeX"}}}