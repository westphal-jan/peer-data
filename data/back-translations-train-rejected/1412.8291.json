{"id": "1412.8291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2014", "title": "Improving approximate RPCA with a k-sparsity prior", "abstract": "A process centric view of robust PCA (RPCA) allows its fast approximate implementation based on a special form o a deep neural network with weights shared across all layers. However, empirically this fast approximation to RPCA fails to find representations that are parsemonious. We resolve these bad local minima by relaxing the elementwise L1 and L2 priors and instead utilize a structure inducing k-sparsity prior. In a discriminative classification task the newly learned representations outperform these from the original approximate RPCA formulation significantly.", "histories": [["v1", "Mon, 29 Dec 2014 09:51:20 GMT  (442kb,D)", "http://arxiv.org/abs/1412.8291v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["maximilian karl", "christian osendorfer"], "accepted": false, "id": "1412.8291"}, "pdf": {"name": "1412.8291.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Christian Osendorfer"], "emails": ["karlma@in.tum.de", "osendorf@in.tum.de"], "sections": [{"heading": null, "text": "A process-centric view of the robust PCA (RPCA) enables its rapid approximate implementation based on a special form of deep neural network with weights divided at all levels. Empirically, however, this rapid approach to the RPCA does not find frugal representations. We solve these bad local minima by loosening the elementary L1 and L2 priors and instead using a structure that has previously generated k-sparsity. In a discriminatory classification task, the newly learned representations significantly exceed those of the original approximate RPCA formulation."}, {"heading": "1 INTRODUCTION", "text": "This paper evaluates an efficient implementation of the sparse encoding. Sparse encoding is an optimization problem, which involves finding a sparse representation of the given data. Not only is it difficult to find an association between the sparse code and the data, but the search for the perfect sparse code for a single data point is a single additional optimization procedure. This process is very time-consuming because you have to optimize two problems one after the other.?? The idea behind an efficient implementation of this sparse encoding problem comes from??. A gradient descent algorithm, which optimizes the sparse code with many iterations, is transformed into a neural network with very few layers, each of which represents an iteration of the gradient descent algorithm. This network is then trained with the same objective function that is used to create the gradient descent algorithm, resulting in an efficient version of the initial optimization."}, {"heading": "2 ROBUST PCA", "text": "The motivation behind the robust PCA is the decomposition of a large matrix into a low-rank matrix and a sparse matrix?. The sparse matrix is also called the outlier matrix, hence the name Robust PCA. This decomposition can be formulated as follows: M = L0 + S0 (1), where M is the large data matrix, L0 is the low-rank matrix, and S0 is the sparse outlier matrix. A similar problem is solved in the commonly used Principal Component Analysis (PCA). However, the normal PCA does not have an outlier matrix. Therefore, it tries to minimize the matrix below rank (L) \u2264 k, where only small disturbances are permitted. Individual large disturbances could make the low-rank matrix different from the real low-rank matrix. By introducing an outlier matrix, these corruptions could be eliminated by capturing the low-rank information.]"}, {"heading": "3 EFFICIENT SPARSE CODING", "text": "The efficient RPCA algorithm? uses the same objective function as the original RPCA formulation?: 1 2 2 2 2 3 X \u2212 DS \u2212 O \u00b2 2F \u00b2 2 (2 D \u00b2 2F \u00b2 2F \u00b2 2F) + 1 3 3 3 This objective function is transformed into a neural network by first deriving the proximal descendant iterations, which means calculating the gradient of the smooth part of the objective function would be S and O and calculating a proximal operator from the non-smooth part. The proximal operator is defined as follows??: p = Argmin u \u00b2 Rm1 2 2 2 2 3 3 \u00b2 u \u2212 z \u00b2 22 + p \u00b2 2), forming the non-smooth part of the objective function. The construction of the proximal descent of this objective function is as follows: p \u00b2 p \u00b2 = argmin \u00b2 n \u00b2 s \u00b2."}, {"heading": "4 INSTABILITIES", "text": "When evaluating this efficient RPCA network at the MNIST, several problems occurred: the objective function consisted of a reconstruction term, a splitter term, and an outlier term. Optimization of this RPCA network led to a reduction of this objective function. At the same time, the reconstruction error in this objective function increased, implying poor reconstruction of the sparse code. However, the output of the network contained every detail of the desired output. This was due to the fact that the network stored all information in the sparse matrix. Therefore, the sparse code was completely empty. Changing the parameters to stabilize this problem resulted in not sparse codes and a good reconstruction, which is also not desired. The problem lies in the regulator for the sparse code. Here, the l2 standard was used for the sparse code and the sparse outlier the l1 standard. Both only affect individual elements of the sparse code and the sparse code."}, {"heading": "5 K-SPARSE REGULARIZER", "text": "The k-sparse function selects the k-largest elements of an array and sets all other elements to zero. This makes it an ideal candidate for building a regularizer that applies only one l1 standard to some of these elements. The new standard is defined as follows: \u0432 S-kSparse (S, k \u0445) \u0445 1It is an l1 standard between the k-sparse operator applied to the sparse code and the sparse code itself. S is the sparse code and k \u0445 the parameter that regulates the number of non-zero elements. This regulator now protects all k-largest elements of the sparse code from the l1 standard."}, {"heading": "6 EFFICIENT K-SPARSE CODING", "text": "This allows a fixed amount of information to be stored in O. (This prevents the network from bundling all the information in the outlier matrix, leaving the sparse code blank.) To further improve the thrift of the sparse code, the standard has also been applied to S. (O, k). (O, k) The optimal parameters of RPCA may not be the perfect parameters for the sparing instance, but it has shown that this new setting is much more robust against variations in the parameters."}, {"heading": "7 EXPERIMENTS", "text": "We were only interested in the relative performance of the two algorithms. The efficient RPCA and the efficient k-sparse coding model were both trained unattended on this data set. To compare the quality of these different representations, the classification error was chosen. For each representation, a supervised logistic regressor was trained to classify the correct number type. Errors for this experiment are presented in 1. They represent the number of incorrectly classified digits in percent. The k-sparse coding model produces surprisingly lower errors compared to RPCA. This shows that k-sparse produces hidden representations that are better suited for classification using linear classifiers. The modification of the k parameter shows small changes in the classification errors and allows some finetuning errors. Very small values of the error rates can be stored in a high error rate."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "The classification quality of an efficient version of RPCA was presented, and an additional add-on was introduced to solve several problems encountered during the use of RPCA. This solution consists of changing the regulator from an l1 standard to a completely new predecessor using the k-sparse function. Due to the mathematical derivation of the network structure from the objective function, this new predecessor automatically tilts within the transfer function. Now, the sparse code has a much more sparse structure, but also the parameter decision has become much more stable. This new k-sparse coding model resulted in significantly fewer classification errors than the original efficient RPCA version. Future work will consist of testing this new k-sparse standard for regular sparse coding or non-negative matrix factorization as well. Another application could be to use it as a regulator for any other machine learning algorithm."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "A process centric view of robust PCA (RPCA) allows its fast approximate implementation based on a special form of a deep neural network with weights shared across all layers. However, empirically this fast approximation to RPCA fails to find representations that are parsemonious. We resolve these bad local minima by relaxing the elementwise L1 and L2 priors and instead utilize a structure inducing k-sparsity prior. In a discriminative classification task the newly learned representations outperform these from the original approximate RPCA formulation significantly.", "creator": "LaTeX with hyperref package"}}}