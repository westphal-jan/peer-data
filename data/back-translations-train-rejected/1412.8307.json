{"id": "1412.8307", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2014", "title": "Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the 'extreme learning machine' algorithm", "abstract": "Deep networks have inspired a renaissance in neural network use, and are becoming the default option for difficult tasks on large datasets. In this report we show that published deep network results on the MNIST handwritten digit dataset can straightforwardly be replicated (error rates below 1%, without use of any distortions) with shallow 'Extreme Learning Machine' (ELM) networks, with a very rapid training time (~10 minutes). When we used distortions of the training set we obtained error rates below 0.6%. To achieve this performance, we introduce several methods for enhancing ELM implementation, which individually and in combination can significantly improve performance, to the point where it is nearly indistinguishable from deep network performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random 'receptive field' sampling of the input ensures the input weight matrix is sparse, with about 90 percent of weights equal to zero, which is a potential advantage for hardware implementations. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST suggest that the ease of use and accuracy of ELM should cause it to be given greater consideration as an alternative to deep networks applied to more challenging datasets.", "histories": [["v1", "Mon, 29 Dec 2014 11:14:59 GMT  (1594kb)", "http://arxiv.org/abs/1412.8307v1", "Submitted; 13 pages, including 5 figures and 1 table"], ["v2", "Wed, 22 Jul 2015 08:28:03 GMT  (1121kb,D)", "http://arxiv.org/abs/1412.8307v2", "Accepted for publication; 9 pages of text, 6 figures and 1 table"]], "COMMENTS": "Submitted; 13 pages, including 5 figures and 1 table", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["mark d mcdonnell", "migel d tissera", "tony vladusich", "r\\'e van schaik", "jonathan tapson"], "accepted": false, "id": "1412.8307"}, "pdf": {"name": "1412.8307.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mark D. McDonnell", "Migel D. Tissera", "Andr\u00e9 van Schaik", "Jonathan Tapson"], "emails": [], "sections": [{"heading": null, "text": "In this report, we show that the published results matter in terms of the number of people who have topped the rankings over the past decade. (Dated: December 30, 2014) Deep Networks have inspired a renaissance in neural network usage, becoming the default option for difficult tasks on large datasets. In this report, we show that the published deep network results on the MNIST dataset dataset dataset dataset (handwritten digital dataset) we digital dataset dataset dataset dataset dataset dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset-dataset. (Dated: December 30, 2014) Deep Netscape networks have inspired a renaissance in neural network usage, and become the default option for difficult tasks on large datasets."}, {"heading": "A. The Extreme Learning Machine: Notation and Training", "text": "This year it is more than ever before."}, {"heading": "B. Computationally efficient methods for training an ELM: iterative methods for large training sets", "text": "This year it is more than ever before in the history of the city."}, {"heading": "II. FASTER AND MORE ACCURATE ELMS BY SHAPING THE INPUT WEIGHTS NON-RANDOMLY", "text": "In this context, it should be noted that this is a very complex and complex matter."}, {"heading": "A. Computed Input Weights for ELM", "text": "The CIW-ELM approach is motivated by taking into account the standard baking propagation algorithm [12]. A feature of the weight learning algorithms is that they operate by adding to the weights a certain proportion of the training samples or a linear sum or difference of the training samples. In other words, apart from a possible random initialization, the weights are forced to take final values drawn from a space defined as base vectors in relation to linear combinations of the input viewing data - see Figure 1. While it has not been argued for no reason that it is a strength of the ELM that it is not physically limited [6], the use of this basis as a limitation of the input weights will normalize the ELM network towards a conventional (baking propagation) solution. The CIW-ELM algorithm is as follows [4]: 1. For use in the following steps, only the matrix data are normalized by subtracting all the mean points and training dimensions."}, {"heading": "B. Constrained Weights for ELM", "text": "Recently, Zhu et al. [9] published a method for limiting the input weights of ELM to the set of difference vectors of class samples. The difference vectors of class samples are the set of vectors that connect samples of one class with samples of another class in the sample space - see Figure 1. In addition, a method is proposed to eliminate from this set the vectors of potentially overlapping spaces (effectively the shorter vectors) and reduce the use of nearly parallel vectors to make the weight space more uniform.The constrained ELM (C-ELM) algorithm we use is as follows and follows largely [9], but we found it unnecessary to implement the elimination of nearly parallel vectors: 1. Randomly, we select M unique pairs of training data so that (a) each pair comes from two different classes; (b) the vector length of the difference between the pairs is smaller than 3."}, {"heading": "C. Receptive Fields for ELM", "text": "We found that data-blind manipulation of input weights improves generalization performance, with the added bonus that the input weight matrix is sparse, with a very high percentage of zero entries that could be beneficial for hardware implementations, or when sparse matrix storage methods are used in software. RF-ELM's approach is inspired by neurobiology and strongly resembles other machine learning approaches, such as [10] Biological sensory neurons tend to be aligned with preferred receptive fields so that they receive input only from a subset of the entire input space. The region of responsiveness tends to be coherent in some relevant dimensions, such as the space for visual and touched systems and frequency for the auditory system. Interestingly, this component asase aspect can be lost beyond the earliest neural layers when combined randomly."}, {"heading": "D. Combining RF-ELM with CIW-ELM and C-ELM", "text": "All three approaches described so far provide weightings for pixels for each hidden layer unit. CIW-ELM and C-ELM evaluate the pixels in such a way that hidden units tend to provide a larger response for training data from a particular class. We found that improved classification performance can be achieved by combining the weight formats provided by CIW-ELM or C-ELM with the receptive field masks provided by RF-ELM. The algorithm for either RF-CIW-ELM or RF-C-ELM is as follows. 1. Follow the first 5 steps of the above CIW-ELM or the first 2 steps of the C-ELM algorithm to obtain a non-normalized input weight matrix, Win the size matrix, in the size matrix of the Wx."}, {"heading": "E. Combining RF-C-ELM with RF-CIW-ELM in a two-layer ELM: RF-CIW-C-ELM", "text": "We found that the results obtained with RF-C-ELM and RF-CIW-ELM are similar in terms of the error rate when applied to the MNIST benchmark, but the errors follow different patterns. Thus, a combination of the two methods seemed to be promising. We combined the two methods using a multi-layer ELM network that consists of an RF-C-ELM network and an RF-CIW-ELM network parallel to the first two layers. Initial neurons 7 of these two networks are then combined using another ELM network that can be considered an ELM encoder, although it has twenty input neurons and ten output neurons; the input neurons are effectively two sets of the same ten labels. The structure is shown in Figure 2. Initially, the two input networks are trained in the usual way to complete, then the autocode layer of the outputs of the input networks, the second layer and the second layer, is clearly defined as their input layer."}, {"heading": "F. Fine Tuning by Backpropagation", "text": "In all the variations of ELM described in this report, the hidden layer weights are learned in an unmonitored mode (where the capacity of supervised learning is limited to the output layer weights), and the output layer weights are solved using a linear regression model, reducing the size of the training data set (60,000 for MNIST), suggesting that the capacity of these networks is less than optimal."}, {"heading": "III. RESULTS AND DISCUSSION FOR THE MNIST BENCHMARK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. SLFN with shaped input-weights", "text": "After normalizing the lines of the input weight matrix to the unit, we multiplied the values of the total input weight matrix for all seven methods by a factor of 2, as it turned out that this scaling was nearly optimal in most cases. To give an indication of the variance resulting from the randomness inherent in each input weight shaping method, we trained 10 ELMs with each method and then recorded the ensemble mean as a function of the hidden layer size M. We also recorded (markers) the actual error rates for each ELM. Figure 3 (a) shows that the error rate decreases roughly linearly with the log of M for the small M before approaching M by 104. Figure 3 (b) (greater error rate than the actual training data) shows that the improvement in the training rate is 0.3% less than the increase in the training rate (c)."}, {"heading": "B. ELM with shaped input-weights and backpropagation", "text": "In this context, it should be noted that this is not an individual case, but a case in which it is an individual case. (...) In this case, it is a case of a single member of a group of people in a group of people. (...) In this case, it is a case of a group of people who are in a group. (...) In this case, it is a group of people who are in a group. (...) In this case, it is a group of people who are in a group. (...) In this case, it is a group of people who are in a group. (...) In this case, it is a group of people who are in a group. (...) In this case, it is a group of people who are in a group. (...)"}, {"heading": "D. Distorting and pre-processing the training set", "text": "Many other approaches to classifying handwritten MNIST digits improve their error rate by pre-processing and / or by increasing the size of the training set by applying affine (translation, rotation and shear) and elastic distortions of the standard set [13-15]. Although this is a useful method of improving the network, the use of non-standard training data invalidates the direct comparison of the network performance we are looking for in this essay. It seems reasonable that the pro-1011 performance of a particular network is representative of the default problem for its potential in advanced versions of this problem. Nevertheless, we have also experimented with distortions of the training set to improve the error rates reported here. For example, with 1 and 2-pixel affine translations, we are able to achieve error rates of less than 0.8% for the default problem. If we add random rotations, scaling, shear and elastic distortions to the training set, adding 62 points in total, we need to increase the error rate by a factor of 57 if we do not get the best result from the first 14 trainings]."}, {"heading": "IV. CONCLUSIONS", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "Acknowledgement", "text": "Mark D. McDonell's contribution was supported by an Australian Research Council Fellowship (project number DP1093425) and Andre 'van Schaik's contribution was supported by the Australian Research Council Discovery Project DP140103001.Tables13."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Deep networks have inspired a renaissance in neural network use, and are becoming the default option for difficult tasks on large datasets. In this report we show that published deep network results on the MNIST handwritten digit dataset can straightforwardly be replicated (error rates below 1%, without use of any distortions) with shallow \u2018Extreme Learning Machine\u2019 (ELM) networks, with a very rapid training time (\u223c10 minutes). When we used distortions of the training set we obtained error rates below 0.6%. To achieve this performance, we introduce several methods for enhancing ELM implementation, which individually and in combination can significantly improve performance, to the point where it is nearly indistinguishable from deep network performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random \u2018receptive field\u2019 sampling of the input ensures the input weight matrix is sparse, with about 90 percent of weights equal to zero, which is a potential advantage for hardware implementations. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hiddenunits required to achieve a particular performance. Our close to state-of-the-art results for MNIST suggest that the ease of use and accuracy of ELM should cause it to be given greater consideration as an alternative to deep networks applied to more challenging datasets.", "creator": "LaTeX with hyperref package"}}}