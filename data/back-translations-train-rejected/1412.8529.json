{"id": "1412.8529", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2014", "title": "A note about the generalisation of the C-tests", "abstract": "In this exploratory note we ask the question of what a measure of performance for all tasks is like if we use a weighting of tasks based on a difficulty function. This difficulty function depends on the complexity of the (acceptable) solution for the task (instead of a universal distribution over tasks or an adaptive test). The resulting aggregations and decompositions are (now retrospectively) seen as the natural (and trivial) interactive generalisation of the $C$-tests.", "histories": [["v1", "Tue, 30 Dec 2014 01:48:10 GMT  (1072kb,D)", "http://arxiv.org/abs/1412.8529v1", "15 pages"], ["v2", "Thu, 26 Mar 2015 00:27:30 GMT  (2262kb,D)", "http://arxiv.org/abs/1412.8529v2", "16 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jose hernandez-orallo"], "accepted": false, "id": "1412.8529"}, "pdf": {"name": "1412.8529.pdf", "metadata": {"source": "CRF", "title": "A note about the generalisation of the C-tests", "authors": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["jorallo@dsic.upv.es"], "sections": [{"heading": null, "text": "In this exploratory note, we ask what a performance measure for all tasks looks like when we apply a weighting of tasks based on a difficulty function, which depends on the complexity of the (acceptable) solution of the task (rather than a universal distribution across tasks or an adaptive test).The resulting aggregations and decompositions are (now in retrospect) considered a natural (and trivial) interactive generalization of the C tests. Keywords: intelligence evaluation, artificial intelligence, C tests, algorithmic information theory, universal psychometry, curve of action."}, {"heading": "1 Introduction", "text": "Since the emergence of algorithmic information theory (AIT) in the 1960s, its use to construct intelligence tests is considered difficult, it was proposed by some and explicitly by [7]. The first actual implementation of a test with the help of AIT was the C test [30, 16], in which the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff's inductive sequence problems: \"Given an initial segment of a sequence, predict its continuation\" (quoted in [46, p.332]). Levin's Kt complexity was used to calculate the difficulty of a sequence of letters. Performance was measured as an aggregate value over a series of difficulties: I (hua), H-predicted h = 1 he N-hit i = 1 hit (xi, h) (1), which is the sequence that deals with difficulties ranging from H to H = 1."}, {"heading": "2 Background", "text": "[48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a more recent presentation of AI evaluation, see [31]), but a common approach is based on average performance in a range of tasks, as in eq. 2.In the following, we will focus on the approaches based on AIT. As mentioned above, the first intelligence test with AIT was the so-called C test [30, 16]. Figure 1 shows examples of sequences that appear in this test. The difficulty of each sequence was calculated as Levin's Kt, a time-weighted version of Kolmogorov complexity K. Some preliminary experimental results showed that human performance correlates with the absolute difficulty (h) of each exercise and also with the results of the IQ tests for the same subjects. Figure 2 shows the results (from [30, 16] Hitrate is defined as the sum of intelligences that is universally."}, {"heading": "2.1 Notation", "text": "We consider tests that are composed of tasks (also called environments or items) and executed by agents (also called policies or subjects). The task set is determined by M. Its elements are usually determined by.. The quantity of agents is determined by.. We define Kolmogorovs complexity as KU (y) = minx LU (x). For both functions we usually apply the subindex U fallen.The expected execution steps of.p when executing the assignment can be applied to tasks and agents. The alternative complexity is determined as KU (y) = minx LU (x).The expected execution steps of.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p.p..p.p..p...p..p...p....p..p.....p.p......p......p..subjects are performed by agents."}, {"heading": "2.2 Difficulty-based decomposition", "text": "It is in fact in [26] where we can find a first connection between the schemas of eq q. 1 and eq. 2. We adapt the definition 14 in [26], which is a generalization of eq. 2 by making the set M explicitly as a parameter, as well as the task probability p and the time limitation \u03c4: Definition 1. The expected average result with time limitations is defined for a task class M, a distribution p and an agent \u03c0 as follows:. First, we define partial results for a given difficulty h as follows:."}, {"heading": "3 Difficulty as task complexity", "text": "The first choice, however, is that we consider p (\u00b5) = 2 \u2212 K (\u00b5) as a constant weight (\u00b5). This is exactly what was taken into account in [43, 44] \u2212 \u2212 \u2212 \u2212 The second choice is that we assume that the difficulty of an environment is ~ (\u00b5) = K (\u00b5) = K (\u00b5). In this very specific case5, we see that ~ is discrete (K is defined by the natural numbers). The second choice is that we assume that the difficulty of an environment gives the execution steps of transition i. Even with these decisions and a discount factor, an infinite sum would make both functions very difficult, since we could have a transition with a large value of i giving a high value. R function is not so critical as it is limited and the contribution decays with i, but for S this is problematic. 4LS is considered as a measure of effort, i.e., memory) could also be considered normally important for analysis."}, {"heading": "4 Difficulty functions", "text": "4.1. Disconnect p (h) from p (m | h) The decomposition in the previous section suggests that we could first try to determine a suitable difficulty level, and then think about a meaningful distribution p (h). Once this is clarified, we could try to find a distribution for all environments of this distribution p (m | h). In other words, once we have determined how relevant a difficulty is, we ask what tasks are to be performed for that difficulty. This is the spirit of the C test [30, 16] as in eq. 1. In fact, we may not need a p (h) that decays dramatically, as the power to increase difficulty is expected to decrease, as in Figure 2.To distinguish p (h) and p (m | h), we will label the former with w (m) and the latter with pM."}, {"heading": "4.2 Acceptable solutions", "text": "If we look at aggregated environments with different scales on R and different difficulties, we can argue that one agent focuses on a few environments with high difficulty, while another focuses on many other environments with small rewards. Difficulty functions allow us to see how each agent works for different difficulty levels, which we call agent-response curves in [26], which are inspired by item-response curves in psychometrics, and not by the complexity of the task as in the previous section.An initial idea of a difficulty curve in psychometry we see that the concept of difficulty curve must be linked to R, i.e., how well the agents work, and not about the complexity of the task as in the previous section.An initial idea of a difficulty curve in psychometry is the expected response for a random agent, i.e., ~ (\u00b5), E (R) () (the edges, we are not (very) significant (the rims)."}, {"heading": "4.3 Properties", "text": "In view of the above, we are now ready to define a few properties about the difficulty functions. Definition 2. A difficulty function is very limited in M, if there is a difficulty function for each \u03c0, so that we have a difficulty h in M, so that for each h \u00b2 h \u00b2 (\u03c0, \u00b5) = 0, A we have a weaker version of the above property as follows: Definition 3. A difficulty function ~ is weakly limited in M, if there is a difficulty limit for each h \u00b2 h. For example, we choose p (\u00b5 | h) = 0, if h \u00b2 for a given h \u00b2. Proposition 2. If a countable difficulty function ~ is weakly limited by R, we have a weak limit for each possible active ingredient."}, {"heading": "4.4 Difficulty as solution complexity", "text": "Now we are ready to ask what happens if we select the difficulty function with respect to surface acceptance, i.e.: ~ [surface acceptance] q = > Difficulty q (surface acceptance), min {L (surface variant, surface variant): E \u2212 R (surface variant, surface variant) ~ 1 \u2212 surface variant} = min {L (surface variant, surface variant): A [surface variant, surface variant): A [surface variant, surface variant) (surface variant, surface variant, surface variant) = min {L (surface variant, surface variant): A [surface variant, surface variant, surface variant, surface variant, surface variant) = 1 (surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant, surface variant,"}, {"heading": "5 Difficulty-conditional task probabilities", "text": "In the preceding sections, we have focused on w (h) and whether it is necessary or not. We have seen difficulty functions where the mere aggregation of p (h) (or w (h) = 1) results in a limited (sentence 2) probability pM (\u03c0, M, pM). The question now is how to select the conditional probability pM (\u00b5 | h). In the C test, Equation 1, this was chosen as a uniform distribution. However, this is not possible in an interactive scenario if all possible tasks are taken into account, as the number of tasks for which there is an acceptable solution of L (\u03c0) = n can be infinite. Even if we cannot specify a uniform distribution, we want a choice of pM (\u00b5 | h) that maintains the variety of tasks (unless there is some special bias to the selection of tasks)."}, {"heading": "5.1 Task probability depends on difficulty", "text": "The first thing we can do is assume that p (\u00b5 | h) in size 9 is p (\u00b5 | h) = 2 \u2212 K (\u00b5) \u03bd (h) if ~ [\u044b] (\u00b5) = h and 0 otherwise, where size (h) is a normalization term to make the mass of the distribution equal to 1, which can be calculated as size (h) = \u2211 \u00b5: ~ [\u0432] (\u00b5) = h 2 \u2212 K (\u00b5). And now we have: size [\u0432] h (p, M, pM) = x x x M, ~ [\u0432] (\u00b5) = h pM (\u00b5) \u00b7 A [\u00b5) = h pM (\u00b5) \u00b7 note 11 for the discrete case: size."}, {"heading": "5.2 Task probability depends on the solution probability", "text": "One of the things about using Equation 13 is that the number of acceptable solutions per difficulty is finite. This is what happened in the C test and this is why a uniform distribution could be used for the inner sum. Now, let's try to break down the inner sum by using the solution and calculate the probability of the problem given with the solution. We first have to define this quantity: pairs (M, 1), 2), 2), a problem can have many acceptable solutions (and maybe none) and an agent can be an acceptable solution for many tasks (and maybe none). Now, we can have an alternative expression of the difficulty corresponding to eq. 13 as follows: \"M\" (p) = min, \"2)."}, {"heading": "6 Using computational steps", "text": "As we mentioned in the introduction, the C test [30, 16] uses Levin's Ct instead of K. Apart from the fact that it is calculable, Kt is related to Levin's search, which makes his choice much more appropriate for a measure of difficulty. However, when we work with interactive tasks and with stochastic tasks and agents, we have to calculate the number of computational steps as E [S (\u03c0, \u00b5). For simplicity, we have so far considered only K. Now, we briefly examine the inclusion of the computational steps. In Section 2.1, we define LS. Now, we define a version that takes into account the tolerance steps as follows: LS [\u0432] (\u03c0, \u00b5), E [LS (\u03c0, \u00b5)], when A [\u043c, \u00b5) = 1 and we define a new difficulty function elsewhere that takes into account computational steps."}, {"heading": "7 Discussion", "text": "We have gone from eq. 1 taken from C-Test to eq. 11 if we use discrete difficulty functions or to eq. 10 if we use continuous difficulty functions. We have seen that difficulties allow a more detailed analysis of what is happening for a particular agent, depending on whether there are actually difficulties with simple or difficult tasks. In fact, given the characteristics of most of the difficulty functions for which there is a maximum difficulty, we do not even need to determine the weight for each difficulty and only calculate the area in which there is an aggregate performance for all difficulties. In fact, given the characteristics of most of the difficulty functions that we have seen, a maximum difficulty of. For example, a maximum difficulty of, let us say h = 200 seems to be more than convenient, as it is really difficult to look into a space of 200 bits (without prior knowledge). So, a finite range of difficulties would be sufficient for the evaluation of feasible agents."}], "references": [{"title": "Can we measure the difficulty of an optimization problem", "author": ["T. Alpcan", "T. Everitt", "M. Hutter"], "venue": "IEEE Information Theory Workshop (ITW),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Robotics competitions as benchmarks for AI research", "author": ["J. Anderson", "J. Baltes", "C.T. Cheng"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The process of research investigations in artificial intelligence - a unified view", "author": ["D. Baldwin", "S.B. Yadav"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Ultimate IQ: one test to rule them all", "author": ["C. Biever"], "venue": "New Scientist,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Artificial intelligence as an experimental science", "author": ["B.G. Buchanan"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "G\u00f6del\u2019s theorem and information", "author": ["G.J. Chaitin"], "venue": "International Journal of Theoretical Physics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1982}, {"title": "How evaluation guides AI research: The message still counts more than the medium", "author": ["P.R. Cohen", "A.E. Howe"], "venue": "AI Magazine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Evaluating research in cooperative distributed problem solving", "author": ["K.S. Decker", "E.H. Durfee", "V.R. Lesser"], "venue": "Distributed Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "How universal can an intelligence test be", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Adaptive Behavior,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Compression and intelligence: social environments and communication", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "Artificial General Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Warning: statistical benchmarking is addictive. Kicking the habit in machine learning", "author": ["C. Drummond", "N. Japkowicz"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "On method overfitting", "author": ["E. Falkenauer"], "venue": "Journal of Heuristics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Evaluation of expert systems: Issues and case studies", "author": ["J. Gaschnig", "P. Klahr", "H. Pople", "E. Shortliffe", "A. Terry"], "venue": "Building expert systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1983}, {"title": "Verification & validation", "author": ["J.R. Geissman", "R.D. Schultz"], "venue": "AI Expert,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1988}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "National Institute of Standards and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Thesis: Computational measures of information gain and reinforcement in inference processes", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "AI Communications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "Artificial General Intelligence, 3rd Intl Conf,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "On evaluating agent performance in a fixed period of time", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "Artificial General Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "On environment difficulty and discriminating power. Autonomous Agents and Multi-Agent Systems, pages", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Mammals, machines and mind games. Who\u2019s the smartest?  The Conversation, http: // theconversation", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "edu. au/ articles/", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "On potential cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Minds and Machines,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa-Cabrera"], "venue": "Artificial General Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Universal psychometrics: Measuring cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "M.V. Hern\u00e1ndez-Lloreda"], "venue": "Cognitive Systems Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Turing machines and recursive Turing Tests", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard"], "venue": "AISB/IACAP 2012 Symposium \u201cRevisiting Turing and his Test\u201d,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Turing Tests with Turing machines", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard"], "venue": "editor, Turing-100,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Computer models solving human intelligence test problems: progress and implications", "author": ["J. Hern\u00e1ndez-Orallo", "F. Mart\u0301\u0131nez-Plumed", "U. Schmid", "M. Siebers", "D.L. Dowe"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity", "author": ["J. Hern\u00e1ndez-Orallo", "N. Minaya-Collado"], "venue": "In Proc. Intl Symposium of Engineering of Intelligent Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "AI evaluation: past, present and future", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": "arXiv preprint arXiv:1408.6908,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Bias and no free lunch in formal measures of intelligence", "author": ["B. Hibbard"], "venue": "Journal of Artificial General Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "On measuring social intelligence: Experiments on competition and cooperation", "author": ["J. Insa-Cabrera", "J.L. Benacloch-Ayuso", "J. Hern\u00e1ndez-Orallo"], "venue": "AGI, volume 7716 of Lecture Notes in Computer Science,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Comparing humans and AI agents", "author": ["J. Insa-Cabrera", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Hern\u00e1ndez-Orallo"], "venue": "Artificial General Intelligence,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Evaluating a reinforcement learning algorithm with a general intelligence test", "author": ["J. Insa-Cabrera", "D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Current Topics in Artificial Intelligence. CAEPIA 2011. LNAI Series", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Definition and properties to assess multi-agent environments as social intelligence tests", "author": ["J. Insa-Cabrera", "J. Hern\u00e1ndez-Orallo"], "venue": "arXiv preprint,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Hern\u00e1ndez-Lloreda. The anynt project intelligence test : Lambda - one", "author": ["J. Insa-Cabrera", "J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa na", "M.V"], "venue": "AISB/IACAP 2012 Symposium \u201cRevisiting Turing and his Test\u201d,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Who are you calling bird-brained? An attempt is being made to devise a universal intelligence test", "author": ["K. Kleiner"], "venue": "The Economist, 398(8723,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Clever methods of overfitting. Machine Learning (Theory), http: // hunch", "author": ["J. Langford"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Research papers in machine learning", "author": ["P. Langley"], "venue": "Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1987}, {"title": "The changing science of machine learning", "author": ["P. Langley"], "venue": "Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Tests of machine intelligence", "author": ["S. Legg", "M. Hutter"], "venue": "Years of Artificial Intelligence,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["S. Legg", "M. Hutter"], "venue": "Minds and Machines,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2007}, {"title": "An approximation of the universal intelligence measure. In Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence, pages 236\u2013249", "author": ["S. Legg", "J. Veness"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Universal sequential search problems", "author": ["L.A. Levin"], "venue": "Problems of Information Transmission,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1973}, {"title": "An introduction to Kolmogorov complexity and its applications (3rd ed.)", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Performance Evaluation and Benchmarking of Intelligent Systems", "author": ["R. Madhavan", "E. Tunstel", "E. Messina"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Computer science as empirical inquiry: Symbols and search", "author": ["A. Newell", "H.A. Simon"], "venue": "Communications of the ACM,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1976}, {"title": "Evaluating expert system tools: A framework and methodology\u2013workshops", "author": ["J. Rothenberg", "J. Paul", "I. Kameny", "J.R. Kipps", "M. Swenson"], "venue": "Technical report, DTIC Document,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1987}, {"title": "An extensible description language for video games", "author": ["T. Schaul"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Performance evaluation of intelligent systems at the national institute of standards and technology (nist)", "author": ["C. Schlenoff", "H. Scott", "S. Balakirsky"], "venue": "Technical report, DTIC Document,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "Artificial intelligence: an empirical science", "author": ["H.A. Simon"], "venue": "Artificial Intelligence,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1995}, {"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "Part I. Information and control,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1964}, {"title": "Handbook of intelligence", "author": ["R.J. Sternberg (ed"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2000}, {"title": "Protecting against evaluation overfitting in empirical reinforcement learning", "author": ["S. Whiteson", "B. Tanner", "M.E. Taylor", "P. Stone"], "venue": "In Adaptive Dynamic Programming And Reinforcement Learning (ADPRL),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2011}, {"title": "Toward a standard metric of machine intelligence", "author": ["R. Yonck"], "venue": "World Future Review,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "Since the inception of algorithmic information theory (AIT) in the 1960s, its use to construct intelligence tests was hinted by some and explicitly suggested by [7].", "startOffset": 161, "endOffset": 164}, {"referenceID": 29, "context": "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff\u2019s inductive inference problems: \u201cgiven an initial segment of a sequence, predict its continuation\u201d (as quoted in [46, p.", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff\u2019s inductive inference problems: \u201cgiven an initial segment of a sequence, predict its continuation\u201d (as quoted in [46, p.", "startOffset": 67, "endOffset": 75}, {"referenceID": 53, "context": "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff\u2019s inductive inference problems: \u201cgiven an initial segment of a sequence, predict its continuation\u201d (as quoted in [46, p.", "startOffset": 165, "endOffset": 173}, {"referenceID": 28, "context": "The first actual implementation of a test using AIT was the C-test [30, 16], where the goal was to find a continuation of a sequence of letters, as in some IQ tasks [54, 29], and in the spirit of Solomonoff\u2019s inductive inference problems: \u201cgiven an initial segment of a sequence, predict its continuation\u201d (as quoted in [46, p.", "startOffset": 165, "endOffset": 173}, {"referenceID": 52, "context": "2 can also be combined with AIT, in a different way, by using a universal distribution [53, 46], i.", "startOffset": 87, "endOffset": 95}, {"referenceID": 45, "context": "2 can also be combined with AIT, in a different way, by using a universal distribution [53, 46], i.", "startOffset": 87, "endOffset": 95}, {"referenceID": 42, "context": ", p(\u03bc) = 2\u2212K(\u03bc), where K(\u03bc) is the Kolmogorov complexity of \u03bc, as first chosen by [43].", "startOffset": 82, "endOffset": 86}, {"referenceID": 42, "context": "The work in [43] has been considered a generalisation of [30, 16], from static sequences (predicting a continuation of a sequence correctly) to dynamic environments (maximising rewards in an MDP).", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "The work in [43] has been considered a generalisation of [30, 16], from static sequences (predicting a continuation of a sequence correctly) to dynamic environments (maximising rewards in an MDP).", "startOffset": 57, "endOffset": 65}, {"referenceID": 15, "context": "The work in [43] has been considered a generalisation of [30, 16], from static sequences (predicting a continuation of a sequence correctly) to dynamic environments (maximising rewards in an MDP).", "startOffset": 57, "endOffset": 65}, {"referenceID": 29, "context": "In this paper we challenge this interpretation and look for a proper generalisation of [30, 17] using the notion of difficulty in the outer sum, as originally conceived and seen in eq.", "startOffset": 87, "endOffset": 95}, {"referenceID": 16, "context": "In this paper we challenge this interpretation and look for a proper generalisation of [30, 17] using the notion of difficulty in the outer sum, as originally conceived and seen in eq.", "startOffset": 87, "endOffset": 95}, {"referenceID": 15, "context": "Figure 1: Several series of different difficulties 9, 12, and 14 used in the C-test [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "In fact, this is discussed in [22] and [21]: the complexity of the environment is roughly an upper bound of the complexity of the solution, but very complex environments can have very simple solutions.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "In fact, this is discussed in [22] and [21]: the complexity of the environment is roughly an upper bound of the complexity of the solution, but very complex environments can have very simple solutions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 47, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 13, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 48, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 14, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 7, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 8, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 39, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 40, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 5, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 51, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 2, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 12, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 38, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 41, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 54, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 11, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 1, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 46, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 50, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 56, "endOffset": 127}, {"referenceID": 30, "context": "AI evaluation has been performed in many different ways [48, 14, 49, 15, 8, 9, 40, 41, 6, 52, 3, 13, 39, 42, 55, 12, 2, 47, 51] (for a recent account of AI evaluation, see [31]), but a common approach is based on averaging performance on a range of tasks, as in eq.", "startOffset": 172, "endOffset": 176}, {"referenceID": 29, "context": "As mentioned above, the first intelligence test using AIT was the so-called C-test [30, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 15, "context": "As mentioned above, the first intelligence test using AIT was the so-called C-test [30, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 29, "context": "Figure 2 shows the results (taken from [30, 16]).", "startOffset": 39, "endOffset": 47}, {"referenceID": 15, "context": "Figure 2 shows the results (taken from [30, 16]).", "startOffset": 39, "endOffset": 47}, {"referenceID": 29, "context": "] with input/output devices for a complex environment\u201d [30] where \u201crewards and penalties could be used instead\u201d [17]) or extending them for other cognitive abilities [18], but not fully formalised.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "] with input/output devices for a complex environment\u201d [30] where \u201crewards and penalties could be used instead\u201d [17]) or extending them for other cognitive abilities [18], but not fully formalised.", "startOffset": 112, "endOffset": 116}, {"referenceID": 17, "context": "] with input/output devices for a complex environment\u201d [30] where \u201crewards and penalties could be used instead\u201d [17]) or extending them for other cognitive abilities [18], but not fully formalised.", "startOffset": 166, "endOffset": 170}, {"referenceID": 42, "context": "AIT and reinforcement learning were finally combined in [43], where all possible environments were considered in eq.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "Figure 2: Results obtained by humans on task of different difficulty in the C-test [16].", "startOffset": 83, "endOffset": 87}, {"referenceID": 31, "context": "applicable version of this appraoch by [32] and [22, secs.", "startOffset": 39, "endOffset": 43}, {"referenceID": 34, "context": "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].", "startOffset": 26, "endOffset": 42}, {"referenceID": 33, "context": "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].", "startOffset": 26, "endOffset": 42}, {"referenceID": 36, "context": "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].", "startOffset": 26, "endOffset": 42}, {"referenceID": 43, "context": "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].", "startOffset": 26, "endOffset": 42}, {"referenceID": 18, "context": "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "Some tests were developed [35, 34, 37, 44], using the environment class defined in [19] and some ways of aggregated rewards [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].", "startOffset": 69, "endOffset": 84}, {"referenceID": 4, "context": "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].", "startOffset": 69, "endOffset": 84}, {"referenceID": 22, "context": "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].", "startOffset": 69, "endOffset": 84}, {"referenceID": 55, "context": "Despite the limited results, the experiment had quite a repercussion [38, 5, 23, 56].", "startOffset": 69, "endOffset": 84}, {"referenceID": 17, "context": "While the aim of all these proposals was to measure intelligence, many interesting things can happen if AIT is applied to cognitive abilities other than intelligence, as suggested in [18] for the passive case and hinted in [22, secs.", "startOffset": 183, "endOffset": 187}, {"referenceID": 3, "context": "2] for the dynamic cases, with the use of different kinds of videogames as environments (two of the most recently introduced benchmarks and competitions are in this direction [4, 50]).", "startOffset": 175, "endOffset": 182}, {"referenceID": 49, "context": "2] for the dynamic cases, with the use of different kinds of videogames as environments (two of the most recently introduced benchmarks and competitions are in this direction [4, 50]).", "startOffset": 175, "endOffset": 182}, {"referenceID": 24, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 45, "endOffset": 69}, {"referenceID": 10, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 45, "endOffset": 69}, {"referenceID": 26, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 45, "endOffset": 69}, {"referenceID": 27, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 45, "endOffset": 69}, {"referenceID": 32, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 45, "endOffset": 69}, {"referenceID": 35, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 45, "endOffset": 69}, {"referenceID": 23, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 228, "endOffset": 232}, {"referenceID": 9, "context": "Also, some hybridisations were also proposed [25, 11, 27, 28, 33, 36], the notion of potential intelligence [24], as well as an integration with the measurement of natural systems under the umbrella of \u2018universal psychometrics\u2019 [26] and the notion of universal test [10].", "startOffset": 266, "endOffset": 270}, {"referenceID": 21, "context": "For MDPs one possibility is to consider the (expected value of the) maximum steps taken by \u03c0 for any transition [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 44, "context": ", [45] or [46]), we can define Kt(\u03c0, \u03bc) = min\u03c0 expLS(\u03c0, \u03bc), which in this case depends on \u03bc as well .", "startOffset": 2, "endOffset": 6}, {"referenceID": 45, "context": ", [45] or [46]), we can define Kt(\u03c0, \u03bc) = min\u03c0 expLS(\u03c0, \u03bc), which in this case depends on \u03bc as well .", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "It is actually in [26], where we can find a first connection between the schemas of eq.", "startOffset": 18, "endOffset": 22}, {"referenceID": 25, "context": "We adapt definition 14 in [26], which is a generalisation of eq.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "And now we use proposition 4 in [26] that decomposes it.", "startOffset": 32, "endOffset": 36}, {"referenceID": 42, "context": "This is exactly what has been considered in [43, 44].", "startOffset": 44, "endOffset": 52}, {"referenceID": 43, "context": "This is exactly what has been considered in [43, 44].", "startOffset": 44, "endOffset": 52}, {"referenceID": 31, "context": "In fact, it is not only that the definition depends strongly on the choice of the reference UTM used for K, which determines the probability of each task 2\u2212K(\u03bc), as argued elsewhere ([32] and [22, secs.", "startOffset": 183, "endOffset": 187}, {"referenceID": 29, "context": "This is the spirit of the C-test [30, 16] as seen in eq.", "startOffset": 33, "endOffset": 41}, {"referenceID": 15, "context": "This is the spirit of the C-test [30, 16] as seen in eq.", "startOffset": 33, "endOffset": 41}, {"referenceID": 25, "context": "Difficulty functions allow us to see how each agent performs for different degrees of difficulty, what we called agent response curves in [26], which are inspired by item response curves in psychometrics (but putting inverting the view between agents and items).", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "Another option is what is done in [21], as ~(\u03bc) , min\u03c0:E[R(\u03c0,\u03bc)]=Rmax(\u03bc) L(\u03c0) where Rmax(\u03bc) = max\u03c0 E[R(\u03c0, \u03bc)].", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "In [21], a \u2018tolerance value\u2019 is considered and, instead of one solution, difficulty is linked to the probability of finding a solution under this tolerance by using different search approaches.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "As we mentioned in the introduction, the C-test [30, 16] used Levin\u2019s Kt instead of K.", "startOffset": 48, "endOffset": 56}, {"referenceID": 15, "context": "As we mentioned in the introduction, the C-test [30, 16] used Levin\u2019s Kt instead of K.", "startOffset": 48, "endOffset": 56}, {"referenceID": 43, "context": "It is pertinent to quote a piece from [44]: \u201cAnother important difference in our work is that we have directly sampled from program space.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "Finally, there is an important question about the choice of a meaningful difficulty function linked to the effort required to find the solution: what if the difficulty depends on the verification of the solution? This has got attention for problems where the solution must be accompanied by a verification, proof or explanation [17, 1].", "startOffset": 328, "endOffset": 335}, {"referenceID": 0, "context": "Finally, there is an important question about the choice of a meaningful difficulty function linked to the effort required to find the solution: what if the difficulty depends on the verification of the solution? This has got attention for problems where the solution must be accompanied by a verification, proof or explanation [17, 1].", "startOffset": 328, "endOffset": 335}], "year": 2017, "abstractText": "In this exploratory note we ask the question of what a measure of performance for all tasks is like if we use a weighting of tasks based on a difficulty function. This difficulty function depends on the complexity of the (acceptable) solution for the task (instead of a universal distribution over tasks or an adaptive test). The resulting aggregations and decompositions are (now retrospectively) seen as the natural (and trivial) interactive generalisation of the C-tests.", "creator": "LaTeX with hyperref package"}}}