{"id": "1412.8534", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2014", "title": "Disjunctive Normal Networks", "abstract": "Artificial neural networks are powerful pattern classifiers; however, they have been surpassed in accuracy by methods such as support vector machines and random forests that are also easier to use and faster to train. Backpropagation, which is used to train artificial neural networks, suffers from the herd effect problem which leads to long training times and limit classification accuracy. We use the disjunctive normal form and approximate the boolean conjunction operations with products to construct a novel network architecture. The proposed model can be trained by minimizing an error function and it allows an effective and intuitive initialization which solves the herd-effect problem associated with backpropagation. This leads to state-of-the art classification accuracy and fast training times. In addition, our model can be jointly optimized with convolutional features in an unified structure leading to state-of-the-art results on computer vision problems with fast convergence rates. A GPU implementation of LDNN with optional convolutional features is also available", "histories": [["v1", "Tue, 30 Dec 2014 02:17:30 GMT  (611kb,D)", "http://arxiv.org/abs/1412.8534v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["mehdi sajjadi", "mojtaba seyedhosseini", "tolga tasdizen"], "accepted": false, "id": "1412.8534"}, "pdf": {"name": "1412.8534.pdf", "metadata": {"source": "CRF", "title": "Disjunctive Normal Networks", "authors": ["Mehdi Sajjadi", "Mojtaba Seyedhosseini"], "emails": ["mehdi@sci.utah.edu"], "sections": [{"heading": null, "text": "In fact, it is the case that most of us are in a position to develop in a different way, as if they were getting involved in another world, than in another world, in which they are getting involved in another world, and in which they are getting involved in another world, and in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in another world, in which they are getting involved in, in which they are getting involved in, in which they are getting involved, in which they are getting involved, in which they are getting involved, in which they are getting involved, in which they are getting involved, in which they are getting involved in, in which they are getting involved in, in which they are getting involved, in which they are getting involved in, in which they are getting involved in, in which they are getting involved in, in which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved in, which they are getting involved with, which they are getting involved with, which they are getting involved in, which they are getting involved in,"}, {"heading": "II. RELATED WORK", "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far."}, {"heading": "III. METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Network Architecture", "text": "Consider the binary classification problem f: Rn \u2192 B, where W = {0, 1}. Let us replace the (x) Rn: f (x) = 1}. Let us define the (x) approximate (x) + as an association of N convex Polytopen (x) > 0}. We can replace Mi with M = maxiMi without loss of generality. Hij is defined in relation to his indicator functionhij (x) = {1, x x x x x x x x x x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x"}, {"heading": "B. Model Initialization", "text": "s look at the subsets of what y = 1 and y = 0 are. Disjunctive normal form allows for a very simple and intuitive initialization of network weights. To initialize an N \u00b7 M LDNN, we must first divide the weight vectors into N and M clusters. Finally, let us initialize vij = c \u2212 j, where c \u2212 j are the centroids of the i'th positive and j'th negative clusters, respectively. We initialize the weight vectors as wij = vij and M clusters. Finally, let us initialize the bias terms bij so that logistical signature processing (x) sets the value of 0.5 at the midpoints of the lines linking the positive and negative clusters."}, {"heading": "C. Model Optimization", "text": "The LDNN model can be trained by selecting the network weights and distortions that minimize the square error pair. Starting from an initialization as described in Section III-B, we (5) minimize differentiation based on gradient parentage. In order to find the updated equations in terms of network weights and biases, we need to find the partial derivatives of the error in terms of network weights and biases. Starting from the fact that every positive training (wpqk) is positive only when i = p and j = q are determined, the derivatives of the error function in terms of network weights, a partial derivative of the errors in terms of network weights and presets will be positive. Starting from the fact that every positive training (wpqk) is not zero, the derivatives of the error function in terms of network weights are achieved using the chain rule \"E.\""}, {"heading": "D. Deep learning with LDNN", "text": "We have the following equation for the proceeding. (1) We have the following equation for the proceeding of the proceeding. (1) We have the following equation for the proceeding of the proceeding. (1) We have the following equation for the proceeding. (2) We have the proceeding of the proceeding. (2) We have the proceeding of the proceeding. (2) We have the proceeding of the proceeding. (2) We are the proceeding of the proceeding. (2) We are the proceeding. (2) We are the proceeding of the proceeding. (3) We are the proceeding. (3) We are the proceeding. (3) We are the proceeding. (4) We are the proceeding. (4) We are. (4) We are. (4) We are the proceeding."}, {"heading": "IV. EXPERIMENTS ON GENERAL CLASSIFICATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Artificial datasets", "text": "We first experimented with the two artificial datasets to evaluate the LDNN algorithm with and without the proposed clustering initialization, and we also compared the LDNN model with the modular neural networks (ModN). [21] To construct the two lunar datasets, we will begin to generate random radii and angle pairs (r); for both moons, it is a uniform random variable between R-W / 2 and R-W / 2, in which the parameters determining the radius and width of the moons are determined; for the uppermost moon, it is a uniformly distributed variable between 0 and 6, which represents a uniformably distributed random variable; for the lower moon, it is a uniformably distributed random variable between the two countries; and the cartesian coordinates for the data points on the uppermost and lower moons are then separated."}, {"heading": "B. Two-class problems", "text": "We experimented with 10 different binary classification datasets from the UCI Machine Learning Repository [56] and the LIBSVM Tools website [57]. For each dataset we trained the LDNN, ModN, MLP, SVM and RF classifiers."}, {"heading": "1) Dataset normalization, training/testing set split:", "text": "In fact, it is such that most of them are able to surpass themselves, \"he told the German press agency in an interview with\" Welt am Sonntag \":\" I do not believe that they are able to change the world. \"He pointed out that most of them are able to change the world:\" I do not believe that they are able to change the world. \"He pointed out that the world in the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world,\" the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world, the world of the world of the world, the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of"}, {"heading": "C. Multi-class problems", "text": "We also experimented with 6 multi-class datasets from the UCI Machine Learning Repository [56]. Each dataset was initially normalized in the same way as described in Section IV-B1. For each dataset, we trained the LDNN, RF, and SVM classifiers except for the MNIST dataset for which the SVM results from [14] are reported. In this essay, an SVM is trained using a feature set generated by a deep-faith network. Model and classifier training parameters were selected as described in Section IV-B2 and are repeated in Table III. LDNN and RF experiments were repeated 20 times to obtain average, minimum, and maximum test errors that are generated in Table III. The LDNN classifier is also related to the idea of space partitioning [32], which combines the learning of a local classifier for each partition into a global function for objective learning."}, {"heading": "V. EXPERIMENTS ON IMAGE CLASSIFICATION", "text": "In this section we evaluate the performance of our proposed depth structure through various experiments. We have integrated our LDNN into a GPU implementation of ConvNet, which is publicly available under [59] Most of the experiments described here are performed with this GPU implementation. In all experiments we found good parameters by cross-validating a small portion of the training data and repeating the training on all training data. For all data sets we repeated the experiments several times and enter two numbers here. One is the average error rate of different experiments and the other is the tuning result of different experiments. To get the tuning result, we calculate the predicted probabilities of different experiments before calculating the error rate. Basically, the only difference between multiple runs of the experiments for each data set is the random initialization of the weights."}, {"heading": "A. MNIST", "text": "In fact, most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) Most of them have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) They have no idea what they are doing. (...) They have no idea what they are doing. (...) They have no idea what they are doing. (...) They have no idea what they are doing. (...) They have no idea what they are doing. (...) They have no idea what they are doing. (...) (...) They have no idea what they are doing. (...) (...) They have no idea what they are doing. (...) (...) (...) () () () () (()) (()) (()) (()) (()) (()) (()) (()) (()) (()) (()) (()) ((())) ()) ((()) (()) (()) (()) () () ()) () () () () () ()) () () () () ()) () () () ()) () () () ()) () () () () ()) () () ()) () () () () ()) () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () (() () () () () (() () () (() () () (() () () () () () () () () () (() (() () () () () () () () (() (())) (((()))) (((("}, {"heading": "B. CIFAR10", "text": "CIFAR10 [62] is a collection of 60,000 tiny 32 \u00d7 32 images across 10 categories (50,000 for training and 10,000 for testing).Our setup for this dataset consists of 2 revolutionary layers followed by two locally connected layers. There are 64 cards in each revolutionary layer and 32 cards in each locally connected layer. Filters are 5 \u00d7 5 in revolutionary layers and 3 \u00d7 3 in locally connected layers (the same as \"layers-conv-local-13pct.cfg\" of [59]).LDNNs with 7 groups and 7 discriminators per group have been used for classification. Training data has also been enhanced with image translations that are done by placing 24 \u00d7 24 truncated versions of the original images in random places. A common pre-processing for this dataset is to subtract the mean per pixel from each image."}, {"heading": "C. NORB", "text": "NORB [63] is a collection of stereo images in 6 classes. The training set contains 10 folds to 29160 images. Typically, only the first two folds are used for training. The test set contains 2 folds to a total of 58320. The original images are 108 x 108. However, we have scaled this structure down to 48 x 48, similar to [49]. The layer configuration for this task is similar to CIFAR10. LDNNs also have 7 groups and 7 discriminators per group. We trained this structure for 75 and 20 epochs with learning rates of 1e-3 and 1e-4. Data translation, rotation and scaling were also used during training. Image translation was performed by randomly cutting the training images to 44 x 44. We trained 4 networks and used the tuning for the final classification. We achieved an error rate of 3.09% in this task. The state of the art for this task is 3.03% as shown in Table VII."}, {"heading": "D. SVHN", "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"}, {"heading": "E. CIFAR100", "text": "CIFAR100 [62] is similar to CIFAR10, but contains tiny images of 100 classes. We used the same setup as CIFAR10. The only difference is that we used LDNNs with 4 groups and 4 discriminants per group instead of 7 and 7. Subtraction and image translation were also used per pixel. Four networks were trained and after the vote an error rate of 36.17% was reached, which shows that our structure is capable of processing data sets with many classes."}, {"heading": "VI. CONCLUSION", "text": "We believe that the LDNN network architecture and education can be an inexpensive alternative to MLPs for supervised learning with artificial neural networks. LDNN classification has several advantages over MLPs: Firstly, it allows simple and intuitive initialization of network weights, which avoids the herd effect. Secondly, learning requires greater parentage step sizes. We show empirically that the LDNs are significantly faster than the MLPs. Similar to LDNN classification, the choice of model complexity also requires the number of constellations (number of positive training clusters) and the number of significant errors in education."}], "references": [{"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks, vol. 2, pp. 359\u2013366, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Math. Control Systems, vol. 2, no. 303\u2013314, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": "Nature, 1986.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "The cascade-correlation learning architecture", "author": ["S.E. Fahlman", "C. Lebiere"], "venue": "Advances in Neural Information Processing Systems 2. Morgan Kaufmann, 1990, pp. 524\u2013532.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Dynamics and algorithms for stochastic learning", "author": ["G. Orr"], "venue": "Ph.D. dissertation, Oregon Graduate Institute, 1995.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Speeding up backpropagation algorithms by using cross-entropy combined with pattern normalization", "author": ["M. Joost", "W. Schiffmann"], "venue": "Int. Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 6, no. 2, 1998.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Exact solution for on-line learning in multilayer neural networks", "author": ["D. Saad", "S. Solla"], "venue": "Physical Review Letters, vol. 74, pp. 4337\u20134340, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptive on-line learning in changing environments", "author": ["N. Murata", "K. Muller", "A. Ziehe", "S. Amari"], "venue": "Advances in Neural Information Processing Systems, vol. 9, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "http://arxiv.org/abs/1207.0580.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1207}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.  12", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Fuzzy min-max neural networks - part 1: Classification", "author": ["P. Simpson"], "venue": "IEEE Trans Neural Networks, vol. 3, no. 5, pp. 776\u2013786, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive membership function fusion and annihilation in if-then rules", "author": ["B. Song", "R.M. II", "S. Oh", "P. Arabshahi", "T. Caudell", "J. Choi"], "venue": "Proc. Int. Conf. Fuzzy Syst., vol. II, 1993, pp. 961\u2013967.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "A fuzzy min-max neural network classifier with compensatory neuron architecture", "author": ["A. Nandedkar", "P. Biswas"], "venue": "Int. Conf. on Pattern Recognition, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "A multi-sieving neural network architecture that decomposes learning tasks automatically", "author": ["B.-L. Lu", "H. Kita", "Y. Nishikawa"], "venue": "Proc. Int. Conf. on Neural Networks, vol. 3, 1994, pp. 1319\u20131324.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "A neural network classifier with disjunctive fuzzy information", "author": ["H.-M. Lee", "K.-H. Chen", "I.-F. Jiang"], "venue": "Neural Networks, vol. 11, pp. 1113\u2013 1125, 1998.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Task decomposition and module combination based on class relations: A modular neural network for pattern classification", "author": ["B.-L. Lu", "M. Ito"], "venue": "IEEE Trans Neural Networks, vol. 10, no. 5, pp. 1244\u20131256, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Steepest descent adaptations of min-max fuzzy if-then rules", "author": ["R.M. II", "S. Oh", "P. Arabshahi", "T. Caudell", "J. Choi", "B. Song"], "venue": "Proc Int. Joint Conf. on Neural Networks, vol. III, 1992, pp. 471\u2013477.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "A learning method of fuzzy inference by descent method", "author": ["H. Normura", "I. Hayashi", "N. Wakami"], "venue": "Proc. Int. Conf. Fuzzy Syst., 1992.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "The delta rule and learning for min-max neural networks", "author": ["X. Zhang", "C.-C. Hang", "S. Tan", "P.-Z. Wang"], "venue": "Proc. Int. Conf. Neural Networks, 1994, pp. 38\u201343.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "The min-max function differentiation and training of fuzzy neural networks", "author": ["\u2014\u2014"], "venue": "IEEE Trans Neural Networks, vol. 7, no. 5, pp. 1139\u20131150, 1996.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Adaptive mixtures of local experts", "author": ["R. Jacobs", "M. Jordan", "S. Nowlan", "G. Hinton"], "venue": "Neural Computation, vol. 3, no. 79\u201387, 1991.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1991}, {"title": "Twenty years of mixture of experts", "author": ["S. Yuksel", "J. Wilson", "P. Gader"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, vol. 23, no. 8, pp. 1177\u20131193, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Discriminant analysis by gaussian mixtures", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B, vol. 58, pp. 155\u2013176, 1996.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Subclass discriminant analysis", "author": ["M. Zhu", "A. Martinez"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 28, no. 8, pp. 1274\u2013 1286, 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Locally linear discriminant analysis for multimodally distributed classes for face recognition with a single model image", "author": ["T.-K. Kim", "J. Kittler"], "venue": "IEEE Trans on Pattern Analysis and Machine Intelligence, vol. 27, pp. 318\u2013327, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Local supervised learning through space partitioning", "author": ["J. Wang", "V. Saligrame"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally adaptive classification piloted by uncertainty", "author": ["J. Dai", "S. Yan", "X. Tang", "J. Kwok"], "venue": "Proc. Int. Conf. on Machine Learning, 2006, pp. 225\u2013232.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning discontinuities with products-of-sigmoids for switching between local models", "author": ["M. Toussaint", "S. Vijayakumar"], "venue": "Proc. Int. Conf. on Machine Learning, 2005, pp. 904\u2013911.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "There is a hole in my data space: Piecewise predictors for heterogenous learning problems", "author": ["O. Dekel", "O. Shamir"], "venue": "Proc. Int. Conf. on Artificial Intelligence and Machine Learninf, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Localized support vector machine and its efficient algorithm", "author": ["H. Cheng", "P. Tang", "R. Jin"], "venue": "Proc. SIAM Int. Conf. on Data Mining, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "The elements of statistical learning: Data mining, inference and prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2001}, {"title": "Dipol - a hybrid piecewise linear classifier", "author": ["B. Schulmeister", "F. Wysotzki"], "venue": "ch. Machine Learning and Statistics: the Interface,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Classification by pairwise coupling", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 26, no. 1, pp. 451\u2013471, 1998.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1998}, {"title": "A part-versus-part method  for massively parallel training of support vector machines", "author": ["B. Lu", "K. Wang", "M. Utiyama", "H. Isahara"], "venue": "Proc. of Int. Joint. Conf. on Neural Networks, 2004, pp. 735\u2013740.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "Probability estimates for multi-class classification by pairwise coupling", "author": ["T. Wu", "C. Lin", "R. Weng"], "venue": "Journal of Machine Learning Research, pp. 975\u20131005, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Local decomposition for rare class analysis", "author": ["J. Wu", "H. Hui", "W. Peng", "J. Chen"], "venue": "Proc. ACM Int. Conf. on Knowledge discovery and data mining, 2007, pp. 814\u2013823.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "On locally linear classification by pairwise coupling", "author": ["F. Chen", "C.-T. Lu", "A. Boedihardjo"], "venue": "Data Mining, 2008. ICDM \u201908. Eighth IEEE International Conference on, dec. 2008, pp. 749 \u2013754.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained classifier: a novel approach to nonlinear classification", "author": ["H. Abbassi", "R. Monsefi", "H. Sadoghi Yazdi"], "venue": "Neural Computing and Applications, pp. 1\u201311, 2012.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel"], "venue": "Advances in Neural Information Processing Systems (NIPS 1989), vol. 2, Denver, CO, 1990.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, November 1998.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, August 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "International Conference on Learning Representations (ICLR2014), April 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition, 2012, pp. 3642\u20133649.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "Proc. International Conference on Machine learning (ICML\u201913), 2013.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Maxout networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Large scale visual recognition challenge 2010.", "author": ["A. Berg", "J. Deng", "L. Fei-Fei"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Ijcnn 2001 challenge: generalization ability and text decoding", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "International Joint Conference on Neural Networks, vol. 2, 2001, pp. 1031\u20136.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2001}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "International Conference on Learning Representations, 2013.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1872\u20131886, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1872}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep, 2009.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F. Huang", "L. Bottou"], "venue": "Computer Vision and Pattern Recognition (CVPR). IEEE Press, 2004.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2004}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, 2011.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "International Conference on Pattern Recognition (ICPR), 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "AN artificial neural network (ANN) consisting of one hidden layer of squashing functions is an universal approximator for continuous functions defined on the unit hypercube [1], [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "AN artificial neural network (ANN) consisting of one hidden layer of squashing functions is an universal approximator for continuous functions defined on the unit hypercube [1], [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "However, until the introduction of the backpropagation algorithm [3], training such multilayer perceptron (MLP) networks was not possible in practice.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "However, eventually MLPs were replaced by more recent techniques such as support vector machines (SVM) [4] and random forests (RF) [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "However, eventually MLPs were replaced by more recent techniques such as support vector machines (SVM) [4] and random forests (RF) [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "An underlying reason for the limited accuracy and high computational cost of training is the herd-effect problem [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "stochastic learning [7], [8], squared error vs.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "cross-entropy [9] and optimal learning rates [10], [11].", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "cross-entropy [9] and optimal learning rates [10], [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "cross-entropy [9] and optimal learning rates [10], [11].", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "proposed a Dropout scheme for backpropagation which helps prevent co-adaptation of feature detectors [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Contrastive divergence [14], [15] can be used to pre-train networks in an unsupervised manner prior to backpropagation such that the herd-effect problem is alleviated.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "Contrastive divergence [14], [15] can be used to pre-train networks in an unsupervised manner prior to backpropagation such that the herd-effect problem is alleviated.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "Fuzzy min-max networks [16], [17], [18] represent the classification function as the union of axis aligned hypercubes in the feature space.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Fuzzy min-max networks [16], [17], [18] represent the classification function as the union of axis aligned hypercubes in the feature space.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Fuzzy min-max networks [16], [17], [18] represent the classification function as the union of axis aligned hypercubes in the feature space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "[19] proposed a multi-sieving network that decomposes learning tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] proposed a disjunctive fuzzy network which is based on prototypes; however, it lacks an objective function and is based on an adhoc training procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Similarly, the modular network proposed by Lu and Ito [21] removes the axis aligned hypercube restriction from fuzzy min-max networks; however, their network can not be learned by minimizing a single energy function.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 149, "endOffset": 153}, {"referenceID": 21, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "Differentiable approximations of min-max functions have been used to construct fuzzy neural network that can be trained using steepest descent [22], [23], [24], [25], but these have produced results that are significantly less accurate than stateof-the-art classification techniques.", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "A closely related approach to ours is adaptive mixtures of local experts which uses a gating network to stochastically select the output from a set of feedforward networks [26].", "startOffset": 172, "endOffset": 176}, {"referenceID": 24, "context": "The reader is referred to [27] for a survey of mixture of expert methods.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "The products of experts approach models complex probability distributions by multiplying simpler distributions is also related [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 26, "context": "Mixture discriminant analysis treats each class as a mixture of Gaussians and learns discriminants between the Gaussians [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "Subclass discriminant analysis also relies on modeling classes as mixtures of Gaussians prior to learning discriminant [30].", "startOffset": 119, "endOffset": 123}, {"referenceID": 28, "context": "Local linear discriminant analysis clusters the data and learns a linear discriminant in each cluster [31].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "partitioning and supervised learning [32].", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "proposed an approach which places local classifiers close to the global decision boundary [33].", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "Toussaint and Vijayakumar propose a products-of-sigmoids model for discontinuously switching between local models [34].", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Another approach greedily builds a piecewise linear classifier by adding classifiers in regions of error clusters [35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 33, "context": "Local versions of SVMs have also been explored [36], [37].", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "Local versions of SVMs have also been explored [36], [37].", "startOffset": 53, "endOffset": 57}, {"referenceID": 35, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 262, "endOffset": 266}, {"referenceID": 36, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 268, "endOffset": 272}, {"referenceID": 37, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 274, "endOffset": 278}, {"referenceID": 38, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 280, "endOffset": 284}, {"referenceID": 39, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 286, "endOffset": 290}, {"referenceID": 40, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 292, "endOffset": 296}, {"referenceID": 41, "context": "These methods typically employ a clustering algorithm, learning classifiers between pairs of positive and negative clusters found by clustering, finally followed by a combination scheme such as voting to integrate the pairwise classifiers into a single decision [38], [39], [40], [41], [42], [43], [44].", "startOffset": 298, "endOffset": 302}, {"referenceID": 18, "context": "The modular network [21] discussed previously also falls into this category.", "startOffset": 20, "endOffset": 24}, {"referenceID": 42, "context": "Neural Networks [45], [46] (ConvNet).", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "Neural Networks [45], [46] (ConvNet).", "startOffset": 22, "endOffset": 26}, {"referenceID": 44, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 148, "endOffset": 152}, {"referenceID": 45, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 154, "endOffset": 158}, {"referenceID": 46, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 160, "endOffset": 164}, {"referenceID": 47, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 166, "endOffset": 170}, {"referenceID": 48, "context": "ConvNets have shown impressive results on many vision tasks including but not limited to classification, detection, localization and scene labeling [47], [48], [49], [50], [51].", "startOffset": 172, "endOffset": 176}, {"referenceID": 49, "context": "For example, the state-ofthe-art results for large 1000-category ImageNet [52] dataset was significantly improved using ConvNets [53].", "startOffset": 74, "endOffset": 78}, {"referenceID": 50, "context": "For example, the state-ofthe-art results for large 1000-category ImageNet [52] dataset was significantly improved using ConvNets [53].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "Usually, joint optimization of deep structures leads to a better solution or at least improves the result of layer-wise learning [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 46, "context": "[49].", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[50].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The idea of DropConnect is inspired by Dropout [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 48, "context": "is Maxout Networks [51].", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "We also compare the LDNN model with the modular neural networks(ModN)) [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Then, for each n \u2208 [1, 7], we trained 50 n \u00d7 n LDNNs starting from random parameter initializations, 50 n \u00d7 n LDNNs initialized from k-means clustering with n clusters per moon and 50 n \u00d7 n ModNs initialized from k-", "startOffset": 19, "endOffset": 25}, {"referenceID": 18, "context": "TESTING ERROR PERCENTAGES OVER 50 REPETITIONS FOR LDNN INITIALIZED WITH RANDOM PARAMETERS, INITIALIZED WITH CLUSTERING AND MODN [21] INITIALIZED WITH CLUSTERING FOR DIFFERENT MODEL SIZES.", "startOffset": 128, "endOffset": 132}, {"referenceID": 5, "context": "The two-spirals dataset is an extremely difficult dataset for the MLP architecture trained with the backpropagation algorithm [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 18, "context": "many fewer parameters than independent, pairwise learning of discriminants as in [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "For SVM training, each dimension of the feature vector was linearly scaled to the range [0, 1].", "startOffset": 88, "endOffset": 94}, {"referenceID": 51, "context": "For the IJCNN dataset cross-validation set was also used in training as in [58] and the number of epochs was fixed at 20.", "startOffset": 75, "endOffset": 79}, {"referenceID": 4, "context": "We tried a range of numbers around the square root of the number of features [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 11, "context": "For SVM training, a RBF kernel was used for all datasets except for the MNIST dataset for which a 9th degree polynomial kernel was used [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "For each dataset we trained the LDNN, RF and SVM classifiers with the exception of the MNIST dataset for which the SVM results are reported from [14].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "The LDNN classifier is also related to the idea of space partitioning [32] which combines partitioning of the space and learning a local classifier for each partition into a global objective function for supervised learning.", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "All space partitioning classifier results are reported from [32].", "startOffset": 60, "endOffset": 64}, {"referenceID": 43, "context": "MNIST [46] is probably the most popular dataset in the area of digit classification.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "58 \u2014 Results taken from [32] Landsat LDNN 2.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "95 \u2014 Results taken from [32] Letter LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "08 \u2014 Results taken from [32] Optdigit LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "23 \u2014 Results taken from [32] Pendigit LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "32 \u2014 Results taken from [32] MNIST LDNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "40 \u2014 Results taken from [14]", "startOffset": 24, "endOffset": 28}, {"referenceID": 29, "context": "THE SPACE PARTITIONING (SP) RESULTS ARE FROM [32].", "startOffset": 45, "endOffset": 49}, {"referenceID": 47, "context": "[50], [60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[50], [60].", "startOffset": 6, "endOffset": 10}, {"referenceID": 48, "context": "The solutions based on maxout Networks [51] (0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 52, "context": "45%) and stochastic pooling [60] (0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 47, "context": "027) DropConnect [50] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "035) Dropout [50] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 53, "context": "039) Scattering Networks [61] 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 48, "context": "43 (single model) Maxout Networks [51] 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 52, "context": "45 (single model) Stochastic Pooling [60] 0.", "startOffset": 37, "endOffset": 41}, {"referenceID": 47, "context": "The result of DropConnect [50] (0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 46, "context": "23% error rate reported in [49] obtained by voting of 35 single networks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 47, "context": "036) DropConnect [50] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "032) Dropout [50] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 46, "context": "016) MC-DNN [49] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 54, "context": "CIFAR10 [62] is a collection of 60000 tiny 32\u00d732 images of 10 categories (50000 for training and 10000 for test).", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "cessing for this dataset is to subtract the per pixel mean of the training set from every image [13].", "startOffset": 96, "endOffset": 100}, {"referenceID": 47, "context": "[50] and obtained by voting of 12 networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "The result of Maxout networks [51] obtained by a much bigger network which has 3 convolutional layers with 192, 384 and 384 maps respectively.", "startOffset": 30, "endOffset": 34}, {"referenceID": 47, "context": "18) DropConnect [50] (5 nets) 9.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "13) DropConnect [50] (12 nets) 9.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "32 (voting error) Dropout [50] 9.", "startOffset": 26, "endOffset": 30}, {"referenceID": 48, "context": "18) Maxout Networks [51] 9.", "startOffset": 20, "endOffset": 24}, {"referenceID": 46, "context": "38 (single model) MC-DNN [49] 11.", "startOffset": 25, "endOffset": 29}, {"referenceID": 55, "context": "NORB [63] is a collection of stereo images in 6 classes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 46, "context": "However, we scaled them down to 48\u00d748 similar to [49].", "startOffset": 49, "endOffset": 53}, {"referenceID": 47, "context": "[50] as shown in Table VII.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "13) DropConnect [50] 3.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "06) Dropout [50] 3.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "16) Multi-column DNN [49] 3.", "startOffset": 21, "endOffset": 25}, {"referenceID": 56, "context": "SVHN [64] is another digit classification task similar to MNIST.", "startOffset": 5, "endOffset": 9}, {"referenceID": 57, "context": "RGB channel of the input image in order to reduce the effect of variations of the images [65].", "startOffset": 89, "endOffset": 93}, {"referenceID": 47, "context": "[50] is obtained after 150 epochs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "037) DropConnect [50] 1.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "039) Dropout [50] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 54, "context": "CIFAR100 [62] is similar to CIFAR10, but it contains tiny images of 100 classes.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "While LDNNs are similar in architecture to modular neural networks [21], they are significantly more accurate owing to the unified training of the network that we introduced.", "startOffset": 67, "endOffset": 71}], "year": 2014, "abstractText": "Artificial neural networks are powerful pattern classifiers; however, they have been surpassed in accuracy by methods such as support vector machines and random forests that are also easier to use and faster to train. Backpropagation, which is used to train artificial neural networks, suffers from the herd effect problem which leads to long training times and limit classification accuracy. We use the disjunctive normal form and approximate the boolean conjunction operations with products to construct a novel network architecture. The proposed model can be trained by minimizing an error function and it allows an effective and intuitive initialization which solves the herdeffect problem associated with backpropagation. This leads to state-of-the art classification accuracy and fast training times. In addition, our model can be jointly optimized with convolutional features in an unified structure leading to state-of-the-art results on computer vision problems with fast convergence rates. A GPU implementation of LDNN with optional convolutional features is also available", "creator": "LaTeX with hyperref package"}}}