{"id": "1501.00037", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2014", "title": "Discriminative Clustering with Relative Constraints", "abstract": "We study the problem of clustering with relative constraints, where each constraint specifies relative similarities among instances. In particular, each constraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$ more similar to $x_j$ than to $x_k$? We consider the scenario where answers to such queries are based on an underlying (but unknown) class concept, which we aim to discover via clustering. Different from most existing methods that only consider constraints derived from yes and no answers, we also incorporate don't know responses. We introduce a Discriminative Clustering method with Relative Constraints (DCRC) which assumes a natural probabilistic relationship between instances, their underlying cluster memberships, and the observed constraints. The objective is to maximize the model likelihood given the constraints, and in the meantime enforce cluster separation and cluster balance by also making use of the unlabeled instances. We evaluated the proposed method using constraints generated from ground-truth class labels, and from (noisy) human judgments from a user study. Experimental results demonstrate: 1) the usefulness of relative constraints, in particular when don't know answers are considered; 2) the improved performance of the proposed method over state-of-the-art methods that utilize either relative or pairwise constraints; and 3) the robustness of our method in the presence of noisy constraints, such as those provided by human judgement.", "histories": [["v1", "Tue, 30 Dec 2014 22:34:24 GMT  (982kb)", "http://arxiv.org/abs/1501.00037v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuanli pei", "xiaoli z fern", "r\\'omer rosales", "teresa vania tjahja"], "accepted": false, "id": "1501.00037"}, "pdf": {"name": "1501.00037.pdf", "metadata": {"source": "CRF", "title": "Discriminative Clustering with Relative Constraints", "authors": ["Yuanli Pei", "Xiaoli Z. Fern", "R\u00f3mer Rosales", "Teresa Vania Tjahja"], "emails": ["tjahjat}@eecs.oregonstate.edu", "rrosales@linkedin.com"], "sections": [{"heading": null, "text": "This year, it has come to the point where it is only a matter of time before a decision is reached in which a decision is reached."}, {"heading": "II. PROBLEM ANALYSIS", "text": "Theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater.theater."}, {"heading": "B. Problem Statement", "text": "Let X = [x1,.., xN] T be the specified data, where each xi-Rd and d is the attribute dimension. Let Y = [y1,.., yN] T be the hidden cluster label vector, where yi is the label of xi. With slight misuse of the notation, we use {(t1, t2, t3)} Mt = 1 to denote the index set of M triplets representing M relative constraints. Each (t1, t2, t3) contains the indexes for the three instances in the t-bound constraint. Let L = [l1,..., lM] T be the constraint label vector, where lt: yes, no, dnk} is the label of (xt1, xt2, xt3). Each specifies the answer to the question: Is xt1 assigned to the constraint label vector, where lt: xt3 is more similar to the index than the xt3 is to our target, which are subgroups to share in the index."}, {"heading": "III. METHODOLOGY", "text": "In this section, we present our probabilistic model and present the proposed objective functions based on this model."}, {"heading": "A. The Probabilistic Model", "text": "We propose a discriminatory cluster model for Relative Constraints (DCRC). Figure 3 shows the proposed probabilistic model that defines the dependencies between instances (xt1, xt2, xt3), their cluster names (yt1, yt2, yt3), and the boundary name for only one relative constraint. Fora collection of constraints, it is possible to have y variables associated with more than one (or no) condition name l when some instances appear in multiple constraints (or do not appear in a given constraint). We use a multi-level logistic classification to model the conditional probability of observed x. In the following, we will use the same notation x to represent the (d + 1) dimensional augmentation of the vector."}, {"heading": "B. Objective", "text": "The first part of our goal is to maximize the probability of the observed constraints relative to the instances, i.e., max. (L | XI; W) = 1 M logP (L | XI; W) = 1 M log \u2211 YIP (L, YI | XI; W), (7) where I index the confined instances as defined in Section II-B, and 1M is a normalization constant. To reduce the overmatch, we also add the standard L-2 regularization for the logistic model, namely R (W) = 4, where each w-K is a vector that is separated by replacing the bias term in wk with 0.In addition to satisfying the constraints, we expect the clusters to be separated by large margins. This goal can be achieved by minimizing the conditional entropy of the instance cluster labels."}, {"heading": "IV. OPTIMIZATION", "text": "In this context, the question of the way in which the distribution mechanisms in each country are questioned in each country is significant; the question of the way in which the distribution mechanisms in each country are questioned is the question of the way in which the distribution mechanisms in each country are questioned; the question of the way in which the distribution mechanisms in each country are questioned in each country is the question of the way in which the distribution mechanisms in each country are highlighted in each country; the question of the way in which the distribution mechanisms in each country are distributed in each country is the question of the way in which the distribution mechanisms in each country are highlighted; the question of the way in which the distribution mechanisms in each country are distributed in each country is the question of the way in which the distribution mechanisms in each country are distributed; and the question of the way in which the distribution mechanisms in each country are distributed in each country is the question of the way in which the distribution mechanisms in each country are distributed; and the question of the way in which the distribution mechanisms in each country are distributed in each country is the way in which the distribution mechanisms in which the distribution mechanisms in each country are questioned; and the question of the way in which the way in which the distribution mechanisms in which the individual countries are questioned in which the distribution mechanisms in each country are discussed; and the question of the way in which the way in which the distribution mechanisms in which the distribution mechanisms in which the individual countries are discussed; and the question of the question of the way in which the distribution mechanisms in which the distribution mechanisms in which the countries are discussed in each country are discussed; and the question of the question of the way in which the way in which the distribution mechanisms in which the countries are discussed in each country are discussed in the"}, {"heading": "B. M-Step", "text": "The M step searches for the parameter W that maximizes the LB. Applying the assumptions of independence again and ignoring all terms that are constant in relation to W, we obtain the following objective maxim WJ = 1M \u2211 YIQ (YI) logP (YI | XI; W) \u2212 \u03bbR (W) + \u03c4 [H (Y | X; W) \u2212 H (YU | XU; W)]. This objective is not concave and a local optimum can be found via the gradient ascent. We used L-BFGS [25] in our experiments. The derivative of J w.r.t. W is: W = 1 M \u0445 i (Qi \u2212 Pi) x T i \u2212 2\u03bbW +."}, {"heading": "C. Complexity and Initialization", "text": "In each E-step, the complexity of calculating the gradient of W in each L-BFGS iteration is O (NKD). Although the approximation of the center field is guaranteed to converge, it is not decisive in the first E-steps to achieve a very close approximation. In practice, we can perform the update of the center field up to a fixed number of iterations (e.g. 100). We observe empirically that the approximation in later EM iterations still converges very quickly. Similarly, in the M-step, we observe that the LBFGS optimization normally converges with very few iterations in later EM runs, and completing a fixed number of iterations for L-BFGS is also sufficient. In general, the EM algorithm is sensitive to the initial CRR values that we usually use as a starting point."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we will experimentally examine the effectiveness of our model in using relative constraints to improve clustering. First, we evaluate all methods both on UCI data sets and on other real-world data sets with noise-free constraints generated from real class labels. Then, we present a preliminary user study in which we ask users to label constraints and evaluate all methods based on these humanely marked (loud) constraints."}, {"heading": "A. Baseline Methods and Evaluation Metric", "text": "We compare our algorithm with existing methods that take into account relative constraints or pair constraints, and the methods that apply pair constraints are Xing's method [2] (distance metric learning for a diagonal matrix) and ITML [26], which are the most modern methods that are normally compared in literature and have publicly available source code.For methods that take into account relative constraints, we compare with: 1) LSML [15], a very young metric learning method that examines relative constraints (we use Euclidean distance as the previous one); 2) SSSVaD [16], a method that directly finds cluster solutions with relative constraints; and 3) sparseLP [13], an earlier method that was not comprehensively compared. We also experimented with an SVM-like method proposed in [12], and observed that its performance is generally worse."}, {"heading": "B. Controlled Experiments", "text": "In this series of experiments, we use simulated disturbances to evaluate all the methods we use for each individual image. (That is, we have used only three additional real datasets: 1) a subset of image segments of the MSRCv2 data1, which comprise the six largest classes of image segments; 2) the HJA Birdsong data [27], which contains automatically extracted segments from spectrograms of Birdsong records, and the goal is to identify the species for each segment; and 3) the Stonefly9 data, which contains insect images and the task is to identify the species of insects for each image. Table III summarizes the dataset information."}, {"heading": "C. Case Study: Human-labeled Constraints", "text": "In fact, it is the case that we are able to get to grips with the problems mentioned in order to solve them."}, {"heading": "VI. RELATED WORK", "text": "The work in [12] - [16] suggests metric learning approaches that use d (xi, xj) < d (xi, xk) to encode that xi is more similar to xj than xk, where d (\u00b7) is the distance function. Work [15] examines learning from relative comparisons between two instances, which can be considered the same kind of constraints when only three different examples are involved. By constructing these methods only consider constraints with yes / no labels, where d (\u00b7) is the distance function. In practical terms, such responses are not always set to cluster form, which restricts their application. In contrast, our hierarchy is more flexible, allowing users to provide large constraints that also do not exist in terms of classifications."}, {"heading": "VII. CONCLUSIONS", "text": "In contrast to existing methods, which only consider yes / no answers to such questions, we investigated the case where the answer could also be dnk (does not know). We developed a probabilistic method DCRC, which learns to cluster the instances based on the answers to such questions. We evaluated the proposed method empirically using both simulated (noise-free) and human-labelled (noise) constraints. The results showed the usefulness of dnk constraints, the significantly improved performance of DCRC over existing methods, and the superiority of our method in terms of robustness over loud constraints."}], "references": [{"title": "Constrained Kmeans Clustering with Background Knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "ICML, 2001, pp. 577\u2013584.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Distance Metric Learning with Application to Clustering with Side-information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"], "venue": "NIPS, 2003, pp. 521\u2013528.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Integrating Constraints and Metric Learning in Semi-supervised Clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "ICML, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Computing Gaussian Mixture Models with EM using Equivalence Constraints", "author": ["N. Shental", "A. Bar-hillel", "T. Hertz", "D. Weinshall"], "venue": "NIPS, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised Learning with Penalized Probabilistic Clustering", "author": ["Z. Lu", "T.K. Leen"], "venue": "NIPS, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "A Probabilistic Framework for Semi-supervised Clustering", "author": ["S. Basu", "M. Bilenko", "R.J. Mooney"], "venue": "KDD, 2004, pp. 59\u201368.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with Constrained and Unlabelled Data", "author": ["T. Lange", "M.H.C. Law", "A.K. Jain", "J.M. Buhmann"], "venue": "CVPR, 2005, pp. 731\u2013738.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Revisiting Probabilistic Models for Clustering with Pair-wise Constraints", "author": ["B. Nelson", "I. Cohen"], "venue": "ICML, 2007, pp. 673\u2013680.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Constraint-Driven Clustering", "author": ["R. Ge", "M. Ester", "W. Jin", "I. Davidson"], "venue": "KDD, 2007, pp. 320\u2013329.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised Clustering with Pairwise Constraints: A Discriminative Approach", "author": ["Z. Lu"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 299\u2013306, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Constrained Clustering: Advances in Algorithms, Theory, and Applications, 1st ed", "author": ["S. Basu", "I. Davidson", "K. Wagstaff"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Learning a Distance Metric from Relative Comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "NIPS, 2003, p. 41.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning Sparse Metrics via Linear Programming", "author": ["R. Rosales", "G. Fung"], "venue": "KDD, 2006, pp. 367\u2013373.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalized Sparse Metric Learning with Relative Comparisons", "author": ["K. Huang", "Y. Ying", "C. Campbell"], "venue": "Knowl. Inf. Syst., vol. 28, no. 1, pp. 25\u201345, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Metric Learning from Relative Comparisons by Minimizing Squared Residual", "author": ["E.Y. Liu", "Z. Guo", "X. Zhang", "V. Jojic", "W. Wang"], "venue": "ICDM, 2012, pp. 978\u2013983.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Semisupervised Clustering with Metric Learning using Relative Comparisons", "author": ["N. Kumar", "K. Kummamuru"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 20, no. 4, pp. 496\u2013503, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering with Relative Constraints", "author": ["E. Liu", "Z. Zhang", "W. Wang"], "venue": "KDD, 2011, pp. 947\u2013955.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised Learning by Entropy Minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "NIPS, 2005, pp. 33\u201340.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Discriminative Clustering by Regularized Information Maximization", "author": ["R. Gomes", "A. Krause", "P. Perona"], "venue": "NIPS, 2010, pp. 775\u2013783.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Mean Field Theory for Sigmoid Belief Networks", "author": ["L. Saul", "T. Jaakkola", "M. Jordan"], "venue": "Journal of Artificial Intelligence Research, vol. 4, pp. 61\u201376, 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "A tutorial on Variational Bayesian Inference", "author": ["C.W. Fox", "S.J. Roberts"], "venue": "Artificial Intelligence Review, vol. 38, no. 2, pp. 85\u201395, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "On Mean Field Convergence and Stationary Regime", "author": ["M. Benaim", "J.-Y.L. Boudec"], "venue": "arXiv preprint arXiv:1111.5710, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "L-bfgs software", "author": ["M. Schmidt"], "venue": "Website, 2012, http://www.di.ens.fr/\u223cmschmidt/Software/minFunc.html.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Information- Theoretic Metric Learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML, 2007, pp. 209\u2013216.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Rank-loss Support Instance Machines for MIML Instance Annotation", "author": ["F. Briggs", "X.Z. Fern", "R. Raich"], "venue": "KDD, 2012, pp. 534\u2013 542.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Dictionary-Free Categorization of Very Similar Objects via Stacked Evidence Trees", "author": ["G. Martinez-Munoz", "N. Larios", "E. Mortensen", "W. Zhang", "A. Yamamuro", "R. Paasch", "N. Payet", "D. Lytle", "L. Shapiro", "S. Todorovic", "A. Moldenke", "T. Dietterich"], "venue": "CVPR, 2009, pp. 549 \u2013556.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical Constraints", "author": ["K. Bade", "A. N\u00fcrnberger"], "venue": "Machine Learning, pp. 1\u201329, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", [1]\u2013[11]), where a pair of instances is indicated to belong to the same cluster by a Must-Link (ML) constraint or to different clusters by a Cannot-Link (CL) constraint.", "startOffset": 2, "endOffset": 5}, {"referenceID": 10, "context": ", [1]\u2013[11]), where a pair of instances is indicated to belong to the same cluster by a Must-Link (ML) constraint or to different clusters by a Cannot-Link (CL) constraint.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "More recently, various studies [12]\u2013[17] have suggested that domain knowledge can also be incorporated in the form of relative comparisons or relative constraints, where each constraint specifies whether instance xi is more similar to xj than to xk .", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "More recently, various studies [12]\u2013[17] have suggested that domain knowledge can also be incorporated in the form of relative comparisons or relative constraints, where each constraint specifies whether instance xi is more similar to xj than to xk .", "startOffset": 36, "endOffset": 40}, {"referenceID": 11, "context": "In the area of learning from relative constraints, most work uses metric learning approaches [12]\u2013[16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "In the area of learning from relative constraints, most work uses metric learning approaches [12]\u2013[16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "This objective can be captured by minimizing the conditional entropy of instance cluster labels given the observed features [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "This can be achieved by maximizing the entropy of the estimated marginal distribution of cluster labels [20], i.", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "We use mean field inference [22], [23] to approximate the posterior distribution in part due to its ease of implementation and convergence properties [24].", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "We use mean field inference [22], [23] to approximate the posterior distribution in part due to its ease of implementation and convergence properties [24].", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "We use mean field inference [22], [23] to approximate the posterior distribution in part due to its ease of implementation and convergence properties [24].", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "We used L-BFGS [25] in our experiments.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "The methods employing pairwise constraints are Xing\u2019s method [2] (distance metric learning for a diagonal matrix) and ITML [26].", "startOffset": 61, "endOffset": 64}, {"referenceID": 23, "context": "The methods employing pairwise constraints are Xing\u2019s method [2] (distance metric learning for a diagonal matrix) and ITML [26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "For methods considering relative constraints, we compare with: 1) LSML [15], a very recent metric learning method studying relative constraints (we use Euclidean distance as the prior); 2) SSSVaD [16], a method that directly finds clustering solutions with relative constraints; and 3) sparseLP [13], an earlier method that hasn\u2019t been extensively compared.", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "For methods considering relative constraints, we compare with: 1) LSML [15], a very recent metric learning method studying relative constraints (we use Euclidean distance as the prior); 2) SSSVaD [16], a method that directly finds clustering solutions with relative constraints; and 3) sparseLP [13], an earlier method that hasn\u2019t been extensively compared.", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "For methods considering relative constraints, we compare with: 1) LSML [15], a very recent metric learning method studying relative constraints (we use Euclidean distance as the prior); 2) SSSVaD [16], a method that directly finds clustering solutions with relative constraints; and 3) sparseLP [13], an earlier method that hasn\u2019t been extensively compared.", "startOffset": 295, "endOffset": 299}, {"referenceID": 11, "context": "We also experimented with a SVM-style method proposed in [12] and observed that its performance is generally worse.", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": "We evaluated the clustering results based on the ground-truth class labels using pairwise F-measure [3], Adjusted Rand Index and Normalized Mutual Information.", "startOffset": 100, "endOffset": 103}, {"referenceID": 24, "context": "We also use three extra real-world datasets: 1) a subset of image segments of the MSRCv2 data1, which contains the six largest classes of the image segments; 2) the HJA Birdsong data [27], which contains automatically extracted segments from spectrograms of birdsong recordings, and the goal is to identify the species for each segment; and 3) the Stonefly9 data [28], which contains insect images and the task is to identify the species of the insect for each image.", "startOffset": 183, "endOffset": 187}, {"referenceID": 25, "context": "We also use three extra real-world datasets: 1) a subset of image segments of the MSRCv2 data1, which contains the six largest classes of the image segments; 2) the HJA Birdsong data [27], which contains automatically extracted segments from spectrograms of birdsong recordings, and the goal is to identify the species for each segment; and 3) the Stonefly9 data [28], which contains insect images and the task is to identify the species of the insect for each image.", "startOffset": 363, "endOffset": 367}, {"referenceID": 24, "context": "We extract features for each segment using the same method as described in [27].", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "RELATED WORK Clustering with Constraints: Various techniques have been proposed for clustering with pairwise constraints [4]\u2013[8], [10].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "RELATED WORK Clustering with Constraints: Various techniques have been proposed for clustering with pairwise constraints [4]\u2013[8], [10].", "startOffset": 125, "endOffset": 128}, {"referenceID": 9, "context": "RELATED WORK Clustering with Constraints: Various techniques have been proposed for clustering with pairwise constraints [4]\u2013[8], [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "The work in [12]\u2013[16] propose metric learning approaches that use d(xi, xj) < d(xi, xk) to encode that xi is more similar to xj than to xk , where d(\u00b7) is the distance function.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "The work in [12]\u2013[16] propose metric learning approaches that use d(xi, xj) < d(xi, xk) to encode that xi is more similar to xj than to xk , where d(\u00b7) is the distance function.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The work [15] studies learning from relative comparisons between two pairs of instances, which can be viewed as the same type of constraints when only three distinct examples are involved.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "In contrast, our method is more flexible by allowing users to provide dnk constraints, There also exist studies that encode the instance relative similarities in the form of hieratical ordering and attempt hierarchical algorithms that directly find clustering solutions satisfying the constraints [17], [29].", "startOffset": 297, "endOffset": 301}, {"referenceID": 26, "context": "In contrast, our method is more flexible by allowing users to provide dnk constraints, There also exist studies that encode the instance relative similarities in the form of hieratical ordering and attempt hierarchical algorithms that directly find clustering solutions satisfying the constraints [17], [29].", "startOffset": 303, "endOffset": 307}, {"referenceID": 17, "context": "The work [19] proposes that to enforce the formed clusters with large separation margins, we could minimize the entropy on the unlabeled data, in addition to learning from the labeled ones.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "The study [20] suggests to also maximize the entropy of the cluster label distribution in order to find balanced clustering solution.", "startOffset": 10, "endOffset": 14}], "year": 2015, "abstractText": "We study the problem of clustering with relative constraints, where each constraint specifies relative similarities among instances. In particular, each constraint (xi, xj , xk) is acquired by posing a query: is instance xi more similar to xj than to xk? We consider the scenario where answers to such queries are based on an underlying (but unknown) class concept, which we aim to discover via clustering. Different from most existing methods that only consider constraints derived from yes and no answers, we also incorporate don\u2019t know responses. We introduce a Discriminative Clustering method with Relative Constraints (DCRC) which assumes a natural probabilistic relationship between instances, their underlying cluster memberships, and the observed constraints. The objective is to maximize the model likelihood given the constraints, and in the meantime enforce cluster separation and cluster balance by also making use of the unlabeled instances. We evaluated the proposed method using constraints generated from ground-truth class labels, and from (noisy) human judgments from a user study. Experimental results demonstrate: 1) the usefulness of relative constraints, in particular when don\u2019t know answers are considered; 2) the improved performance of the proposed method over state-of-theart methods that utilize either relative or pairwise constraints; and 3) the robustness of our method in the presence of noisy constraints, such as those provided by human judgement.", "creator": "LaTeX with hyperref package"}}}