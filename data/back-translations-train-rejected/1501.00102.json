{"id": "1501.00102", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2014", "title": "ModDrop: adaptive multi-modal gesture recognition", "abstract": "We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.", "histories": [["v1", "Wed, 31 Dec 2014 09:55:43 GMT  (7779kb)", "http://arxiv.org/abs/1501.00102v1", "14 pages, 7 figures"], ["v2", "Sat, 6 Jun 2015 14:46:33 GMT  (3382kb,D)", "http://arxiv.org/abs/1501.00102v2", "14 pages, 7 figures"]], "COMMENTS": "14 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CV cs.HC cs.LG", "authors": ["natalia neverova", "christian wolf", "graham w taylor", "florian nebout"], "accepted": false, "id": "1501.00102"}, "pdf": {"name": "1501.00102.pdf", "metadata": {"source": "CRF", "title": "ModDrop: adaptive multi-modal gesture recognition", "authors": ["Natalia Neverova", "Christian Wolf", "Graham Taylor", "Florian Nebout"], "emails": ["firstname.surname@liris.cnrs.fr"], "sections": [{"heading": null, "text": "ar Xiv: 150 1.00 102v 1 [cs.C V] 31 Dec 201 4Index Terms - Gesture Recognition, Convolutionary Neural Networks, Multimodal Learning, Depth Learning"}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2 RELATED WORK", "text": "This year, the time has come for a realignment."}, {"heading": "3 GESTURE CLASSIFICATION", "text": "On a data set like ChaLearn 2014 LAP, we face several central challenges: we learn representations at multiple spatial and temporal levels, integrate the various modalities, and train a complex model when the number of examples described is not at web level like static image records (e.g. [3]); we start by describing how to master the first two challenges at the architectural level; our training strategy, which addresses the last problem, is in Sec. 4.Our proposed deep neural network consists of a combination of parallel paths (see Figure 2); each path learns a representation and performs a gesture classification at its own temporal level."}, {"heading": "3.1 Articulated pose", "text": "The complete body skeleton, provided by modern consumer cameras and associated middleware, consists of 20 or fewer joints identified by their coordinates in a 3D coordinate system with the depth sensor. For our purposes, we use only 11 joints corresponding to the torso. We formulate a pose descriptor consisting of 7 logical subgroups, as in [49]. After [50], we first calculate normalized joint positions, as well as their velocities and accelerations, and then expand the descriptor with a series of characteristic angles and pairs of distances. The skeleton is presented as a tree structure with the HipCenter joint playing the role of a root knot. Its coordinates are subtracted from the rest of the vectors to eliminate the influence of the body."}, {"heading": "3.2 Depth and intensity video", "text": "Two video streams serve as a source of information about hand posture and finger articulation. Bounding boxes containing images of hands are clipped around positions of the right and left hand joints. To eliminate the influence of the person's position with respect to the camera and keep the hand size roughly constant, the size of each bounding box is normalized by the distance between the hand and the sensor. Within each group of frames that form a dynamic pose, the hand position is stabilized by minimizing interframe square foot distances, which are calculated as the sum of all pixels, and corresponding frames are linked to form a single spatial-time volume. Color flow is converted to grayscale, and both depth and intensity frames are normalized to zero mean and unit variance. On the left, videos are rotated around the vertical axis and combined with right-sided distances in a single training group."}, {"heading": "3.3 Audio stream", "text": "Recent advances in speech processing have shown that the use of poorly preprocessed raw audio data in combination with deep learning results in higher performance compared to state-of-the-art systems based on handcrafted features (typically from the mellow frequency receiver coefficient or MFCC family). Deng et al. [51] demonstrated the advantage of primitive spectral features, such as 2D spectrograms, in combination with deep autoencoders. Ngiam et al. [46] applied the same strategy to the task of multimodal speech recognition while augmenting the audio signal with visual features. Further experiments by Microsoft [51] have shown that ConvNets seem to be particularly efficient in this context, as they allow for the acquisition and modelling of structure and inventories typical of language skills. Comparative analysis of our previous approach [48] based on phoneme recognition sequences from MFCC performance data has shown us to achieve a much deeper learning strategy based on the latter."}, {"heading": "4 TRAINING PROCEDURE", "text": "This year is the highest in the history of the country."}, {"heading": "5 INTER-SCALE FUSION DURING TEST TIME", "text": "As soon as individual single-stage predictions are available, we apply a simple coordination strategy for fusion with a single weight per model. We point out here that the introduction of additional weights per class per model and training meta-classifiers (such as an MLP) quickly leads to overmatch in this step. For each given frame t, network results per frame are achieved by aggregating and time filtering predictions on each scale with corresponding weights around the empirically defined microbes: ok (t) = 4 \u2211 s = 2\u00b5s0 \u2211 j = \u2212 4sos, k (t + j), (19) where os, k (t + j) is the value of class k determined for a spatio-time block starting from frame t + j in step s. Finally, the class name l (t) is assigned to the frame with the maximum score: l (t) = argmaxk (t)."}, {"heading": "6 GESTURE LOCALIZATION", "text": "With increasing duration of a dynamic gesture, the detection rate of the classifier increases at the expense of precision in localizing the gestures. The use of larger sliding windows leads to loud predictions in the pre-stroke and post-stroke phase due to the simultaneous overlapping of several gestures. On the other hand, too short dynamic poses are also not discriminatory, since most gesture classes have a similar appearance in their initial and final phases (e.g. raising or lowering of hands).To solve this problem, we introduce an additional binary classifier that distinguishes rest moments from activity phases. This classifier is able to precisely locate the start and end points of each gesture. The module is a two-tiered, fully connected network that uses the articulated pose descriptor as an input. All training frames with a gesture designation in the finest temporal resolution s = 1 are used as positive examples, while a series of frames immediately preceding a frame and therefore are considered as a frame with no such motion in the first class. \""}, {"heading": "7 EXPERIMENTS", "text": "The Chalearn 2014 Looking at People Challenge (Track 3) [13] dataset includes 13,858 cases of Italian gestures performed by different people and recorded with an RGB-D sensor. It includes colour, video and mocap streams; the gestures come from a large vocabulary from which 20 categories are identified to detect and recognise them, and the rest are considered arbitrary movements; each gesture in the training set is accompanied by a Ground Truth Label and information on their start and end points; for the challenge, the corpus was divided into development, validation and test kits; the test data was released to participants after submitting their source code. To further explore the dynamics of learning in multimodal systems, we supplemented the data with audio recordings from a dataset published as part of Chalearn 2013."}, {"heading": "7.1 Experimental setup", "text": "All hidden units in the classification and localization modules have hyperbolic tangent activations. Hyperparameters have been optimized on the validation data with early stop to prevent the models from overlapping and without additional regulation. For convenience, Fusion 10 weights are set to \u00b5s = 1 for the various timescales, as well as the weight of the base model (see Section 5). The deep learning architecture is implemented using the Theano library. A single-scale predictor operates at frame rates close to real time (24 fps on the GPU). We followed the evaluation process proposed by the challenge organizers and adopted the Jaccard index to quantify model performance: Js, n = As, n \u00b2 Bs, n \u00b2 s, BPU)."}, {"heading": "7.2 Baseline models", "text": "In addition to the main pipeline, we have implemented a base model based on an ensemble classifier trained in a similar iterative manner, but based on purely handmade descriptors. The purpose of this comparison was to examine the relative advantages and disadvantages of using learned representations as well as the nuances of fusion. We also found it advantageous to combine the proposed deep network with the base method in a hybrid model (see Table 5).The base line used for visual models is described in detail in [49].We use depth and intensity hand images and extract three sets of characteristics. HoG characteristics describe hand posture in the image plane. Depth histograms describe posture along the third spatial dimension. The third set of characteristics consists of derivatives of HOGs and depth histograms that reflect the temporal dynamics of the hand shape. Extremely randomized trees (ERT) [are used] for data classification in the traditional architecture, as we used in the Gestural [58] for data classification and during the tracing process."}, {"heading": "7.3 Results on the ChaLearn 2014 LAP dataset", "text": "The top 10 results of the ChaLearn 2014 LAP Challenge (Track 3) are listed in Table 2. Our winning entry [49], which corresponds to a hybrid model (i.e. a combination of the proposed deep neural architecture and the ERT base model), outperforms the second best by a margin of 1.61 percentage points. We also point out that the multi-scale neural architecture still performs best, as does the top single-scale neural model alone (see Tables 3 and 5). In the work following the challenge, we were able to improve the score by a further 2.0 percentage points to 0.870 by adding additional capacities to the model, optimizing the architectures of the video and skeleton paths, and using an advanced training and fusion method that was not used for the challenge. Detailed information on the performance of neural architectures for each modality and for each scale is provided in Table 3, including multimodal setting and performations testing."}, {"heading": "7.4 Results on the ChaLearn 2014 LAP dataset augmented with audio", "text": "In order to show how the proposed model can be extended further with any gesture modalities, we introduce the language into the existing setup. In this setting, each gesture in the data set is accompanied by a word or a short phrase expressing the same meaning and uttered by each actor during the execution of the gesture. As expected, the introduction of a new data channel led to a significant increase in classification performance (1.3 points on the Jaccard index, see Table 3). As with the other modalities, an audio-specific neural network was initially pre-trained discriminatively on the audio data alone. Next, the same fusion procedure was applied without any change. In this case, the quality of the predictions generated by the audio path depends on the timing frequency: the best performance was achieved for dynamic poses lasting 0.5 s (see Table 3)."}, {"heading": "7.5 Impact of the different fusion strategies", "text": "We will examine the relative benefits of different training strategies, starting with preliminary experiments with the MNIST dataset [66] and then a more comprehensive analysis of the ChaLearn 2014 dataset, which has been extended to include audio."}, {"heading": "7.5.1 Preliminary experiments on MNIST dataset", "text": "As a sanitary check of the ModDrop Fusion, we transform the MNIST data sets [66] to imitate multimodal data. A classic deep-learning benchmark, MNIST consists of 28 \u00d7 28 gray zone images of handwritten digits in which 60k images are used for testing. We use the original version without data interchangeability. We also avoid any data pre-processing and apply a simple architecture: a multilayer perceptor with two hidden layers (i.e. no revolutionary layers).12We cut each digital image into 4 quarters and assume that each quarter corresponds to a modality (see Fig. 7). Despite the apparent simplicity of this formulation, we show that the results obtained reflect the dynamics of a real layer."}, {"heading": "7.5.2 Experiments on ChaLearn 2014 LAP with audio", "text": "In a real multimodal environment, the optimization and balancing of a tree-structured architecture is an extremely difficult task, as its separate parallel paths vary in complexity and operate on different characteristic spaces. The problem becomes even harder under the constraint of real-time performance and consequently the limited capacity of the network. Our experiments have shown that insufficient modelling capacity of one of the modality-specific subnets leads to a drastic deterioration of the performance of the entire system due to the multiplicative nature of the merger process. These bottlenecks are typically difficult to find without thorough pro-channel testing. We propose to start by optimizing the architecture and hyperparameters for each modality individually through discriminatory accusations. During the input paths values are initialized with pre-rehearsed values and fine-tuning the output-shared layers during training. In addition, the common layers can be initialized with pre-formed blocks."}, {"heading": "8 CONCLUSION", "text": "Each of the visual modalities captures spatial information on a specific spatial scale (such as the movement of the upper body or a hand), and the entire system operates on two time scales. The model can be further extended (depending on the sensors available) and expanded to include any channels, introducing additional parallel paths without significant changes to the overall structure. We illustrate this concept by adding language to video. Multiple spatial and time scales per channel can be easily integrated. Finally, we have examined various aspects of multimodal fusion in terms of shared performance across a complete set of modalities, as well as the robustness of the classifier in terms of noise and waste of one or more data channels. As a result, we have proposed a modal control strategy (ModDrop) that enables our model to obtain stable predictions even when inputs are damaged."}, {"heading": "Acknowledgement", "text": "This work was partly financed by the French Interabot Scholarship, a project of the type \"Investissement's d'Avenir / Briques Ge'ne'riques du Logiciel Embarque.\""}], "references": [{"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "ICLR, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Hierarchical Features for Scene Labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Indoor Semantic Segmentation using depth information", "author": ["C. Couprie", "F. Cl\u00e9ment", "L. Najman", "Y. LeCun"], "venue": "ICLR, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "c. G\u00fcl\u00e7ehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio"], "venue": "ICMI, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Y. Taigman", "M. Yang", "M.A. Ranzato", "L. Wolf"], "venue": "CVPR, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatio-Temporal Convolutional Sparse Auto-Encoder for Sequence Classification", "author": ["M. Baccouche", "F. Mamalet", "C. Wolf", "C. Garcia", "A. Baskurt"], "venue": "BMVC, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale Video Classification with Convolutional Neural Networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "F.- F. Li"], "venue": "CVPR, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-Stream Convolutional Networks for Action Recognition in Videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "inarXiv:1406.2199v1, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation", "author": ["A. Jain", "J. Tompson", "Y. LeCun", "C. Bregler"], "venue": "ACCV, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "ChaLearn Looking at People Challenge 2014: Dataset and Results", "author": ["S. Escalera", "X. Bar\u00f3", "J. Gonz\u00e0lez", "M. Bautista", "M. Madadi", "M. Reyes", "V. Ponce", "H. Escalante", "J. Shotton", "I. Guyon"], "venue": "ECCVW, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Dense trajectories and motion boundary descriptors for action recognition", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "IJCV, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation of local spatio-temporal features for action recognition", "author": ["H. Wang", "M.M. Ullah", "A. Klaser", "I. Laptev", "C. Schmid"], "venue": "BMVC, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Behavior Recognition via Sparse Spatio-Temporal Features", "author": ["P. Doll\u00e1r", "V. Rabaud", "G. Cottrell", "S. Belongie"], "venue": "VS-PETS, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marsza\u0142ek", "C. Schmid", "B. Rozenfeld"], "venue": "CVPR, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A spatio-temporal descriptor based on 3D-gradients", "author": ["A. Kl\u00e4ser", "M. Marsza\u0142ek", "C. Schmid"], "venue": "BMVC, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "AnEfficientDenseand Scale- Invariant Spatio-Temporal Interest Point Detector", "author": ["G. Willems", "T. Tuytelaars", "L. Gool"], "venue": "ECCV, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "A. Fitzgibbon", "M. Cook", "T. Sharp", "M. Finocchio", "R. Moore", "A. Kipman", "A. Blake"], "venue": "CVPR, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Real time hand pose estimation using depth sensors", "author": ["C. Keskin", "F. Kira\u00e7", "Y. Kara", "L. Akarun"], "venue": "ICCV Workshop, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time Articulated Hand Pose Estimation using Semi-supervised Transductive Regression Forests", "author": ["D. Tang", "T.-H. Yu", "T.-K. Kim"], "venue": "ICCV, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks", "author": ["J. Tompson", "M. Stein", "Y. LeCun", "K. Perlin"], "venue": "ACM Transaction on Graphics, 2014.  14", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Hand segmentation with structured convolutional learning", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "ACCV, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient model-based 3D tracking of hand articulations using Kinect", "author": ["I. Oikonomidis", "N. Kyriazis", "A. Argyros"], "venue": "BMVC, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Realtime and Robust Hand Tracking from Depth", "author": ["C. Qian", "X. Sun", "Y. Wei", "X. Tang", "J. Sun"], "venue": "CVPR, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture", "author": ["D. Tang", "H.J. Chang", "A. Tejani", "T.-K. Kim"], "venue": "CVPR, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond Physical Connections: Tree Models in Human Pose Estimation", "author": ["F. Wang", "Y. Li"], "venue": "CVPR, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining actionlet ensemble for action recognition with depth cameras", "author": ["J. Wang", "Z. Liu", "Y. Wu", "J. Yuan"], "venue": "CVPR, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Unstructured Human Activity Detection from RGBD Images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "ICRA, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Online RGB-D gesture recognition with extreme learning machines", "author": ["X. Chen", "M. Koskela"], "venue": "ICMI, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A Multi-scale Boosted Detector for Efficient and Robust Gesture Recognition", "author": ["C. Monnier", "S. German", "A. Ost"], "venue": "ECCVW, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric Gesture Labeling from Multi-modal Data", "author": ["J.Y. Chang"], "venue": "ECCV Workshop, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "A Multi-modal Gesture Recognition System Using Audio, Video, and Skeletal Joint Data Categories and Subject Descriptors", "author": ["K. Nandakumar", "W.K. Wah", "C.S.M. Alice", "N.W.Z. Terence", "W.J. Gang", "Y.W. Yun"], "venue": "ICMI Workshop, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "CVPR, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "CVPR, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep learning of invariant Spatio-Temporal Features from Video", "author": ["B. Chen", "J.-A. Ting", "B. Marlin", "N. de Freitas"], "venue": "NIPSW, 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "3D Convolutional Neural Networks for Human Action Recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "PAMI, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiple Kernel Learning, Conic Duality, and the SMO Algorithm", "author": ["F. Bach", "G. Lanckriet", "M. Jordan"], "venue": "ICML, 2004.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "On Feature Combination for Multiclass Object Classification", "author": ["P. Gehler", "S. Nowozin"], "venue": "ICCV, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust Late Fusion With Rank Minimization", "author": ["G. Ye", "D. Liu", "I.-H. Jhuo", "S.-F. Chang"], "venue": "CVPR, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Sample-Specific Late Fusion for Visual Category Recognition", "author": ["D. Liu", "K.-T. Lai", "G. Ye", "M. Chen", "S. Chang"], "venue": "CVPR, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "FeatureWeighting via Optimal Thresholding for Video Analysis", "author": ["Z. Xu", "Y. Yang", "I. Tsang", "N. Sebe", "A. Hauptmann"], "venue": "ICCV, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal Feature Fusion for Robust Event Detection in Web Videos", "author": ["P. Natarajan", "S. Wu", "S. Vitaladevuni", "X. Zhuang", "S. Tsakalidis", "U. Park", "R. Prasad", "P. Natarajan"], "venue": "CVPR, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kin", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal learning with Deep Boltzmann Machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "A multi-scale approach to gesture detection and recognition", "author": ["N. Neverova", "C. Wolf", "G. Paci", "G. Sommavilla", "G.W. Taylor", "F. Nebout"], "venue": "ICCV Workshop, 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-scale deep learning for gesture detection and localization", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "ECCVW, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection", "author": ["M. Zanfir", "M. Leordeanu", "C. Sminchisescu"], "venue": "ICCV, 2013.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Recent advances in deep learning for speech recognition at Microsoft", "author": ["L. Deng", "J. Li", "J. Huang", "K. Yao", "D. Yu", "F. Seide", "M. Seltzer", "G. Zweig", "X. He", "J. Williams", "Y. Gong", "A. Acero"], "venue": "ICASSP, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "On combining classifiers using sum and product rules", "author": ["L.A. Alexandre", "A.C. Campilho", "M. Kamel"], "venue": "Pattern Recognition Letters, no. 22, 2001, pp. 1283\u20131289.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2001}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv:1302.4389v4, 2013.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "The dropout learning algorithm", "author": ["P. Baldi", "P. Sadowski"], "venue": "Journal of Artificial Intelligence, vol. 210, pp. 78\u2013122, 2014.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv:1207.0580, 2012.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast dropout training", "author": ["S. Wang", "C. Manning"], "venue": "ICML, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Elements of Large-Sample Theory", "author": ["E.L. Lehmann"], "venue": "ICML, 1998.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1998}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine learning, 63(1), 3-42, 2006.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "Julius - an open source realtime large vocabulary recognition engine", "author": ["A. Lee", "T. Kawahara", "K. Shikano"], "venue": "Interspeech, 2001.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2001}, {"title": "Gesture Recognition using Template Based Random Forest Classifiers", "author": ["N. Camgoz", "A. Kindiroglu", "L. Akarun"], "venue": "ECCVW, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous gesture recognition from articulated poses", "author": ["G. Evangelidis", "G. Singh", "R. Horaud"], "venue": "ECCV Workshop, 2014.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Action and Gesture Temporal Spotting with Super Vector Representation", "author": ["X. Peng", "L. Wang", "Z. Cai"], "venue": "ECCVW, 2014.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-modality Gesture Detection and Recognition With Unsupervision, Randomization and Discrimination", "author": ["G. Chen", "D. Clarke", "M. Giuliani", "D. Weikersdorfer", "A. Knoll"], "venue": "ECCVW, 2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Sign Language Recognition Using Convolutional Neural Networks", "author": ["L. Pigou", "S. Dieleman", "P.-J. Kindermans"], "venue": "ECCVW, 2014.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Dynamic Neural Networks for Gesture Segmentation and Recognition", "author": ["D. Wu"], "venue": "ECCV Workshop, 2014.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 191, "endOffset": 194}, {"referenceID": 2, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 235, "endOffset": 238}, {"referenceID": 4, "context": "Recently, the field of deep learning has made a tremendous impact in computer vision, demonstrating previously unattainable performance on the tasks of object detection and localization [1], [2], recognition [3] and image segmentation [4], [5].", "startOffset": 240, "endOffset": 243}, {"referenceID": 5, "context": "Convolutional neural networks (ConvNets) [6] have excelled on several scientific competitions such as ILSVRC [3], Emotion Recognition in the Wild [7], Kaggle Dogs vs.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "Convolutional neural networks (ConvNets) [6] have excelled on several scientific competitions such as ILSVRC [3], Emotion Recognition in the Wild [7], Kaggle Dogs vs.", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Convolutional neural networks (ConvNets) [6] have excelled on several scientific competitions such as ILSVRC [3], Emotion Recognition in the Wild [7], Kaggle Dogs vs.", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "Cats [2] and Galaxy Zoo.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "[8] recently claimed to have reached human-level performance using ConvNets for face recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 194, "endOffset": 197}, {"referenceID": 9, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 10, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 205, "endOffset": 209}, {"referenceID": 11, "context": "On the other hand, extending these models to problems involving the understanding of video content is still in its infancy, this idea having been explored only in a small number of recent works [9], [10], [11], [12].", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "The deep learning method described in this paper placed first in the 2014 version of this competition [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 201, "endOffset": 205}, {"referenceID": 14, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 213, "endOffset": 217}, {"referenceID": 16, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 219, "endOffset": 223}, {"referenceID": 17, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 225, "endOffset": 229}, {"referenceID": 18, "context": "Traditional approaches to action and distant gesture recognition from video typically include sparse or dense extraction of spatial or spatio-temporal engineered descriptors followed by classification [14], [15], [16], [17], [18], [19].", "startOffset": 231, "endOffset": 235}, {"referenceID": 19, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 182, "endOffset": 186}, {"referenceID": 21, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 188, "endOffset": 192}, {"referenceID": 22, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 194, "endOffset": 198}, {"referenceID": 23, "context": "A group of recent works is dedicated to inferring the hand pose through pixel-wise hand segmentation and estimating the positions of hand or body joints in a bottom-up fashion [20], [21], [22], [23], [24].", "startOffset": 200, "endOffset": 204}, {"referenceID": 24, "context": "In parallel, tracking-based approaches are advancing quickly [25], [26].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "In parallel, tracking-based approaches are advancing quickly [25], [26].", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "On the other hand, in [27] the authors proposed the Latent Regression Forest for coarse-to-fine search of joint positions.", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "Finally, graphical models, exploring spatial relationships between body and hand parts, have recently attracted close attention [28], [29].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "Finally, graphical models, exploring spatial relationships between body and hand parts, have recently attracted close attention [28], [29].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "In [30], a combination of skeletal features and local occupancy patterns (LOP) were calculated from depth maps to describe hand joints.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [31], skeletal information was integrated in two ways for extracting HoG features from RGB and depth images: either from global bounding boxes containing a whole body or from regions containing an arm, a torso and a head.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 11, "endOffset": 15}, {"referenceID": 32, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "Similarly, [32], [33], [34] fused skeletal information with HoG features extracted from either RGB or depth, while [35] proposed a combination of a covariance descriptor representing skeletal joint data with spatio-temporal interest points extracted from RGB augmented with audio.", "startOffset": 115, "endOffset": 119}, {"referenceID": 35, "context": "Independent subspace analysis (ISA) [36] as well as autoencoders [37], [9] are examples of efficient unsupervised methods for learning hierarchies of invariant spatio-temporal features.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "Independent subspace analysis (ISA) [36] as well as autoencoders [37], [9] are examples of efficient unsupervised methods for learning hierarchies of invariant spatio-temporal features.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "Independent subspace analysis (ISA) [36] as well as autoencoders [37], [9] are examples of efficient unsupervised methods for learning hierarchies of invariant spatio-temporal features.", "startOffset": 71, "endOffset": 74}, {"referenceID": 37, "context": "Spacetime deep belief networks [38] produce high-level representations of video sequences using convolutional RBMs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 38, "context": "A method proposed in [39] is based on low-level preprocessing of the video input and employs a 3D convolutional network for learning of midlevel spatio-temporal representations and classification.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "[10] have proposed a convolutional architecture for large-scale video classification operating at two spatial resolutions (fovea and context streams).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "While in most practical applications, late fusion of scores output by several models offers a cheap and surprisingly effective solution [7], both late and early fusion of either final or intermediate data representations remain under active investigation.", "startOffset": 136, "endOffset": 139}, {"referenceID": 39, "context": "Multiple Kernel Learning (MKL) [40] has been actively discussed in this context.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "At the same time, as shown by [41], simple additive or multiplicative averaging of kernels may reach the same level of performance while being orders of magnitude faster.", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "[42] proposed a late fusion strategy compensating for errors of individual classifiers by minimising the rank of a score matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "In follow-up work [43], they identified sample-specific optimal fusion weights by enforcing similarity in fusion scores for visually similar labeled and unlabeled samples.", "startOffset": 18, "endOffset": 22}, {"referenceID": 43, "context": "[44] introduced the Feature Weighting via Optimal Thresholding (FWOT) algorithm jointly optimising feature weights and thresholds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] employed multiple strategies, including MKL-based combinations of features, Bayesian model combination, and weighted average fusion of scores from multiple systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46] employed sparse RBMs and bimodal deep antoencoders to learn cross-modality correlations in the context of audiovisual speech classification of isolated letters and digits.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] used a multi-modal deep Boltzmann machine in a generative fashion to tackle the problem of integrating images and text annotations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] won the 2013 Emotion Recognition in the Wild Challenge by training convolutional architectures on several modalities, such as facial expressions from video, audio, scene context and features extracted around mouth regions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 47, "context": "Finally, in [48] the authors proposed a multi-modal convolutional network for gesture detection and classification from a combination of depth, skeletal information and audio.", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "[3]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Our model is therefore different from the one proposed in [4], where by \u201cmulti-scale\u201d Farabet et al.", "startOffset": 58, "endOffset": 61}, {"referenceID": 45, "context": "However, initial discriminative learning of individual data representations from each isolated channel followed by fusion has proven to be efficient in similar tasks [46].", "startOffset": 166, "endOffset": 170}, {"referenceID": 48, "context": "We formulate a pose descriptor consisting of 7 logical subsets as described in [49].", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": "Following [50], we first calculate normalized joint positions, as well as their velocities and accelerations, and then augment the descriptor with a set of characteristic angles and pairwise distances.", "startOffset": 10, "endOffset": 14}, {"referenceID": 48, "context": "Inclination angles are formed by all triples of anatomically connected joints plus two \u201cvirtual\u201d angles [49].", "startOffset": 104, "endOffset": 108}, {"referenceID": 50, "context": "[51] demonstrated the advantage of using primitive spectral features, such as 2D spectrograms, in combination with deep autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46] applied the same strategy to the task of multi-modal speech recognition while augmenting the audio signal with visual features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Further experiments from Microsoft [51] have shown that ConvNets appear to be especially efficient in this context since they allow the capture and modeling of structure and invariances that are typical for speech.", "startOffset": 35, "endOffset": 39}, {"referenceID": 47, "context": "Comparative analysis of our previous approach [48] based on phoneme recognition from sequences of MFCC features and a deep learning framework has demonstrated that the latter strategy allows us to obtain significantly better performance on the ChaLearn dataset (see Sec.", "startOffset": 46, "endOffset": 50}, {"referenceID": 50, "context": "As it was experimentally demonstrated by [51], the step of the scale transform is important.", "startOffset": 41, "endOffset": 45}, {"referenceID": 51, "context": "A number of works have shown that among fusion strategies, the weighted arithmetic mean of per-model outputs is the least sensitive to errors of individual classifiers [52].", "startOffset": 168, "endOffset": 172}, {"referenceID": 52, "context": "Unfortunately, implementing the arithmetic mean in the case of early fusion and non-linear shared layers is not straightforward [53].", "startOffset": 128, "endOffset": 132}, {"referenceID": 53, "context": "It has been shown though [54], that in dropout-like [55] systems activation units of complete models produce a weighted normalized geometric mean of per-model outputs.", "startOffset": 25, "endOffset": 29}, {"referenceID": 54, "context": "It has been shown though [54], that in dropout-like [55] systems activation units of complete models produce a weighted normalized geometric mean of per-model outputs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 54, "context": "Inspired by the concept of dropout [55] as the normalized geometric mean of an exponential number of weakly trained models, we aim on exploiting a priori information about groupings in the feature set.", "startOffset": 35, "endOffset": 39}, {"referenceID": 53, "context": "Along the lines of [54], we consider two situations corresponding to two different loss functions: E\u03a3, corresponding to the \u201ccomplete network\u201d where all modalities are present, and \u1ebc where ModDrop is performed.", "startOffset": 19, "endOffset": 23}, {"referenceID": 53, "context": "Taking the expectation of this expression requires an expression introduced in [54], which approximates E[\u03c3(x)] by \u03c3(E[x]).", "startOffset": 79, "endOffset": 83}, {"referenceID": 55, "context": "(16) Weights in a single layer of a neural network typically obey a unimodal distribution with zero expectation [56].", "startOffset": 112, "endOffset": 116}, {"referenceID": 56, "context": "It can be shown [57] that under these assumptions, Lyapunov\u2019s condition is satisfied and that Lyapunov\u2019s central mean theorem holds; in this case the sum of products of inputs and weights will tend to a normal distribution given that the number of training samples is sufficiently large.", "startOffset": 16, "endOffset": 20}, {"referenceID": 53, "context": "Finally, as has been shown by [54] for dropout, the multiplier proportional to the derivative of the sigmoid activation makes the regularization effect adaptive to the magnitude of the weights.", "startOffset": 30, "endOffset": 34}, {"referenceID": 53, "context": "Our experiments have shown that ModDrop achieves the best results if combined with dropout, which introduces an adaptive L2 regularization term \u00ca in the error function [54]:", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "The Chalearn 2014 Looking at People Challenge (track 3) dataset [13] consists of 13,858 instances of Italian conversational gestures performed by different people and recorded with a consumer RGB-D sensor.", "startOffset": 64, "endOffset": 68}, {"referenceID": 48, "context": "The baseline used for visual models is described in detail in [49].", "startOffset": 62, "endOffset": 66}, {"referenceID": 57, "context": "Extremely randomized trees (ERT) [58] are adopted for data fusion and gesture classification.", "startOffset": 33, "endOffset": 37}, {"referenceID": 48, "context": "During training, we followed the same iterative strategy as in the case of the neural architecture (see [49] for more details).", "startOffset": 104, "endOffset": 108}, {"referenceID": 48, "context": "1 Ours [49] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 59, "context": "[60] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[61] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "745 3 Chang [34] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 61, "context": "[62] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[63] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[64] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "6 Wu [65] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "A baseline has also been created for the audio channel, where we compare the proposed deep learning approach to a traditional phoneme recognition framework, as described in [48], and implemented with the Julius engine [59].", "startOffset": 173, "endOffset": 177}, {"referenceID": 58, "context": "A baseline has also been created for the audio channel, where we compare the proposed deep learning approach to a traditional phoneme recognition framework, as described in [48], and implemented with the Julius engine [59].", "startOffset": 218, "endOffset": 222}, {"referenceID": 48, "context": "Our winning entry [49] corresponding to a hybrid model (i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 60, "context": "[61], submitted entry 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[60] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[61], after competition 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "768 \u2013 Wu and Shao [65] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 32, "context": "[33] (validation set) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "791 \u2013 Chang [34] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 63, "context": "[64] \u2013 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62] \u2013 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Ours, submitted entry [49] 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 48, "context": "781 (6) Ours [49] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 48, "context": "849 (1) Ours [49] + ERT 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 47, "context": "Phoneme recognition [48] 64.", "startOffset": 20, "endOffset": 24}, {"referenceID": 47, "context": "Table 6 compares the performance of the proposed solution based on learning representations from mel-frequency spectrograms with the baseline model involving traditional phoneme recognition [48].", "startOffset": 190, "endOffset": 194}, {"referenceID": 54, "context": "Dropout, 784-1200-1200-10 [55] 107 2395210 N Dropout, 784-500-40-10 (ours) 119 412950 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 54, "context": "Currently, the state-of-the-art for a fully-connected 7821200-1200-10 network with dropout regularization (50% for hidden units and 20% for the input) and tanh activations [55] is 110 errors on the MNIST test set (see Table 7).", "startOffset": 172, "endOffset": 176}], "year": 2017, "abstractText": "We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.", "creator": "LaTeX with hyperref package"}}}