{"id": "1501.00299", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jan-2015", "title": "Sequence Modeling using Gated Recurrent Neural Networks", "abstract": "In this paper, we have used Recurrent Neural Networks to capture and model human motion data and generate motions by prediction of the next immediate data point at each time-step. Our RNN is armed with recently proposed Gated Recurrent Units which has shown promising results in some sequence modeling problems such as Machine Translation and Speech Synthesis. We demonstrate that this model is able to capture long-term dependencies in data and generate realistic motions.", "histories": [["v1", "Thu, 1 Jan 2015 18:37:36 GMT  (6163kb,D)", "http://arxiv.org/abs/1501.00299v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["mohammad pezeshki"], "accepted": false, "id": "1501.00299"}, "pdf": {"name": "1501.00299.pdf", "metadata": {"source": "CRF", "title": "Sequence Modeling using Gated Recurrent Neural Networks", "authors": ["Mohammad Pezeshki"], "emails": ["mohammadpz@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "One of the early models of sequence modeling was the Hidden Markov Model (HMM) [1]. HMMs are able to capture data distribution using multinomial latent variables. In this model, each data point at the time t is conditioned to the hidden state at the time t. And the hidden state at the time t \u2212 1. In HMMs, both P (xt | st) and P (st | st \u2212 1) are the same for all time steps. A similar idea of parameter release is used in the Recurrent Neural Network (RNN) [2]. RNNNs are an extension of upstream neural networks whose weights are shared in the database for each time step. Consequently, we can apply RNNNs to sequential input data. Theoretically, RNNs are able to capture sequences with arbitrary sections. Their weights are shared in the database at each step."}, {"heading": "2 Recurrent Neural Network", "text": "Simple Recurrent Neural Network, which has been proven to be able to implement a Turing Machine [7], is an extension of forward-facing neural networks. The idea in RNNs is that they share parameters for different time steps. This idea, called parameter release, allows RNNs to be used for sequential data. RNNNs have memory and can remember input values for a certain period of time. Formally, if the input sequence x = {x1, x2,..., xN} then any hidden state function of the current input is andar Xiv: 150 1.00 299v 1 [cs.N E] 1J is a previous hidden state.ht = F\u03b8 (ht \u2212 1, xt), which is a linear regression followed by a non-linear ity.ht = H (Whhht \u2212 1, Wxhxt), where H is a non-linear function, which is vanilla Tanilla."}, {"heading": "2.1 Generative Recurrent Neural Network", "text": "We can use a recursive neural network as a generative model in such a way that the output of the network in time step t \u2212 1 defines a probability distribution over the next input in time step t. According to the chain rule, we can write the common probability distribution over the input sequence as follows. P (x1, x2,..., xN) = P (x1) P (x2 | x1)... P (xT | x1,..., XT \u2212 1) Now we can model each of these conditional probability distributions as a function of hidden states. P (xt | x1,..., xt \u2212 1) = f (ht) Obviously, since there is a fixed length vector and {x1,..., xt \u2212 1} is a variable length sequence, it can be considered a lossy compression."}, {"heading": "2.2 Gated Recurrent Unit", "text": "Gated Recurrent Unit (GRU) differs from simple RNN in the sense that in GRU each hidden unit has two gates. These gates are called update and reset gates, which control the flow of information within each hidden unit. Each hidden state in time step t is calculated as follows: ht = (1 \u2212 zt) \u0441ht \u2212 1 + \u0432\u043e\u0432\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "3 Experimental results", "text": "In this section we describe the motion dataset and the results for modeling and generating human movements."}, {"heading": "3.1 Ability to capture long-term dependency", "text": "As we discussed in section 2, simple RNNs are not able to capture long-term dependencies in data due to optimization problems (gradient vanishing problem), so instead of using the activation function of Tanh, we use GRU. Here, we are trying to show that GRU performs much better. The task is to read a sequence of random numbers, store them for some time periods, and then output a function that is sum above the input value. We created 100 different sequences, each containing 20 rows (time steps) and 2 columns (attributes of each data point). We trained the models so that the output at some point is t (yt) a function of previous input values.yt = xt \u2212 3 [0] + x (t \u2212 5) [1] [1] [1] Therefore, we expect models to store 5 time steps back and learn when the dimensions of the previous inputs are good to use. [For both models, input Gx + 5 is very \u2212 1.]"}, {"heading": "3.2 Dataset", "text": "Among some Motion Capture (MOCAP) datasets, we used simple walking movements from the MIT Motion dataset [6]. The dataset is generated by filming a man wearing a cloth with 17 small lights that determine the position of the body joints. Each data point in our dataset consists of information about global orientation and displacement. To create more realistic movements, we used the same pre-processing as Taylor et al. [14]. Our final dataset contains 375 rows, in which each row contains 49 floor inventory, mean and variance features of body joints while walking."}, {"heading": "3.3 Motion generation", "text": "We trained our GRU Recurrent Neural Network, which has 49 input units and 120 hidden units in a single hidden layer. We then use it in a generative way, passing each output to the model as xt + 1 at a given time. To initialize the model, we first feed the model with 50 images of the training data and then let it generate sequences of any length. Regeneration quality is so good that it is indistinguishable from real training data to the naked eye. Figure 6 shows the average across all 49 features for better visualization, while the initialization and generation phases are shown in Figure 7."}, {"heading": "4 Conclusion", "text": "In this work, we demonstrated that the Gated Recurrent Unit helps to optimize recurrent neural network problems when there is a long-term data dependency. We conducted our experiments using a toy example and generatively using the MIT motion dataset and showed that GRU performs much better in both storage and generation than simple recurrent neural networks with conventional tanh activation."}], "references": [{"title": "An introduction to hidden Markov models.", "author": ["Rabiner", "Lawrence", "Biing-Hwang Juang"], "venue": "ASSP Magazine, IEEE", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Learning Internal Representations by Error Propagation, Parallel Distributed Processing, Explorations in the Microstructure of Cognition, ed", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "DE Rumelhart and J. McClelland", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Learning long-term dependencies with gradient descent is difficult.", "author": ["Bengio", "Yoshua", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on 5.2", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.", "author": ["Chung", "Junyoung"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Style translation for human motion.", "author": ["Hsu", "Eugene", "Kari Pulli", "Jovan Popovi"], "venue": "ACM Transactions on Graphics (TOG)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Turing computability with neural nets.", "author": ["Siegelmann", "Hava T", "Eduardo D. Sontag"], "venue": "Applied Mathematics Letters", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Learning recurrent neural networks with hessian-free optimization.", "author": ["Martens", "James", "Ilya Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "On the difficulty of training recurrent neural networks.", "author": ["Pascanu", "Razvan", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Advances in optimizing recurrent networks.", "author": ["Bengio", "Yoshua", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["Graves", "Alex", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate.", "author": ["Bahdanau", "Dzmitry", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator.", "author": ["Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Modeling human motion using binary latent variables.", "author": ["Taylor", "Graham W", "Geoffrey E. Hinton", "Sam T. Roweis"], "venue": "Advances in neural information processing systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "One of the early models for sequence modeling was Hidden Markov Model (HMM) [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "A similar idea of parameter sharing is used in Recurrent Neural Network (RNN) [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "[3], there are some difficulties during training RNNs on sequences with longterm dependencies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5], Gated Recurrent Unit performs much more better than conventional Tanh units.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In the folowing sections, we are going to introduce the model, train it on the MIT motion database [6], and show that it is capable of capturing complexities of human body motions.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Simple Recurrent Neural Network which has been shown to be able to implement a Turing Machine [7] is an extension of feedforward neural networks.", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "[3], there are some optimization issues when we try to train such models with long-term dependency in data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Although the problem still remains, gating methods have shown promissing results in comparison with Vanilla RNN in different task such as Speech Recognition [11], Machine Translation [12], and Image Caption Generation [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 11, "context": "Although the problem still remains, gating methods have shown promissing results in comparison with Vanilla RNN in different task such as Speech Recognition [11], Machine Translation [12], and Image Caption Generation [13].", "startOffset": 183, "endOffset": 187}, {"referenceID": 12, "context": "Although the problem still remains, gating methods have shown promissing results in comparison with Vanilla RNN in different task such as Speech Recognition [11], Machine Translation [12], and Image Caption Generation [13].", "startOffset": 218, "endOffset": 222}, {"referenceID": 3, "context": "One of the models which exploits a gating mechanism is Gated Recurrent Unit [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "[5]", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "yt = xt\u22123[0] + x(t\u2212 5)[1]", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "Among some Motion Capture (MOCAP) datasets, we used simple walking motion from MIT Motion dataset [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6]", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "In this paper, we have used Recurrent Neural Networks to capture and model human motion data and generate motions by prediction of the next immediate data point at each time-step. Our RNN is armed with recently proposed Gated Recurrent Units which has shown promissing results in some sequence modeling problems such as Machine Translation and Speech Synthesis. We demonstrate that this model is able to capture long-term dependencies in data and generate realistic motions.", "creator": "LaTeX with hyperref package"}}}