{"id": "1501.00358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2015", "title": "Comprehend DeepWalk as Matrix Factorization", "abstract": "Word2vec, as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry M_{ij} is logarithm of the average probability that node i randomly walks to node j in fix steps.", "histories": [["v1", "Fri, 2 Jan 2015 07:57:14 GMT  (4kb)", "http://arxiv.org/abs/1501.00358v1", "4 pages"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cheng yang", "zhiyuan liu"], "accepted": false, "id": "1501.00358"}, "pdf": {"name": "1501.00358.pdf", "metadata": {"source": "CRF", "title": "Comprehend DeepWalk as Matrix Factorization", "authors": ["Cheng Yang", "Zhiyuan Liu"], "emails": ["cheng-ya14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 150 1.00 358v 1 [cs.L G] 2J an2 01"}, {"heading": "1 Notation", "text": "Network G = (V, E). Node context set D is created from random gear, where each part of D is a node context pair (v, c). V is the set of nodes and VC is the set of context nodes. In most cases V = VC. Consider a node context pair (v, c): # (v, c) denotes the number of times (v, c) that occur in D. # (v) = c \"VC # (v, c\") and # (c) = x \"V\" V \"(v,\" c) denotes the number of times that v \"and c\" occur in D. \"Note that the DeepWalk algorithm embeds a node v in a d\" dimensional vector \u2212 \u2192 v \"Rd.\" A context node c \"VC is also represented by a d\" VC # (v, \"c\")."}, {"heading": "2 Proof", "text": "Perozzi et al. implemented the DeepWalk algorithm with Skip-Gram and the hierarchical Softmax model. Note that Hierarchical Softmax [5] [4] is a variant of Softmax to accelerate training time. In this section we provide evidence for both Negative Sampling and Softmax with Skip-Gram model."}, {"heading": "2.1 Negative Sampling", "text": "Negative sampling roughly maximizes the probability of the Softmax function by randomly selecting k negative samples from the set context. Levy and Goldberg showed that Skip-Gram with negative sampling model (SGNS) implicitly factored a word-context matrix [1] by assuming that the dimensionality d is sufficiently large. In other words, we can assign each product \u2212 v \u00b7 \u2212 \u2212 \u2212 \u2212 to a value independent of the others. In the SGNS model, we have P ((v, c) - D) = Circularity (\u2212 v \u00b7 \u2212 c) = Circularity (\u2212 v # \u2212 \u2212 \u2212 \u2212 c) negative samples for each node-context pair (v, c) according to the distribution PD (cN) = (cN) | D |. Then the objective function can be written for c (v \u00b7 v \u00b7 v \u00b7 v \u00b7 v \u00b7 c)."}, {"heading": "2.2 Softmax", "text": "Since both negative sampling and hierarchical Softmax are variants of Softmax, we pay more attention to the Softmax model and will discuss it further in the next section. We also assume that the values of \u2212 \u2192 v \u00b7 \u2212 \u2212 \u2212 \u2192 c are independent of each other. In the Softmax model, P (v, c) \u00b7 log e \u2212 \u2192 v \u00b7 \u2212 \u2212 c \u00b2 VC e \u2212 \u2192 v \u00b7 \u2212 \u2212 \u2212 \u2212 \u2212 c \u00b2 and the objective function isl = x (v, c) \u00b7 log e \u2212 (v, c) \u00b7 log e \u2212 \u2192 c \u00b2 c \u00b2 VC e \u2212 \u2192 v \u2212 \u2212 \u2212 c \u00b2 c \"after extraction of all terms associated with \u2212 v \u2212 \u2212 \u2212 c, such as l (v, c) havel (v, c) = (v, c) log e \u2212 (v \u00b7 \u2212 c) c \u00b2 VC, c \u00b2 c \u00b2 c c c \u00b2 c \u00b2 c \u00b2 c c \u00b2 c (v \u00b2) c c \u00b2 c c (v \u00b2) c c c \u00b2 c (c) c \u00b2 c (c) c c \u00b2 c (c) c v \u00b2 c, c c, c \u00b2 c (c) c c, c \u00b2 c, c \u00b2 c, c, c \u00b2 c (c, c) v \u00b2 v (v), v (c), v (c \u00b2), v (c), v (v (c), c \u00b2, c (c) v (v (c), c \u00b2, c (c) v (v (c), c \u00b2, c (c), c (c \u00b2, c (c) v (v (c), c \u00b2, c (c), c (c \u00b2, c, c (c), c (c) v (v (v (v)."}, {"heading": "3 Discussion", "text": "It is clear that the method of scanning node context pairs has effects on MatrixM = Aij times for the graph. In this section we will discuss # (v) | D |, (c) | D | and # (v, c) # (v) # (v) # (v) based on an ideal scanning method for the DeepWalk algorithm. Suppose the graph is connected and undirected and the window size is. We can easily generalize this scanning method by using only (RWi, RWj) in the D. algorithm 1 Ideal node context pair sampling algorithm generate at an infinite long random path RW. Denote RWi as a node at position i of RW # 2, where i = 0, 1, 2,.. for i = 0, 1, 2,."}], "references": [{"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "arXiv preprint arXiv:1403.6652,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Abstract Word2vec [2], as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "issued Skip-Gram and Negative Sampling model [3] for developing this toolbox.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk [6] for learning node embedding on a graph.", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "Note that Hierarchical Softmax [5] [4] is a variant of softmax for speeding the training time.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Note that Hierarchical Softmax [5] [4] is a variant of softmax for speeding the training time.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Goldberg showed that Skip-Gram with Negative Sampling model(SGNS) is implicitly factorizing a word-context matrix [1] by assuming that dimensionality d is sufficiently large.", "startOffset": 114, "endOffset": 117}], "year": 2015, "abstractText": "Word2vec [2], as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model [3] for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk [6] for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry Mij is logarithm of the average probability that node i randomly walks to node j in fix steps. We will explain it in section 3. 1 Notation Network G = (V,E). Node-context set D is generated from random walk, where each piece of D is a node-context pair (v, c). V is the set of nodes and VC is the set of context nodes. In most cases, V = VC . Consider a node-context pair (v, c): #(v, c) denotes the number of times (v, c) appears inD. #(v) = \u2211 c\u2208VC #(v, c) and #(c) = \u2211 v\u2032\u2208V #(v , c) denotes the number of times v and c appears in D. Note that |D| = \u2211 v\u2032\u2208V \u2211 c\u2208VC #(v, c). DeepWalk algorithm embeds a node v into a d-dimension vector \u2212\u2192v \u2208 R. Also, a context node c \u2208 VC is represented by a d-dimension vector \u2212\u2192c \u2208 R. Let W be a |V | \u00d7 d matrix where row i is vector vi and H be a |VC | \u00d7 d matrix where row j is vector cj . Our goal is to figure out a matrix M = WH T . 2 Proof Perozzi et al. implemented DeepWalk algorithm with Skip-Gram and Hierarchical Softmax model. Note that Hierarchical Softmax [5] [4] is a variant of softmax for speeding the training time. In this section, we give proofs for both Negative Sampling and softmax with Skip-Gram model. 2.1 Negative Sampling Negative Sampling approximately maximizes the probability of softmax function by randomly choosing k negative samples from context set. Levy and", "creator": "LaTeX with hyperref package"}}}