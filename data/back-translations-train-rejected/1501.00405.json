{"id": "1501.00405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2015", "title": "Efficiently Discovering Frequent Motifs in Large-scale Sensor Data", "abstract": "While analyzing vehicular sensor data, we found that frequently occurring waveforms could serve as features for further analysis, such as rule mining, classification, and anomaly detection. The discovery of waveform patterns, also known as time-series motifs, has been studied extensively; however, available techniques for discovering frequently occurring time-series motifs were found lacking in either efficiency or quality: Standard subsequence clustering results in poor quality, to the extent that it has even been termed 'meaningless'. Variants of hierarchical clustering using techniques for efficient discovery of 'exact pair motifs' find high-quality frequent motifs, but at the cost of high computational complexity, making such techniques unusable for our voluminous vehicular sensor data. We show that good quality frequent motifs can be discovered using bounded spherical clustering of time-series subsequences, which we refer to as COIN clustering, with near linear complexity in time-series size. COIN clustering addresses many of the challenges that previously led to subsequence clustering being viewed as meaningless. We describe an end-to-end motif-discovery procedure using a sequence of pre and post-processing techniques that remove trivial-matches and shifted-motifs, which also plagued previous subsequence-clustering approaches. We demonstrate that our technique efficiently discovers frequent motifs in voluminous vehicular sensor data as well as in publicly available data sets.", "histories": [["v1", "Fri, 2 Jan 2015 14:09:46 GMT  (2588kb,D)", "http://arxiv.org/abs/1501.00405v1", "13 pages, 8 figures, Technical Report"]], "COMMENTS": "13 pages, 8 figures, Technical Report", "reviews": [], "SUBJECTS": "cs.DB cs.LG", "authors": ["puneet agarwal", "gautam shroff", "sarmimala saikia", "zaigham khan"], "accepted": false, "id": "1501.00405"}, "pdf": {"name": "1501.00405.pdf", "metadata": {"source": "CRF", "title": "Efficiently Discovering Frequent Motifs in Large-scale Sensor Data", "authors": ["Puneet Agarwal", "Gautam Shroff", "Sarmimala Saikia", "Zaigham Khan"], "emails": ["zaigham.khan}@tcs.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "2 Background", "text": "The problem of the discovery of common time series motifs has been discussed extensively in previous research [6, 13, 15, 23]. However, our formulation is somewhat different and so we define formally common motifs in the context of time series data resulting from a single sensor as follows: If we look at a time series Ti = {v1, v2,.., vni}, which represents values generated from a single sensor sampled at regular intervals on a time scale t1... In practice, there will be many time series for a single sensor, T = {T1,.., Tm}: Each Ti arises from a single operation of one of the underlying systems: in our case, each Ti arises from a continuous run of a vehicle, and the entire T data consists of data from multiple runs of many vailreaders."}, {"heading": "3 Spherical Clustering for Discovering Frequent Motifs", "text": "It is not only a question of time, but also of the way in which the different types of terms are related to each other in the way in which the individual terms are related to each other. [3] It is a question of time in which the individual terms of the individual terms are related to each other. [4] It is a question of time in which the different terms and terms of the individual terms are related to each other. [5] Moreover, highly correlated terms are in the way in which the individual terms are very different in relation to the individual terms and terms of the individual terms. [6, 10] These terms must be identified and removed after the terms of the terms have been found. [5] Other, highly correlated terms effectively lie along a long sub-dimensionality in space. As we traverse the length of these distinctions, the successive terms of the points (sub-sequences) that we are actually close to each other."}, {"heading": "4 COIN Clustering", "text": "We introduce the term COIN clustering to describe techniques that lead to spherical clusters that limit the maximum distance between two members of a single cluster to less than a fixed cluster diameter, 2R. COIN clustering uses the pictorial analogy of a \"coin\" to perform clustering (although \"sphere\" would be more correct), with the radius of the coin as the distance threshold R. Our goal is to ensure that the distance between them is smaller than a) the clusters are in the form of a limited diameter, while the time complexity is less than O (n2). According to definition 2.1, two time series subsequences are considered similar if the distance between them is less than 1; so in COIN with the threshold R, all sub-sequences within a cluster are guaranteed to be similar that they are measured at most 2Rs."}, {"heading": "4.1 Coin Clustering: BIRCH acceleration", "text": "BIRCH [27] is a well-known method for clustering with a delimited spherical shape (COIN), which stores clusters on leaf nodes of a height-balanced tree, in which each node has at most B children. Each cluster is represented by its cluster characteristic (CF), which consists of (N, LS, SS) three times. N is the number of points in this cluster, LS is the linear sum of all points in the cluster, and SS is the square sum of all points in the cluster. The non-leaf nodes store the sum of the CF triple of its children. With the help of the LS and N of the CF triple, centrification can be easily calculated. If we look up the target cluster of a point si (line # 9) in this tree, we start with the children of the root node and find the nearest node Nn among them, i.e. arg minj D (si, ej), children of the root node."}, {"heading": "4.2 Coin Clustering: LSH acceleration", "text": "While COIN clustering of subsequences using BIRCH acceleration works well in practice, it seems to be an inherently serial technique. Further, because the nature of BIRCH compares the points with summary statistics of each node in its tree, rather than the actual cluster centroids, it is often the case that a subsequence is not placed in the cluster closest to it. We now describe an alternative acceleration technology that potentially addresses the above issues while remaining almost as efficient as BIRCH \u00a9 2015 Tata Consultancy Services Ltd. 5 Patent # 772 / MUM / 2014, TCS ResearchWe use locality sensitive hashing (LSH) [17] to create the candidate cluster sets for each subsequence. We project sequences of random hyperplanes as proposed in [7]. Each sequence is hashashed with n random hyper-level normally (the vectors A)."}, {"heading": "5 Freq. Motif Discovery Process", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Pre-Processing & Clustering (F1 - F7)", "text": "We now describe the complete process of frequent motif finding, see Figure 3. (Some of these steps have also been used in [6, 15, 23].) Based on the given time series, we first normalize them (F1) by calculating z-values and generating sub-sequences (F2) by using a moving window of size w. Then, we filter uninteresting sub-sequences (F3) where the maximum deviation of the z-score values is less than one (taking into account the standard deviation of the unnormed series). Next, however, we merge the levels (F4) of these sub-sequences by first reducing the number of dimensions from w to d by using piecemeal averages. Then, we subtract the local mean of each sub-sequence from it so that all sub-sequences are zero mean, but this step is reversed at a later stage of the process (F4) by decreasing the number of dimensions from w to d by using incremental averages."}, {"heading": "5.2 Extract Motifs (F8 - F10)", "text": "After moving the item q2 to the next sequence (< 1), we reject clusters that are moved in other clusters (see definition 3.1) by proceeding in pairs, for each pair of clusters. Since the number of clusters in each cluster is much smaller than the number of subsequences n and the total number of clusters k, so this step, although of square complexity, has no significant impact on performance. First, we sort subsequences in each cluster by their start times. We identify the cluster with smaller support (H1) and iterate over the sequences of that cluster to compare them with a few subsequences of the second cluster (H2). We maintain two pointers that move on the ordered subsequences of the two clusters {H1, H2} each. At the beginning of both pointers {q1, q2} we are placed at the first subsequence of the respective cluster."}, {"heading": "6 Experiments and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Datasets and Infrastructure", "text": "We report on the experimental results of the vehicle sensor data for 59 different runs of different vehicles. All these vehicles were driven for almost 3 hours. During the driving of these vehicles, values of 27 different sensors were continuously recorded, while we reported the results of the experiments only on selected 6 sensors. This led us to our primary data set for this essay, which contains m = 59 time series for each sensor. These sensor values were recorded at a regular interval of 1 second; consequently, the number of sequences for a window length of w = 20. In addition to the vehicle sensor data, we also conducted experiments on some publicly available data sets to facilitate the comparison of our approach with those in the research literature. These public data sets are electrocardiogram, bird calls and temperature data [19].For our vehicle sensor algorithm COIN-BIRCH, the BIRCH code for step F6 was extracted [16]."}, {"heading": "6.2 Parameter values for experiments", "text": "In this section, we describe how the values of these parameters were selected for different experiments. Before clustering the subsequences, we normalize the entire time series. The first parameter R binds the size of the clusters, i.e. it is a measure of how far the different subsequences within a subject can be from each other. In our experiments, we found that R = 1 mostly works for w = 20 in all time series (with which we have experimented) and uses the same value, unless otherwise specified. Furthermore, when we discover the subjects with width w > 20, we adjust R like Radj = R \u00d7 \u221a w / 20. This strategy was observed to work well for subject widths up to w = 200, and not for subjects wider than 200."}, {"heading": "6.3 Results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7 Related Work", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times. \""}, {"heading": "8 Using Motifs Further", "text": "As we mentioned at the beginning, motives are only a potentially useful element in the overall process of sensor data analysis. For example, one of the objectives of our analysis is to predict the occurrence of errors in a vehicle as early as possible. If we consider the historical behavior of a sensor that precedes an error to be \"negative\" within a reasonable timeframe (e.g. hours or most days, typically with a gap that excludes the segment immediately before the error), we can consider the problem of error forecasting as one of the time series classification. Frequent motives, especially those that occur frequently within the series of positive stories, are one of the characteristics that are used while a classifier is trained to detect errors early, or for rule-based techniques such as [21]. Frequent motives as well as discredited levels of sensor value, its derivation, etc. and other features can also be combined for such tasks: [more] a time series."}, {"heading": "9 Conclusion and Discussion", "text": "First, we highlight the key aspects of our algorithm and then present a summary of what was presented in this paper. First, even with our best efforts, we could not find any other publication that directly addresses the same issue as ours, except [6], which is clearly exceeded by our approach from the efficiency perspective. For more details, see Section 7. For the same reason, almost all publicly available data sets are not suitable to demonstrate the usefulness of our approach, as they are all much too small. We have provided the end-to-end source code of our approach [1] along with a comparatively larger time series extracted from our vehicular sensor data (though still much smaller than our real data). Furthermore, it was pointed out that the problems of trivial matches and smooth effects should be resolved, some of the subsequences removed before clustering; we also remove selected subsequences before clustering (see Section 5)."}], "references": [{"title": "A brief survey on sequence classification", "author": ["Z. Xing", "J. Pei", "E. Keogh"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Efficient proper length time series motif discovery", "author": ["S. Yingchareonthawornchai", "H. Sivaraks", "T. Rakthanmanon", "C.A. Ratanamahatana"], "venue": "In 13th International Conference on Data Mining (ICDM). IEEE,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Clustering time series using unsupervised-shapelets", "author": ["J. Zakaria", "A. Mueen", "E. Keogh"], "venue": "In Proc. of the 2012 IEEE 12th International Conference on Data Mining, ICDM \u201912. IEEE Computer Society,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Birch: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "In ACM SIGMOD international Conference on Management of Data,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}], "referenceMentions": [], "year": 2015, "abstractText": "While analyzing vehicular sensor data, we found that frequently occurring waveforms could serve as features for further analysis, such as rule mining, classification, and anomaly detection. The discovery of waveform patterns, also known as time-series motifs, has been studied extensively; however, available techniques for discovering frequently ocurring timeseries motifs were found lacking in either efficiency or quality: Standard subsequence clustering results in poor quality, to the extent that it has even been termed \u2018meaningless\u2019. Variants of hierarchical clustering using techniques for efficient discovery of \u2018exact pair motifs\u2019 find high-quality frequent motifs, but at the cost of high computational complexity, making such techniques unusable for our voluminous vehicular sensor data. We show that good quality frequent motifs can be discovered using bounded spherical clustering of time-series subsequences, which we refer to as COIN clustering, with near linear complexity in time-series size. COIN clustering addresses many of the challenges that previously led to subsequence clustering being viewed as meaningless. We describe an end-to-end motif-discovery procedure using a sequence of pre and post-processing techniques that remove trivial-matches and shifted-motifs, which also plagued previous subsequenceclustering approaches. We demonstrate that our technique efficiently discovers frequent motifs in voluminous vehicular sensor data as well as in publicly available data sets.", "creator": "LaTeX with hyperref package"}}}