{"id": "1501.00437", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2015", "title": "Computational Feasibility of Clustering under Clusterability Assumptions", "abstract": "It is well known that most of the common clustering objectives are NP-hard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such 'clusterable' instances. In other words, hope that \"Clustering is difficult only when it does not matter\" (CDNM thesis, for short).", "histories": [["v1", "Fri, 2 Jan 2015 17:10:52 GMT  (21kb)", "http://arxiv.org/abs/1501.00437v1", null]], "reviews": [], "SUBJECTS": "cs.CC cs.LG", "authors": ["shai ben-david"], "accepted": false, "id": "1501.00437"}, "pdf": {"name": "1501.00437.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shai Ben-David"], "emails": ["shai@uwaterloo.ca"], "sections": [{"heading": null, "text": "ar Xiv: 150 1.00 437v 1 One approach to theoretically understanding this apparent discrepancy is to develop concepts of clusterability that distinguish realistically interesting input data from worst-case data sets. It is hoped that there will be cluster algorithms that are demonstrably efficient in such \"clusterable\" cases. In other words, we hope that \"clustering is difficult only when it does not matter\" 1 (the CDNM thesis for short), and we believe that this may indeed be the case to some extent. This paper provides an overview of current work along this line of research and a critical assessment of its results. Our conclusion is that the CDNM thesis is far from being formally substantiated. We will begin by discussing what requirements should be met to formally support the validity of the CDNM thesis."}, {"heading": "1 Introduction", "text": "The purpose of this note is twofold: First, I would like to give a personally biased overview of research into the computational complexity of clusters under assumptions about data beauty. Having been working in this area for some time now, I feel that while the TCS community appreciates work that can be of practical relevance (and clustering is clearly a task that arises in many applications), there is sometimes a significant gap between research motivation and actual technical outcomes in this area. A secondary goal of this paper is to draw the attention of the theoretical research community to some of these gaps and promote further work in a direction that would otherwise have been resolved."}, {"heading": "1.1 Alternatives to worst-case for measuring computational complexity", "text": "In this context, it should be noted that such a task is a very difficult one (assuming that P 6 = NP). We assume that for each algorithm there are an infinite number of instances on which it will have to work hard, but for many problems this measure is unrealistic. Compared to the experience of solving them for practical instances, we believe that algorithms that are likely to occur in practice or in another instance are very pessimistic."}, {"heading": "1.2 A focus on clustering tasks", "text": "Clustering is a very useful paradigm that is used in a wide range of data exploration tasks. \"Clustering\" should be understood as a generic term for a large and diverse collection of tasks and algorithmic paradigms. In this context, we focus on cluster tasks, which are defined as discrete optimization problems. Most of these optimization problems are NP-hard. We want to investigate whether this hardness remains a problem if we limit our attention to \"clusterable data\" - data for which a meaningful cluster structure exists (it can be argued that it makes no sense to apply a cluster algorithm to a given data set). In other words, we want to assess the extent to which current theoretical work supports the thesis that \"clustering is difficult only when it does not matter\" (CDNM).For the sake of specificity, we focus on two popular cluster goals, k-means and k-median."}, {"heading": "1.3 Outline of the paper", "text": "We begin with this note by listing in Section 2 what we consider to be clustering capability requirements that aim to substantiate the CDNM thesis. In Section 4, we list various concepts of clustering capability that have been proposed in the context of this line of research. \u2022 These include: Additive Disturbance Robustness (APR), [1]; Multiplicative Disturbance Robustness (MPR), [16]; (\u03b1) Disturbance Resistance, [8]; (Separatedness, [22]; Unambiguity of Optimal, [6] (they call it (c,) Stability of Cluster Stability (they call it) Proximity Stability of the Center, [5] and (1 +) Weak Deletion Stability, [4].The main body of this work is an investigation, in Section 5, of how well the current views and results meet the requirements (Section 2)."}, {"heading": "2 Requirements from notions of clusterability", "text": "This year, more than ever before in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place"}, {"heading": "3 Definitions and basic notions", "text": "We consider clustering tasks that can be described as follows: \u2022 Input is a finite subset X of a metric space (Y, d) 3 and a number k. If X = Y or Y is any Euclidean space (with the Euclidean distance), we omit to mention it explicitly. \u2022 Solution space S is a collection of partitions of input set X into k subsets (a.k.a. k clusters). \u2022 The problem is determined by an objective function, O, the maps pairs of (instance, clustering) to the real numbers. The goal of the algorithm is to find a clustering in the solution space that minimizes this goal for the given input instance. We let CO (X, d) be ArgMinC-SO (X, d)) (namely the set of clustering in the solution space that minimizes the objective cost of input."}, {"heading": "3.1 Measures of clustering approximations", "text": "When it comes to approximation algorithms for clustering, there is another technical point that needs to be borne in mind, namely the way in which the difference between an optimal solution and an approximate one is measured. There are at least two different approaches to quantifying this gap. The first, and probably the most common, is to consider only the cost of the solutions. In other words, if you specify an objective cluster function O as an input set X and cluster formation, then you say that C is an approximate good solution if O (C, X) \u2264 Opt (1 + 1) (alternatively, you could consider additive approximations to the cost, namely requiring O (C, X) \u2264 Opt (X) +. Additive approximations naturally arise in the context of statistical machine learning, where approximate solutions are based on small samples of the input data."}, {"heading": "4 Notions of clusterability", "text": "Over the past few years, there have been several interesting publications along the lines described above that show that for various terms of clustering capability, there are actually algorithms that find optimal clustering in polytime for all appropriate clusterable cases. [1] Below is a (possibly not exhaustive) list of the most important terms of clustering capability that have been discussed in this context. [2] Most of these definitions can be applied to any of the above cluster objectives. [3] An input dataset is resilient if small disturbances in clustering capability do not result in a change in optimal clustering. [3] Additional disturbance of objective (APR) [5] is an input set (X), d) is an input APR if for some optimal k clustering C, for any d clustering."}, {"heading": "5 To what extent do the notions meet the require-", "text": "While all the above terms sound intuitively plausible (concrete arguments supporting this plausibility can be found in the papers presenting them), the quantitative values of the cluster capability parameters are indispensable for evaluating this plausibility. In the following, we will see that the currently known results with respect to these terms only provide the desired efficiency of the calculation if the cluster capability parameters are set to values that exceed what one would expect from practical inputs in order to satisfy them. To keep this comment in mind, we offer a relatively high overview of some of the most important relevant results. However, since the actual values of the parameters (which define the cluster capability parameters) determine both the runtime of the algorithms and the limitations of the clarity conditions, these concrete values are needed if we are to evaluate, and the gap between what we currently know and the optimistic CDNM thesis."}, {"heading": "5.1 Computational efficiency of clustering clusterable inputs", "text": "An important distinction in this context concerns the importance of dependence on exponential dependencies in relation to the course of cluster solutions determined by the selection of cluster centers from the input set. (Note: Cluster centers are located in all possible cluster centers.) For such problems, the term \"practicable\" usually refers to the runtime specified in both m and k. (On the other hand, tasks such as the input set are located in some euclimatic spaces, Rn, and cluster centers can be arbitrary points in this space, which are often NP values for fixed values, if one takes the spatial dimension n as a parameter that runtime is a function of. For such problems, algorithms that have polynomial dependence on m and n, they can be considered \"feasible,\" even if they have exponential dependencies on runtime."}, {"heading": "5.2 How restrictive are the clusterability parameters required for the efficiency of computation results?", "text": "In this subsection, we will examine the above-cited cluster capability conditions with respect to the degree of separation between clusters that require these conditions (in optimal clustering) when their clustering capability (or lack of data level) assumes parameters sufficient to meet the efficiency of the corresponding proposed cluster requirement. To measure these cluster separation requirements, we will focus on the relationship between the average distance of a point to the center of its cluster, AvDis = OPT / m (which can be considered an average \"cluster radius\" in optimal cluster formation), the minimum distance of a point from any other center, w2 (x) = mini 6 = i (x) d (x), where c1,., ck are the cluster centers in an optimal data set, and i (x) is the index of the cluster, x belongs to."}, {"heading": "5.3 Efficient testability of the clusterability conditions", "text": "When it comes to testing whether a given clustering instance meets any of the above clustering capability conditions, one important point to note is that they are all formulated with respect to the state of optimal clustering of the given data. Finding such optimal clustering is NP-hard. Beyond that, however, as far as I am aware, there is no efficient algorithm for testing, since a dataset (X, d) and a clustering of the same, C is whether C is an optimal clustering property for (X, d) (say, w.r.t. Either the k mean or the k-median target) exists. I therefore suspect that testing each of the conditions we have discussed here is NP-hard.Some of these conditions can also be formulated as niceness property of a given clustering (rather than as property of the data). For example, given a k clustering C for an instance (X, d), defined by a vector of the centers, c1."}, {"heading": "5.4 Implications for common practical clustering algorithms", "text": "It would be very interesting to present results showing that a popular cluster algorithm (or the application of a practical approximation algorithm) efficiently guarantees high-quality clustering under different or more relaxed conditions. [3] Recent work can be seen as a step in this direction, asking under which separation conditions different convex relaxations restore exactly the \"right\" cluster formation. However, this work addresses a different version of cluster problems where the data is assumed to be generated by a parameterized generative model (a balanced mixture of spherical Gaussians in the case of this paper) and aims to restore these parameters."}, {"heading": "6 Conclusions", "text": "Several concepts of clusterability have been proposed so far. Depending on the values of the parameters that define these concepts, each of them ranges from very lenient to a highly restrictive data requirement. For each notion, there is a range of parameters, so that optimal clustering can be found rather trivially for data that meets the clusterability requirement in this area. Clusterability with parameter values sufficient for the efficient cluster results currently available proves to be quite strong requirements that greatly limit the practical significance of the results currently available. The current failure to support the CDNM thesis may come from various sources. First, perhaps the thesis is simply wrong, while it may well be the case that some practical cluster tasks are indeed computationally difficult for some real data instances, there are many other cases where data of practical interest is not to be found."}, {"heading": "6.1 Call for a change of perspective on the complexity of clustering", "text": "All of the above work, as well as most of the current theoretical work to calculate the complexity of clustering, focuses on concrete clustering objectives that aim to find the best clustering for a given number of clusters. However, the practice of clustering is broad. There are applications such as clustering to detect record duplications in databases (such as patient records from different hospitals and clinics) where the user does not specify the number of clusters in advance and aims to detect sets of similar items to each other to the extent that such sets occur in the input data. In other applications, such as vector quantification of signal transmission or setup tasks, while the objective function is usually fixed (say, k-means), there is no implicit \"target clustering\" and the usefulness of the resulting clustering is not diminished by finding different near to optimal solutions."}, {"heading": "7 Some followup open problems", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "Acknowledgements", "text": "I am grateful to Shalev Ben-David, Lev Rayzin, and Ruth Urner for the insightful discussions on this paper."}], "references": [{"title": "Clusterability: A theoretical study", "author": ["Margareta Ackerman", "Shai Ben-David"], "venue": "In AISTATS, pages", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Characterization of linkage-based clustering", "author": ["Margareta Ackerman", "Shai Ben-David", "David Loker"], "venue": "In COLT 2010 - The 23rd Conference on Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Relax, no need to round: Integrality of clustering formulations", "author": ["Pranjal Awasthi", "Afonso Bandera", "Moses Charikar", "Ravishankar Krishnaswami", "Soledad Voilar", "Rachel Ward"], "venue": "CoRR, Stat.ML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Stability yields a ptas for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In FOCS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Inf. Process. Lett.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In SODA,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "J. ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Yingyu Liang"], "venue": "In ICALP", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Alternative measures of computational complexity with applications to agnostic learning", "author": ["Shai Ben-David"], "venue": "In TAMC, pages 231\u2013235,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering", "author": ["Shai Ben-David"], "venue": "Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "On the theory of average case complexity", "author": ["Shai Ben-David", "Benny Chor", "Oded Goldreich", "Michael Luby"], "venue": "In STOC,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Stability of k -means clustering", "author": ["Shai Ben-David", "D\u00e1vid P\u00e1l", "Hans-Ulrich Simon"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Efficient learning of linear perceptrons", "author": ["Shai Ben-David", "Hans-Ulrich Simon"], "venue": "In NIPS, pages 189\u2013195,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Data stability in clustering: A closer look", "author": ["Shalev Ben-David", "Lev Reyzin"], "venue": "Theoretical Computer Science, page To appear,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "On the practically interesting instances of maxcut", "author": ["Yonatan Bilu", "Amit Daniely", "Nati Linial", "Michael Saks"], "venue": "In STACS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Are stable instances easy", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "In ICS, pages 332\u2013341,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability & Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Clustering is difficult only when it does not matter", "author": ["Amit Daniely", "Nati Linial", "Michael Saks"], "venue": "CoRR, abs/1205.4891,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Average case complete problems", "author": ["Leonid A. Levin"], "venue": "SIAM J. Comput.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1986}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "In FOCS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Data stability in clustering: A closer look", "author": ["Lev Reyzin"], "venue": "In ALT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Stability and model selection in k-means clustering", "author": ["Ohad Shamir", "Naftali Tishby"], "venue": "Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Smoothed analysis of algorithms: why the simplex algorithm usually takes polynomial time", "author": ["Daniel A. Spielman", "Shang-Hua Teng"], "venue": "In STOC,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}], "referenceMentions": [{"referenceID": 17, "context": "1This phrase is in fact a title of a recent paper \u2013 [18].", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "Average Case Complexity ([20], [11]), analyzes run time w.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Average Case Complexity ([20], [11]), analyzes run time w.", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "Smoothed Analysis ([25]) examines the running time of a given algorithm by taking the worst case over all inputs of the average runtime of the algorithm over some vicinity of the input.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "[9], and [16] propose general notions of tamed instances that apply across different problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[9], and [16] propose general notions of tamed instances that apply across different problems.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "Algorithms that efficiently solve NP-hard problems on such perturbation robust instances have been shown to exist for agnostic learning of half-spaces ([13]) and for graph partitioning problems ( [17]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "Algorithms that efficiently solve NP-hard problems on such perturbation robust instances have been shown to exist for agnostic learning of half-spaces ([13]) and for graph partitioning problems ( [17]).", "startOffset": 196, "endOffset": 200}, {"referenceID": 5, "context": "[6] formalized a uniqueness of the optimal solution criterion as a notion of well behaved clustering instances, which can also be applied to other types of problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (\u03b1, \u01eb) Perturbation Resilience, [8]; \u01eb -Separatedness, [22]; Uniqueness of optimum, [6] (they call it", "startOffset": 55, "endOffset": 58}, {"referenceID": 15, "context": "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (\u03b1, \u01eb) Perturbation Resilience, [8]; \u01eb -Separatedness, [22]; Uniqueness of optimum, [6] (they call it", "startOffset": 106, "endOffset": 110}, {"referenceID": 7, "context": "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (\u03b1, \u01eb) Perturbation Resilience, [8]; \u01eb -Separatedness, [22]; Uniqueness of optimum, [6] (they call it", "startOffset": 144, "endOffset": 147}, {"referenceID": 20, "context": "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (\u03b1, \u01eb) Perturbation Resilience, [8]; \u01eb -Separatedness, [22]; Uniqueness of optimum, [6] (they call it", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "These include: Additive perturbation robustness (APR), [1]; Multiplicative perturbation robustness (MPR), [16]; (\u03b1, \u01eb) Perturbation Resilience, [8]; \u01eb -Separatedness, [22]; Uniqueness of optimum, [6] (they call it", "startOffset": 196, "endOffset": 199}, {"referenceID": 4, "context": "(c, \u01eb)-approximation-stablility); \u03b1-center stability, [5]; and (1 + \u03b1) Weak Deletion Stability, [4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "(c, \u01eb)-approximation-stablility); \u03b1-center stability, [5]; and (1 + \u03b1) Weak Deletion Stability, [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 20, "context": "\u2022 The values of \u01eb for which \u01eb -Separatedness is shown (in [22]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, the average distance of a point from its cluster center should be smaller than the minimal distance between distinct cluster centers by a factor of at least 200.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "\u2022 The values of parameters for which (c, \u01eb) approximation stability is shown (in [6]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, for all but an \u01eb-fraction of the input points, the distance of a point to its own cluster center is smaller than its distance to the next closest center by at least 20 times the average point-to-its-cluster-center-distance.", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "\u2022 The values of \u03b1 for which (1 + \u03b1) weak deletion stability is shown (in [4]) to allow poly(k) clustering algorithms imply that, in the optimal clustering, the vast majority of the clusters are so distant from the rest of the data points that any point outside such a cluster is further from the center of that cluster by at least log(k) times the \u201daverage radius\u201d of its own cluster.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "(a) Additive perturbation robustness (APR) [1]5: An input set (X, d) is \u01eb-APR if for some optimal k-clustering C, for every d, if |d(x, y) \u2212 d(x, y)| \u2264 \u01eb for every x, y \u2208 X , then C \u2208 CO(X, d).", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "(b) Multiplicative perturbation robustness (MPR) [16]: An input set (X, d) is \u03b1-MPR if for some optimal k-clustering C such that for every d,", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "5The definition of robustness, as well as the implied efficiency of clustering result, in [1] are particular cases of a more general definition and more general results of [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "5The definition of robustness, as well as the implied efficiency of clustering result, in [1] are particular cases of a more general definition and more general results of [9].", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "6 The definition in [1] is formulated as robustness w.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "(c) [8] propose the following relaxation of the MPR requirement: A data set (X, d) is (\u03b1, \u01eb)-perturbation resilient if there exists some optimal k-clustering C such that for every d, if d(x, y) \u2264 d(x, y) \u2264 \u03b1d(x, y) for every x, y \u2208 X , then for some C \u2032 \u2208 CO(X, d), Derr(C,C ) \u2264 \u01eb.", "startOffset": 4, "endOffset": 7}, {"referenceID": 20, "context": "\u01eb -Separatedness: [22]7 discuss clustering w.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "Uniqueness of optimum: [7]8 define a data set to be (c, \u01eb)-approximationstable with respect to some target clustering CT if every clustering C of X whose objective cost over (X, d) is within a factor c of the objective cost of CT (on (X, d)) is \u01eb-close to CT w.", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "of [7]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "\u03b1-center stability: [5] define an instance (X, d) to be \u03b1-center stable (with respect to some center based clustering objective O) if for any optimal clustering C \u2208 CO(X, d) defined by centers c1, .", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "(1 + \u03b1) Weak Deletion Stability: [4] define an instance for k-clustering to satisfy the (1 + \u03b1) Weak Deletion Stability condition if, for all i 6= j,", "startOffset": 33, "endOffset": 36}, {"referenceID": 19, "context": "7This is a journal version of [21], where the definition and the main results were initially introduced.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "8This is a journal version of [6], where the definition and the main results were initially introduced.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Additive perturbation robustness (APR): [1] show that for every centerbased clustering objective and every \u03bc > 0 there exists an algorithm that", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "Using the results of [10] the parameter m in the runtime can be replaced by nk \u03bc\u01eb if one settles for a solution whose cost is at most OPT (X) + \u01eb|X|D(X) (recall that D(X) is the diameter of the input set).", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "Multiplicative perturbation robustness (MPR): [5] show that for every \u01eb > 0 there exists an algorithm that finds an optimal solution to the kmedian clustering problem for all inputs that are (3 \u2212 \u01eb)- MPR in time O (m +mk).", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "[8] improve these results to assuming only (1 + \u221a 2)-MPR", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] show that for the Max-Cut objective (which considers clustering into k = 2 clusters), there exist algorithms that find the optimal solution for any \u221a m-MPR input in time polynomial in m (they also show the existence of efficient algorithms for solving Max-Cut under other data assumptions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Furthermore, [8] show that there is a polynomial time algorithm that for any", "startOffset": 13, "endOffset": 16}, {"referenceID": 20, "context": "\u01eb -Separatedness: [22] focus of the k-means objective.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "Uniqueness of optimum: [7] propose algorithms that, for data sets that are (1 + \u03b1, \u01eb)-approximation-stable find, in time polynomial in m and k clusterings that are O(\u01eb+\u01eb/\u03b1) close (w.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "\u03b1-center stability: [8] present an algorithm that, for any \u03b1 \u2265 1 + \u221a 2 outputs an optimal k-median clustering, as well as a binary hierarchical clustering tree for which the optimal k-means clustering is a pruning of that tree, in time polynomial in m and k.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "(1 + \u03b1) Weak Deletion Stability: For the k-means objective, [4] propose an algorithm that given any positive k, \u01eb and \u03b1, for any input X satisfying the (1 + \u03b1) Weak Deletion Stability condition it finds a clustering C such that O(C) \u2264 (1 + \u01eb)OPT (X) in time m(k logm).", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "(a) Additive perturbation robustness (APR): The first point to note about the efficiency results of [1] is that they focus on the case of fixed number of clusters and therefore their runtime upper bounds are not polynomial in k.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "However, the efficiency of clustering results under this condition ([8]) apply only when the number of such violations does not exceed the number of points in the smallest cluster.", "startOffset": 68, "endOffset": 71}, {"referenceID": 20, "context": "1, the results are cited the way they appear in [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "In other words, in order satisfy the \u01eb-separatedness clusterability condition, with a parameter \u01eb that suffices to guarantee success of the [22] proposed algorithm, the data must be organized in small clusters that are extremely well separated.", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "Uniqueness of optimum: To appreciate the tradeoffs between data niceness requirements and the efficiency of the clustering algorithm (of [7] ) on such data, it is worthwhile to review Lemma 3.", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "Lemma 2 ([7]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "9While [7] phrases its results w.", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "On one hand we have the [8] efficiency result for \u03b1 > 1 + \u221a 2, and on the other hand there is an almost matching lower bound:", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "Theorem 3 ([14]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "Another relevant result of [14] is that once the parameter \u03b1 exceeds 2+ \u221a 3, data satisfying the \u03b1-center stability condition is somewhat trivial.", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": "(1 + \u03b1) Weak Deletion Stability: The [4] bound on the running time of the algorithm has only polynomial explicit dependence on the number of clusters k.", "startOffset": 37, "endOffset": 40}, {"referenceID": 21, "context": "10This is a journal version of [23] where the result initially appeared.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "clusterability requirement, parameterized by \u03b1, is not extremely strong, the running time formula of [4] is, in fact, exponential in k.", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "Furthermore, [4] show the following similar manifestation of the strong implications on the (1 + \u03b1) WDS condition, in terms of the lower bounds it implies on between-cluster-centers distances:", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "In fact, the positive results of [8], showing efficient clustering algorithm for", "startOffset": 33, "endOffset": 36}, {"referenceID": 20, "context": "Among all the works surveyed in this note, only one, the results of [22], address (a feasible variant of) a practical algorithm - the popular Lloyd clustering algorithm.", "startOffset": 68, "endOffset": 72}, {"referenceID": 2, "context": "The recent work of [3] can be viewed as a step in that direction.", "startOffset": 19, "endOffset": 22}, {"referenceID": 13, "context": "Even for that notion, the lower bounds of [14] require the input data to be an arbitrary metric space and do not apply to data in a Euclidean space.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": ", [12], [24].", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": ", [12], [24].", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "Clustering via linkage based hierarchical clustering trees: The algorithm of [8] is based on a linkage-based12 agglomerative construction of a cluster tree that is guaranteed to have any (1 + \u221a 2)-center stable clustering of the input data as a pruning of that tree.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "See [2] for a more detailed discussion.", "startOffset": 4, "endOffset": 7}], "year": 2015, "abstractText": "It is well known that most of the common clustering objectives are NPhard to optimize. In practice, however, clustering is being routinely carried out. One approach for providing theoretical understanding of this seeming discrepancy is to come up with notions of clusterability that distinguish realistically interesting input data from worst-case data sets. The hope is that there will be clustering algorithms that are provably efficient on such \u201cclusterable\u201d instances. In other words, hope that \u201cClustering is difficult only when it does not matter\u201d1 (the CDNM thesis for short). We believe that to some extent this may indeed be the case. This paper provides a survey of recent papers along this line of research and a critical evaluation their results. Our bottom line conclusion is that that CDNM thesis is still far from being formally substantiated. We start by discussing which requirements should be met in order to provide formal support the validity of the CDNM thesis. In particular, we list some implied requirements for notions of clusterability. We then examine existing results in view of these requirements and outline some research challenges and open questions. 1This phrase is in fact a title of a recent paper \u2013 [18].", "creator": "LaTeX with hyperref package"}}}