{"id": "1501.00503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2015", "title": "An Empirical Study of the L2-Boost technique with Echo State Networks", "abstract": "A particular case of Recurrent Neural Network (RNN) was introduced at the beginning of the 2000s under the name of Echo State Networks (ESNs). The ESN model overcomes the limitations during the training of the RNNs while introducing no significant disadvantages. Although the model presents some well-identified drawbacks when the parameters are not well initialised. The performance of an ESN is highly dependent on its internal parameters and pattern of connectivity of the hidden-hidden weights Often, the tuning of the network parameters can be hard and can impact in the accuracy of the models.", "histories": [["v1", "Fri, 2 Jan 2015 21:15:00 GMT  (97kb,D)", "http://arxiv.org/abs/1501.00503v1", "To appear in Journal of Network and Innovative Computing, Volume 2, Issue 1, pp. 120 - 127, 2014"]], "COMMENTS": "To appear in Journal of Network and Innovative Computing, Volume 2, Issue 1, pp. 120 - 127, 2014", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sebasti\\'an basterrech"], "accepted": false, "id": "1501.00503"}, "pdf": {"name": "1501.00503.pdf", "metadata": {"source": "CRF", "title": "An Empirical Study of the L2-Boost technique with Echo State Networks", "authors": ["Sebasti\u00e1n Basterrech"], "emails": ["Sebastian.Basterrech.Tiscordio@vsb.cz"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. BACKGROUND", "text": "In this section, we begin by specifying the context in which the ESN model and the L2 boost technique are applied. Next, we introduce the additive models and introduce a description of the L2 boost technique."}, {"heading": "A. Problem Specification", "text": "We start with the specification of a monitored learning problem. In the face of a data set L = {(x (t), y (t)): t = 1,.., T}, where the points x and y are either a class or a numerical response, we call Nx the dimension of the input vector x and Ny the dimension of the output vector y. We assume that the mapping between the input x and the output vector y is given by a certain unknown function F (\u00b7). The goal is to learn a parametric function F (x (t), L) in such a way that a certain error range between F (x (t), L) and y (t) is minimized for allt. The problem is called regression problem if the learning set has numerical variables. Otherwise, it is called classification problem (x)."}, {"heading": "B. Additive Models", "text": "In [4], the boosting model was analyzed under the form of an additive model. In view of a series of functions f (m): RNX \u2192 RNY, m = 1., M characterized by a series of parameters \u03b8 and expansion coefficients \u03b2, f (m) (x) = \u03b2 (m) h (x, \u03b8 (m), an additive model has the following formF (x) = M (m) (x). (1) The functions {h (x)} M1 are referred to as basic functions (x). They are not fixed a priori and are selected depending on the cost function used and the dataset. An important parameter of the model is the number of basic functions (M) considered in the expression (1). This parameter controls the generalization error of the model. Since the main objective in a learning task is to find a predictor with low generalization, the parameter M plays an important role in the accuracy of an additive model."}, {"heading": "III. MODELING TIME-SERIES WITH ECHO STATE NETWORKS", "text": "In fact, it is a purely intellectual game, which seeks to put people's interests first, not to put them centre stage."}, {"heading": "A. Formalization of the Echo State Network Model", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is not a country, a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "IV. EMPIRICAL RESULTS", "text": "We begin this section by describing the benchmark problems. Next, we specify the experimental setup. We close this section with an analysis of our empirical results."}, {"heading": "A. Description of the Benchmark Problems", "text": "We use the following range of time series benchmarks: \u2022 solid order NARMA. This dataset has a high nonlinearity and is widely used in the RC literature. We create the NARMA series according to the description in [28], [32], b (t + 1) = \u03b11 (t) + \u03b12b (t) k \u2212 1 \u2211 i = 0 b (t \u2212 i) + \u03b13s (t \u2212 (k \u2212 1))) s (t) + \u03b14, with s (t) \u0445 Unif [0, 0.5] and the constant values shown in Table I. To evaluate the memorization capability of the model, we consider two simulated series with k = 10 and k = 30. The task is to predict 1 \u03b12 \u03b13 \u03b14 10 0.3 0.05 1.5 0.1 30 0.2 0.004 1.5 0.001"}, {"heading": "B. Experimental Setup", "text": "We summarize the setting of the most important parameters related to the benchmark problems in Table II. Selected benchmarks have been widely used in the RC literature [7], [28], [31], [36]. In all cases, we use the Normalized Mean Square Error (NMSE) as the yardstick for the accuracy problem [9]. The learning method used to calculate the initial weight matrix was offline comb regression. This algorithm has a regularization parameter that we adjust for each benchmark problem. Prior to processing the data, the step was to normalize the patterns in the interval [0, 1] We examined the algorithm performance for several reservoir sizes. The range of reservoir sizes is specified for each benchmark problem. The link between the input layer and the reservoir [0.2] is fully connected to the voir matrix."}, {"heading": "C. Result Analysis", "text": "The list of candidates for president is long."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "In the early 2000s, an efficient technique for training and designing an RNN was developed under the name Echo State Network (ESN). This approach overcomes the limitations of training RNN using the Gradient Descent Method. The performance of an ESN depends to a large extent on its parameters and connectivity patterns of the hidden weights. In addition, network setup can be computationally expensive, especially for calculating the spectral radius of the hidden weight matrix. In this article, we explored the promotion of ideas with ESNs to build a robust new learning tool.In particular, we explored the use of L2 Boost with randomly initialized ESNs. We merge a number of weak individual ESNs because they are randomly initialized, and we do not use additional computational effort to do the initial hidden weights.Despite the realization of numerous tests, we cannot claim that L2 Boost is better initialized with ESN as a method that does not work well according to the hidden structure."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was developed as part of the project IT4Innovations Centre of Excellence, Register No. CZ.1.05 / 1.1.00 / 02.0070 supported by the Operational Programme \"Research and Development for Innovation\" financed by the Structural Funds of the European Union and the State Budget of the Czech Republic. In addition, this article was elaborated as part of the project New Creative Teams in Priorities of Scientific Research, Register No. CZ.1.07 / 2.3.00 / 30.0055."}], "references": [{"title": "The strength of weak learnability", "author": ["R.E. Shapire"], "venue": "Machine Learning, vol. 5, no. 2, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "AT&T Bell Laboratories, New Jersey, USA, Tech. Rep., January 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Experiments with a new Boosting Algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Machine Learning: Proceedings of Thirteenth International Conference, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Additive Logistic Regression: a Statistical View of Boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "The Annals of Statistics, vol. 28, no. 2, pp. 337\u2013407, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Boosting with the L2 loss: Regression and Classification", "author": ["P. B\u00fchlmann", "B. Yu"], "venue": "Journal of the American Statistical Association, vol. 98, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Liquid State Machines: Motivation, Theory, and Applications", "author": ["W. Maass"], "venue": "In Computability in Context: Computation and Logic in the Real World. Imperial College Press, 2010, pp. 275\u2013296.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks", "author": ["H. Jaeger"], "venue": "German National Research Center for Information Technology, Tech. Rep. 148, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Training Recurrent Networks by Evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F. Gomez"], "venue": "Neural Networks, vol. 19, pp. 757\u2013 779, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Reservoir Computing Approaches to Recurrent Neural Network Training", "author": ["M. Lukos\u0306evic\u0306ius", "H. Jaeger"], "venue": "Computer Science Review, pp. 127\u2013149, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A Practical Guide to Applying Echo State Networks", "author": ["M. Luko\u0161evi\u010dius"], "venue": "Neural Networks: Tricks of the Trade, ser. Lecture Notes in Computer Science, G. Montavon, G. Orr, and K.-R. M\u00fcller, Eds. Springer Berlin Heidelberg, 2012, vol. 7700, pp. 659\u2013686. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-35289-8_36", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "An Approach to Reservoir Computing Design and Training", "author": ["A.A. Ferreira", "T.B. Ludermir", "R.R.B. De Aquino"], "venue": "Expert Syst. Appl., vol. 40, no. 10, pp. 4172\u20134182, Aug. 2013. [Online]. Available: http://dx.doi.org/10.1016/j.eswa.2013.01.029", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Popular Ensemble Methods: an Empirical study", "author": ["D. Opitz", "R. Maclin"], "venue": "Journal of Artificial Intelligence Research, vol. 11, pp. 169\u2013198, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "An Empirical Study of L2-Boost with Echo State Networks", "author": ["S. Basterrech"], "venue": "IEEE Intelligent Systems Design and Applications (ISDA), December 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The elements of Statistical Learning, ser. Spring series in statistics", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Extensions to metric-based model selection", "author": ["Y. Bengio", "N. Chapados"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1209\u20131227, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Greedy Function Approximation: A Gradient Boosting Machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, vol. 29, pp. 1189\u20131232, 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Boosting Neural Networks", "author": ["H. Schwenk", "Y. Bengio"], "venue": "Neural Computation, vol. 12, no. 8, pp. 1869\u20131887, Aug. 2000.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1869}, {"title": "Numerical Recipes in C, 2nd ed", "author": ["W. Press", "S. Teukolsky", "W. Vetterling", "B. Flannery"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Learning Recurrent Neural Networks with Hessian-Free Optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceeding of the 28th International Conference on Machine Learning, 2011, pp. 1033\u20131040.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of the 30th International Conference on Machine Learning, vol. 28, pp. 37\u201348, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time computing without stable states: a new framework for a neural computation based on perturbations", "author": ["W. Maass", "T. Natschl\u00e4ger", "H. Markram"], "venue": "Neural Computation, pp. 2531\u20132560, november 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Backpropagation-Decorrelation: online recurrent learning with O(N) complexity", "author": ["J.J. Steil"], "venue": "Proceedings of IJCNN\u201904, vol. 1, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimization and applications of Echo State Networks with leaky-integrator neurons", "author": ["H. Jaeger", "M. Lukos\u0306evic\u0306ius", "D. Popovici", "U. Siewert"], "venue": "Neural Networks, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving Reservoirs using Intrinsic Plasticity", "author": ["B. Schrauwen", "M. Wardermann", "D. Verstraeten", "J.J. Steil", "D. Stroobandt"], "venue": "Neurocomputing, vol. 71, pp. 1159\u20131171, March 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Echo State Queueing Network: a new Reservoir Computing learning tool", "author": ["S. Basterrech", "G. Rubino"], "venue": "IEEE Consumer Comunications & Networking Conference (CCNC\u201913), January 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Reservoir Computing and Extreme Learning Machines for Non-linear Time-series Data Analysis", "author": ["J.B. Butcher", "D. Verstraeten", "B. Schrauwen", "C.R. Day", "P.W. Haycock"], "venue": "Neural Networks, vol. 38, pp. 76\u201389, feb 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimum Complexity Echo State Network", "author": ["A. Rodan", "P. Tin\u0306o"], "venue": "IEEE Transactions on Neural Networks, pp. 131\u2013144, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "On self-organizing reservoirs and their hierarchies", "author": ["M. Lukos\u0306evic\u0306ius"], "venue": "Jacobs University, Bremen, Tech. Rep. 25, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Studies on Reservoir Initialization and Dynamics Shaping in Echo State Networks", "author": ["J. Boedecker", "O. Obst", "N.M. Mayer", "M. Asada"], "venue": "Proceedings of the 17th European Symposium On Artificial Neural Networks (ESANN\u201909), Evere, Belgium, Apr. 2009, pp. 227\u2013232.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Self-Organizing Maps and Scale-Invariant Maps in Echo State Networks", "author": ["S. Basterrech", "C. Fyfe", "G. Rubino"], "venue": "Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on, nov. 2011, pp. 94\u201399.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Information dynamics based self-adaptive reservoir for delay temporal memory tasks", "author": ["S. Dasgupta", "F. Worgotter", "P. Manoonpong"], "venue": "Evolving Systems, pp. 1\u201315, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "The Santa Fe Time Series Competition Data", "author": ["U. Huebner", "N.B. Abraham", "C.O. Weiss"], "venue": "available at: http://goo.gl/6IKBb9, date of access: 12 September 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A two dimensional mapping with a strange attractor", "author": ["M. H\u00e9non"], "venue": "Commun. Math. Phys., vol. 50, pp. 69\u201377, 1976.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1976}, {"title": "Time series data library", "author": ["R. Hyndman"], "venue": "available at: http://goo.gl/PZzReR, date of access: 12 September 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "An experimental unification of reservoir computing methods", "author": ["D. Verstraeten", "B. Schrauwen", "M. D\u2019Haene", "D. Stroobandt"], "venue": "Neural Networks, no. 3, pp. 287\u2013289, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "It has been successful used in supervised learning problems since its apparition in the 1990s [1]\u2013[3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "It has been successful used in supervised learning problems since its apparition in the 1990s [1]\u2013[3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "Several variations of the original Boosting idea have been introduced over the years [4], [5], one of the most popular is called AdaBoost [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Several variations of the original Boosting idea have been introduced over the years [4], [5], one of the most popular is called AdaBoost [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "Several variations of the original Boosting idea have been introduced over the years [4], [5], one of the most popular is called AdaBoost [3].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "An analogy between AdaBoost and additive models was studied in [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "developed a variation of the Boosting technique called L2-Boost that is constructed from an additive model and the functional gradient descent method [5].", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "For instance, as far as we know one of the most popular RC models the Echo State Networks (ESN) [7], has the best known learning performance on the Mackey-Glass time-series prediction problem [8], [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 7, "context": "For instance, as far as we know one of the most popular RC models the Echo State Networks (ESN) [7], has the best known learning performance on the Mackey-Glass time-series prediction problem [8], [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 8, "context": "For instance, as far as we know one of the most popular RC models the Echo State Networks (ESN) [7], has the best known learning performance on the Mackey-Glass time-series prediction problem [8], [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 9, "context": "The setting of these parameters often requires the human expertise and several empirical trials [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "For instance, the time complexity of an algorithm that computes the spectral radius of a N \u00d7N matrix is equal to O(N) [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "There are empirical evidence in the Machine Learning literature that show that this baseline approach sometimes performs better than other ensemble methods [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "This work is a revised and expanded version of the article [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "In the case of regression problems, it is recommended to use the a quadratic distance [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "Even though we can also use a quadratic distance in classification problems, it is recommendable to use the Kullback-Leibler distance in this domain [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "Although, it can be also used for non-temporal supervised learning problems [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": ") we would like to predict the value y (k > 0) [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "In [4] was analyzed the Boosting model under the form of an Additive model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "A relationship between the gradient descent technique and stage-wise additive expansions was introduced at the beginning of the 2000s [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "The introduction of the gradient descent algorithm using a boosting approach was an essential contribution in the field of ensemble learning methods [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "It allowed to start to use boosting in regression problems [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "A Boost method for regression problems with quadratic error distance was introduced under the name of L2-Boost in [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Other boosting variants were presented for other kind of distances, some of them are described in [4], [5], [16].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Other boosting variants were presented for other kind of distances, some of them are described in [4], [5], [16].", "startOffset": 103, "endOffset": 106}, {"referenceID": 15, "context": "Other boosting variants were presented for other kind of distances, some of them are described in [4], [5], [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "We refer by epoch to the iteration of the training algorithm through all the patterns in the training set [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "Another difference between L2-Boost and other boosting methods is that L2-Boost presents a tendency to overfit the data [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "The model with contracting linear learners converge to the fully saturated model [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 17, "context": "Fit an initial model using a least squares fit (see [18]): F\u0302 (\u00b7) = h(\u00b7, \u03b8); for (m = 1, .", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "The circuits of the network enable to store temporal information, in order to learn and memorize the input history [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "Next, the network updates its hidden state via a non-linear activation function using the input pattern and the network state at the precedent time [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "The most important is that is hard to train a RNN using gradient descent methods [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "In [20] is analyzed the main limitations of the algorithms of the gradient descent type for training RNNs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "These drawbacks are identified under the names of vanishing and the exploding gradient problems [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "The exploding gradient phenomena refers to the opposite, when the gradient norm large increases during the training process [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "Recently, an effective algorithm to train RNN was introduced [19], the algorithm uses the Hessian-free Optimization for setting the network parameters.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "The two pioneering RC models are Echo State Network (ESN) [7] and Liquid State Machine (LSM) [22].", "startOffset": 58, "endOffset": 61}, {"referenceID": 21, "context": "The two pioneering RC models are Echo State Network (ESN) [7] and Liquid State Machine (LSM) [22].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "This computational paradigm covers the main limitations related to learning processes in RNNs obtaining acceptable performance in practical applications [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 22, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 171, "endOffset": 175}, {"referenceID": 23, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 222, "endOffset": 226}, {"referenceID": 7, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 236, "endOffset": 239}, {"referenceID": 24, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 262, "endOffset": 266}, {"referenceID": 25, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 297, "endOffset": 301}, {"referenceID": 26, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 344, "endOffset": 348}, {"referenceID": 6, "context": "In [7] was analyzed the stability of the reservoir dynamics in the ESN model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "It becomes independent of its initial conditions [9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "These conditions were summarized in the Echo State Property (ESP) [7].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "In practice, the stability of the ESN is almost always ensured when the spectral radius of the reservoir matrix is less than 1 [9], [28].", "startOffset": 127, "endOffset": 130}, {"referenceID": 27, "context": "In practice, the stability of the ESN is almost always ensured when the spectral radius of the reservoir matrix is less than 1 [9], [28].", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "The computation of the spectra requires an important computational effort [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "A guide about the initialization procedure can be seen from [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "In our experiments we use ridge linear regression for computing the readout weights w [28].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "In order to evaluate the performance of this procedure, we compare the reached accuracy of the L2-Boost technique with a simple baseline approach [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "We generate the NARMA serie following the description in [28], [32],", "startOffset": 57, "endOffset": 61}, {"referenceID": 31, "context": "We generate the NARMA serie following the description in [28], [32],", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "\u2022 The Santa Fe Laser data set [33].", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "These pulsations more or less follow the theoretical Lorenz model of a two level system [33].", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "It is a prototypical invertible map with chaotic solutions proposed in [34].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "The data is normalized in [0, 1].", "startOffset": 26, "endOffset": 32}, {"referenceID": 34, "context": "\u2022 Freedman\u2019s non linear time data set [35].", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 67, "endOffset": 70}, {"referenceID": 27, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 72, "endOffset": 76}, {"referenceID": 30, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 78, "endOffset": 82}, {"referenceID": 35, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "In all cases, we use the Normalized Mean Square Error (NMSE) as measure of accuracy model [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "The pre-processing data step consisted in normalizing the patterns in the interval [0, 1] We investigated the algorithm performance for several reservoir sizes.", "startOffset": 83, "endOffset": 89}, {"referenceID": 4, "context": "We can found a similar remarks for the L2-Boost technique in non-temporal learning tasks [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 27, "context": "The accuracy it is also of the same order that results presented in the RC literature using a single well-initialized ESN [28], [31].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "The accuracy it is also of the same order that results presented in the RC literature using a single well-initialized ESN [28], [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "This behavior about the impact of the reservoir size on the accuracy of the model, also happens with single ESNs [10], [28], [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "This behavior about the impact of the reservoir size on the accuracy of the model, also happens with single ESNs [10], [28], [31].", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "This behavior about the impact of the reservoir size on the accuracy of the model, also happens with single ESNs [10], [28], [31].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "166 (NMSE) 50 [28] 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "0425 (NMSE) 200 [28] 30TH NARMA 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "4542 (NRMSE) 100 [30] SANTA FE LASER 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 27, "context": "0184 (NMSE) 50 [28] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "00819 (NMSE) 200 [28] HENON MAP 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 27, "context": "00975 (NMSE) 50 [28] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "00868 (NMSE) 200 [28] FREEDMAN\u2019S 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "0004302 (MSE) 40 [31]", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "In the case of the Freedman\u2019s non linear time data, the reservoir initialization was done using the Scale Invariant Map method [31], and the Mean Square Error (MSE) was the error measure.", "startOffset": 127, "endOffset": 131}, {"referenceID": 29, "context": "The error measure was the Normalized Root Square Error (NRMSE) [30].", "startOffset": 63, "endOffset": 67}], "year": 2015, "abstractText": "A particular case of Recurrent Neural Network (RNN) was introduced at the beginning of the 2000s under the name of Echo State Networks (ESNs). The ESN model overcomes the limitations during the training of the RNNs while introducing no significant disadvantages. Although the model presents some well-identified drawbacks when the parameters are not well initialized. The performance of an ESN is highly dependent on its internal parameters and pattern of connectivity of the hiddenhidden weights Often, the tuning of the network parameters can be hard and can impact in the accuracy of the models. In this work, we investigate the performance of a specific boosting technique (called L2-Boost) with ESNs as single predictors. The L2-Boost technique has been shown to be an effective tool to combine \u201cweak\u201d predictors in regression problems. In this study, we use an ensemble of random initialized ESNs (without control their parameters) as \u201cweak\u201d predictors of the boosting procedure. We evaluate our approach on five well-know time-series benchmark problems. Additionally, we compare this technique with a baseline approach that consists of averaging the prediction of an ensemble of ESNs. Keywords-L2-boosting, Echo State Network, Time-series modeling, Reservoir Computing, Ensemble Methods", "creator": "LaTeX with hyperref package"}}}