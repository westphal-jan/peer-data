{"id": "1501.00687", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2015", "title": "On Enhancing The Performance Of Nearest Neighbour Classifiers Using Hassanat Distance Metric", "abstract": "We showed in this work how the Hassanat distance metric enhances the performance of the nearest neighbour classifiers. The results demonstrate the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance and Euclidian distance. Moreover, we proved that the Hassanat distance metric is invariant to data scale, noise and outliers. Throughout this work, it is clearly notable that both ENN and IINC performed very well with the distance investigated, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted from our results that there is no optimal algorithm that can solve all real-life problems perfectly; this is supported by the no-free-lunch theorem", "histories": [["v1", "Sun, 4 Jan 2015 15:37:20 GMT  (220kb)", "http://arxiv.org/abs/1501.00687v1", "Canadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015"]], "COMMENTS": "Canadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mouhammd alkasassbeh", "ghada a altarawneh", "ahmad b a hassanat"], "accepted": false, "id": "1501.00687"}, "pdf": {"name": "1501.00687.pdf", "metadata": {"source": "CRF", "title": "ON ENHANCING THE PERFORMANCE OF NEAREST NEIGHBOUR CLASSIFIERS USING HASSANAT DISTANCE METRIC", "authors": ["Mouhammd Alkasassbeh", "Ghada A. Altarawneh", "Ahmad B. Hassanat"], "emails": ["malkasasbeh@gmail.com,", "ahmad.hassanat@gmail.com"], "sections": [{"heading": null, "text": "In fact, the majority of people who are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "NEAREST NEIGHBOUR CLASSIFIERS", "text": "We will describe the traditional KNN, Inverted Indexes of Neighbors Classifier (IINC) (Jirina & Jirina, 2008; Jirina & Jirina, 2011) and Ensemble Nearest Neighbour Classifier (ENN) (Hassanat, 2014)."}, {"heading": "KNN", "text": "KNN is a very simple but effective classifier used for pattern classification. It categorizes an unlabeled test example with the label of the majority of the examples among its nearest (most similar) neighbors in the training set (see Figure 1). The similarity depends on a specific distance metric, usually ED or MD, so the performance of the classifier significantly depends on the distance metric used. Due to its simplicity and popularity, it has been extensively used in pattern recognition, machine learning, text categorization, data mining, object labeling, etc. (Kataria & Singh, 2013) (Bhatia & Vandana, 2010) and (Hassanat, 2009) it has some limitations, according to asCanadian Journal of Pure and Applied Sciences (CJPAS). Volume 9, Issue 1, February 2015. Storage requirement and time complexity because it is completely dependent on each example in the training set."}, {"heading": "ENN", "text": "The ENN classifier (Hassanat, 2014) uses an ensemble learning approach based on the same nearest neighbor rule. In essence, the traditional KNN classifier is changed each time by a different K. Starting from k = 1 to k = the square root of the number of examples in the training set, each classifier selects for a specific class. Then, it uses the weighted sum rule to identify the class, i.e. the class with the highest sum (by 1-NN, 3-NN, 5-NN...) is chosen. The weight used is expressed by: () = () (3) When a test point is compared with all examples, using odd numbers to increase the speed of the algorithm by avoiding the even classifiers and to avoid the chance that two different classes have the same number of votes. The weight used is expressed by: () = () (3) When a test point is compared with all examples, the sum of classes is created by A, and the next function is weighted to an array (S)."}, {"heading": "Hassanat distance metric", "text": "The similarity function between any two points using the Hassanat metric (Hassanat, 2014) is: (!, \"< 1 >?.\" @ A, BA? CD @ A, BA, E F1 >?. \"@ A, BA |?.\" @ A, BA |? CD @ A, BA |?. \"@ A, BA |, E Along the vector dimensions, the distance is: DHIJJIKIL A, B, OD AP, where A and B are both vectors of size m. A real number. This measurement is invariant to similarity measurements, noise, and outliers because it limits the interval [0, 1 [. It reaches 1 only when the maximum value approaches infinity, or when the minimum value approaches minus infinity. This is represented by Figure 3 and Eq. 8."}, {"heading": "DATA USED FOR OUR EXPERIMENTS", "text": "To evaluate the efficiency of the Hassanate distance when used with some classifiers, twenty-eight sets have been selected to represent real-world classification problems taken from the UCI Machine Learning Repository (Bache & Lichman, 2013), which are databases, domain theories, and data generators used by the machine learning community since the!, \"[0!,\"; 0 + (6), B Y (7) i, and Bi arebyA, B Y 1 (8) miscellaneous data was created in 1987 by David Aha and fellow UC Irvine students, and is used by researchers, students, and educators around the world as the primary source of machine learning datasets. To learn more about each set of data, visit http: / / mlr.cs.umass.edu / ml / we in our thesis, where # E means the number of examples, and # F means the number of features and # C means."}, {"heading": "RESULTS AND DISCUSSION", "text": "Each data set is divided into two data sets, one for training, and the other for testing. 30% of the data set is used for testing, and 70% of the data is for training. Each classifier is used to classify the test samples at Manhattan distance (see Table 2). All exper Hassanat distance (see Table 3) was used as a test sample, experiment on each data set were random examples of testing and training. The accuracy of each classifier in each data set is the average of 10 rounds. Table 1 shows the data setber of Classes.iments were repeated with 30% of the data that were repeated, and each was repeated to which it is used in each data set."}, {"heading": "CONCLUSION", "text": "This work is a new attempt to improve the performance of some closest classifiers using Hassanate Distance Measurement. Experimental results from a variety of real-world datasets have shown the superiority of this distance measurement metric over traditional and most commonly used distances, such as the distance from Manhattan. Furthermore, we have proven that this distance measurement metric is invariable to data scales, noise, and outliers, and therefore recommend other researchers to use such a distance for other classification problems. Our future work will focus on exploiting and investigating the power of Hassanate Distance Measurement for other real-world problems, such as content-based image repetition and cluster problems."}, {"heading": "ACKNOWLEDGMENT", "text": "All records used in this post are from the UCI Irvine Machine Learning Repository, so the authors would like to thank and acknowledge the people behind this great corpus, as well as the anonymous reviewers of this post."}, {"heading": "EVALUATION OF DISTANCE METRICS: APPLICATION TO FINGERPRINT RECOGNITION.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "International Journal of Pattern Recognition and Artificial", "text": "Intelligence, 25 (6), pp.777-806.Bhatia, N. & Vandana, A., 2010. Survey of Nearest Neighbor Techniques. (IJCSIS) International Journal of Computer Science and Information Security, 8 (2), pp.30205.Cover, T.M. & Hart, P.E., 1967. Nearest Neighbor Pattern Classification. IEEE Trans. Inform. Theory, IT-13, pp.2127.Duda, R.O., Hart, P.E. & Stork, D.G., 2001. Pattern Classification. 2nd ed. Wiley.Fix, E. & Hodges, J., 1951. 4 Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties. Randolph Field, Texas: USAF School of Aviation Medicine.Hamamoto, Y., Uchimura, S. & Tomita, Ensemble."}, {"heading": "International Conference on Intelligent Systems Design and", "text": "Applications (ISDA2010), Cairo, 2010.Canadian Journal of Pure and Applied Sciences (CJPAS), Volume 9, Issue 1, February 2015, Jirina, M. & Jirina, M.J., 2011, Classifiers Based on Inverted Distances, In K. Funatsu, Ed. New Fundamental Technologies in Data Mining, InTech. Ch. 19. pp.369-87. Kataria, A. & Singh, M.D., 2013, A Review of Data Classification Using K-Nearest Neighbour Algorithm."}, {"heading": "International Journal of Emerging Technology and", "text": "Advanced Engineering, 3 (6), pp.354-60.Nathan Krislock, H.W., 2012. Euclidean Distance Matrices and Applications. Springer US.Weinberger, K.Q. & Saul, L.K., 2009. Distance Metric Learning for Large Margin Nearest Neighbor Classification. Journal of Machine Learning Research, 10, pp.207-44.Wu, X.a.V.K.e., 2010. The Top Ten Algorithms in Data Mining. CRC Press.Yang, L., 2006. Distance Metric Learning: A comprehensive survey. Technical Report. Michigan State University."}], "references": [{"title": "Voting Over Multiple Condensed Nearest Neoghbors", "author": ["E. Alpaydin"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Alpaydin,? \\Q1997\\E", "shortCiteRegEx": "Alpaydin", "year": 1997}, {"title": "PERFORMANCE EVALUATION OF DISTANCE METRICS: APPLICATION TO FINGERPRINT RECOGNITION", "author": ["S.D. Bharkad", "M. Kokare"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bharkad and Kokare,? \\Q2011\\E", "shortCiteRegEx": "Bharkad and Kokare", "year": 2011}, {"title": "Survey of Nearest Neighbor Techniques", "author": ["N. Bhatia", "A. Vandana"], "venue": "(IJCSIS) International Journal of Computer Science and Information Security,", "citeRegEx": "Bhatia and Vandana,? \\Q2010\\E", "shortCiteRegEx": "Bhatia and Vandana", "year": 2010}, {"title": "Nearest Neighbor Pattern Classification", "author": ["T.M. Cover"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Cover,? \\Q1967\\E", "shortCiteRegEx": "Cover", "year": 1967}, {"title": "Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties. Randolph Field, Texas: USAF School of Aviation Medicine", "author": ["E. Fix", "J. Hodges"], "venue": null, "citeRegEx": "Fix and Hodges,? \\Q1951\\E", "shortCiteRegEx": "Fix and Hodges", "year": 1951}, {"title": "A Bootstrap Technique for Nearest Neighbor Classifier Design", "author": ["Y. Hamamoto", "S. Uchimura", "S. Tomita"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,", "citeRegEx": "Hamamoto et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hamamoto et al\\.", "year": 1997}, {"title": "Visual Words for Automatic LipReading", "author": ["A.B. Hassanat"], "venue": "PhD Thesis. Buckingham, UK: University of Buckingham", "citeRegEx": "Hassanat,? \\Q2009\\E", "shortCiteRegEx": "Hassanat", "year": 2009}, {"title": "Dimensionality Invariant Similarity Measure", "author": ["A.B. Hassanat"], "venue": "Journal of American Science,", "citeRegEx": "Hassanat,? \\Q2014\\E", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach", "author": ["A.B. Hassanat"], "venue": "International Journal of Computer Science and Information Security,", "citeRegEx": "Hassanat,? \\Q2014\\E", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Using Singularity Exponent in Distance Based Classifier", "author": ["M. Jirina", "M.J. Jirina"], "venue": "In Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA2010). Cairo,", "citeRegEx": "Jirina and Jirina,? \\Q2010\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2010}, {"title": "Classifiers Based on Inverted Distances", "author": ["M. Jirina", "M.J. Jirina"], "venue": "ed. New Fundamental Technologies in Data Mining. InTech. Ch", "citeRegEx": "Jirina and Jirina,? \\Q2011\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2011}, {"title": "A Review of Data Classification Using K-Nearest Neighbour Algorithm", "author": ["A. Kataria", "M.D. Singh"], "venue": "International Journal of Emerging Technology and Advanced Engineering,", "citeRegEx": "Kataria and Singh,? \\Q2013\\E", "shortCiteRegEx": "Kataria and Singh", "year": 2013}, {"title": "Euclidean Distance Matrices and Applications", "author": ["H.W. Nathan Krislock"], "venue": null, "citeRegEx": "Krislock,? \\Q2012\\E", "shortCiteRegEx": "Krislock", "year": 2012}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "The top ten algorithms in data mining", "author": ["Wu", "X.a.V.K.e"], "venue": null, "citeRegEx": "Wu and X.a.V.K.e.,? \\Q2010\\E", "shortCiteRegEx": "Wu and X.a.V.K.e.", "year": 2010}, {"title": "Distance metric learning: A comprehensive survey", "author": ["L. Yang"], "venue": "Technical report", "citeRegEx": "Yang,? \\Q2006\\E", "shortCiteRegEx": "Yang", "year": 2006}], "referenceMentions": [{"referenceID": 5, "context": "It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers (Hamamoto et al., 1997)(Alpaydin, 1997).", "startOffset": 119, "endOffset": 142}, {"referenceID": 0, "context": ", 1997)(Alpaydin, 1997).", "startOffset": 7, "endOffset": 23}, {"referenceID": 15, "context": "New similarity measures are needed, in particular, for use in distance learning (Yang, 2006), where classifiers such as the k-nearest neighbour (KNN) are heavily dependent upon choosing the best distance.", "startOffset": 80, "endOffset": 92}, {"referenceID": 7, "context": "To solve those problems, Hassanat proposed an interesting distance metric (Hassanat, 2014), which is invariant to the different scales in multi dimensions data.", "startOffset": 74, "endOffset": 90}, {"referenceID": 7, "context": "We will describe the traditional KNN, Inverted Indexes of Neighbours Classifier (IINC) (Jirina & Jirina, 2008; Jirina & Jirina, 2011) and Ensemble Nearest Neighbour classifiers (ENN) (Hassanat, 2014).", "startOffset": 183, "endOffset": 199}, {"referenceID": 6, "context": "(Kataria & Singh, 2013)(Bhatia & Vandana, 2010) and (Hassanat, 2009).", "startOffset": 52, "endOffset": 68}, {"referenceID": 7, "context": "The ENN classifier (Hassanat, 2014) uses an ensemble learning approach based on the same nearest neighbour rule.", "startOffset": 19, "endOffset": 35}, {"referenceID": 7, "context": "Simple example showing the ENN classifier (Hassanat, 2014)", "startOffset": 42, "endOffset": 58}, {"referenceID": 7, "context": "The similarity function between any two points using Hassanat Metric (Hassanat, 2014) is written as: :(!\" , ;\" < 1 > ?\" .", "startOffset": 69, "endOffset": 85}, {"referenceID": 7, "context": "Representation of Hassanat distance metric between the points 0 and n, where n belongs to [-10, 10](Hassanat, 2014)", "startOffset": 99, "endOffset": 115}, {"referenceID": 7, "context": "As can be seen from Table 2, the results in general are worse than those in (Hassanat, 2014), because we repeated the experiments without normalizing the data sets using Manhattan (same) distance.", "startOffset": 76, "endOffset": 92}, {"referenceID": 7, "context": "These results confirm some results in (Hassanat, 2014), such as the superiority of the IINC and the ENN classifiers, in terms of being independent from choosing the optimal k neighbours.", "startOffset": 38, "endOffset": 54}, {"referenceID": 7, "context": "1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy, and this result is confirmed by (Hassanat, 2014).", "startOffset": 126, "endOffset": 142}], "year": 2015, "abstractText": "We showed in this work how the Hassanat distance metric enhances the performance of the nearest neighbour classifiers. The results demonstrate the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance and Euclidian distance. Moreover, we proved that the Hassanat distance metric is invariant to data scale, noise and outliers. Throughout this work, it is clearly notable that both ENN and IINC performed very well with the distance investigated, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted from our results that there is no optimal algorithm that can solve all reallife problems perfectly; this is supported by the no-free-lunch theorem.", "creator": "PDFCreator Version 1.7.3"}}}