{"id": "1501.01242", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2015", "title": "Efficient Online Relative Comparison Kernel Learning", "abstract": "Learning a kernel matrix from relative comparison human feedback is an important problem with applications in collaborative filtering, object retrieval, and search. For learning a kernel over a large number of objects, existing methods face significant scalability issues inhibiting the application of these methods to settings where a kernel is learned in an online and timely fashion. In this paper we propose a novel framework called Efficient online Relative comparison Kernel LEarning (ERKLE), for efficiently learning the similarity of a large set of objects in an online manner. We learn a kernel from relative comparisons via stochastic gradient descent, one query response at a time, by taking advantage of the sparse and low-rank properties of the gradient to efficiently restrict the kernel to lie in the space of positive semidefinite matrices. In addition, we derive a passive-aggressive online update for minimally satisfying new relative comparisons as to not disrupt the influence of previously obtained comparisons. Experimentally, we demonstrate a considerable improvement in speed while obtaining improved or comparable accuracy compared to current methods in the online learning setting.", "histories": [["v1", "Tue, 6 Jan 2015 17:19:06 GMT  (83kb,D)", "https://arxiv.org/abs/1501.01242v1", "Extended version of the paper appearing in The Proceedings of the 2015 SIAM International Conference on Data Mining (SDM15)"], ["v2", "Mon, 12 Jan 2015 14:10:03 GMT  (83kb,D)", "http://arxiv.org/abs/1501.01242v2", "Extended version of the paper appearing in The Proceedings of the 2015 SIAM International Conference on Data Mining (SDM15)"]], "COMMENTS": "Extended version of the paper appearing in The Proceedings of the 2015 SIAM International Conference on Data Mining (SDM15)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eric heim", "matthew berger", "lee m seversky", "milos hauskrecht university of pittsburgh", "air force research laboratory", "information directorate)"], "accepted": false, "id": "1501.01242"}, "pdf": {"name": "1501.01242.pdf", "metadata": {"source": "CRF", "title": "Efficient Online Relative Comparison Kernel Learning", "authors": ["Eric Heim", "Matthew Berger", "Lee M. Seversky", "Milos Hauskrecht"], "emails": ["milos}@cs.pitt.edu", "lee.seversky}@us.af.mil"], "sections": [{"heading": null, "text": "Keywords Online Learning, Kernel Learning, Relative Comparisons."}, {"heading": "1 Introduction", "text": "It is only a matter of time before it will happen, until it will happen."}, {"heading": "2 Related Work", "text": "The problem of learning a core matrix, driven by relative comparative feedback, has been the focus of recent work. Most of the newer techniques differ mainly in the choice of loss function. For example, Generalized Non-metric Multidimensional Scaling [1] uses hinged losses, Crowd Kernel Learning [25] uses a scale invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function which, given the Euclidean distances, can be considered as a solution to a nucleated specific case of the classic non-metric scaling problem."}, {"heading": "3 Preliminaries", "text": "In this section we formally define RCKL and provide a brief overview of the RCKL methods. Let Sn + learn the set of n \u00b7 n PSD matrices, and Mab is the entry in a column b of a matrix M. The goal of RCKL is to learn a PSD matrix K + about n objects, since a set of triplets (a, b), d2K (a, c), where d2K (a, b) is more similar than c), that quared distance constraints are met: (3,2), (a, c), (b) < d2K (a, c), where d2K (a) = K aa + Kbb \u2212 2Kab.We say a kernel K satisfies a triplet ti = (ai, bi, ci)."}, {"heading": "4 Efficient Online Relative Comparison Kernel Learning (ERKLE)", "text": "This projection is usually found by first finding the original composition of K \"i\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii ii ii\" ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii ii \"ii\" ii \"ii\" ii \"ii\" ii ii \"ii\" ii ii \"ii\" ii \"ii\" ii \"ii\" ii ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii \"ii\" ii"}, {"heading": "5 Experiments", "text": "In this section, we evaluate ERKLE by comparing it to batch RCKL methods. Batch methods are not really applicable to the online learning environment, but can be applied in what is often referred to as \"mini-batches.\" In the mini-batch learning environment, each time a new batch of m-triplets is received, which is performed on all received triplets so far. So we compare ERKLE to their batch counterparts in mini-batches.We evaluate each method on four different data sets, each with its own challenges. First, we start with a small synthetic experiment to evaluate how the methods work in an idealized environment. Second, a large synthetic experiment is conducted to show how ERKLE and Batch compare in terms of practical runtime. Third, a data set of triplets about popular music artists is used to evaluate how the methods work in a real environment."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we have developed a method to learn a PSD kernel matrix from relative comparisons applied in an online manner. Taking advantage of the sparse and low structure of the online formulation, we show how we can perform stochastic gradient updates of complexity O (n2). We show how passive-aggressive online learning benefits our method in terms of generalizing invisible triplets, and in conjunction with the stochastic gradient structure that allows us to perform a small number of necessary PSD projections in practice. Experimentally, we show on synthetic and real data that our method learns cores that are as generalized and often better at relative comparisons than batch methods. For future work, we wish to improve the online RCKL in three ways."}, {"heading": "A Derivation of STE Passive-Aggressive Step Size", "text": "To derive the STE version of the passive-aggressive step variable, we would like to solve the following optimization (4,19): min \u03b4j\u03b42js.t. p K \u2032 j tj \u2265 P, \u03b4j \u2265 0As with the GNMDS derivative assuming that the triplet is not fulfilled with a probability greater or equal to P, only a positive value of \u03b4j can fulfill the first constraint, making the positive constraint on \u03b4j superfluous. Furthermore, the smallest \u03b4j that fulfills the remaining constraint is the one that makes the left side exactly zero. Consequently, the inequality constraint can be treated as equality. Next, we take the Lagrange: \u03b42j + \u03b1 (log (P) \u2212 log (1 \u2212 P) + \u03b1 (dKj \u2212 P) + \u03b1 (dKj \u2212 P) + \u03b1 (dKj \u2212 P) + \u03b1 (dKj \u2212 P) with respect to this protocol (a, b) + \u03b1 (1) \u2212 dKJ with respect to the partial solution (c), \u2212 this \u2212 P \u2212 this is a solution \u2212 j \u2212 P \u2212 1 in relation to dough \u2212 P \u2212 1 \u2212 P \u2212 a \u2212 P \u2212 -1."}, {"heading": "B ERKLE with Multiple Passes", "text": "Algorithm 2 ERKLE with Multiple Passes Input: \u03b2: # of triplets step over 1: K0 \u2190 I 2: for j = 1, 2,... do 3: K \u2032 j \u2190 Kj \u2212 1 \u2212 \u017e\u017e\u017el (Kj \u2212 1, tj) 4: Kj \u2012 1S + (K \u2032 j) 5: if j > 2\u03b2 then 6: for k = 1, 2,..., \u03b2 \u2212 1 do 7: end for 11: end forAlgorithm 1 is similar to the original ERKLE algorithm. After a sufficient number of triplets has been achieved (in our experiments we chose 2\u03b2), \u03b2 \u2212 1 triplets are selected from all previously observed triplets, we can reduce the probability of the pleplets from the original ERKLE algorithms that were selected later, not in the originally selected, but in the randomly selected 7 triplets."}], "references": [{"title": "Generalized non-metric multidimensional scaling", "author": ["S. Agarwal", "J. Wills", "L. Cayton", "G. Lanckriet", "D.J. Kriegman", "S.J. Belongie"], "venue": "AISTATS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Sequence labelling svms trained in one pass", "author": ["A. Bordes", "N. Usunier", "L. Bottou"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 146\u2013161. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks, 17:9", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient low-rank stochastic gradient descent methods for solving semidefinite programs", "author": ["J. Chen", "T. Yang", "S. Zhu"], "venue": "AISTATS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "JMLR, 7:551\u2013 585", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning perceptual kernels for visualization design", "author": ["C. Demiralp", "M.S. Bernstein", "J. Heer"], "venue": "Infovis", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "The quest for ground truth in musical artist similarity", "author": ["D.P.W. Ellis", "B. Whitman", "A. Berenzweig", "S. Lawrence"], "venue": "ISMIR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Projection-free online learning", "author": ["E. Hazan", "S. Kale"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-dimensional embedding using adaptively selected ordinal data", "author": ["K.G. Jamieson", "R.D. Nowak"], "venue": "Allerton", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "SIGKDD", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Ordrec: an ordinal model for predicting personalized item rating distributions", "author": ["Y. Koren", "J. Sill"], "venue": "RecSys", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Whittlesearch: Image search with relative attribute feedback", "author": ["A. Kovashka", "D. Parikh", "K. Grauman"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonmetric multidimensional scaling: a numerical method", "author": ["J.B. Kruskal"], "venue": "Psychometrika, 29(2):115\u2013129", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1964}, {"title": "Music information retrieval using social tags and audio", "author": ["M. Levy", "M. Sandler"], "venue": "Multimedia, IEEE Transactions on, 11(3):383\u2013395", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic gradient descent with only one projection", "author": ["M. Mahdavi", "T. Yang", "R. Jin", "S. Zhu", "J. Yi"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multi-modal similarity", "author": ["B. McFee", "G. Lanckriet"], "venue": "JMLR, 12:491\u2013523", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["E. Moulines", "F.R. Bach"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, 42(3):145\u2013175", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pages 400\u2013407", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Nicolas L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "In NIPS", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Learning with kernels: support vector machines", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "regularization, optimization, and beyond. MIT press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Absolute identification by relative judgment", "author": ["N. Stewart", "G.D.A. Brown", "N. Chater"], "venue": "Psychological review, 112(4):881", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptively learning the crowd kernel", "author": ["O. Tamuz", "C. Liu", "O. Shamir", "A. Kalai", "S.J. Belongie"], "venue": "ICML", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic triplet embedding", "author": ["L. Van Der Maaten", "K. Weinberger"], "venue": "In MLSP,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Similarity comparisons for interactive fine-grained categorization", "author": ["C. Wah", "G. Van Horn", "S. Branson", "S. Maji", "P. Perona", "S.J. Belongie"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Breaking the curse of kernelization: Budgeted stochastic gradient descent for largescale svm training", "author": ["Z. Wang", "K. Crammer", "S. Vucetic"], "venue": "JMLR, 13(1):3103\u20133131", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "NIPS", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "and R", "author": ["E. Zudilova-Seinstra", "T. Adriaansen"], "venue": "van Liere. Trends in interactive visualization: state-of-the-art survey. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].", "startOffset": 125, "endOffset": 145}, {"referenceID": 9, "context": "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].", "startOffset": 125, "endOffset": 145}, {"referenceID": 11, "context": "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].", "startOffset": 125, "endOffset": 145}, {"referenceID": 13, "context": "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].", "startOffset": 125, "endOffset": 145}, {"referenceID": 28, "context": "It has been shown that by incorporating human feedback, the overall performance of such applications can be greatly improved [12, 10, 13, 15, 30].", "startOffset": 125, "endOffset": 145}, {"referenceID": 21, "context": "Kernels are used for modeling object relationships in many learning techniques [23], and hence are applicable to many methods that utilize kernels for these applications.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "For instance, naive forms of supervision such as numerical judgments between pairs of objects have been shown to be very noisy [24].", "startOffset": 127, "endOffset": 131}, {"referenceID": 25, "context": "Recent works addressing fine-grained categorization [27] and perceptual visualization design [6] have shown the practicality and benefit of learning kernels from relative comparisons.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Recent works addressing fine-grained categorization [27] and perceptual visualization design [6] have shown the practicality and benefit of learning kernels from relative comparisons.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Many RCKL methods [1, 26] learn a kernel by solving a semidefinite program (SDP) in batch, where all obtained relative comparisons are required to learn the kernel.", "startOffset": 18, "endOffset": 25}, {"referenceID": 24, "context": "Many RCKL methods [1, 26] learn a kernel by solving a semidefinite program (SDP) in batch, where all obtained relative comparisons are required to learn the kernel.", "startOffset": 18, "endOffset": 25}, {"referenceID": 23, "context": "For example in crowdsourcing, it is often of interest to minimize the number of dispatched tasks and thus the cost of the crowd by leveraging active learning techniques [25, 9] to adaptively select the most informative relative comparison query.", "startOffset": 169, "endOffset": 176}, {"referenceID": 8, "context": "For example in crowdsourcing, it is often of interest to minimize the number of dispatched tasks and thus the cost of the crowd by leveraging active learning techniques [25, 9] to adaptively select the most informative relative comparison query.", "startOffset": 169, "endOffset": 176}, {"referenceID": 2, "context": "ERKLE employs stochastic gradient descent [3] for RCKL, taking advantage of the sparse and low-rank structure of the RCKL gradient over a single comparison to devise fast updates that only require finding the smallest eigenvector and eigenvalue of a ar X iv :1 50 1.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "We show that the gradient structure, which enables such an efficient update, generalizes several wellknown convex RCKL methods [1, 26].", "startOffset": 127, "endOffset": 134}, {"referenceID": 24, "context": "We show that the gradient structure, which enables such an efficient update, generalizes several wellknown convex RCKL methods [1, 26].", "startOffset": 127, "endOffset": 134}, {"referenceID": 4, "context": "Motivated by work in online learning [5], we also derive a passive-aggressive version of ERKLE to ensure learned kernels model the most recently obtained relative comparisons without over-fitting.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "For instance, Generalized Non-metric Multidimensional Scaling [1] employs hinge loss, Crowd Kernel Learning [25] uses a scale-invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function.", "startOffset": 62, "endOffset": 65}, {"referenceID": 23, "context": "For instance, Generalized Non-metric Multidimensional Scaling [1] employs hinge loss, Crowd Kernel Learning [25] uses a scale-invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function.", "startOffset": 108, "endOffset": 112}, {"referenceID": 24, "context": "For instance, Generalized Non-metric Multidimensional Scaling [1] employs hinge loss, Crowd Kernel Learning [25] uses a scale-invariant loss, and Stochastic Triplet Embedding [26] uses a logistic loss function.", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "The aforementioned RCKL methods can be viewed as solving a kernelized special case of the classic non-metric multidimensional scaling problem [14], where the goal is to find an embedding of objects in R such that they satisfy given Euclidean distance constraints.", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Stochastic gradient descent techniques [21] are a popular class of methods for online learning of high-dimensional data for a very general class of functions, where recent techniques [29, 22] have demonstrated competitive performance with batch techniques.", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "Stochastic gradient descent techniques [21] are a popular class of methods for online learning of high-dimensional data for a very general class of functions, where recent techniques [29, 22] have demonstrated competitive performance with batch techniques.", "startOffset": 183, "endOffset": 191}, {"referenceID": 20, "context": "Stochastic gradient descent techniques [21] are a popular class of methods for online learning of high-dimensional data for a very general class of functions, where recent techniques [29, 22] have demonstrated competitive performance with batch techniques.", "startOffset": 183, "endOffset": 191}, {"referenceID": 7, "context": "In particular, recent methods [8, 16] have developed efficient methods to solve SDPs in an online fashion.", "startOffset": 30, "endOffset": 37}, {"referenceID": 14, "context": "In particular, recent methods [8, 16] have developed efficient methods to solve SDPs in an online fashion.", "startOffset": 30, "endOffset": 37}, {"referenceID": 3, "context": "The work of [4] shows how to devise efficient update schemes for solving SDPs when the gradient of the objective function is low-rank.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "Our passive-aggressive step size procedure is similar to that which is introduced in [5] for other online learning problems.", "startOffset": 85, "endOffset": 88}, {"referenceID": 24, "context": "The Stochastic Triplet Embedding (STE) approach of [26] defines l (K, t) = \u2212 log pt as the loss function, where pt is the probability that a triplet is satisfied:", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "Generalized Nonmetric Multidimensional Scaling (GNMDS) [1] uses a hinge loss, where l (K, t = (a, b, c)) is defined as:", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "1 Stochastic Gradient Step To create an efficient and online framework for RCKL \u2013 ERKLE \u2013 we leverage stochastic gradient descent techniques [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "For more discussion on this characteristic of stochastic methods see [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "If Kj has one negative eigenvalue, line 4 of Algorithm 1 results in a PSD matrix Kj that is closest to Kj in terms of Frobenius distance by Case 2 of Theorem 4 in [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "Early work [3] on the topic of learning rates suggest that \u03b4j should satisfy two constraints: \u2211\u221e j=1 \u03b4 2 j < \u221e and \u2211\u221e j=1 \u03b4j = \u221e.", "startOffset": 11, "endOffset": 14}, {"referenceID": 16, "context": "Later work [18] suggests a more aggressive setting of \u03b4j = 1/ \u221a j.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "It is this observation that motivates Passive-Aggressive (PA) Online Learning [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "Even for a proper setting of \u03b4j , it has been shown that stochastic methods perform best when multiple rounds of updates or passes are performed on the observed samples [2, 20, 28].", "startOffset": 169, "endOffset": 180}, {"referenceID": 18, "context": "Even for a proper setting of \u03b4j , it has been shown that stochastic methods perform best when multiple rounds of updates or passes are performed on the observed samples [2, 20, 28].", "startOffset": 169, "endOffset": 180}, {"referenceID": 26, "context": "Even for a proper setting of \u03b4j , it has been shown that stochastic methods perform best when multiple rounds of updates or passes are performed on the observed samples [2, 20, 28].", "startOffset": 169, "endOffset": 180}, {"referenceID": 23, "context": "We used the batch STE, GNMDS, and CKL (Crowd Kernel Learning [25]) MATLAB implementations specified by [26] in which the eig MATLAB function is used to find the all eigenvalues and eigenvectors for projection onto the PSD cone.", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "We used the batch STE, GNMDS, and CKL (Crowd Kernel Learning [25]) MATLAB implementations specified by [26] in which the eig MATLAB function is used to find the all eigenvalues and eigenvectors for projection onto the PSD cone.", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "The aset400 data set [7] contains 16,385 relative comparisons over 412 artists.", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "4 Outdoor Scene Similarity Our final experiment used triplets over 200 randomly chosen images of scenes from the Outdoor Scene Recognition (OSR) data set [19].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "After an initial 1200 randomly chosen queries were answered (every object appeared as the head of a triplet 6 times), 20 \u201crounds\u201d of 200 triplets were adaptively chosen according to the adaptive selection criterion in [25], resulting in 3600 total triplets.", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "This can be done out of model using a denoising method [17], or in model using a threshold on the passive-aggressive learning rate.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Prior work has proposed an adaptive selection scheme which operates in mini-batches [25]; however, such a scheme is too expensive", "startOffset": 84, "endOffset": 88}], "year": 2015, "abstractText": "Learning a kernel matrix from relative comparison human feedback is an important problem with applications in collaborative filtering, object retrieval, and search. For learning a kernel over a large number of objects, existing methods face significant scalability issues inhibiting the application of these methods to settings where a kernel is learned in an online and timely fashion. In this paper we propose a novel framework called Efficient online Relative comparison Kernel LEarning (ERKLE), for efficiently learning the similarity of a large set of objects in an online manner. We learn a kernel from relative comparisons via stochastic gradient descent, one query response at a time, by taking advantage of the sparse and low-rank properties of the gradient to efficiently restrict the kernel to lie in the space of positive semidefinite matrices. In addition, we derive a passive-aggressive online update for minimally satisfying new relative comparisons as to not disrupt the influence of previously obtained comparisons. Experimentally, we demonstrate a considerable improvement in speed while obtaining improved or comparable accuracy compared to current methods in the online learning setting.", "creator": "LaTeX with hyperref package"}}}