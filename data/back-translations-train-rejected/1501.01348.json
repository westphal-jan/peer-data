{"id": "1501.01348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2015", "title": "Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data", "abstract": "High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap.", "histories": [["v1", "Wed, 7 Jan 2015 02:13:09 GMT  (1278kb,D)", "http://arxiv.org/abs/1501.01348v1", "5 pages, 3 figures. Submitted to MLCB 2014 (NIPS workshop, Machine Learning in Computational Biology)"]], "COMMENTS": "5 pages, 3 figures. Submitted to MLCB 2014 (NIPS workshop, Machine Learning in Computational Biology)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lee zamparo", "zhaolei zhang"], "accepted": false, "id": "1501.01348"}, "pdf": {"name": "1501.01348.pdf", "metadata": {"source": "CRF", "title": "Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data", "authors": ["Lee Zamparo", "Zhaolei Zhang"], "emails": ["zamparo@cs.toronto.edu", "zhaolei.zhang@utoronto.ca"], "sections": [{"heading": null, "text": "High-content screening uses large collections of unlabeled cell image data to think about genetics or cell biology. Two important tasks are to identify those cells that exhibit interesting phenotypes, and to identify subpopulations enriched with these phenotypes. Explorative data analysis typically involves dimensionality reduction followed by clusters in the hope that clusters represent a phenotype. We suggest the use of stacked denoising auto-encoders to perform dimensionality reduction for high-content screening."}, {"heading": "1 Introduction", "text": "The use of machine learning methods to apply phenotype designations to cells has proven to be an effective way to uncover novel roles of genes in model organisms [Vizeacoumar et al., 2010] and in humans [Fuchs et al., 2010]. In an unattended model, characterization of all cells distributing across different populations is a cluster problem across many millions of high-dimensional data points. This complicates the cluster problem, and it is preferable to transform the data into a much lower dimensional space. How to best perform dimensionality reduction is far from clear. Particularly important qualities in this application case are the ability to scale to millions of data points, and the flexibility to model nonlinear relationships between covariates in the map from a higher to a lower dimensional space. Standard dimensionality reduction algorithms fail in one or both of these criteria, and the ability to model non-linear A, but the comatrix is not formed quickly in the comatrix."}, {"heading": "2 Stacked De-noising Autoencoders", "text": "The idea of composing simpler models in layers to build more complex ones has proven itself with a variety of base models, including stacked noise reduction autoencoders (abbreviated SdA), to name but one example [Hinton and Salakhutdinov, 2006, Ranzato et al., 2008, Vincent et al., 2010] These models have often been used as unattended pre-training; a layered scheme for initializing the parameters of a multi-layer perceptor, which is then trained on real versus model-predicted labels of the data by minimizing an appropriate loss function. Where the labeled data is abundant, supervised training is now prevalent, overlooking the pre-training in favor of randomly initialized models in combination with powerful new regulation techniques [Srivastava, 2013, Goodfellow et al., 2013]. These methods are ill-suited for high-grade screenings where biological knowledge and specialized ampling are scarce."}, {"heading": "2.1 De-noising Autoencoders", "text": "The noise reduction autoencoder [Vincent et al., 2008] takes the input data x- < d and maps it to a hidden representation y [0, 1] n, n d through a damaged noisy mapping y = f\u03b8 (x-ig) = sigmoid (Wx-b), parameterized by \u03b8 = {W, b}. W is a n \u00b7 d weight matrix and b is a bias vector. The input x is corrupted in x-shape by randomly setting a certain proportion of the values from x to zero. The latent representation y is then set to a reconstructed vector z-z < d via z = g\u03b8 (y) = W \"y + b\" with the parameters W, \"b.. The parameters W, d are set to minimize the reconstruction loss L (x, z)."}, {"heading": "2.2 Training", "text": "Hinton and Salakhutdinov [Hinton and Salakhutdinov, 2006] used a four-layer auto-encoder for MNIST, but we started with three-layer networks and tried both 4-layer and 5-layer networks. Both 3- and 4-layer networks achieved better results, measured by reconstruction errors on a validation set. For a certain number of layers, each candidate model was a point on a grid with a step size of 100: 700.... 1000 \u00d7 500.... 900 \u00d7 100.... 400 \u00d7 50.. 10. Each dA layer in each model was trained in advance using a stochastic progression of 50 epochs in batches to 100 samples. Minimal mean reconstruction errors for each layer were recorded. After selecting the five most powerful models for both 3 and 4-layer networks, we performed grid searches on each of the unfeasible hyperparameters dynamics, noise rate, learning rate, learning rate, learning rate, learning rate on the differentiation rate and differentiation rate."}, {"heading": "3 Experiments", "text": "We used data derived from images of yeast cell strains that were transformed to express a GFP fusion protein that functioned as a marker for DNA lesions. Each well of a 384-well plate contained a homozygous population of yeast cells with a series of genetic deletions. Four field images per well were recorded, each containing several hundred cells. Upon acquisition, the images were executed through an image processing pipeline with CellProfilers to segment the cells in each field and represent each as a vector of intensity, shape, and texture characteristics [Kamentsky et al., 2011]. The cell phenotypes in this study consisted of three classes: DNA lesions, non-round nuclei, or wild type1. A validation set of approximately 10,000 labeled cells was generated by manually labeling the images for each phenotype, followed by a labeling manually and a labeling against all remaining SVM models."}, {"heading": "4 Discussion", "text": "The study in Figure 2 shows that SdA models have consistently been classified as the best models for reducing dimensionality in preparation for phenotype-based clustering. There is a loss of homogeneity due to the use of an algorithm that cannot learn nonlinear transformations, as shown by the gap between isomap and PCA. To try to understand what explains the gap between the SdA models and the other algorithms, sample the results from each algorithm and calculate estimates. [1] Yeast cells that have neither DNA damage focus nor a non-round nucleus phenotype of distribution across both the interlabel and intra label distances between points are an example of the estimated distance distributions between kernel PCA and a three-layered SdA model. While the points sampled from the kernel-PCA model have a smaller intra-label distances sequence, the intra-label sequence has a greater distance sequence of SdA points from the three-layered SdA model."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra et al", "J. 2010] Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Global versus local methods in nonlinear dimensionality reduction", "author": ["De Silva et al", "V. 2003] De Silva", "J.J.B. Tenenbaum", "V.D. Silva"], "venue": "Advances in Neural Information Processing Systems 15,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al", "J. 2010] Duchi", "E. Hazan", "Y. Singer"], "venue": "Technical report,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Clustering phenotype populations by genome-wide RNAi and multiparametric imaging", "author": ["Fuchs et al", "F. 2010] Fuchs", "G. Pau", "D. Kranz", "O. Sklyar", "C. Budjan", "S. Steinbrink", "T. Horn", "A. Pedal", "W. Huber", "M. Boutros"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["Ham et al", "J. 2004] Ham", "D.D. Lee", "S. Mika", "B. Sch\u00f6lkopf"], "venue": "In Twenty-first international conference on Machine learning - ICML", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Salakhutdinov", "G.E. 2006] Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improved structure, function and compatibility for CellProfiler: modular high-throughput image analysis", "author": ["Kamentsky et al", "L. 2011] Kamentsky", "T.R. Jones", "A. Fraser", "Bray", "M.-A", "D.J. Logan", "K.L. Madden", "V. Ljosa", "C. Rueden", "K.W. Eliceiri", "A.E. Carpenter"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Sparse Feature Learning for Deep Belief Networks", "author": ["Ranzato et al", "M. 2008] Ranzato", "Boureau", "Y.-l", "Y.L. Cun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent et al", "P. 2008] Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A"], "venue": "In Proceedings of the 25th international conference on Machine learning - ICML", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent et al", "P. 2010] Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Integrating high-throughput genetic interaction mapping and high-content screening to explore yeast spindle morphogenesis", "author": ["Vizeacoumar et al", "F.J. 2010] Vizeacoumar", "N. van Dyk", "F. S Vizeacoumar", "V. Cheung", "J. Li", "Y. Sydorskyy", "N. Case", "Z. Li", "A. Datti", "C. Nislow", "B. Raught", "Z. Zhang", "B. Frey", "K. Bloom", "C. Boone", "B.J. Andrews"], "venue": "The Journal of cell biology,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}], "referenceMentions": [], "year": 2015, "abstractText": "High-content screening uses large collections of unlabeled cell image data to reason about genetics or cell biology. Two important tasks are to identify those cells which bear interesting phenotypes, and to identify sub-populations enriched for these phenotypes. This exploratory data analysis usually involves dimensionality reduction followed by clustering, in the hope that clusters represent a phenotype. We propose the use of stacked de-noising auto-encoders to perform dimensionality reduction for high-content screening. We demonstrate the superior performance of our approach over PCA, Local Linear Embedding, Kernel PCA and Isomap.", "creator": "LaTeX with hyperref package"}}}