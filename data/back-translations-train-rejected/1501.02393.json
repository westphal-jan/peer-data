{"id": "1501.02393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2015", "title": "Riemannian Metric Learning for Symmetric Positive Definite Matrices", "abstract": "Over the past few years, symmetric positive definite (SPD) matrices have been receiving considerable attention from computer vision community. Though various distance measures have been proposed in the past for comparing SPD matrices, the two most widely-used measures are affine-invariant distance and log-Euclidean distance. This is because these two measures are true geodesic distances induced by Riemannian geometry. In this work, we focus on the log-Euclidean Riemannian geometry and propose a data-driven approach for learning Riemannian metrics/geodesic distances for SPD matrices. We show that the geodesic distance learned using the proposed approach performs better than various existing distance measures when evaluated on face matching and clustering tasks.", "histories": [["v1", "Sat, 10 Jan 2015 21:12:09 GMT  (9kb)", "http://arxiv.org/abs/1501.02393v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["raviteja vemulapalli", "david w jacobs"], "accepted": false, "id": "1501.02393"}, "pdf": {"name": "1501.02393.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Raviteja Vemulapalli", "David W. Jacobs"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 1.02 393v 1 [cs.C VNotations- I denotes the identity matrix of appropriate size. - < > denotes an internal product. - Sn denotes the amount of n \u00b7 n symmetrical matrices. - S + + n denotes the amount of n \u00b7 n symmetrical positive definitive matrices. - TpM denotes the tangential space to the manifold M at the site p p Masticatory matrices. - S + n denotes the amount of n \u00b7 n symmetrical positive matrices. - TpM denotes the tangential space to the manifold M at the site p Masticatory matrices."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached. \"We have it in our hands,\" he says, \"but it is not so long ago that an agreement was reached.\""}, {"heading": "2 Distances to compare SPD matrices", "text": "In order to be able to compare SPD matrices, various distance measurements have been used in the literature, which have been derived from different geometric, statistical or information theory considerations. Although many of these distances attempt to capture the nonlinearity of SPD matrices, not all are geodetic distances induced by Riemann metrics. Tables 1 and 2 summarize these distances and their properties, including the logically induced Frobenius distance [6] and the affin-invariant distance [1]."}, {"heading": "3 Mahalanobis distance learning using ITML", "text": "Information theoretical metric learning [7] is a technique for learning Mahalanobis distance functions from data based on similarity and dissimilarity. Let {xi} N i = 1 be a series of N points in Rd. Given the pairs of similar points S and pairs of dissimilar points D. The goal of ITML is to learn an SPD matrix M so that the Mahalanobis distance parameterized by M is below a given threshold l for similar pairs of points and above a given threshold u for unequal pairs of points. Let Dld denote the LogDet divergence between SPD matrices that asDld (P, Q) = track (PQ \u2212 1) \u2212 n; P, Q-S + n. (1) ITML formulates the LogDet divergence between SPD matrices that asDld (P, Q) = track (PQ \u2212 1); n-S + P nP."}, {"heading": "4 Log-Euclidean Riemannian metric learning", "text": "\"It's about being able to put ourselves at the top,\" he says."}, {"heading": "5 Experiments", "text": "In this section we evaluate the performance of the proposed Rieman metric / geodetic distance learning approach in two applications: (i) Face matching using labeled faces in the wild (LFW) and (ii) Semi-supervised clustering using ETH80 dataset."}, {"heading": "5.1 Face matching using LFW face dataset", "text": "In this experiment, we are able to use the same image pairs for the same person and the same image pairs, where there are 1100 different image pairs and 1100 different image pairs, where there are 500 similar image pairs and 500 different image pairs. - An image pairing pattern is similar when both image pairs correspond to the same person and different image pairs. - The evaluation of the different image pairs consists of 3000 similar image pairs and 3000 similar image pairs. - It is further divided into 10 similar image pairs, each of which is assigned 300 different image pairs and 300 different image pairs. - The evaluation of 3000 similar image pairs and 3000 different image pairs."}, {"heading": "5.2 Semi-supervised clustering using ETH80 object dataset", "text": "In this experiment, we are interested in clustering the images in the ETH80 datasets in different object categories. Dataset The ETH80 Object Dataset [10] consists of 256 x 256 images of 8 object categories, each with 10 different object categories. Each object example has 41 images taken from different angles. Each object category has 410 images, which result in a total of 3280 images. Feature Extraction We convert each pixel in an image into a 9-dimensional feature vector, which is captured by [x, y), G (x, y), B (x, y), E (x, y), E (x, y), E (x), E (x, y), E (x), E (x, y), E (x, y), E (x, y), E (x, E, E, E (x, E, E, E, E, E, E, E (x, E, E, E, E, E, E, E, E, E, E (x, E, E, E, E, E, E, E, E, E, E, E (x), E (x, E, E, E, E, E (x), E (x, E, E, E, e), E (x, E (x), E (x, E, E (x, E, E, E), E (x, E (x, E, E, E, E, E, E, E, E (x), E (x), E (x), E (x, E (x), E (x, E (x), E (x, E (x, E, E, E, E, E, E (x), E (x), E (x, E (x), E (x, E, E (x, E, E (x), E, E (x, E, E (x, E, E, E, E, E, E, E, E (x), E (x, E (x), (x, E (x), E (x, E (x, E (x, e), E (x), E (x), (x, E (x,"}, {"heading": "6 Conclusion", "text": "Based on the log-euclidean framework [6], we have shown how geodetic distance functions for S + + n can be learned simply by learning the Mahalanobis distance functions in the logarithmic range. We have conducted experiments with face and object data sets. Face matching and semi-supervised object categorization results clearly show that the learned log-euclidean distance performs significantly better than other distances."}], "references": [{"title": "A Riemannian Framework for Tensor Com- puting", "author": ["X. Pennec", "P. Fillard", "N. Ayache"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Clustering and Dimensionality Reduction on Riemannian Manifolds", "author": ["A. Goh", "R. Vidal"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Region Covariance: A Fast Descriptor for De- tection and Classification", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": "In ECCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Pedestrian Detection via Classification on Rie- mannian Manifolds", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Lovel, \u201cSparse Coding and Dictio- nary Learning for Symmetric Positive Definite Matrices: A Kernel Approach", "author": ["M. Harandi", "C. Sanderson", "R. Hartley"], "venue": "In ECCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Log-Euclidean Metrics for Fast and Simple Calculus on Diffusion Tensors", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "Magnetic Resonance in Medicine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Information-Theoretic Met- ric Learning ", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Distance Metric Learning for Large Margin", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Nearest Neighbor Classification\u201d,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "L.-Miller, \u201cLabeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments\u201d, Univer- sity of Massachusetts, Amherst", "author": ["G.B. Huang", "M. Ramesh", "T. Berg"], "venue": "Technical Report 07-49,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Analyzing Appearance and Contour Based Methods for Object Categorization", "author": ["B. Leibe", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Efficient Similarity Search for Covariance Matrices via the Jensen-Bregman LogDet Divergence", "author": ["A. Cherian", "S. Sra", "A. Banerjee", "N. Papanikolopoulos"], "venue": "In ICCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "An Affine Invariant Tensor Dissimilarity Measure and its Applications to Tensor-valued Image Segmentation", "author": ["Z. Wang", "B.C. Vemuri"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Non-Euclidean Statistics for Covari- ance Matrices, with Applications to Diffusion Tensor Imaging", "author": ["I.L. Dryden", "A. Koloydenko", "D. Zhou"], "venue": "The Annals of Ap- plied Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Examples of SPD matrices in computer vision include diffusion tensors [1], structure tensors [2] and covariance region descriptors [3].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Examples of SPD matrices in computer vision include diffusion tensors [1], structure tensors [2] and covariance region descriptors [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Examples of SPD matrices in computer vision include diffusion tensors [1], structure tensors [2] and covariance region descriptors [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 0, "context": "Diffusion tensors arise naturally in medical imaging [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "In optical flow estimation and motion segmentation, structure tensors are often employed to encode important image features, such as texture and motion [2].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "Covariance region descriptors are used in texture classification [3], object detection [4], object tracking, action recognition and face recognition [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Covariance region descriptors are used in texture classification [3], object detection [4], object tracking, action recognition and face recognition [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Covariance region descriptors are used in texture classification [3], object detection [4], object tracking, action recognition and face recognition [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "Among them, the two most widely-used distance measures are the affine-invariant distance [1] and the log-Frobenius distance [6] (also referred to as logEuclidean distance in the literature).", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Among them, the two most widely-used distance measures are the affine-invariant distance [1] and the log-Frobenius distance [6] (also referred to as logEuclidean distance in the literature).", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "The log-Euclidean framework [6] proposed by Arsigny et.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "Since TIS n = Sn is a vector space, this result allows us to learn log-Euclidean Riemannian metrics and corresponding log-Euclidean geodesic distances from the data by using Mahalanobis distance learning techniques like information-theoretic metric learning (ITML) [7] and large margin nearest neighbor distance learning [8] in TIS n .", "startOffset": 265, "endOffset": 268}, {"referenceID": 7, "context": "Since TIS n = Sn is a vector space, this result allows us to learn log-Euclidean Riemannian metrics and corresponding log-Euclidean geodesic distances from the data by using Mahalanobis distance learning techniques like information-theoretic metric learning (ITML) [7] and large margin nearest neighbor distance learning [8] in TIS n .", "startOffset": 321, "endOffset": 324}, {"referenceID": 5, "context": "Among them, the logFrobenius distance[6] and the affine-invariant distance[1] are the most popular ones.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "Among them, the logFrobenius distance[6] and the affine-invariant distance[1] are the most popular ones.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "3 Mahalanobis distance learning using ITML Information theoretic metric learning [7] is a technique for learning Mahalanobis distance functions from the data based on similarity and dissimilarity constraints.", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "In this work, we use the publicly available ITML code provided by the authors of [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Frobenius \u2016P1 \u2212 P2\u2016F Yes Yes No CholeskyFrobenius [13] \u2016Chol(P1)\u2212 Chol(P2)\u2016F Yes Yes No", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "J-divergence [12] 1 2 \u221a", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "trace(P1P 2 + P2P \u22121 1 )\u2212 2n Yes No No Jensen-Bregman LogDet Divergence[11] \u221a", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Affine-invariant [1] \u2016log (", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "Log-Frobenius [6] \u2016log(P1)\u2212 log(P2)\u2016F Yes Yes Yes", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Table 2: SPD matrix distances and their properties Distance Distance from S n Affine invariance Scale invariance Rotation invariance Inversion invariance Frobenius Finite No No Yes No Cholesky-Frobenius [13] Finite No No No No", "startOffset": 203, "endOffset": 207}, {"referenceID": 11, "context": "J-divergence [12] Infinite Yes Yes Yes Yes Jensen-Bregman LogDet Divergence[11] Infinite Yes Yes Yes Yes", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "J-divergence [12] Infinite Yes Yes Yes Yes Jensen-Bregman LogDet Divergence[11] Infinite Yes Yes Yes Yes", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "Affine-invariant [1] Infinite Yes Yes Yes Yes Log-Frobenius [6] Infinite No Yes Yes Yes", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "Affine-invariant [1] Infinite Yes Yes Yes Yes Log-Frobenius [6] Infinite No Yes Yes Yes", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "4 Log-Euclidean Riemannian metric learning The log-Euclidean framework [6] proposed by Arsigny et.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "We have the following result based on the log-Euclidean framework introduced in [6]: Result 4.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Dataset: The LFW dataset [9] is a collection of face photographs designed for studying the problem of unconstrained face recognition.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Following [3], we convert each pixel in an image into a 9-dimensional feature vector given by", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "Dataset The ETH80 object dataset [10] consists of 256\u00d7 256 images of 8 object categories with each category including 10 different object instances.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "Based on the log-Euclidean framework [6], we have shown how geodesic distance functions can be learned for S n by simply learning Mahalanobis distance functions in the logarithm domain.", "startOffset": 37, "endOffset": 40}], "year": 2015, "abstractText": "Over the past few years, symmetric positive definite matrices (SPD) have been receiving considerable attention from computer vision community. Though various distance measures have been proposed in the past for comparing SPD matrices, the two most widely-used measures are affine-invariant distance and log-Euclidean distance. This is because these two measures are true geodesic distances induced by Riemannian geometry. In this work, we focus on the log-Euclidean Riemannian geometry and propose a data-driven approach for learning Riemannian metrics/geodesic distances for SPD matrices. We show that the geodesic distance learned using the proposed approach performs better than various existing distance measures when evaluated on face matching and clustering tasks. Notations \u2013 I denotes the identity matrix of appropriate size. \u2013 \u3008 , \u3009 denotes an inner product. \u2013 Sn denotes the set of n\u00d7 n symmetric matrices. \u2013 S n denotes the set of n\u00d7 n symmetric positive definite matrices. \u2013 TpM denotes the tangent space to the manifold M at the point p \u2208 M. \u2013 \u2016 \u2016F denotes the matrix Frobenius norm. \u2013 Chol(P) denotes the lower triangular matrix obtained from the Cholesky decomposition of a matrix P. \u2013 exp() and log() denote matrix exponential and logarithm respectively. \u2013 \u2202 \u2202x and \u2202 2 \u2202x represent partial derivatives. 2 Raviteja Vemulapalli, David W. Jacobs", "creator": "LaTeX with hyperref package"}}}