{"id": "1501.02432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2015", "title": "Learning a Fuzzy Hyperplane Fat Margin Classifier with Minimum VC dimension", "abstract": "The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning machine, and a low VC dimension leads to good generalization. The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing an exact bound on the VC dimension. This paper extends the MCM classifier to the fuzzy domain. The use of a fuzzy membership is known to reduce the effect of outliers, and to reduce the effect of noise on learning. Experimental results show, that on a number of benchmark datasets, the the fuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of generalization, and that the fuzzy MCM uses fewer support vectors. On several benchmark datasets, the fuzzy MCM classifier yields excellent test set accuracies while using one-tenth the number of support vectors used by SVMs.", "histories": [["v1", "Sun, 11 Jan 2015 09:29:05 GMT  (113kb)", "http://arxiv.org/abs/1501.02432v1", "arXiv admin note: text overlap witharXiv:1410.4573"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1410.4573", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jayadeva", "sanjit singh batra", "siddarth sabharwal"], "accepted": false, "id": "1501.02432"}, "pdf": {"name": "1501.02432.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sanjit S. Batra", "Siddarth Sabharwal"], "emails": ["jayadeva@ee.iitd.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 150 1.02 432v 1 [cs.L G] 1The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning machine, and a low VC dimension results in a good generalization. The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing an exact boundary to the VC dimension. This essay extends the MCM classifier to the fuzzy domain. The use of a fuzzy membership is known to reduce the effect of outliers and the impact of noise on learning. Experimental results show that for a number of benchmark datasets the fuzzy MCM classifier SVMs and conventional MCM exceed the fuzzy MCM in terms of generalization and that the fuzzy MCM uses fewer support vectors. For several benchmark datasets, the fuzzy MCM classifier SVM delivers an excellent number of SVM complexes, while the SVM is used by more accurate test machines:"}, {"heading": "1. Introduction", "text": "This year it is so far that it will be able to erect the aforementioned lcihsrteeSe rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "2. The Linear Minimal Complexity Machine Classifier", "text": "The motivation for the MCM stems from some outstanding work ongeneralization [11, 12, 13, 14].Consider a binary classification dataset with n-dimensional samples xi, i = 1, 2,..., M, where each sample is associated with a label yi + 1, \u2212 1). Vapnik [13] showed that the VC dimension in relation to the fat limit hyperplane classifier with margin d + 1, 2 + min. (1), where R encloses the radius of the smallest sphere, all training samples. Burges, in [4], explained that \"the above arguments strongly suggest that algorithms that give a better generalization give perfor-mance. Further evidence for this can be found in the following theorem of (Vapnik, 1998), which we cite without proof.\""}, {"heading": "3. The Fuzzy Minimal Complexity Machine Classifier", "text": "In the linear soft margin MCM formulation (9) - (12), the error variables qi, i = 1, 2,..., M measure the error of misclassification on the respective data samples, and the second term of the objective function in (9) is a weighted sum of all misclassification errors. In this case, the hyperparameter C evaluates all variables equally qi; this effectively means that errors made on all samples are equally important. In reality, noise tends to have corrupt training samples, and robust learning requires us to ignore outliers by assigning less importance to samples on which one has less confidence. Some samples may not be representative of a class. For example, a person exhibiting some symptoms of disease may exhibit characteristics that overlap with healthy subjects as well as with unhealthy subjects. Therefore, belonging to the class to which a sample belongs tends to be blurred, with a blurred membership (between 1 and 1)."}, {"heading": "4. The Fuzzy Kernel MCM", "text": "We look at a map \u03c6 (x), which maps the input samples from n to r, wherer > n. The dividing hyperplane in the image space is given by uT\u03c6 (x) + v = 0. (18) Following (14) - (17) the optimization problem for the fuzzy kernel MCM can be shown that Min w, b, h, qh + C \u00b7 M \u2211 i = 1siqi (19) h \u2265 yi \u00b7 [w T\u03c6 (xi) + b] + qi, i = 1, 2,..., M (20) yi \u00b7 [w T \u03c6 (xi) + b] + qi \u2265 1, i = 1, 2,..., M (21) qi \u2265 yi \u00b7 0, i = 1, 2,..., M. (22) The image vectors \u03c6 (xi), i = 1, 2,..., M form an overcomplete basis in the empirical character space in which w also lies."}, {"heading": "5. Experimental results", "text": "The FMCM was converted to MATLAB. The FMCS values were calculated by using the approaches outlined in MATLAB. In this case, the value of the FMCS is due to the FMCS. The FMCS values were converted from the FMCS to the FMCS. The FMCS values were raised from the FMCS to the FMCS values. The FMCS values were raised from the FMCS to the FMCS values. The FMCS values of the FMCS values were raised from the FMCS to the FMCS values. The FMCS values of the FMCS values were raised from the FMCS values to the FMCS values. The FMCS values of the FMCS values were raised from the FMCS values to the FMCS values."}, {"heading": "6. Conclusion", "text": "In this paper, we propose a way to build a fuzzy hyperplane classifier called the Fuzzy Minimal Complexity Machine (MCM) that learns a fuzzy cassifier with a small VC dimension. Fuzzy MCM involves solving a linear programming problem. Experimental results show that the fuzzy MCM outperforms the fuzzy SVM in terms of test set accuracy for a number of selected benchmark datasets. At the same time, the number of support vectors is lower, often by a significant factor, often up to 10 or more. It has not escaped our attention that the proposed approach can be expanded to include fuzzy least squares classifiers as well as tasks such as fuzzy regression and fuzzy time series prediction; in fact, a large number of variants of fuzzy SVMs can be re-examined from the perspective of fuzzy MCM."}], "references": [{"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural processing letters, vol. 9, no. 3, pp. 293\u2013300, 1999. 15", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Proximal support vector machine classifiers", "author": ["G. Fung", "O.L. Mangasarian"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 77\u201386.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": "Data mining and knowledge discovery, vol. 2, no. 2, pp. 121\u2013167, 1998.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning a hyperplane classifier by minimizing an exact bound on the {VC} dimension", "author": ["Jayadeva"], "venue": "Neurocomputing, vol. 149, Part B, no. 0, pp. 683 \u2013 689, 2015. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0925231214010194", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Fuzzy svm with a new fuzzy membership function", "author": ["X. Jiang", "Z. Yi", "J.C. Lv"], "venue": "Neural Computing & Applications, vol. 15, no. 3-4, pp. 268\u2013276, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust support vector machine with bullet hole image classification", "author": ["Q. Song", "W. Hu", "W. Xie"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, vol. 32, no. 4, pp. 440\u2013 448, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Fuzzy support vector machines", "author": ["C.-F. Lin", "S.-D. Wang"], "venue": "Neural Networks, IEEE Transactions on, vol. 13, no. 2, pp. 464\u2013471, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Discovering informative patterns and data cleaning.", "author": ["I. Guyon", "N. Matic", "V. Vapnik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A framework for structural risk minimisation", "author": ["J. Shawe-Taylor", "P.L. Bartlett", "R.C. Williamson", "M. Anthony"], "venue": "Proceedings of the ninth annual conference on Computational learning theory. ACM, 1996, pp. 68\u201376. 16", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Structural risk minimization over data-dependent hierarchies", "author": ["\u2014\u2014"], "venue": "Information Theory, IEEE Transactions on, vol. 44, no. 5, pp. 1926\u20131940, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1926}, {"title": "Statistical learning theory", "author": ["V.N. Vapnik"], "venue": "1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning with kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "The most commonly used variants are the maximum margin L1 norm SVM [1], and the least squares SVM (LSSVM) [2], both of which require", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "The most commonly used variants are the maximum margin L1 norm SVM [1], and the least squares SVM (LSSVM) [2], both of which require", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "The proximal SVM [3] is also similar in spirit to the LSSVM.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "However, according to Burges [4], SVMs can have a very large VC dimension, and that \u201cat present there exists no theory which shows that good generalization performance is guaranteed for SVMs\u201d.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "In recent work [5], we have shown how to learn a bounded margin hyperplane classifier, termed as the Minimal Complexity Machine (MCM) by minimizing an exact bound on its VC dimension.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "Such samples may be thought of as not lying entirely in one class, but belonging to both classes to a certain degree [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "It is well known that SVMs are very sensitive to outliers [7, 8, 9].", "startOffset": 58, "endOffset": 67}, {"referenceID": 7, "context": "It is well known that SVMs are very sensitive to outliers [7, 8, 9].", "startOffset": 58, "endOffset": 67}, {"referenceID": 8, "context": "It is well known that SVMs are very sensitive to outliers [7, 8, 9].", "startOffset": 58, "endOffset": 67}, {"referenceID": 7, "context": "Fuzzy support vector machines (FSVM) [8] were proposed to address this problem.", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "As we show in the sequel, an interesting example is that of the \u2019haberman\u2019 dataset from the UCI machine learning repository [10], that has 306 samples.", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].", "startOffset": 83, "endOffset": 99}, {"referenceID": 11, "context": "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].", "startOffset": 83, "endOffset": 99}, {"referenceID": 12, "context": "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].", "startOffset": 83, "endOffset": 99}, {"referenceID": 13, "context": "The motivation for the MCM originates from some outstanding work on generalization [11, 12, 13, 14].", "startOffset": 83, "endOffset": 99}, {"referenceID": 12, "context": "Vapnik [13] showed that the VC dimension \u03b3 for fat margin hyperplane classifiers with margin d \u2265 dmin satisfies \u03b3 \u2264 1 +Min( R d2min , n) (1)", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "Burges, in [4], stated that \u201cthe above arguments strongly suggest that algorithms that minimize R 2 d2 can be expected to give better generalization performance.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "Details may be found in [5].", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "In [5], we show that there exist constants \u03b1, \u03b2 > 0, \u03b1, \u03b2 \u2208 R such that \u03b1h \u2264 \u03b3 \u2264 \u03b2h, (4) or, in other words, h constitutes a tight or exact (\u03b8) bound on the VC dimension \u03b3.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [5], we further show that the optimization problem (5) may be reduced to the linear programming problem", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Lin and Wang proposed fuzzy SVMs in [8], wherein they suggested that each sample be associated with a fuzzy membership si.", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "Fuzzy membership values were computed by using the approach outlined in [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "In order to evaluate the FMCM, we chose a number of benchmark datasets from the UCI machine learning repository [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "The results of the classical MCM have not been duplicated from [5] for the sake of brevity; an added reason is that a fair comparison would be between two methods that use a fuzzy methodology.", "startOffset": 63, "endOffset": 66}], "year": 2015, "abstractText": "The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning machine, and a low VC dimension leads to good generalization. The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing an exact bound on the VC dimension. This paper extends the MCM classifier to the fuzzy domain. The use of a fuzzy membership is known to reduce the effect of outliers, and to reduce the effect of noise on learning. Experimental results show, that on a number of benchmark datasets, the the fuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of generalization, and that the fuzzy MCM uses fewer support vectors. On several benchmark datasets, the fuzzy MCM classifier yields excellent test set accuracies while using one-tenth the number of support vectors used by SVMs.", "creator": "LaTeX with hyperref package"}}}