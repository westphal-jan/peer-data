{"id": "1501.02527", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Autodetection and Classification of Hidden Cultural City Districts from Yelp Reviews", "abstract": "Topic models are a way to discover underlying themes in an otherwise unstructured collection of documents. In this study, we specifically used the Latent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to classify restaurants based off of their reviews. Furthermore, we hypothesize that within a city, restaurants can be grouped into similar \"clusters\" based on both location and similarity. We used several different clustering methods, including K-means Clustering and a Probabilistic Mixture Model, in order to uncover and classify districts, both well-known and hidden (i.e. cultural areas like Chinatown or hearsay like \"the best street for Italian restaurants\") within a city. We use these models to display and label different clusters on a map. We also introduce a topic similarity heatmap that displays the similarity distribution in a city to a new restaurant.", "histories": [["v1", "Mon, 12 Jan 2015 03:10:01 GMT  (8884kb,D)", "http://arxiv.org/abs/1501.02527v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["harini suresh", "nicholas locascio"], "accepted": false, "id": "1501.02527"}, "pdf": {"name": "1501.02527.pdf", "metadata": {"source": "CRF", "title": "Autodetection and Classification of Hidden Cultural City Districts from Yelp Reviews", "authors": ["Harini Suresh", "Nick Locascio"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION Cities are often defined by their dominant cultural characteristics. However, individual cities themselves host a wide variety of different cultural districts. Streets separated by just a few blocks convey very different impressions. These implicit boundaries and classifications are not documented on official maps and are usually learned only after a great deal of time and life experience in a particular city."}, {"heading": "A. Motivation", "text": "We believe that a sense of these districts is valuable to a much broader population. Some examples are: 1. New Businesses: For business owners or entrepreneurs who want to open a new restaurant or expand to another location, it is undoubtedly a valuable insight to know which areas of a city restaurant are very similar or different from this particular store. 2. Newcomers: For tourists who are moving to the city, or anyone else who is new to the city, it is often a tedious and daunting task to get a sense of things, such as where they are most likely to find a good Thai restaurant, the block for dim sum, or the best area for an elegant, upscale dinner with good wine. 3. Anyone trying to explore: Even people who already have a sense of the city can be surprised by a Holein-the-Wall caf\u00e9 or undiscovered area. The LDA model we describe is the most weighted classification * This work was supported by Joshua Tenenbaum 1H."}, {"heading": "B. The Yelp Dataset", "text": "Published in 2013, the Yelp Academic dataset now includes more than 42,000 companies with more than one million reviews [9], and has been used in scientific work including mood analysis, text layout systems, and recommendation engines. The quality and sheer size of the dataset is of great value to our research, and its natural-language user ratings are critical to our cultural recognition and classification system."}, {"heading": "II. METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. The LDA Model", "text": "Latent Dirichlet Allocation (LDA), first introduced in 2003 by Lead et. al. [1], has been applied to numerous and diverse areas: from computer vision [2,3] to recommendation systems [4] to spam filtering [5]. LDA assumes that a collection of documents D can be treated like a \"bag of words,\" in which each document d is generated by the following process, with hyperparameters \u03b1 and \u03b2: 1. Let's assume that each topic k has a fixed distribution over all words in D, that is, \"dirichlet\" (\u03b2) 2. Select the topic distribution of the document \u0445d \u0445 Dirichlet (\u03b1) 3. To generate each word w: a. select a topic zi from discrete (\u03b8d) ar Xiv: 150 1.02 527v 1 [cs.C L] 12 January 2015b. Choose a word wi from discrete (\u0432zi) Using this model, LDA is able to learn the topic zi from mixes, the documents it is trained on (p)."}, {"heading": "B. Training LDA", "text": "To implement LDA, we used tools from the Python library Gensim, which provides functionality to analyze the semantic structure in texts [6]. Based on the results of the Expectation Maximization algorithm used by Huang et. al. [7], we chose K = 50 as the number of topics to extract. We used hyperparameters \u03b1 and \u03b2 with symmetrical 1.0 / K priorities. We cleaned the ratings to remove punctuation, numbers and a list of stopwords that consist of the English Stop Words list in the Scikit Learn Python library [11]. Additionally, we determined that after this initial cleaning, the model should only take into account the 40,000 medium-frequency words, eliminating words that occurred only a handful of times, as well as generic food-related words that occurred frequently."}, {"heading": "C. Training Examples", "text": "Our LDA model produced 50 topics. Each topic is a collection of word-weight pairs. Words with high corresponding weights are most representative of the topic. The topic of word weights are normalized so that W \u2211 wi wi = 1. Table I is a small sample of selected topics generated by our model. Table I shows the topic #, the name we chose for the topic based on its word distribution, and the word distribution. You can see the full list of topics and their weighting in the appendix."}, {"heading": "D. LDA Inference", "text": "We used our trained LDA model to predict theme distributions for each of the 3855 restaurants. A restaurant's theme distribution is a collection of coupled theme numbers and corresponding weights. Topics with high corresponding weights are most representative for the restaurant. Theme distribution weights are normalized so that W \u2211 wi = 1. A sample of theme predictions is shown in Table II."}, {"heading": "E. Clustering Preparation", "text": "We assume that the Chinese districts in a city are characterized by the proximity and similarity of restaurants. < 2) In our model, therefore, we represent each restaurant as a combined vector of its coordinate position and its LDA assigned weight distribution. < 2) This vector has 52 dimensions, 2 of which represent the spatial location of the restaurant, and 50 of which represent the LDA theme of the restaurant. To prevent our results from being arbitrarily outlined by these different scales, we used a scaling method in which we multiply the topic of weight distribution by a constant S. By varying S, we can give the topic more or less influence on the clustering."}, {"heading": "G. Determining a Cluster\u2019s Label", "text": "To determine the labeling of a cluster, we take the average theme vector for all restaurants in the cluster. We then selected the two most important themes that describe a cluster and used their human-assigned labels. These labels are superimposed in Figure 5. We selected the top two labels to reveal not only the most common topic within a cluster, but also underlying categories that might be less obvious. Using our Gaussian mixed cluster, we were able to expand this labeling with appropriate orientations. As each cluster is characterized by a Gaussian with two 3Unfortunately, label rotations often lead to a collision of labels. If this system were implemented as an analysis interface, this problem could be mitigated by a zoomable display, concealing clashing labels or other methods. However, our paper is more concerned with researching these labeling methods that address the variation with the user experience than with the top labeling."}, {"heading": "H. Cultural HeatMap and Optimal Placement for New Restaurants", "text": "While the accumulation of restaurants on space and themes illuminates the many cultural centers of a city, it does not show how a particular theme is distributed throughout the city. To show this distribution for a particular theme, we presented the thematic similarity in a thermal imaging map. We drew our LDA conclusion on the reviews of a novel restaurant, and from this we derived a thematic distribution of this novel restaurant. We divided the city into a grid of 20x20 squares. For each square, we calculated the average thematic similarity from the center of the square to all restaurants in the city. We used a Gaussian weight to scale the thematic similarity by proximity. For each square, we calculated a similarity metric Sim (center, novel), with: Sim (center, novel) = 1-R-R-R-R-2 \u2212 dist (Roman, ri), 2 \u2212 dist (Roman, ri)."}, {"heading": "III. DISCUSSION & APPLICATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Evaluation of Results", "text": "We found that the resulting LDA themes (Appendix III) were well-defined and descriptive; we observed that the words within a given topic fit well into a specific category of food types or cultures, and we had very little difficulty identifying them by default words and weights; in addition, the themes themselves seem to be relatively clear of each other, with few overlapping themes; the general area where we saw the most overlap was the topic of buffet restaurants. Topics # 0, # 5, # 48 each concerned buffet restaurants; however, if we looked at the words in each, we were able to distinguish \"seafood / buffet\" (# 5) and \"upscale / buffet\" from a more general \"buffet\" theme (# 48); if we looked at the K-middle clusters of Las Vegas restaurants, we observed that our clusters predefined areas of \"pho\" and \"uppackage.\""}, {"heading": "B. Applications of Automatic Cluster Labeling", "text": "Figure 8 in the appendix shows the results of marking two other cities (Phoenix and Endinberg) with this method. These maps can be analysis tools with various applications, including, but not limited to, identifying new restaurant locations, understanding the cultural regions of a city, discovering unexplored areas of your city, choosing where to live or which route to take for a walk in the park."}, {"heading": "C. Applications of Topic Heat Map", "text": "Like the automatic method of cultural labelling, the Topic Heat Map can be used as a useful analytical tool, which can be used to identify specific known and hidden cultural incubators; a hidden cultural incubator can be a market opportunity for sustained growth; a city's Topical Heat Map can be a particularly valuable asset for a new restaurant or chain looking for strategies for exactly where to place a new store location; and the Heat Map could actually be used to perform a detailed analysis of what type of location is optimal for different types of restaurants (see IV Part 3 for more details)."}, {"heading": "IV. FURTHER RESEARCH", "text": "1) Using the timestamps for reviews, it is possible to filter reviews based on when they were written. 2) In our study, we use the Elbow method and the Gap statistics to predetermine an appropriate number of clusters to use. Instead, it may prove valuable to use an interlocking Chinese restaurant process to learn a hierarchy of clusters and subclusters.2) For example, this could divide Chinatown into different sub-clusters under the general Chinese cluster, which could be used to denote the charts at different scales and zoom levels.3) In addition, the use of a Chinese restaurant process as part of a non-parametric mixture model would allow the model to flexibly add more clusters as needed, and it could be more likely to find the optimal number of clusters at different scales.3) The similarity sheatmap that we have developed with Yelp stars could be used to analyze what type of restaurant is actually a very similar site."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Joshua Tenenbaum for his support of this work."}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research 3, pp. 993", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-Latnt Dirichlet Allocation: A Hierarchical Model for Human Action Recognition", "author": ["Y. Wang", "P. Sabzmeydani", "G. Mori"], "venue": "Lecture Notes in Computer Science, vol. 4814, A. Elgammal, B. Rosenhahn, R. Klette, Eds., Heidelberg: Springer Berlin", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Semantic Annotation of Satellite Images Using Latent Dirichlet Allocation", "author": ["M. Lienou", "H. Maitre", "M. Datcu"], "venue": "Geoscience and Remote Sensing Letters, IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Latent dirichlet allocation for tag recommendation", "author": ["R. Krestel", "P. Fankhauser", "W. Nejdl"], "venue": "Proc. of the third ACM conf. on recommender systems, New York", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent dirichlet allocation in web spam filtering", "author": ["I. Br", "J. Szab", "A. Benczr"], "venue": "Proc. of the 4th int. workshop on adversarial information retrieval on the web, New York", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving Restaurants by Extracting Subtopics from Yelp Reviews", "author": ["J. Huang", "S. Rogers", "E. Joo"], "venue": "presented at iConference, Berlin", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Online Learning for Latent Dirichlet Allocation", "author": ["M. Hoffman", "D. Blei", "F. Bach"], "venue": "Advances in Neural Information Processing Systems 23", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "J. R. Statist. Soc., vol. 63, part 2, pp. 411", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning, vol. 12, pp. 2825", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic models of cognition: exploring representations and inductive biases", "author": ["T. Griffiths", "N. Chater", "C. Kemp", "A. Perfors", "J.B. Tenenbaum"], "venue": "Trends in Cognitive Sciences, vol. 14, pp. 357", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic models of cognition: Conceptual foundations", "author": ["N. Chater", "J.B. Tenenbaum", "A. Yuille"], "venue": "Trends in Cognitive Sciences, vol. 10", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Rules and Similarity in Concept Learning", "author": ["J.B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems 12, pp. 59", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Psychological Review", "author": ["A.N. Sanborn", "T.L. Griffiths", "D.J. Navarro"], "venue": "vol. 117, pp. 1144", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "Recent cognitive science research has had major successes in probabilistic generative models of human cognition [12, 13].", "startOffset": 112, "endOffset": 120}, {"referenceID": 10, "context": "Recent cognitive science research has had major successes in probabilistic generative models of human cognition [12, 13].", "startOffset": 112, "endOffset": 120}, {"referenceID": 11, "context": "Specifically, research by Tenenbaum shows strong support for Bayesian concept learning [14] and Sanborn et.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "use Dirichlet Process Mixture Models for category learning that emulates human learning [15].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "in 2003 [1], has been applied to numerous and diverse fields: from computer vision [2,3] to recommendation systems [4] to spam filtering [5].", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "in 2003 [1], has been applied to numerous and diverse fields: from computer vision [2,3] to recommendation systems [4] to spam filtering [5].", "startOffset": 83, "endOffset": 88}, {"referenceID": 2, "context": "in 2003 [1], has been applied to numerous and diverse fields: from computer vision [2,3] to recommendation systems [4] to spam filtering [5].", "startOffset": 83, "endOffset": 88}, {"referenceID": 3, "context": "in 2003 [1], has been applied to numerous and diverse fields: from computer vision [2,3] to recommendation systems [4] to spam filtering [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "in 2003 [1], has been applied to numerous and diverse fields: from computer vision [2,3] to recommendation systems [4] to spam filtering [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "[7] to determine the optimal number of topics for Yelp restaurant reviews in Phoenix, we chose K = 50 as the number of topics to extract.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We cleaned the reviews to remove punctuation, numbers, and a list of stopwords made up of the \u201cEnglish Stop Words\u201d list in the Scikit-learn python library [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "[8] and results in an LDA topic model object that can be queried with new, unseen documents to return an optimal topic distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Because of the shortcomings of the elbow method, we also used the Gap Statistic [10, 16] to determine the optimal C with which to cluster.", "startOffset": 80, "endOffset": 88}, {"referenceID": 8, "context": "Using tools from the Python library Scikit-learn [11], we performed K-means clustering on all 3855 Vegas restaurants with random K++ means initialization and 300 iterations, specifying C = 30 and S = 0.", "startOffset": 49, "endOffset": 53}], "year": 2015, "abstractText": "Topic models are a way to discover underlying themes in an otherwise unstructured collection of documents. In this study, we specifically used the Latent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to classify restaurants based off of their reviews. Furthermore, we hypothesize that within a city, restaurants can be grouped into similar \u201cclusters based on both location and similarity. We used several different clustering methods, including Kmeans Clustering and a Probabilistic Mixture Model, in order to uncover and classify districts, both well-known and hidden (i.e. cultural areas like Chinatown or hearsay like \u201cthe best street for Italian restaurants\u201d) within a city. We use these models to display and label different clusters on a map. We also introduce a topic similarity heatmap that displays the similarity distribution in a city to a new restaurant.", "creator": "LaTeX with hyperref package"}}}