{"id": "1501.02592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2015", "title": "Photonic Delay Systems as Machine Learning Implementations", "abstract": "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers.", "histories": [["v1", "Mon, 12 Jan 2015 10:25:31 GMT  (773kb,D)", "http://arxiv.org/abs/1501.02592v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["michiel hermans", "miguel soriano", "joni dambre", "peter bienstman", "ingo fischer"], "accepted": false, "id": "1501.02592"}, "pdf": {"name": "1501.02592.pdf", "metadata": {"source": "CRF", "title": "Photonic Delay Systems as Machine Learning Implementations", "authors": ["Michiel Hermans", "Miguel C. Soriano", "Joni Dambre", "Peter Bienstman", "Ingo Fischer"], "emails": ["(michiel.hermans@ulb.ac.be)"], "sections": [{"heading": null, "text": "Nonlinear photonic delay systems are interesting implementation platforms for machine learning. They can be extremely fast, have a high degree of parallelism, and potentially consume much less power than digital processors. So far, they have been successfully used for signal processing using the reservoir computing paradigm. In this work, we show that their scope can be greatly expanded if we apply backward gradient lineage over time to a model of the system to optimize the input coding of such systems. We conduct physical experiments that show that the obtained input coding works well in reality, and we show that optimized systems work much better than the traditional reservoir computing approach. The results presented here show that common gradient lineage techniques from machine learning can indeed be applied to physically neuro-inspired analog computers."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Physical System", "text": "In this section we will explain the details of the physical system. We will begin by formally introducing its retardation dynamics \u03b2 = \u03b2 dynamics, which works in continuous time. Next, we will explain how the feedback delay can be used to realize a high-dimensional state space encoded in time, and we will show that, combined with a special input and output coding, the setup can be considered a special case of RNN. Finally, we will explain how we can view the input and output coding of the system of discrete time actualization equation. The physical system we use in this work is a delayed feedback system, which has an Ikeda-type dynamic ([18, 34]).We will offer a schematic representation of the physical setup in Figure 1. It consists of a laser source, a Mach-tenth modulator, a long optical fiber (approximately 4 km), an electronic transducer, and a deceleration line."}, {"heading": "2.1 Input and Output Encoding", "text": "In fact, most of us are able to move to another world, in which we are able, in which we are able to change the world, and in which we are able to change the world."}, {"heading": "2.2 Converting the System to a Trainable Machine Learning Model", "text": "In fact, it is quite possible to simulate the system, and we can certainly ignore it by applying different equation solutions and applying a non-linear parameter gradient. However, in our case it is the significant computational cost. Note: In a common discrete time RNN, a single state corresponds to updating a single matrix vector multiplication and applying a non-linear compilation of the complete time trace of ai (t).This is much more expensive to compile the fact than we need in most gradient algorithms."}, {"heading": "2.3 Hybrid Training Approach", "text": "One of the challenges we faced when we tried to match the model with the experimental data was that we only achieved a sufficiently good match if we adjusted the \u03b2 and \u03c6 values very carefully. We can physically control these parameters, but the precise determination of their numerical values proved not trivial in the experiments, especially since they tend to exhibit slight drift behavior over longer periods (in the order of hours). As a result, it proved challenging to train parameters in the simulation and simply apply them directly to the DCMZ. Therefore, we applied a hybrid approach between gradient descent and the RC approach. We trained both the input and output masks in simulations. Next, we used only the input masks for the physical build-up. After recording all the data, we trained the output weights using gradient descent, this time on the measurement data itself."}, {"heading": "2.4 Input Limitations", "text": "Another physical limitation is the fact that the voltages that can be generated by the electronic part of the system are limited within a range determined by its supply voltage.The output voltage of the electronic part serves as the input of the Mach-Zehnder interferometer and corresponds to the term a (t \u2212 D) + z (t) in the argument of the square sine in Equation 1 (the offset phase is controlled by a separate voltage source).The voltage range that we were able to cover before the amplifiers began to saturate corresponded roughly to a range of [\u2212 \u03c0 / 2 \u00b7 \u03c0 / 2] in Equation 1: a full wavelength. Instead of taking into account the saturation of the amplifiers in our simulations, we ensured that if the input argument z (t) was outside this range, we mapped it back into that range by adding or subtracting \u03c0."}, {"heading": "3 Experiments", "text": "We tested the use of BPTT to train the input masks in both simulation and experiment on two benchmark tasks. First, we looked at the commonly used MNIST dataset for written number recognition, in which we used the dynamics of the system indirectly. Second, we applied it to the TIMIT phoneme dataset. For the MNIST experiment, we used Nm = 400 masking steps. For TIMIT, we used Nm = 600."}, {"heading": "3.1 MNIST", "text": "This year it has come to the point where it will be able to retaliate, \"he said.\" We have never lost so much time as this year, \"he said.\" We have never lost so much time, \"he said,\" but we have never lost so much time. \""}, {"heading": "3.2 TIMIT", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc rf\u00fc the rf\u00fc"}, {"heading": "4 Discussion and Future Work", "text": "Dre eeisrVnlrsrteeeeaeeeeeeeteeeeteeteerrrrrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrsrrrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrsrsrsrrrrrrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "Acknowledgements", "text": "P.B., M.H. and J.D. acknowledge the support of the Interuniversity Attraction Pole (IAP) Photonics @ be of the Belgian Office for Science Policy, the ERC NaResCo Starting Grant and the Seventh Framework Programme of the European Union under the funding agreement No. 604102 (Human Brain Project). M.C.S. and I.F. acknowledge the support of MINECO (Spain), Comunitat Auto noma de les Illes Balears, FEDER, and the European Commission within the framework of the projects TEC2012-36335 (TRIPHOP) and Grups Competitius. M.H. and I.F. acknowledge the support of the Universitat de les Balears for an invited scholarship for young researchers. We also acknowledge Prof. L. Larger for the development of the optoelectronic delay system."}], "references": [{"title": "Constructing optimized binary masks for reservoir computing with delay systems", "author": ["Lennert Appeltant", "Guy Van der Sande", "Jan Danckaert", "Ingo Fischer"], "venue": "Scientific reports,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Advances in optimizing recurrent networks", "author": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Parallel photonic information processing at gigabyte per second data rates using transient states", "author": ["Daniel Brunner", "Miguel C Soriano", "Claudio R Mirasso", "Ingo Fischer"], "venue": "Nature communications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Locomotion without a brain: physical reservoir computing in tensegrity structures", "author": ["Ken Caluwaerts", "Michiel D\u2019Haene", "David Verstraeten", "Benjamin Schrauwen"], "venue": "Artificial life,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A fast online algorithm for large margin training of online continuous density hidden markov models", "author": ["Chih-Chieh Cheng", "Fei Sha", "Lawrence Saul"], "venue": "In Interspeech", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["Dan Cire\u015fan", "Ueli Meier", "Luca Maria Gambardella", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["Dan Cire\u015fan", "Ueli Meier", "J\u00fcrgen Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Pattern recognition in a bucket", "author": ["Chrisantha Fernando", "Sampsa Sojakka"], "venue": "In Proceedings of the 7th European Conference on Artificial Life,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "National Institute of Standards, Technology (US, Linguistic Data Consortium, Information Science, Technology Office, United States, and Defense Advanced Research Projects Agency)", "author": ["John Garofolo"], "venue": "TIMIT Acoustic-phonetic Continuous Speech Corpus. Linguistic Data Consortium,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Towards a theoretical foundation for morphological computation with compliant bodies", "author": ["Helmut Hauser", "Auke J Ijspeert", "Rudolf M F\u00fcchslin", "Rolf Pfeifer", "Wolfgang Maass"], "venue": "Biological cybernetics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Optoelectronic systems trained with backpropagation through time", "author": ["Michiel Hermans", "Joni Dambre", "Peter Bienstman"], "venue": "IEEE Transactions in Neural Networks and Learning Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Automated design of complex dynamic systems", "author": ["Michiel Hermans", "Benjamin Schrauwen", "Peter Bienstman", "Joni Dambre"], "venue": "PloS One,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Short term memory in echo state networks", "author": ["Herbert Jaeger"], "venue": "Technical Report GMD Report 152,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless telecommunication", "author": ["Herbert Jaeger", "Harald Haas"], "venue": "Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Pac-bayesian approach for minimization of phoneme error rate", "author": ["Joseph Keshet", "David McAllester", "Tamir Hazan"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Ikeda-based nonlinear delayed dynamics for application to secure optical transmission systems using chaos", "author": ["Laurent Larger", "Jean-Pierre Goedgebuer", "Vladimir Udaltsov"], "venue": "Comptes Rendus Physique,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Photonic information processing beyond turing: an optoelectronic implementation of reservoir computing", "author": ["Laurent Larger", "Miguel C Soriano", "Daniel Brunner", "Lennert Appeltant", "Jose M Guti\u00e9rrez", "Luis Pesquera", "Claudio R Mirasso", "Ingo Fischer"], "venue": "Optics express,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["Mantas Lukosevicius", "Herbert Jaeger"], "venue": "Computer Science Review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Real-time computing without stable states: A new framework for neural computation based on perturbations", "author": ["Wolfgang Maass", "Thomas Natschl\u00e4ger", "Henri Markram"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Optoelectronic reservoir computing", "author": ["Yvan Paquot", "Francois Duport", "Antoneo Smerieri", "Joni Dambre", "Benjamin Schrauwen", "Marc Haelterman", "Serge Massar"], "venue": "Scientific Reports,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Minimum complexity echo state network", "author": ["Ali Rodan", "Peter Tino"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Discriminative recurrent sparse auto-encoders", "author": ["Jason Tyler Rolfe", "Yann LeCun"], "venue": "arXiv preprint arXiv:1301.3775,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Learning internal representations by error propagation", "author": ["David Rumelhart", "Geoffrey Hinton", "Ronald Williams"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1986}, {"title": "Opto-electronic reservoir computing: tackling noise-induced performance degradation", "author": ["Miguel C Soriano", "Silvia Ort\u0301\u0131n", "Daniel Brunner", "Laurent Larger", "Claudio Mirasso", "Ingo Fischer", "L\u00fa\u0131s Pesquera"], "venue": "Optics express,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Backpropagation-Decorrelation: Online recurrent learning with O(N) complexity", "author": ["Jochen Steil"], "venue": "In Proceedings of the International Joint Conference on Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Phoneme recognition with large hierarchical reservoirs", "author": ["Fabian Triefenbach", "Azaraksh Jalalvand", "Benjamin Schrauwen", "Jean-Pierre Martens"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Toward optical signal processing using photonic reservoir computing", "author": ["Kristof Vandoorne", "Wouter Dierckx", "Benjamin Schrauwen", "David Verstraeten", "Roel Baets", "Peter Bienstman", "Jan van Campenhout"], "venue": "Optics Express,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Experimental demonstration of reservoir computing on a silicon photonics chip", "author": ["Kristof Vandoorne", "Pauline Mechet", "Thomas Van Vaerenbergh", "Martin Fiers", "Geert Morthier", "David Verstraeten", "Benjamin Schrauwen", "Joni Dambre", "Peter Bienstman"], "venue": "Nature communications,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Strongly asymmetric square waves in a time-delayed system", "author": ["Lionel Weicker", "Thomas Erneux", "Otti DHuys", "Jan Danckaert", "Maxime Jacquot", "Yanne Chembo", "Laurent Larger"], "venue": "Physical Review E,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "This development allowed researchers to dramatically scale up their models, in turn leading to the major improvements on stateof-the-art performances on tasks such as computer vision ([17, 6]).", "startOffset": 184, "endOffset": 191}, {"referenceID": 5, "context": "This development allowed researchers to dramatically scale up their models, in turn leading to the major improvements on stateof-the-art performances on tasks such as computer vision ([17, 6]).", "startOffset": 184, "endOffset": 191}, {"referenceID": 29, "context": "Even though GPUs have been used to speed up training RNNs ([30, 12]), the total obtainable acceleration for a given GPU architecture will still be limited by the number of sequential operations required in an RNN, which is typically much higher than in common neural networks.", "startOffset": 59, "endOffset": 67}, {"referenceID": 11, "context": "Even though GPUs have been used to speed up training RNNs ([30, 12]), the total obtainable acceleration for a given GPU architecture will still be limited by the number of sequential operations required in an RNN, which is typically much higher than in common neural networks.", "startOffset": 59, "endOffset": 67}, {"referenceID": 21, "context": "Recent attempts to solve this problem using the Hessian-free approach have proved promising ([22]).", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "Other attempts using stochastic gradient descent combined with more heuristic ideas have been described in [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 13, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 14, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 20, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 27, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 19, "context": "A steadily growing branch of research is concerned with Reservoir Computing (RC), a concept which employs high-dimensional, randomly initialized dynamical systems (termed the reservoir) to perform feature extraction on time series ([14, 15, 21, 28, 20]).", "startOffset": 232, "endOffset": 252}, {"referenceID": 7, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 174, "endOffset": 181}, {"referenceID": 9, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 174, "endOffset": 181}, {"referenceID": 18, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 209, "endOffset": 217}, {"referenceID": 22, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 209, "endOffset": 217}, {"referenceID": 2, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 243, "endOffset": 246}, {"referenceID": 31, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 275, "endOffset": 283}, {"referenceID": 32, "context": "The RC concept has been demonstrated to work on a variety of physical implementation platforms, such as water ripples ([8]), mechanical constructs and tensegrity structures ([4, 10]), electro-optical devices ([19, 23]), fully optical devices ([3]) and nanophotonic circuits ([32, 33]).", "startOffset": 275, "endOffset": 283}, {"referenceID": 18, "context": "Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system ([19, 23, 27]).", "startOffset": 156, "endOffset": 168}, {"referenceID": 22, "context": "Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system ([19, 23, 27]).", "startOffset": 156, "endOffset": 168}, {"referenceID": 26, "context": "Specifically, we will employ a physical dynamical system that has been studied extensively from the RC paradigm, a delayed feedback electro-optical system ([19, 23, 27]).", "startOffset": 156, "endOffset": 168}, {"referenceID": 23, "context": "way (by ensuring a high diversity in the network\u2019s response ([24, 1])), a way to create task-specific input encodings is still lacking.", "startOffset": 61, "endOffset": 68}, {"referenceID": 0, "context": "way (by ensuring a high diversity in the network\u2019s response ([24, 1])), a way to create task-specific input encodings is still lacking.", "startOffset": 61, "endOffset": 68}, {"referenceID": 12, "context": "In [13], the possibility to use backpropagation through time (BPTT) ([26]) as a generic optimization tool for physical dynamical systems was addressed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "In [13], the possibility to use backpropagation through time (BPTT) ([26]) as a generic optimization tool for physical dynamical systems was addressed.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "In [11] simulated results of BPTT used as an optimization method for input encoding in the physical system described above were presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The physical system we employ in this paper is a delayed feedback system exhibiting Ikeda-type dynamics ([18, 34]).", "startOffset": 105, "endOffset": 113}, {"referenceID": 33, "context": "The physical system we employ in this paper is a delayed feedback system exhibiting Ikeda-type dynamics ([18, 34]).", "startOffset": 105, "endOffset": 113}, {"referenceID": 18, "context": "The measured output signal is well described by the following differential equation ([19]):", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Note that the parameters \u03b2 and \u03c6, together with the global scaling of the input signal z(t), control the global dynamical behavior of the system ([19]).", "startOffset": 146, "endOffset": 150}, {"referenceID": 22, "context": "In fact, using a difference of one masking step between D and P has been the basis for opto-electronic systems that do not have a low-pass filter ([23]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "In [13] it was shown that BPTT can be applied to models of continuous-time dynamical systems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "To classify static images using a dynamical system, we follow an approach similar to the one introduced in [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "For training we used Nesterov momentum ([29]), with momentum coefficient 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "0% [7] [5]", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "0% [7] [5]", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "We applied frame-wise phoneme recognition to the TIMIT dataset ([9]).", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "For an overview of other results on frame error rate please check [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "This has been evidenced in, for example, [31], where good results on the TIMIT dataset were achieved by using Echo State Networks (a particular kind of Reservoir Computing) of up to 20,000 nodes.", "startOffset": 41, "endOffset": 45}], "year": 2015, "abstractText": "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers. \u2217OPERA Photonique, Universit/\u2019e Libre de Bruxelles, Avenue F. Roosevelt 50, 1050 Brussels (michiel.hermans@ulb.ac.be) \u2020Instituto de F\u0301\u0131sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Campus Universitat de les Illes Balears, E-07122 Palma de Mallorca, Spain \u2021ELIS departement, Ghent University, Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium \u00a7INTEC departement, Ghent University, Sint Pietersnieuwstraat 41, 9000 Ghent, Belgium \u00b6Instituto de F\u0301\u0131sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Campus Universitat de les Illes Balears, E-07122 Palma de Mallorca, Spain 1 ar X iv :1 50 1. 02 59 2v 1 [ cs .N E ] 1 2 Ja n 20 15", "creator": "LaTeX with hyperref package"}}}