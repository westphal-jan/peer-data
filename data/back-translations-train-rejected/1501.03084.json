{"id": "1501.03084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2015", "title": "Deep Learning with Nonparametric Clustering", "abstract": "Clustering is an essential problem in machine learning and data mining. One vital factor that impacts clustering performance is how to learn or design the data representation (or features). Fortunately, recent advances in deep learning can learn unsupervised features effectively, and have yielded state of the art performance in many classification problems, such as character recognition, object recognition and document categorization. However, little attention has been paid to the potential of deep learning for unsupervised clustering problems. In this paper, we propose a deep belief network with nonparametric clustering. As an unsupervised method, our model first leverages the advantages of deep learning for feature representation and dimension reduction. Then, it performs nonparametric clustering under a maximum margin framework -- a discriminative clustering model and can be trained online efficiently in the code space. Lastly model parameters are refined in the deep belief network. Thus, this model can learn features for clustering and infer model complexity in an unified framework. The experimental results show the advantage of our approach over competitive baselines.", "histories": [["v1", "Tue, 13 Jan 2015 17:26:26 GMT  (274kb,D)", "http://arxiv.org/abs/1501.03084v1", "14 pages, 6 figures"]], "COMMENTS": "14 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen"], "accepted": false, "id": "1501.03084"}, "pdf": {"name": "1501.03084.pdf", "metadata": {"source": "CRF", "title": "Deep Learning with Nonparametric Clustering", "authors": ["Gang Chen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in a position, puts us in the position we are in."}, {"heading": "2 Related work", "text": "In this context, it should be noted that the models we have mentioned are not a purely quantitative solution, but a purely quantitative one."}, {"heading": "3 Deep learning with nonparametric maximum margin clustering", "text": "In this section, we will first review RBM and DBN in terms of learning features and then introduce the non-parametric method of maximum margin clustering (NMMC), which was developed based on the features learned from DBN. Finally, we will refine our model in terms of cluster markers for the data."}, {"heading": "3.1 Feature learning with deep belief network", "text": "Suppose we have a training set D = {vi} Ni = 1 in which vi-Rd. An RBM with n hidden units is a parametric model of the common distribution between a layer of hidden variables h = (h1,..., hn) and the observations v = (v1,..., vd). The RBM joint probability takes the form: p (v, h). e-E (v, h) (1), where the energy function isE (v, h) = \u2212 hTW1v \u2212 bTv \u2212 cTh (2). And we can calculate the following conditional probability: p (v | h)."}, {"heading": "3.2 Nonparametric maximum margin clustering", "text": "Nonparametric Maximum Margin Clustering (NMMC) is a discriminatory cluster model for cluster analysis. In view of the nonlinear mapping with DBN, we can first map the original training data D = {vi} Ni = 1 into codes X = {xi} Ni = 1 in the embedding space. Then, with X = {xi} Ni = 1 and its cluster indicators z = {zi} Ni = 1, we can suggest the following conditional probability for nonparametric clustering: P (z, {\u03b1} Kk = 1 | X)."}, {"heading": "3.2.1 Gibbs sampling", "text": "Considering the data points X = [xi] Ni = 1 and its cluster indicators z = q = q | Ni = 1, the Gibbs scan includes iterations that take turns taking samples from the conditional probability while other variables remain fixed. For each indicator, we can derive its conditional after-effects as follows: p (zi = k | z \u2212 i, xi, {\u03b8k} Kk = 1, \u03bb \u2212 Kk = 1) (7) = p (zi = k | i, z \u2212 i, Kk = 1) (6). (zi = k | z \u2212 i, xi \u2212 Kk = 1) p (xi | zi \u2212 k} Kk = 1) (7) = p (zi = k | z \u2212 i, \u03b1) p (xi | \u041ak) Kk = 1) (8)."}, {"heading": "3.2.2 Online maximum margin learning", "text": "We follow the passive aggressive algorithm (PA) [5] below to learn component parameters in our discriminatory model with maximum margins [25]. We designate the instance presented to the algorithm on round t by xt-Rn, which is associated with a unique denomination zt-1, K. Note that the denomination zt is determined by the above Gibbs sampling algorithm in Equation (10). We will define the instance as a parameter vector composed of all parameters [1, K] Kk = 1 (meaning that the z-th block is in equation, or says that the z-th block is in equation), and vice versa (xt, zt) is a feature vector relating to input xt and output zt, which is composed of K blocks, and all blocks except zt-th are set as the zero vector, while zt-th is set to xt."}, {"heading": "3.3 Fine-tuning the model", "text": "After determining the number of clusters and labels for all training data, we can take the fine-tuning process to refine the DBN parameters. Note that the objective function in Eq. (12) takes the l1 hinge loss as in [23]. Thus, one possible way is that we can take the sub-gradient and reverse the error to update the DBN parameters. In our approach, we use a different method and update only the top layer weights WL and in the deep structures. This fine-tuning process is inspired by the classification RBM [15] for model refinement. Basically, we assume that the top DBN layer weights WL and SVM weight can be combined into a classification RBM, as in [15] by maximizing the common probability p (x, z) after deriving the cluster labels for all instances with NMMC."}, {"heading": "4 Experimental Results", "text": "To analyze our model, we performed clustering analyses on two types of data: images and documents, and compared our results with competing baselines. For all experiments, including pretraining and fine tuning, we set the learning rate as 0.1, the maximum epoch to 100, and used CD-1 to learn the weights and biases in deep belief. We used the customized method to evaluate all clustering results, and also used the clustering methods as 0.1, the maximum epoch to 100, and used CD-1 to compare the illustrations of handwriting from 0 to 9 with a training set of 60,000 examples and a test set of 10,000 examples, and was widely used to test character recognition methods. In the experiment, we randomly selected 5000 images from the training sets for parameter learning and 1000 examples from the test sets to test our model."}], "references": [{"title": "Mixtures of dirichlet processes with applications to bayesian nonparametric problems", "author": ["C.E. Antoniak"], "venue": "Annals of Statistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "Support vector clustering", "author": ["A. Ben-Hur", "D. Horn", "H.T. Siegelmann", "V. Vapnik"], "venue": "J. Mach. Learn. Res", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "PAMI pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Variational inference for dirichlet process mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "JMLR pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "J. Mach. Learn. Res", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1973}, {"title": "Dirichlet process mixtures of generalized linear models", "author": ["L.A. Hannah", "D.M. Blei", "W.B. Powell"], "venue": "J. Mach. Learn. Res. pp", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science 313(5786),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Comput", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification 2(1),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1985}, {"title": "A nonparametric variable clustering model", "author": ["D.A. Knowles", "K. Palla", "Z. Ghahramani"], "venue": "In: NIPS. pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Collapsed variational Dirichlet process mixture models", "author": ["K. Kurihara", "M. Welling", "Y. Teh"], "venue": "Proc. Int. Jt. Conf. Artif. Intell", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "J. Mach. Learn. Res", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Deep supervised tdistributed embedding", "author": ["M.R. Min", "L. van der Maaten", "Z. Yuan", "A.J. Bonner", "Z. Zhang"], "venue": "Omnipress", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Implicit mixtures of restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Markov chain sampling methods for dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In: ADVANCES IN LARGE MARGIN CLASSIFIERS", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W. Rand"], "venue": "Journal of the American Statistical Association 66(336),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1971}, {"title": "The infinite gaussian mixture model. In: NIPS12", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Graphical models for visual object recognition and tracking", "author": ["E.B. Sudderth"], "venue": "Ph.D. thesis,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Deep learning using support vector machines", "author": ["Y. Tang"], "venue": "Workshop on Representational Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Dirichlet processes. In: Encyclopedia of Machine Learning", "author": ["Y.W. Teh"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "J. Mach. Learn. Res", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle"], "venue": "In: International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "In: NIPS17", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 33, "endOffset": 44}, {"referenceID": 25, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 33, "endOffset": 44}, {"referenceID": 2, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 33, "endOffset": 44}, {"referenceID": 8, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 99, "endOffset": 106}, {"referenceID": 26, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 99, "endOffset": 106}, {"referenceID": 9, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 135, "endOffset": 147}, {"referenceID": 14, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 135, "endOffset": 147}, {"referenceID": 22, "context": "Recent advances in deep learning [10, 26, 3] have attracted great attention in dimension reduction [9, 27] and classification problems [10, 15, 23].", "startOffset": 135, "endOffset": 147}, {"referenceID": 5, "context": "The advantages of deep learning are that they give mappings which can capture meaningful structure information in the code space and introduce bias towards configurations of the parameter space that are helpful for unsupervised learning [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 2, "context": "restricted Boltzmann machines), with the purpose to yield more abstract and ultimately more useful representations [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "In addition, deep learning with gradient descent scales linearly in time and space with the number of train cases, which makes it possible to apply to large scale data sets [9].", "startOffset": 173, "endOffset": 176}, {"referenceID": 20, "context": "Another important research topic in clustering analysis is how to adapt model complexity for increasing volumes in the era of big data [21, 4, 24].", "startOffset": 135, "endOffset": 146}, {"referenceID": 3, "context": "Another important research topic in clustering analysis is how to adapt model complexity for increasing volumes in the era of big data [21, 4, 24].", "startOffset": 135, "endOffset": 146}, {"referenceID": 23, "context": "Another important research topic in clustering analysis is how to adapt model complexity for increasing volumes in the era of big data [21, 4, 24].", "startOffset": 135, "endOffset": 146}, {"referenceID": 0, "context": "This approach is an unsupervised clustering method, inspired by the advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models [1, 7, 4].", "startOffset": 161, "endOffset": 170}, {"referenceID": 6, "context": "This approach is an unsupervised clustering method, inspired by the advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models [1, 7, 4].", "startOffset": 161, "endOffset": 170}, {"referenceID": 3, "context": "This approach is an unsupervised clustering method, inspired by the advances in unsupervised feature learning with DBN, as well as nonparametric Bayesian models [1, 7, 4].", "startOffset": 161, "endOffset": 170}, {"referenceID": 1, "context": "However, they [2, 28] either cannot learn parameters online efficiently or need to define the number of clusters like other clustering approaches, such as k-means, Gaussian mixture model (GMM) and spectral clustering.", "startOffset": 14, "endOffset": 21}, {"referenceID": 27, "context": "However, they [2, 28] either cannot learn parameters online efficiently or need to define the number of clusters like other clustering approaches, such as k-means, Gaussian mixture model (GMM) and spectral clustering.", "startOffset": 14, "endOffset": 21}, {"referenceID": 3, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 13, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 7, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 11, "context": "Considering the weakness of parametric models mentioned above, many nonparametric methods [4, 14, 8, 12] have been proposed to handle the model complexity problems.", "startOffset": 90, "endOffset": 104}, {"referenceID": 0, "context": "One of the widely used nonparametric models for clustering is Dirichlet process mixture (DPM) [1, 7].", "startOffset": 94, "endOffset": 100}, {"referenceID": 6, "context": "One of the widely used nonparametric models for clustering is Dirichlet process mixture (DPM) [1, 7].", "startOffset": 94, "endOffset": 100}, {"referenceID": 8, "context": "Unsupervised feature learning with deep structures was first proposed in [9] for dimension reduction.", "startOffset": 73, "endOffset": 76}, {"referenceID": 26, "context": "Later, this unsupervised approach was developed into semi-supervised embedding [27] and supervised mapping [16] scenarios.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "Later, this unsupervised approach was developed into semi-supervised embedding [27] and supervised mapping [16] scenarios.", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 14, "endOffset": 17}, {"referenceID": 14, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 76, "endOffset": 84}, {"referenceID": 22, "context": ", square loss [9], logistic regression [15] or support vector machine (SVM) [13, 23] for classification in the code space.", "startOffset": 76, "endOffset": 84}, {"referenceID": 5, "context": "The success behind deep learning is that it can learn useful information for data visualization and classification [6, 3].", "startOffset": 115, "endOffset": 121}, {"referenceID": 2, "context": "The success behind deep learning is that it can learn useful information for data visualization and classification [6, 3].", "startOffset": 115, "endOffset": 121}, {"referenceID": 16, "context": "A recent interesting approach is the implicit mixture of RBMs [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "To learn RBM parameters, we need to optimize the negative log likelihood \u2212logp(v) on training data D, the parameters updating can be calculated with a efficient stochastic descent method, namely contrastive divergence (CD) [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "A Deep Belief Network (DBN) is composed of stacked RBMs [9] learned layer by layer greedily, where the top layer is an RBM and the lower layers can be interpreted as a directed sigmoid belief network [3], shown in Fig.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "A Deep Belief Network (DBN) is composed of stacked RBMs [9] learned layer by layer greedily, where the top layer is an RBM and the lower layers can be interpreted as a directed sigmoid belief network [3], shown in Fig.", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "Recall that Dirichlet process mixture (DPM) [1, 7] is the widely used nonparametric Bayesian approach for clustering analysis and model learning, specified with DP prior measure G0 and \u03b1.", "startOffset": 44, "endOffset": 50}, {"referenceID": 6, "context": "Recall that Dirichlet process mixture (DPM) [1, 7] is the widely used nonparametric Bayesian approach for clustering analysis and model learning, specified with DP prior measure G0 and \u03b1.", "startOffset": 44, "endOffset": 50}, {"referenceID": 13, "context": "The essential difference between our model and DPM is that we maximize a conditional probability, instead of joint probability as in DPM [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "For each iteration (on the whole dataset), we also update \u03b1 with adaptive rejection sampling [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "(9) satisfies the general form of exponential families, which are functions solely of the chosen sufficient statistics [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "We follow the passive aggressive algorithm (PA) [5] below in order to learn component parameters in our discriminative model with maximum margins [25].", "startOffset": 48, "endOffset": 51}, {"referenceID": 24, "context": "We follow the passive aggressive algorithm (PA) [5] below in order to learn component parameters in our discriminative model with maximum margins [25].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Following the passive aggressive (PA) algorithm [5], we optimize the objective function:", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "For convergence analysis and time complexity, refer to [5].", "startOffset": 55, "endOffset": 58}, {"referenceID": 22, "context": "(12) takes the l1 hinge loss as in [23].", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "This fine-tuning process is inspired by the classification RBM [15] for model refining.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "Basically, we assume the top DBN layer weight WL and SVM weight \u0398 can be combined into a classification RBM as in [15] by maximizing the joint likelihood p(x, z) after we infer the cluster labels for all instances with NMMC.", "startOffset": 114, "endOffset": 118}, {"referenceID": 18, "context": "Note that there is mapping from SVM\u2019s scores to probabilistic outputs with logistic function [19], which can maintain label consistency between the SVM classifier and the softmax function.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "(4) with deep feature learning is non-convex, which can be easily trapped into local minimum with L-BFGS [9]; (2) if there was clustering error in the top layer, it could be easily propagated in the backpropagation stage; (3) To only update the top layer can effectively handle the overfitting problem.", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "We used the adjusted Rand Index [11, 20] to evaluate all the clustering results.", "startOffset": 32, "endOffset": 40}, {"referenceID": 19, "context": "We used the adjusted Rand Index [11, 20] to evaluate all the clustering results.", "startOffset": 32, "endOffset": 40}, {"referenceID": 16, "context": "02 IMRBM [17] (n = 100, K = 10) 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "010 IMRBM [17] (n = 200, K = 20) 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "We currently use DBN [10] instead of deep autoencoders [9] for fast feature learning because the latter is time-consuming for dimension reduction.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "We currently use DBN [10] instead of deep autoencoders [9] for fast feature learning because the latter is time-consuming for dimension reduction.", "startOffset": 55, "endOffset": 58}], "year": 2015, "abstractText": "Clustering is an essential problem in machine learning and data mining. One vital factor that impacts clustering performance is how to learn or design the data representation (or features). Fortunately, recent advances in deep learning can learn unsupervised features effectively, and have yielded state of the art performance in many classification problems, such as character recognition, object recognition and document categorization. However, little attention has been paid to the potential of deep learning for unsupervised clustering problems. In this paper, we propose a deep belief network with nonparametric clustering. As an unsupervised method, our model first leverages the advantages of deep learning for feature representation and dimension reduction. Then, it performs nonparametric clustering under a maximum margin framework \u2013 a discriminative clustering model and can be trained online efficiently in the code space. Lastly model parameters are refined in the deep belief network. Thus, this model can learn features for clustering and infer model complexity in an unified framework. The experimental results show the advantage of our approach over competitive baselines.", "creator": "LaTeX with hyperref package"}}}