{"id": "1501.03093", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2015", "title": "MultiGain: A controller synthesis tool for MDPs with multiple mean-payoff objectives", "abstract": "We present MultiGain, a tool to synthesize strategies for Markov decision processes (MDPs) with multiple mean-payoff objectives. Our models are described in PRISM, and our tool uses the existing interface and simulator of PRISM. Our tool extends PRISM by adding novel algorithms for multiple mean-payoff objectives, and also provides features such as (i)~generating strategies and exploring them for simulation, and checking them with respect to other properties; and (ii)~generating an approximate Pareto curve for two mean-payoff objectives. In addition, we present a new practical algorithm for the analysis of MDPs with multiple mean-payoff objectives under memoryless strategies.", "histories": [["v1", "Tue, 13 Jan 2015 18:04:46 GMT  (84kb,D)", "http://arxiv.org/abs/1501.03093v1", "Extended version for a TACAS 2015 tool demo paper"]], "COMMENTS": "Extended version for a TACAS 2015 tool demo paper", "reviews": [], "SUBJECTS": "cs.AI cs.LO", "authors": ["tom\\'a\\v{s} br\\'azdil", "krishnendu chatterjee", "vojt\\v{e}ch forejt", "anton\\'in ku\\v{c}era"], "accepted": false, "id": "1501.03093"}, "pdf": {"name": "1501.03093.pdf", "metadata": {"source": "CRF", "title": "MultiGain: A controller synthesis tool for MDPs with multiple mean-payoff objectives", "authors": ["Tom\u00e1\u0161 Br\u00e1zdil", "Krishnendu Chatterjee", "Vojt\u011bch Forejt"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Markov decision-making processes (MDPs) are the de facto model for the analysis of probability systems with non-determinism [11], with a wide range of applications [5]. In each state of an MDP, a controller selects one of several actions (the non-deterministic decisions), and the current state and action gives a probability distribution across the successor states. A classic objective used to investigate quantitative properties of systems is the limit-means-payment target, in which a reward (or cost) is associated with each run of the reward. MDPs with individual means-payment targets have been well studied in the literature (see, for example, the billing). But in many modeling areas there is no single goal that can be optimized, potential, potential targets, potential targets that are potential."}, {"heading": "2 Definitions", "text": "MDPs and Strategies. An MDP G = (S, A, Act, \u03b4) consists of (i) a finite set S of states; (ii) a finite set A of acts; (iii) an empowerment function Act: S \u2192 2A\\ {\u2205}, which assigns to each state a probability distribution over the successor states (dist (S), denotes all probability distributions over S); and (iv) a transitional function \u03b4: S \u00b7 A \u2192 dist (S), which gives a state s and an action, gives a probability distribution over the successor states (dist (S). (S) denotes all probability distributions over S). W.l.o.g. We assume that each action is activated in exactly one state, and we denote this state Src (a). We will therefore assume that we have a solution: A \u2192 dist (S). Strategies describe how to select the next action, which consists of an actualized path from the state to a series of action elements (a series of MDP and a series of action elements)."}, {"heading": "3 Algorithms and Implementation", "text": "We first recall the existing results of MDPs with multiple disbursement problems [6] and then describe our implementation and expansion. Before we present the existing results, we first call the presentation of maximum end components in MDP. Maximum end components (A) > 0 then s'T; and (2) for all s, t'T there is a finite path from s to t that all states and actions appearing in the path to T and B each belong to the end components. (T, B) is a maximum end component (MEC) when it is maximum. pointwise subset ordering. An MDP is unichain when all B's are satisfactory."}, {"heading": "4 Experimental Results: Case Studies", "text": "We have evaluated our tool on two standard case studies, adapted from [1], and also mention other applications in which our tool could be used.Dining Philosophers is a case study based on the algorithm of [9], which extends Lehmann and Rabin's randomized solution [12] to the restaurant philosophers problem, so that there is no requirement for fairness assumptions. The constant N indicates the number of philosophers. We use two reward structures, think and eat respectfully for the number of philosophers who currently think and eat. Randomized Mutual Exclusion models provide a solution to the problem of mutual exclusion by. The parameter N indicates the number of processes that compete for access to the critical section. Here, we define reward structures that attempt and criticize those currently trying to reach the critical section, and those that are therein, at present (the latter number is obviously never more than 1).Evaluation The statistics for some of our experiments are available in Table 1 (the full results are available on the website)."}, {"heading": "17. R. Wimmer, B. Braitling, B. Becker, E. M. Hahn, P. Crouzen, H. Hermanns, A. Dhama, and O. E. Theel. Symblicit calculation of long-run averages for concurrent probabilistic", "text": "Systems. In QEST, pp. 27-36. IEEE Computer Society, 2010."}, {"heading": "5 Appendix", "text": "The main idea of the correctness argument of the Boolean combination of linear constraints is as follows: In the face of a memory-less strategy \u03c3, once the strategy is established, we get a Markov chain with two types of states, temporary states and recurring states. For recurring states, the variable x represents the flow equations and intuitively the variable xa the long-term average frequency of the action a. The constraint with respect to an act (s) xa = 0 is the constraint to show that a state is temporary. The variables ya are used to determine the probabilities to achieve the recurring classes. For an action b, if Src) is a temporary act, then for the first additional constraint for memory-less strategies, since it is a memory-less class (Src) xa = 0."}], "references": [{"title": "The steady-state control problem for Markov decision processes", "author": ["S. Akshay", "N. Bertrand", "S. Haddad", "L. H\u00e9lou\u00ebt"], "venue": "In QEST, pages 290\u2013304,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Principles of model checking", "author": ["C. Baier", "J.-P. Katoen"], "venue": "MIT Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Two views on multiple mean-payoff objectives in Markov decision processes", "author": ["T. Br\u00e1zdil", "V. Bro\u017eek", "K. Chatterjee", "V. Forejt", "A. Ku\u010dera"], "venue": "In LICS 2011, pages 33\u201342. IEEE Computer Society,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Markov decision processes with multiple long-run average objectives", "author": ["K. Chatterjee"], "venue": "In FSTTCS, pages 473\u2013484,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "PRISM-games: A model checker for stochastic multi-player games", "author": ["T. Chen", "V. Forejt", "M. Kwiatkowska", "D. Parker", "A. Simaitis"], "venue": "In TACAS\u201913, volume 7795 of LNCS. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Randomized dining philosophers without fairness assumption", "author": ["M. Duflot", "L. Fribourg", "C. Picaronny"], "venue": "Distributed Computing, 17(1):65\u201376,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Pareto curves for probabilistic model checking", "author": ["V. Forejt", "M. Kwiatkowska", "D. Parker"], "venue": "In ATVA\u201912, volume 7561 of LNCS, pages 317\u2013332. Springer,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": "MIT Press,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1960}, {"title": "On the advantage of free choice: A symmetric and fully distributed solution to the dining philosophers problem", "author": ["D. Lehmann", "M. Rabin"], "venue": "In POPL\u201981,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1981}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": "J. Wiley and Sons,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "N -process mutual exclusion with bounded waiting by 4 log2N -valued shared variable", "author": ["M. Rabin"], "venue": "Journal of Computer and System Sciences, 25(1):66\u201375,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Theory of Linear and Integer Programming", "author": ["A. Schrijver"], "venue": "John Wiley & Sons,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Synthesis from incompatible specifications", "author": ["P. \u010cern\u00fd", "S. Gopi", "T.A. Henzinger", "A. Radhakrishna", "N. Totla"], "venue": "In EMSOFT, pages 53\u201362,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Symblicit calculation of long-run averages for concurrent probabilistic systems", "author": ["R. Wimmer", "B. Braitling", "B. Becker", "E.M. Hahn", "P. Crouzen", "H. Hermanns", "A. Dhama", "O.E. Theel"], "venue": "In QEST, pages 27\u201336. IEEE Computer Society,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "Markov decision processes (MDPs) are the de facto model for analysis of probabilistic systems with non-determinism [11], with a wide range of applications [5].", "startOffset": 115, "endOffset": 119}, {"referenceID": 1, "context": "Markov decision processes (MDPs) are the de facto model for analysis of probabilistic systems with non-determinism [11], with a wide range of applications [5].", "startOffset": 155, "endOffset": 158}, {"referenceID": 9, "context": ", [13]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "The complexity of MDPs with multiple mean-payoff objectives was studied in [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "Our contributions are as follows: (1) we extend PRISM with novel algorithms for multiple mean-payoff objectives from [6]; (2) develop on the results of [6] to synthesize strategies, and explore them for simulation, and check them with respect to other properties (as done in PRISM-games [8]); and (3) for the important special case of two mean-payoff objectives we provide the feature to visualize the approximate Pareto curve (where the Pareto curve represents the \u201ctrade-off\u201d curve and consists of solutions that are not strictly dominated ar X iv :1 50 1.", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Our contributions are as follows: (1) we extend PRISM with novel algorithms for multiple mean-payoff objectives from [6]; (2) develop on the results of [6] to synthesize strategies, and explore them for simulation, and check them with respect to other properties (as done in PRISM-games [8]); and (3) for the important special case of two mean-payoff objectives we provide the feature to visualize the approximate Pareto curve (where the Pareto curve represents the \u201ctrade-off\u201d curve and consists of solutions that are not strictly dominated ar X iv :1 50 1.", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "Our contributions are as follows: (1) we extend PRISM with novel algorithms for multiple mean-payoff objectives from [6]; (2) develop on the results of [6] to synthesize strategies, and explore them for simulation, and check them with respect to other properties (as done in PRISM-games [8]); and (3) for the important special case of two mean-payoff objectives we provide the feature to visualize the approximate Pareto curve (where the Pareto curve represents the \u201ctrade-off\u201d curve and consists of solutions that are not strictly dominated ar X iv :1 50 1.", "startOffset": 287, "endOffset": 290}, {"referenceID": 3, "context": "Finally, we present a new practical approach for analysis of MDPs with multiple mean-payoff objectives under memoryless strategies: previously an NP bound was shown in [7] by guessing all bottom strongly connected components (BSCCs) of the MDP graph for a memoryless strategy and this gave an exponential enumerative algorithm; in contrast, we present a linear reduction to solving a boolean combination of linear constraints (which is a special class of mixed integer linear programming where the integer variables are binary).", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "The memory elements are updated stochastically in each transition, and the next action is chosen probabilistically (among enabled actions) based on the current state and current memory [6].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "We first recall the existing results for MDPs with multiple mean-payoff objectives [6], and then describe our implementation and extensions.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Result from [6].", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "The results of [6] showed that (i) the existence question can be answered in polynomial time, by reduction to linear programming; (ii) if there exists a strategy for the existence problem, then there exists a witness strategy with only two-memory states.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "The polynomial-time algorithm is as follows: it was shown in [6] that the answer to the existence problem is yes iff there exists a non-negative solution to the system of linear inequalities given in Fig.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "We have implemented the algorithm of [6].", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "The correctness of the witness construction follows from [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "For memoryless strategies, the current upper bound is NP [7] and the previous algorithm enumerates all possible BSCCs under a memoryless strategy.", "startOffset": 57, "endOffset": 60}, {"referenceID": 11, "context": "We present a polynomial-time reduction to solving a boolean combination of linear constraints, that can be easily encoded using MILP with binary variables [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "The weights are selected in a way similar to [10], allowing us to obtain the approximation of the curve.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "Along with the algorithm from [6] we have also implemented a visual representation of the Pareto curve for two-dimensional objectives.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "In addition, we adapted a feature from PRISM-games [8] which allows the user to generate strategies, so that they can be explored and investigated by simulation.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Dining philosophers is a case study based on the algorithm of [9], which extends Lehmann and Rabin\u2019s randomised solution [12] to the dining philosophers problem so that there is no requirement for fairness assumptions.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "Dining philosophers is a case study based on the algorithm of [9], which extends Lehmann and Rabin\u2019s randomised solution [12] to the dining philosophers problem so that there is no requirement for fairness assumptions.", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "Randomised Mutual Exclusion models a solution to the mutual exclusion problem by [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "(A) The problem of synthesis from incompatible specifications was considered in [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "In adversarial environments the problem reduces to games and for probabilistic environments to MDPs, with multiple mean-payoff objectives [16].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "(B) The problem of synthesis of steady state distributions for ergodic MDPs was considered in [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "The steady state distribution synthesis question of [4] then reduces to the existence question for multiple mean-payoff MDPs.", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "The limiting factor is the LP solver, and so an interesting direction would be to extend the results of [17] to multiple objectives.", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "We present MultiGain, a tool to synthesize strategies for Markov decision processes (MDPs) with multiple mean-payoff objectives. Our models are described in PRISM, and our tool uses the existing interface and simulator of PRISM. Our tool extends PRISM by adding novel algorithms for multiple mean-payoff objectives, and also provides features such as (i) generating strategies and exploring them for simulation, and checking them with respect to other properties; and (ii) generating an approximate Pareto curve for two mean-payoff objectives. In addition, we present a new practical algorithm for the analysis of MDPs with multiple mean-payoff objectives under memoryless strategies.", "creator": "LaTeX with hyperref package"}}}