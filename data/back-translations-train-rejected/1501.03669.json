{"id": "1501.03669", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2015", "title": "A Proximal Approach for Sparse Multiclass SVM", "abstract": "Sparsity-inducing penalties are useful tools to design multiclass support vector machines (SVMs). In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by Crammer and Singer. We provide two algorithms: the first one dealing with the hinge loss as a penalty term, and the other one addressing the case when the hinge loss is enforced through a constraint. The related convex optimization problems can be efficiently solved thanks to the flexibility offered by recent primal-dual proximal algorithms and epigraphical splitting techniques. Experiments carried out on several datasets demonstrate the interest of considering the exact expression of the hinge loss rather than a smooth approximation. The efficiency of the proposed algorithms w.r.t. several state-of-the-art methods is also assessed through comparisons of execution times.", "histories": [["v1", "Thu, 15 Jan 2015 13:23:14 GMT  (280kb,D)", "https://arxiv.org/abs/1501.03669v1", null], ["v2", "Fri, 16 Jan 2015 09:26:32 GMT  (280kb,D)", "http://arxiv.org/abs/1501.03669v2", "Corrected the name of an author"], ["v3", "Fri, 6 Feb 2015 23:36:03 GMT  (303kb,D)", "http://arxiv.org/abs/1501.03669v3", null], ["v4", "Sun, 26 Apr 2015 15:33:36 GMT  (303kb,D)", "http://arxiv.org/abs/1501.03669v4", null], ["v5", "Mon, 14 Dec 2015 09:49:32 GMT  (304kb,D)", "http://arxiv.org/abs/1501.03669v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["g chierchia", "nelly pustelnik", "jean-christophe pesquet", "b pesquet-popescu"], "accepted": false, "id": "1501.03669"}, "pdf": {"name": "1501.03669.pdf", "metadata": {"source": "CRF", "title": "A Proximal Approach for Sparse Multiclass SVM\u2217", "authors": ["G. Chierchia", "Nelly Pustelnik", "Jean-Christophe Pesquet", "B. Pesquet-Popescu"], "emails": ["first.last@telecom-paristech.fr).", "nelly.pustelnik@ens-lyon.fr.", "jean-christophe.pesquet@univ-paris-est.fr."], "sections": [{"heading": null, "text": "In this paper, we propose a convex optimization approach to efficiently and accurately solve the multi-class SVM learning problem, which includes sparse regularization and the multi-class hinge loss formulated by [1]. We provide two algorithms: the first, which treats hinge loss as a punitive term, and the second, which treats the case where the hinge loss is forced by a restriction. Convex optimization problems associated with it can be efficiently solved thanks to the flexibility offered by current primary-dual proximal algorithms and epigraphic splitting techniques. Experiments conducted on multiple data sets show an interest in considering the exact expression of hinge loss rather than a smooth approximation. The efficiency of the proposed algorithms is also evaluated by comparing execution times."}, {"heading": "1 Introduction", "text": "Support vector machines (SVMs) have gained a great deal of popularity in solving large-scale classification problems. In fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6]. Consequently, the great difficulty encountered in this type of application is due to computing costs. SVM learning is traditionally solved by using standard lagrangian duality techniques [7, 1]. However, this approach has several advantages, such as the kernel trick [8], or the ability to break the problem into a sequence of smaller ones. [9, 10] Some work is also proposed to align the dual problem with cutting plan methods in order to tackle scenarios with thousands or even an infinite number of classes. [3, 6] This work has been supported by the CNRS IMAG's OPTIMISME project."}, {"heading": "1.1 Related work", "text": "The idea goes back to the work of [23], who has shown that the \"1-norm-regularization\" can effectively perform \"feature selection\" by reducing small coefficients to zero. However, other forms of regularization have also been investigated, such as the \"0-norm [24], the\" p-norm with p > 0 [25], the \"\u221e-norm [26], and the combination of\" 0-1-norms [27] or \"1-2 norms\" [28]. Another solution was proposed by [29], which proposes the SVM learning problem by using an indicator vector vector-vector-vector-vector vector (whose components are either 0 or 1) to model active features and to solve the resulting schema-schema-combinatorial schema problem by convex-centric relativization using a cutting-layer algorithm. [30] proposed an accelerated algorithm that includes SVM-quantum classes, the schema-schema-schema-schema-schema-schema-schema with a convex-schema-schema-schema-procedure-schema-algorithm-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-schema-"}, {"heading": "1.2 Contributions", "text": "The algorithmic solutions proposed in the literature for dealing with sparse multi-class SVMs are either cutting-plane methods [29], proximal algorithms [30, 38] or linear programming techniques [33, 37]. However, both cutting-plane methods and proximal algorithms have been used to find an approximate solution, while linear programming techniques may not be able to scale well to large data sets. In this paper, we propose a novel approach based on proximal tools and newer epigraphic splitting techniques [21], which allow us to precisely solve the sparse multi-class SVM learning problem by an efficient primary-dual proximal method [19, 20]."}, {"heading": "1.3 Outline", "text": "The paper is structured as follows: In Section 2 we formulate the multi-class problem SVM with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach using three standard data sets and compare it with the methods proposed in [38], [30], [37] and [11]."}, {"heading": "1.4 Notation", "text": "\"0\" (RX) denotes the set of correct, lower semicontinuous, convex functions from Euclidean space RX to] \u2212 \u221e, + \u221e]. The inscription of \"0\" (RX) is the nonempty, closed convex subset of \"RX \u00b7 R\" defined as \"epilepsy\" = \"y,\" \"RX,\" \"RX\" and \"y.\" For each x-convex subset of \"x,\" the subdifferential of \"x\" is equal to \"(x) =\" u \"RX\" < y \u2212 x | u > + economic subset of \"y.\" If \"C\" is a nonempty, closed convex subset of \"X,\" then \"C\" is the indicator function of \"C,\" equal to \"C\" and otherwise. \""}, {"heading": "2 Sparse Multiclass SVM", "text": "A multi-class classifier can be modeled as a function d: RN \u2192 {1,.., K}, which predicts the class k: RN {1,.., K} associated with a given observation u: RN (e.g. a signal, an image, or a diagram).This predictor relies on K various differentiation functions Dk: RN 7 \u2192 R, which measure for each k: {1,..., K} the probability that an observation belongs to class k. Consequently, the predictor selects the class that best matches an observation, i.e. (u): Argmax k: {1,..., K} Dk (u). In supervised learning, the differentiation functions from a series of L input output pairs S = {(u ', z'): RN \u00d7 1,."}, {"heading": "2.1 Background", "text": "The goal of learning is to find the vector x in such a way that the input / output pair (u ', z') \"S\" is correctly predicted by the classifier, i.e., z '= Argmax k' (u ') > (u') > x (k ') > x (k). According to the definition of argmax, the aforementioned equality if1 (..., L') max k 6 = z '(u') > (u ') > (k) \u2212 x (z') < 0, or, equivalent, (..., L ') maxk 6 = z' dingung (u ') > (k) max k (u') > (u ') > (k')."}, {"heading": "2.2 Proposed approach", "text": "We extend problem (5) by replacing the square '2-norm regularization' with a general function g-0 (R (M + 1) K. Furthermore, we rewrite the hinge loss in equivalent form by using for each 'x' -x (...) the linear operator T ': R (M + 1) K 7 (RK) 7 (RK) 1 x x x x x (M + 1) K' x (k) \u2212 x (z ')] 1 x (z') 1 x (K) -x (K). The vector r '(r (k) K) 7 (K)' 1 x x (K) k '."}, {"heading": "3 Optimization method", "text": "Solving problems (7) and (8) requires an efficient algorithm for dealing with non-smooth functions and hard constraints. In the convex optimization literature, proximal algorithms are one of the most efficient approaches for dealing with non-smooth problems [41, 42, 15, 43, 44]. The key tool in these methods is the proximity operator [45], defined as a kind of subgrading step for the function, since p = proxident (y) is clearly defined by the inclusion of y \u2212 p). Furthermore, it falls back on the projection of a closed convex set C-H in the case as p = proxident (y) by the inclusion of y \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 (p)."}, {"heading": "3.1 Regularized formulation", "text": "Problem (7) fits well within the framework of the FBPD algorithm, since the proximity operators can be efficiently calculated both g and (h ') 1 \u2264 \"\u2264 L. In fact, proxg has a closed form for several norms and mixed norms [43, 41], while (proxh') 1 \u2264\" L can be calculated by projecting onto the standard formula as described in Proposition 3.1. The projection onto the simplex can therefore be efficiently calculated using the method proposed by [49]. Proposition 3.1. For each \"solution {1,.,.,.,\" (Rocky ('), (\u2212 RK), example h \"(') = y (') \u2212 Problem (') + r ') + r') + r '), (10) withS\u03bb = (u = (u (k), projected)."}, {"heading": "3.2 Constrained formulation", "text": "Problem (8) presents a more difficult arithmetical question, since the projection on the hinge-loss constraint set cannot be evaluated in closed form, and it would require a limited square problem to solve each iteration. To cope with this constraint, we propose to introduce a vector of auxiliary variables (\"). (\") 1 \u2264 \"L in the minimization process, so that problem (8) h '(T' x) in asminimize (\" X \"). (\" X \"). R (M + 1).\" RL g (x). \"t L.\" (\").\" (\") \u2264\" L. \"(\" I \") h.\" (T'x) \u2264. \"(\" X \").\" (\"R\". \"(X\"). \"(.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (.). (. (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (.). (. (.).). (. (.). (.).). (. (.). (.). (.). (. (.). (.).). (. (.). (.). (.). (.). (. (.).). (.). (. (.).). (.). (. (.). (.). (.).). (.). (.). (. (.). (.). (. (.). (.). (.). (.). (. (.).). (.). (. (. (.).). (.).).). (. (.). (.). (.). (.).). (. (.).). (.).). (. (.).). (.).)."}, {"heading": "4 Numerical results", "text": "In this section, we numerically evaluate the performance of the sparse multiclass SVM w.r.t. The three following databases. \u2022 Leukemia database. The first experiment concerns the classification of microarray data. The database under consideration contains 72 samples of N = M = 7129 gene expression levels (so that \u03c6 (u) = u) of patients with K = 3 types of leukemia is measured [54]. The database is normally organized in L = 38 training samples and 34 test samples. 4 In our experiments, we used blocks of 5 genes for mixed standard regularization. \u2022 MNIST database concerns the classification of handwritten numbers. The second experiment concerns the classification of handwritten numbers. More precisely, we look at the MNIST database [55], which contains a number of 28 \u00d7 28 grayscale images (N = 784), the numbers from 0 to 9 (K = 10)."}, {"heading": "4.1 Assessment of classification accuracy", "text": "In this section we evaluate the classification errors we have obtained with the sparse multi-class SVM formulated in the problems (7) - (8). Our goal is to show that the exact loss of hinges enables us to achieve better performance than its nearly smooth versions, especially with a few training data. Therefore, we compare the proposed method with the following approaches: \u2022 the multi-class SVM proposed by [38] minimizes x-R (M + 1) K-g (x) + 0-K-6 = z '(max {0, \u00b5') > (x) \u2212 x (z ')} 2, (19) \u2022 multinomial logistic regression (e.g. see [11])))) minimizes x-R (M + 1) K-g (x) + 1-K-K (1)."}, {"heading": "4.2 Assessment of execution times", "text": "In this section, we compare the execution times of algorithms 1 and 2 with 7 \u2022 of a FISTA implementation of the problem (19), \u2022 of a forward-looking implementation of the problem (20), \u2022 of an FBPD implementation of the problem (12), which has been reformulated with linear constraints, minimize (x,..., L), (M + 1), (M + 1), (B), (B), (B), (K), (A), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K), (K, K, K, K, K, K, K, K, (K, K, K, K, K), (K, K, K, K, K, K, K, K (, K), (K, K), (K), (K), (K), (K), (K), (K), (K, (K), (K), (K), (K, (K), (K), (K), (K, (K), (K), (K, (K), (K), (K, (K), (K), (K, (K, K, K, K, K, K (, K), K (, K (, K), K (, K (, K), K (, K (, K), K (, K (), K (, K (, K (, K), K (, K, K (, K), K (, K (, K (, K), K (, K, K, K (, K, K (, K), K (, K (, K, K), K (, K (, K), K (, K (, K (, K), K, K (, K (, K), K (, K (, K, K (, K), K (, K (, K), K (, K), K (, K"}, {"heading": "4.3 Quadratic regularization", "text": "Although our focus is on sparse learning, we propose to complete our analysis by evaluating the efficiency of the proposed algorithms if g is a quadratic regulation function. To this end, we compare the execution times of algorithms 1 and 2 with the SVM structure algorithm proposed by [6], which provides a numerical approach to solving problem (4) by a cuttingplane technique. Figure 4 reports on the execution times (averaged to 10 training sets) achieved by the above methods in the MNIST database with L-3K, 5K, 10K, 50K, 100K, 500K} and different values of \u03b1. In this experiment, we set the stop criterion to 10 \u2212 3 in all methods and the regulation parameters of the SVM structure to L / \u03b1. The results show that the proposed algorithms are competitive with state-of-the-art solutions in scenarios with a limited number of training data, which cannot be claimed as larger for SVM / \u03b1."}, {"heading": "5 Conclusions", "text": "We have proposed two efficient algorithms for learning a sparse multi-class SVM. Our approach makes it possible to minimize a criterion that includes the loss of multi-class hinges and a sparse regularization. In the literature, such a criterion is typically approached by replacing the loss of hinges with a smooth penalty, such as the loss of hinges or the logistical loss. In this article, we have presented two solutions that deal directly with the loss of hinges: one that addresses the regulated formulation and the other that is adapted to the limited formulation. The performance of the proposed solutions was evaluated via three databases in scenarios with a few training data. Results show that the use of the hinge loss instead of an approximation leads to a slightly better classification accuracy and tends to make the method more robust compared to the choice of the regularization parameter, while the proposed algorithms are often faster than state-of-the-art solutions."}], "references": [{"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 2, pp. 265\u2013392, Jan. 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "A speech recognizer based on multiclass SVMs with HMM-guided segmentation", "author": ["D. Mart\u0301\u0131n-Iglesias", "J. Bernal-Chaves", "C. Pel\u00e1ez-Moreno", "A. Gallardo-Anto\u013a\u0131n", "F. D\u0131\u0301az-de Ma\u0155\u0131a"], "venue": "Nonlinear Analyses and Algorithms for Speech Processing, vol. 3817, pp. 257\u2013266, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 1453\u20131484, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Large-scale learning with SVM and convolutional for generic object categorization", "author": ["F.J. Huang", "Y. LeCun"], "venue": "Conference on Computer Vision and Pattern Recognition, New York, USA, 17-22 Jun. 2006, pp. 284\u2013291.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "Conference on Computer Vision and Pattern Recognition, Anchorage, AK, 23-28 June 2008, pp. 1\u20138.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning, vol. 77, no. 1, pp. 27\u201359, Oct. 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["M. Aizerman", "E. Braverman", "L. Rozonoer"], "venue": "Automation and Remote Control, vol. 25, pp. 821\u2013837, 1964.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1964}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods - Support Vector Learning, B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, Eds., pp. 185\u2013208. MIT Press, Cambridge, USA, Jan. 1998.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Large-scale multiclass support vector machine training via euclidean projection onto the simplex", "author": ["M. Blondel", "A. Fujino", "N. Ueda"], "venue": "International Conference on Pattern Recognition, Stockholm, Sweden, 24-28 August 2014, pp. 1289\u20131294.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse multinomial logistic regression: Fast algorithms and generalization bounds", "author": ["B. Krishnapuram", "L. Carin", "M.A.T. Figueiredo", "A.J. Hartemink"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 27, no. 6, June 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Boosting with structural sparsity", "author": ["J. Duchi", "Y. Singer"], "venue": "International Conference on Machine Learning, Montreal, Canada, 14-18 June 2009, pp. 297\u2013304.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "A comparison of optimization methods and software for large-scale L1-regularized linear classification", "author": ["G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "Machine Learning, vol. 11, pp. 3183\u20133234, Dec. 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "`p \u2212 `q penalty for sparse linear and sparse multiple kernel multi-task learning", "author": ["A. Rakotomamonjy", "R. Flamary", "G. Gasso", "S. Canu"], "venue": "IEEE Trans. on Neural Networks, vol. 22, no. 8, pp. 1307\u20131320, Aug. 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 1, pp. 1\u2013106, Jan. 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonparametric sparsity and regularization", "author": ["L. Rosasco", "S. Villa", "S. Mosci", "M. Santoro", "A. Verri"], "venue": "Journal of Machine Learning Research, vol. 14, pp. 1665\u20131714, July 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic feature learning for spatio-spectral image classification with sparse SVM", "author": ["D. Tuia", "M. Volpi", "M. Dalla Mura", "A. Rakotomamonjy", "R. Flamary"], "venue": "IEEE Trans. on Geoscience and Remote Sensing, vol. 52, no. 10, pp. 6062\u20136074, Oct. 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximal methods for the latent group lasso penalty", "author": ["S. Villa", "L. Rosasco", "S. Mosci", "A. Verri"], "venue": "Computational Optimization and Applications, vol. 58, no. 2, pp. 381\u2013407, Dec. 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A splitting algorithm for dual monotone inclusions involving cocoercive operators", "author": ["B.C. V\u0169"], "venue": "Advances in Computational Mathematics, vol. 38, no. 3, pp. 667\u2013681, Apr. 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms", "author": ["L. Condat"], "venue": "Journal of Optimization Theory and Applications, vol. 158, no. 2, pp. 460\u2013479, Aug. 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Epigraphical projection and proximal tools for solving constrained convex optimization problems", "author": ["G. Chierchia", "N. Pustelnik", "J.-C. Pesquet", "B. Pesquet-Popescu"], "venue": "Signal, Image and Video Processing, July 2014. 20", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Epigraphical proximal projection for sparse multiclass SVM", "author": ["G. Chierchia", "N. Pustelnik", "J.-C. Pesquet", "B. Pesquet-Popescu"], "venue": "International Conference on Acoustics, Speech and Signal Processing, Florence, Italy, 4-9 May 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection via concave minimization and support vector machines", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "International Conference on Machine Learning, Madison, USA, 1998, pp. 82\u201390.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Use of the zero-norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Sch\u00f6lkopf", "M. Tipping"], "venue": "Machine Learning, vol. 3, pp. 1439\u20131461, 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Support vector machines with adaptive Lq penalty", "author": ["Y. Liu", "H. Helen Zhang", "C. Park", "J. Ahn"], "venue": "Computational Statistics and Data Analysis, vol. 51, no. 12, pp. 6380\u20136394, Aug. 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "The f\u221e-norm support vector machine", "author": ["H. Zou", "M. Yuan"], "venue": "Statistica Sinica, vol. 18, pp. 379\u2013398, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Variable selection via a combination of the L0 and L1 penalties", "author": ["Y. Liu", "Y. Wu"], "venue": "Journal of Computational and Graphical Statistics, vol. 14, no. 4, pp. 782\u2013798, Dec. 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "The doubly regularized support vector machine", "author": ["L. Wang", "J. Zhu", "H. Zou"], "venue": "Statistica Sinica, vol. 16, pp. 589\u2013616, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "International Conference on Machine Learning, Haifa, Israel, 21-24 June 2010, pp. 1047\u20131054.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Non-convex regularizations for feature selection in ranking with sparse SVM", "author": ["L. Laporte", "R. Flamary", "S. Canu", "S. D\u00e9jean", "J. Mothe"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, vol. 25, no. 6, pp. 1118 \u2013 1130, June 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E.J. Cand\u00e9s", "M.B. Wakin", "S. Boyd"], "venue": "Journal of Fourier Analysis and Applications, vol. 14, no. 5, pp. 877\u2013905, Dec. 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "In defense of one-vs-all classification", "author": ["R. Rifkin", "A. Klautau"], "venue": "Machine Learning, vol. 5, pp. 101\u2013141, 2004.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "On l1-norm multi-class support vector machines: methodology and theory", "author": ["L. Wang", "X. Shen"], "venue": "Journal of the American Statistical Association, vol. 102, pp. 583\u2013594, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B, vol. 68, pp. 49\u201367, 2006.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "The group Lasso for logistic regression", "author": ["L. Meier", "S. Van De Geer", "P. B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society: Series B, vol. 70, no. 1, pp. 53\u201371, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Statistics and Computing, vol. 20, no. 2, pp. 231\u2013252, 2010. 21", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Variable selection for multicategory SVM via sup-norm regularization", "author": ["H.H. Zhang", "Y. Liu", "Y. Wu", "J. Zhu"], "venue": "Electronic Journal of Statistics, vol. 2, pp. 149\u2013167, 2008.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Block coordinate descent algorithms for large-scale sparse multiclass classification", "author": ["M. Blondel", "K. Seki", "K. Uehara"], "venue": "Machine Learning, vol. 93, no. 1, pp. 31\u201352, Oct. 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition", "author": ["T.M. Cover"], "venue": "IEEE Trans. on Electronic Computers, vol. EC-14, no. 3, pp. 326\u2013334, June 1965.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1965}, {"title": "Convex Optimization, Cambrige", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2004}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Fixed-Point Algorithms for Inverse Problems in Science and Engineering, H. H. Bauschke, R. S. Burachik, P. L. Combettes, V. Elser, D. R. Luke, and H. Wolkowicz, Eds., pp. 185\u2013212. Springer-Verlag, New York, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Primal-dual splitting algorithm for solving inclusions with mixtures of composite, Lipschitzian, and parallel-sum type monotone operators", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Set-Valued and Variational Analysis, vol. 20, no. 2, pp. 307\u2013330, June 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in Optimization, vol. 1, no. 3, pp. 123231, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Playing with duality: An overview of recent primal-dual approaches for solving large-scale optimization problems", "author": ["N. Komodakis", "J.-C. Pesquet"], "venue": "IEEE Signal Processing Magazine, 2014, accepted for publication.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Proximit\u00e9 et dualit\u00e9 dans un espace hilbertien", "author": ["J.J. Moreau"], "venue": "Bulletin de la Soci\u00e9t\u00e9 Math\u00e9matique de France, vol. 93, pp. 273\u2013299, 1965.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1965}, {"title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "author": ["A. Chambolle", "T. Pock"], "venue": "Journal of Mathematical Imaging and Vision, vol. 40, no. 1, May 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "A forward-backward view of some primal-dual optimization methods in image recovery", "author": ["P.L. Combettes", "L. Condat", "J.-C. Pesquet", "B.C. V\u0169"], "venue": "International Conference on Image Processing, Paris, France, 27-30 October 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Variable metric forward-backward splitting with applications to monotone inclusions in duality", "author": ["P.L. Combettes", "B.C. V\u0169"], "venue": "Optimization, vol. 63, no. 9, pp. 1289\u20131318, Sept. 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast projection onto the simplex and the l1 ball", "author": ["L. Condat"], "venue": "2014, Available online at http://hal.archives-ouvertes.fr/hal-01056171.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "Parallel implementations of a disparity estimation algorithm based on a proximal splitting method", "author": ["R. Gaetano", "G. Chierchia", "B. Pesquet-Popescu"], "venue": "Visual Communication and Image Processing, San Diego, USA, 27-30 November 2012, pp. 1\u20136.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1990}, {"title": "Probing the Pareto frontier for basis pursuit solutions", "author": ["E. Van Den Berg", "M.P. Friedlander"], "venue": "SIAM Journal on Scientific Computing, vol. 31, no. 2, pp. 890\u2013912, Nov. 2008.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring", "author": ["T.R. Golub", "D.K. Slonim", "P. Tamayo", "C. Huard", "M. Gaasenbeek", "J.P. Mesirov", "H. Coller", "M.L. Loh", "J.R. Downing", "M.A. Caligiuri", "C.D. Bloomfield", "E.S. Lander"], "venue": "Science, vol. 286, no. 5439, pp. 531\u2013537, 1999.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1999}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov. 1998.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1998}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1872\u20131886, Aug. 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1872}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "International Conference on Machine Learning, Tahoe City, USA, 9-12 July 1995, pp. 331\u2013339.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1995}, {"title": "Text categorization with suport vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "European Conference on Machine Learning, Chemnitz, Germany, 21-24 April 1998, pp. 137\u2013142. 23", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 1, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 2, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 3, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 4, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 5, "context": "As a matter of fact, many applications considered in the literature deal with a large amount of training data or a huge (even infinite) number of classes [2, 3, 4, 5, 6].", "startOffset": 154, "endOffset": 169}, {"referenceID": 0, "context": "The SVM learning problem is classically solved by using standard Lagrangian duality techniques [7, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 6, "context": "This approach brings in several advantages, such as the kernel trick [8], or the possibility to break the problem down into a sequence of smaller ones [9, 10].", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "This approach brings in several advantages, such as the kernel trick [8], or the possibility to break the problem down into a sequence of smaller ones [9, 10].", "startOffset": 151, "endOffset": 158}, {"referenceID": 8, "context": "This approach brings in several advantages, such as the kernel trick [8], or the possibility to break the problem down into a sequence of smaller ones [9, 10].", "startOffset": 151, "endOffset": 158}, {"referenceID": 2, "context": "Some works also proposed to approximate the dual problem using cutting plane approaches, in order to address scenarios with thousands or even an infinite number of classes [3, 6].", "startOffset": 172, "endOffset": 178}, {"referenceID": 5, "context": "Some works also proposed to approximate the dual problem using cutting plane approaches, in order to address scenarios with thousands or even an infinite number of classes [3, 6].", "startOffset": 172, "endOffset": 178}, {"referenceID": 9, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 10, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 11, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 12, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 13, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 14, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 15, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 16, "context": "In this respect, the `1-norm and, more generally, the `1,p-norm regularization have attracted much attention over the past decade [11, 12, 13, 14, 15, 16, 17, 18].", "startOffset": 130, "endOffset": 162}, {"referenceID": 17, "context": "To this end, we propose two algorithms based on a primal-dual proximal method [19, 20] and a novel epigraphical splitting technique [21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 18, "context": "To this end, we propose two algorithms based on a primal-dual proximal method [19, 20] and a novel epigraphical splitting technique [21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 19, "context": "To this end, we propose two algorithms based on a primal-dual proximal method [19, 20] and a novel epigraphical splitting technique [21].", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "In addition to more detailed theoretical developments, this paper extends our preliminary work [22] by providing a new algorithm, and a larger number of experiments including comparisons with state-of-the-art methods for different types of database.", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "The idea traces back to the work by [23], who demonstrated that the `1-norm regularization can effectively perform \u201cfeature selection\u201d by shrinking small coefficients to zero.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 162, "endOffset": 166}, {"referenceID": 26, "context": "Other forms of regularization have also been studied, such as the `0-norm [24], the `p-norm with p > 0 [25], the `\u221e-norm [26], and the combination of `0-`1 norms [27] or `1-`2 norms [28].", "startOffset": 182, "endOffset": 186}, {"referenceID": 27, "context": "A different solution was proposed by [29], who reformulated the SVM learning problem by using an indicator vector (its components being either equal to 0 or 1) to model the active features, and solved the resulting combinatorial problem by convex relaxation using a cutting-plane algorithm.", "startOffset": 37, "endOffset": 41}, {"referenceID": 28, "context": "More recently, [30] proposed an accelerated algorithm for `1-regularized SVMs involving the square hinge loss.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "They also proposed a procedure for handling nonconvex regularization (using the reweighted `1-minimization scheme by [31]), showing that nonconvex penalties lead to similar prediction quality while using less features than convex ones.", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "Binary SVMs can be turned into multiclass classifiers by a variety of strategies, such as the one-vs-all approach [7, 32].", "startOffset": 114, "endOffset": 121}, {"referenceID": 0, "context": "[1] therefore proposed a direct formulation of multiclass SVMs by generalizing the notion of margins used in the binary case.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "A simple example is the `1-regularized multiclass SVM, which can be addressed by linear programming techniques [33].", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 33, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 10, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 34, "context": "to impose group sparsity [34, 35, 12, 36].", "startOffset": 25, "endOffset": 41}, {"referenceID": 35, "context": "In the context of multiclass SVMs, [37] proposed to deal with the `1,\u221e-norm regularization by reformulating the SVM learning problem in terms of linear programming.", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "More recently, [38] proposed an algorithm to handle `1,2-regularized SVMs involving a smooth loss function.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 148, "endOffset": 156}, {"referenceID": 36, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 148, "endOffset": 156}, {"referenceID": 31, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 191, "endOffset": 199}, {"referenceID": 35, "context": "The algorithmic solutions proposed in the literature to deal with sparse multiclass SVMs are either cutting-plane methods [29], proximal algorithms [30, 38], or linear programming techniques [33, 37].", "startOffset": 191, "endOffset": 199}, {"referenceID": 19, "context": "In this paper, we propose a novel approach based on proximal tools and recent epigraphical splitting techniques [21], which allow us to exactly solve the sparse multiclass SVM learning problem through an efficient primal-dual proximal method [19, 20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 17, "context": "In this paper, we propose a novel approach based on proximal tools and recent epigraphical splitting techniques [21], which allow us to exactly solve the sparse multiclass SVM learning problem through an efficient primal-dual proximal method [19, 20].", "startOffset": 242, "endOffset": 250}, {"referenceID": 18, "context": "In this paper, we propose a novel approach based on proximal tools and recent epigraphical splitting techniques [21], which allow us to exactly solve the sparse multiclass SVM learning problem through an efficient primal-dual proximal method [19, 20].", "startOffset": 242, "endOffset": 250}, {"referenceID": 36, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 274, "endOffset": 278}, {"referenceID": 28, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 280, "endOffset": 284}, {"referenceID": 35, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 286, "endOffset": 290}, {"referenceID": 9, "context": "In Section 2, we formulate the multiclass SVM problem with sparse regularization, in Section 3 we provide the proximal tools needed to solve the proposed problem, and in Section 4 we evaluate our approach on three standard datasets and compare it to the methods proposed by [38], [30], [37], and [11].", "startOffset": 296, "endOffset": 300}, {"referenceID": 37, "context": ", L} } , and they are assumed to be linear in some feature representation of inputs [39].", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "The multiclass SVM learning problem is thus obtained by adding a quadratic regularization [1], yielding2", "startOffset": 90, "endOffset": 93}, {"referenceID": 38, "context": "By using standard convex analysis [40], the above problem can be equivalently rewritten without slack variables as", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "A popular example is the `1-norm, as it ensures that the solution will have a number of coefficients exactly equal to zero, depending on the strength of the regularization [15].", "startOffset": 172, "endOffset": 176}, {"referenceID": 39, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 40, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 13, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 41, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 42, "context": "one of the most efficient approaches to deal with nonsmooth problems [41, 42, 15, 43, 44].", "startOffset": 69, "endOffset": 89}, {"referenceID": 43, "context": "The key tool in these methods is the proximity operator [45], defined for a function \u03c8 \u2208 \u03930(H) as (\u2200u \u2208 H) prox\u03c8(u) = argmin v\u2208H 1 2 \u2016v \u2212 u\u2016 + \u03c8(v).", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 18, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 44, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 45, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 46, "context": "In the next sections, we will present two different approaches based on a Forward-Backward based Primal-Dual method (FBPD) [19, 20, 46, 47, 48], which we have selected among the large panel of proximal algorithms for its simplicity to deal with large-size linear operators.", "startOffset": 123, "endOffset": 143}, {"referenceID": 41, "context": "Indeed, proxg has a closed form for several norms and mixed norms [43, 41], while (proxh`)1\u2264`\u2264L can be computed through the projection onto the standard simplex, as described in Proposition 3.", "startOffset": 66, "endOffset": 74}, {"referenceID": 39, "context": "Indeed, proxg has a closed form for several norms and mixed norms [43, 41], while (proxh`)1\u2264`\u2264L can be computed through the projection onto the standard simplex, as described in Proposition 3.", "startOffset": 66, "endOffset": 74}, {"referenceID": 47, "context": "The projection onto the simplex can be efficiently computed with the method proposed by [49].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "The iterations associated with Problem (7) are summarized in Algorithm 1, where the sequence (x)i\u2208N is guaranteed to converge to a solution to Problem (7), provided that such a solution exists [19, 20].", "startOffset": 193, "endOffset": 201}, {"referenceID": 18, "context": "The iterations associated with Problem (7) are summarized in Algorithm 1, where the sequence (x)i\u2208N is guaranteed to converge to a solution to Problem (7), provided that such a solution exists [19, 20].", "startOffset": 193, "endOffset": 201}, {"referenceID": 0, "context": "In the case when g = (1/2)\u2016\u00b7\u20162, the primal and dual solutions are linked by x = \u2212T>y, and thus Problem (11) reduces to the (Lagrangian) dual formulation of Problem (4) used in standard SVMs [1].", "startOffset": 190, "endOffset": 193}, {"referenceID": 17, "context": "The iterations related to Problem (13) are listed in Algorithm 2, where the sequence (x[i], \u03b6 )i\u2208N is guaranteed to converge to a solution to (13), provided that such a solution exists [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 18, "context": "The iterations related to Problem (13) are listed in Algorithm 2, where the sequence (x[i], \u03b6 )i\u2208N is guaranteed to converge to a solution to (13), provided that such a solution exists [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 49, "context": "Secondly, these projections can be computed in parallel, since they are defined over disjoint blocks whose number is given by the cardinality L of the training set (we refer to [51] for an example of parallel implementation on GP-GPUs).", "startOffset": 177, "endOffset": 181}, {"referenceID": 50, "context": "Note that the expensive sorting operation can be avoided by using a heap data structure [52], which keeps a partially-sorted sequence such that the first element is the largest.", "startOffset": 88, "endOffset": 92}, {"referenceID": 52, "context": "The considered database contains 72 samples of N = M = 7129 gene expression levels (so that \u03c6(u) = u) measured from patients having K = 3 types of leukemia disease [54].", "startOffset": 164, "endOffset": 168}, {"referenceID": 53, "context": "More precisely, we consider the MNIST database [55], which contains a number of 28\u00d7 28 grayscale images (N = 784) displaying digits from 0 to 9 (K = 10).", "startOffset": 47, "endOffset": 51}, {"referenceID": 54, "context": "5 In our experiments, we defined the mapping \u03c6 by resorting to the scattering convolution network [56] with m = 2 wavelet layers scaled up to 2J = 4, which transforms an input image of size 28\u00d7 28 in 81 images of size 14\u00d7 14 (thus M = 15876).", "startOffset": 98, "endOffset": 102}, {"referenceID": 55, "context": "More precisely, we consider the News20 database [57], which contains a number of documents partitioned across K = 20 different newsgroups.", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "6 In our experiments, we defined the mapping \u03c6 by resorting to the term frequency \u2013 inverse document frequency transformation [58], yielding M = 26214.", "startOffset": 126, "endOffset": 130}, {"referenceID": 36, "context": "For the regularization, we used `1,2-norm in the same way as [38].", "startOffset": 61, "endOffset": 65}, {"referenceID": 36, "context": "\u2022 the multiclass SVM proposed by [38]", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": ", see [11])", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "\u2022 the binary SVM by [30] based on the \u201cone-vs-all\u201d strategy, which aims, for every k \u2208 {1, .", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "This approach is conceptually similar to the linear programming methods proposed by [33] and [37] for `1- or `1,+\u221e-regularized SVMs.", "startOffset": 84, "endOffset": 88}, {"referenceID": 35, "context": "This approach is conceptually similar to the linear programming methods proposed by [33] and [37] for `1- or `1,+\u221e-regularized SVMs.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "To this end, we compare the execution times of Algorithms 1 and 2 with the SVM-struct algorithm proposed by [6], which provides a numerical approach for solving Problem (4) through a cuttingplane technique.", "startOffset": 108, "endOffset": 111}], "year": 2015, "abstractText": "Sparsity-inducing penalties are useful tools to design multiclass support vector machines (SVMs). In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by [1]. We provide two algorithms: the first one dealing with the hinge loss as a penalty term, and the other one addressing the case when the hinge loss is enforced through a constraint. The related convex optimization problems can be efficiently solved thanks to the flexibility offered by recent primal-dual proximal algorithms and epigraphical splitting techniques. Experiments carried out on several datasets demonstrate the interest of considering the exact expression of the hinge loss rather than a smooth approximation. The efficiency of the proposed algorithms w.r.t. several state-of-the-art methods is also assessed through comparisons of execution times.", "creator": "LaTeX with hyperref package"}}}