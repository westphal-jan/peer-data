{"id": "1501.03784", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2015", "title": "Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing", "abstract": "This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.", "histories": [["v1", "Thu, 15 Jan 2015 19:25:32 GMT  (606kb,D)", "http://arxiv.org/abs/1501.03784v1", "9 pages, 13 figures"]], "COMMENTS": "9 pages, 13 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["denis kleyko", "evgeny osipov", "alexander senior", "asad i khan", "y ahmet \\c{s}ekercio\\u{g}lu"], "accepted": false, "id": "1501.03784"}, "pdf": {"name": "1501.03784.pdf", "metadata": {"source": "CRF", "title": "Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing", "authors": ["Denis Kleyko", "Evgeny Osipov", "Alexander Senior", "Asad I. Khan", "Ahmet \u015eekercio\u011flu"], "emails": ["Evgeny.Osipov@ltu.se;", "Denis.Kleyko@ltu.se).", "Asad.Khan@monash.edu).", "Alexander.Senior@monash.edu;", "met.Sekercioglu@monash.edu)."], "sections": [{"heading": null, "text": "Index Terms - Holographic Graph Neuron, Pattern recognition, Vector Hololic Architecture, Holoash Symbolic Architecture, Holographic Memory, hyperdimensional computing.I. INTRODUCTIONGRAPH Neuron (GN) is an approach for storing patterns of generic sensor stimuli for later template matching. It is based on the hypothesis that a better associative memory resource can be created by shifting the focus from high-speed sequential CPU processing to parallel network-centric processing [2], [3]. Unlike modern machine learning approaches, GN enables the introduction of new patterns in the learning group without the need for retraining. It exhibits a high degree of scalability i.e., its performance and accuracy do not degrade as the number of stored patterns increases over time.Vector Symbolic Architectures (VSA) [4] are a bio-inspired method for cognitive reasoning and coding."}, {"heading": "II. RELATED WORK", "text": "The concept of AM was originally developed in an effort to harness the power and speed of existing computer systems to solve large-scale and computationally intensive problems by simulating biological neurosystems. Hierarchical Graph Neuron's (HGN) approach is a kind of associative reminder that characterizes the hierarchical structure in its implementation. Hierarchical structures in associative storage models are of interest because they have shown that they improve the rate of recall in pattern recognition applications. HGN's distributed scheme also enables better control of network resources. This scheme also compares with contemporary approaches such as Self-Organizing Map and Support Vector Machine in terms of speed and accuracy."}, {"heading": "III. OVERVIEW OF ESSENTIAL PARTS OF RELEVANT CONCEPTS AND THEORIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Hierarchical Graph Neuron", "text": "Figure 1 illustrates the hierarchical graph-neuron approach. Consider only the bottom layer of the construction without the hierarchy of the top nodes; this bottom layer is the original flat network of graph neurons [2]. Each GN is a model for a series of general sensory values. Considered as a network, graph neurons can be modelled by an array in which columns are individual GNs and rows are possible symbols that a neuron can recognize. For example, if there are only two possible symbols, say \"X\" and \"Y,\" in the alphabet of a pattern, then only two lines are needed to represent these symbols. The number of columns2 in the GN array determines the size of the patterns it can analyze. An input pattern is defined as stimulus generated within the network. In Figure 1, each GN can analyze a symbol (\"X\" or \"Y) of a pattern that consists of five elements."}, {"heading": "B. Fundamentals of Vector Symbolic Architecture and Binary Spatter Codes", "text": "Vector Symbolic Architecture is an approach to coding and operations on distributed representation of information. VSA has been used so far mainly in the field of cognitive computation for the representation and reasoning of semantically bound information [4], [15].The fundamental difference between distributed and localized representations of data is as follows: in traditional (localistic) computing architectures, each bit and its position within a structure of bits is significant (for example, a field in a database has a predefined offset among other fields and a symbolic value in ASCII codes), whereas in a distributed representation all entities are represented by binary random vectors of very high dimensions. These representations are also referred to as Binary Spatter Codes (BSC), although for the remainders of the article we use the term HD vector when referring to BSC codes, for reasons of brevity."}, {"heading": "IV. HOLOGRAPHIC GRAPH NEURON", "text": "In this section, one of the most important contributions of this paper is presented: the introduction of the VSA data representation for the implementation of the HGN approach."}, {"heading": "A. Motivation for Holographic Graph Neuron and outline of the solution", "text": "An important issue in hierarchical models is the overhead of resource requirements, especially in terms of the number of processing elements required. We propose a holographic approach that allows a flat GN array to work with greater accuracy and comparable retrieval time than HGN, without the need for complex topology and additional nodes. Figure 2 illustrates the overall logic of the proposed solution. Essentially, we replace the need to maintain topological relationships by condensing parts of the pattern observed by GNs into a holographic representation."}, {"heading": "B. Encoding", "text": "In the case of HoloGN, all its elements (i.e. symbols of individual neurons) are uniquely indexed and the index of a particular element is derived depending on the GN ID. Let IVj be a high-dimensional initialization vector for GN j. The initialized vectors for different GN are selected so that they are orthogonal to each other. Then, the HD index of the element i in GN j is calculated as EHD (j, i) = Sh (IVj, i), where Sh () is a cyclic shifting operation that leads to the generation of a vector orthogonal to IVj HD vector [17], [18].4"}, {"heading": "C. Construction of VSA-representation of activated GNs", "text": "Consider the number of individual graph neurons. When a GN array is exposed to a pattern, the activated elements communicate their HD-represented indices to all other GNs; the holographic representation of the activated elements is then: HGN = [n \u2211 j = 1 (EHDj)], (3) where EHDj is the HD index of the activated element in TextGN j, and the addition is the bundling or threshold sum as described in III-B. As described above, the hamming distance between the individual components and the composition vector in the resulting HD vector is strictly less than 0.5. We will use this property later when constructing data structures for retrieving patterns in HoloGN."}, {"heading": "D. Data structures for storing and retrieving holographic representations in HGN elements", "text": "HoloGN stores the holographic representation of the entire pattern (3) observed across all GNs. One possible architecture is that all stored patterns are collected and stored centrally at a processing node. Depending on the application of HoloGN, the stored patterns can be stored either in an unsorted list or in bundles. The first mode of storing HoloGN patterns corresponds to the case in which the structure of the observed patterns is unknown; the second mode is used in the case of supervised learning. In the next section, different use and recall strategies of HoloGN are described."}, {"heading": "V. HOLOGN RECALL STRATEGIES", "text": "In this section, two important recall modes are presented and evaluated: the one-shot case and the case of supervised learning."}, {"heading": "A. Time-efficient \u03be-accurate recall in an unsorted HoloGN storage", "text": "The usual steps in both call modes are the procedure for time-efficient searching over an (unsorted) list of HoloGN datasets. Remember that all manipulations with VSA encoded entities are performed with simple bit-wise arithmetic operations, and calculations to obtain the hamming distance between entities. However, for this article we assume that no specific optimized implementation of VSA's bit-wise operations is performed; this is because such operations are tailored to the architectures of specific microprocessors that operate with words of much smaller dimensions (typically 32 or 64 bits). Therefore, adopting these methods for implementing bit-wise operations on words of thousands of bits is cumbersome. Instead, an easily analyzable computational model is adopted in this article, which could also be adapted to an implementation on specialized computer-based architectures."}, {"heading": "B. Case-study 1: Best-match probing under one-shot learning", "text": "The first use of HoloGN indicates the existence of the target query pattern among the previously stored HoloGN patterns. In this case, the perfect match would be indicated by a hamming distance of zero. Therefore, the zero deviation reflects the degree of proximity of the query to one or more stored HoloGN patterns. In the following example, the accuracy of the HoloGN callback was compared with the performance of the original HGN approach. To allow a fair comparison, the 7 by 5 pixel letters of the Roman alphabet (as in [2]) were used. In the memorization phase, a series of noise-free images of letters depicted in Figure 4 were presented to both architectures. In the retrieval phase, images of the same letters were distored with varying accuracy (between 1 bit, corresponding to a distortion of 2.9% of the size of the pattern, and 5 bits corresponding to 14.3% of the distortion)."}, {"heading": "C. Case-study 2: HoloGN recall under the supervised learning", "text": "The analysis presented above is a very positive result for the proposed bio-inspired associative memory pattern processing architecture, since the accuracy of the original HGN approach has proven to be as accurate as artificial neural networks with back propagation [2]. While formal relationships to the framework of artificial neural networks were established outside the scope of this work, in this section the results of the pattern recognition accuracy of the HoloGN architecture are presented within the framework of supervised learning. In this case, the HoloGN is presented with a series of randomly distorted patterns for each letter with varying degree of distortion (between 3% and 43%), as shown in Figure 5. In the experiments, up to 500 patterns for each letter and each degree of distortion were presented for learning to use. For the respective degree of distortion, i all e presented patterns of the respective letter Li were bundled into a single HoloGN representation (L) = [e \u00b2 (Gi = 1) Li (HN)."}, {"heading": "VI. PATTERN DECODING AND SUBPATTERN-BASED ANALYSIS", "text": "There is a class of pattern recognition applications that require an understanding of the details of the retrieval results. For example, if a retrieval provides several possible patterns of given memory accuracy, the task would be to understand the overlapping elements. This section looks at two aspects of this task: a robust decoding of elementary components from a distributed VSA representation; and a quantitative measurement of similarity by directly comparing distributed representations without having to decrypt these representations. The VSA approach of representing data structures by definition makes decrypting the individual components a tedious task, requiring a brutal force test of the inclusion of possible high-dimensional code words for each GN. The majority sum used to create HoloGN representations of the observed patterns sets a limit on the number of high-dimensional code word operations above which robust decoding of individual operations is impossible."}, {"heading": "A. Preliminaries", "text": "The density of a randomly generated HD vector (i.e. the number of ones in an HD vector) is referred to as k. The probability of selecting a random vector of length d with the density k, describing the probability of the appearance of 1, defined as p, by (2). The mean density of a random vector is equal to d \u00b7 p. Note that the density of randomly generated HD vectors is actually evidently different from the mean. However, after (1) the density k approaches the mean with the increase in dimensionality. In other words, the probability of creating an HD vector with k > d \u00b7 p or k < < d \u00b7 p decreases with the increase in dimensionality. Define thr as the threshold probability of generating a vector with a certain deviation from dimensionality. (lt.) Let k \u2212 and k + characterize the lower edge and the interval."}, {"heading": "B. Capacity of HoloGN representations", "text": "Suppose there is an item memory [4] that contains HDvectors representing atomic concepts3. Remember that if multiple HD vectors are bundled by the majority sum, the noise of the inverted bits increases with the number of components. At a given dimensionality d, there is a limit to the number of bundled HD vectors beyond which the resulting HD vector becomes orthogonal to each component vector; therefore, the A capacity of the resulting vector is defined as the maximum number of mutually orthogonal HD vectors that can be robustly decoded from their majority sum composition by decoding the item memory memory.To characterize the capacity of the composition vector for a given dimensionality, one must decode the level of HD vectors robustly introduced by bundling by its majority vectors d, where the number of atomic actors can be decoded by (8)."}, {"heading": "C. Calculation of the number of common component vectors in two resulting vectors", "text": "Given the rather conservative limits of the number of the most robust decodable elements in a distributed representation, it is important that the proposed HoloGN architecture be able to estimate the similarity between different patterns without decoding them. This subsection represents a method for quantitatively measuring the number of overlapping elements in these patterns, considering the number of overlapping elements as a function of their relative hamming distance. (Denote m and n as lengths of two patterns, m < n, and denote c as the number of common elements in these patterns. Let M be a c \u00b7 d matrix of common elements in which each row contains a random HDvector of dimension d encoding element.) Denote an arbitrary column of the matrix M as C. Since the rows in M are independent, the density of the inserts in each column also follows the binomial distribution with p = 0.5 and the length c. Denote the number of inserts in the column as C | 1.to calculate the distance between the two known patterns, if the instances in each column are known as the Hamming representations."}, {"heading": "VII. CONCLUSION", "text": "In this article, Holographic Graph Neuron was introduced - a novel approach for storing patterns of generic sensor stimuli. HoloGN builds on the previous Graph Neuron algorithm and uses a vector symbolic representation to encode the states of the Graph Neuron. Adopting the vector symbolic representation ensures a single-layer design for the approach, which implies the computational simplicity of the operations. The presented approach has the number of 9 unique properties. First of all, it allows a linear (in terms of the number of entries stored) time scan for any sub-pattern. While maintaining the previously reported properties of the hierarchical Graph Neuron, HoloGN also improves the noise resistance of the architecture by significantly improving the accuracy of pattern memory."}], "references": [{"title": "Holographic graph neuron", "author": ["E. Osipov", "A.I. Khan", "A. Anang"], "venue": "In Computer and Information Sciences (ICCOINS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A hierarchical graph neuron scheme for real-time pattern recognition", "author": ["B.B. Nasution", "A.I. Khan"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "One-shot associative memory method for distorted pattern recognition", "author": ["A.I. Khan", "A.H. Muhamad Amin"], "venue": "In AI 2007: Advances in Artificial Intelligence, 20th Australian Joint Conference on Artificial Intelligence, Gold Coast, Australia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors", "author": ["P. Kanerva"], "venue": "Cognitive Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Vector symbolic architectures: A new building material for artificial general intelligence", "author": ["S.D. Levy", "R. Gayler"], "venue": "In Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Holographic reduced representations", "author": ["T. Plate"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Representing objects, relations, and sequences", "author": ["S.I. Gallant", "T.W. Okaywe"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fly like a fly", "author": ["R. Zbikowski"], "venue": "IEEE Spectrum,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Digital cameras with designs inspired by the arthropod", "author": ["Y.M. Song", "Y. Xie", "V. Malyarchuk", "J. Xiao", "I. Jung", "K-J. Choi", "Z. Liu", "H. Park", "C. Lu", "R-H. Kim", "R. Li", "K.B. Crozier", "Y. Huang", "J.A. Rogers"], "venue": "eye. Nature,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Content Addressable Memory", "author": ["K.J. Schultz"], "venue": "N. T. Limited,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "A family of binary spatter codes", "author": ["P. Kanerva"], "venue": "In ICANN \u201995: International Conference on Artificial Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Learning behavior hierarchies via high-dimensional sensor projection", "author": ["S. Bajracharya S. Levy", "R. Gayler"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Dependable mac layer architecture based on holographic data representation using hyperdimensional binary spatter codes", "author": ["D. Kleyko", "N. Lyamin", "E. Osipov", "L. Riliskis"], "venue": "In Multiple Access Communications : 5th International Workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Collective communication for dense sensing environments", "author": ["P. Jakimovski", "H.R. Schmidtke", "S. Sigg", "L. Weiss-Ferreira-Chaves", "M. Beigl"], "venue": "Journal of Ambient Intelligence and Smart Environments (JAISE),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Distributed Representations and Nested Compositional Structure", "author": ["T. Plate"], "venue": "University of Toronto, PhD Thesis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Sparse distributed memory", "author": ["P. Kanerva"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1988}, {"title": "Generalized chirp-like polyphase sequences with optimum correlation properties", "author": ["B.M. Popovic"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "On bidirectional transitions between localist and distributed representations: The case of common substrings search using vector symbolic architecture", "author": ["D. Kleyko", "E. Osipov"], "venue": "Procedia Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Fully distributed representation", "author": ["P. Kanerva"], "venue": "In Real world computing symposium,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "It is based on the hypothesis that a better associative memory resource can be created by changing the emphasis from high speed sequential CPU processing to parallel network-centric processing [2], [3].", "startOffset": 193, "endOffset": 196}, {"referenceID": 2, "context": "It is based on the hypothesis that a better associative memory resource can be created by changing the emphasis from high speed sequential CPU processing to parallel network-centric processing [2], [3].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "Vector Symbolic Architectures (VSA) [4] are a bio-inspired method of representing concepts and their meaning for modeling cognitive reasoning.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation.", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation.", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "This work was initially presented at IEEE International Conference on Computer and Information Sciences, ICCOINS 2014 [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "These compound eyes consist of large number of sensors with limited and localized processing capabilities for performing relatively complex sensing tasks [9].", "startOffset": 154, "endOffset": 157}, {"referenceID": 9, "context": "Associative memory (AM) is a sub-domain of artificial neural networks, which utilises the benefits of content-addressable memory (CAM) [10] in microcomputers.", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": "The Hierarchical Graph Neuron (HGN) approach [2] is a type of associative memory which signifies the hierarchical structure in its implementation.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "1In [8], a house fly\u2019s properties are compared and contrasted with an advanced fighter plane as follows: \u201cWhereas the F-35 Joint Strike Fighter, the most advanced fighter plane in the world, takes a few measurements - airspeed, rate of climb, rotations, and so on and then plugs them into complex equations, which it must solve in real time, the fly relies on many measurements from a variety of sensors but does relatively little computation.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "Vector Symbolic Architectures [5] are a class of connectionist models that use hyper-dimensional vectors (i.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Distributed representations of data structures are an approach actively used in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [11].", "startOffset": 179, "endOffset": 182}, {"referenceID": 10, "context": "Distributed representations of data structures are an approach actively used in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [11].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "In [12] a VSA-based knowledge-representation architecture is proposed for learning arbitrarily complex, hierarchical, symbolic relationships (patterns) between sensors and actuators in robotics.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Recently the theory of hyper-dimensional computing, and VSA in particular, were adopted for implementing novel communication protocols and architectures for collective communications in machine-to-machine communication scenarios [13], [14].", "startOffset": 229, "endOffset": 233}, {"referenceID": 13, "context": "Recently the theory of hyper-dimensional computing, and VSA in particular, were adopted for implementing novel communication protocols and architectures for collective communications in machine-to-machine communication scenarios [13], [14].", "startOffset": 235, "endOffset": 239}, {"referenceID": 12, "context": "This article presents an algorithmic ground for further design of the distributed HoloGN on top of the architecture presented in [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 1, "context": "Consider only the bottom layer of the construction without the hierarchy of upper nodes; this bottom level is the original flat network of Graph Neurons [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "The accuracy of HGN was demonstrated to be comparable to the accuracy of Neural Network with back-propagation [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "VSA has previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [15].", "startOffset": 138, "endOffset": 141}, {"referenceID": 14, "context": "VSA has previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 3, "context": "High dimensionality refers to that fact that in HDvectors, several thousand positions (of binary numbers) are used for representing a single entity; [4] proposes the use of vectors of 10000 binary elements.", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": "Interested readers are referred to [4] and [16] for comprehensive analysis of probabilistic properties of the hyperdimensional representation space.", "startOffset": 35, "endOffset": 38}, {"referenceID": 15, "context": "Interested readers are referred to [4] and [16] for comprehensive analysis of probabilistic properties of the hyperdimensional representation space.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "3) Generation of HD-vectors: Random binary vectors with the above properties can be generated with Zadoff-Chu sequences [17], a method widely used in telecommunications to generate pseudo-orthogonal preambles.", "startOffset": 120, "endOffset": 124}, {"referenceID": 3, "context": "Note that the cyclic shift is a special case of the permutation operation [4].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": ", binding, permutation [4].", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "Then the HD-index of element i in GN j is computed as E (j,i) = Sh(IVj , i), where Sh() is a cyclic shift operation resulting in the generation of a vector orthogonal to IVj HD-vector [17], [18].", "startOffset": 184, "endOffset": 188}, {"referenceID": 17, "context": "Then the HD-index of element i in GN j is computed as E (j,i) = Sh(IVj , i), where Sh() is a cyclic shift operation resulting in the generation of a vector orthogonal to IVj HD-vector [17], [18].", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "For the sake of fair comparison the 7 by 5 pixels letters of the Roman alphabet (as in [2]) were used.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "The analysis presented above is a very positive result for the proposed bio-inspired associative memory based pattern processing architecture, since the accuracy of the original HGN approach was demonstrated to be as accurate as artificial neural networks with back-propagation [2].", "startOffset": 278, "endOffset": 281}, {"referenceID": 3, "context": "Suppose there exists an item memory [4] containing HDvectors representing atomic concepts3.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "This is calculated as in [19] by (8), where n is a number of atomic vectors in the resulting majority sum vector.", "startOffset": 25, "endOffset": 29}], "year": 2015, "abstractText": "This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.", "creator": "LaTeX with hyperref package"}}}