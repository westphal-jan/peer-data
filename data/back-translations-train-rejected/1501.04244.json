{"id": "1501.04244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2015", "title": "Generalised Random Forest Space Overview", "abstract": "Assuming a view of the Random Forest as a special case of a nested ensemble of interchangeable modules, we construct a generalisation space allowing one to easily develop novel methods based on this algorithm. We discuss the role and required properties of modules at each level, especially in context of some already proposed RF generalisations.", "histories": [["v1", "Sat, 17 Jan 2015 23:42:08 GMT  (20kb,D)", "http://arxiv.org/abs/1501.04244v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["miron b kursa"], "accepted": false, "id": "1501.04244"}, "pdf": {"name": "1501.04244.pdf", "metadata": {"source": "CRF", "title": "Generalised Random Forest Space Overview", "authors": ["Miron B. Kursa", "M.B. Kursa"], "emails": ["M.Kursa@icm.edu.pl"], "sections": [{"heading": "1 Introduction", "text": "Random Forest (RF) is a popular, powerful machine learning method proposed by Breiman [2001]. Although the canonical version of this algorithm is known to be very versatile and works well in numerous applications, many variants of this method have been proposed for many different purposes: to expand RF capabilities, to generalize specific, non-standard data, to increase accuracy under certain conditions, to improve the measurement of the meaning of attributes generated by this method, to speed up training or prediction, to name but a few. This work aims to provide a conceptual framework of general Random Forest (GRF) methods that are useful both in classifying existing RF variants and in defining research opportunities in this area."}, {"heading": "2 Generalised Random Forest", "text": "In the presented model of the generalized random forest, we assume a three-layered nested ensemble structure, as shown in Figure 1. Data is fed into the model via numerous pivot models, which must correspond to the shape of the data and be very simple and easy to train.Pivot models do not need to be precise or robust; their intended role is to become an interface between the input and internal logic of the GRF model.In other words, we assume that they handle the construction step of the modeling process.Pivot models are transformed into a meaningful ensemble model with the sharpening ensembles; these modules of this class orchestrate the generation of pivot points by utilizing the information from the decision attribute. The sharpening ensemble is expected to produce exact models regardless of their rar Xiv: 150 1.04 244v 1 [cs.L Jan 17 G], however, they should not require any sharpness of the ensemble; however, they should not require any kind of external sharpness."}, {"heading": "3 Pivot models", "text": "In fact, it is so that most of us are able to obey the rules that they have imposed on themselves. (...) In fact, it is so that they are able to obey the rules. (...) In fact, it is so that they do not. (...) It is not so that they do. (...) It is not so that they do. (...) It is so. (...) It is so. (...) \"(...). (...)\" (...) \"(...)\" (...) \"(...)\" (...) \"((...)\" ((...) \"((...)\" (...) \"(...)\" () \"(...\") ((...) \"()\" (... \") (...\" () () (\"() (().)\" (... \"() () (\") (() () ()."}, {"heading": "5 Conditioning ensemble", "text": "As already mentioned, the role of the conditioning ensemble is to justify a robust prediction from a set of sharpening ensembles of unknown reliability, to which end this module must either somehow evaluate the member models or ensure the independence of the members, so that the noise generated by the overmatched ensembles is average during voting; the first approach is rarely used so far, mainly because a reliable method of assessing robustness in practice would mean that the use of the entire GRF structure is superfluous; the second approach is usually, as in the canonical Random Forest algorithm, implemented by bootstrap aggregation (bagging), as also proposed by Breiman [1996], or by variation of this method. Strictly speaking, the method generates a number of object subsamples for each of the sharpening ensembles; in this way they are presented with differently stressed incarnations of training data, thus providing a wider range of less correlated aspects."}, {"heading": "6 Conclusions", "text": "Rethinking Random Forest as a three-tiered nested ensemble, we propose a generic, modular framework for extending and modifying this method. Acknowledgements. This work was funded by the National Science Centre, grant 2011 / 01 / N / ST6 / 07035."}], "references": [{"title": "Image classification using random forests and ferns", "author": ["Anna Bosch", "Andrew Zisserman", "Xavier Munoz"], "venue": "IEEE 11th International Conference on Computer Vision, pages 1\u20138", "citeRegEx": "Bosch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bosch et al\\.", "year": 2007}, {"title": "Signal classification using random forest with kernels", "author": ["Jiguo Cao", "Guangzhe Fan"], "venue": "Sixth Advanced International Conference on Telecommunications,", "citeRegEx": "Cao and Fan.,? \\Q2010\\E", "shortCiteRegEx": "Cao and Fan.", "year": 2010}, {"title": "Kernel-induced classification trees and random forests", "author": ["Guangzhe Fan"], "venue": null, "citeRegEx": "Fan.,? \\Q2009\\E", "shortCiteRegEx": "Fan.", "year": 2009}, {"title": "Functional data classification for temporal gene expression data with kernel-induced random forests", "author": ["Guangzhe Fan", "Jiguo Cao", "Jiheng Wang"], "venue": "IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology,", "citeRegEx": "Fan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2010}, {"title": "Extremely randomized trees", "author": ["Pierre Geurts", "Damien Ernst", "Louis Wehenkel"], "venue": "Machine Learning,", "citeRegEx": "Geurts et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geurts et al\\.", "year": 2006}, {"title": "rFerns: An implementation of the random ferns method for general-purpose machine learning", "author": ["Miron Bartosz Kursa"], "venue": "Journal of Statistical Software,", "citeRegEx": "Kursa.,? \\Q2014\\E", "shortCiteRegEx": "Kursa.", "year": 2014}, {"title": "Rotation forest: A new classifier ensemble method", "author": ["Juan J Rod\u0155\u0131guez", "Ludmila I Kuncheva", "Carlos J Alonso"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Rod\u0155\u0131guez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rod\u0155\u0131guez et al\\.", "year": 2006}, {"title": "The strength of weak learnability", "author": ["Robert E. Schapire"], "venue": "Machine learning,", "citeRegEx": "Schapire.,? \\Q1990\\E", "shortCiteRegEx": "Schapire.", "year": 1990}, {"title": "Classification of tumor samples from expression data using decision trunks", "author": ["Benjamin Ulfenborg", "Karin Klinga-Levan", "Bj\u00f6rn Olsson"], "venue": "Cancer Informatics,", "citeRegEx": "Ulfenborg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ulfenborg et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The other popular idea, started by Geurts et al. [2006] with their Extra-Trees method, is to generate pivots at random, obviously with constraint that a resulting criterion should not sent all objects in one direction.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "The other popular idea, started by Geurts et al. [2006] with their Extra-Trees method, is to generate pivots at random, obviously with constraint that a resulting criterion should not sent all objects in one direction. Such a method removes a problem of finding an appropriate homogeneity measure and performing the optimisation, which in return increases the computational efficiency and, in a number of problems, the robustness of a final model as it leads to a greater divergence among sharpening ensemble models. Naturally, this way one also removes an impact of a possible problems with the homogeneity measure, which may surface for instance in case of unbalanced sets. Still, there are problems in the probability of finding even a slightly meaningful pivot is very small, and they usually case random pivot-based algorithm to perform poorly. And in-between solution is to use some kind of heuristic instead of a full optimisation; a simple example of this technique is to generate some number of random pivots and select the best one. Other approaches include generic soft optimisation methods like genetic algorithms, randomly reducing the search space (often the set of used predictors, ad done by the standard Random Forest) or disturbing the homogeneity measure. Rod\u0155\u0131guez et al. [2006] proposed to employ PCA in pivot generation.", "startOffset": 35, "endOffset": 1299}, {"referenceID": 0, "context": "Bosch et al. [2007] perform image classification with GRF with pivots calculating two vector image descriptors, summing their valus with random weights and comparing with a random threshold.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Bosch et al. [2007] perform image classification with GRF with pivots calculating two vector image descriptors, summing their valus with random weights and comparing with a random threshold. Fan [2009] proposed to employ some kernel function used for SVM and build pivot by partial application of this kernel to a randomly selected object and compare the result with a threshold optimised in terms of information gain.", "startOffset": 0, "endOffset": 202}, {"referenceID": 7, "context": "\u2013 Although boosting [Schapire, 1990] is almost always considered as a standalone ensemble, wrapping it in another layer may lead to a better resilience to noise and mislabelled objects.", "startOffset": 20, "endOffset": 36}, {"referenceID": 5, "context": ", 2008, Kursa, 2014] is basically a form of a full decision tree in which each pivot module at a given depth is identical. To this end, evaluation of a given pivot is not dependent on the others and can happen in any order, which makes a fern more computationally effective than a decision tree. On the other hand, this makes optimisation of a fern is highly non-obvious, thus individual pivots are usually generated randomly. This way the leaves of a fern are often non-homogeneous, and thus the prediction of a fern is often encoded as a vector of class probabilities, which naturally requires appropriate adaptation of the voting scheme present in the conditioning ensemble. \u2013 decision trunk, proposed by Ulfenborg et al. [2013], is composed of a flat series of pivot models similar to a fern, though requires the decision to be binary (say A or B), and employs pivot modules (segments) which classify into three groups, A, B meaning that it is certain that an object belongs to a respective class, and ? meaning that the decision is relayed to a next pivot classifier within the trunk.", "startOffset": 8, "endOffset": 732}], "year": 2015, "abstractText": "Assuming a view of the Random Forest as a special case of a nested ensemble of interchangeable modules, we construct a generalisation space allowing one to easily develop novel methods based on this algorithm. We discuss the role and required properties of modules at each level, especially in context of some already proposed RF generalisations.", "creator": "LaTeX with hyperref package"}}}