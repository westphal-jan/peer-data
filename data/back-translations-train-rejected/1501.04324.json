{"id": "1501.04324", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2015", "title": "Phrase Based Language Model For Statistical Machine Translation", "abstract": "We consider phrase based Language Models (LM), which generalize the commonly used word level models. Similar concept on phrase based LMs appears in speech recognition, which is rather specialized and thus less suitable for machine translation (MT). In contrast to the dependency LM, we first introduce the exhaustive phrase-based LMs tailored for MT use. Preliminary experimental results show that our approach outperform word based LMs with the respect to perplexity and translation quality.", "histories": [["v1", "Sun, 18 Jan 2015 16:37:53 GMT  (20kb)", "http://arxiv.org/abs/1501.04324v1", "5 pages. This version of the paper was submitted for review to EMNLP 2013. The title, the idea and the content of this paper was presented by the first author in the machine translation group meeting at the MSRA-NLC lab (Microsoft Research Asia, Natural Language Computing) on July 16, 2013"]], "COMMENTS": "5 pages. This version of the paper was submitted for review to EMNLP 2013. The title, the idea and the content of this paper was presented by the first author in the machine translation group meeting at the MSRA-NLC lab (Microsoft Research Asia, Natural Language Computing) on July 16, 2013", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jia xu", "geliang chen"], "accepted": false, "id": "1501.04324"}, "pdf": {"name": "1501.04324.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["xu@tsinghua.edu.cn,", "cglcgl200208@126.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 1.04 324v 1 [cs.C L] 18. Jan 20We are looking at Phrase-based Language Models (LM), which generalize common word-level models. A similar concept for Phrase-based LMs occurs in speech recognition, which is more specialized and therefore less suitable for Machine Translation (MT). In contrast to the LM dependence, we are first introducing the extensive Phrase-based LMs, which are tailored to MT use. Initial experimental results show that our approach exceeds word-based LMs in terms of perplexity and translation quality."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is"}, {"heading": "2 Phrase Based LM", "text": "We will consider a sentence as a sequence of words wI1 = w1w2 = J sentence j = J sentence j = J segment J = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment = J segment 1 as a product of the probabilities of each word that gave its previous n \u2212 1 words: P (wI1) sequence k1 words: P (wI1) = sequence kJ sequence ki i i i i \u00b7 kJ sequence = kJ sequence \u2212 n sequence \u2212 n sequence \u2212 n sequence \u2212 n: P (wI1) sequence \u2212 n sequence \u2212 n: P (wi i i i i i i i i n sequence \u2212 n \u2212 n sequence \u2212 n)."}, {"heading": "3 Experiments", "text": "This is an ongoing work, and we conducted preliminary experiments on the IWSLT task (IWSLT, 2011) and then evaluated the performance of the LM by measuring LM helplessness and MT translation performance. Due to the arithmetical requirement, we used only sentences that contained no more than 15 words in the training corpus and no more than 10 words in the test corpus (Dev2010, Tst2010 and Tst2011), as in Table 2.We used word-based LM in Equation 1 as the basic method (base). We calculated the perplexities of Tst2011 with different n-gram orders using both the sum model and the max model, with and without smoothing (p.) as in Section 2. Table 3 shows that the perplexities in our approaches are all lower than those in the base method (base).For MT, we selected the individual best translation results based on the LM-100 perplexity best translation candidate."}, {"heading": "4 Conclusion", "text": "We have presented two phrase models that consider phrases as basic components of a sentence and perform an exhaustive search. Our future work will focus on the efficiency of a larger data track as well as improvements in smoothing methods."}], "references": [{"title": "The Theory of Parsing, Translation and Compiling, volume", "author": ["Aho", "Ullman1972] Alfred V. Aho", "Jeffrey D. Ullman"], "venue": null, "citeRegEx": "Aho et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Aho et al\\.", "year": 1972}, {"title": "Alternation. Journal of the Association for Computing Machinery, 28(1):114\u2013133", "author": ["Dexter C. Kozen", "Larry J. Stockmeyer"], "venue": null, "citeRegEx": "Chandra et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Chandra et al\\.", "year": 1981}, {"title": "Algorithms on Strings, Trees and Sequences", "author": ["Dan Gusfield"], "venue": null, "citeRegEx": "Gusfield.,? \\Q1997\\E", "shortCiteRegEx": "Gusfield.", "year": 1997}, {"title": "Phrase-Based Language Models for Speech Recognition", "author": ["Kuo", "Reichl1999] Hong-Kwang Jeff Kuo", "Wolfgang Reichl"], "venue": "In EUROSPEECH", "citeRegEx": "Kuo et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kuo et al\\.", "year": 1999}, {"title": "Building Phrase Based Language Model from Large Corpus", "author": ["Haijiang Tang"], "venue": null, "citeRegEx": "Tang.,? \\Q2002\\E", "shortCiteRegEx": "Tang.", "year": 2002}, {"title": "Statistical Machine Translation: From Single Word Models to Alignment Templates", "author": ["F. Och"], "venue": "Ph.D. thesis, RWTH Aachen,", "citeRegEx": "Och.,? \\Q2002\\E", "shortCiteRegEx": "Och.", "year": 2002}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni et"], "venue": "In Proc. of ACL,", "citeRegEx": "et.,? \\Q2002\\E", "shortCiteRegEx": "et.", "year": 2002}, {"title": "Statistical phrasebased translation", "author": ["Koehn", "Och", "Marcu2003] P. Koehn", "F. Och", "D. Marcu"], "venue": "In Proc. of HLT-NAACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Deriving Phrase-based Language Models", "author": ["Heeman", "Damnati1997] Peter A. Heeman", "Geraldine Damnati"], "venue": "In Automatic Speech Recognition and Understanding,", "citeRegEx": "Heeman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Heeman et al\\.", "year": 1997}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown", "S. Della Pietra", "V. Della Pietra", "R. Mercer1993] P. Brown", "R. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Dependence language model for information retrieval", "author": ["Gao et"], "venue": "In Proc. of ACM,", "citeRegEx": "et.,? \\Q2004\\E", "shortCiteRegEx": "et.", "year": 2004}, {"title": "A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language", "author": ["Shen et"], "venue": null, "citeRegEx": "et.,? \\Q2008\\E", "shortCiteRegEx": "et.", "year": 2008}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["R. Rosenfeld"], "venue": "Proc. of IEEE,", "citeRegEx": "Rosenfeld.,? \\Q2000\\E", "shortCiteRegEx": "Rosenfeld.", "year": 2000}, {"title": "SRILM-an extensible language modeling toolkit. In INTERSPEECH", "author": ["A. Stolcke"], "venue": null, "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Lafferty et"], "venue": "In Intl. Conf. on Machine Learning,", "citeRegEx": "et.,? \\Q2001\\E", "shortCiteRegEx": "et.", "year": 2001}, {"title": "Discriminative language modeling with conditional random fields and the perceptron algorithm", "author": ["Roark", "Saraclar", "Collins", "Johnson2004] B. Roark", "M. Saraclar", "M. Collins", "M. Johnson"], "venue": "In Proc. of ACL,", "citeRegEx": "Roark et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2004}, {"title": "A structured language model", "author": ["C. Chelba"], "venue": "In Proc. of ACL,", "citeRegEx": "Chelba.,? \\Q1997\\E", "shortCiteRegEx": "Chelba.", "year": 1997}], "referenceMentions": [{"referenceID": 5, "context": "In machine translation, it measures the fluency and well-formness of a translation, and therefore is important for the translation quality, see (Och, 2002) and (Koehn, Och and Marcu, 2003) etc.", "startOffset": 144, "endOffset": 155}, {"referenceID": 13, "context": "Common applications of LMs include estimating the distribution based on N-gram coverage of words, to predict word and word orders, as in (Stolcke, 2002) and (Lafferty et.", "startOffset": 137, "endOffset": 152}, {"referenceID": 4, "context": "In (Kuo and Reichl, 1999) and (Tang, 2002), new phrases are added to the lexicon with different measure function.", "startOffset": 30, "endOffset": 42}, {"referenceID": 13, "context": "In the word based LM (Stolcke, 2002), the probability of a sentence Pr(wI 1) 1is defined as the product of the probabilities of each word given its previous n\u2212 1 words:", "startOffset": 21, "endOffset": 36}], "year": 2015, "abstractText": "We consider phrase based Language Models (LM), which generalize the commonly used word level models. Similar concept on phrase based LMs appears in speech recognition, which is rather specialized and thus less suitable for machine translation (MT). In contrast to the dependency LM, we first introduce the exhaustive phrase-based LMs tailored for MT use. Preliminary experimental results show that our approach outperform word based LMs with the respect to perplexity and translation quality.", "creator": "LaTeX with hyperref package"}}}