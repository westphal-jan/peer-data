{"id": "1501.04684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2015", "title": "Slice Sampling for Probabilistic Programming", "abstract": "We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net.", "histories": [["v1", "Tue, 20 Jan 2015 00:24:14 GMT  (933kb,D)", "http://arxiv.org/abs/1501.04684v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["razvan ranca", "zoubin ghahramani"], "accepted": false, "id": "1501.04684"}, "pdf": {"name": "1501.04684.pdf", "metadata": {"source": "CRF", "title": "Slice Sampling for Probabilistic Programming", "authors": ["Razvan Ranca", "Zoubin Ghahramani"], "emails": [], "sections": [{"heading": null, "text": "We present the first universal slice sampling inference engine for probabilistic programs, which is part of StocPy, a new Turing Complete probabilistic programming language available as a Python library. We present a transdimensional generalization of the slice sampling engine necessary for the inference engine to work on tracks with different numbers of random variables. We show that StocPy is advantageous in terms of flexibility and ease of use compared to other PPLs, and that slice sampling can surpass earlier inference methods. Our experiments include logistic regression, HMM, and the Bayesan Neural Net."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2 StocPy", "text": "We are implementing a novel PPL, StocPy, which will be made available online (Ranca [2014]). StocPy has both a Metropolis-Hastings inference engine (implemented in the style presented by Wingate et al. [2011]) and a slice sampling inference engine, which we will discuss further in Section 3."}, {"heading": "2.1 Why make a Python PPL?", "text": "The popularity of PPLs such as BUGS is partly due to their simplicity. On the other hand, lisp-based PPLs such as Goodman et al. [2008] offer greater flexibility, but at the cost of ease of use. StocPy is able to offer the same power as the lisp-based PPLs, while the user can work in Python, a language that is both very friendly to beginners and extremely popular for prototypes. In addition, with the help of Python's tools, we can easily make StocPy available as a library, and that we make use of Python's flexible programming language in defining models (e.g. stochastic mutual recursive functions, use of globals)."}, {"heading": "2.2 StocPy Programming Style", "text": "As I said, a custom model can use most common Python attributes as long as the model is confined to a file as if it were a lightweight. Essentially, the user should undergo all stochastic operations."}, {"heading": "3 Slice Sampling Inference Engine", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Advantages of Slice Sampling", "text": "\"We only need an example from the field of curves P (x).A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \"\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \"A\".A \".A\".A \"A\".A \"A\".A \".A\".A \"A\".A \".A\".A \".A\".A \"A\".A \".A\" A \".A\" A \".A\".A \".A\" A \".A\".A \"A\".A \".A\".A \".A\" A \".A\".A \".A\".A \".A\".A \".A\".A \"A\" \".A\" A \".A\".A \"\".A \".A\".A \".A\".A \".A\".A \".A\" \"A\".A \"A\".A \".A\".A \".A\""}, {"heading": "3.2 Inference engine construction", "text": "As shown in Wingate et al. [2011], we consider an example x as a program execution trace and P \u0445 (x) as the probability of all stochastic variables scanned in track x.The bottleneck in the inference motors is the calculation of the trace probability. Metropolis calculates this value exactly once per sample, whereas slice sampling needs to be performed at least three times (one for xl, xr and the next x) and potentially much more often. Therefore, StocPy provides the ability to perform inferences until a certain number of trace log probability calculations have been performed, allowing Metropolis and slice sampling to be compared directly and fairly with each other. Furthermore, all experiments comparing Slice with Metropolis show the algorithms for the same time period. However, we have decided to use the trace probability calculations."}, {"heading": "3.2.1 Trans-dimensional models", "text": "To understand the additional complications that transdimensional models represent for inference machines, let us consider a simple example from Wood et al. [2014], the branching model. This model has 2 variables whose values determine the distribution of a 3rd variable that we condition. This model is trans-dimensional, because on different tracks either 1 or both of the 2 variables are sampled. Rewriting the model so that both variables are always sampled, even if one of them is not used, leaves the rear invariant unused. Therefore, one method to perform inferences correctly in a trans-dimensional model is to always stomp all variables that could be used in a track. However, this approach will be extremely inefficient in large models and is not a workable general solution. In Figure 7, we use this trick to see what the space of possible tracks looks like, what they look like, and what the true rear track is."}, {"heading": "3.3 Transdimensional Slice Sampling", "text": "To understand the transdimensional corrections, we can place them within the framework of the reversible Jump Markov Chain Monte Carlo (RJMCMC, Green and Hastie [2009]). Informally, we can imagine slice sampling as a form of RJMCMC, in which we carefully select the next sample, so that the probability of acceptance is always 1. We include a brief explanation of the RJMCMC notation in the appendix. To place the slice sampling within this framework, we can imagine an execution track of the program as state x. The choice of movement we make corresponds to the choice of the random variable we want to re-select. Therefore, if there are | D | random variables in the current track and we select one to ample uniformly, then jm (x) = 1 / D |. Once the variable is selected, we can define a de-ministerial function that will follow a new value by presenting the sampling uniformly."}, {"heading": "4 Empirical Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Inferring the mean of a Gaussian", "text": "We start with the conclusion of the mean of a Gaussian. This time, the previous and rear are of similar size, but the rear is shifted by an unusual observation. In this context, we will consider 3 models, namely 1-dimensional, 2-dimensional and trans-dimensional. The model specifications are in Figure 9 and the back of the models are in Figure 10. We will now consider the performance of Metropolis, sampling and some different mixtures of the two across the 3 models. The mixing methods work by extracting a distorted coin before extracting each sample to decide which inference method to use. To compare the inference machines, we will extract a certain number of trace probability calculations and then repeat this process 100 times, starting from different random seeds. We will then plot the medians and quarters of the KS differences from the true back, across all the runs generated."}, {"heading": "4.2 Anglican models", "text": "The model specifications for this are set out in Section 2.2 and in the supplementary material. To evaluate the motors, we use 100 independent samples each. In Figure 12, we record the convergence of the empirical posterior with the true posterior as the number of trace probabilities increases. For continuous distributions, we use the Kolmogorov-Smirnov statistics (as before), while for discrete samples, we use the Kullback-Leibler divergence to measure the similarity of two distributions. In the branch and HMM models, we use the naive Metropolis-Smirnov statistics (as before), while we use the Kullback-Leibler divergence to measure the similarity of two distributions. In the branching and HMM models (12), naive Metropolis-Hastings surpass all slice combinations. These models are quite small and are limited to a few data apoints, meaning that they have relative disadvantages and disadvantages."}, {"heading": "4.3 Models for classification", "text": "Finally, we look at some new models, namely logistic regression and a small neural network. As we perform the classification, we can actually plot the mean square error that the model achieves after a certain number of trace probability calculations. As before, we perform separate gradients, starting with different random seeds, and plot both the median and the 25% and 75% quarters. First, we perform a logistic regression using the known iris dataset obtained by Bache and Lichman [2013]. The result is shown in Figure 13 and shows that slice sampling converges significantly faster than MH. Second, we train a small Bayesian neural network on the same dataset. Our neural network consists of an input layer of the same dimensionality as the data (4), two hidden layers of 4 and 2 neurons, and a differential layer of a single neuron as the same dataset. Our neural network consists of an input layer of the same dimensionality as the data (2), showing that the hidden layers of 4 and 2 neurons converge converges significantly faster than MH, and a differential layer of a single neuron the same dataset."}, {"heading": "5 Related Work", "text": "Our slice sampling inference engine is based on the slice sampling method introduced by Neal [2003] and influenced by the computer-friendly slice sampler shown in MacKay [2003]. Slice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs such as BUGS (Lunn et al. [2009] and STAN (Stan Development Team [2014]). However, the slice samplers used in these languages are not directly exposed to the user, but are only used internally by the language. Therefore, the slice samplers available in these languages are not intended to generalize all models, especially trans dimensionality corrections. Poon and Domingos [2006] also propose a slice sampling-based solution, this time to Markov logic problems."}, {"heading": "6 Conclusion", "text": "We have demonstrated the advantages that StocPy offers to both users and developers of PPLs, namely the flexibility, clarity, conciseness and simplicity of prototyping. We have also implemented a novel inference machine in StocPy that solves the problem of trans-dimensional scanning. To our knowledge, this is the first universal inference machine and the first sampling method that solves the problem of trans-dimensional scanning. We have evaluated this sampling engine empirically and shown that the potential benefits far outweigh the potential costs compared to individual metropolis. While Metropolis works well with very small models, where the previous and the back models are similar, Slice offers considerable advantages as the distributions vary from each other."}, {"heading": "1 Detailed slice-sampling pseudocode", "text": "Here we present the detailed pseudocode for a possible slice sampling implementation. Algorithm 1 is the high-level logic of slice sampling, which was also represented on paper. Other algorithms extend this and show the operations at the lower level. Specifically, algorithm 2 shows how we could perform the operation described in line 5 of algorithm 1 by exponentially increasing the [xl, xr] interval. Likewise, algorithm 3 describes how the operation from line 6 of algorithm 1 is performed, namely by sampling the next x on the already defined segment [xl, xr] so that x \u2212 n is selected \"uniformly below the curve.\" Algorithm 1 Slice Sampling 1: Select initial x1 so that P (x1) > x x x 2: Select the number of samples from the already defined segment [xl: x x: x form: height for current function: > N: xxt = x1 pick: x x) x x x x: Pick 1 (pick: x x: current: x: xx1 pick: x: x: xx1 pick for current function)."}, {"heading": "2 True Posteriors", "text": "In Figure 1 we see that the problem of \"hard\" Gaussian mean inference leads to an analytical posterior that is very close to a Gaussian mean of 2 with a standard deviation of 0.032. The second posterior, shown in Figure 2, shows the difference between the front and rear of the Marsaglia model. This figure provides an intuitive understanding of why Metropolis is surpassed by samples from the Marsaglia model. It is easy to imagine how badly Metropolis will perform in this model if it only uses samples from the previous model."}, {"heading": "3 Additional Sample Programs in StocPy and Anglican", "text": "For the sake of completeness and to get a better idea of the \"feeling\" of StocPy, we show the two remaining Anglican models not presented in the paper: a Dirichlet Process Mixture (Figure 3) and a Marsaglia (Figure 4). For comparison, we also show the implementation of these models in Anglican. As in the paper, we can see that StocPy is more concise on the more complex model, largely thanks to the built-in Python operators. In addition, ar Xiv: 150 1.04 684v 1 [cs.A I] 2 0Ja n20 15clarity is a subjective perception, but we would argue that the StocPy formulations are friendlier to people without strong CS or technical background, as domain experts do."}], "references": [{"title": "Modern compiler implementation in ML", "author": ["A.W. Appel"], "venue": "Cambridge university press,", "citeRegEx": "Appel.,? \\Q1998\\E", "shortCiteRegEx": "Appel.", "year": 1998}, {"title": "A language for generative models", "author": ["N. Goodman", "V. Mansinghka", "D. Roy", "K. Bonawitz", "J. Tenenbaum. Church"], "venue": "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "An agenda for probabilistic programming: Usable, portable, and ubiquitous", "author": ["A.D. Gordon"], "venue": null, "citeRegEx": "Gordon.,? \\Q2013\\E", "shortCiteRegEx": "Gordon.", "year": 2013}, {"title": "Reversible jump MCMC", "author": ["P.J. Green", "D.I. Hastie"], "venue": "Genetics, 155(3):1391\u20131403,", "citeRegEx": "Green and Hastie.,? \\Q2009\\E", "shortCiteRegEx": "Green and Hastie.", "year": 2009}, {"title": "Exploring network structure, dynamics, and function using NetworkX", "author": ["A.A. Hagberg", "D.A. Schult", "P.J. Swart"], "venue": "In Proceedings of the 7th Python in Science Conference", "citeRegEx": "Hagberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hagberg et al\\.", "year": 2008}, {"title": "The BUGS project: Evolution, critique and future directions", "author": ["D. Lunn", "D. Spiegelhalter", "A. Thomas", "N. Best"], "venue": "Statistics in medicine,", "citeRegEx": "Lunn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lunn et al\\.", "year": 2009}, {"title": "Information theory, inference and learning algorithms", "author": ["D.J. MacKay"], "venue": "Cambridge university press,", "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "Venture: a higher-order probabilistic programming platform with programmable inference", "author": ["V. Mansinghka", "D. Selsam", "Y. Perov"], "venue": "arXiv preprint arXiv:1404.0099,", "citeRegEx": "Mansinghka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mansinghka et al\\.", "year": 2014}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Statistical relational learning,", "citeRegEx": "Milch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Milch et al\\.", "year": 2007}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["H. Poon", "P. Domingos"], "venue": "In AAAI,", "citeRegEx": "Poon and Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2006}, {"title": "StocPy: An expressive probabilistic programming language, in python", "author": ["R. Ranca"], "venue": "https://github. com/RazvanRanca/StocPy,", "citeRegEx": "Ranca.,? \\Q2014\\E", "shortCiteRegEx": "Ranca.", "year": 2014}, {"title": "Lightweight implementations of probabilistic programming languages via transformational compilation", "author": ["D. Wingate", "A. Stuhlmueller", "N.D. Goodman"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wingate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wingate et al\\.", "year": 2011}, {"title": "A new approach to probabilistic programming inference", "author": ["F. Wood", "J.W. van de Meent", "V. Mansinghka"], "venue": "In Proceedings of the 17th International conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "There has been a recent surge of interest in probabilistic programming, as demonstrated by the continued development of new languages (eg: Wood et al. [2014], Goodman et al.", "startOffset": 139, "endOffset": 158}, {"referenceID": 1, "context": "[2014], Goodman et al. [2008], Lunn et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "[2014], Goodman et al. [2008], Lunn et al. [2009], Milch et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 1, "context": "[2014], Goodman et al. [2008], Lunn et al. [2009], Milch et al. [2007]) and by the recent DARPA program encouraging further research in this direction.", "startOffset": 8, "endOffset": 71}, {"referenceID": 2, "context": "and inference performance (Gordon [2013]).", "startOffset": 27, "endOffset": 41}, {"referenceID": 2, "context": "and inference performance (Gordon [2013]). In this paper we address usability by presenting a new PPL, StocPy, available online as a Python library (Ranca [2014]).", "startOffset": 27, "endOffset": 162}, {"referenceID": 11, "context": "StocPy is based on the PPL design presented by Wingate et al. [2011], but is written purely in Python, and works on model definitions written in the same language.", "startOffset": 47, "endOffset": 69}, {"referenceID": 11, "context": "StocPy is based on the PPL design presented by Wingate et al. [2011], but is written purely in Python, and works on model definitions written in the same language. This enables us to take full advantage of Python\u2019s excellent prototyping abilities and fast development cycles, and thus allows us to specify models in very flexible ways. For instance models containing complex control flow and elements such as stochastic (mutual) recursion are easily representable. Additionally, the pure Python implementation means StocPy itself provides a good base for further experiments into PPLs, such as defining novel naming techniques for stochastic variables, looking at program recompilation to improve inference performance, or testing out new inference engines. We illustrate the benefits StocPy offers by discussing some of the language\u2019s features and contrasting the definitions of several models in StocPy against those in Anglican (Wood et al. [2014]).", "startOffset": 47, "endOffset": 951}, {"referenceID": 10, "context": "We implement a novel PPL, StocPy, which is made available online (Ranca [2014]).", "startOffset": 66, "endOffset": 79}, {"referenceID": 10, "context": "We implement a novel PPL, StocPy, which is made available online (Ranca [2014]). StocPy has both a Metropolis-Hastings inference engine (implemented in the style presented by Wingate et al. [2011]) and a Slice sampling inference engine which we further discuss in Section 3.", "startOffset": 66, "endOffset": 197}, {"referenceID": 1, "context": "On the other hand, lisp-based PPLs such as Goodman et al. [2008] offer greater flexibility but at the price of usability.", "startOffset": 43, "endOffset": 65}, {"referenceID": 4, "context": "For instance, Python\u2019s excellent network and graph libraries (eg: Hagberg et al. [2008]) could be used to define a novel naming convention for stochastic primitives, which takes the program\u2019s control flow into account.", "startOffset": 66, "endOffset": 88}, {"referenceID": 4, "context": "For instance, Python\u2019s excellent network and graph libraries (eg: Hagberg et al. [2008]) could be used to define a novel naming convention for stochastic primitives, which takes the program\u2019s control flow into account. Such a naming convention is required by the framework of Wingate et al. [2011] and could be an improvement over the simpler version presented in that paper.", "startOffset": 66, "endOffset": 298}, {"referenceID": 12, "context": "The models are 2 of those presented by Wood et al. [2014] (more models are included in the supplementary material).", "startOffset": 39, "endOffset": 58}, {"referenceID": 6, "context": "Slice sampling, however, adjusts an inadequate step size of size S with a cost that is only logarithmic in the size of S (MacKay [2003]).", "startOffset": 122, "endOffset": 136}, {"referenceID": 11, "context": "We refer to the MH proposal described in Wingate et al. [2011], which is the same as the \u201cRDB\u201d benchmark used in Wood et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 11, "context": "We refer to the MH proposal described in Wingate et al. [2011], which is the same as the \u201cRDB\u201d benchmark used in Wood et al. [2014]. In this MH implementation, whenever we wish to resample a variable, we run the model until we encounter the variable and then sample it from its prior conditioned on the variables we have seen so far in the current run.", "startOffset": 41, "endOffset": 132}, {"referenceID": 11, "context": "As presented in Wingate et al. [2011], we consider a sample x to be a program execution trace and P \u2217(x) to be the likelihood of all stochastic variables sampled in trace x.", "startOffset": 16, "endOffset": 38}, {"referenceID": 12, "context": "In order to understand the additional complications that trans-dimensional models present for inference engines we look at a simple example taken from Wood et al. [2014], the Branching model.", "startOffset": 151, "endOffset": 170}, {"referenceID": 3, "context": "To understand the trans-dimensional corrections we can place them in the framework of Reversible Jump Markov Chain Monte Carlo (RJMCMC, Green and Hastie [2009]).", "startOffset": 136, "endOffset": 160}, {"referenceID": 11, "context": "The correction we aplly to account for this is similar to the one applied by Wingate et al. [2011]. Specifically we update P \u2217(x) = P \u2217(x) + log ( |D|\u2217pstale |D\u2032|\u2217pfresh ) , where", "startOffset": 77, "endOffset": 99}, {"referenceID": 12, "context": "In order to further test the Slice sampling inference engine we look at 3 of the models defined in Wood et al. [2014]. The model specifications for these are provided in Section 2.", "startOffset": 99, "endOffset": 118}, {"referenceID": 6, "context": "Our slice sampling inference engine is based on the slice sampling method presented by Neal [2003], and influenced by the computer friendly slice sampler shown in MacKay [2003].", "startOffset": 163, "endOffset": 177}, {"referenceID": 5, "context": "Slice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs, such as BUGS (Lunn et al. [2009]) and STAN (Stan Development Team [2014]).", "startOffset": 157, "endOffset": 176}, {"referenceID": 5, "context": "Slice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs, such as BUGS (Lunn et al. [2009]) and STAN (Stan Development Team [2014]).", "startOffset": 157, "endOffset": 216}, {"referenceID": 5, "context": "Slice sampling techniques have been applied to a wide range of inference problems (Neal [2003]) and are used in some of the most popular PPLs, such as BUGS (Lunn et al. [2009]) and STAN (Stan Development Team [2014]). However, the slice samplers these languages employ are not exposed directly to the user, but instead only used internally by the language. Therefore, the slice samplers present in these languages are not intended to generalise to all models and, specifically, make no mention of trans-dimensionality corrections. Poon and Domingos [2006], also proposes a slice sampling based solution, this time to Markov Logic problems.", "startOffset": 157, "endOffset": 556}, {"referenceID": 7, "context": "The most similar use-case to ours would be in Venture (Mansinghka et al. [2014]).", "startOffset": 55, "endOffset": 80}], "year": 2015, "abstractText": "We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net.", "creator": "LaTeX with hyperref package"}}}