{"id": "1501.04725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2015", "title": "Learning Invariants using Decision Trees", "abstract": "The problem of inferring an inductive invariant for verifying program safety can be formulated in terms of binary classification. This is a standard problem in machine learning: given a sample of good and bad points, one is asked to find a classifier that generalizes from the sample and separates the two sets. Here, the good points are the reachable states of the program, and the bad points are those that reach a safety property violation. Thus, a learned classifier is a candidate invariant. In this paper, we propose a new algorithm that uses decision trees to learn candidate invariants in the form of arbitrary Boolean combinations of numerical inequalities. We have used our algorithm to verify C programs taken from the literature. The algorithm is able to infer safe invariants for a range of challenging benchmarks and compares favorably to other ML-based invariant inference techniques. In particular, it scales well to large sample sets.", "histories": [["v1", "Tue, 20 Jan 2015 07:20:30 GMT  (338kb,D)", "http://arxiv.org/abs/1501.04725v1", "15 pages, 2 figures"]], "COMMENTS": "15 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.PL cs.LG", "authors": ["siddharth krishna", "christian puhrsch", "thomas wies"], "accepted": false, "id": "1501.04725"}, "pdf": {"name": "1501.04725.pdf", "metadata": {"source": "CRF", "title": "Learning Invariants using Decision Trees", "authors": ["Siddharth Krishna", "Christian Puhrsch", "Thomas Wies"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where one feels able to leave the country in order to return to the USA, to go to the USA in order to find a new home there."}, {"heading": "2 Overview", "text": "In this area, we can give an illustrative example and go through the steps that we have taken in our algorithm to calculate the extent that we have set out to do. To that end, we have trawled through the program in each area, it's about how we behave in each area, it's about how we behave in each area, it's about how we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area, it's about the way we behave in each area."}, {"heading": "3 Preliminaries", "text": "This year it is more than ever before."}, {"heading": "4 Algorithm", "text": "This year it is more than ever before."}, {"heading": "5 Implementation and Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Implementation", "text": "We implemented our algorithm in Python by using the scikit-learn library as a decision tree classifier [25] as a DT learner LearnDT. This implementation uses the CART algorithm [7], which greedily learns as described in Section 3, and uses the Gini index. We implemented a simple and naive sampler: We looked at all states that met the requirement that the value of each variable was in the r'L, Ls interval. For these states, we ran the program with a bound I to the number of loops, and collected all states that were achieved as good states. To find bad states, we looked at all states that were one Margin M away from each good state, the program ran most I iterations), and when the program failed to make a claim, we collected all states as bad states that way."}, {"heading": "5.2 Evaluation", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "6 Related Work and Conclusions", "text": "This year is the highest in the history of the country."}], "references": [{"title": "Craig interpretation. In SAS, volume 7460 of LNCS, pages 300\u2013316", "author": ["A. Albarghouthi", "A. Gurfinkel", "M. Chechik"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "UFO: verification with interpolants and abstract interpretation - (competition contribution)", "author": ["A. Albarghouthi", "A. Gurfinkel", "Y. Li", "S. Chaki", "M. Chechik"], "venue": "In TACAS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Boolean and cartesian abstraction for model checking C programs", "author": ["T. Ball", "A. Podelski", "S.K. Rajamani"], "venue": "In TACAS, volume 2031 of LNCS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Boogie: A modular reusable verifier for object-oriented programs", "author": ["M. Barnett", "B.E. Chang", "R. DeLine", "B. Jacobs", "K.R.M. Leino"], "venue": "FMCO", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Cpachecker: A tool for configurable software verification", "author": ["D. Beyer", "M.E. Keremoglu"], "venue": "In CAV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Learnability and the vapnikchervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth"], "venue": "J. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Classification and Regression Trees", "author": ["L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Counterexample-guided abstraction refinement", "author": ["E.M. Clarke", "O. Grumberg", "S. Jha", "Y. Lu", "H. Veith"], "venue": "In CAV, volume 1855 of LNCS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints", "author": ["P. Cousot", "R. Cousot"], "venue": "In POPL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1977}, {"title": "Systematic design of program analysis frameworks", "author": ["P. Cousot", "R. Cousot"], "venue": "In POPL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1979}, {"title": "The astre\u00e9 analyzer", "author": ["P. Cousot", "R. Cousot", "J. Feret", "L. Mauborgne", "A. Min\u00e9", "D. Monniaux", "X. Rival"], "venue": "In ESOP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "An Efficient SMT Solver", "author": ["L. De Moura", "N. Bj\u00f8rner. Z"], "venue": "In TACAS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Inductive invariant generation via abductive inference", "author": ["I. Dillig", "T. Dillig", "B. Li", "K.L. McMillan"], "venue": "In OOPSLA,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "ICE: A robust framework for learning invariants", "author": ["P. Garg", "C. L\u00f6ding", "P. Madhusudan", "D. Neider"], "venue": "In CAV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Compositional may-must program analysis: unleashing the power of alternation", "author": ["P. Godefroid", "A.V. Nori", "S.K. Rajamani", "S. Tetali"], "venue": "In POPL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Construction of abstract state graphs with PVS", "author": ["S. Graf", "H. Saidi"], "venue": "In CAV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "SYNERGY: a new algorithm for property checking", "author": ["B.S. Gulavani", "T.A. Henzinger", "Y. Kannan", "A.V. Nori", "S.K. Rajamani"], "venue": "In FSE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Invgen: An efficient invariant generator", "author": ["A. Gupta", "A. Rybalchenko"], "venue": "In CAV, volume 5643 of LNCS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Constructing optimal binary decision trees is np-complete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Inf. Process. Lett.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1976}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1986}, {"title": "Tree automata-based refinement with application to horn clause verification", "author": ["B. Kafle", "J.P. Gallagher"], "venue": "In VMCAI, LNCS. Springer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "The octagon abstract domain", "author": ["A. Min\u00e9"], "venue": "Higher-Order and Symbolic Computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Machine learning. McGraw Hill series in computer science", "author": ["T.M. Mitchell"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Foundations of machine learning 2014: Homework assignment 1 (problem c)", "author": ["M. Mohri"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Counterexample-guided focus", "author": ["A. Podelski", "T. Wies"], "venue": "In POPL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan. C"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1993}, {"title": "Dynamic inference of likely data preconditions over predicates by tree learning", "author": ["S. Sankaranarayanan", "S. Chaudhuri", "F. Ivancic", "A. Gupta"], "venue": "In ISSTA,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "On the density of families of sets", "author": ["N. Sauer"], "venue": "J. Comb. Theor. Ser. A,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1972}, {"title": "From invariant checking to invariant inference using randomized search", "author": ["R. Sharma", "A. Aiken"], "venue": "In CAV,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Verification as learning geometric concepts", "author": ["R. Sharma", "S. Gupta", "B. Hariharan", "A. Aiken", "A.V. Nori"], "venue": "editors, SAS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Interpolants as classifiers", "author": ["R. Sharma", "A.V. Nori", "A. Aiken"], "venue": "In CAV, volume 7358 of LNCS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Commun. ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1984}, {"title": "Testing, abstraction, theorem proving: better together", "author": ["G. Yorsh", "T. Ball", "M. Sagiv"], "venue": "In ISSTA,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}], "referenceMentions": [{"referenceID": 8, "context": "Some techniques, such as abstract interpretation [9], are effective at striking a good balance between scalability and precision by allowing the analysis to be fine-tuned for a specific class of programs and properties.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "This fine-tuning requires careful engineering of the analysis [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Instead of manually adapting the analysis to work well across many similar programs, refinement-based techniques adapt the analysis automatically to the given program and property at hand [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 14, "context": "A promising approach to achieve this automatic adaptation is to exploit synergies between static analysis and testing [15,17,34].", "startOffset": 118, "endOffset": 128}, {"referenceID": 16, "context": "A promising approach to achieve this automatic adaptation is to exploit synergies between static analysis and testing [15,17,34].", "startOffset": 118, "endOffset": 128}, {"referenceID": 33, "context": "A promising approach to achieve this automatic adaptation is to exploit synergies between static analysis and testing [15,17,34].", "startOffset": 118, "endOffset": 128}, {"referenceID": 13, "context": "Particularly interesting is the use of Machine Learning (ML) to infer likely invariants from test data [14,31,32].", "startOffset": 103, "endOffset": 113}, {"referenceID": 30, "context": "Particularly interesting is the use of Machine Learning (ML) to infer likely invariants from test data [14,31,32].", "startOffset": 103, "endOffset": 113}, {"referenceID": 31, "context": "Particularly interesting is the use of Machine Learning (ML) to infer likely invariants from test data [14,31,32].", "startOffset": 103, "endOffset": 113}, {"referenceID": 30, "context": "Invariant inference can be viewed as a binary classification problem [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 32, "context": "More precisely, using the Probably Approximately Correct model for learning [33], we provide a bound on the sample size that ensures that our algorithm successfully learns a safe inductive invariant with a given probability.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "In particular, inspired by the octagon abstract domain [22], we use as features the set of all hyperplane slopes of the form \u0306xi  \u0306 xj , where 1 \u010f i \u0103 j \u010f d.", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 29, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 30, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 31, "context": "Moreover, we observed that DT learners often produce simpler invariants and scale better to large sample sets compared to other ML-based invariant inference techniques such as [14, 30\u201332].", "startOffset": 176, "endOffset": 187}, {"referenceID": 30, "context": "Following [31], we view the problem of computing a safe inductive invariant for a program P \u201c pS,R, init, safeq as a binary classification problem by defining the input space as the set of all program states X \u201c S.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "A Decision Tree (DT) [23] is a binary tree that represents a Boolean function.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "However, the task of finding the smallest (in terms of number of nodes) DT for a particular function can be shown to be NP-complete [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Standard algorithms to learn DTs work by greedily selecting at each node the co-ordinate and threshold that separates the remaining training data best [7,27].", "startOffset": 151, "endOffset": 157}, {"referenceID": 26, "context": "Standard algorithms to learn DTs work by greedily selecting at each node the co-ordinate and threshold that separates the remaining training data best [7,27].", "startOffset": 151, "endOffset": 157}, {"referenceID": 24, "context": "There are other measures as well, such as the Gini index, which is used by the DT learner we used in our experiments [25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 32, "context": "We can justify this observation using Valiant\u2019s PAC (probably approximately correct) model [33].", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "[6]:", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Now one can use a basic, well-known lemma from [24] combined with Sauer\u2019s Lemma [29] to get that the VC dimension of bounded decision trees is OpKd logKq.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "Now one can use a basic, well-known lemma from [24] combined with Sauer\u2019s Lemma [29] to get that the VC dimension of bounded decision trees is OpKd logKq.", "startOffset": 80, "endOffset": 84}, {"referenceID": 6, "context": "However, the running time of the learning routine for DTs is Opmn logpnqq, where m is the number of hyperplane slopes in H and n is the number of sample points [7, 25].", "startOffset": 160, "endOffset": 167}, {"referenceID": 24, "context": "However, the running time of the learning routine for DTs is Opmn logpnqq, where m is the number of hyperplane slopes in H and n is the number of sample points [7, 25].", "startOffset": 160, "endOffset": 167}, {"referenceID": 24, "context": "We implemented our algorithm in Python, using the scikit-learn library\u2019s decision tree classifier [25] as the DT learner LearnDT.", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "This implementation uses the CART algorithm from [7] which learns in a greedy manner as described in Section 3, and uses the Gini index.", "startOffset": 49, "endOffset": 52}, {"referenceID": 30, "context": "In [31], the authors suggest working in the null space of the good states, viewed as a matrix.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Inspired by this, we propose using Principal Component Analysis [20] on the good states to generate slopes.", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "Finally, for the IsInvariant routine, we used the program verifier Boogie [4], which allowed us to annotate our programs with the invariants we verified.", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "Boogie uses the SMT solver Z3 [12] as a back-end.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "\u2013 ICE [14]: an ML algorithm based on ICE-learning that uses an SMT solver to learn numerical invariants.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "\u2013 MCMC [30]: an ML algorithm based on Markov Chain Monte Carlo methods.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "\u2013 SC [31]: an ML algorithm based on set cover.", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "We only had access to the learning algorithm proposed in [31] and not the sampling procedure.", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "\u2013 CPAchecker [5]: a configurable software model checker.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "\u2013 UFO [2]: a software model checker that combines abstract interpretation and interpolation (denoted CPA).", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "\u2013 InvGen [18]: an inference tool for linear invariants that combines abstract interpretation, constraint solving, and testing.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 13, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 17, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 30, "context": "In particular, we considered a subset of the benchmarks from [13, 14, 18, 31].", "startOffset": 61, "endOffset": 77}, {"referenceID": 13, "context": "Octagonal: ex23 [14] 4 conj.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "02 fig6 [14] 2 conj.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "01 fig9 [14] 2 conj.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "01 hola10 [13] 4 conj.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "03 F F nested2 [31] 4 conj.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "03 nested5 [18] 4 conj.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "03 fig1 [14] 2 disj.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "64 F test1 [31] 4 disj.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "04 cegar2 [14] 3 ABC 5 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "18 F gopan [31] 2 ABC 8 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "29 F hola18 [13] 3 ABC 6 1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "38 F hola19 [13] 4 ABC 7 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "20 F TO F F F popl07 [31] 2 ABC 7 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "20 F prog4 [31] 3 ABC 8 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "13 F F F sum1 [14] 3 ABC 6 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "17 F trex3 [14] 8 ABC 9 8.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "Non octagonal: hola15 [13] 3 conj.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "Modulus: hola02 [13] 4 conj.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "06 F NA F F F hola06 [13] 4 conj.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "82 F NA F F F hola22 [13] 4 conj.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "Non octagonal modulus: hola34 [13] 4 ABC 6 1.", "startOffset": 30, "endOffset": 34}, {"referenceID": 30, "context": "[31] used the greedy set cover algorithm SC to learn invariants in the form of arbitrary Boolean combinations of linear inequalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Our algorithm based on decision trees is simpler than the set cover algorithm, works better on our benchmarks (which includes most of the benchmarks from [31]), and scales much better to large sample sets of test data.", "startOffset": 154, "endOffset": 158}, {"referenceID": 30, "context": "This is because the greedy algorithm for set cover takes time Ophnq where h is the number of hyperplanes, and [31] considers one hyperplane for every candidate slope and sample point, yielding h \u201c mn.", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "The ICE-learning framework [14] solves this problem by formulating invariant generation as a more general classification problem that also accounts for implication constraints between points.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "The paper [14] also proposes a concrete algorithm for inferring linear invariants that fits into the ICE-learning framework (referred to as ICE in our evaluation).", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "If we compare the complexity of learning given a fixed sample, our algorithm performs better than [14] both in terms of running time and expressiveness of the invariant.", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "The ICE algorithm of [14] iterates through templates for the invariant.", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "Another ICE-learning algorithm based on randomized search was proposed in [30] (the algorithm MCMC in our evaluation).", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "These parameters have to be fixed for the algorithm in [30].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Decision trees have been previously used for inferring likely preconditions of procedures [28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "In particular, the algorithm proposed in [28] only learns formulas that fall into a finite abstract domain (Boolean combinations of a given finite set of predicates), whereas we use decision trees to learn more general formulas in an infinite abstract domain (e.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "Other techniques for inferring such invariants include predicate abstraction [16] as well as abstract interpretation techniques such as disjunctive completion [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "Other techniques for inferring such invariants include predicate abstraction [16] as well as abstract interpretation techniques such as disjunctive completion [10].", "startOffset": 159, "endOffset": 163}, {"referenceID": 2, "context": "However, for efficiency reasons, many static analyses are restricted to inferring conjunctive invariants in practice [3,11].", "startOffset": 117, "endOffset": 123}, {"referenceID": 10, "context": "However, for efficiency reasons, many static analyses are restricted to inferring conjunctive invariants in practice [3,11].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": "There exist techniques for recovering loss of precision due to imprecise joins using counterexample-guided refinement [1, 21, 26].", "startOffset": 118, "endOffset": 129}, {"referenceID": 20, "context": "There exist techniques for recovering loss of precision due to imprecise joins using counterexample-guided refinement [1, 21, 26].", "startOffset": 118, "endOffset": 129}, {"referenceID": 25, "context": "There exist techniques for recovering loss of precision due to imprecise joins using counterexample-guided refinement [1, 21, 26].", "startOffset": 118, "endOffset": 129}], "year": 2015, "abstractText": "The problem of inferring an inductive invariant for verifying program safety can be formulated in terms of binary classification. This is a standard problem in machine learning: given a sample of good and bad points, one is asked to find a classifier that generalizes from the sample and separates the two sets. Here, the good points are the reachable states of the program, and the bad points are those that reach a safety property violation. Thus, a learned classifier is a candidate invariant. In this paper, we propose a new algorithm that uses decision trees to learn candidate invariants in the form of arbitrary Boolean combinations of numerical inequalities. We have used our algorithm to verify C programs taken from the literature. The algorithm is able to infer safe invariants for a range of challenging benchmarks and compares favorably to other ML-based invariant inference techniques. In particular, it scales well to large sample sets.", "creator": "TeX"}}}