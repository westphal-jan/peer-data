{"id": "1501.05290", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2015", "title": "Managing large-scale scientific hypotheses as uncertain and probabilistic data", "abstract": "In view of the paradigm shift that makes science ever more data-driven, in this thesis we propose a synthesis method for encoding and managing large-scale deterministic scientific hypotheses as uncertain and probabilistic data. In the form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomena. For computing predictions, however, deterministic hypotheses are used asymmetrically as functions. We build upon Simon's notion of structural equations in order to extract the (so-called) causal ordering embedded in a hypothesis structure (set of mathematical equations).", "histories": [["v1", "Wed, 21 Jan 2015 20:46:23 GMT  (1504kb,D)", "https://arxiv.org/abs/1501.05290v1", "139 pages, 58 figures, 1 table. PhD thesis, National Laboratory for Scientific Computing (LNCC), Brazil, January 2015"], ["v2", "Thu, 12 Feb 2015 20:52:29 GMT  (1503kb,D)", "http://arxiv.org/abs/1501.05290v2", "145 pages, 61 figures, 1 table. PhD thesis, National Laboratory for Scientific Computing (LNCC), Brazil, February 2015"]], "COMMENTS": "139 pages, 58 figures, 1 table. PhD thesis, National Laboratory for Scientific Computing (LNCC), Brazil, January 2015", "reviews": [], "SUBJECTS": "cs.DB cs.AI cs.CE", "authors": ["bernardo gon\\c{c}alves"], "accepted": false, "id": "1501.05290"}, "pdf": {"name": "1501.05290.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bernardo Gon\u00e7alves"], "emails": [], "sections": [{"heading": null, "text": "National Laboratory for Scientific ComputingGraduate Program in Computational ModelingManaging large scale science hypotheses as uncertainty and probabilistic dataByBernardo Gonc, RJ - BRASILFEBRUARY - 2015ar Xiv: 150 1,05 290v 2 [cs.D B] 12 February 2015MANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES ASUNCERTAIN AND PROBABILISTIC DATABernardo Gonc, alvesTHESIS SUBMITTED TO THE EXAMINING COMMITTEE IN PARTIAL FULFILLMENT OF THE REQUIREMENTS AND PROBILISTIC DATABernardo Gonc."}, {"heading": "Acknowledgments", "text": "In fact, the fact is that most of them are able to survive on their own and that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 4", "text": "The vision of the DB formulates the problem of hypotheses coding as a problem of probabilistic database design. This gives rise to a number of technical questions. We will now present the technical context, the materials and methods that were identified and selected in this work as the basis for the implementation of the DB vision with regard to probabilistic database design. In the following, we will outline the technical research questions that are to be answered by the core of the thesis. 1.1. Problem area and specific objective of this thesis was to examine the capabilities of probability databases in order to enable hypotheses data management as a special case of simulation data management. In the continuation, we will first characterise the application case of hypotheses data management and then formulate it with regard to probabilistic DB design."}, {"heading": "1.1.1 Simulation data management", "text": "Simulation laboratories provide scientists and engineers with very large, potentially huge datasets that reconstruct phenomena of interest in high resolution. Famous examples include the John Hopkins Turbulence Databases [6] and the Neuroscience Simulation Datasets of the Human Brain Project (HBP) [7]. A key motivation for providing such data is to facilitate new insights and discoveries by hypotheses testing against observations. Nevertheless, the application case for exploratory analytics is currently well understood and many of its challenges are already being overcome, so that high-resolution simulation data is increasingly accessible [8, 9], only recently, within the framework of this theory, the application of hypotheses management to predictive analytics is taken into account [10]. Indeed, there is an urgent call for innovative technology to integrate (whether preserved) data and (simulated) theories into a uniform framework [11, 12, 13]."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 5", "text": "discovery) and predictive analytics (justification correlation) and emphasizes the loop between the formulation of hypotheses and testing [15].Simulation data generated and tuned from a combination of theoretical and empirical principles have a characteristic feature that needs to be taken into account in comparison with data generated by high-throughput technology in large-scale scientific experiments. They have a pronounced uncertainty component that motivates the use of hypotheses data management for predictive analytics [10]. Essential aspects of hypotheses data management can be described as follows, as opposed to simulation data management - Table 1.1 summarizes our comparison. \u2022 Sample data. Hytheses management should not deal with the same data volume as in simulation data management for explorative analysis, but only with samples thereof. This is, for example, based on the architectural design of the particle physics experiment and the raw data volume of the STATS 71 (where the raw data volume is significantly reduced)."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 6", "text": "In this context, it should be noted that the data collected is only the fragment (sample) of the simulation data that corresponds to the results of the simulation for a comparative analysis. Thus, in \u00a7 6.2.2, we show a predictive analytical study obtained from the Virtual Physiological Rat Project (VPR1001-M), in which the simulation data (heart rates) from a baroreflex model are compared with observations on a Dahl-SS rat strain."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 7", "text": "D1D2.... Dpsim p i = 1RiETLferent of the simulation data management.An essential point that distinguishes the hypothesis management is that a fact or data unit is defined by its predictive content. I.e., any clearly predicted fact (with available data dependencies) is a claim. Accordingly, the data should be dissected and organized for a stress-centered access pattern."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 8", "text": "To anticipate Chapter 2, the synthesis method that we have developed in this work for processing hypotheses as uncertain and probable data includes a design theory pipeline (see Fig. 1.4) that extends the pipeline shown in Fig. 1.3."}, {"heading": "1.1.2 Probabilistic database design", "text": "Probabilistic databases (p-DB's) have evolved into mature technologies over the last decade with the advent of new data models and query processing techniques. [17] One of the most advanced probabilistic data models is the U-Relational Representation System, with its probabilistic world-set algebra (p-WSA) implemented in MayBMS. This is an elegant extension of the relational model for managing large-scale uncertain and probabilistic data described in this thesis. We look at U-relationships from the perspective of p-DB design, for which a noformal design methodology has been proposed so far. Despite the advanced state of probabilistic data management techniques, a lack of methodologies for systematic design of p-DB's could prevent wider application. The availability of design methods has been considered one of the most important success factors for the rapid growth of applications in the field of graphical models (GM's), which are considered as research in [DB's]."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 9", "text": "The first is less systematic, since the user must model the data and correlations by guiding the entire p-DB construction process (MayBMS \"use cases [18] are illustrated in this way, for example); the second includes analytical techniques to extract the data and learn correlations from external sources, possibly unstructured, into a p-DB under some ad hoc scheme. This is the prevailing current application motivated by information extraction and data integration [17, pp. 10-3]. In this thesis, we present a third-type methodology for extracting data dependencies from some pre-existing formal specifications (the mathematical structure of the hypothesis) in order to algorithmically synthesize a p-DB. Such a construction method is successful, e.g. for building Bayesian networks [19]."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 10", "text": "It is expected that the implicit causal dependencies in the given hypotheses structure will be processed. If necessary, we will present such concepts in detail in the context."}, {"heading": "1.1.3 Structural equations", "text": "The flattening of application mathematical models into the p-DB hypothesis is not easy, however. The aim of this thesis was to examine appropriate abstractions of mathematical models to grasp their semantics (in part) to an extent that is tailored to the management of hypotheses (as opposed to, say, the model solution).We will abstract mathematical models into intermediates that can be further encoded in fd '.In fact, in a landmark article, Simon introduced an asymmetrical, functional relationship between variables that establishes a (so-called) causal order. [24] This became known as structural equation models (REM) or simply as \"structural equations\" (cf. also [25]).Along these lines, our goal is to extract the causal order implicit in the structure of a deterabilic hypothesis that represents a causal theory that we consider as the basis for collecting causal information."}, {"heading": "1.1.4 Uncertainty Model", "text": "In uncertain and probabilistic data management, there are essentially two sources of uncertainty: incompleteness (missing data) and diversity (contradictory data). The kind of uncertainty that this paper deals with is the multitude of hypotheses studies that have been identified to aim at the same phenomenon record, that is, the uncertainty arises from the existence of competing hypotheses. If multiple hypotheses and studies are inserted for the same phenomenon, the system interprets them as the definition of a probability distribution."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 11", "text": "Such a probability distribution (generally uniform) on the multiplicity of hypotheses is in accordance with probability theory under semantics of possible worlds [17, chap. 1]. It is modelled into the U-relational data model and its p-WSA operators and implemented into the MayBMS system as we will see it in \u00a7 5.1.8. For example, the overall actor, despite the name, performs standard (non-Bayesian) probability conclusions on such a probability distribution. Ultimately, however, there is a need to condition the initial probability distribution in the presence of observations. We will then apply Bayesian conclusions to conditioning so that the previous probability distribution can be updated to a posterior. Informal discussion of this section opens the way for a number of technological research questions that we outline nex.RQ5."}, {"heading": "1.2. THESIS STATEMENT 12", "text": "At the conceptual level? Do we now have the technical means to speak of hypotheses that are \"good\" in terms of the principles of the philosophy of science? The core of this thesis is dedicated to answering these questions, and we will carry them out in chapters 3, 4 and 5.1.2. This thesis states that it is possible to effectively encode and manage large deterministic scientific hypotheses as uncertain and probable data. Their central challenges are both conceptual and technical. Conceptually, we offer basic, non-obvious abstractions to define and encode hypotheses as data. Technically, we provide a set of algorithms that assemble a design theory pipeline to encode hypotheses as uncertain and probable data, and verify their efficiency and correctness. The applicability and effectiveness of our method is demonstrated in realistic case studies of computer-aided science."}, {"heading": "1.3. THESIS CONTRIBUTIONS 13", "text": "All performance tests carried out in this work encompass our design theory techniques for coding and synthesizing databases with U-relational hypotheses. [27, 18] In terms of uncertainty and statistical analysis, we adhere to (i) certain forms of data diversity that constitute the model of uncertainty discussed in this work; then (ii) we perform probabilistic conclusions with reference to MayBMS; and (iii) finally (at the application level) we perform Bayesian conclusions so that a posterior probability distribution is spread through p-DB updates. We do not offer an additional form of uncertainty management. Rather, we manage the data extracted into the system (under user control) and process its uncertainty with respect to the specific sources of uncertainty identified in the DB (see Chapter 2, 1.3)."}, {"heading": "1.3.1 Innovative Contributions", "text": "This thesis presents the vision of hypotheses as data (and their application), the so-called \"DB vision.\" It was published in the Vision Track of VLDB 2014 [10], because of its (sic.) potentially highly effective visionary content. The innovative DB system was described in a \"System Prototype Demonstration Paper\" [28].9"}, {"heading": "1.3.2 Technical Contributions", "text": "Our detailed technical contributions (see chapters 3, 4 and 5) are formulated in a formal methodology for establishing p-DB hypotheses, which is described in a technical report [29].10 The methodology is yet to be published along with our realistic test bed scenarios and performance assessments. 9 Preliminary version available under CoRR abs / 1411.7419. 10 Preliminary version available under CoRR abs / 1411.5196."}, {"heading": "1.4. THESIS OUTLINE 14", "text": "The problem of encrypting a hypothesis \"like data\" such as the formal specification (set of mathematical equations) is presented and addressed by an encryption scheme that transforms the equations into fd's with guarantees regarding the maintenance of the hypothesis of causal structures. [Chapter 3] The problem of encrypting a hypothesis \"like data\" such as the formal specification (set of mathematical equations) is presented by an encryption scheme that causalizes the hypothesis of the causal structure of the hypothesis. [Chapter 4] The causal reasoning method of causal reasoning is presented as acyclic pseudo-transitive reasoning on the encoded fd's. It processes the hypothetical causal order to find the \"first causes\" for each of its predictive variables. Chapter 5 [DB] Problem is the presented uncertainty."}, {"heading": "2.1. RUNNING EXAMPLE 18", "text": "In database research (e.g. [36]), uncertainty is usually considered an undesirable property that impedes data quality. We refer to U-relations and p-WSA as implemented in MayBMS to demonstrate that the ability to introduce controlled uncertainty into an (otherwise complete) simulation dataset can be a tool for \"deep\" predictive analysis on a range of competing or alternative hypotheses. Fig.2.2 shows such a scenario of hypotheses \"as data that compete with each other to explain a phenomenon as data.\" As a roadmap for most of the remainder of the chapter, we claim that if hypotheses can be coded and identified (see Fig.2.2), and their uncertainty is quantified by some probability distributions (see Fig.2.4), they can be evaluated / classified and searched by the user under selectivity criteria. Furthermore, their probabilities can be considered (possibly) as evidence for the presence of the Earth (2.1)."}, {"heading": "2.1. RUNNING EXAMPLE 19", "text": "Due to the parameter uncertainty, six simulation tests are carried out for H1, and four each for H2 and H3. 2The construction of a data warehouse (DW) requires a simple user description of an investigation. This means that descriptive records of the dimensions of the phenomena and hypotheses (see Fig. 2.3) must first be inserted in such a way that basic referential limitations by the associated data sets (fact tables) are fulfilled. For example, each of the six data sets for the hypothesis H1 is to insert its id-ID = 1 as a foreign key from the table HYPOTHESIS further into its synthesized relations. Fig. 2.4 shows the \"large\" fact table H1 for the hypothesis, which is loaded with its trial data sets = 1. Although the table HYPOTHESIS appears faster than usual for some hypotheses, we must now consider all hypotheses as an equation of W questions."}, {"heading": "2.2. HYPOTHESIS ENCODING 20", "text": "2.2. Hypothesis EncodingWe aim to extract for each hypothesis a set of fd's from their mathematical equations. Suppose we get a series of equations of hypothesis H1 and let us examine the set of fd's we are aiming at. 8H1. Law of free falla (t) = g v (t) = \u2212 gt + v0 s (t) = \u2212 (g / 2) t2 + v0 t + s0\u0432 1 = {\u03c6 \u2192 g, \u03c6 \u2192 v0, g \u00b2 a, g \u00b2 a, g v0 t \u00b2 v, g v0 s0 x \u00b2. To derive from the equations of H1 an answer to their implicit data dependencies, we focus on their implicit data dependencies and get rid of constants and possibly complex mathematical constructs. Equation v (t) = gt + v0, e.g., written in this way (roughly), suggests that ability is predictable."}, {"heading": "2.3. REASONING OVER FD\u2019S 21", "text": "The central point here is that when the hypotheses structure (set of equations) is given in a machine-readable data format, the data are carefully derived from the equations of H2, H3. These, n.b., vary in the structure w.r.t. H1 (e.g., they include the parameter D, the diameter of the object).H2. Stokes' law H3. Velocity-squared Law a (t) = 0 a (t) = 0v (t) = \u2212 \u221a gD / 4.6 \u00d7 10 \u2212 4 v (t) t + s0,000 (t) t + s0\u04452 = 3 = (t)."}, {"heading": "2.4. UNCERTAINTY INTRODUCTION 22", "text": "In Figure 2.4, the predictive power {32,0, 32,2} is qualified by values of a system under the same pair (? 7 \u2192 1,? 7 \u2192 1), which should functionally determine it in H1. Therefore, we admit that a special attribute \"trial id\" tid \"is superimposed in H1 for a trivial repair, for the time being, until uncertainty can be introduced in a controlled manner by synthesis\" 4U. \"It is intended to identify simulation attempts and\" pretend \"that the integrity of the data is not lost. Under this imposed certainty, the raw simulation study data are safely loaded from the files (see Figure 2.4). Note: how\" certainty \"is kept at the expense of redundancy and, for the most part important, opacity for predictable analysis (since the inconsistency between hypotheses is isolated or hidden)."}, {"heading": "2.4. UNCERTAINTY INTRODUCTION 23", "text": "The formal semantics of p-DB's (cf. \u00a7 5.1) can be seen as generating the data cleanup. In the context of p-DB's [17], the data cleanup does not have to be done at once - which is more error-prone [37]. Rather, it can be carried out gradually, namely by keeping all mutually inconsistent tuples under a probability distribution (ibid.) that can be updated in the face of evidence until the probabilities of some tuples eventually tend towards zero in order to be eliminated. This motivates Remark 1.Remark 1 Let us consider U-relational tuples as a quantity distribution (ibid.) until the quantity of a phenomenon can ultimately be reduced to zero."}, {"heading": "2.4. UNCERTAINTY INTRODUCTION 24", "text": "Q1. Create Table Y1 [g] as a selection U.\u03c6, U.g (repair key \u03c6 in (selection \u03c6, g, counting (*) as Fr from the H1 group by \u03c6, g) weight by Fr) as U; the result set of Q1 is stored in Y1 [g], see Figure 2.6. Note that the possible values of g are mapped to random variable x1 and that Table H1 is considered as the source for a common probability distribution (based on the values of the input parameters of H1), which may not be uniform: We count the frequency Ffor every possible value of a u factor Zi Z (as for g in Q1) and pass it as an argument to the weight distribution by construct.So far, we have presented the method of u-factorization informally. Today, we are moving to u-factorization."}, {"heading": "2.5. PREDICTIVE ANALYTICS 25", "text": "Compare the relationships H1 [a] and Y1 [a]. Taking into account the correlations shown in the fd g table, we can transfer the uncertainty resulting from the hypothesis to a phenomenon, and the only parameter a makes sense, that is, to position tuples of Y1 [a] exactly in the space of possible worlds. A central point is that the entire synthesis process is usable for the algorithm design. Except for the user's description, the construction of the database is fully automated based on the hypotheses structure (equation law) and the raw hypothesis study data. 2.5. Predictive analyses of the users of Example 1 must be able to predict the phenomenon = 1 w.r.. Q."}, {"heading": "2.5. PREDICTIVE ANALYTICS 26", "text": "The user can make informed decisions in the light of such confidence aggregates, which ultimately have to be conditioned in the light of evidence (observed data). Example 2 shows such Bayesian conditioning for discrete random variables associated with the possible values of predictive attributes (such as position s) whose domain is continuous. Example 2 Suppose that the position s = 2250 feet is observed at t = 3 seconds, with standard deviation \u03c3 = 20. Then Prior is updated to posterior by applying Bayes \"theorem for the normal mean with a discrete preceding value [16] (see Fig. 2.8)."}, {"heading": "2.6. RELATED WORK 27", "text": "The method uses the normal density function (2,1) with (say) \u03c3 = 20 to obtain the probability f (y | \u00b5k) of any alternative prediction of s from Y [s] as the mean of y for observed s = 2250. Bayes \"rule (2,2) p (y | k) = f (y | p) [16].f (y | k) = 1 270 p (2,2) then applies. In the general case (see examples from chapter 6) we actually have a phenomenon\" as data: \"a sample of independent observed values y1,..., yn (e.g., Brazil's population observed by census over the years)."}, {"heading": "2.6. RELATED WORK 28", "text": "Let us now discuss the work that we understand mainly in the context of our vision of data-driven hypotheses management and analysis."}, {"heading": "2.6.1 Models-and-data", "text": "Haas et al. [35] offer an original long-term perspective on the evolution of database technology. They characterize the data typically administered by traditional DB systems as a record of the past, not as a conclusion or insight or solution (ibid.). In the context of scientific databases, their position suggests that DB technology was designed for empirical data, not the theoretical data generated by simulation based on domain-specific principles or scientific hypotheses. They recognize that current DB technology has elevated the art of scalable \"descriptive\" analysis to a very high level. However, they point out that nowadays (sic.) what companies really need is a \"prescriptive\" analysis to identify optimal business, policy, investment and engineering decisions in the face of uncertainty. Such analyses, in turn, are based on deep \"predictive\" analyses that go beyond purely statistical predictions, and a basic understanding of the system's behavior."}, {"heading": "2.6.2 Scientific simulation data", "text": "As already mentioned, the ETL of science is characterized by its rare, only gradual updates and by large raw files as data sources [8]."}, {"heading": "2.6. RELATED WORK 29", "text": "The extreme magnitude of the raw data has motivated such unconventional approaches to data exploration, namely \"immersive\" query processing (move the program to the data) [6, 40] or \"in situ\" query processing in the raw files [41, 42], both using the spatial structure of the data in their indexing schemes. This research line is motivated to equip scientific end-users for immediate interaction with their very large simulation datasets. 14 The NoDB approach argues in particular to eliminate such an ETL phase (i.e. load) for direct access to data \"in situ\" in the raw data datasets."}, {"heading": "2.6.3 Hypothesis encoding", "text": "Our framework is comparable to the initiatives of bioinformatics, which deal with the coding of hypotheses in the RDF data model [43]: (i) the robot scientist [44] is a knowledge base system (KBS) for the automated generation and testing of hypotheses 14. Sometimes it is said: \"Here are my files, here are my queries, where are my results?\" [41]."}, {"heading": "2.7. SUMMARY: KEY POINTS 30", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they want to do, and that they need to do it in order to understand what they need to do. (...) Most people who are able to understand themselves are able to understand the things they need to do in order to understand what they need to do. (...) Most people who are able to understand the things they need to do in order to understand what they can do. (...) The people who are able to understand what they need to do in order to understand what they need to do. (...) The people who are able to understand the things they need to do are able to understand the things they can do. (...) The people who are able to understand what they need to do in order to understand what they have to do. (...) The people who are able to understand the world are able to understand what they have to do."}, {"heading": "2.7. SUMMARY: KEY POINTS 31", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 33", "text": "To emphasize the point, we consider Newton's second law F = ma in such a scale formation. The modeler can use it either to calculate (predict) acceleration values, for example, acceleration values that have a lot of mass and different force intensities, or to predict force intensities that have a fixed acceleration (for example, to test a technical dynamometer). The point is that Newton's equation is not sufficient to derive predictions, that is, it has a number of variables | V | = 3 greater than | E | = 1. It must be completed with two more equations to be considered as (applied) hypothesis \"data.\" Although it is usually interpreted as asymmetry versus a technically decisive semantics, there is nothing in its semantics that so 1 Compare the two systems given in Figure 3.1.2 that does not appear as the sum of the order triggering an E."}, {"heading": "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 34", "text": "We say that S is self-contained or complete when | E | = | V |.In short, we are interested in systems of equations that are \"structural\" (Def. 1) and \"complete\" (Def. 2), namely that there are so many equations as variables and no subset of equations has fewer variables than equations. [3] Complete structures can be solved for unambiguous sets of their variables. [4] However, this work is not about solving sets of mathematical equations at all, but about processing their causal order in terms of U-relational DB design. Simon's concept of causal order has its roots in econometric studies (cf."}, {"heading": "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 35", "text": "Simon has informally described an algorithm (cf. [24]) that can be used in a complete structure S (E, V) > > | to calculate a partial causal mapping of partitions within the set of equations to equal-cardinal partitions within the set of variables. As Dash and Druzdzel [48] have shown, the causal mapping returned by Simon's (so-called) causal order algorithm (COA) is not complete if S has variables that are strongly coupled (because they can only be determined simultaneously), and they have shown that any complete mapping via S must be consistent with the partial mapping of COA (COA), which is partially done by construction (merging strongly coupled variables into partitions or clusters) to force its induced causal graph Gp to acyclic mapping."}, {"heading": "3.2. THE PROBLEM OF CAUSAL ORDERING 36", "text": "Algorithm 1 COAt as a variant of Simon's COA.1: Procedure COAt (S: structure over E and V) Required: S given is complete, i.e., | E | = | V | Ensure: Returns causal total mappings: E \u2192 V2: do 4: Sk \"Sk\" Sk \"S.\" Minimal structures S \"can be found in S 5: V\" S \"(V) 6: do 7: x\" any arbitrary x \"V\" 8: x \"for all f\" S \"(E). Since x4 and x5 are strongly coupled (see Fig.3.2b), COAt presents them arbitrarily (e.g. it could be a problem that results in 4\" x5 \")."}, {"heading": "3.2. THE PROBLEM OF CAUSAL ORDERING 37", "text": "However, Simon's approach, as we will see below, is not the only way to deal with the problem of causal order. In fact, in order to study the arithmetical properties of SEM and the problem of causal order, we can find that any structure S (E, V) satisfying Def. 1 can be directly modelled as a two-part graph G = (V1, V2, E), where the set E of equations and the set V of variables are the disjunctive vertex propositions, i.e., V1 7, V2, V2, V and E 7, and E 7, \u2192 S is the set of edges that connects equations with the variables occurring in them. Fig. 3.4 shows the two-part graph G corresponding to the structure in Example 3 - for a comprehensive text on graphics concepts and the related algorithmic problems, cf."}, {"heading": "3.2. THE PROBLEM OF CAUSAL ORDERING 38", "text": "(i.e., with the lowest K \u2265 2) in its corresponding two-sided diagram (e.g., see Figure 3.4). Here, in Def. 6, we take a specific notion of pseudo-biclique.Def. 6 Let G = (A-B, E) be a two-sided diagram. We say that G is a K-balanced pseudo-biclique problem (BPBP) if | A-B | = K with | E-2K and, for all wells u-A-B, deg (u) \u2265 2.Now, we (originally) give the balanced pseudo-biclique problem (BPBP) as the decision problem. (BPBP) Using a two-sided diagram G = (V1, V2, E) and a positive integer K \u2265 2, G contains a K-balanced pseudo-biclique problem (BPBP). (BPBP) 1 The balanced pseudo-biclique problem is a pseudo-BP."}, {"heading": "3.3. TOTAL CAUSAL MAPPINGS 39", "text": "Nevertheless, the problem of causal order can be efficiently solved by finding another, less notorious approach based on nayak [49], which we introduce and build on. 3.3. Total causal allocations The problem of causal allocation can be solved in polynomial time by (i) finding a complete causal representation of what is given by the structure S (cf. Def. 5); and then (ii) by calculating the transitive allocation C + E of the defined structures C (cf. Equation 3.1) of the direct causal dependencies induced by the structure S."}, {"heading": "3.3. TOTAL CAUSAL MAPPINGS 40", "text": "For a given structure S, there may be several total causal mappings via S (recall example 3), but the causal order of S must be unique (see Fig. 3.3). Therefore, one question that arises is whether the transitive closure C + E is equal for any total causal mappings via S. Proposition 1, originally from Nayak [49], ensures that this is the case. Proposition 1 Let S (E, V) be a structure, and 1: E \u2192 V and 2: V are any total causal mappraisals via S. Then C + 1 = C + 2.Proof 3 The evidence is based on an argument from Nayak [49], which we present in arguably a much clearer way (see Appendix, \u00a7 A.1.2). Intuitively, it shows that the mappraisals via the variable are an equation f."}, {"heading": "3.3. TOTAL CAUSAL MAPPINGS 41", "text": "In this thesis we assume the causal mapping of the Hopcroft-Karp algorithms [57], which are known to be polynomial, surrounded by O (v1).6 The problem of finding maximum agreement is a well-studied algorithmic problem. [6] This is that we are transferring the problem of total agreement of the causal causal causal causal causal causal causal causal causal.2 (see Alg. 2) to the problem of maximum agreement in a two-part Grapitel (v1).7 This is that we are transferring the problem of total agreement of the total causal causal causal causal causal causal causal causal causal causal causal.7 This is the problem of total agreement of the total causal causal causal causal causal causal causal causal causal causal causal causal causal causal.2"}, {"heading": "3.4. THE ENCODING SCHEME 43", "text": "3.4. The Encoding SchemeWe will encode variables as relational attributes and assign equations ontofd's by total causal mappings. Let Z be a set of attribute symbols, so that Z'V, where S (E, V) is a complete structure; and let \u03c6, \u0432 / \u0394Z be two special attribute symbols preserved to identify (or) phenomena and hypotheses. We will explicitly distinguish symbols in Z, which are assigned by the user to the structure S, from epistemological symbols \u03c6 and \u0432. Then let us consider a sense of Simons in the nature of scientific modeling and intervention [24], summarized in Def. 8.Def. 8 Let S (E, V) be a structure and x (V) be a structure and x (V) be an equation. We say that x is an exogenous system, if there is an equation (f), {f), ars {= V."}, {"heading": "3.4. THE ENCODING SCHEME 44", "text": "This means that the hypotheses identifier \u0432 captures the semantics of the data level of the hypotheses equation. 7We encode complete structures in fd sentences using (Alg. 3) h encoding. Fig.4.1 represents a fd set of defined V variables, h encoding (S), encoding the same structure S from Example 3.Algorithm 3 Hypothesis encoding 1: Procedure h encoding (S: Structure over E and V, D: Domain variables) Required: S is given a complete structure, i.e., | E | = | V | Note: Returns a non-redundant fd set 1: Procedure h encoding (S: Structure over E and V, D: Domain variables). Required: S is given a complete structure, i.e., | E | = | V | Note: Returns a non-redundant fd set: Procedure h, and Domain variables (V)."}, {"heading": "3.5. EXPERIMENTS 45", "text": "(a) Each fd in \u03a3 has the form X \u2192 A, where | A | = 1; (b) For no < X, A > \u03a3 we have (\u03a3 \u2212 {< X, A >}) + = \u03a3 +; (c) for each fd X \u2192 A in \u03a3 there is no Y-X that (\u03a3\\ {X \u2192 A}) + = \u03a3 +. For an fd set that satisfies such properties (Def. 9), we say that it (a) singleton-rhs, (b) is not redundant and (c) is left-reduced. It is said that it has an attribute A in X that is \"foreign.\" w.r.t. If it is not left-reduced (Def. 9-c) [22, p. 74]. Finally, an fd is X-Y-shaped procedure that is trivial when Y X. Note: The action of a trivial fd structure in a fd."}, {"heading": "3.6. RELATED WORK 46", "text": "We have performed ten runs for each order of magnitude tested, and this corresponds to its mean run time in ms.8 | | The diagram is shown in Fig. 3.8 in logscale base 2. Indeed, an approximate sub-square slope is expected for the curve structure length. These scalability results are consistent with the computational complexity of h-coding, which is limited (see Conclusion 1) by Intel. 93.6. Related work models for modeling physical and socioeconomic systems as equations are a traditional modeling approach, and a very large number of models exist to this day. Simon's early work on structural equations and causal order includes a specific notion of causality aimed at further contributing to the potential of such a modeling approach (cf. [59]). It is designed to identify causal influences between variables (or their values) that are implicit in the system."}, {"heading": "3.6. RELATED WORK 47", "text": "Nevertheless, there are two important differences that need to be emphasized: \u2022 Such work is predominantly devoted to dealing with (statistical) qualitative hypotheses and not (deterministic) quantitative hypotheses; \u2022 The causal model is taken for granted or derived from data, rather than being converted or synthesized from a series of equations.These are the two core differences that also apply to our work in relation to the mass of existing work in probabilistic DB. However, our main point here is to clarify the technical context and state of the art of the causal order problem. A few papers deal with the extraction of a causal model from an earlier formal specification, such as a set of equations. This is one reason why causal order is a problem of causal order that is still scarcely investigated."}, {"heading": "3.7. SUMMARY OF RESULTS 48", "text": "We process the causal order of a hypothesis structure (abstracted as SEM) in relation to acyclic causal reasoning about fd's and prove its correctness, made possible by the encryption scheme presented in this chapter. \"In this chapter, we have examined the design-theoretical properties held by such an encoded fd theorem. We list the results we have achieved as follows: \u2022 Through Theorem 1, we know that Simon's causal order of a structure is irrevocable; \u2022 by drawing on the work of Simon [24] and Nayak [49] (cf. Proposals 1 and 2), we have an approach to process the causal order of a structure."}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 50", "text": "R5 \u2192 Y and Y \u00b7 W, then XZ \u2192 W. In the face of an fd sentence, one can obtain \u03a3 +, the closure of \u03a3, by a finite application of the rules R0-R5. We are concerned with arguing about an fd set to process its implicit causal order, which, as we will see in \u00a7 4.3, can be executed in terms of (pseudo) transitive thinking. Note that R2 is a special case of R5, if Z = \u2205, then we will refer to R5 reasoning and understand R2. The next definition opens up a way to develop a causal process + very efficient. Let us be a fd setting on attributes U, with X U. Then X +, attributing X w.r.t., the set of attributes A is as follows: < X, X, A > formances +. Bernstein has long predefined algorithms (algorithm to compose X + 4) to XClosure."}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 51", "text": "The similarity between this kind of reasoning and the causal reasoning will be shown shortly in the following. < 10 Let's set a series of Fd's on attributes U. Then the pseudo-transitive application of rule R5 to Fd's in \u03a3. In this case we can write X. \u2212 \u2192 Y and omit \"w.r.t. \u03a3,\" if it can be understood from context. We are indeed interested in a very specific correct subset of rule R5 on Fd's in \u03a3. \u2212 In this case we can make a \"compact\" representation of the causal order implicit in \u03a3. Note: To characterize such a special subset, we must be careful. \u2212 r.t. The presence of cycles in the causal order is. Def. 11 Let's set a series of Fd's on attributes implicit."}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 52", "text": "We illustrate in the following some steps of thinking for the partial calculation of a feature folding taking into account the subset of fd's in \u03a3 with \u03c6 / \u0394X.1. x1 x2 x3 x5 \u0432 \u2192 x4 [given] 2. x1 x3 x4 \u0432 \u2192 x5 [given] 3. x5 \u0432 \u2192 x7 [given] 4. x1 x3 x4 \u0432 \u2192 x7 [R5 over (2), (3)] 5. x1 x2 x3 x5 \u0432 \u2192 x7 [R5 over (1), (4)]"}, {"heading": "6. \u2234 x1 x2 x3 x4 \u03c5 \u2192 x7 [R5 over (2), (5)] .", "text": "Note that (6) is still suitable for another application of R5, say about (1) to derive (7) x1 x2 x3 x5 \u0432 \u2192 x7. Although (1) and (6) have the form (1) X \u2192 A and (6) Y \u2192 B with Y, we also have X. \u2212 \u2192 Y, which characterizes a cycle that does not fetch anything in Y. Indeed, if we consider only the form (1-3) of fd, then (6) even Def. 11 suffices and is then folded. The same applies, for example, to (1) by an empty application of R5. 2Lemma 2 Let S (E, V) be a complete structure, a total causal mapping over S and a fd set encoded by the given S. If < X, A >, then A #, the attribute folding of A exists (w.r.t.) and is equal over Annex 7 and a d-set encoded by the given S."}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 53", "text": "At its core lies (Alg. 6) the folding, which can be understood as a non-obvious variant of XClosure (cf. Alg. 4), which was designed for acyclic pseudo-transitive thinking. To calculate the folding of attribute A in fd < X, A > accuration, the algorithm Afolding can be regarded as tracing the implicit causal order in the direction of A. Similarly, in relation to the directed graph induced by the causal sequence (cf. Fig. 3.3), this graph sequence would include traversal to identify the nodes x-V that have xa-V in their attainability, i.e. x-xa."}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 54", "text": "Algorithm 6 Folding an attribute w.r.t. to fd set.1: Procedure AFolding (\u03a3: fd set, A: Attribute) Require: \u03a3 is sparsimonious Ensure: Returns A #, the Attribute Folding of A (w.r.t.) 2: 0 0 0 0 6: while size < A? | do. If A (i + 1) = A (i) 7: size \u2190 A # stores attrs. A is determined that the attribute Folding of (cf. \u00a7 4.3) 5: size \u2190 0 6: while size < | do."}, {"heading": "4.3. EQUIVALENCE WITH CAUSAL ORDERING 55", "text": "That is, AFolding can be implemented to run in linear time in | S | 2Corollary 2. Let S (E, V) be a complete structure, and specify a fd group that is encoded. Then, Folding (\u03a3) algorithm correctly computes \u03a3 #, the folding of a group in time that is f (| S |), where f (S |) represents the temporal complexity of (Alg. 6) AFolding.Exhibit 9 See Appendix, \u00a7 A.2.3. 2Finally, it will be expedient to come up with an idea of parsimonious fd groups (see Def. 13), which represents a distinguishing feature of such mathematical information systems compared to arbitrary information.Definition 9 See Appendix, \u00a7 A.2.3 Let us specify the fd group on meaningful fd groups. Then let us say that it is parsimonionic when it is a mathematical information system distinguishing from such a feature."}, {"heading": "4.3. EQUIVALENCE WITH CAUSAL ORDERING 56", "text": "Theorem 4 Let S (E, V) be a complete structure, \"a total causal figure over S and\" a fd set encoded by \"given S. Then xa, xb and V are such that xb is causally dependent on xa, i.e. (xa, xb) that there is some non-trivial fd < X, B >, B with A-X, where B 7 \u2192 xb and A-7 \u2192 xa.Production of 11 We examine the statement by induction. We first consider the\" if \"direction and then its\" only if \"vice versa. See Annex \u00a7 A.2.5. 2Def. 14 then gives useful terminology for a nice concept towards our chapter. Def. 14 Let S (E, V) be a structure with variables xa, xb and V, and\" only if \"s verse converse."}, {"heading": "4.3. EQUIVALENCE WITH CAUSAL ORDERING 57", "text": "Figure 4.2 illustrates the projection of an fd set and the folding that is applied over such a fd subset to calculate the first causes of endogenous variables.Lemma 3 Let S (E, V) be a complete structure, namely a complete causal figure over S and an fd set encoded by the given S set. Then, a variable xa-V can only be a first cause of a variable xb-V, where < X, B >, and B 7 \u2192 xb, A 7 \u2192 xa, if either (i) A-X or (ii) A / x, but it is a < Z, C > B with A-Z and C-X.Proof 13 We prove the statement by construction of Theorem 4, see Annex A.2.7. 2Finally, Theorem 5 clarifies the purpose of the folding and its meaning in relation to causal order. < Theorem 5 Let S > jb be a complete structure."}, {"heading": "4.4. EXPERIMENTS 58", "text": "Note 5 Observe that, on the one hand, the objective of calculating the transitive catch C + 0 of a series of induced causal dependencies C is to derive the entire causal order of a given structure. On the other hand, the goal of folding is not to make all variables (attributes) of a given variable (attribute) causally dependent, but only all their initial causes (if any).2Specifically, the results just shown include a method to calculate all their initial causes for each endogenous variable (attribute).This is a core objective of the argumentation method developed in this chapter to enable the automatic synthesis of hypotheses as uncertain and probable (U-relational) data. 4.4. Experimental Fig. 4.3 shows the results of experiments we have performed to investigate how effective the causal reasoning on fd is. 4.4 The causal reasoning on fd is in practice, in particular its behavior for which we have generated the hypotheses 4.3."}, {"heading": "4.5. RELATED WORK 59", "text": "A still unexplored problem in database research literature (thinking about fd's is discussed in detail in Maier [22]).In recent years, some basic work on causality in databases has emerged [64]. They are motivated to improve the usability of the database by providing explanations for query answers (and non-answers) to the user. Essentially, the idea of AI causality work is borrowed (cf. \u00a7 3.6) to identify the causal order between tuples, which can be computationally expensive, since the database instance can be very large. In conjunctive queries, the system should be able to explain to the user which tuples \"caused\" this answer, or why possibly expected tuples are missing. This requires a causal chain of tuples for a given query, which can be very expensive, as the database instance can be very extensive. In conjunctural queries, the specific tuples [65] is addressed very efficiently."}, {"heading": "4.6. SUMMARY OF RESULTS 60", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able, that we are able, that we are able, that we are able, that we are able, that we are able, to be able, to be able, to be able, to be able, to be able, to be able, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position."}, {"heading": "5.1. PRELIMINARIES: U-RELATIONS AND PROBABILISTIC WSA 62", "text": "of the relationships Ri1,..., R i m and numbers 0 < p [i] \u2264 1 such that [1 \u2264 i \u2264 n p [i] = 1. Element Ri1,..., R i m, p [i], W is a possible world, where p [i] is its probability [18].Probabilistic world set algebra (p-WSA) consists of the operations of rela-tional algebra, an operation to calculate the tuple trust conf, and the repair key operation to introduce uncertainty - by moving to alternative worlds as a maximum subset of repairs of an argument key [18].Leave R '[ U] a relationship and XA U. For any world < R1,..., p >.W, leave A-U only contain numerical values that are Jig and leave R'die fd (U\\ A)."}, {"heading": "5.2. RUNNING EXAMPLE 63", "text": "If k = 0 or '= 0 (or both), then R or S (or both) are classical relations, but the above paraphrasing rules apply accordingly. All that is rewritten is economical translation (sic. [18]): the number of algebraic operations does not increase and each of the operation selection, projection and connection remains the same. Query plans are hardly more complicated than input queries. In fact, it has been confirmed from the outset that relational database query optimizers perform well in practice.For a comprehensive overview of U relations and p-WSA, refer to the query plans [18]. In this thesis, we consider U relations from the perspective of the p-DB design, for which no methodology has yet been proposed."}, {"heading": "5.2. RUNNING EXAMPLE 64", "text": "\"Variable x is a function of the time t given initial condition x0.\" \"x\" = \"b\" (5,2) x \"b\" (1 \u2212 x / K) x (5,3) x \"x\" (b \u2212 py) y \"=\" y \"(rx \u2212 d) (5,4) The models are completed (by the user) with additional equations to provide the values of exogenous variables (or\" input parameters \"), 2 e.g. x0 = 200, b = 10, so that we can SEM\" s (or) Sk \"(Ek, Vk) for k = 1.. 3, \u2022 E1 = {f1 (t), f2 (x0), f3 (b), f3 (b), f4 (b), f4 (b)."}, {"heading": "5.2. RUNNING EXAMPLE 65", "text": "In view of the \"large\" Fact Table H3, the p-DB synthesis consists of two main parts: to process the \"empirical\" uncertainty present in the \"large\" Fact Table and to synthesize (dissect) it into independent U-factors (U-factorization), and then precisely transfer it into the prediction data (U-propagation)."}, {"heading": "5.3. U-FACTORIZATION 66", "text": "5.3. U-FactorizationAs we have seen in \u00a7 5.1, the repair key operation allows to create a discrete random variable to repair an argumentation key in a given relationship. Our goal is to develop a technique to perform such an operation in principle for the management of hypotheses. It is a basic design principle to have exactly one random variable for each particular uncertainty factor (\"u-factor\" in short) that requires careful identification of the actual sources of uncertainty present in the relationships H. The multiplicity of (competing) hypotheses is itself a standard variable, namely the theoretical u-factor. Consider an \"explanation table\" such as H0 in Fig. 5.1, in which all available hypotheses and their target phenomena are stored. We can take such a H0 as an explanation table for the three hypotheses of H0 and the hypotheses of H0."}, {"heading": "5.3. U-FACTORIZATION 67", "text": "U-factor learning is meant to process only the attributes Z-U from Hk [U], which are interpreted as exogenous in the given hypothesis, i.e. for all A-Z there is one fd < X, A > GP-K, the latter being the \u03c6 projection of GP-K. Such attributes are then \"officially\" unrelated. In fact, we mean \"casual\" fd correlations, which may occasionally appear in the experimental input data in a number of studies; e.g. x0 \u2194 y0 apply in H3, but not because x0 and y0 are (theoretically) related in principle. Figure 5.4 helps illustrate problem 1 with the \"large\" fact table. We emphasize u-factors {b, d} and {p, r} in the colors green and red. Note that values of b strongly correlate (one-to-to-to-one) with values of D for GM = 1, exactly like p, d and r."}, {"heading": "5.3. U-FACTORIZATION 68", "text": "To illustrate, we look at Hypothesis 3 and its experimental input data recorded in H3 in Fig. 5.3. We show its resulting fd quantity 3 in Fig. 5.5 (left), together with its folding of fd's with equivalent left sides. The latter is then entered in Fig. 5.5 (right) to obtain the final information required for the actual synthesis of U relations as shown in Fig. 16. To obtain an illustration of the merging of fd's with equivalent left sides, note in Fig. 5.5 (right) that in Fig. 0 b p t x contains the final information (Fig. # 3) +.Def. 16 Let Sk and Hk leave the complete structure and \"large\" table of facts of the hypothesis and in Fig."}, {"heading": "5.3. U-FACTORIZATION 69", "text": "Note 6 Let us define the u-factorization of the structure Sk via the \"large\" fact table Hk. This is ensured by the merging algorithm that groups fd's into Hk with equivalent left sides. 2We are now able to apply an idea of u-factor decomposition that is inDef. 17 into the query formula (5,5) in p-WSA's extension of relational algebra.Def. 17 Let Sk be the complete structure of the hypothesis, and Hk [U] its \"large\" fact table so that it is the u-factorization of Sk over Hk. Now let Ga'U be a set of attributes Ga = AG so that for all B-G there is an fd factorization."}, {"heading": "5.4. U-PROPAGATION 70", "text": "We remember that all the mechanisms developed so far, from the hypothesis to the causal reasoning for u-factorization to u-factorization, are used to enable predictive analysis, so there should be exactly one fd < X > attribute included. # In Theorem 5, for every first cause xb # xc, there is B-X where B-xb is xb-xc. Now, note that when u-factor learning is done, it is partially processed with fd's from fd's from fd's (and partially with fd's from fd's). < Z-Z it is only when done with u-factor learning, it is partially processed with fd's from fd's, and partially with fd's from fd's."}, {"heading": "5.4. U-PROPAGATION 71", "text": "All of this (cf. Def. 19) is abstracted into general p-ESC query factors (5,6) and synthesized in (Alg. 8) to achieve u-factorization (Part II). Def. 19 Let Sk be the complete structure of the hypothesis k, and Hk [U] its \"large\" fact table, so that \"k\" is the u-factorization of Sk over Hk. Now we define the U relation Y j k [Vj Dj | S T] by the query formula (5,6), and say that \"Y\" is a predictive projection of Hk, where: Y \"T\" is a fd (Y0). / (. / i \"I\" Y attributes Y \"k\" i \"k\" k \"k\" k \"k\" k \"k\" k \": Y-k\" i. \""}, {"heading": "5.4. U-PROPAGATION 72", "text": "Fig. 5.7 shows the rendered U relations for hypothesis k = 3, whose \"big fact table\" is shown in Fig. 5.3. Note that tid = 6 in H3 now corresponds \u03b8 = {x1 7 \u2192 3, x2 7 \u2192 1, x3 7 \u2192 3, x4 7 \u2192 2}, where \u03b8 defines a certain world in W, the probability of which is Pr (\u03b8) \u2248.055. This value is derived from the boundary probabilities given in the world table W (see Fig. 5.7) as a result of the application of the formulas Equation 5.1 and Equation 5.5.Note 7 that although (Alg. 8) the synthesis for each hypothesis k works locally, the effects of the p-DB synthesis (U-Intro) in the pipeline are global due to the (global) \"explanation\" relationship H0 (then U-relation Y0), e.g. Fig. 5.7."}, {"heading": "5.5. DESIGN-THEORETIC PROPERTIES 73", "text": "Typical queries include the overall process conf (), in which the probability (or trust) for each tuple in the probability space covered by the hypothesis contest is \"true.\" Queries are illustrated in Chapter 6.5.5. Design Theoretical Properties In order for the U-intro method to make sense, we must examine design-theoretical properties of U-factor projections and predictive projections synthesized from the \"large\" fact table Hk for the sake of predictive analytics. In particular, to make the predictions stress-centered, we contend that they should meet Boyce-Codd normal form (BCNF, cf. Def. 21) w.r.t. the repaired factorization of Hk; and in order for them to represent a correct decomposition of the uncertainty present in Hk, their connection should be loss-free (the data in Hk, Def. Def."}, {"heading": "5.5.1 Claim-Centered Decomposition", "text": "As pointed out in note 6, each fd in the u factorization table Hk is a claim (cf. note 6), then the same applies to the repaired factorization Hk. Therefore, for a claim-centered decomposition of the \"large\" fact table Hk, it is desirable that the U-relational scheme Y k, that it satisfies BCNF w.r.t., is mixed with the uncertainty of another claim. Def. 20 Let R [U] be a relationship scheme over fixed U of attributes and a set of fd's. Then the projection of fd's on R [U], written as \"ltU,\" is mixed with the uncertainty of another claim. Def. 20 Let R [U] be a relationship scheme over fixed U of attributes and a set of fd's. Then the projection of fd'R on [U] is all of the attributes."}, {"heading": "5.5. DESIGN-THEORETIC PROPERTIES 74", "text": "(b) A scheme R is in BCNF if all its schemes R1,..., Rn and R are included in BCNF. Example 7. To illustrate the concept of the BCNF, consider the canonical fd prediction, since B \u2192 B, B \u2192 C} is about the attributes U = {A, B, C} and a preliminary scheme that contains a single relationship R [ABC]. This relationship is not in BCNF, because B \u2192 C violates it (C * B, but B is not a super key for R). 2Note also that an overcomposed scheme (trivial) can satisfy the BCNF.For example, let it apply = {A \u2192 B, C} then by Def. 21 both schemes R = R1 [ABC] and R \u2032 R = {R1 [AB], R2 [AC]} the superordinate schemes in BCNF w.r.t. The second scheme, however, breaks the data into two tables, making it more difficult to access."}, {"heading": "5.5.2 Correctness of Uncertainty Decomposition", "text": "It should be remembered from the preliminary stages (cf. \u00a7 5.1) that the U-relational equivalent of the relational product operation (main sub-operation of the joining operation) was introduced. Now, we provide the classical definition of a lossless joining [20], i.e.,"}, {"heading": "5.5. DESIGN-THEORETIC PROPERTIES 75", "text": "Def. 22 Let R [U] be a (U-) relational schema synthesized into collection R = n i = 1Ri and let \u03a3 be an fd set on attributes U. We say that R has a lossless join w.r.t. \u03a3 if for every instance r of R [U] satisfactors, we have r =. / ni = 1 \u03c0Ri (r).The lossless join property is of interest, that our decomposition ofthe data from the \"big\" fact table into u-factor projections so that they join to \"annotate\" the predictive projections when propagated by the U-relational join operation is correct. Theorem 7 Guaranteed. Sk 7 Let Sk be the case.Theorem 7 Let Sk be the complete structure of hypothesis k, and Hk [U] its big projectionsate. \""}, {"heading": "5.6. RELATED WORK 76", "text": "It is well known that correct processing of the causal order (cf. the results of chapter 4), in total Theorem 7, guarantees that the first causes are correctly linked to the predictive variables they affect. 2As we have seen, the p-DB synthesis technique presented here is essentially oriented towards design-theoretical properties. It is also motivated by computational power, since the decomposition of uncertainty is also desirable in order to speed up probabilistic processing [17, pp. 30-1]. In fact, the U-Intro method is entirely based on U-relations and p-WSA as implemented in the MayBMS system. Its computational performance is dominated by U-relation query processing. We present experimental studies on the U-Intro method in \u00a7 6.4, since they are designed from an applicability point. The aim is to provide some reference computational measures for prospective users."}, {"heading": "5.6. RELATED WORK 77", "text": "We point to the classical dependency theory and the U-relational operations (i.e. the uncertain introduction operator) to systematically reconstruct the p-DB from scratch. We focus on extracting and processing fd's in the direction of a factorized U-relational schema. The synthetic schema is in BCNF and has a lossless connection. Despite some major differences, our synthesis method builds on the classicist theory of relational schema design by synthesis. [62] Classical design by synthesis was once criticized for its too strong \"uniformity\" of the fd assumption. [72, p. 443] because it reduces the problem of design to symbolic schema design by neglecting semantic problems."}, {"heading": "5.7. SUMMARY OF RESULTS 78", "text": "This year it is more than ever before."}, {"heading": "6.2.1 Case: Hemoglobin Oxygen Saturation", "text": "In this case, we emphasize the potential of data-driven hypotheses analysis versus handmade (visual) curve fit. We examine three different hypotheses that perform \"narrowly\" visually relative to their target phenomena dataset, see Figure 6.1. All of them have been empirically (separately) adapted to the observations (\"R1s1\" dataset) as best as possible and are now compared with each other in global view3 http: / / www.physiome.org / jsim / docs / MML _ Intro.html. 4 http: / / www.physiome.org / Models / modelDB /."}, {"heading": "6.2. CASE STUDIES 81", "text": "Example 8 The resources of this example are shown in Fig. 6.2. We consider the entries of the physiomatic model described in connection with HYPOTHESIS, which are related to the phenomenon described in relation to PHENOMENONE (cf. explanatory relationship H0). For each hypothesis, a single hypothesis study (its best fit) is taken into account. 2"}, {"heading": "6.2. CASE STUDIES 82", "text": "Encoding. The fd encoding of hypotheses is shown in Fig. 6.3, Fig. 6.4 and Fig. 6.5.Symbol mappings. As we have seen, the insertion of hypotheses study data sets requires the indication of a target phenomenon and the corresponding mappings from the hypotheses symbols to the symbols of the target phenomena. In this case, we have: \u2022 M287 \u2192 1 = {pO2 7 \u2192 pO2, SHbO2 H 7 \u2192 SHbO2}; \u2022 M317 \u2192 1 = {pO2 7 \u2192 pO2 Ad 7 \u2192 SHbO2}; \u2022 M327 \u2192 1 = {pO2 7 \u2192 pO2, SHbO2 D 7 \u2192 SHbO2};"}, {"heading": "6.2. CASE STUDIES 83", "text": "Hypothesis management. Query Q1 illustrates the peculiarity of the hypotheses management for this case. We consider that the user is interested in all SHbO2 predictions about a subset of the pO2 domain. Its result set is presented in Figure 6.6.Q1. (select phi, upsilon, tid, \"pO2,\" \"SHbO2 H\" as SHbO2 of Y28, where phi = 1 and \"pO2\" > = 20 and \"pO2\" < = 40) Association of all (select phi, upsilon, tid, \"\" pO2, \"\" SHbO2 Ad \"as SHbO2 of Y31,\" where phi = 1 and \"pO2\" > = 20 and \"pO2\" < = O2, \"pO2,\" pO2 \"pO2,\" pO2 \"pO2,\" pO2 \"pO2,\" pO2, \"pO2\" pbO2 D \"as SHbO2 of Y31,\" pO2 \"pO2\" and \"pO2,\" pO2 \"pO2\" pO2, \"pO2\" pO2 \"pO2,\" pO2 \"pO2,\" pO2 \"pO2\" pO2, \"pO2\" pO2 \"pO2,\" pO2 \"pO2,\" pO2 \"pO2\" pO2, \"pO2\" pO2, \"pO2\" pO2 \"pO2,\" pO2 \"pO2\" pO2, \"pO2\" pO2, \"pO2\" pO2 \"pO2,\" pO2 \"pO2,\" pO2 \"pO2\" pO2 \"pO2,\" pO2 \"pO2,\" pO2 \"pO2\" pO2, \"pO2\" pO2, \"pO2\" pO2 \"pO2,\" pO2 \"pO2,\" pO2 \"pO2\" pO2, \"pO2\" pO2, \"pO2,\" pO2 \"pO2,\" pO2 \"pO2,\" pO2 \"pO2,\" pO2, \"pO2,\" pO2, \"pO2,\" pO2, \"pO2\" pO2, \"pO2\" pO2, \"pO2,\" pO2, \"pO2,\" pO2, \"pO2,\" pO2, \"pO2"}, {"heading": "6.2. CASE STUDIES 84", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.2 Case: Baroreflex Dysfunction in Dahl SS Rat", "text": "This case is taken from the Virtual Physiological Council project, 5 where we show the potential of data-driven hypothesis management and analysis for modeling. Fig. 6.8 shows the best fit of a baroreflex model to an observational dataset obtained through experiments on Dahl SS rats [76]. We, in turn, use the DB to perform such hypothesis management and analysis. We generate 1K studies through a parameter sweep script and insert them into the database. The best fit is then automatically inserted by Bayesian Inference.Example 9 The resources of this example are shown in Fig. 6.9. We look at the single hypotheses entry described in relation to HYPOTHESIS, and the phenomenon described in relation to PHENOMENON. By parameter sweep, 1K studies are inserted into the DB for management and analytics. 2Encoding. The fd coding of the hypothesis is shown in 6.b."}, {"heading": "6.2. CASE STUDIES 85", "text": "Symbol mappings. We consider that the user provides symbol mappings: \u2022 M10017 \u2192 2 = {time 7 \u2192 time, HR 7 \u2192 HR}; hypotheses management. In query Q2, we consider that the user is interested in times when the heart rate is higher than a threshold, say, 300 beats / min. The result set is shown in Figure 6.10.Q2. Select phi, upsilon, tid, \"time,\" \"HR\" from Y1001, where phi = 2 and \"HR\" > = 300 order after \"time,\" tid; hypotheses analysis. Figure 6.11 shows the results of the analysis of the phenomenon \u03c6 = 2 after conditioning the probability distribution in the presence of observations (\"SSBN9 HR\" dataset). Since in this case it is model matching, i.e. 1K slightly different parameter settings, the study ranking is determined by small differences in the probability distribution (Figure 6.11)."}, {"heading": "6.2.3 Case: Myogenic Behavior of a Blood Vessel", "text": "Computer models of physiology can take into account a variety of effects that take place at different levels of biological organization, from the organ to the cell."}, {"heading": "6.2. CASE STUDIES 86", "text": "Typically, a complex model is developed gradually, for example by adding details to an existing model or expanding its dimensionality (e.g. by extending it from a stationary to a dynamic representation of phenomena).In this case study (cf. example 10), we consider alternative models of the myogenic behavior of a human blood vessel."}, {"heading": "6.2. CASE STUDIES 87", "text": "Example 10 (see Fig. 6.14): We look at the entries of the physiome model displayed in connection with HYPOTHESIS and two phenomena (see relation PHENOMENON). One study is shown for Hypothesis \u0432 = 60 and two for Hypothesis \u0432 = 89. 2Coding. The fd coding of hypotheses \u0432 {60, 89} is shown (or) in Fig. 6.15 and Fig. 6.16 Symbol Mappings. We consider that the user provides symbol mappings: \u2022 M607 \u2192 1 = {t 7 \u2192 time, D 7 \u2192 diameter}; \u2022 M897 \u2192 1 = {t 7 \u2192 time, D 7 \u2192 diameter};"}, {"heading": "6.2. CASE STUDIES 88", "text": "Hypothesis management. Query Q3 illustrates the peculiarity of hypotheses management for this case. The user selects all diameter forecasts within the time interval t [100, 300] (see diagram in Fig. 6.13). The set of results is shown in Fig. 6.17."}, {"heading": "6.3. SYSTEM PROTOTYPE 89", "text": "Q3. select phi, upsilon, tid, \"t,\" \"D\" from Y60 claiming 1 where phi = 3 and \"t\" > = 100 and \"t\" < = 300 Union select all phi, upsilon, tid, \"t,\" \"D\" from Y89 claiming 1 where phi = 3 and \"t\" > = 100 and \"t\" < = 300 order of \"t,\" upsilon, tid; Hypothesis Analytics. Fig. 6.18 shows the results of the analysis of the phenomenon \u03c6 = 3 after conditioning the probability distribution in the presence of observations, namely \"Davis Sikes Fig. 3 Myo DigData\" datasetet.In this case study, two preliminary models were considered under a uniform previous probability distribution updated to a subordinate distribution. Note that although the hypothesis \u00dc = 60 concentrated its probability weight in a single study, the Bayesian conclusion was considered as the prototype for page 89 of the best explanation for the motyp = 3 of the system."}, {"heading": "6.3. SYSTEM PROTOTYPE 90", "text": "We have developed a demonstration of this prototype (see [28]), in which we go through the entire design-by-synthesis pipeline (Fig. 1.4) and examine application scenarios. In this section, we will show a brief demonstration of the system in the scenario of population dynamics previously presented in this thesis. The demonstration extends over three phases. In the first phase, we will show the ETL process to give a sense of what the user has to do in relation to simple phenomena, hypotheses naming and file uploading so that their phenomena and hypotheses are available in the system as data. In the second phase, we will reproduce some typical queries of the hypotheses management (such as those shown in the previous section). In the third phase, we will enter the hypotheses analysis module. The user will select a phenomenon for a hypotheses evaluation study, and the system will list all predictions with their probabilities based on some selectivity data (e.g., based on the 1920 years of their observational criteria)."}, {"heading": "6.3.1 Demo Screenshots", "text": "Figure 6.21 shows screenshots of the system. Figure 6.21 (a) shows the research projects currently available to a user. Figure 6.21 (b, c) shows the ETL interfaces for defining phenomena and hypotheses data (by synthesis) and then the insertion of hypotheses study data sets, i.e. explanations of a hypothesis towards a target phenomenon. Figure 6.21 (d) shows the interface for basic hypotheses management by listing the predictions of a given simulation study. Figure 6.21 (e, f) shows two registers of the hypotheses analysis module, namely the selection of observations and then the consideration of the corresponding alternative predictions, which are sorted according to their conditioned probabilities."}, {"heading": "6.3.2 Demo Case: Population Dynamics", "text": "In this case, we refer to a well-known problem from computer science, namely population dynamics scenarios to demonstrate the prototype of the DB system. Fig. 6.19 shows census data from the USA from 1790 to 1990.6 Fig. 6.20 shows observational data from Hudson's Bay from 1900 to 1920 on the Lynx hare population [77].6 Cf. https: / / www.census.gov / population / censusdata / table-4.pdf."}, {"heading": "6.3. SYSTEM PROTOTYPE 91", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3. SYSTEM PROTOTYPE 92", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3. SYSTEM PROTOTYPE 93", "text": "Example 11 (see Fig. 6.22): We look at the model entries and two phenomena displayed in connection with HYPOTHESIS (see relation PHENOMENON). \u03c6 = 1 is represented by three studies for economic value hypothesis = 1 and six for economic value hypothesis = 2. \u03c6 = 2 is represented by two studies for economic value hypothesis = 1 and economic value hypothesis = 2 and six studies for economic value hypothesis = 3. Note the data definition interfaces in Fig. 6.21 (b, c). 2Coding. The fd coding of hypotheses of higher economy is shown (or) in Fig. 6.23, Fig. 6.24 and Fig. 6.25. See hypotheses structure processing in Fig. 6.21 (c).Symbol mappings."}, {"heading": "6.3. SYSTEM PROTOTYPE 94", "text": "\u2022 M17 \u2192 1 = {t 7 \u2192 year, x 7 \u2192 population}; \u2022 M27 \u2192 1 = {t 7 \u2192 year, x 7 \u2192 population}; \u2022 M17 \u2192 2 = {t 7 \u2192 year, x 7 \u2192 Lynx}; \u2022 M27 \u2192 2 = {t 7 \u2192 year, x 7 \u2192 Lynx}; \u2022 M3 7 \u2192 2 = {t 7 \u2192 year, x 7 \u2192 Lynx}; Hypothesis management. Query Q4 illustrates the peculiarity of hypotheses management in this case. The user selects hypotheses: \"t,\" \"y,\" \"x\" from the Lotka-Volterra model and filters his available data for experimental purposes tid = 6 on phenomenon? p = 2. Both the form-based query setup and its result set are shown in Figure 6.21 (d)."}, {"heading": "6.4. EXPERIMENTS 95", "text": "6.4. Experiments The efficiency and scalability of the U-Relational Representation System and the p-WSA query algebra have been comprehensively demonstrated [27]. D-DB's as the U-Relational Hypothesis of DB's must therefore be as efficient and scalable as any U-Relational DB. In these experiments (see Figure 6.28), we offer some measurements of the functionality of B-DB's method in the particular context of our physical test bed in the real world. Our goal here is to give a concrete sense of how efficient the B-DB methodology can be. However, most of these tests (the four diagrams below in Fig 6.28) involve the data level and then require more hardware. Our current test setup (Personal Computer) 7 allows us to achieve a scale 7. These experiments were conducted on an Intel Core i5 2.3GHZ / 4GB Mac running time (BMX 6.X and May3) with a BMX 10.X and a BMX 10.X."}, {"heading": "6.4. EXPERIMENTS 96", "text": "For the first two graphs (XML extraction and encoding), we have collected the response time to the measurement of interest over different structure lengths. Each of these graphs corresponds to a real physiom hypothesis from the table in Fig. 6.29. The last hypothesis in this table, o = 379, is used for testing the last four graphs in Fig. 6.28, namely u-learning, u-factorization, u-propagation and conditioning. We have outlined a different number of studies (n trials), each with 1MB. The last test in each of these four graphs, with 1K trials, processes 1 GB of uncertain data at a time and then fits into the machine's main memory. We interpret the performance results shown in these graphs as follows for each measurement of performance. \u2022 Extraction. Some fluctuations may be due to the practicality of XML-DOM methods."}, {"heading": "6.4. EXPERIMENTS 97", "text": "HYPOTHESIS: name | S | | E | 186 Regulatory Vessel 40 20 89 Myo Dyn Resp wFit 73 28 60 Myogen Compliant Vessel 100 38 75 Baroreceptor Lu et al 2001 153 74 70 4-State Sarcomere Energetics 298 91 120 Comp four gen weibel lung 440 186 91 Cardiopulmonary Mechanics 1132 412 93 CardiopulmonMechGasBloodExch 1593 525 153 HighlyIntegHuman 1624 538 154 HighlyIntHuman wIntervention 1919 634 379 Baroreflex SB CT 171 74"}, {"heading": "6.5. DISCUSSION 98", "text": "The latter is the most expensive sub-method of the synthesis method. \u2022 Conditioning. The conditioning method is used for a selected phenomenon.It consists of four main parts. First, it performs a probabilistic inference sub-query by operation conf () on the correct predictive projection of the \"large\" fact table of each hypothesis associated with the phenomenon. Second, it combines the results of each such sub-query by unifying all queries whose result is a multi-hypothetical prediction table. Third, it loads the observational data and the predictive data from the multi-hypothesis table into memory to apply Bayesian inference.Finally, the previous probability distribution of the previous prediction table is updated with the posterior and all corresponding marginal probabilities are updated in their original U-relational tables."}, {"heading": "6.5. DISCUSSION 99", "text": "It therefore has the potential to be a step toward higher standards of reproducibility and scalability. Realistic assumptions. The core assumption of our framework is that the hypotheses are given in a formal specification that is codable into a complete SEM (fulfills Defs. 1, 2). Also, as a semantic assumption that is standard in scientific modelling, we consider a one-to-one correspondence between real world entities and variable / attribute symbols within a structure, and that all of them are in some of their equations / fd's. For most scientific use cases involving deterministic models (if not all), such assumptions are perfectly reasonable. It may be a subject of future work (cf. \u00a7 7.3) to explore business use cases as equivalent."}, {"heading": "6.5. DISCUSSION 100", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.6. CONCLUSIONS 101", "text": "Several types of dynamic systems can be modeled in this formalism. Ap-plications have grown from the gene regulatory network into social networking and stock market prediction analytics. 6.6. Conclusions In this chapter we have demonstrated and discussed the applicability of the HDB methodology. We have referred to application scenarios from the Physiom Research Project. We have shown in some detail the process of building a HDB with representative models from the Physiom Model Repository. This qualitative assessment is followed by experiments that provide a concrete sense of the performance of the HDB for models with up to 600 + mathematical variables. Chapter 7Conclusions In this chapter we will reexamine the research questions of RNA theory."}, {"heading": "7.1. REVISITING THE RESEARCH QUESTIONS 103", "text": "If it is possible to evaluate / submit hypotheses in the presence of (some fragments) of evidence, this is significant. RQ3. Does each piece of simulated data qualify as a scientific hypothesis? What is the difference between managing \"simulation data\" and managing \"hypotheses\" as data? Early in Table 1.1, we provided a comparison between simulation data management and hypotheses data management. In addition, the scientific research process in Chapter 2 is abstracted as a well-defined problem of data cleansing. Hytheses are considered from an applied scientific perspective and then reduced to data so that a piece of simulation data is considered as a hypothesis whenever it is assigned to explain some specific phenomena. RQ4. Is there a proper (machine-readable) data format that we can use to extract automatically expressed hypotheses from Chapter 2."}, {"heading": "7.1. REVISITING THE RESEARCH QUESTIONS 104", "text": "RQ6. What is the link between SEM and fd's? Can we develop an en-coding scheme to \"orient equations\" and then effectively transform one into the other with guarantees? Once we do, what design-theoretical properties do such a set of fd's have? Also in Chapter 3, we have presented an algorithmic coding scheme to transform a SEM into a series of fd's with guarantees in terms of maintaining the hypothesis causal structure. Our study of this problem has put some interesting properties of the resulting fd's, in particular that they are always \"non-redundant\" and, compared to arbitrary information systems, more precise and economic in the sense that for each given attribute there is a fd with it in its rhetoric. RQ7."}, {"heading": "7.2. SIGNIFICANCE AND LIMITATIONS 105", "text": "We have shown that the synthesized p-DB scheme has both properties. Do we now have the technical means to speak of hypotheses that are \"good\" in relation to the principles of the philosophy of science? Equipped with the design-theoretical machinery proposed in this thesis, we are able to automatically (1) extract their causal order in an SEM, (2) recognize its strongly coupled components, and decide how they work for a given predictive power."}, {"heading": "7.3. OPEN PROBLEMS AND FUTURE WORK 106", "text": "* First synthesis method for the construction of p-DB's from some already existing formal specifications. \u2022 Definition of a concrete use case of data-driven hypothesis management and analysis. \u2022 New class of applications introduced for p-DB's. \u2022 Solution of the problem of Bayes' conditioning in p-DB's. Now some limitations of the thesis are listed. \u2022 The Bayesian inference is implemented at application level, but not formalized as a principal technical solution within p-DB's research. \u2022 The coding scheme for transforming the mathematical system of hypotesis into a set of fd's that allow the synthesis of p-DB is applicable only to structured deterministic models, not to stochastic ones.7.3. Open problems and future WorkOpen problems and issues of future work are listed (no particular order)."}, {"heading": "7.4. FINAL CONSIDERATIONS 107", "text": "(6) Developing automatic data collection techniques to make the data definition of both hypotheses and phenomena in the DB statistically useful.7.4. In this paper, we have developed a vision of the DB, which essentially represents the abstraction of hypotheses as uncertain and probable data. It includes a design-theoretical methodology for systematically constructing and managing the DB hypotheses. It is intended to provide a principled approach that enables scientists and engineers to manage and evaluate large scientific hypotheses as theoretical data (Rate / Rank).We have addressed some key technical challenges to the DB vision in order to properly encode deterministic hypotheses as uncertain and probable data. As envisaged by Jim Gray [1], the scientific method has shifted towards a data-driven discipline that is rapidly gaining ground [3]."}, {"heading": "Appendix A", "text": "\"We show that in every recursive step of the COA we must find all non-trivial subsets.\" (Def. 2) that a structure S (E, V) is complete when it turns into an optimization problem associated with the decision problem BPBP, which we know from Lemma 1 to be NP Complete. (Def. 2) that a structure S (E, V) is complete when the structure in Fig. 3.5 (left), Note (Def. 9), the minimal structure S (E), V), where E (f2, f3)."}], "references": [{"title": "The fourth paradigm: Dataintensive scientific discovery", "author": ["T. HEY", "S. TANSLEY", "K. TOLLE"], "venue": "Microsoft Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Distilling free-form natural laws from experimental data", "author": ["M. SCHMIDT", "H. LIPSON"], "venue": "Science, Washington, v. 324,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Data science and prediction", "author": ["V. DHAR"], "venue": "Communications of the ACM, v. 56,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Big data and its technical challenges", "author": ["H.V. JAGADISH", "J. GEHRKE", "A. LABRINIDIS", "Y. PAPAKONSTANTI- NOU", "J.M. PATEL", "R. RAMAKRISHNAN", "C. SHAHABI"], "venue": "Communications of the ACM, v. 57,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Computational Science: Ensuring America\u2019s competitiveness", "author": ["M.R. BENIOFF", "E.D. (Eds.). LAZOWSKA"], "venue": "PITAC (US President\u2019s Information Technology Advisory Committee),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Data exploration of turbulence simulations using a database cluster", "author": ["E. PERLMAN", "R. BURNS", "Y. LI", "C. MENEVEAU"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "The Blue Brain Project", "author": ["H. MARKRAM"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Managing scientific data", "author": ["A. AILAMAKI", "V. KANTERE", "D. DASH"], "venue": "Comm. ACM, v. 53,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Scientific data management at the Johns Hopkins Institute for  BIBLIOGRAPHY  109 Data Intensive Engineering and Science", "author": ["Y. AHMAD", "R. BURNS", "M. KAZHDAN", "C. MENEVEAU", "A. SZALAY", "A. TERZIS"], "venue": "SIGMOD Record, v. 39,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Beyond big data", "author": ["J.B. CUSHING"], "venue": "Computing in Science & Engineering, v. 15,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Point: Hypotheses first", "author": ["R. WEINBERG"], "venue": "Nature, London, v. 464,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Counterpoint: Data first", "author": ["T. GOLUB"], "venue": "Nature, London, v. 464,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Where is the brain in the Human Brain Project", "author": ["Y. FR\u00c9GNAC", "G. LAURENT"], "venue": "Nature, London,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A historical introduction to the philosophy of science", "author": ["J. LOSEE"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Introduction to bayesian statistics", "author": ["W.M. BOLSTAD"], "venue": "2nd. ed. Wiley- Interscience,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "MayBMS: A system for managing large uncertain and probabilistic databases", "author": ["C. KOCH"], "venue": "Managing and Mining Uncertain Data, Chapter", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Principles of Databases and Knowledge-Base Systems", "author": ["J. ULLMAN"], "venue": "Computer Science Press,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Theory of relational databases", "author": ["D. MAIER"], "venue": "Computer Science Press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1983}, {"title": "A call to arms: Revisiting database design", "author": ["A. BADIA", "D. LEMIRE"], "venue": "SIG- MOD Record, v. 40,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Causal ordering and identifiability", "author": ["H. SIMON"], "venue": "Studies in Econometric Methods,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1953}, {"title": "Causality: Models, reasoning, and inference", "author": ["J. PEARL"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "A survey of web information extraction systems", "author": ["C.-H. CHANG", "M. KAYED", "M.R. GIRGIS", "K. SHAALAN"], "venue": "IEEE Transactions on Knowledge and Data Engineering, v. 18,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Fast and simple relational processing of uncertain data", "author": ["L. ANTOVA", "T. JANSEN", "C. KOCH", "D. OLTEANU"], "venue": "Proc. of IEEE ICDE", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "\u03a5-DB: A system for datadriven hypothesis management and analytics", "author": ["B. GONCALVES", "F.C. SILVA", "F. PORTO"], "venue": "Technical report, LNCC,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Design-theoretic encoding of deterministic hypotheses as constraints and correlations in U-relational databases", "author": ["B. GONCALVES", "F. PORTO"], "venue": "Technical report, LNCC,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Cause and counterfactual", "author": ["H. SIMON", "N. RESCHER"], "venue": "Philosophy of Science, v. 33,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1966}, {"title": "ModelDB: A database to support computational neuroscience", "author": ["M. HINES", "T. MORSE", "M. MIGLIORE", "N. CARNEVALE", "G. SHEPHERD"], "venue": "J. Comput. Neurosci., v. 17,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "BioModels Database: A repository of mathematical models of biological processes", "author": ["V. CHELLIAH", "C. LAIBE", "N. Le Nov\u00e8re"], "venue": "Method. Mol. Biol.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Integration from proteins to organs: the Physiome Project", "author": ["P.J. HUNTER", "T.K. BORG"], "venue": "Nat. Rev. Mol. Cell. Biol.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Strategies for the Physiome Project", "author": ["J.B. BASSINGTHWAIGHTE"], "venue": "Ann. Biomed. Eng., v", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Data cleaning: Problems and current approaches", "author": ["E. RAHM", "H. Hai Do"], "venue": "IEEE Data Engineering Bulletin, v. 23,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Modeling and querying possible repairs in duplicate detection", "author": ["G. BESKALES", "M.A. SOLIMAN", "I.F. ILYAS", "S. BEN-DAVID"], "venue": "PVLDB, v. 2,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Data model for scientific models and hypotheses", "author": ["F. PORTO", "S. SPACAPPIETRA"], "venue": "The evolution of Conceptual Modeling,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Data-driven Neuroscience: Enabling breakthroughs via innovative data management", "author": ["A. STOUGIANNIS", "F. TAUHEED", "M. PAVLOVIC", "T. HEINIS", "A. AILA- MAKI"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "I/O streaming evaluation of batch queries for data-intensive computational turbulence", "author": ["K. KANOV", "E.A. PERLMAN", "R.C. BURNS", "Y. AHMAD", "A.S. SZALAY"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "NoDB: Efficient query execution on raw data", "author": ["I. ALAGIANNIS", "R. BOROVICA", "M. BRANCO", "S. IDREOS", "A. AILAMAKI"], "venue": "files. In: Proc. of ACM SIGMOD", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Representation of research hypotheses", "author": ["L. SOLDATOVA", "A. RZHETSKY"], "venue": "J. Biomed. Sem.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "The automation of science", "author": ["KING", "R. D"], "venue": "Science, Washington, v. 324,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "HyBrow: a prototype system for computer-aided hypothesis evaluation", "author": ["S RACUNAS"], "venue": "Bioinformatics, v. 20,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2004}, {"title": "SWAN: A distributed knowledge infrastructure for alzheimer disease research", "author": ["GAO Y"], "venue": "J. Web Semantics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "Hypotheses, evidence and relationships: The HypER approach for representing scientific knowledge claims", "author": ["A de WAARD"], "venue": "ISWC Proc. of Workshop on Semantic Web Applications in Scientific Discourse", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "A note on the correctness of the causal ordering algorithm", "author": ["D. DASH", "M.J. DRUZDZEL"], "venue": "Artif. Intell., v. 172,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2008}, {"title": "Causality in Bayesian belief networks", "author": ["M.J. DRUZDZEL", "H.A. SIMON"], "venue": "Proc. of Int. Conf. on Uncertainty in Artificial Intelligence", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1993}, {"title": "Biclustering algorithms for biological data analysis: A survey", "author": ["S.C. MADEIRA", "A.L. OLIVEIRA"], "venue": "IEEE Transactions on Computational Biology and Bioinformatics,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "Information-theoretic coclustering", "author": ["I.S. DHILLON", "S. MALLELA", "D.S. MODHA"], "venue": "Proc. of ACM SIGKDD", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2003}, {"title": "An efficient algorithm for solving pseudo clique enumeration problem", "author": ["UNO T"], "venue": "Algorithmica, v. 56,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2010}, {"title": "Computers and intractability: A guide to the theory of NP-completeness", "author": ["M.R. GAREY", "D.S. JOHNSON"], "venue": "1st. ed., Series of Books in the Mathematical Sciences", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1979}, {"title": "The NP-completeness column: an ongoing guide", "author": ["D.S. JOHNSON"], "venue": "J. Algorithms, v. 8,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1987}, {"title": "An n algorithm for maximum matchings in bipartite graphs", "author": ["J.E. HOPCROFT", "R.M. KARP"], "venue": "SIAM Journal on Computing,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1973}, {"title": "On the definition of the causal relation", "author": ["H. SIMON"], "venue": "The Journal of Philosophy,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1952}, {"title": "Automated modelling of physical systems", "author": ["P.P. NAYAK"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1996}, {"title": "Constraint management in conceptual design. In: Knowledge Based Expert Systems in Engineering: Planning and Design", "author": ["D. SERRANO", "D.C. GOSSARD"], "venue": "Computational Mechanics Publications,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1987}, {"title": "Synthesizing third normal form relations from functional dependencies", "author": ["P. BERNSTEIN"], "venue": "ACM Trans. on Database Systems,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1976}, {"title": "Computational problems related to the design of normal form relational schemas", "author": ["C. BEERI", "P. BERNSTEIN"], "venue": "ACM Trans. on Database Systems, v. 4,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1979}, {"title": "Causality in databases", "author": ["A MELIOU"], "venue": "IEEE Data Eng. Bull., v. 33,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2010}, {"title": "The complexity of causality and responsibility for query answers and non-answers", "author": ["A. MELIOU", "W. GATTERBAUER", "K.F. MOORE", "D. SUCIU"], "venue": "PVLDB, v. 4,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2010}, {"title": "Sensitivity analysis and explanations for robust query evaluation in probabilistic databases", "author": ["B. KANAGAL", "J. LI", "A. DESHPANDE"], "venue": "Proc. of ACM SIGMOD", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2011}, {"title": "Reverse data management", "author": ["A. MELIOU", "W. GATTERBAUER", "D. SUCIU"], "venue": "PVLDB, v. 4,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2011}, {"title": "TANE: An efficient algorithm for discovering functional and approximate dependencies", "author": ["Y HUHTALA"], "venue": "Computer Journal, v. 42,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1999}, {"title": "Modeling and reasoning with Bayesian Networks", "author": ["A. DARWICHE"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2009}, {"title": "Causal Networks: Semantics and expressiveness", "author": ["J.P. TOM S. VERMA"], "venue": "Proc. of the 4th Conf. on Uncertainty in Artificial Intelligence (UAI\u201988). North-Holland Publishing Co.,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1988}, {"title": "Schema design for uncertain databases", "author": ["A. Das Sarma", "J. ULLMAN", "J. WIDOM"], "venue": "Proc. of AMW", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2007}, {"title": "The decomposition versus synthetic approach to relational database design", "author": ["R. FAGIN"], "venue": "Proc. of VLDB", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1977}, {"title": "Making database systems usable", "author": ["JAGADISH", "H. V"], "venue": "In: SIGMOD", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2007}, {"title": "CRIUS: User-friendly database design", "author": ["L. QIAN", "K. LEFEVRE", "H.V. JAGADISH"], "venue": "PVLDB, v. 4,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2010}, {"title": "Conditioning probabilistic databases", "author": ["C. KOCH", "D. OLTEANU"], "venue": "PVLDB, v. 1,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2008}, {"title": "Identifying physiological origins of baroreflex dysfunction in salt-sensitive hypertension in the Dahl SS rat", "author": ["S.M. BUGENHAGEN", "A.W.J. COWLEY", "D.A. BEARD"], "venue": "Physiological Genomics,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2010}, {"title": "The ten-year cycle in numbers of the lynx in Canada", "author": ["C. ELTON", "M. NICHOLSON"], "venue": "Journal of Animal Ecology, v. 11,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1942}, {"title": "Predicting essential components of signal transduction networks: A dynamic model of guard cell abscisic acid signaling", "author": ["S. LI", "S.M. ASSMANN", "R. ALBERT"], "venue": "PLOS Biology,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In view of the paradigm shift that makes science ever more data-driven [1], in this thesis we demonstrate that large deterministic scientific hypotheses can be effectively encoded and managed as a kind of uncertain and probabilistic data.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Hypotheses can also be learned in large scale, as exhibited in the Eureqa project [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "In fact, we can refer nowadays to a broad, modern context of data science [3] and big data [4] in which the complexity and scale of so-called \u2018data-driven\u2019 problems require proper data management tools for the predicted data to be analyzed effectively.", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "In fact, we can refer nowadays to a broad, modern context of data science [3] and big data [4] in which the complexity and scale of so-called \u2018data-driven\u2019 problems require proper data management tools for the predicted data to be analyzed effectively.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": ") \u201ca rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems\u201d [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "It is generally considered that computational science models, interpreted here as hypotheses to explain real-world phenomena, are of strategic relevance [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].", "startOffset": 215, "endOffset": 221}, {"referenceID": 8, "context": "Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].", "startOffset": 215, "endOffset": 221}, {"referenceID": 9, "context": "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].", "startOffset": 137, "endOffset": 149}, {"referenceID": 10, "context": "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].", "startOffset": 137, "endOffset": 149}, {"referenceID": 11, "context": "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].", "startOffset": 137, "endOffset": 149}, {"referenceID": 13, "context": "discovery) and predictive analytics (context of justification), and highlights the loop between hypothesis formulation and testing [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Note that such a \u2018sampling\u2019 does not incur in any additional uncertainty as typical of statistical sampling [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "One of the state-of-the-art probabilistic data models is the U-relational representation system with its probabilistic world-set algebra (p-WSA) implemented in MayBMS [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "The first is the less systematic, as the user has to model for the data and correlations by steering all the p-DB construction process (MayBMS\u2019 use cases [18], e.", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "[20, 21, 22]) that are basic input to algorithmic synthesis.", "startOffset": 0, "endOffset": 12}, {"referenceID": 17, "context": "[20, 21, 22]) that are basic input to algorithmic synthesis.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "A related concept which is also a major one for us is that of normalization [20, 21, 22], viz.", "startOffset": 76, "endOffset": 88}, {"referenceID": 17, "context": "A related concept which is also a major one for us is that of normalization [20, 21, 22], viz.", "startOffset": 76, "endOffset": 88}, {"referenceID": 19, "context": "In fact, given a system of equations with a set of variables appearing in them, in a seminal article Simon introduced an asymmetrical, functional relation among variables that establishes a (so-called) causal ordering [24].", "startOffset": 218, "endOffset": 222}, {"referenceID": 20, "context": "also [25]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "Although we perform some sort of information extraction [26] for the acquisition of hypotheses from some model repositories on the web, it is very basic and ad-hoc in order to obtain a testbed for our method.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18].", "startOffset": 107, "endOffset": 115}, {"referenceID": 15, "context": "In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18].", "startOffset": 107, "endOffset": 115}, {"referenceID": 23, "context": "The innovative system of \u03a5-DB has been described in a \u2018system prototype demonstration\u2019 paper [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "Chapters 3, 4, and 5) are formulated into a formal method for the design of hypothesis p-DB\u2019s which is described in a technical report [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "High-throughput technology and large-scale scientific experiments provide scientists with empirical data that has to be extracted, transformed and loaded before it is ready for analysis [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 13, "context": "Hypotheses, however, are tentative explanations of phenomena [15], which characterizes a different kind of uncertain data.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": ", in the Eureqa project [2].", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "Scientific hypotheses are tested by way of their predictions [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "However, for computing predictions, deterministic hypotheses are applied asymmetrically as functions [30].", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "observable, but only their joint results (the predictions) [15].", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": ", [31, 32, 33]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 27, "context": ", [31, 32, 33]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 28, "context": ", [31, 32, 33]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 28, "context": "The Physiome project [33, 34], e.", "startOffset": 21, "endOffset": 29}, {"referenceID": 29, "context": "The Physiome project [33, 34], e.", "startOffset": 21, "endOffset": 29}, {"referenceID": 15, "context": "The vision of \u03a5-DB is currently set to be delivered on top of U-relations and probabilistic world-set algebra (p-WSA) [18].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": ", [36]), uncertainty is usually seen as an undesirable property that hinders data quality.", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "This may be non-obvious but is quite convenient a design decision for the envisioned system of \u03a5-DB because hypotheses, as (abstract) universal statements [15], can only be derived predictions from (be empirically grounded) by assigning (callibrating) them onto some real-world phenomenon.", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "1, U-relations have in their schema a set of pairs (Vi, Di) of condition columns [18] to map each discrete random variable xi created by the repair-key operation to one of its possible values (e.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "The 11 Hypotheses are \u2018universal\u2019 by definition [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "In the context of p-DB\u2019s [17], data cleaning does not have to be one-shot \u2014 which is more error-prone [37].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Note that it abstracts the goal of a data-intensive hypothesis evaluation study, or the scientific method itself [15], as the repair of each \u03c6 as a key.", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "That is illustrated by query Q3, which creates integrative table Y [s]; and by query Q4, which computes the confidence aggregate operation [18] for all s tuples where t = 3 (Fig.", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "Then, by applying Bayes\u2019 theorem for normal mean with a discrete prior [16], Prior is updated to Posterior (see Fig.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "2) to get the posterior p(\u03bck | y) [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": ", yn |\u03bck) for each competing trial \u03bck, is computed as a product \u220fn j=1 f(yj |\u03bckj) of the single likelihoods f(yj |\u03bckj) [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "The vision of managing hypotheses as data has some roots in Porto and Spaccapietra [38], who motivated a conceptual data model to support (the socalled) in silico science by means of a scientific model management system.", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "2 Scientific simulation data As previsouly mentioned, science\u2019s ETL is distinguished by its unfrequent, incremental-only updates and by having large raw files as data sources [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics.", "startOffset": 115, "endOffset": 118}, {"referenceID": 33, "context": "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics.", "startOffset": 152, "endOffset": 156}, {"referenceID": 5, "context": ", the \u2018immersive\u2019 query processing (move the program to the data) [6, 40], or \u2018in situ\u2019 query processing in the raw files [41, 42].", "startOffset": 66, "endOffset": 73}, {"referenceID": 34, "context": ", the \u2018immersive\u2019 query processing (move the program to the data) [6, 40], or \u2018in situ\u2019 query processing in the raw files [41, 42].", "startOffset": 66, "endOffset": 73}, {"referenceID": 35, "context": ", the \u2018immersive\u2019 query processing (move the program to the data) [6, 40], or \u2018in situ\u2019 query processing in the raw files [41, 42].", "startOffset": 122, "endOffset": 130}, {"referenceID": 35, "context": ", the loading) for a direct access to data \u2018in situ\u2019 in the raw data files [42].", "startOffset": 75, "endOffset": 79}, {"referenceID": 36, "context": "3 Hypothesis encoding Our framework is comparable with Bioinformatics\u2019 initiatives that address hypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses 14 Sometimes phrased \u2018here is my files, here is my queries, where are my results?\u2019 [41].", "startOffset": 140, "endOffset": 144}, {"referenceID": 37, "context": "3 Hypothesis encoding Our framework is comparable with Bioinformatics\u2019 initiatives that address hypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses 14 Sometimes phrased \u2018here is my files, here is my queries, where are my results?\u2019 [41].", "startOffset": 170, "endOffset": 174}, {"referenceID": 38, "context": "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.", "startOffset": 205, "endOffset": 209}, {"referenceID": 37, "context": "The Robot Scientist relies on rule-based logic programming analytics to automatically generate and test RDF-encoded hypotheses of the kind \u2018gene G has function A\u2019 against RDF-encoded empirical data [44].", "startOffset": 198, "endOffset": 202}, {"referenceID": 38, "context": "HyBrow is likewise, but hypotheses are formulated by the user about biological events [45].", "startOffset": 86, "endOffset": 90}, {"referenceID": 39, "context": "In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].", "startOffset": 20, "endOffset": 24}, {"referenceID": 40, "context": "In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].", "startOffset": 208, "endOffset": 212}, {"referenceID": 19, "context": "2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon\u2019s classical approach [24, 48] is intractable.", "startOffset": 154, "endOffset": 162}, {"referenceID": 41, "context": "2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon\u2019s classical approach [24, 48] is intractable.", "startOffset": 154, "endOffset": 162}, {"referenceID": 19, "context": "Given a system of mathematical equations involving a set of variables, to build a structural equation model (SEM) is, essentially, to establish a one-to-one mapping between equations and variables [24].", "startOffset": 197, "endOffset": 201}, {"referenceID": 19, "context": ", [24, 49, 48]) and adapt it for the encoding of hypotheses into fd\u2019s.", "startOffset": 2, "endOffset": 14}, {"referenceID": 41, "context": ", [24, 49, 48]) and adapt it for the encoding of hypotheses into fd\u2019s.", "startOffset": 2, "endOffset": 14}, {"referenceID": 19, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 79, "endOffset": 91}, {"referenceID": 20, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 79, "endOffset": 91}, {"referenceID": 41, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 79, "endOffset": 91}, {"referenceID": 19, "context": ", row multiplication by a constant) on the structure matrix may hinder the structure\u2019s causal ordering and then are not valid in general [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "[24]) that, given a complete structure S(E ,V), can be used to compute a partial causal mapping \u03c6p from partitions on the set of equations to same-cardinality partitions on the set of variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "As shown by Dash and Druzdzel [48], the causal mapping returned by Simon\u2019s (socalled) Causal Ordering Algorithm (COA) is not total when S has variables that are strongly coupled (because they can only be determined simultaneously).", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "They also have shown that any total mapping \u03c6 over S must be consistent with COA\u2019s partial mapping \u03c6p [48].", "startOffset": 102, "endOffset": 106}, {"referenceID": 43, "context": "line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in", "startOffset": 123, "endOffset": 131}, {"referenceID": 44, "context": "line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in", "startOffset": 123, "endOffset": 131}, {"referenceID": 45, "context": ", [54]) have come with the notion of pseudo-biclique (also called \u2018quasi-biclique\u2019), which is a relaxation of the biclique concept to allow some less rigid notion of connectivity than the \u2018complete connectivity\u2019 required in a biclique.", "startOffset": 2, "endOffset": 6}, {"referenceID": 46, "context": "Proof 1 We show (by restriction [55]) that the BPBP is a generalization of the balanced biclique problem (BBP), referred \u2018balanced complete bipartite subgraph\u2019 problem [55, GT24, p.", "startOffset": 32, "endOffset": 36}, {"referenceID": 41, "context": ", [48]) geared for reasoning over GM\u2019s.", "startOffset": 2, "endOffset": 6}, {"referenceID": 48, "context": "In this thesis we adopt the Hopcroft-Karp algorithm [57], which is known to be polynomial-time, bounded by O( \u221a |V1|+ |V2| |E|).", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Then we consider a sense of Simon\u2019s into the nature of scientific modeling and interventions [24], summarized in Def.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "We draw attention to the significance of Theorem 2, as it sheds light on a connection between Simon\u2019s complete structures [24] and fd sets [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "We draw attention to the significance of Theorem 2, as it sheds light on a connection between Simon\u2019s complete structures [24] and fd sets [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 49, "context": "[59]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[25, 19]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "Dash and Druzdzel revisit the problem and re-motivate it in light of modern applications [48].", "startOffset": 89, "endOffset": 93}, {"referenceID": 51, "context": "Inspired on Serrano and Gossard\u2019s work on constraint modeling and reasoning [61], Nayak reports an approach that is provably quite effective to process the causal ordering: extract a total causal mapping and then compute the transitive closure of the direct causal dependencies.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "\u2022 By building upon on the work of Simon [24] and Nayak [49] (cf.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "As usual notational conventions from the DB literature [20, 21], we write X, Y, Z to denote sets of relational attributes and A,B,C to denote singleton attribute sets.", "startOffset": 55, "endOffset": 63}, {"referenceID": 16, "context": "Functional dependency theory relies on Armstrong\u2019s inference rules (or axioms) of (R0) reflexivity, (R1) augmentation and (R2) transitivity, which forms a sound and complete inference system for reasoning over fd\u2019s [20].", "startOffset": 215, "endOffset": 219}, {"referenceID": 52, "context": "[62]), where \u03a3 and U are (resp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "yet unexplored problem in the database research literature (reasoning over fd\u2019s is extensively covered in Maier [22]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 54, "context": "Recent years have seen the emergence of some foundational work in causality in databases [64].", "startOffset": 89, "endOffset": 93}, {"referenceID": 55, "context": "For conjunctive queries, the causality is said to be computed very efficiently [65].", "startOffset": 79, "endOffset": 83}, {"referenceID": 56, "context": "al is the so-called sensitivity analysis [66], which is aimed at establishing a more refined connection between the query answer (output) and elements of the DB instance (input) for supporting user interventions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 57, "context": "This line of work is strongly related to the vision of \u2018reverse data management\u2019 [67].", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "Three remarkable features of U-relations are: expressiveness (being closed under positive relational algebra queries); succinctness (efficient storage of a very large number of possible worlds through vertical decompositions to support attributelevel uncertainty); and efficient query processing (including confidence computation) [18].", "startOffset": 331, "endOffset": 335}, {"referenceID": 15, "context": ", R i m, p [i] \u2208W is a possible world, with p being its probability [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Probabilistic world-set algebra (p-WSA) consists of the operations of relational algebra, an operation for computing tuple confidence conf, and the repairkey operation for introducing uncertainty \u2014 by giving rise to alternative worlds as maximal-subset repairs of an argument key [18].", "startOffset": 280, "endOffset": 284}, {"referenceID": 15, "context": "[18]) to map each discrete random variable xi to one of its possible values (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18]): the number of algebraic operations does not increase and each of the operations selection, projection and product/join remains of the same kind.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For a comprehensive overview of U-relations and p-WSA we refer the reader to [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Hypotheses, nonetheless, are (abstract) \u2018universal statements\u2019 [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 58, "context": ", see [68]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 58, "context": "4 In short, we make use of relational algebra group-by operation and build a pruned lattice of attribute groups having the same number of rows under the grouping (similarly to [68]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "Now, we provide the classical definition of a lossless join [20], i.", "startOffset": 60, "endOffset": 64}, {"referenceID": 59, "context": "Informed on research on Graphical Models (GM) [69], Suciu et al.", "startOffset": 46, "endOffset": 50}, {"referenceID": 60, "context": ", by using a set of axioms (the so-called \u2018graphoids\u2019) for reasoning about the probabilistic independence of variables [70].", "startOffset": 119, "endOffset": 123}, {"referenceID": 60, "context": "In fact, a connection between database normalization theory and factor decomposition in Graphical Models (GM) has been discussed by Verma and Pearl [70], but has not been explored since then.", "startOffset": 148, "endOffset": 152}, {"referenceID": 61, "context": "[71].", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "design [71].", "startOffset": 7, "endOffset": 11}, {"referenceID": 52, "context": "Despite some major differences, our synthesis method builds upon the classical theory of relational schema design by synthesis [62].", "startOffset": 127, "endOffset": 131}, {"referenceID": 52, "context": "Classical design by synthesis [62] was once criticized due to its too strong \u2018uniqueness\u2019 of fd\u2019s assumption [72, p.", "startOffset": 30, "endOffset": 34}, {"referenceID": 63, "context": "The last decade has seen significant research effort to make DB systems really usable [73].", "startOffset": 86, "endOffset": 90}, {"referenceID": 64, "context": "For instance, in comparison, the CRIUS system supports another kind of user-friendly DB design approach that provides users with a spreadsheet-like direct manipulation interface to increasingly add structure to their data [74].", "startOffset": 222, "endOffset": 226}, {"referenceID": 65, "context": "It has been firstly addressed by Koch and Olteanu motivated by data cleaning applications [75].", "startOffset": 90, "endOffset": 94}, {"referenceID": 29, "context": "The Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33].", "startOffset": 150, "endOffset": 158}, {"referenceID": 28, "context": "The Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33].", "startOffset": 150, "endOffset": 158}, {"referenceID": 66, "context": "8 shows the best fit of a baroreflex model for an observational dataset acquired by experiment on Dahl SS rat [76].", "startOffset": 110, "endOffset": 114}, {"referenceID": 66, "context": "(source: [76]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "and molecular levels [33].", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "[28]), in which we go through the whole design-by-synthesis pipeline (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "20 shows observational data collected from Hudson\u2019s Bay from 1900 to 1920 on the Lynx-Hare population [77].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "The efficiency and scalability of the U-relational representation system and its p-WSA query algebra have been extensively demonstrated [27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 1, "context": ", learning the equations, say, from Eureqa [2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 68, "context": "31) published in [78].", "startOffset": 17, "endOffset": 21}, {"referenceID": 68, "context": "Example of Boolean Network model (source: [78]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "All these are technical means to [15]: (1\u2032) extract the hypothesis \u2018empirical content\u2019 and \u2018predictive power;\u2019 (2\u2032) unravel its cohesiveness and how parsimonious it is in terms of the number of different claims or epistemological units carried within it, as well as its empirical grounding (\u2018first causes\u2019); and finally, we shall be able to (3\u2032) appraise it in face of competing or alternative explanations.", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "This thesis addresses the pressing call for large-scale, data-driven hypothesis management and analytics [35, 3, 10].", "startOffset": 105, "endOffset": 116}, {"referenceID": 24, "context": "[10, 29, 28]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 23, "context": "[10, 29, 28]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": ", multi-valued dependencies [20]), approximate fd\u2019s [68], conditional fd\u2019s [79]) to extend the scope of \u03a5-DB towards structured stochastic models.", "startOffset": 28, "endOffset": 32}, {"referenceID": 58, "context": ", multi-valued dependencies [20]), approximate fd\u2019s [68], conditional fd\u2019s [79]) to extend the scope of \u03a5-DB towards structured stochastic models.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "As envisioned by Jim Gray [1], the scientific method has been shifting towards being operated as a data-driven discipline which is rapidly gaining ground [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "As envisioned by Jim Gray [1], the scientific method has been shifting towards being operated as a data-driven discipline which is rapidly gaining ground [3].", "startOffset": 154, "endOffset": 157}], "year": 2015, "abstractText": "of Thesis presented to LNCC/MCT in partial fulfillment of the requirements for the degree of Doctor of Sciences (D.Sc.) MANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES AS UNCERTAIN AND PROBABILISTIC DATA Bernardo Gon\u00e7alves February 2015 Advisor: Fabio Porto, D.Sc. In view of the paradigm shift that makes science ever more data-driven, in this thesis we propose a synthesis method for encoding and managing large-scale deterministic scientific hypotheses as uncertain and probabilistic data. In the form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomena. For computing predictions, however, deterministic hypotheses can be abstracted as functions. We build upon Simon\u2019s notion of structural equations in order to efficiently extract the (so-called) causal ordering between variables, implicit in a hypothesis structure (set of mathematical equations). We show how to process the hypothesis predictive structure effectively through original algorithms for encoding it into a set of functional dependencies (fd\u2019s) and then performing causal reasoning in terms of acyclic pseudo-transitive reasoning over fd\u2019s. Such reasoning reveals important causal dependencies implicit in the hypothesis predictive data and guide our synthesis of a probabilistic database. Like in the field of graphical models in AI, such a probabilistic database should be normalized so that the uncertainty arisen from competing hypotheses is decomposed into factors and propagated properly onto predictive data by recovering its joint probability distribution through a lossless join. That is motivated as a design-theoretic principle for data-driven hypothesis management and predictive analytics. The method is applicable to both quantitative and qualitative deterministic hypotheses and demonstrated in realistic use cases from computational science. Resumo da Tese apresentada ao LNCC/MCT como parte dos requisitos necess\u00e1rios para a obten\u00e7\u00e3o do grau de Doutor em Ci\u00eancias (D.Sc.) GER\u00caNCIA DE HIP\u00d3TESES CIENT\u00cdFICAS DE LARGA-ESCALA COMO DADOS INCERTOS E PROBABIL\u00cdSTICOS Bernardo Gon\u00e7alves Fevereiro, 2015 Orientador: Fabio Porto, D.Sc. Tendo em vista a mudan\u00e7a de paradigma que faz da ci\u00eancia cada vez mais guiada por dados, nesta tese propomos um m\u00e9todo para codifica\u00e7\u00e3o e ger\u00eancia de hip\u00f3teses cient\u0301\u0131ficas determi\u0144\u0131sticas de larga escala como dados incertos e probabi\u013a\u0131sticos. Na forma de equa\u00e7\u00f5es matem\u00e1ticas, hip\u00f3teses relacionam simetricamente aspectos do fen\u00f4meno de estudo. Para computa\u00e7\u00e3o de predi\u00e7\u00f5es, no entanto, hip\u00f3teses determi\u0144\u0131sticas podem ser abstr\u00e1\u0131das como fun\u00e7\u00f5es. Levamos adiante a no\u00e7\u00e3o de Simon de equa\u00e7\u00f5es estruturais para extrair de forma eficiente a ent\u00e3o chamada ordena\u00e7\u00e3o causal imp\u013a\u0131cita na estrutura de uma hip\u00f3tese. Mostramos como processar a estrutura preditiva de uma hip\u00f3tese atrav\u00e9s de algoritmos originais para sua codifica\u00e7\u00e3o como um conjunto de depend\u00eancias funcionais (df\u2019s) e ent\u00e3o realizamos infer\u00eancia causal em termos de racio\u0107\u0131nio a\u0107\u0131clico pseudo-transitivo sobre df\u2019s. Tal racio\u0107\u0131nio revela importantes depend\u00eancias causais imp\u013a\u0131citas nos dados preditivos da hip\u00f3tese, que conduzem nossa \u015b\u0131ntese do banco de dados probabi\u013a\u0131stico. Como na \u00e1rea de modelos gr\u00e1ficos (IA), o banco de dados probabi\u013a\u0131stico deve ser normalizado de tal forma que a incerteza oriunda de hip\u00f3teses alternativas seja decomposta em fatores e propagada propriamente recuperando sua distribui\u00e7\u00e3o de probabilidade conjunta via jun\u00e7\u00e3o \u2018lossless.\u2019 Isso \u00e9 motivado como um prin\u0107\u0131pio te\u00f3rico de projeto para ger\u00eancia e an\u00e1lise de hip\u00f3teses. O m\u00e9todo proposto \u00e9 aplic\u00e1vel a hip\u00f3teses determi\u0144\u0131sticas quantitativas e qualitativas e \u00e9 demonstrado em casos rea\u013a\u0131sticos de ci\u00eancia computacional.", "creator": "LaTeX with hyperref package"}}}