{"id": "1501.05396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2015", "title": "Deep Multimodal Learning for Audio-Visual Speech Recognition", "abstract": "In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of $41\\%$ under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of $35.83\\%$ demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of $34.03\\%$.", "histories": [["v1", "Thu, 22 Jan 2015 05:25:33 GMT  (323kb,D)", "http://arxiv.org/abs/1501.05396v1", "ICASSP 2015"]], "COMMENTS": "ICASSP 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["youssef mroueh", "etienne marcheret", "vaibhava goel"], "accepted": false, "id": "1501.05396"}, "pdf": {"name": "1501.05396.pdf", "metadata": {"source": "CRF", "title": "Deep Multi-Modal Learning for Audio-Visual Speech Recognition", "authors": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "2 Audio-Visual Data Set & Feature Extraction", "text": "In this section we present the IBM AV-ASR Large Vocabulary Studio dataset and our feature extraction pipeline."}, {"heading": "2.1 IBM AV-ASR Large Vocabulary Studio Dataset", "text": "The IBM AV-ASR Large Vocabulary Studio Dataset consists of 40 hours of audiovisual recording from 262 speakers performed in clean studio conditions. Audio is sampled at 16 KHz, along with video frame rate of 30 frames per second at a resolution of 704 \u00d7 480. Vocabulary size in these recordings is 10,400 words. This record has been divided into a test set of 2 hours of audio + video from 22 speakers, the rest used for training."}, {"heading": "2.2 Feature Extraction", "text": "For the audio channel, we extract 24 MFCC coefficients at 100 frames per second. Nine consecutive images with MFCC coefficients are stacked and projected to 40 dimensions using an LDA matrix. We enter into the neural audio network by concatenating \u00b1 4 LDA images with the central frame of interest, resulting in a vector for audio functions of the dimension 360. For the visual channel, we start by recognizing the face in the image using the openCV implementation of the Viola Jones algorithm. We then create a mouthpiece model using an openCV mouthpiece detection model. Both use the ENCARA2 model as described in [MDHL11]. To obtain an invariant representation of small distortions and scales, we then extract Level 1 and Level 2 scatter coefficients [BM13] on the 64 x 64-64 mouth text, and then add the interest to the audio quality of the same region of the mouth (and then reduce it to 60 by means of LDA analysis)."}, {"heading": "2.3 Context-dependent Phoneme Targets", "text": "Each audio + video image is labeled with one of 1328 targets that represent context-dependent phonemes. 42 phones in the phonetic context of \u00b1 2 are grouped by decision trees down to 1328 classes. We measure the classification error rate at the level of these 1328 classes, this is called the telephone error rate (PER)."}, {"heading": "3 Uni-modal DNNs & Feature Fusion", "text": "In the supervised multimodal scenario we are examining, we obtain a training set S of examples and C classes designated N: S = {(x1i, x2i, yi), i = 1. N}, yi-Y = {1. C}, where x1i, x 2 i correspond to the first and second modality attribute vectors. We note ti = eyi the classification targets, where {ey} y-Y is the canonical basis in RC. Let us be the rear probability of being in class y in view of the two modalities x1 and x2. In a classification task, we would like to find the model that maximizes cross-entropy E: E = 1 N-rate i = 1 C-rate y = 1 tyi-log (y | x 1 i, x 2 i)."}, {"heading": "4 Bilinear Deep Neural Network", "text": "In the previous section, the training was performed separately to the two modalities, in this section we deal with the common exercise problem and introduce the bilinear bimodal DNN. For a DNN, we consider the dimension of an input v 'by K.' As shown in Figure 1, we consider two DNNs, one for each modality that we merge at the level of the decision function. For the simplicity of exposure, we assume the same number of layers L (L1 = L2 = L). For the intermediate layers, we have the standard separate networks: hj '= perimeter (W j' + b j '), one for the decision function. For the simplicity of exposure, we assume the same number of layers L (L1 = L2 = L)."}, {"heading": "4.1 Factored Bilinear Softmax", "text": "As the number of classes increases, the bilinear model becomes cumbersome, and we need large training sets to obtain better estimates of the parameters. (To reduce the computational complexity of the model, we propose the use of a factorization of the bilinear term similar to that in [MZHP], but in our case motivated by the Canonical Correlation Analysis (CCA). (HSST04): W y = U1diag (wy) U 2, >, y = 1.. C, (3) where U1, RK1L \u00d7 F, U2, RK2L \u00b7 F, and diag (wy) is a diagonal matrix with wy on its diagonal plane. (For numerical stability, we consider it as the numerical stability that we consider as numerical stability. (U j, j, 1, 2} where it is a regulation parameter."}, {"heading": "4.2 Factored Bilinear Softmax With Sharing", "text": "If the classes we want to predict how the leaves of a tree structure of depth two are organized, we can further reduce the computational complexity by dividing the weights between leaves with the same parent node. This is the case in AV-ASR, since the 1328 contextual phoneme states are organized as leaves of a tree, with the parent nodes corresponding to 42 different phoneme categories. In this case, we share the bilinear term about leaves with the same parents. By doing this in the case of AV-ASR, we only take into account the correlations between the audio and the visual channel at the phoneme level, and not on a fine-grained grid of contextual states. We can imagine this division as a pooling operation at the phoneme level. Formally, we assume that the label Y is divided into G, non-intersecting groups {Yg} g = 1... G, we assume that: W y = W g = 1ag wg (Yg) > U."}, {"heading": "5 Back-propagation with the Factored Bilinear DNN with Sharing", "text": "In this section, we give the back-propagation algorithm and the updating rules for the bilinear DNN = = > shared network (bi2-DDN-wS). Remember that our classes have a tree structure with leaves y and parent nodes g; a training example is therefore characterized by its background designation y (states) and its parent node g (phonemes), (x1, x2, y), y (1. G), and g (1. G). We use the notation g (y) to observe the group to which y belongs, and we set Rootg (y) = 1, Rootg = 0, g = 1. G, g = 6 (y). For bilinear softmax with shared use, we track the errors at the level of labels (states) as well as the group level (phonemes)."}, {"heading": "6 Combining Posteriors from Bimodal and Bilinear Bimodal Net-", "text": "We are experimenting with various factorized bi2 DNN-wS architectures randomly initialized on the IBM AV-ASR Large Vocabulary Studio Dataset. We are using the following notation for the architecture of the bilinear network: [archa | archv | F], where archa and archv are the architectures of the audio / visual network, and F is the dimension of the merged space. We are looking at architectures by increasing the complexity of Arch = [360, 500, 500, 200, 1328 | 540, 500, 200, 1328 | 200], Arch1 = [360, 600, 600, 400, 1328 | F = 100] and Arch2 = [360, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 1328 | F = 100]."}, {"heading": "7 Conclusion", "text": "In this thesis, we have investigated deep multimodal learning for the task of phonetic classification of audiovisual and visual modalities. We show that even under clean acoustic conditions, the use of visual channels in addition to language leads to significantly improved classification performance. A bilinear bimodal DNN is introduced that effectively exploits the correlation between the audiovisual and visual modalities and leads to a further reduction in the error rate."}], "references": [{"title": "In International Conference on Machine Learning (ICML)", "author": ["Galen Andrew", "Raman Arora", "Karen Livescu", "Jeff Bilmes. Deep canonical correlation analysis"], "venue": "Atlanta, Georgia,", "citeRegEx": "AALB13", "shortCiteRegEx": null, "year": 2013}, {"title": "35(8):1872\u20131886", "author": ["Joan Bruna", "Stephane Mallat. Invariant scattering convolution networks. IEEE Trans. Pattern Anal. Mach. Intell."], "venue": "August", "citeRegEx": "BM13", "shortCiteRegEx": null, "year": 2013}, {"title": "The challenge of multispeaker lip-reading", "author": ["S. Cox", "R. Harvey", "Y. Lan", "J. Newman"], "venue": "International Conference on Auditory-Visual Speech Processing", "citeRegEx": "CHLN08", "shortCiteRegEx": null, "year": 2008}, {"title": "Information theoretic feature extraction for audio-visual speech recognition", "author": ["Mihai Gurban"], "venue": "IEEE Transactions on signal processing,", "citeRegEx": "Gea09", "shortCiteRegEx": null, "year": 2009}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R. Hardoon", "Sndor Szedmk", "John Shawe-Taylor"], "venue": "Neural Computation, 16(12):2639\u20132664,", "citeRegEx": "HSST04", "shortCiteRegEx": null, "year": 2004}, {"title": "In Proceedings of the HCSNet Workshop on Use of Vision in Human-computer Interaction - Volume 56", "author": ["Patrick Lucey", "Sridha Sridharan. Patch-based representation of visual speech"], "venue": "VisHCI \u201906, pages 79\u201385, Darlinghurst, Australia, Australia,", "citeRegEx": "LS06", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine Vision and Applications", "author": ["Modesto Castrillon Mcastrillon", "Oscar Deniz", "Daniel Hernandez", "Javier Lorenzo. A comparison of face", "facial feature detectors based on the violajones general object detection framework"], "venue": "22(3):481\u2013494,", "citeRegEx": "MDHL11", "shortCiteRegEx": null, "year": 2011}, {"title": "and Paul Duchnowski", "author": ["Uwe Meier", "Wolfgang Hrst"], "venue": "Adaptive bimodal sensor fusion for automatic speechreading,", "citeRegEx": "MHD96", "shortCiteRegEx": null, "year": 1996}, {"title": "Hearing lips and seeing voices", "author": ["H. McGurk", "J. MacDonald"], "venue": "Nature, 264:746\u2013748", "citeRegEx": "MM76", "shortCiteRegEx": null, "year": 1976}, {"title": "In International Conference on Machine Learning (ICML)", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y. Ng. Multimodal deep learning"], "venue": "Bellevue, USA, June", "citeRegEx": "NKK+11", "shortCiteRegEx": null, "year": 2011}, {"title": "In INTERSPEECH 2006 - ICSLP", "author": ["Vassilis Pitsikalis", "Athanassios Katsamanis", "George Papandreou", "Petros Maragos. Adaptive multimodal fusion by uncertainty compensation"], "venue": "Ninth International Conference on Spoken Language Processing, Pittsburgh, PA, USA, September 17-21, 2006,", "citeRegEx": "PKPM06", "shortCiteRegEx": null, "year": 2006}, {"title": "In IEEE 9th Workshop on Multimedia Signal Processing", "author": ["George Papandreou", "Athanassios Katsamanis", "Vassilis Pitsikalis", "Petros Maragos. Multimodal fusion", "learning with uncertain features applied to audiovisual speech recognition"], "venue": "MMSP 2007, Chania, Crete, Greece, October 1-3, 2007, pages 264\u2013267,", "citeRegEx": "PKPM07", "shortCiteRegEx": null, "year": 2007}, {"title": "Speech & Language Processing", "author": ["George Papandreou", "Athanassios Katsamanis", "Vassilis Pitsikalis", "Petros Maragos. Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition. IEEE Transactions on Audio"], "venue": "17(3):423\u2013435,", "citeRegEx": "PKPM09", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio-visual automatic speech recognition: An overview", "author": ["G. Potamianos", "C. Neti", "J. Luettin", "I. Matthews"], "venue": "Issues in Visual and Audio-Visual Speech Processing. MIT Press", "citeRegEx": "PNLM04", "shortCiteRegEx": null, "year": 2004}, {"title": "Lipreading and audio-visual speech perception", "author": ["Q. Summerfield"], "venue": "Trans. R. Soc., London", "citeRegEx": "Sum92", "shortCiteRegEx": null, "year": 1992}, {"title": "Integration of acoustic and visual speech signals using neural networks", "author": ["Ben P. Yuhas", "Moise H. Goldstein", "Terrence J. Sejnowski"], "venue": "IEEE Communications Magazine,", "citeRegEx": "YGS89", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 8, "context": "This has been demonstrated by the so called McGurk effect [MM76], which shows that a voicing of ba and a mouthing of ga is perceived as being da.", "startOffset": 58, "endOffset": 64}, {"referenceID": 2, "context": "In the presence of noise and multiple speakers (cocktail party effect), humans rely on lip reading in order to enhance speech recognition [CHLN08].", "startOffset": 138, "endOffset": 146}, {"referenceID": 14, "context": "The visual information is also important in a clean speech scenario as it helps in disambiguating voices with similar acoustics [Sum92].", "startOffset": 128, "endOffset": 135}, {"referenceID": 6, "context": "Both these utilize the ENCARA2 model as described in [MDHL11].", "startOffset": 53, "endOffset": 61}, {"referenceID": 1, "context": "In order to get an invariant representation to small distortions and scales we then extract level 1 and level 2 scattering coefficients [BM13] on the 64\u00d7 64 mouth region of interest and then reduce their dimension to 60 using LDA (Linear discriminant Analysis).", "startOffset": 136, "endOffset": 142}, {"referenceID": 4, "context": "that is similar to the one in [MZHP], but is motivated in our case by Canonical Correlation Analysis (CCA) [HSST04]: W y = Udiag(wy)U 2,>, y = 1 .", "startOffset": 107, "endOffset": 115}, {"referenceID": 0, "context": "Deep CCA of [AALB13] shares similarities with this model.", "startOffset": 12, "endOffset": 20}], "year": 2015, "abstractText": "In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of 41% under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of 35.83% demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of 34.03%.", "creator": "LaTeX with hyperref package"}}}