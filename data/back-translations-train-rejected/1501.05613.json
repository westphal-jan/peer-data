{"id": "1501.05613", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2015", "title": "Second-Order Belief Hidden Markov Models", "abstract": "Hidden Markov Models (HMMs) are learning methods for pattern recognition. The probabilistic HMMs have been one of the most used techniques based on the Bayesian model. First-order probabilistic HMMs were adapted to the theory of belief functions such that Bayesian probabilities were replaced with mass functions. In this paper, we present a second-order Hidden Markov Model using belief functions. Previous works in belief HMMs have been focused on the first-order HMMs. We extend them to the second-order model.", "histories": [["v1", "Thu, 22 Jan 2015 19:56:34 GMT  (147kb,D)", "http://arxiv.org/abs/1501.05613v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jungyeul park", "mouna chebbah", "siwar jendoubi", "arnaud martin"], "accepted": false, "id": "1501.05613"}, "pdf": {"name": "1501.05613.pdf", "metadata": {"source": "CRF", "title": "Second-order Belief Hidden Markov Models", "authors": ["Jungyeul Park", "Mouna Chebbah", "Siwar Jendoubi", "Arnaud Martin"], "emails": ["jungyeul.park@univ-rennes1.fr", "mouna.chebbah@univ-rennes1.fr", "arnaud.martin@univ-rennes1.fr", "jendoubi.siwar@etudiant.univ-rennes1.fr"], "sections": [{"heading": null, "text": "Keywords: Faith Functions, Dempster-Shafer Theory, First Order Faith HMM, Second Order Faith HMM, Probabilistic HMM"}, {"heading": "1 Introduction", "text": "A Hidden Markov Model (HMM) is one of the most important statistical models in machine learning [?]. A HMM is a classifier or labeler that can assign a label or class to each unit in a sequence [?]. However, it has been used successfully for several decades in many applications for processing text and language, such as the Part-of-Speech (POS) meeting, where state transition and output observation depend only on a previous state that does not exactly correspond to real applications [?]. Therefore, they require a number of complexities in the HMM. In fact, the first order adoption of HMM, where state transition and output observation depend only on a previous state, is very different."}, {"heading": "2 First-order probabilistic HMMs", "text": "The POS marker is a task for determining the most likely estimated sequence of n tags against the observation sequence of v-words. Accordingly [?], a first-order probability calculation can be characterized as follows: N The number of states in a model St = {st1, st2, \u00b7 stN} at the time. M The number of different first-order observation symbols can be characterized as follows: V = {v1, v2, \u00b7, vM}. A = {ai j} The amount of transition probability distributions. B = {b j (ot)} The observation probability distributions in a state j. p) The initial probability distribution in Figure 1 illustrates the first-order probability distribution HMM, which makes it possible to estimate the probability of the sequence st \u2212 1i and s the sequence."}, {"heading": "3 Second-order probabilistic HMMs", "text": "Now we declare the extension of the first order model to a second order problem. (Figure 2 shows the second order probability calculation HMM, which allows to estimate the probability of the sequence of three states st \u2212 2i, s \u2212 1j and s \u00b7 t k, whereby the probability of the second order HMM is characterized by three fundamental problems: - Likelihood: The second order model is based on an observation bk (ot). In contrast to the first order model, the probability of the transition is based on two previous tags, where ai = P (stk | s t) t i, s \u2212 1 j) as follows: - Likelihood: The second order model is based on an observation bk (ot)."}, {"heading": "4 First-order Belief HMMs", "text": "s mass functions (bbas) (bbas) (bbas) [bbas) (bbas) [bbas) [bbas] (bb). (b) The number of states in a model (St1, St2, StN). (b) The number of different observation symbols V = {m.ta [St \u2212 1i] (Stj). (Stj)) The number of states in all possible subranges of the states. (B = {m.tb) The number of different observation symbols V = {m.ta). (St \u2212 1i) (Stj). (Stj). (Stj)."}, {"heading": "5 Second-order Belief HMMs", "text": "Like the first order HMM, N, M, B and p, the second order HMM (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) k (S) s (S) s (S) s (S) s (S) s (S) s) k (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s) s (S) s (S) s) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S) s (S (S) s) s (S (S) s (S) s (S (S) s) s (S (S (S) s) s (S (S (S (S) s) s) s (S (S (S (S) s) s (S (S (S (S (S) s) s) s (S (S (S (S (S) s) s) s (S (S (S (S (S (S (S (S (S (S) s) s) s) s) s) s (S (S (S (S (S (S (S (S (S (S (S (S (S) s) s) s) s) s (S (S (S (S (S (S (S (S (S (s) s) s) s) s (S (S (S (S (S (S (S (s) s) s) s) s) s (S (S (S (S (S (S (S (S (s) s) s) s (S (S (S (S (s) s) s"}, {"heading": "6 Conclusion and future perspectives", "text": "The problem of POS marking has been regarded as one of the most important tasks for natural language processing systems. We have described such a problem on the basis of HMM and tried to apply our idea to the theory of belief functions. Previous work on belief HMMs has been extended to the second order model. With the proposed method we will be able to easily expand the overarching model for belief HMMs. Some technical aspects still need to be taken into account. Robust implementation for belief HMMs is required, where we can generally find over a million observations in the training data to solve the problem of POS marking. As described above, the choice of inverse pignistic transformations would be empirically verifiable.5 We plan to implement these technical aspects in the near future. Current work is described to rely on a supervised learning paradigm from designated training data. In fact, the forward-facing algorithm can perform in HMM's totally uncontrolled learning, but it is poorly known in EM."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Hidden Markov Models (HMMs) are learning methods for pattern recognition. The probabilistic HMMs have been one of the most used techniques based on the Bayesian model. First-order probabilistic HMMs were adapted to the theory of belief functions such that Bayesian probabilities were replaced with mass functions. In this paper, we present a second-order Hidden Markov Model using belief functions. Previous works in belief HMMs have been focused on the first-order HMMs. We extend them to the second-order model.", "creator": "LaTeX with hyperref package"}}}