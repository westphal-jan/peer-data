{"id": "1501.05940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2015", "title": "A New Efficient Method for Calculating Similarity Between Web Services", "abstract": "Web services allow communication between heterogeneous systems in a distributed environment. Their enormous success and their increased use led to the fact that thousands of Web services are present on the Internet. This significant number of Web services which not cease to increase has led to problems of the difficulty in locating and classifying web services, these problems are encountered mainly during the operations of web services discovery and substitution. Traditional ways of search based on keywords are not successful in this context, their results do not support the structure of Web services and they consider in their search only the identifiers of the web service description language (WSDL) interface elements. The methods based on semantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web service with a semantic description allow raising partially this problem, but their complexity and difficulty delays their adoption in real cases. Measuring the similarity between the web services interfaces is the most suitable solution for this kind of problems, it will classify available web services so as to know those that best match the searched profile and those that do not match. Thus, the main goal of this work is to study the degree of similarity between any two web services by offering a new method that is more effective than existing works.", "histories": [["v1", "Thu, 22 Jan 2015 22:18:45 GMT  (603kb)", "http://arxiv.org/abs/1501.05940v1", "7 pages, 4 figures, 8 tables, International Journal of Advanced Computer Science and Applications (IJACSA),Vol. 5, No. 8, 2014"]], "COMMENTS": "7 pages, 4 figures, 8 tables, International Journal of Advanced Computer Science and Applications (IJACSA),Vol. 5, No. 8, 2014", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR cs.SE", "authors": ["t rachad", "j boutahar", "s el ghazi"], "accepted": false, "id": "1501.05940"}, "pdf": {"name": "1501.05940.pdf", "metadata": {"source": "META", "title": "A New Efficient Method for Calculating Similarity Between Web Services", "authors": ["T. RACHAD"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that we will be able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution. \""}, {"heading": "A. syntactic similarity", "text": "The syntactical similarity consists in assigning a real number r to a string pair S1 and S2, indicating the degree of syntactical similarity between S1 and S2. There are essentially two ways of measuring the degree of similarity between two concepts: processing distances: where distance is the cost of the optimal sequence of processing operations that turn S1 into S2 or S2 into S1. Processing operations are character insertion, deletion and substitution. A small value of r indicates greater similarity. There are several algorithms based on processing distances, the most well-known being being: Minkowsky (1964), manhaten, Levenstein (1965), Monger-Elkan (1996), Smith-Waterman (1981). In [1] the authors conducted a comparative study of processing distances and concluded that Monger-Elkan [2] provides the best result."}, {"heading": "B. Semantic Similarity", "text": "The semantic similarity consists in the assignment to a word pair w1 and w2 of a real number r, which indicates the degree of semantic similarity between them. Similarities are measured by comparing the senses of the two words. Thus, two words are similar (with a certain degree of similarity) semantic if they mean the same thing (synonyms), they have an opposite meaning (antonyms), they are used in the same way or inherit the same type, they are used in the same context or if one is some kind of similarity. To measure the semantic similarity between words, we need a lexical hierarchy such as WordNet [7]. WordNet is a lexical database that aims to identify, classify and relativize the semantic and lexical contents of the English language."}, {"heading": "C. Word sense disambiguation", "text": "All algorithms for measuring the semantic similarity between two words refer to the measure of similarity between the senses of the two words. All algorithms for measuring the semantic similarity take into account either common sense or the senses that show the greatest similarity during the comparison process. However, the meaning of a word changes with the context in which the word appears. Therefore, we must extract the exact senses of different words before turning to the measure of similarity. Literal ambiguity is the scientific term attributed to the process of searching for the exact meaning of a word in a particular context."}, {"heading": "D. distance between two sets", "text": "In the course of this work, we must calculate the degree of similarity between two groups of concepts, which elements (concepts) are connected by a measure of similarity (Figure 2). In this work, we chose the Hausdorff algorithm [14] to calculate the degree of similarity between two groups of concepts. It is used to calculate the similarity between two objects represented by two groups of points. Therefore, the problem is attributed to the calculation of the distance between the two groups of points.In [14], the authors claim that there are 24 possible ways to measure the distance between two groups of points based on the Hausdorff distance, and they came to the conclusion that the modified Hausdorff distance (MHD) has the highest power to measure the similarity between two objects.The modified Hausdorff distance between two groups of points S1 and S2 is defined by the expression: (1) 1.2 (), 2.1 (max) 2.1 (gSSgSSg2 distance between two objects must be measured in our SSS2 case (SSSSS1 is a distance from the SSS2)."}, {"heading": "A. Structure of a wsdl file", "text": "WSDL is an XML file that follows a standard format for describing a web service. It mainly describes the operations provided by and access to the web service. A WSDL file has the following structure (Table 2): In our work, we only look at the operations and their inputs and outputs. We consider a web service to be a set of operations and the operations received and returned elements that will be part of the WSDL schema. All other elements are ignored, as their names are often automatically generated and depend on the tool used in creating the web service and therefore do not interfere with similarity measurement. 63 | P a g ewww.ijacsa.thesai.org"}, {"heading": "B. Preliminary declarations", "text": "Let WS1 and WS2 be two web services for which we want to calculate the similarity. Let S1 and S2 be their schemes. Let F and G be two types of operations such as: F = {f / f is an operation of WS1} and G = {g / g is an operation of WS2}. Let D and D \"have the departure sets of f and g respectively and let A and A\" have the arrival sets of f and g each with D, A P (S1) and D, A'P (S2), with P (S1) and P (S2) the sets of parts of S1 and S2.Our goal is to measure the similarity of WsdlSim (WS1, WS2) between two web services WS1 and WS2. This calculation depends on the similarity between the operations of the two web services."}, {"heading": "C. similarity between two data sets", "text": "In our thesis we consider that each data set E is part of a web service scheme S. This data set has a tree structure (Figure 3), as the name of the set E is the root of the tree, the internal nodes correspond to the elements of the complex types and leaves of the tree correspond to measure the similarity between two data sets, all existing works try to compare all the nodes of the two sets, ie e and e'j, they calculate the similarity between ei and e'j by using both the syntactic, semantic and structural similarity of the two representations of E and E ', which makes the calculations very complex."}, {"heading": "D. Similarity between two operations", "text": "Let f and g be two operations so that f-F and g-G with f: DA and g: D'A ', the similarity between the two operations f and g is the sum of the similarities between their input and output sets (inputs and outputs) and the similarity between their names: (3) OPSim (f, g) = p1 * SetSim (D, D') + p2 * SetSim (A, A ') + p3 * SentenceSim (f, g) / (P1 + P2 + P3) In the calculation we use a weighting to determine the order of the meaning of each of the similarity variables. In the measurements we have performed, it was taken into account that p1 = 1, p2 = 1 and p3 = 2."}, {"heading": "E. Similarity between two web services", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the Deutsche Presse-Agentur in an interview with the Frankfurter Allgemeine Zeitung (Friday).\" I don't think we will be able to change the world. \""}], "references": [{"title": "A Comparison of String Distance Metrics for Name Matching Tasks", "author": ["William W. Cohen", "Pradeep Ravikumar", "Stephen E. Fienberg"], "venue": "Proceedings of IJCAI-03 Workshop on Information Integration,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "The Field Matching Problem: Algorithms and Applications", "author": ["Alvaro E. Monge", "Charles P. Elkan"], "venue": "Proceedings of the Second International Conference on Knowledge Discovery and Data Mining KDD, page 267\u2014270,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Probabilistic linkage of large public health data file ", "author": ["A. M"], "venue": "Statistics in Medicine,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "The state of record linkage and current research problems \u201c, Statistics of Income Division, Internal Revenue Service Publication R99/04,\u200e", "author": ["W.E. Winkler"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Overview of Record Linkage and Current Research Directions \u201c, Research Report Series, RRS,\u200e", "author": ["W.E. Winkler"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Semantic distance inWordNet: An experimental, application-oriented evaluation of five measures\u201d, in workshop on wordnet and other lexical resources, second meeting of the north american chapter of the association for computational linguistics", "author": ["Alexander Budanitsky", "Graeme Hirst"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Comparison Latent Semantic and WordNet Approach for Semantic Similarity Calculation", "author": ["Wayan Simri Wicaksana", "Bambang Wahyudi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Combining local context and WordNet similarity for word sense identificationC\u201d \u201cWordNet: An Electronic Lexical DatabaseReferences\u201d, Ed", "author": ["Leacock", "M Chodorow"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Verbs semantics and lexical selection", "author": ["Z Wu", "M Palmer"], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "T Pedersen", "author": ["S Banerjee"], "venue": "\u201cAn adapted Lesk algorithm for word sense disambiguation using WordNet\u201d, Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, Pages 136-145, Springer-Verlag London, UK ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "A modified Hausdorff distance for object matching", "author": ["Marie-Pierre Dubuisson", "Anil K. Jain"], "venue": "In Proceedings of 12th International Conference on Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "web service discovery based on computation of semantic similarity distance and qos normalization", "author": ["Jeberson Retna Raj"], "venue": "Indian Journal of Computer Science and Engineering, pages 566-568", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "WSSim: a Tool for the Measurement of Web Service Interface Similarity\u201d, in proceedings of the french-speaking conference on Software Architectures", "author": ["Okba Tibermacine", "Chouki Tibermacine", "Foudil Cherif"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "A Comparison of Web Service Interface Similarity Measures", "author": ["Natallia Kokash"], "venue": "STAIRS:,Pages 220-231,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "\u201dMeasuring Similarity of Web Services Based on WSDL", "author": ["Fangfang Liu", "Yuliang Shi", "Jie Yu", "Tianhong Wang", "Jingzhe Wu"], "venue": "In proceeding of: IEEE International Conference on Web Services,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Similarity-based Web Service Matchmaking\u201d, In proceeding of: Services Computing", "author": ["Jian Wu", "Zhaohui Wu"], "venue": "IEEE International Conference on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "In [1] the authors carried out a comparative study of edit distances based methods and concluded that Monger-Elkan [2] provides the best result.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [1] the authors carried out a comparative study of edit distances based methods and concluded that Monger-Elkan [2] provides the best result.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "The most known algorithms are: Jaro [3, 4]; Jaro-Winkler [5, 6].", "startOffset": 36, "endOffset": 42}, {"referenceID": 3, "context": "The most known algorithms are: Jaro [3, 4]; Jaro-Winkler [5, 6].", "startOffset": 57, "endOffset": 63}, {"referenceID": 4, "context": "The most known algorithms are: Jaro [3, 4]; Jaro-Winkler [5, 6].", "startOffset": 57, "endOffset": 63}, {"referenceID": 0, "context": "Work [1] shows that jarowinkler is the most powerful and fastest measurement.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "In [8] authors compared experimentally five measures of semantic similarity in wordnet (ie Hirst and St-Onge, Leacock and Chodorow, Resnik, Lin and finally Jiang and Conrath) by examining their performance in spelling correction systems and by comparing their performance with human judgments.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [10] authors evaluated the similarity measures in three different domains (transport, book and business) with reference to human judgment and experts judgment, they concluded that at recall, WordNet with Jean Conrath provide the best result at three domains, at Precession, there is no significant method can provide dominant result and At f-measure (that combine recall and precision measures), WordNet with Wu-Palmer has tendency better than the others.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "The third evaluation work discussed in [10] seems the most rigorous for us, because it uses both human and experts judgments and because the tests are carried out in three different areas.", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "Adapted Lesk algorithm described in [11], [12] and [13] is adopted to remove the ambiguity of meaning in a given context.", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "Adapted Lesk algorithm described in [11], [12] and [13] is adopted to remove the ambiguity of meaning in a given context.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "Adapted Lesk algorithm described in [11], [12] and [13] is adopted to remove the ambiguity of meaning in a given context.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "In this work we chose the Hausdorff algorithm [14] to calculate the degree of similarity between two sets of concepts.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "In [14] authors affirm that there are 24 possible ways to measure the distance between two sets of points using the Hausdorff distance and they concluded that the modified Hausdorff distance (MHD) has the highest performance to measure similarity between two objects.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In [15] authors use google Normalised distance to calculate the semantic similarity between two concepts, it is a statistical method based on results returned by the Google search engine and does not take into account the context of concepts in which we want to compute the similarity.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [16] authors use at the same time several metrics to calculate the semantic similarity, and use several metrics to calculate the syntactic similarity.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [16] the authors did not measure the precision of their method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [17] authors measure the similarity between two web services by measuring the similarities between the descriptions of the different concepts included in the wsdl file.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [18] authors use the same approach as the work cited in [15] using several kinds of functions to evaluate a similarity matrix except that their method does not exceed 70% in precision and recall.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In [18] authors use the same approach as the work cited in [15] using several kinds of functions to evaluate a similarity matrix except that their method does not exceed 70% in precision and recall.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "In [19] authors have ignored the names of the operations in the calculation of similarity, and they considered only the inputs and outputs of simple type, while the operations of a web service have often input and output withe complex type.", "startOffset": 3, "endOffset": 7}], "year": 2014, "abstractText": "Web services allow communication between heterogeneous systems in a distributed environment. Their enormous success and their increased use led to the fact that thousands of Web services are present on the Internet. This significant number of Web services which not cease to increase has led to problems of the difficulty in locating and classifying web services, these problems are encountered mainly during the operations of web services discovery and substitution. Traditional ways of search based on keywords are not successful in this context, their results do not support the structure of Web services and they consider in their search only the identifiers of the web service description language (WSDL) interface elements. The methods based on semantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web service with a semantic description allow raising partially this problem, but their complexity and difficulty delays their adoption in real cases. Measuring the similarity between the web services interfaces is the most suitable solution for this kind of problems, it will classify available web services so as to know those that best match the searched profile and those that do not match. Thus, the main goal of this work is to study the degree of similarity between any two web services by offering a new method that is more effective than existing works. Keywords\u2014web service; semantic similarity; syntactic similarity; WordNet; word sense disambiguation; Hausdorff distance", "creator": "Microsoft\u00ae Word 2010"}}}