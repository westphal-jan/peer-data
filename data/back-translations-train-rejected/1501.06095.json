{"id": "1501.06095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2015", "title": "Between Pure and Approximate Differential Privacy", "abstract": "We show a new lower bound on the sample complexity of $(\\varepsilon, \\delta)$-differentially private algorithms that accurately answer statistical queries on high-dimensional databases. The novelty of our bound is that it depends optimally on the parameter $\\delta$, which loosely corresponds to the probability that the algorithm fails to be private, and is the first to smoothly interpolate between approximate differential privacy ($\\delta &gt; 0$) and pure differential privacy ($\\delta = 0$).", "histories": [["v1", "Sat, 24 Jan 2015 23:26:21 GMT  (18kb,D)", "http://arxiv.org/abs/1501.06095v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.LG", "authors": ["thomas steinke", "jonathan ullman"], "accepted": false, "id": "1501.06095"}, "pdf": {"name": "1501.06095.pdf", "metadata": {"source": "CRF", "title": "Between Pure and Approximate Differential Privacy", "authors": ["Thomas Steinke", "Jonathan Ullman"], "emails": ["tsteinke@seas.harvard.edu.", "jullman@cs.columbia.edu."], "sections": [{"heading": null, "text": "Specifically, we look at a database D-1-n-d and its disposable margins, i.e. the d queries of the form \"Which fraction of individual data sets has the i-bit set to + 1?\" We show that it is necessary for the i-bit to be set to + 1 in order to answer all these questions within errors \u00b1 \u03b1 (on average) and at the same time satisfy (\u03b5, \u03b4) differential privacy (Bun, Ullman and Vadhan, STOC '14). In addition to our lower limit, we provide new purely and roughly differentiated private algorithms to answer arbitrary statistical questions that improve the sample complexity of the standard Laplace and Gaussian mechanisms for achieving worst-case accuracy."}, {"heading": "1 Introduction 1", "text": "1.1 Average case against worst-case errors...................................... 2 1.2 Techniques........................"}, {"heading": "2 Preliminaries 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Lower Bounds for Approximate Differential Privacy 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 New Mechanisms for L\u221e Error 7", "text": "4.1 Pure differentiated privacy..................................... 7 4.2 Approximate differentiated privacy...................... 9References 10A Alternative lower limit for pure differentiated privacy 12"}, {"heading": "1 Introduction", "text": "The goal of data privacy preservation analysis is to enable a comprehensive statistical analysis of a database while protecting the privacy of individuals whose data is stored in the database. A formal privacy guarantee is provided by (\u03b5, \u03b4) differential privacy. [D] very roughly, a limit is set on the extent of the influence that an individual record has on the information published and the likelihood that this limit will have a significant impact on the information published through the database, making the definition more stringent. [D] very roughly, is a natural way to measure the tradeoff between privacy and utility. [D] the minimum number of records that is sufficient to publish a given set of statistics on the database, while we both achieve different privacy and statistical accuracy."}, {"heading": "1.1 Average-Case Versus Worst-Case Error", "text": "Our lower limit applies to mechanisms with an average case (L1) error warranty. Therefore, it also applies to algorithms that guarantee a worst-case (L) error. The Laplace mechanism provides a suitable upper limit for an average error. In many cases, worst-case error warranties are preferable.For worst-case errors, the sample complexity of the Laplace mechanism deteriorates by an additional logd factor compared to (1).Surprisingly, this degradation is not necessary.We present algorithms that are marginal in all respects and improve the sample complexity of the Laplace mechanism by a rough logd factor. These algorithms show that the widely used technique of adding an independent noise to each query is suboptimal if the goal is to achieve a worst-case error warranty. Our algorithm for pure differential privacy fulfils a 1.2 / 4 precept for each of the following theories."}, {"heading": "1.2 Techniques", "text": "This means that we are not confining ourselves to privacy, but to privacy and the associated learning problems [Ull13, BUV14, HU14, SU14]. Specifically, we have shown that fingerprinting codes can be used to construct an attack that shows that any mechanism that gives precise answers to disposable databases is not private. Specifically, a fingerprinting code provides a distribution of individual data and a corresponding \"tracer\" algorithm, when a database is constructed from the data of a fixed subset of individuals, the tracer algorithm can identify at least one of the individuals in that subset that only provides more detailed answers to the disposable margins of the database."}, {"heading": "2 Preliminaries", "text": "We define a database D (\u00b1 1) n \u00b7 d as a matrix of n rows in which each row corresponds to an individual and each row has the dimension d (consists of d binary attributes). We say that two databases D, D (\u00b1 1) n \u00b7 d are adjacent if they differ only by a single line, and we call this D \u00b2 D \u00b2. Specifically, we can replace the ith row of a database D with a certain element of {\u00b1 1} d to get another database D \u2212 i \u00b2 d. Definition 2.1 (Differential Privacy [DMNS06]). Let M: {\u00b1 1} n \u00b7 d \u2192 R be a randomized mechanism. We say that M (\u03b5) -differentiated is private if for each two adjacent databases D \u00b2 and each subset S \u00b2."}, {"heading": "3 Lower Bounds for Approximate Differential Privacy", "text": "The following definition is tailored to the application of privacy."}, {"heading": "4 New Mechanisms for L\u221e Error", "text": "Adding independent noise seems very natural for one-way street edge zones, but is suboptimal if you are interested in worst-case margin of error (i.e. L \u221e) and not average margin of error (i.e. L1)."}, {"heading": "4.1 Pure Differential Privacy", "text": "Theorem 1.2 follows from theorem 4.1. In particular, the mechanism M: (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n (D) n n (D) n n (D) n n (D) n (D) n n n (D) n n) n (D) n n (D) n n n (D) n n n n n (D) n n n n n (D) n n n n n (D) n n n n n (D) n n (D) n n (D) n n (D) n n (D) n n n (D) n n n (D) n n n (n n (n (n) n n n n n (n n n (D n (n (n) n n n n n n (D n (n) n n n n n (D n (n (n) n n n n (D n (n) n n n (D) n n n n n (D) n n n n (n n (D) n) n n n n n n n (n n n (n (D) n n n n n (n n (D) n n n n (n (n) n n n) n (D) n n n n n n (n n n (n n n (n n n n (n (n n n (n) n (n n n n) n) n n (D) n n n n n n n n n (n n (n n n (n n n n n n (n n n n n (n (n n n n n n n (n n n (n n n n n n n n (n n n n n (n n n n n n n n n (n n n n n n n n n n n n n (n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n"}, {"heading": "4.2 Approximate Differential Privacy", "text": "Our approximate differential privacy algorithm makes use of a powerful literature tool [DNR + 09, HR10, DNPR10, RR10] called the vector algorithm. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 For each c, k \u2265 1, \u03b5, \u03b1, \u03b2 > 0, andn \u2265 0, log (1 / 3) log (k / 4) log log log log log (k / 4) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) log (2) (2) log (2) log (2) (2) log (2) log (2)."}, {"heading": "A Alternative Lower Bound for Pure Differential Privacy", "text": "It is well known [HT10] that any \u03b5-differential private mechanism that responds to d > > one-way marginals requires n \u2265 (d / \u03b5) patterns. Our techniques provide an alternative simple proof for these facts.Theorem A.1. Let M: {\u00b1 1} n \u00b7 d \u2192 [\u00b1 1] d be an \u03b5-differential private mechanism. The proof is based on a special case of Hoeffdings Inequality: Lemma A.2 (Hoeffdings Inequality).Let X: \u2212 1) n is uniformly random and an \"Rn fixed.\" ThenP X [< a > D: 800x, (ltSillings Inequality: Lemma A.2 (Hoeffdings Inequality)."}], "references": [{"title": "Practical privacy: the sulq framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In PODS, pages 128\u2013138", "citeRegEx": "Blum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2005}, {"title": "Collusion-secure fingerprinting for digital data", "author": ["Dan Boneh", "James Shaw"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Boneh and Shaw.,? \\Q1998\\E", "shortCiteRegEx": "Boneh and Shaw.", "year": 1998}, {"title": "Fingerprinting codes and the price of approximate differential privacy", "author": ["Mark Bun", "Jonathan Ullman", "Salil P. Vadhan"], "venue": "In STOC,", "citeRegEx": "Bun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bun et al\\.", "year": 2014}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor"], "venue": "In EUROCRYPT,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In TCC,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "Revealing information while preserving privacy", "author": ["Irit Dinur", "Kobbi Nissim"], "venue": "In PODS, pages 202\u2013210", "citeRegEx": "Dinur and Nissim.,? \\Q2003\\E", "shortCiteRegEx": "Dinur and Nissim.", "year": 2003}, {"title": "Privacy-preserving datamining on vertically partitioned databases", "author": ["Cynthia Dwork", "Kobbi Nissim"], "venue": "In CRYPTO,", "citeRegEx": "Dwork and Nissim.,? \\Q2004\\E", "shortCiteRegEx": "Dwork and Nissim.", "year": 2004}, {"title": "Differential privacy under continual observation", "author": ["Cynthia Dwork", "Moni Naor", "Toniann Pitassi", "Guy N. Rothblum"], "venue": "In Proceedings of the Forty-second ACM Symposium on Theory of Computing,", "citeRegEx": "Dwork et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2010}, {"title": "On the complexity of differentially private data release: efficient algorithms and hardness results", "author": ["Cynthia Dwork", "Moni Naor", "Omer Reingold", "Guy N. Rothblum", "Salil P. Vadhan"], "venue": "In STOC,", "citeRegEx": "Dwork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2009}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Dwork and Roth.,? \\Q2013\\E", "shortCiteRegEx": "Dwork and Roth.", "year": 2013}, {"title": "A multiplicative weights mechanism for privacypreserving data analysis", "author": ["Moritz Hardt", "Guy Rothblum"], "venue": "In Proc. 51st Foundations of Computer Science (FOCS),", "citeRegEx": "Hardt and Rothblum.,? \\Q2010\\E", "shortCiteRegEx": "Hardt and Rothblum.", "year": 2010}, {"title": "On the geometry of differential privacy", "author": ["Moritz Hardt", "Kunal Talwar"], "venue": "In Proceedings of the Forty-second ACM Symposium on Theory of Computing,", "citeRegEx": "Hardt and Talwar.,? \\Q2010\\E", "shortCiteRegEx": "Hardt and Talwar.", "year": 2010}, {"title": "Preventing false discovery in interactive data analysis is hard", "author": ["Moritz Hardt", "Jonathan Ullman"], "venue": "In FOCS. IEEE, October", "citeRegEx": "Hardt and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Ullman.", "year": 2014}, {"title": "On the \u201csemantics\u201d of differential privacy: A bayesian formulation", "author": ["Shiva Prasad Kasiviswanathan", "Adam Smith"], "venue": "CoRR, abs/0803.3946,", "citeRegEx": "Kasiviswanathan and Smith.,? \\Q2008\\E", "shortCiteRegEx": "Kasiviswanathan and Smith.", "year": 2008}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "McSherry and Talwar.,? \\Q2007\\E", "shortCiteRegEx": "McSherry and Talwar.", "year": 2007}, {"title": "Interactive privacy via the median mechanism", "author": ["Aaron Roth", "Tim Roughgarden"], "venue": "In Proc. 42nd Symposium on Theory of Computing (STOC),", "citeRegEx": "Roth and Roughgarden.,? \\Q2010\\E", "shortCiteRegEx": "Roth and Roughgarden.", "year": 2010}, {"title": "Interactive fingerprinting codes and the hardness of preventing false discovery", "author": ["Thomas Steinke", "Jonathan Ullman"], "venue": "CoRR, abs/1410.1228,", "citeRegEx": "Steinke and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Steinke and Ullman.", "year": 2014}, {"title": "Optimal probabilistic fingerprint codes", "author": ["G\u00e1bor Tardos"], "venue": "J. ACM,", "citeRegEx": "Tardos.,? \\Q2008\\E", "shortCiteRegEx": "Tardos.", "year": 2008}, {"title": "Answering n2+o(1) counting queries with differential privacy is hard", "author": ["Jonathan Ullman"], "venue": "In STOC, pages 361\u2013370", "citeRegEx": "Ullman.,? \\Q2013\\E", "shortCiteRegEx": "Ullman.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "We show a new lower bound on the sample complexity of (\u03b5,\u03b4)-differentially private algorithms that accurately answer statistical queries on high-dimensional databases. The novelty of our bound is that it depends optimally on the parameter \u03b4, which loosely corresponds to the probability that the algorithm fails to be private, and is the first to smoothly interpolate between approximate differential privacy (\u03b4 > 0) and pure differential privacy (\u03b4 = 0). Specifically, we consider a database D \u2208 {\u00b11}n\u00d7d and its one-way marginals, which are the d queries of the form \u201cWhat fraction of individual records have the i-th bit set to +1?\u201d We show that in order to answer all of these queries to within error \u00b1\u03b1 (on average) while satisfying (\u03b5,\u03b4)-differential privacy, it is necessary that n \u2265\u03a9 \uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u221ad log(1/\u03b4) \u03b1\u03b5 \uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 , which is optimal up to constant factors. To prove our lower bound, we build on the connection between fingerprinting codes and lower bounds in differential privacy (Bun, Ullman, and Vadhan, STOC\u201914). In addition to our lower bound, we give new purely and approximately differentially private algorithms for answering arbitrary statistical queries that improve on the sample complexity of the standard Laplace and Gaussian mechanisms for achieving worst-case accuracy guarantees by a logarithmic factor. \u2217Harvard University School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616. Email: tsteinke@seas.harvard.edu. \u2020Columbia University Department of Computer Science. Supported by a Junior Fellowship from the Simons Society of Fellows. Email: jullman@cs.columbia.edu. ar X iv :1 50 1. 06 09 5v 1 [ cs .D S] 2 4 Ja n 20 15", "creator": "LaTeX with hyperref package"}}}