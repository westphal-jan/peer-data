{"id": "1501.06115", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2015", "title": "Constrained Extreme Learning Machines: A Study on Classification Cases", "abstract": "Extreme learning machine (ELM) is an extremely fast learning method and has a powerful performance for pattern recognition tasks proven by enormous researches and engineers. However, its good generalization ability is built on large numbers of hidden neurons, which is not beneficial to real time response in the test process. In this paper, we proposed new ways, named \"constrained extreme learning machines\" (CELMs), to randomly select hidden neurons based on sample distribution. Compared to completely random selection of hidden nodes in ELM, the CELMs randomly select hidden nodes from the constrained vector space containing some basic combinations of original sample vectors. The experimental results show that the CELMs have better generalization ability than traditional ELM, SVM and some other related methods. Additionally, the CELMs have a similar fast learning speed as ELM.", "histories": [["v1", "Sun, 25 Jan 2015 05:11:34 GMT  (623kb)", "http://arxiv.org/abs/1501.06115v1", "14 pages, 6 figure, journel"], ["v2", "Wed, 4 Feb 2015 11:42:01 GMT  (589kb)", "http://arxiv.org/abs/1501.06115v2", "14 pages, 6 figure, journel"]], "COMMENTS": "14 pages, 6 figure, journel", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["wentao zhu", "jun miao", "laiyun qing"], "accepted": false, "id": "1501.06115"}, "pdf": {"name": "1501.06115.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wentaozhu1991@gmail.com)."], "sections": [{"heading": null, "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "II. REVIEW OF EXTREME LEARNING MACHINE", "text": "ELM is a type of SLFNs. The hidden layer parameters, i.e. the connection weights from the input layer to the hidden nodes, are randomly generated in ELM. The output layer is a linear system in which the connection weights from the hidden layer to the output layer are learned by calculating the hidden layer parameters. However, the ELM network can exhibit an extremely high learning speed due to the simple network structure and its solution in closed form. Furthermore, the randomness does not necessarily cause ELM to iterate these hidden layer parameters. In view of the training samples and class designations > PRODUCT THIS LINE WITH IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 3 = {(,) |,, 1, 2, x, i, i, x, x, x, x, i, x, x, x n."}, {"heading": "III. CONSTRAINED EXTREME LEARNING MACHINES", "text": "In this section we will introduce the CELMs with the idea of using simple linear combinations of sample vectors to create hidden nodes in the traditional ELM network structure."}, {"heading": "A. Constrained Difference Extreme Learning Machine", "text": "It is not the first time that the United States and other countries have agreed on a common line. It is also not the first time that they have agreed on a common line. It is the second time that they have agreed on a common line. It is the second time that they have agreed on a common line. It is the second time that they have agreed on a common line. It is the third time that they have agreed on a common line. It is the third time that they have agreed on a common line. It is the third time that they have agreed on a common line. It is the third time that they have agreed on a common line. It is the first time that they have agreed on a common line. It is the third time that they have agreed on a common line. It is the third time that they have agreed on a common line."}, {"heading": "B. Sample Extreme Learning Machine", "text": "The Sample Learning Machine (SELM) utilizessample vectors that are randomly drawn from the training set to construct the weights from the input layer to the hidden layer. The sample itself can be randomly transformed from standard 1 to serve the linear classification in the output layer. The normalized sample vectors are then assigned as weights from the input layer to the hidden layer. The distortions used in SELM are randomly derived from the uniform distribution as those in ELM. The SELM model is a bit like kernel-based methods. If the activation function is sigmoid function, we can as1 (,), exp [) the ith hidden node in SELM."}, {"heading": "C. Constrained Sum Extreme Learning Machine", "text": "The Constrained Sum Extreme Learning Machine (CSELM) uses sum vectors of random sample vectors selected within the class to construct the weights from the input layer to the hidden plane. CSELM first randomly selects any two sample vectors selected within the class c'x and c '\u2032 x, calculates the sum of the two vectors c c '\u2032 + x, then normalizes the sum vector as22 c cc c c c L '\u2032 \u2032 \u2032 + \u2032 + x. Normalized sum vectors are assigned as weights from the input layer to the hidden layer. The distortions used in CSELM are also randomly generated from the even distribution as in ELM. The sum vectors used here under duress were initially inspired by the differential vectors between the class samples. The limited sum vectors can also be considered as some derived samples that may slightly weaken the influence of noise in samples."}, {"heading": "CSELM.", "text": "From the above discussion we can design the SELM algorithm as algorithm 3. The essence of CSELM is to restrict the input connection weights of the hidden neuron so that they correspond to the instructions of the derivative robust sample vectors. Thus, the random weights must be selected from the sentence composed of the sum vectors within the class sample vectors. Algorithm 3: Formation of the contracted sum Extreme Learning Machine (CSELM) input: the training patterns = {(,) |, 1,} n m i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i"}, {"heading": "D. Random Sum Extreme Learning Machine", "text": "The Random Sum Extreme Learning Machine (RSELM) uses sum vectors of random vectors, independent of classes, to construct the weights from the input layer to the hidden plane. RSELM first randomly selects any two-point vectors, where b and x are calculated as the sum of the two vectors. The distortions used in RSELM are also randomly generated from the even distribution as those in ELM. The sum vectors of the random samples used here are assigned as weights from the input layer to the hidden plane. The distortions used in RSELM are generated just as randomly as those in ELM. The sum vectors of the random sample planes used here accelerate the velocity of the hidden layer weights."}, {"heading": "E. Constrained Mixed Extreme Learning Machine", "text": "The Constrained Mixed Extreme Learning Machine (CMELM) uses mixed vectors with class-bound differential vectors and class-bound sum vectors to construct the weights from the input layer to the hidden level. CMELM first generates half-numbers of hidden nodes, whose weights and biases are constructed with restricted sum vectors, and then generates the others whose weights and biases are constructed with hidden difference vectors. Hidden sum vectors are normalized as those of CSELM, and the restricted difference vectors are normalized as those of CDELM. Normalized sum vectors are assigned as weights from the input layer to the hidden layer. Restricted mixed vectors can be considered > REPLACE."}, {"heading": "IV. PERFORMANCE EVALUATION", "text": "In this section, we evaluate the proposed CELMs and compare them with some classifiers, such as ELM, SVM and some related deep learning methods, on both synthetic and real data sets. Ten rounds of experiments are performed for each data set. In each experiment, the training set and test set are randomly generated from samples from synthetic data sets and the UCI database [39]. Samples from the UCI database are normalized to zero mean and unit variance. Performance is recorded using the means and standard deviations of classification accuracy. In these experiments, we also compare the CELMs with the orthogonal ELM [40], which makes weight vectors orthogonal to each other and orthogonally distorts them to each other. The aim of this comparison is to compare CELMs with other ELM-related methods that appear in the literature."}, {"heading": "A. Experiments on Synthetic Dataset", "text": "It will be able to maintain the symmetrical shape of the spiral, and the rest will be used as a test. These data will be derived from the original spiral data, and the two sets will be generated in each of the ten rounds of experiments."}, {"heading": "B. Experiments on UCI Datasets", "text": ", \"he said in an interview with the\" New York Times. \"He pointed out that in recent years it has been in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "ELM.", "text": "The results of the previous layer are transformed by a random matrix, and then the transformed results are added to the original characteristics, and the modified characteristics are entered into the next layer after they have been transformed by a sigmoid function. DCN is also a deep learning model, but its building block is an ELM-based model in which parts of the hidden nodes are built with random projection and the other part of the hidden nodes are built with RBM weights [31, 45]. Instead of the way in which the results of the previous layer are added to the next layer as a bias, the output of the previous layer is considered a bias of the next layer, the output of the previous layer is used as a bias [the results of the previous layer] and the results of the next layer DELM are used as a default."}, {"heading": "TABLE \u2161", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "PERFORMANCE ON MNIST DATASET", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "TABLE \u2162", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "PERFORMANCE ON CIFAR-10 DATASET", "text": "Sample ELM 7000 notes 0.733 Random Sum ELM 7000 notes 0.727 Constrained Mixed ELM 7000 notes 0.723 The CELM can perform better than the linear SVM, DCN and R2SVM. The CELMs have a test accuracy of at least 8 percent higher than that of the linear SVM. The CELM has 60 layers, and each layer is a linear SVM transformation. Although R2SVM and DCN have many layers, the CELMs are covered by a hidden layer. The CELMs of a hidden layer have a test accuracy of at least 3 percent higher than that of these layers, and each layer is a linear SVM transformation."}, {"heading": "V. DISCUSSION", "text": "We have compared CELMs with several ELM-based methods, such as normalized ELM [37], orthogonal ELM, and ELM. The most important contributions of the CELMs to the ELM study are: 1) we introduce a limited hidden weight based on sample distributions, 2) we normalize the hidden weights around the square of their l2 standards, other than the l2 standard (the normalized ELM is taken as a strategy.) Several studies have validated the effectiveness of CELMs [57]. From the experiments of the CELMs, we find that if the number of hidden nodes is small, the CELMs significantly exceed the normalized ELM, orthogonal ELM, and ELM. However, if the number of hidden nodes is large, the margins between the ELM and the normalized ELM are not so large. Observation shows that the limited weights and the normalized strategy both work for the success of the CELMs."}, {"heading": "VI. CONCLUSION", "text": "In order to address the inefficient use of hidden nodes in the ELM, novel learning models, CELMs, have been proposed in this paper. CELMs limit their random weight generation from a smaller space than the ELM, i.e. they replace the completely random weight vectors with those randomly drawn from the set of simple linear combinations of sample vectors. CELMs \"main contribution is that they introduce sample distribution before building the hidden layer in order to achieve a better mapping of the properties and benefit from the linear classification of the next layer. Effective mapping of the properties contributes significantly to the efficient use of hidden nodes in the ELM. Extensive comparisons between CELMs and some related methods on both synthetic and real data sets showed that CELMs perform better in almost all cases. However, the CELMs still have some problems that typical ELMs have in property. One of these is that CELMs address the problem of hidden ELMs, although the problem of overlying the ELMs is very large."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Dr. L. L. C. Kasun, Dr. H.Zhou, Prof. G.-B. Huang from Nanyang Technological University, Singapore and Prof. C. M. Vong from the University of Macau, Macau, for their kind help with the Multi-Layer Extreme Learning Machine (ML-ELM)."}], "references": [{"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["M. Leshno", "V. Ya. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks, vol. 6(6), pp. 861\u2013867, 1993.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Capabilities of a four-layered feedforward neural network: four layers versus three", "author": ["S. Tamura", "M. Tateishi"], "venue": "IEEE Trans. Neural Networks, vol. 8(2), pp. 251\u2013255, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions", "author": ["G.-B. Huang", "H.A. Babri"], "venue": "IEEE Trans. Neural Networks, vol. 9(1), pp. 224\u2013229, 1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Beyond regression: new tools for prediction and analysis in the behavioral sciences", "author": ["P. Werbos"], "venue": "Ph.D. Thesis Harvard University, Cambridge, MA, 1974.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1974}, {"title": "The Levenberg-Marquardt algorithm: implementation and theory", "author": ["J.J. Mor\u00e9"], "venue": "Numerical analysis, Springer Berlin Heidelberg, pp. 105-116, 1978.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1978}, {"title": "A simple procedure for pruning back-propagation trained neural networks", "author": ["E.D. Karnin"], "venue": "IEEE Trans. Neural Networks, vol. 1(2), pp. 239-242, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Evolving artificial neural networks", "author": ["X. Yao"], "venue": "Proceedings of the IEEE, vol. 87(9), pp. 1423-1447, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast generic selection of features for neural network classifiers", "author": ["F.Z. Brill", "D.E. Brown", "W.N. Martin"], "venue": "IEEE Trans. Neural Networks, vol. 3(2), pp. 324-328, 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20(3), pp. 273-297, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "A one-layer recurrent neural network for support vector machine learning", "author": ["Y. Xia", "J. Wang"], "venue": "IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 34, no. 2, pp. 1261\u20131269, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Adaptive radial basis function nonlinearities, and the problem of generalization", "author": ["D. Lowe"], "venue": "First IEE International Conference on Artificial Neural Networks (Conf. Publ. No. 313), IET, pp. 171-175, October, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "The No-Prop algorithm: A new learning algorithm for multilayer neural networks", "author": ["B. Widrow", "A. Greenblatt", "Y. Kim", "D. Park"], "venue": "Neural Networks, vol. 37, pp. 182-188, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive switching circuits", "author": ["B. Widrow", "M.E. Hoff"], "venue": "IRE WESCON Convention Record, 1960.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1960}, {"title": "Extreme learning machine: Theory and applications", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, vol. 70 (1\u20133), pp. 489\u2013501, Dec. 2006, [Code: http://www.ntu.edu.sg/home/egbhuang/elm_ random_hidden_nodes.html].", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: A new learning scheme of feedforward neural networks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Proceedings of International Joint Conference on Neural Networks (IJCNN2004), vol. 2, (Budapest, Hungary), pp. 985\u2013990, 25-29 Jul., 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.-B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Transactions > REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < 14 on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 42(2), pp. 513-529, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Trans. Inf. Theory, vol. 44(2), pp. 525\u2013536, 1998.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Semi-supervised and unsupervised extreme learning machines", "author": ["G. Huang", "S. Song", "J.N. Gupta", "C. Wu"], "venue": "IEEE Trans Cybern, 2014, in press.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Online sequential fuzzy extreme learning machine for function approximation and classification problems", "author": ["H.-J. Rong", "G.-B. Huang", "N. Sundararajan", "P. Saratchandran"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39(4), pp. 1067-1072, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A neuro-fuzzy inference system through integration of fuzzy logic and extreme learning machines", "author": ["Z.-L. Sun", "K.-F. Au", "T.-M. Choi"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 37(5), pp. 1321-1331, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Vehicle detection in driving simulation using extreme learning machine", "author": ["W. Zhu", "J. Miao", "J. Hu", "L. Qing"], "venue": "Neurocomputing, vol. 128, pp. 160-165, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse Extreme Learning Machine for Classification", "author": ["Z. Bai", "G.-B. Huang", "D. Wang", "H. Wang", "M.B. Westover"], "venue": "IEEE Transactions on Cybernetics, 2014, in press.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Evolutionary extreme learning machine", "author": ["Q.-Y. Zhu", "A.K. Qin", "P.N. Suganthan", "G.-B. Huang"], "venue": "Pattern Recognition, vol. 38(10), pp. 1759\u2013 1763, Oct. 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "A constructive enhancement for online sequential extreme learning machine", "author": ["L. Yuan", "Y.C. Soh", "G.-B. Huang"], "venue": "Proceedings of International Joint Conference on Neural Networks (IJCNN2009), pp. 1708\u20131713, 14-19 Jun., 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "OP-ELM: optimally pruned extreme learning machine", "author": ["Y. Miche", "A. Sorjamaa", "P. Bas", "O. Simula", "C. Jutten", "A.A. Lendasse"], "venue": "Neural Networks, IEEE Transactions on, vol. 21(1), pp. 158-162, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast pruned-extreme learning machine for classification problem", "author": ["H.J. Rong", "Y.S. Ong", "A.H. Tan", "Z. Zhu"], "venue": "Neurocomputing, vol. 72(1), pp. 359-366, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient and effective algorithms for training single-hidden-layer neural networks", "author": ["D. Yu", "L. Deng"], "venue": "Pattern Recognition Letters, vol. 33(5), pp. 554\u2013558, 1 Apr. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "The MIT Press, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Classifiability-based discriminatory projection pursuit", "author": ["Y. Su", "S. Shan", "X. Chen", "W. Gao"], "venue": "IEEE Trans. Neural Networks, vol. 22(12), pp. 2050-2061, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with recursive perceptual representations", "author": ["O. Vinyals", "Y. Jia", "L. Deng", "T. Darrell"], "venue": "Advances in Neural Information Processing Systems, pp. 2834-2842, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convex net: A scalable architecture for speech pattern classification", "author": ["L. Deng", "D. Yu"], "venue": "Proceedings of the Interspeech 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master's thesis, Department of Computer Science, University of Toronto, 2009, [http://www.cs.toronto.edu/~kriz/ cifar.html].", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17(6), pp. 1411-1423, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimization method based extreme learning machine for classification", "author": ["G.-B. Huang", "X. Ding", "H. Zhou"], "venue": "Neurocomputing, vol. 74(1), pp. 155-163, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust regression with extreme support vectors", "author": ["W. Zhu", "J. Miao", "L. Qing"], "venue": "pattern recognition letters, vol. 45, pp. 205-210, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Extreme support vector machine classifier", "author": ["Q. Liu", "Q. He", "Z. Shi"], "venue": "Advances in Knowledge Discovery and Data Mining, Springer Berlin Heidelberg, pp. 222-233, 2008.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained Extreme Learning Machine: a Novel Highly Discriminative Random Feedforward Neural Network", "author": ["W. Zhu", "J. Miao", "L. Qing"], "venue": "Proceedings of International Joint Conference on Neural Networks (IJCNN2014), pp. 800-807, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop", "N.M. Nasrabadi"], "venue": "New York: springer,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "UCI Repository of machine learning databases", "author": ["C.L. Blake", "C.J. Merz"], "venue": "[http://www.ics.uci.edu/~mlearn/MLRepository. html]. Irvine, CA: University of California. Department of Information and Computer Science,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1998}, {"title": "Representational Learning with Extreme Learning Machine for Big Data", "author": ["L.L.C. Kasun", "H. Zhou", "G.-B. Huang", "C.M. Vong"], "venue": "IEEE Intelligent Systems, vol. 28, no. 6, pp. 31-34, December 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "IEEE International Conference on Neural Networks, pp. 586-591, 1993.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1993}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86(11), pp. 2278-2324, 1998, [Online]. Available: http://yann.lecun.com/exdb/mnist.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11) (pp. 921-928), 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "On the difference between orthogonal matching pursuit and orthogonal least squares", "author": ["T. Blumensath", "M.E. Davies"], "venue": "unpublished manuscript, 2007, [http://www.see.ed.ac.uk/~tblumens/papers/BD OMPvsOLS07.pdf].", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation, vol. 18(7), pp. 1527-1554, 2006.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371-3408, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R.R.R. Salakhutdinov"], "venue": "Science, vol. 313(5786), pp. 504-507, 2006.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient learning of deep Boltzmann machines", "author": ["R. Salakhutdinov", "H. Larochelle"], "venue": "International Conference on Artificial Intelligence and Statistics, pp. 693-700, 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning a parametric embedding by preserving local structure", "author": ["L. Maaten"], "venue": "International Conference on Artificial Intelligence and Statistics,  pp.  384-391,  2009, [Code: http://homepage.tudelft.nl/19j49/t-SNE.html].", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290(5500), pp. 2323-2326, 2000.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Extreme support vector regression", "author": ["W. Zhu", "J. Miao", "L. Qing"], "venue": "Proceedings of International Conference on Extreme Learning Machines (ELM2013), Beijing, China, Spring-Verlag, 15-17 Oct. 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Classifiability-based discriminatory projection pursuit", "author": ["Y. Su", "S. Shan", "X. Chen", "W. Gao"], "venue": "IEEE Trans. Neural Networks, vol. 22(12), pp. 2050-2061, 2011.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast, simple and accurate handwritten digit classification using extreme learning machines with shaped input-weights", "author": ["M.D. McDonnell", "M.D. Tissera", "A. van Schaik"], "venue": "arXiv preprint arXiv:1412.8307, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has been proven that the learning capacity of a multilayer feedforward neural network with non-polynomial activation functions can approximate any continuous function [1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "The learning capacity of SLFNs is not inferior to that of multilayer feedforward neural networks, as proved in [2, 3].", "startOffset": 111, "endOffset": 117}, {"referenceID": 2, "context": "The learning capacity of SLFNs is not inferior to that of multilayer feedforward neural networks, as proved in [2, 3].", "startOffset": 111, "endOffset": 117}, {"referenceID": 3, "context": "The most famous gradient based learning method is back-propagation algorithm [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "(LM) method [5], dynamically network construction [6], evolutionary algorithms [7] and generic optimization [8], the enhanced methods require heavy computation or cannot obtain a global optimal solution.", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "One of the most popular optimization based SLFNs is Support Vector Machine (SVM) [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "SVM is a very popular method attracting many researchers [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Least Mean Square (LMS) based methods, such as Radial Basis Function network [11] and No-Prop network [12] based on LMS algorithm [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Least Mean Square (LMS) based methods, such as Radial Basis Function network [11] and No-Prop network [12] based on LMS algorithm [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Least Mean Square (LMS) based methods, such as Radial Basis Function network [11] and No-Prop network [12] based on LMS algorithm [13].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "[14] proposed a novel extremely fast learning model of SLFNs, called Extreme Learning Machine (ELM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] further completely proved the random feature mapping theory rigorously.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "After the random nonlinear feature mapping in the hidden layer, the rest of ELM can be considered as a linear system [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "The essence of the linear system used by ELM is to minimize the training error and the norm of connection weights from the hidden layer to the output layer at the same time [16].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "Hence ELM has a good generalization performance according to the feedforward neural network theory [12, 17].", "startOffset": 99, "endOffset": 107}, {"referenceID": 16, "context": "Hence ELM has a good generalization performance according to the feedforward neural network theory [12, 17].", "startOffset": 99, "endOffset": 107}, {"referenceID": 15, "context": "Additionally, ELM has a unified framework for classification, regression, semi-supervised, supervised and unsupervised tasks [16, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 17, "context": "Additionally, ELM has a unified framework for classification, regression, semi-supervised, supervised and unsupervised tasks [16, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 18, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 19, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 20, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 21, "context": "These advantages lead to the popularity of ELM both for researchers and engineers [19, 20, 21, 22].", "startOffset": 82, "endOffset": 98}, {"referenceID": 22, "context": "However, the random selection of hidden layer parameters makes quite inefficient use of hidden nodes [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "Use online incremental learning methods to add hidden layer nodes dynamically [23, 24].", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Use online incremental learning methods to add hidden layer nodes dynamically [23, 24].", "startOffset": 78, "endOffset": 86}, {"referenceID": 24, "context": "Use pruning methods to select the candidate hidden layer nodes [25, 26].", "startOffset": 63, "endOffset": 71}, {"referenceID": 25, "context": "Use pruning methods to select the candidate hidden layer nodes [25, 26].", "startOffset": 63, "endOffset": 71}, {"referenceID": 26, "context": "Use gradient based methods to update the weights from the input layer to the hidden layer in ELM [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "LDA [28] is probably the most commonly used method to extract discriminative features.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "[29] proposed a projection pursuit based LDA method to overcome these problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "We also compared the CELM algorithms with SVM and ELM related algorithms [30, 31] on CIFAR-10 data set [32].", "startOffset": 73, "endOffset": 81}, {"referenceID": 30, "context": "We also compared the CELM algorithms with SVM and ELM related algorithms [30, 31] on CIFAR-10 data set [32].", "startOffset": 73, "endOffset": 81}, {"referenceID": 31, "context": "We also compared the CELM algorithms with SVM and ELM related algorithms [30, 31] on CIFAR-10 data set [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "The output layer is a linear system, where the connection weights from the hidden layer to the output layer are learned by computing the Moore-Penrose generalized inverse [14].", "startOffset": 171, "endOffset": 175}, {"referenceID": 32, "context": "However, the condition number of the random projected matrix H may be very large and the above traditional ELM model may encounter ill-posed problems [33].", "startOffset": 150, "endOffset": 154}, {"referenceID": 33, "context": "In practice, regularized term with hidden layer\u2019s output connection weights \u03b2 is added into the optimization objective to avoid the problem [34, 35, 36].", "startOffset": 140, "endOffset": 152}, {"referenceID": 34, "context": "In practice, regularized term with hidden layer\u2019s output connection weights \u03b2 is added into the optimization objective to avoid the problem [34, 35, 36].", "startOffset": 140, "endOffset": 152}, {"referenceID": 35, "context": "In practice, regularized term with hidden layer\u2019s output connection weights \u03b2 is added into the optimization objective to avoid the problem [34, 35, 36].", "startOffset": 140, "endOffset": 152}, {"referenceID": 33, "context": "As analyzed in theory and further verified by the simulation results in [34], ELM for classification tends to achieve better generalization performance than traditional SVM.", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "Although the method [29] is rather complex with many embedded trivial tricks, it shows that the difference vectors of between-class samples are effective to classification tasks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "In our originally Constrained Difference Extreme Learning Machine [37], we deleted the difference vectors of small norms and too relevant difference vectors.", "startOffset": 66, "endOffset": 70}, {"referenceID": 37, "context": "Although the two processes can improve the performance [38], the improvement is very little in our experiments.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "The difference between kernel ELM [16] and our SELM is that ELM kernel uses all the training samples.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 0, "context": "draw the corresponding bias b randomly from [0,1] uniform distribution.", "startOffset": 44, "endOffset": 49}, {"referenceID": 38, "context": "In each experiment, the training set and the test set are randomly generated using the samples from synthetic datasets and UCI database [39].", "startOffset": 136, "endOffset": 140}, {"referenceID": 39, "context": "In these experiments, we also compare the CELMs with the orthogonal ELM [40], which makes weight vectors orthogonal to each other and biases orthogonal to each other.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "The code of ELM used in the experiments was downloaded from [14].", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "To retain the symmetrical shape of the spiral, we normalize the samples into the range [-1, 1] as same as that in [14].", "startOffset": 87, "endOffset": 94}, {"referenceID": 13, "context": "To retain the symmetrical shape of the spiral, we normalize the samples into the range [-1, 1] as same as that in [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 38, "context": "Experiments on UCI Datasets Six datasets from UCI database [39], including Wisconsin diagnostic breast cancer dataset (WDBC) and digit datasets in five different features (MDD-fac, fou, kar, pix, zer), are used for evaluating the proposed CELMs.", "startOffset": 59, "endOffset": 63}, {"referenceID": 40, "context": "The training method of BP neural network is RPROP [41] due to time and memory problems in the experiments.", "startOffset": 50, "endOffset": 54}, {"referenceID": 42, "context": "The liner SVM code used is the MATLAB code obtained from [43].", "startOffset": 57, "endOffset": 61}, {"referenceID": 41, "context": ", MNIST [42] and CIFAR-10 [32].", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": ", MNIST [42] and CIFAR-10 [32].", "startOffset": 26, "endOffset": 30}, {"referenceID": 42, "context": "In CIFAR-10 dataset, the standard evaluation pipeline defined in [43] is adopted.", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "The codebook is trained with OMP-1 [44] and the codebook size is 50 in the experiment.", "startOffset": 35, "endOffset": 39}, {"referenceID": 39, "context": "Experiments are conducted on MNIST data set to compare CELMs with ELM, orthogonal ELM and Multi-Layer Extreme Learning Machine (ML-ELM) [40].", "startOffset": 136, "endOffset": 140}, {"referenceID": 45, "context": "The ML-ELM is a Stacked Denoising Auto Encoder (SDAE) model [46] based on ELM.", "startOffset": 60, "endOffset": 64}, {"referenceID": 47, "context": "The original ML-ELM with three layers of hidden nodes 700-700-15000 can outperform other deep learning methods, such as Deep Belief Network (DBN) [48], Deep Boltzmann Machine (DBM) [48], Stacked Auto Encoder (SAE) and SDAE [46].", "startOffset": 146, "endOffset": 150}, {"referenceID": 47, "context": "The original ML-ELM with three layers of hidden nodes 700-700-15000 can outperform other deep learning methods, such as Deep Belief Network (DBN) [48], Deep Boltzmann Machine (DBM) [48], Stacked Auto Encoder (SAE) and SDAE [46].", "startOffset": 181, "endOffset": 185}, {"referenceID": 45, "context": "The original ML-ELM with three layers of hidden nodes 700-700-15000 can outperform other deep learning methods, such as Deep Belief Network (DBN) [48], Deep Boltzmann Machine (DBM) [48], Stacked Auto Encoder (SAE) and SDAE [46].", "startOffset": 223, "endOffset": 227}, {"referenceID": 29, "context": ", Linear SVM and RSVM [30], and deep ELM related method, e.", "startOffset": 22, "endOffset": 26}, {"referenceID": 30, "context": ", DCN [31], on CIFAR-10 dataset.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "The DCN is also a deep learning model, but its building block is an ELM based model, in which parts of the hidden nodes are built with random projection and the other part of hidden nodes are built with RBM weights [31, 45].", "startOffset": 215, "endOffset": 223}, {"referenceID": 44, "context": "The DCN is also a deep learning model, but its building block is an ELM based model, in which parts of the hidden nodes are built with random projection and the other part of hidden nodes are built with RBM weights [31, 45].", "startOffset": 215, "endOffset": 223}, {"referenceID": 29, "context": "Table III shows the performances of ELM, orthogonal ELM, CELMs, linear-SVM, RSVM [30] and DCN [31] methods.", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "Table III shows the performances of ELM, orthogonal ELM, CELMs, linear-SVM, RSVM [30] and DCN [31] methods.", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Note the performances of linear SVM, RSVM and DCN are cited from [30].", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "And the experimental conditions of ELM, orthogonal ELM and CELMs, such as the features and the number of used training and test sets, are the same with [30].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "In [30], the RSVM has 60 layers, and each layer is a linear SVM after random projection and sigmoid transformation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "The visualization method, t-SNE [49], is used.", "startOffset": 32, "endOffset": 36}, {"referenceID": 37, "context": "The s-SNE has a much better visualization effect than other methods, such as PCA [38], LLE [50] and Auto Encoder [47], on MNIST data set.", "startOffset": 81, "endOffset": 85}, {"referenceID": 49, "context": "The s-SNE has a much better visualization effect than other methods, such as PCA [38], LLE [50] and Auto Encoder [47], on MNIST data set.", "startOffset": 91, "endOffset": 95}, {"referenceID": 46, "context": "The s-SNE has a much better visualization effect than other methods, such as PCA [38], LLE [50] and Auto Encoder [47], on MNIST data set.", "startOffset": 113, "endOffset": 117}, {"referenceID": 36, "context": "We have compared CELMs with several ELM based methods, such as normalized ELM [37], orthogonal ELM, and ELM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 51, "context": "The weights based on sample distribution work well for the dimension reduction [52].", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "To our relief, the methods in [34, 35, 51] can tackle the problem effectively.", "startOffset": 30, "endOffset": 42}, {"referenceID": 34, "context": "To our relief, the methods in [34, 35, 51] can tackle the problem effectively.", "startOffset": 30, "endOffset": 42}, {"referenceID": 50, "context": "To our relief, the methods in [34, 35, 51] can tackle the problem effectively.", "startOffset": 30, "endOffset": 42}], "year": 2015, "abstractText": null, "creator": "Acrobat PDFMaker 9.0 Word \u7248"}}}