{"id": "1501.06237", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2015", "title": "Deep Transductive Semi-supervised Maximum Margin Clustering", "abstract": "Semi-supervised clustering is an very important topic in machine learning and computer vision. The key challenge of this problem is how to learn a metric, such that the instances sharing the same label are more likely close to each other on the embedded space. However, little attention has been paid to learn better representations when the data lie on non-linear manifold. Fortunately, deep learning has led to great success on feature learning recently. Inspired by the advances of deep learning, we propose a deep transductive semi-supervised maximum margin clustering approach. More specifically, given pairwise constraints, we exploit both labeled and unlabeled data to learn a non-linear mapping under maximum margin framework for clustering analysis. Thus, our model unifies transductive learning, feature learning and maximum margin techniques in the semi-supervised clustering framework. We pretrain the deep network structure with restricted Boltzmann machines (RBMs) layer by layer greedily, and optimize our objective function with gradient descent. By checking the most violated constraints, our approach updates the model parameters through error backpropagation, in which deep features are learned automatically. The experimental results shows that our model is significantly better than the state of the art on semi-supervised clustering.", "histories": [["v1", "Mon, 26 Jan 2015 02:28:18 GMT  (58kb,D)", "http://arxiv.org/abs/1501.06237v1", "14"]], "COMMENTS": "14", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen"], "accepted": false, "id": "1501.06237"}, "pdf": {"name": "1501.06237.pdf", "metadata": {"source": "CRF", "title": "Deep Transductive Semi-supervised Maximum Margin Clustering", "authors": ["Gang Chen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The fact is that we are able to manoeuvre ourselves into a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation in which we are in a situation."}, {"heading": "2 Related work", "text": "In fact, the fact is that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, a process and a process, a process, a process, a process, a process, a process and a process in which there is a process, a process, a process and a process in which there is a process, a process, and a process, and a process in which there is a process."}, {"heading": "3 Deep Transductive Semi-supervised Maximum Margin Cluster-", "text": "ingIn this section we will present the transductive semi-supervised maximum margin clustering, in which deep characteristics are simultaneously learned in a uniform framework."}, {"heading": "3.1 Overview of our approach", "text": "Let X = {xi} Ni = 1 (xi-RD) be a set of N examples belonging to K clusters referred to as Z. In addition to the unlabeled data, there are other partially labeled data in the form of pair-by-pair constraints C = {(xi, xj, \u03b4 (zi = zj)}, which are a kind of secondary information to determine whether the two constraints (xi, xj) originate from the same cluster or not (indicated by the \u03b4 function). Most methods attempt to learn weights wk-RD for each cluster k = [1, K] in order to satisfy these constraints as far as possible. Instead of learning a linear mapping or a Mahalanobis metric [23, 30] we are interested in a non-linear mapping function. To make it easy to understand, we assume that we have learned a non-linear mapping function f: RD \u2192 Rd."}, {"heading": "3.2 Objective function", "text": "Similarly, as in recent years, we will integrate the irrationally limited learning content into the framework, in which we provide the data. (...) Similarly, as in the past 20, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,"}, {"heading": "3.3 Parameter learning", "text": "We learn the parameters in an alternate way: (1) data projection, given the model parameters; (2) and then optimize the model parameters with gradient descend. To calculate the gradients of the parameters, we must first find the most violated limits. (2) For the same label pairs, we have the following most violated values: A + = 1, hj, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0"}, {"heading": "4 Experiments", "text": "In this section, we presented a series of experiments comparing our method with the most advanced semi-monitored cluster methods on a wide range of datasets, including UCI and Reuters datasets in Table 1, as well as the MNIST digits, COIL-20 and COIL-100 datasets. We also investigated whether or not the transductive constraint in Equivalent (7) is helpful for cluster analysis."}, {"heading": "4.1 Experimental setup", "text": "In the experiments, we compared our method with the state of the art semi-supervised cluster approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34]. Note that Xing, ITML and KISSME are the semi-supervised approaches to metric learning (Mahalanobis). Therefore, we used these methods to learn the metric and calculate the distances between all distances, then we used the kernel k mean [7] for clustering. Our method and CMMC are similar, which can be directly optimized for clustering. As for parameter setting, we set \u03bb = 0.02 and \u03b2 = 1. The learning rate will decrease in the iterations in our model by setting a distance W = 1 \u03bb \u00d7 (i + 1), where i is the index for iterations; while the learning rate for weights is fixed in the deep network, with a distance of 0.01, for performance."}, {"heading": "4.2 Results", "text": "It is indeed the case that we are able to set out in search of new paths that will lead us to another world, in which we can move to another world."}, {"heading": "5 Conclusions", "text": "In this paper, we propose a deep transductive, semi-supervised approach to maximum margin clustering. On the one hand, we use deep learning to learn non-linear representations that can be used as input for the semi-supervised clustering model. On the other hand, we incorporate non-label instances into our semi-supervised clustering framework. Thus, our model combines transductive learning, deep learning, maximum margin and semi-supervised clustering in one framework. Compared to conventional methods, our approach can learn non-linear mappings as well as use transductive information to improve clustering performance. We train the deep structure with stacked restricted Boltzmann machines, layer by layer, greedy for feature representations and optimize our objective function with decent gradient. We demonstrate the advantages of our model over the state of the art in the experiments."}], "references": [{"title": "Learning distance functions using equivalence relations", "author": ["A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning, pages 11\u201318,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "PAMI, pages 1798\u20131828,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating constraints and metric learning in semi-supervised clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, pages 11\u2013, New York, NY, USA,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM J. Sci. Comput., 16(5):1190\u20131208, Sept.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Semi-supervised clustering with user feedback", "author": ["D. Cohn", "R. Caruana", "A. Mccallum"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 209\u2013216, New York, NY, USA,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel k-means, spectral clustering and normalized cuts", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "KDD,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Why does unsupervised pre-training help deep learning? J", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "Mach. Learn. Res., 11:625\u2013660, Mar.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "On herding and the perceptron cycling theorem", "author": ["A. Gelfand", "Y. Chen", "L. van der Maaten", "M. Welling"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S.T. Roweis"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems 17, pages 513\u2013520. MIT Press,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput., 18(7):1527\u20131554, jul", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507, July", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Maximum margin clustering with pairwise constraints", "author": ["Y. Hu", "J. Wang", "N. Yu", "X.-S. Hua"], "venue": "ICDM, pages 253\u2013262. IEEE Computer Society,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification, 2(1):193\u2013218,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1985}, {"title": "From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering", "author": ["D. Klein", "S.D. Kamvar", "C.D. Manning"], "venue": "Proceedings of the Nineteenth International Conference on Machine Learning, ICML \u201902, pages 307\u2013314, San Francisco, CA, USA,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Koestinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Proc. IEEE Intern. Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "ICML, pages 536\u2013543, New York, NY, USA,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "J. Mach. Learn. Res., 13(1):643\u2013669, Mar.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving classification with pairwise constraints: A margin-based approach", "author": ["N. Nguyen", "R. Caruana"], "venue": "ECML/PKDD (2), pages 113\u2013124,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W. Rand"], "venue": "Journal of the American Statistical Association, 66(336):846\u2013850,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1971}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, pages 94\u2013, New York, NY, USA,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep learning using support vector machines", "author": ["Y. Tang"], "venue": "Workshop on Representational Learning, ICML 2013, volume abs/1306.0239,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, pages 1453\u20131484,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Generalized maximum margin clustering and unsupervised kernel learning", "author": ["H. Valizadegan", "R. Jin"], "venue": "B. Schlkopf, J. Platt, and T. Hoffman, editors, NIPS, pages 1417\u20131424. MIT Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Springer-Verlag New York, Inc.,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res., 11:3371\u20133408, Dec.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages 577\u2013584, San Francisco, CA, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res., 10:207\u2013244, June", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle"], "venue": "International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 15, pages 505\u2013512. MIT Press,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "NIPS17, pages 1537\u20131544,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Semi-supervised maximum margin clustering with pairwise constraints", "author": ["H. Zeng", "Y. ming Cheung"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "On the value of pairwise constraints in classification and consistency", "author": ["J. Zhang", "R. Yan"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 1111\u20131118, New York, NY, USA,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 31, "context": "In general, a pairwise constraint between two examples indicates whether they belong to the same cluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes that the pair of instances should be partitioned into the same cluster, while a different-label (or cannot-link) constraint specifies that the pair of instances should be assigned into different clusters [32, 23, 30].", "startOffset": 408, "endOffset": 420}, {"referenceID": 22, "context": "In general, a pairwise constraint between two examples indicates whether they belong to the same cluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes that the pair of instances should be partitioned into the same cluster, while a different-label (or cannot-link) constraint specifies that the pair of instances should be assigned into different clusters [32, 23, 30].", "startOffset": 408, "endOffset": 420}, {"referenceID": 29, "context": "In general, a pairwise constraint between two examples indicates whether they belong to the same cluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes that the pair of instances should be partitioned into the same cluster, while a different-label (or cannot-link) constraint specifies that the pair of instances should be assigned into different clusters [32, 23, 30].", "startOffset": 408, "endOffset": 420}, {"referenceID": 31, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 2, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 10, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 22, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 5, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 29, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 33, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 26, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 24, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 34, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 29, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 33, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 26, "context": "Although kernel methods are widely used for non-linear cases, it is a shallow approach and needs to specify hyper parameters in most situations [27, 30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 29, "context": "Although kernel methods are widely used for non-linear cases, it is a shallow approach and needs to specify hyper parameters in most situations [27, 30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 7, "context": "the training of deep networks provide a way to learn non-linear transformations of data, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "the training of deep networks provide a way to learn non-linear transformations of data, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 11, "context": "Inspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach, which inherits both advantages from deep learning and maximum margin methods.", "startOffset": 29, "endOffset": 40}, {"referenceID": 27, "context": "Inspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach, which inherits both advantages from deep learning and maximum margin methods.", "startOffset": 29, "endOffset": 40}, {"referenceID": 1, "context": "Inspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach, which inherits both advantages from deep learning and maximum margin methods.", "startOffset": 29, "endOffset": 40}, {"referenceID": 30, "context": "Our method can learn features automatically from observation, kind of learning a metric as in [31].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which is helpful for clustering and classification [2].", "startOffset": 19, "endOffset": 27}, {"referenceID": 33, "context": "Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which is helpful for clustering and classification [2].", "startOffset": 19, "endOffset": 27}, {"referenceID": 1, "context": "Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which is helpful for clustering and classification [2].", "startOffset": 139, "endOffset": 142}, {"referenceID": 24, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 198, "endOffset": 206}, {"referenceID": 29, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 198, "endOffset": 206}, {"referenceID": 22, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 243, "endOffset": 251}, {"referenceID": 29, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 243, "endOffset": 251}, {"referenceID": 32, "context": "The maximum margin clustering (MMC) aims to find the hyperplanes that can partition the data into different clusters over all possible labels with large margins [33, 26, 35].", "startOffset": 161, "endOffset": 173}, {"referenceID": 25, "context": "The maximum margin clustering (MMC) aims to find the hyperplanes that can partition the data into different clusters over all possible labels with large margins [33, 26, 35].", "startOffset": 161, "endOffset": 173}, {"referenceID": 34, "context": "The maximum margin clustering (MMC) aims to find the hyperplanes that can partition the data into different clusters over all possible labels with large margins [33, 26, 35].", "startOffset": 161, "endOffset": 173}, {"referenceID": 13, "context": "Nevertheless, the accuracy of the clustering results by MMC may not be good sometimes due to the nature of its unsupervised learning [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 15, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 31, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 0, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 4, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 2, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 10, "context": "In particular, COPKmeans [11] is a semi-supervised variant of Kmeans, by following the same clustering procedure of Kmeans while avoiding violations of pairwise constraints.", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "MPCKmeans [3] extended Kmeans and utilized both metric learning and pairwise constraints in the clustering process.", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "More recently, [20] show that they can improve classification with pairwise constraints under maximum margin framework.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "[34] leverage the margin-based approach on the semi-supervised clustering problems, and yield competitive results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 10, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 9, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 5, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 29, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "The pseudo-metric [23] parameterized by positive semi-definite matrices (PSD) is learned with an online updating rule, that alternates between projections onto PSD and onto half-space constraints imposed by the instance pairs.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "[32] proposed to learn a distance metric (Mahalanobis) that respects pairwise constraints for clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "In [6], an information-theoretic approach to learning a Mahalanobis distance function via LogDet divergence is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 29, "context": "Recently, a supervised approach to learn Mahalanobis metric is also proposed in [30], by minimizing the pairwise distances between instances in the same cluster, while increasing the separation between data points with dissimilar classes.", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "On the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension", "startOffset": 52, "endOffset": 63}, {"referenceID": 27, "context": "On the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension", "startOffset": 52, "endOffset": 63}, {"referenceID": 1, "context": "On the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension", "startOffset": 52, "endOffset": 63}, {"referenceID": 12, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 10, "endOffset": 18}, {"referenceID": 30, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 10, "endOffset": 18}, {"referenceID": 11, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 47, "endOffset": 55}, {"referenceID": 18, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 47, "endOffset": 55}, {"referenceID": 7, "context": "In a sense, the success of deep learning lies on learned features, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 118, "endOffset": 124}, {"referenceID": 1, "context": "In a sense, the success of deep learning lies on learned features, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 118, "endOffset": 124}, {"referenceID": 17, "context": "For example, the binary hidden units in the discriminative Restricted Boltzmann Machines (RBMs) [18, 9] can model latent features of the data that improve classification.", "startOffset": 96, "endOffset": 103}, {"referenceID": 8, "context": "For example, the binary hidden units in the discriminative Restricted Boltzmann Machines (RBMs) [18, 9] can model latent features of the data that improve classification.", "startOffset": 96, "endOffset": 103}, {"referenceID": 30, "context": "The deep learning for semi-supervised embedding [31] extends shallow semi-supervised learning techniques such as kernel methods with deep neural networks, and yield promising results.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "The work of [24] is most related to our proposed algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "Instead of learning a linear mapping or Mahalanobis metric [23, 30], we are interested in a non-linear mapping function.", "startOffset": 59, "endOffset": 67}, {"referenceID": 29, "context": "Instead of learning a linear mapping or Mahalanobis metric [23, 30], we are interested in a non-linear mapping function.", "startOffset": 59, "endOffset": 67}, {"referenceID": 24, "context": "Just like the multi-class classification problems [25], we use the joint feature representation \u03a6(h, z) for each (h, z) \u2208 X \u00d7 Z", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "The clustering of testing examples is done in the same manner as the multiclass SVM [25],", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "In a similar manner as in [20, 34], we will incorporate the pairwise constraint information into the marginbased clustering framework.", "startOffset": 26, "endOffset": 34}, {"referenceID": 33, "context": "In a similar manner as in [20, 34], we will incorporate the pairwise constraint information into the marginbased clustering framework.", "startOffset": 26, "endOffset": 34}, {"referenceID": 11, "context": "Initialization: We used stacked RBMs to initialize the weights layer by layer greedily in the deep network, with contrastive divergence [12] (we used CD-1 in our experiments).", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "As for the clustering weight W, we take a similar strategy as in [34] to initialize it.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "We also tried L-BFGS [4, 22] to update model parameters, but it did not perform well.", "startOffset": 21, "endOffset": 28}, {"referenceID": 21, "context": "We also tried L-BFGS [4, 22] to update model parameters, but it did not perform well.", "startOffset": 21, "endOffset": 28}, {"referenceID": 31, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 121, "endOffset": 125}, {"referenceID": 5, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 132, "endOffset": 135}, {"referenceID": 16, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 144, "endOffset": 148}, {"referenceID": 33, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 158, "endOffset": 162}, {"referenceID": 31, "context": "Methods Accuracy (%) Adjusted Rand Index Glass Wdbc Wine Sonar Segmentation Reuters Glass Wdbc Wine Sonar Segmentation Reuters Xing [32] 46.", "startOffset": 132, "endOffset": 136}, {"referenceID": 5, "context": "14 ITML [6] 47.", "startOffset": 8, "endOffset": 11}, {"referenceID": 16, "context": "15 KISSME [17] 36.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "17 CMMC [34] 43.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "Thus, we used those methods to learn the metric and calculate the distances between all instances, then we used the kernel k-means [7] for clustering.", "startOffset": 131, "endOffset": 134}, {"referenceID": 33, "context": "We used the accuracy (the most possible matching between the obtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our method in all the experiments.", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": "We used the accuracy (the most possible matching between the obtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our method in all the experiments.", "startOffset": 146, "endOffset": 154}, {"referenceID": 20, "context": "We used the accuracy (the most possible matching between the obtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our method in all the experiments.", "startOffset": 146, "endOffset": 154}], "year": 2015, "abstractText": "Semi-supervised clustering is an very important topic in machine learning and computer vision. The key challenge of this problem is how to learn a metric, such that the instances sharing the same label are more likely close to each other on the embedded space. However, little attention has been paid to learn better representations when the data lie on non-linear manifold. Fortunately, deep learning has led to great success on feature learning recently. Inspired by the advances of deep learning, we propose a deep transductive semi-supervised maximum margin clustering approach. More specifically, given pairwise constraints, we exploit both labeled and unlabeled data to learn a non-linear mapping under maximum margin framework for clustering analysis. Thus, our model unifies transductive learning, feature learning and maximum margin techniques in the semi-supervised clustering framework. We pretrain the deep network structure with restricted Boltzmann machines (RBMs) layer by layer greedily, and optimize our objective function with gradient descent. By checking the most violated constraints, our approach updates the model parameters through error backpropagation, in which deep features are learned automatically. The experimental results shows that our model is significantly better than the state of the art on semisupervised clustering.", "creator": "LaTeX with hyperref package"}}}