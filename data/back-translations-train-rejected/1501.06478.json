{"id": "1501.06478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2015", "title": "Compressed Support Vector Machines", "abstract": "Support vector machines (SVM) can classify data sets along highly non-linear decision boundaries because of the kernel-trick. This expressiveness comes at a price: During test-time, the SVM classifier needs to compute the kernel inner-product between a test sample and all support vectors. With large training data sets, the time required for this computation can be substantial. In this paper, we introduce a post-processing algorithm, which compresses the learned SVM model by reducing and optimizing support vectors. We evaluate our algorithm on several medium-scaled real-world data sets, demonstrating that it maintains high test accuracy while reducing the test-time evaluation cost by several orders of magnitude---in some cases from hours to seconds. It is fair to say that most of the work in this paper was previously been invented by Burges and Sch\\\"olkopf almost 20 years ago. For most of the time during which we conducted this research, we were unaware of this prior work. However, in the past two decades, computing power has increased drastically, and we can therefore provide empirical insights that were not possible in their original paper.", "histories": [["v1", "Mon, 26 Jan 2015 16:51:34 GMT  (1724kb,D)", "https://arxiv.org/abs/1501.06478v1", null], ["v2", "Mon, 2 Feb 2015 16:24:38 GMT  (1724kb,D)", "http://arxiv.org/abs/1501.06478v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhixiang xu", "jacob r gardner", "stephen tyree", "kilian q weinberger"], "accepted": false, "id": "1501.06478"}, "pdf": {"name": "1501.06478.pdf", "metadata": {"source": "CRF", "title": "Compressed Support Vector Machines", "authors": ["Zhixiang (Eddie", "Jacob R. Gardner", "Stephen Tyree", "Kilian Q. Weinberger"], "emails": ["xuzx@cse.wustl.edu", "gardner.jake@gmail.com", "swtyree@wustl.edu", "kilian@wustl.edu"], "sections": [{"heading": "1 Introductions", "text": "Support vector machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real-world applications, including email spam classification [7], face recognition [12] and gene selection [11]. In real-world applications, valuation costs (in terms of memory and CPU) are critical, especially for settings with severe resource constraints (e.g. embedded devices, mobile phones or tablets) or frequently repeated tasks (e.g. webmail spam classification, face recognition in uploaded images) that can be performed billions of times a day."}, {"heading": "2 Related Work", "text": "Burges and Scholkopf invented Compressed Vector Machines long before us. While conducting our research, we were very late in realizing their work. We still consider our perspective and additional experiments valuable and decided to publish our results as a technology report. However, we want to emphasize that all academic credits should go to them, which dominate the entire test time. Their goal is to minimize the feature extraction costs [3; 10; 17; 23; 25] focuses on scenarios where the features are limited on demand and the extraction costs. Their goal is to minimize the feature extraction costs."}, {"heading": "3 Background", "text": "Let us transfer the data into a higher, possibly limited space. (...) Let us transfer the data into a higher, possibly limited space. (...) Let us learn this. (...) Let us transfer the data into a higher, possibly non-linear space. (...) Let us transform the data into a higher, possibly non-linear space. (...) Let us transform the data into another, non-linear space. (...) Let us transform the data into a higher, non-linear space. (...) Let us transform the data into a higher, non-linear space. (...) Let us transform the data into another, non-linear space. (...) Let us transform the data into a higher, non-linear space. (...) Let us transform the data into a higher, non-linear space. (...) Let us transform the data into a higher, non-linear space."}, {"heading": "4 Method", "text": "We see CVM as a kind of post-processing of costs related to the original SVM solution. We can rely on the prediction function (3) to formulate the accurate SVM classification rating. Let's take a closer look at the cost of calculating a test time rating. (3) We assume that the compression costs are identical across all test inputs and all support vectors. (3) As shown in (3), we need to have a prediction function for the input xt and support for all test inputs.We assume that the compression costs are identical across all test inputs and all support vectors."}, {"heading": "5 Results", "text": "In this section, we will first demonstrate Compressed Vector Machine (CVM) on a synthetic dataset to graphically illustrate each step in the algorithm, then evaluate CVM on several medium-sized real datasets; the dataset contains 600 sample data from two classes (red and blue), where each input contains two characteristics; the blue inputs are sampled by a Gaussian distribution with mean and variance 1, and the red inputs are evaluated by a noisy circle around the blue inputs; as shown in Figure 2 (a), the dataset is not linearly separable; for simplicity, we treat all inputs as training inputs; to evaluate CVM, we first learn an RBF kernel from the full training set; we plot the resulting optimal decision boundaries in Figure 2 (b) with a black curve."}, {"heading": "6 Acknowledgments", "text": "Most of this work had previously been invented by Burges and Scho \ufffd lkopf [2], whose research at the time was truly visionary."}], "references": [{"title": "Improving the accuracy and speed of support vector machines", "author": ["Chris C. Burges", "Bernhard Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Fast classification using sparse decision dags", "author": ["R. Busa-Fekete", "D. Benbouzid", "B. K\u00e9gl"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "O. Chapelle"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The forgetron: A kernel-based perceptron on a budget", "author": ["Ofer Dekel", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "SIAM Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Support vector machines for spam categorization", "author": ["Harris Drucker", "Donghui Wu", "Vladimir N Vapnik"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Bayesian optimization with inequality constraints", "author": ["Jacob Gardner", "Matt Kusner", "Kilian Q. Weinberger", "John Cunningham", "Zhixiang Xu"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["A. Grubb", "J.A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["Isabelle Guyon", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Face recognition with support vector machines: Global versus component-based approach", "author": ["Bernd Heisele", "Purdy Ho", "Tomaso Poggio"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Building support vector machines with reduced classifier complexity", "author": ["S Sathiya Keerthi", "Olivier Chapelle", "Dennis DeCoste"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Stochastic neighbor compression", "author": ["Matt Kusner", "Stephen Tyree", "Kilian Q. Weinberger", "Kunal Agrawal"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Joint cascade optimization using a product of boosted classifiers", "author": ["L. Lefakis", "F. Fleuret"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Large margin dags for multiclass classification", "author": ["John C Platt", "Nello Cristianini", "John Shawe-taylor"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Using classifier cascades for scalable e-mail classification", "author": ["J. Pujara", "H. Daum\u00e9 III", "L. Getoor"], "venue": "In CEAS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Boosting classifier cascades", "author": ["M. Saberian", "N. Vasconcelos"], "venue": "In NIPS, pages 2047\u20132055", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "The support vector machine", "author": ["Cristianini Shawe-Taylor", "Smola Sch\u00f6lkopf"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Sparse gaussian processes using pseudo-inputs", "author": ["Edward Snelson", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Local supervised learning through space partitioning", "author": ["J. Wang", "V. Saligrama"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale svm training", "author": ["Zhuang Wang", "Koby Crammer", "Slobodan Vucetic"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Cost-sensitive tree of classifiers", "author": ["Zhixiang Xu", "Matt Kusner", "Minmin Chen", "Kilian Q. Weinberger"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Anytime representation learning", "author": ["Zhixiang (Eddie) Xu", "Matt J. Kusner", "Gao Huang", "Kilian Q. Weinberger"], "venue": "In Sanjoy Dasgupta and David McAllester, editors,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A reduction of the elastic net to support vector machines with an application to gpu computing", "author": ["Quan Zhou", "Wenlin Chen", "Shiji Song", "Jacob R. Gardner", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Support Vector Machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real world applications, including email spam classification [7], face recognition [12] and gene selection [11].", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "Support Vector Machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real world applications, including email spam classification [7], face recognition [12] and gene selection [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 9, "context": "Support Vector Machines (SVM) are arguably one of the great success stories of machine learning and have been used in many real world applications, including email spam classification [7], face recognition [12] and gene selection [11].", "startOffset": 230, "endOffset": 234}, {"referenceID": 18, "context": "Specifically, we focus on kernel support vector machine (SVM) [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "Previous work has reduced the classifier complexity by selecting few support vectors through budgeted training [6; 24] or with heuristic selection prior to learning [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "The resulting model is a standard SVM classifier (thus can be saved, for example, in a LibSVM [4] compatible file).", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Burges and Sch\u00f6lkopf [2] invented Compressed Vector Machines long before us.", "startOffset": 21, "endOffset": 24}, {"referenceID": 24, "context": "More recently, [26] introduces an algorithm to reduce the test-time cost specifically for the SVM classifier.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "[6] propose an algorithm to limit the memory usage for kernel based online classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Similar to [6], [24] also focusses on online kernel SVM, and attacks primarily the training time complexity.", "startOffset": 11, "endOffset": 14}, {"referenceID": 22, "context": "Similar to [6], [24] also focusses on online kernel SVM, and attacks primarily the training time complexity.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Of particular relevance is [13], which, specifically reduces the SVM evaluation cost by reducing the number of support vectors.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "Similar approaches have successfully learned pseudo-inputs for compressed nearest neighbor classification sets [14] and sparse Gaussian process regression models [22].", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "Similar approaches have successfully learned pseudo-inputs for compressed nearest neighbor classification sets [14] and sparse Gaussian process regression models [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 19, "context": "For simplicity we assume binary classification in the following section, but our algorithm is easily extended to multi-class settings using one-vs-one [21], one-vs-all [18], or DAG [16] approaches, and results are included for several multi-class datasets.", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "For simplicity we assume binary classification in the following section, but our algorithm is easily extended to multi-class settings using one-vs-one [21], one-vs-all [18], or DAG [16] approaches, and results are included for several multi-class datasets.", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "For simplicity we assume binary classification in the following section, but our algorithm is easily extended to multi-class settings using one-vs-one [21], one-vs-all [18], or DAG [16] approaches, and results are included for several multi-class datasets.", "startOffset": 181, "endOffset": 185}, {"referenceID": 18, "context": "Most importantly, the kernel-trick [20] may be employed to learn highly non-linear decision boundaries for data sets that are not linearly separable.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "LARS [8] is a widely used forward selection algorithm because of its simplicity and efficiency.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "Constrained Bayesian optimization [9] supports efficient constrained joint hyperparameter optimizations of this type.", "startOffset": 34, "endOffset": 37}, {"referenceID": 25, "context": "Additionally, the L1-penalized support vector selection in the LARS-SVM step may benefit from recent work on highly parallel Elastic Net solvers [27].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "We use LibSVM [4] to train a regular RBF kernel SVM using regularization parameter C and RBF kernel width \u03c3 selected on a 20% validation split.", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "Figure 3 also shows a comparison of CVM with Reduced-SVM [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Most of this work was previously invented by Burges and Sch\u00f6lkopf [2] whose research was truly visionary at the time.", "startOffset": 66, "endOffset": 69}], "year": 2015, "abstractText": "Support vector machines (SVM) can classify data sets along highly non-linear decision boundaries because of the kernel-trick. This expressiveness comes at a price: During test-time, the SVM classifier needs to compute the kernel innerproduct between a test sample and all support vectors. With large training data sets, the time required for this computation can be substantial. In this paper, we introduce a post-processing algorithm, which compresses the learned SVM model by reducing and optimizing support vectors. We evaluate our algorithm on several medium-scaled real-world data sets, demonstrating that it maintains high test accuracy while reducing the test-time evaluation cost by several orders of magnitude\u2014in some cases from hours to seconds. It is fair to say that most of the work in this paper was previously been invented by Burges and Sch\u00f6lkopf almost 20 years ago. For most of the time during which we conducted this research, we were unaware of this prior work. However, in the past two decades, computing power has increased drastically, and we can therefore provide empirical insights that were not possible in their original paper.", "creator": "LaTeX with hyperref package"}}}