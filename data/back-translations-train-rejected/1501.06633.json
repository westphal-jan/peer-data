{"id": "1501.06633", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2015", "title": "maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs", "abstract": "This paper describes maxDNN, a computationally efficient convolution kernel for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3\\% computational efficiency on typical deep learning network architectures using a single kernel. The design combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We only address forward propagation (FPROP) operation of the network, but we believe that the same techniques used here will be effective for backward propagation (BPROP) as well.", "histories": [["v1", "Tue, 27 Jan 2015 01:19:12 GMT  (24kb)", "http://arxiv.org/abs/1501.06633v1", "7 pages, 2 figures, 1 table"], ["v2", "Wed, 28 Jan 2015 01:16:33 GMT  (24kb)", "http://arxiv.org/abs/1501.06633v2", "7 pages, 2 figures, 1 table"], ["v3", "Fri, 30 Jan 2015 23:50:49 GMT  (24kb)", "http://arxiv.org/abs/1501.06633v3", "7 pages, 2 figures, 1 table"]], "COMMENTS": "7 pages, 2 figures, 1 table", "reviews": [], "SUBJECTS": "cs.NE cs.DC cs.LG", "authors": ["andrew lavin"], "accepted": false, "id": "1501.06633"}, "pdf": {"name": "1501.06633.pdf", "metadata": {"source": "CRF", "title": "maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs", "authors": ["Andrew Lavin"], "emails": ["alavin@acm.org"], "sections": [{"heading": null, "text": "ar Xiv: 150 1.06 633v 1 [cs.N E] 27 Yes"}, {"heading": "1 Introduction", "text": "The central algorithm for revolutionary neural networks is the 2D convolution of a bank of multi-channel filters against a minibatch of multi-channel 2D cards [7]. Modern GPUs have demonstrated the ability to calculate convolutions significantly faster than CPUs [5]. cuda-convennet2 and cuDNN are the leading GPU implementations of spatial convolution [2]. fbcunn is a GPU implementation of Frequency Domain Convolution that has a speed advantage for many useful convolution forms [8]. When comparing convolution cores, it is common to report execution time or throughput. The problem with these measurements is that they reconcile the individual problems of algorithm complexity, computing efficiency and device peak efficiency."}, {"heading": "2 Convolution", "text": "The folding used in Deep Learning uses a minibatch of Nb 2D multichannel cards of size Wi \u00d7 Hi \u00b7 Nc and a bank of No 2D multichannel filters of size Sk \u00b7 Sk \u00b7 Nc. Folding Nb cards against No-Filter is performed separately in corresponding channels and the result is added across all channels. A step is chosen that is possibly equal to 1, which results in the desired output card size, Where \u00b7 Ho \u00b7 Nc. The origin of the folding can be offset by an optional fill parameter, and the image is wrapped in an apron of zeros to handle boundary contamination. We also scale the results according to a scalar, \u03b1.The algorithmic complexity of the folding is: C (Nb, Wo, Ho, Nc, No, Sk) = 2NbWoHo (NcS 2 k + 1) No FLOPs where a single operation is multiplied as 2 LOPs."}, {"heading": "3 cuda-convnet2", "text": "Cuda-Convnet2 [4] is perhaps the most efficient folding in a variety of popular network formats [2]. It uses an interesting strategy to implement direct folding: The map and filter data are arranged in memory so that the inner dimension is the number of lots or filters; the outer dimensions are the width, height and channels of the cards; the calculation of Nb \u00b7 No output values for a single output card coordinate is merely a matrix that extends between a rolled out input map patch of size Nb \u00d7 S 2 k Nc and the filter matrix of size S2 k Nc \u00d7 No.Each of the no columns of the filter matrix contains the S 2 k Nc weights for a single filter. The S2k Nc columns of the input matrix must be collected, including the filter matrix of size S2 k Nc, and the filter matrix of size S2 \u00d7 Nc the target filter matrix must not contain the columns of the Nabex."}, {"heading": "3.1 Maxas: The Maxwell Assembler", "text": "Maxas is an open source assembler for the NVIDIA Maxwell architecture, created by Scott Gray [3], which gives the developer complete control over the scheduling of instructions and the allocation of registers. The project includes an SGEMM implementation that achieves 96% computing power on Maxwell GM204 GPUs. The project is of interest not only to the assembler himself, but also to the SGEMM implementation and associated documentation, which is the best example yet published for building highly efficient kernels. Several advanced techniques are used, including: \u2022 Using instructions for loading 128-bit textures to reduce the number of global loads, increasing the size of the maximum array to 4GB (based on the maximum texture index 228), and reducing the indexing of calculations \u2022 Double buffering of global memory loads to hide the global memory latency \u2022 Double buffering of shared memory veils to reduce the number of shared memory by blocking global memory slots."}, {"heading": "3.2 maxDNN", "text": "The strategy behind our maxDNN kernel combines the style of Cuda Convnet2 folding with the matrix multiplier mounting code of Maxas SGEMM. Maxas SGEMM64 has been modified to allow each block to traverse an area of the input card to calculate a 64 x 64 filter image block for a single output card coordinate. The z-coordinate of the block index is used to enumerate the filter image blocks. Basically, this only required adjusting the indexing calculations in the existing SGEMM64 code. To reduce the number of index calculations required to traverse the input card field, we highlighted the calculation of the pixel / channel offset positions in constant memory. The pixel offset is simply added to the patch offset to calculate the input card offset. This replaces the 3 nested loops with a single loop over the 8% of the result, which is calculated by a single loop over an inner loop."}, {"heading": "3.3 Experiments", "text": "We compare the performance of maxDNN with cuDNN v.2 RC1 on a GEFORCE GTX980 graphics card using the NVIDIA Maxwell GM204 GPU. cudaconvnet2 was not used because it was not optimized for the Maxwell architecture. CuDNN v.2 RC2 was also available, but showed significantly worse performance, so we measured performance with computing power on RC1.We, which is the ratio of the actual throughput of the program to the peak throughput of the device. GM204 consists of 16 processors, each with 128 cores. Each core is capable of running 1 multiplicate per cycle, so you can calculate the device peak throughput by peak throughput = 2FLOPs \u00b7 128 \u00b7 16 \u00b7 GPU clock frequency."}, {"heading": "3.4 Results", "text": "Table 1 compares the computing power of cuDNN and maxDNN for FPROP convolution at Alexnet and Overfeat.maxDNN efficiency for Alexnet v.2 ranges from 93.4% to 95.5%. The worst performance is at the input level, where a patch only has 11 x 11 x 3 elements, which reduces the size of the main loop in which almost all FLOPs are performed, compared to the initialization and memory code sections, which as a fixed overhead.maxDNN efficiency for Overfeat reaches 96.3% and is above 94.4% for all levels except the first, which reaches only 70.3%. This is due to the fact that the number of filters in this layer, 96, is not a multiple of the block size, 64 x 64. We believe that this could be addressed with a kernel using a block size of 64 x 32, which would significantly decrease the computing intensity in terms of global kernel memory hit rates, but significantly higher the Core Ratio-2.5."}, {"heading": "3.5 Conclusion", "text": "We developed an efficient folding core for Maxwell GPUs using the Maxas assembler, Maxas SGEMM64 source code, and cuda Convnet2 approach to folding. We believe the same approach could also be applied to the BPROP operation of Convolutionary Neural Networks. The efficiency of maxDNN folding competes with that of the best SGEMM implementations, so maxDNN is a proof of existence that highly efficient GPU folding is possible."}, {"heading": "Acknowledgement", "text": "The author thanks eBay Research Labs Machine Learning Director Dennis DeCoste for his guidance during this project."}], "references": [{"title": "cudnn: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Maxas: Assembler for nvidia maxwell architecture", "author": ["Scott Gray"], "venue": "https://github.com/NervanaSystems/maxas,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Performance upper bound analysis and optimization of sgemm on fermi and kepler gpus", "author": ["Junjie Lai", "Andr\u00e9 Seznec"], "venue": "In Code Generation and Optimization (CGO),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Yann LeCun", "Fu Jie Huang", "Leon Bottou"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["Nicolas Vasilache", "Jeff Johnson", "Micha\u00ebl Mathieu", "Soumith Chintala", "Serkan Piantino", "Yann LeCun"], "venue": "CoRR, abs/1412.7580,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "The central algorithm of convolutional neural networks is the 2D convolution of a bank of multi-channel filters against a minibatch of multi-channel 2D maps [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "Modern GPUs have demonstrated the ability to compute convolutions significantly faster than CPUs [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "fbcunn is a GPU implementation of frequency domain convolution that has a speed advantage for many useful convolution shapes [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "75% of the TFLOPS number reported on spec sheets [6].", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "The Maxas project is an open source assembler for NVIDIA Maxwell GPUs, created by Scott Gray [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "Maxas is an open source assembler for the NVIDIA Maxwell architecture, created by Scott Gray [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "One of the stated design goals of cuDNN was to achieve consistently high efficiency on a variety of convolution shapes using a single kernel [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "Although cuDNN reports flexibility with respect to minibatch size [1], we can see that in practice the performance varies a lot with respect to map dimensions.", "startOffset": 66, "endOffset": 69}], "year": 2015, "abstractText": "This paper describes maxDNN, a computationally efficient convolution kernel for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3% computational efficiency on typical deep learning network architectures using a single kernel. The design combines ideas from cudaconvnet2 with the Maxas SGEMM assembly code. We only address forward propagation (FPROP) operation of the network, but we believe that the same techniques used here will be effective for backward propagation (BPROP) as well.", "creator": "LaTeX with hyperref package"}}}