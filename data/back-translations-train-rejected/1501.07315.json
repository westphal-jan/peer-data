{"id": "1501.07315", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2015", "title": "Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation", "abstract": "Applications involving dictionary learning, non-negative matrix factorization, subspace clustering, and parallel factor tensor decomposition tasks motivate well algorithms for per-block-convex and non-smooth optimization problems. By leveraging the stochastic approximation paradigm and first-order acceleration schemes, this paper develops an online and modular learning algorithm for a large class of non-convex data models, where convexity is manifested only per-block of variables whenever the rest of them are held fixed. The advocated algorithm incurs computational complexity that scales linearly with the number of unknowns. Under minimal assumptions on the cost functions of the composite optimization task, without bounding constraints on the optimization variables, or any explicit information on bounds of Lipschitz coefficients, the expected cost evaluated online at the resultant iterates is provably convergent with quadratic rate to an accumulation point of the (per-block) minima, while subgradients of the expected cost asymptotically vanish in the mean-squared sense. The merits of the general approach are demonstrated in two online learning setups: (i) Robust linear regression using a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised dictionary learning for network-wide link load tracking and imputation with missing entries. Numerical tests on synthetic and real data highlight the potential of the proposed framework for streaming data analytics by demonstrating superior performance over block coordinate descent, and reduced complexity relative to the popular alternating-direction method of multipliers.", "histories": [["v1", "Thu, 29 Jan 2015 00:15:28 GMT  (610kb)", "http://arxiv.org/abs/1501.07315v1", "Preliminary results of this work appear in the Proc. of the IEEE Intern. Conf. Acoustics, Speech, and Signal Process. (ICASSP), Florence, Italy, May 4-9, 2014"], ["v2", "Mon, 12 Dec 2016 22:18:49 GMT  (0kb,I)", "http://arxiv.org/abs/1501.07315v2", "The paper has been withdrawn by the author due to the need for a careful and very long revision process"], ["v3", "Thu, 26 Jan 2017 15:49:01 GMT  (0kb,I)", "http://arxiv.org/abs/1501.07315v3", "The paper has been withdrawn by the author due to the need for a careful and very long revision process"]], "COMMENTS": "Preliminary results of this work appear in the Proc. of the IEEE Intern. Conf. Acoustics, Speech, and Signal Process. (ICASSP), Florence, Italy, May 4-9, 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["konstantinos slavakis", "georgios b giannakis"], "accepted": false, "id": "1501.07315"}, "pdf": {"name": "1501.07315.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Konstantinos Slavakis", "Georgios B. Giannakis"], "emails": ["kslavaki@umn.edu", "georgios@umn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 1,07 315v 1 [cs.L G] 29 Jan 20"}, {"heading": "1 Introduction", "text": "It is not as if it were a matter of a way and a way in which it is a matter of a way and a way in which it is a matter of a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about which it is about a way in which it is about a way and a way in which it is about a way in which it is"}, {"heading": "2 Preliminaries", "text": "(8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8) (8) (8) (8 (8) (8) (8) (8) (8"}, {"heading": "3 Algorithm", "text": "The algorithm of this section includes the acceleration module of Table 1 in the online learning setup of (3). Since variables are divided into blocks x = (x),.., x (b),.., x (B)), the proposed algorithm takes advantage of the per-block convexity of the cost Ft and the specified data Ot, the algorithms visit all blocks of variables one after the other to solve the per-block b. The basic principles of this modular algorithm are presented in the block diagram of Figure 1. Per iteration (time span) t and given data Ot. The algorithms visit all blocks of variables one after the other to solve the per-block b convex minimization task. (b) Mb ft (b) | x (b) | x (\u2212 b). Arguments (\u2212 b) t (b). Symbol Ot is deleted from the x (b) (x)."}, {"heading": "4 Main Result", "text": "The following premises will play a role in the subsequent discussion. (As0) (Stationarity b) (Stationarity b) (Stationarity b) (Expectation F (x): = EO {Ft (x; Ot)}. (As2). (As2). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S). (S)."}, {"heading": "5 Examples", "text": "Here are two concrete examples of practical interest."}, {"heading": "5.1 Total least-squares", "text": "Data (yt) t-Z > 0 are generated by yt = u-Z > 0, where (u-T, s-T, s-T) n (u-T, s-T) n (u-T, s-T) n (u-T) n (u-T, s) n (u-T, s) n (u-T) n (u-T) n (u-T) n (u-T) n (u-T) n (u-T) n (u-T) n (u-T) n (u-n) n n-u u u u (n-T) n (n-n n) n (n-T) n (n-n) n (n-T) n-u u u u (n-T) n-n (n-n) n (n-T) n-u u (n) n-n n n n n n n n n n (n-T) n-u u (n-T) n-u u u (n-T) n-n n n n n n (n-T) n-u u u u (n-T) n-u u u (n-T) n n n n n n-u u (n-T) n-u u u (n-T) n-u u u u (n-T) n-n n n n n n n n n n (n-T) n-u u u u u-T (n-T) n-T) n (u-T) n-u-u-T) n (u-T) n-u-u-T (n-T) n-u-u-u-T n n n n (n-T) n n-u-u-u (n-T) n-u-u-T n n n-u-u (u-T) n n n n n n n n n n n-u (u-T (u-T) n-u-T) n-u-u-T) n-u (u-T) n n-u-T n-u-u-T n n n n-u-u-n n n n n (u-T n n n n n n n n n n n-"}, {"heading": "5.2 Semi-supervised dictionary learning", "text": "The following examples should be mentioned: D (V, E), D (D), D (D), D (D), D (D), D (D), D), D (D), D (D), D (D), D (D), D (D), D (D), D (D), D (D), D), D (D), D (D), D), D (D), D), D (D), D (D), D (D), D (D), D (D), D (D), D), D (D), D), D (D), D (D), D (D), D), D (D), D (D), D (D), D (D), D (D), D (D), D (D), D), D (D), D (D), D (D), D)."}, {"heading": "6 Numerical Tests", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Synthetic data", "text": "12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "6.2 Real data", "text": "In the context of Sec. 5.2, the advocated algorithm for estimating and tracking network-based widelink loads from the measurement archive of Internet2 is validated [48]. Analysis of the Internet2 backbone network yields a graph with V = 54 wells. Using network topology and routing information, network-wide link loads (\u03c7t) are provided 30,000 t = 1-RV (in Gbps). Per time span, only M = 30 of the components randomly selected via Mt. RM \u00b7 V are observed in yt. RM \u00b7 V. The cardinality of time-varying dictionaries is constantly set to Q = 80. In order to cope with pronounced time variations in Internet2 link loads, the maintenance factor for Sec. 5.2 is tightly adjusted to the maintenance factors set in Sec. 5.2."}, {"heading": "7 Conclusions", "text": "This manuscript presented a modular online learning algorithm that extended the arguments originally developed to accelerate best-in-class methods for convex batch optimization tasks to the context of per-block convex and stochastic approximation; the proposed framework demonstrated a computational complexity that scales the number of unknowns linearly; assuming no knowledge of the underlying data statistics, the convergence rate of the expected loss of the resulting iterates proved to be traceable; rigorous theoretical analyses were performed in the Hilbert space of r.vs. finite second order moments; the framework was tested on two instances of broad practical interest: (i) frugality expectation compression based on the TLS criterion; and (ii) semi-monitored DL for network-wide linkload tracking and imputation computation moments worked on compact future tests based on compressed algorithms rather than on compressive and compressed data that could be prised more effectively in CDs."}, {"heading": "A Appendices", "text": "The probative force of Ft (\u00b7 b) implies that for both sides of the previous inequality the probative force of E (\u00b7 b), the probative force of F (\u00b7 b), the probative force of F (\u00b7 b), the probative force of F (\u00b7 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of F (\u2212 b), the probative force of \u2212 (\u2212), the probative force of F (\u2212), the probative force of \u2212 (\u2212)."}, {"heading": "It can be verified by (M7) that \u00b5\u03c4+1\u03c8\u03c4+1 \u2212 (\u00b5\u03c4+1 \u2212 \u03bb1)x\u03c4 = \u03b7\u03c4\u03bb\u03c4\u00b5\u03c4\u03b6\u03c4/\u03b2\u03c4 + \u00b5\u03c4 (1 \u2212 \u03b7\u03c4\u03bb\u03c4/\u03b2\u03c4 )\u03c8\u03c4 \u2212", "text": "(1)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5) By the definition of Prox\u03b2\u03c4gb(h) as the (unique) minimizer Prox\u03b2\u03c4gb(h) = argmin\u03be[\u2016h \u2212 \u03be\u20162/2 +", "text": "(b) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c)) c)) c) c))) c))))))) c))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))"}, {"heading": "Acknowledgments", "text": "The authors thank Dr. P. Forero of SPAWAR for providing the method code in [8]."}], "references": [{"title": "Dictionary learning", "author": ["I. To\u0161i\u0107", "P. Frossard"], "venue": "IEEE Signal Process. Magaz., vol. 28, no. 2, pp. 27\u201338, Mar. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Machine Learn. Research, vol. 11, pp. 19\u201360, Mar. 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, Oct. 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Process. Magaz., vol. 28, no. 2, pp. 52\u201368, Mar. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review, vol. 51, no. 3, pp. 455\u2013500, 2009. 27", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Prediction of partially observed dynamical processes over networks via dictionary learning", "author": ["P. Forero", "K. Rajawat", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 13, pp. 3305\u20133320, July 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Convergence of block coordinate decent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., vol. 109, pp. 475\u2013494, June 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["P. Tseng", "S. Yun"], "venue": "Math. Program., Ser. B, vol. 117, pp. 387\u2013423, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated block-coordinate relaxation for regularized optimization", "author": ["S.J. Wright"], "venue": "SIAM J. Optim., vol. 22, no. 1, pp. 159\u2013186, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM J. Optim., vol. 22, no. 2, pp. 341\u2013362, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM J. Imaging, vol. 6, no. 3, pp. 1758\u2013 1789, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Math. Program., Ser. A, pp. 1\u201338, Dec. 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 7, pp. 2479\u20132493, July 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity-cognizant total least-squares for perturbed compressive sampling", "author": ["H. Zhu", "G. Leus", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 59, no. 5, pp. 2002\u20132016, May 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["M. Razaviyayn", "M. Hong", "Z.-Q. Luo"], "venue": "SIAM J. Optim., vol. 23, no. 2, pp. 1126\u20131153, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic coordinate descent methods for regularized smooth and nonsmooth losses", "author": ["Q. Tao", "K. Kong", "D. Chu", "G. Wu"], "venue": "Lecture Notes in Computer Science, ser. Machine Learning and Knowledge Discovery in Databases, P. A. Flach, T. de Bie, and N. Cristianini, Eds., vol. 7523. Springer, 2012, pp. 537\u2013552.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Decentralized sparsity-regularized rank minimization: Algorithms and applications", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 11, pp. 5374\u20135388, Nov. 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inform. Theory, vol. 52, pp. 1289\u20131306, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Modeling and optimization for big data analytics", "author": ["K. Slavakis", "G.B. Giannakis", "G. Mateos"], "venue": "IEEE Signal Process. Magaz., vol. 31, no. 5, pp. 18\u201331, Sept. 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic approximation vis-\u00e0-vis online learning for big data analytics", "author": ["K. Slavakis", "S.-J. Kim", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Signal Process. Magaz., vol. 31, no. 6, pp. 124\u2013129, Nov. 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications, 2nd ed", "author": ["H.J. Kushner", "G.G. Yin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "A method for solving the convex programming problem with convergence rate O(1/k)", "author": ["Y. Nesterov"], "venue": "Dokl. Akad. Nauk SSSR, vol. 269, pp. 543\u2013547, 1983, in Russian.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1983}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107\u2013194, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "J. Machine Learn. Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization", "author": ["S. Ghadimi", "G. Lan", "H. Zhang"], "venue": "Aug. 2013, arXiv:1308.6594. 28", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic successive minimization method for nonsmooth nonconvex optimization with applications to transceiver design in wireless communication networks", "author": ["M. Razaviyayn", "M. Sanjabi", "Z.-Q. Luo"], "venue": "Jul. 2013, arXiv:1307.4457.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Sur l\u2019approximation par \u00e9l\u00e9ments finis et la r\u00e9solution par p\u00e9nalisationdualit\u00e9 d\u2019une classe de probl\u00e8mes de Dirichlet non lin\u00e9aires", "author": ["R. Glowinski", "A. Marrocco"], "venue": "Rev. Francaise d\u2019Aut. Inf. Rech. Oper., vol. 9, no. 2, pp. 41\u201376, 1975.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1975}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finiteelement approximations", "author": ["D. Gabay", "B. Mercier"], "venue": "Comp. Math. Appl., vol. 2, pp. 17\u201340, 1976.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1976}, {"title": "Distributed LMS for consensus-based in-network adaptive processing", "author": ["I.D. Schizas", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 6, pp. 2365\u20132381, June 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance analysis of the consensus-based distributed LMS algorithm", "author": ["G. Mateos", "I.D. Schizas", "G.B. Giannakis"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 2009, Dec. 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L.Q. Tran", "A. Gray"], "venue": "Proc. ICML, Atlanta, Georgia: USA, June 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "On the O(1/t) convergence rate of the Douglas-Rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM J. Numerical Analysis, vol. 50, no. 2, pp. 700\u2013709, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "On the o(1/k) convergence and parallelization of the alternating direction method of multipliers", "author": ["W. Deng", "M.-J. Lai", "W. Yin"], "venue": "2013, arXiv:1312.3040.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "2012, arXiv:1208.3922.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Image Process., vol. 18, pp. 2419\u20132439, 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Over-relaxation of the fast iterative shrinkage-thresholding algorithm with variable stepsize", "author": ["M. Yamagishi", "I. Yamada"], "venue": "Inverse Problems, vol. 27, no. 10, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Acceleration of adaptive proximal forward-backward splitting method and its application to sparse system identification", "author": ["M. Yamagishi", "M. Yukawa", "I. Yamada"], "venue": "Proc. ICASSP, Prague: Czech Republic, May 22\u201327 2011, pp. 4296\u20134299.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerated gradient methods for stochastic optimization and online learning", "author": ["C. Hu", "J.T. Kwok", "W. Pan"], "venue": "Proc. NIPS, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated gradient methods for nonconvex nonlinear and stochastic programming", "author": ["S. Ghadimi", "G. Lan"], "venue": "Oct. 2013, arXiv:1310.3787.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Online dictionary learning from big data using accelerated stochastic approximation algorithms", "author": ["K. Slavakis", "G.B. Giannakis"], "venue": "Proc. ICASSP, Florence: Italy, May 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": "New York: Springer,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer-Verlag, 2011, pp. 185\u2013212.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Mathematical Foundations of the Calculus of Probability", "author": ["J. Neveu"], "venue": "San Francisco: Holden-Day,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1965}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditski", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim., vol. 19, no. 4, pp. 1574\u20131609, 2009.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 110, "endOffset": 115}, {"referenceID": 1, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 110, "endOffset": 115}, {"referenceID": 2, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Areas of interest span signal processing and machine learning applications including dictionary learning (DL) [1\u20133], non-negative matrix factorization (NMF) [4], subspace clustering (SSC) [5], parallel factor (PARAFAC) decomposition of multi-way tensors [6], and total least-squares (TLS) [7], to name a few.", "startOffset": 254, "endOffset": 257}, {"referenceID": 0, "context": ",dQ], Q \u2265 M , times an unknown sparse coefficient vector st [2, 3].", "startOffset": 60, "endOffset": 66}, {"referenceID": 1, "context": ",dQ], Q \u2265 M , times an unknown sparse coefficient vector st [2, 3].", "startOffset": 60, "endOffset": 66}, {"referenceID": 5, "context": "Sparsity on the other hand, renders DL representations identifiable even when yt has missing entries [8], due to e.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 6, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 7, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 8, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 9, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 10, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 11, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 12, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 13, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 14, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 15, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 16, "context": "Mainly for offline optimization, block coordinate descent methods (BCDMs) are popular largely because they exploit efficiently the per-block-convexity of the cost functions involved [3, 9\u201319].", "startOffset": 182, "endOffset": 191}, {"referenceID": 1, "context": ", st\u22121],Dt\u22121) as follows [3] st \u2208 argmins Ft ( [St\u22121, s],Dt\u22121;Ot ) (2a) Dt \u2208 argminD Ft(St,D;Ot) .", "startOffset": 25, "endOffset": 28}, {"referenceID": 17, "context": "Given Ot, each step in (2) is a convex optimization task: Basis pursuit [20] in (2a), and constrained leastsquares (LS) in (2b).", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "However, the per-block minimizations in BCD may not be affordable by todays big data applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22].", "startOffset": 174, "endOffset": 181}, {"referenceID": 19, "context": "However, the per-block minimizations in BCD may not be affordable by todays big data applications, where the sheer volume and dimensionality of Ot strain computing resources [21,22].", "startOffset": 174, "endOffset": 181}, {"referenceID": 20, "context": "Further, as data are streaming, analytics must often be performed in real time, without a chance to revisit past entries \u2013 a feature common to stochastic approximation (SA) setups [23].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": ") the number of unknowns; and v) iterations converge quadratically to a solution of (3), which is optimal among first-order methods in the sense of [24].", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 22, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 23, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 24, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 25, "context": "To place our contributions i)-v) in context, related first-order online BCDMs include the proximal stochastic (sub)gradient iterations [23, 25\u201328], whose convergence tends to be slow even for convex problems, on top of being challenged by step-size choices.", "startOffset": 135, "endOffset": 146}, {"referenceID": 26, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 27, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 28, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 29, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 98, "endOffset": 105}, {"referenceID": 30, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 31, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 32, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 33, "context": "A relevant stochastic algorithm is the SAbased alternating-direction method of multipliers (ADMM) [29\u201332], that is known to be sublinearly convergent for convex costs [33\u201336], but no similar results are available for per-block-convex functions.", "startOffset": 167, "endOffset": 174}, {"referenceID": 21, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 34, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 35, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 36, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 37, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 126, "endOffset": 137}, {"referenceID": 38, "context": "On the other hand, accelerated first-order quadratically convergent iterations are available for off-line convex optimization [24, 37\u201340]; see also [41] for related SA-based minimizers of convex costs.", "startOffset": 148, "endOffset": 152}, {"referenceID": 39, "context": "Even though [42] deals with non-convex costs, it requires bounds on the (primal) variables, knowledge of a", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "Our work markedly broadens the offline acceleration technique introduced for convex costs in [39], to per-block-convex and to online SA setups.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "Numerical tests corroborate our analytical claims, and demonstrate that under a linear computational complexity footprint the proposed algorithm outperforms BCDMs and the computationally heavier ADMM-based alternatives [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 40, "context": "Preliminary results were presented in [43], and outlined in [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "Preliminary results were presented in [43], and outlined in [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "A first-order algorithm for the off-line minimization of a convex cost \u03c6(x) := f(x) + g(x), x \u2208 M, was studied in [39] (presented for convenience in Table 1), where M is a linear vector space; f is convex as well as L-Lipschitz continuously differentiable; and g is convex but possibly non-smooth, e.", "startOffset": 114, "endOffset": 118}, {"referenceID": 41, "context": "The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Prox\u03b2ig : M \u2192 M : x 7\u2192 argmin\u03be\u2208M\u2016x \u2212 \u03be\u2016/2 + \u03b2ig(\u03be) for any \u03b2i \u2208 R>0 [44].", "startOffset": 55, "endOffset": 59}, {"referenceID": 41, "context": "The engine under the hood is the forward-backward (FB) [44] or proximal-gradient iteration of line 5, where the proximal mapping is defined as Prox\u03b2ig : M \u2192 M : x 7\u2192 argmin\u03be\u2208M\u2016x \u2212 \u03be\u2016/2 + \u03b2ig(\u03be) for any \u03b2i \u2208 R>0 [44].", "startOffset": 211, "endOffset": 215}, {"referenceID": 42, "context": ", Prox\u2016\u00b7\u20161 boils down to the soft-thresholding operator [45].", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "If the FB iteration were performed with \u03c8i+1 taking the place of \u03b6i in line 5, then (\u03c8i)i\u2208Z\u22650 would converge to a minimizer of \u03c6 [44], but with no claims on quadratic rate of convergence.", "startOffset": 129, "endOffset": 133}, {"referenceID": 34, "context": "Parameters {\u03b7i+1, \u03bbi+1} in line 2 are used to define stepsize \u03b2i+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1\u2212 \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L, (1+ \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L] per iteration, as opposed to the rigid \u03b2i+1 = 1/L in [37, 38].", "startOffset": 249, "endOffset": 257}, {"referenceID": 35, "context": "Parameters {\u03b7i+1, \u03bbi+1} in line 2 are used to define stepsize \u03b2i+1 through line 3, offering the flexibility of a variable stepsize from the interval [(1\u2212 \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L, (1+ \u221a 1\u2212 \u03b7i+1\u03bbi+1L)/L] per iteration, as opposed to the rigid \u03b2i+1 = 1/L in [37, 38].", "startOffset": 249, "endOffset": 257}, {"referenceID": 36, "context": "Table 1: Minimizing the convex cost \u03c6 := f + g [39] Require: \u03bb\u030c, \u03b7\u030c \u2208 R>0; \u03bc1 := \u03bb1 \u2208 [\u03bb\u030c, 1]; Lipschitz coeff.", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "Under proper parameter selection (\u03b7i = \u03b2i = 1/L, \u03bbi = 1), the algorithm of Table 1 boils down to [38].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "Moreover, with its guaranteed monotonically non-increasing behavior of cost values through line 6, and the flexibility offered by the variable step-sizes (\u03b2i)i\u2208Z>0 in line 3, the algorithm in Table 1 has merits over [37].", "startOffset": 216, "endOffset": 220}, {"referenceID": 34, "context": "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, \u03c8i, \u03b6i}.", "startOffset": 25, "endOffset": 33}, {"referenceID": 35, "context": "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, \u03c8i, \u03b6i}.", "startOffset": 25, "endOffset": 33}, {"referenceID": 36, "context": "Notwithstanding, neither [37, 38] nor [39] can offer guarantees on the convergence of the (primal) variables {xi, \u03c8i, \u03b6i}.", "startOffset": 38, "endOffset": 42}, {"referenceID": 41, "context": ") functions defined on Mb with values in R\u222a{+\u221e} [44].", "startOffset": 48, "endOffset": 52}, {"referenceID": 41, "context": "Mb} [44].", "startOffset": 4, "endOffset": 8}, {"referenceID": 43, "context": "the \u03c3-algebra A [46].", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "X [46].", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "O, conditioned on X [46].", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": ", Rb} in the context of Table 1, and not infinitely often (i \u2192 +\u221e) as in the batch and off-line mode of [39].", "startOffset": 104, "endOffset": 108}, {"referenceID": 41, "context": ") If limk\u2192\u221e E{\u2016\u03bek\u2016} = +\u221e for any (\u03bek)k\u2208Z\u22650 \u2282 H, then limk\u2192\u221e E{F (\u03bek)} = +\u221e [44].", "startOffset": 75, "endOffset": 79}, {"referenceID": 39, "context": "As2 will be used to prevent the proposed algorithm from generating unbounded sequences of estimates, without any a-priori enforcement of hard bounds on the variables, as in [42].", "startOffset": 173, "endOffset": 177}, {"referenceID": 44, "context": "More specifically, As6b assumes existence of a strong sequential cluster point and bounds a sequence of subgradients of the expected cost, similarly to the bound on gradients introduced in [47].", "startOffset": 189, "endOffset": 193}, {"referenceID": 13, "context": "Motivated by the TLS criterion and the resultant errors-in-variables (EIV) modeling approach [7, 16], the following sequence of per-block-convex costs is considered:", "startOffset": 93, "endOffset": 100}, {"referenceID": 5, "context": "Following [8], consider an undirected graph G(V, E), where V denotes the set of all vertices or nodes, with cardinality V, and E is the set of all edges.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "where \u2206t := \u2211t \u03c4=1 \u03b4 t\u2212\u03c4 ; \u2016s\u2016 and \u2016s\u20161 are as in (5), while the term including L quantifies prior knowledge on the topology of G, promotes \u201csmooth\u201d solutions over strongly connected nodes of G, and is instrumental in imputing missing entries [8].", "startOffset": 243, "endOffset": 246}, {"referenceID": 43, "context": "With regard to the selection of L\u0302 in As4, recall that Markov\u2019s inequality dictates that Pr(L (b) \u03c4 \u2265 L\u0302) \u2264 E{L \u03c4 }/L\u0302, for any L\u0302 [46].", "startOffset": 131, "endOffset": 135}, {"referenceID": 22, "context": "The algorithm in Table 2 is tested against a block-version of the classical online (sub)gradient descent method [25], tagged as BOGD in Fig.", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.", "startOffset": 73, "endOffset": 76}, {"referenceID": 26, "context": "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.", "startOffset": 149, "endOffset": 156}, {"referenceID": 27, "context": "The advocated algorithm is tested against the state-of-the-art scheme in [8] which relies on a GaussSeidel alternating minimization scheme: (i) ADMM [29,30] is employed to minimize a cost closely related to (6) w.", "startOffset": 149, "endOffset": 156}, {"referenceID": 5, "context": "It is worth noticing here that ADMM in [8] requires multiple iterations to achieve a prescribed estimation accuracy, and that no matrix inversion was incorporated in the realization of Table 2.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "The proposed method and [8] perform similarly, scoring mean (normalized) estimation errors of 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 36, "context": "3) The following proof is based on the one developed in [39] for the off-line, convex analytic case.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "Forero of SPAWAR for providing the code of the method in [8].", "startOffset": 57, "endOffset": 60}], "year": 2017, "abstractText": "Applications involving dictionary learning, non-negative matrix factorization, subspace clustering, and parallel factor tensor decomposition tasks motivate well algorithms for per-block-convex and non-smooth optimization problems. By leveraging the stochastic approximation paradigm and first-order acceleration schemes, this paper develops an online and modular learning algorithm for a large class of non-convex data models, where convexity is manifested only per-block of variables whenever the rest of them are held fixed. The advocated algorithm incurs computational complexity that scales linearly with the number of unknowns. Under minimal assumptions on the cost functions of the composite optimization task, without bounding constraints on the optimization variables, or any explicit information on bounds of Lipschitz coefficients, the expected cost evaluated online at the resultant iterates is provably convergent with quadratic rate to an accumulation point of the (perblock) minima, while subgradients of the expected cost asymptotically vanish in the mean-squared sense. The merits of the general approach are demonstrated in two online learning setups: (i) Robust linear regression using a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised dictionary learning for network-wide link load tracking and imputation with missing entries. Numerical tests on synthetic and real data highlight the potential of the proposed framework for streaming data analytics by demonstrating superior performance over block coordinate descent, and reduced complexity relative to the popular alternating-direction method of multipliers.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}