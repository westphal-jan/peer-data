{"id": "1501.07584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2015", "title": "Efficient Divide-And-Conquer Classification Based on Feature-Space Decomposition", "abstract": "This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively.", "histories": [["v1", "Thu, 29 Jan 2015 20:41:29 GMT  (13kb)", "http://arxiv.org/abs/1501.07584v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi guo", "bo-wei chen", "feng jiang", "xiangyang ji", "sun-yuan kung"], "accepted": false, "id": "1501.07584"}, "pdf": {"name": "1501.07584.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Qi Guo", "Bo-Wei Chen", "Feng Jiang", "Xiangyang Ji", "Sun-Yuan Kung"], "emails": ["dennisbwc@gmail.com)"], "sections": [{"heading": null, "text": "ar Xiv: 150 1,07 584v 1 [cs.L G] 29 Jan 20Index terms - feature space decomposition, feature space division, fusion, division and conquest, classification"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is the case that most people who stand up for people's rights are standing up for the rights of people who are not standing up for people's rights, but for the rights of people who are not standing up for people's rights, but are standing up for the rights of people who are not exercising their rights and duties. In fact, it is the case that most of them are able to respect and respect people's rights. Indeed, it is the case that most of them are able to respect and respect people's rights."}, {"heading": "2. SYSTEM OVERVIEW", "text": "Faced with an M \u00b7 N data matrix X with N instances and M characteristics and a 1 \u00b7 N label vector y = subsequently marking the attribute space as BA, and X being the projection of the N instances onto BA. We first define the attribute space decomposition method D = {T, I}, where T is a function for transforming the attribute space, and I is a set of attribute index groups. Decomposition method D contains five submethods discussed in Section 3.1, namely Random Decomposition (RD), Principal Component Analysis (PCA), Discriminant Component Analysis (DCA), Block Cholesky Decomposition (BCD), and Approximate Block Decomposition (ABD). In addition, both have an M \u00b7 M \u00b7 M \u00b7 M component Analysis (PCA), Discriminant Block Component Analysis (DCA), Discriminant Componate Decomposition (ABD), and Rolky (CD)."}, {"heading": "3. PROPOSED DIVISION AND FUSION METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Feature-Space Decomposition", "text": "Section 2 shows that the merit of the proposed method is to perform sub-space classification and to ignore sub-space dependence. Theoretically, the decomposition method should be able to reduce the dependence as much as possible between two sub-spaces, while the remaining sub-space dependence remains blocked, which is why the transformation should be carried out on the feature space in front of the division. Of all the sub-methods in this study, the simplest idea is RD, which builds the feature space directly on I. Its WRD is an M \u00b7 M identity matrix. As for PCA, we run PCA on the data matrix X and divide the features according to I. Since PCA diagonizes eliminate the feature covariance class, this method eliminates the relevance of different features between and within the sub-spaces. If the data obey the Gaussian distribution, it eliminates the PCA dependence on features within and also within the feature classes."}, {"heading": "3.2. Feature Subspace Fusion", "text": "After receiving the classification result matrix R from the local classifier, we weight the result of each subspace by training a global classifier fn + 1 using R as the data matrix and y as the label. Output of fn + 1 is the final predictive result. Observations show that m < 50 < < N and TRBFKRR [4] are favorable results for fn + 1. Since the training complexity of TRBFKRR is min (N3, J2N + J3), where J = (m + p) and p is TRBF order, it is efficient to train with a large number of instances and a small number of attributes like R. on the data matrix."}, {"heading": "4. EXPERIMENTAL RESULT", "text": "This year we are dealing with a very good result, \"he said in an interview with the German Press Agency.\" We are very satisfied, \"he said,\" but we are not ready yet. \""}, {"heading": "5. CONCLUSION", "text": "The experimental results show that our \"parts and victory\" classification scheme can reduce error rates (e.g. by 10.53% and 7.53% in covty and RCV1 datasets) compared to training directly using the entire datasets, and that it outperforms the modern fast SVM solver by reducing the overfitting problem. Future work will focus on providing theoretical analyses of the decomposition of traits and their effects on the classification of parts and conquerors. [1] The results are cited by Keerthi et al. [12] 2Results are cited by Hsieh et al. [6]"}, {"heading": "6. REFERENCES", "text": "[1] V. N. Vapnik and V. Vapnik, Statistical Learning Theory, vol. 2, New York, NY: Wiley, 1998. [2] N. Christianini and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-based Learning Methods, Cambridge: Cambridge University Press, 2000. [3] S.-Y. Kung, Kernel Methods and Machine Learning, Cambridge: Cambridge University Press, 2014. [4] S.-Y. Kung and P.-Y. Wu, \"On efficient learning and classification kernel methods,\" in Proc. 2012 IEEE Int. Kung. Conf. Acoustics, Speech and Signal Processing, Kyoto, Japan, Mar. pp. 2065-2068. [5] Y. Zhang, J. Duchi, and M. Wainwright, \"Divide and cross conquer kernel ridge regression,\" in Proc. 2013."}], "references": [{"title": "Statistical Learning Theory", "author": ["V.N. Vapnik", "V. Vapnik"], "venue": "vol. 2, New York, NY: Wiley", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge: Cambridge University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Kernel Methods and Machine Learning", "author": ["S.-Y. Kung"], "venue": "Cambridge: Cambridge University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "On efficient learning and classification kernel methods,", "author": ["S.-Y. Kung", "P.-Y. Wu"], "venue": "IEEE Int. Conf. Acoustics, Speech and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "and M", "author": ["Y. Zhang", "J. Duchi"], "venue": "Wainwright, \u201cDivide and conquer kernel ridge regression,\u201d in Proc. 2013 Conf. on Learning Theory, Princeton, NJ, USA, Jun. 12-14", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "and I", "author": ["C.-J. Hsieh", "S. Si"], "venue": "S. Dhillon, \u201cA divide-andconquer solver for kernel support vector machines,\u201d in Proc. 31st Int. Conf. Machine Learning, Beijing, China", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "An algorithm for generalized matrix eigenvalue problems,", "author": ["C.B. Moler", "G.W. Stewart"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1973}, {"title": "Liblinear: A library for large linear classification,", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Making large-scale SVM learning practical,", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods \u2013 Support Vector Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Decomposition methods for linear support vector machines,", "author": ["W. Kao", "K. Chung", "C. Sun", "C. Lin"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Large scale semisupervised linear svms,", "author": ["V. Sindhwani", "S.S. Keerthi"], "venue": "Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A modified finite newton method for fast solution of large scale linear svms,", "author": ["S.S. Keerthi", "D. DeCoste"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models.", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Typical kernel-based classification, such as Support Vector Machines (SVMs) [1] and Kernel Ridge Regression (KRR) [2], usually employs Radial Basis Functions (RBFs) as the kernel, for RBFs can effectively delineate the distribution of the data by using mixtures of Gaussian models.", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "Furthermore, RBFs can map the input features into the intrinsic space [3] that is spanned by infinite-dimensional vectors.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "This correspondingly increases the opportunity of creating a discriminant hyperplane in the empirical space [3], subsequently enhancing discriminability.", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "[4] proposed using Truncated Radical Basis Functions (TRBFs) to avoid generating infinite dimensions in the intrinsic space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Although computational load is relieved without losing too much accuracy, however, that method [4] did not improve discriminability and separability between features.", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "In response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "In response to such a demand, several divide-and-conquer classifiers [5],[6] based on kernel tricks have been developed so far.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "used divide-and-conquer KRR [5] to support computation of large-scale data.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "[6] focused on systematic data division before applying divideand-conquer classifiers to the data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "DCA also conducts orthogonal transformation like PCA, while its discriminant matrix is [Sw + \u03c1I]S , where Sw is the within-class scatter matrix, and \u03c1 is the ridge parameter [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 6, "context": "We conduct generalized eigenvector decomposition [7] to obtain the eigenvectors \u03bd1, \u03bd2, .", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Observations show that m < 50 << N and TRBFKRR[4] generates favorable results for fn+1.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,\u00b7 \u00b7 \u00b7 ,fh in our system respectively and", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "In this section, we use LibLinear [8] and DCSVM [6] as local classifiers f1,f1,\u00b7 \u00b7 \u00b7 ,fh in our system respectively and", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "We compare our results with four common fast linear SVM solvers, namely, Liblinear [8], SVMlight [9], BSVM [10] and L2-SVM-MFN [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "[6], as is shown in Table 5.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] 2Results are cited from Hsieh et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6]", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively.", "creator": "LaTeX with hyperref package"}}}