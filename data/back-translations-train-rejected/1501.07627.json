{"id": "1501.07627", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2015", "title": "Representing Objects, Relations, and Sequences", "abstract": "Vector Symbolic Architectures (VSAs) are high-dimensional vector representations of objects (eg., words, image parts), relations (eg., sentence structures), and sequences for use with machine learning algorithms. They consist of a vector addition operator for representing a collection of unordered objects, a Binding operator for associating groups of objects, and a methodology for encoding complex structures.", "histories": [["v1", "Thu, 29 Jan 2015 22:13:02 GMT  (960kb)", "http://arxiv.org/abs/1501.07627v1", "41 pages"]], "COMMENTS": "41 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stephen i gallant", "t wendy okaywe"], "accepted": false, "id": "1501.07627"}, "pdf": {"name": "1501.07627.pdf", "metadata": {"source": "CRF", "title": "Representing Objects, Relations, and Sequences", "authors": ["Stephen I. Gallant", "Wendy Okaywe", "Pitney Bowes"], "emails": ["sgallant@mmres.com,", "wokaywe@mmres.com"], "sections": [{"heading": null, "text": "Gallant and Okaywe: Representing Objects, Relations and Sequences 1Vector Symbolic Architectures (VSAs) are high-dimensional vector representations of objects (e.g. words, image parts), relationships (e.g. sentence structures), and sequences for use with machine learning algorithms. They consist of a vector addition operator to represent a cluster of disordered objects, a binding operator to associate object groups, and a methodology for encoding complex structures. We first develop constraints that machine learning imposes on VSAs: for example, similar structures must be represented by similar vectors. The constraints suggest that current VSAs should represent phrases (\"The Intelligent Brazilian Girl\") by simply binding term totals, in addition to directly binding the terms. We show that matrix multiplication can be used as a binding operator for a VSA, and that matrix elements can be selected randomly according to the zip."}, {"heading": "1. Introduction", "text": "In fact, it is the case that one is able to find a solution that enables one to find a solution that adapts to the needs of the people."}, {"heading": "2. Requirements for a Good Representation of Objects,", "text": "In fact, most of them will be able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "3. Representing Objects, Relations and Sequences", "text": "This year it is more than ever before."}, {"heading": "1. actor + the + smart + girl + phraseHas3words", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2. verb + saw + phraseHas1word", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3. object + the + gray + elephant + phraseHas3words", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4. Learning and Representation: Three Stages", "text": "This year, it is only a matter of time before the Presidency of the Council of the European Union enters into force, until the Presidency of the Council of the European Union enters into force."}, {"heading": "5. Capacity: Analytics and Simulation", "text": "This year it has come to the point where we are able to mention the aforementioned lcsrVo in order to stir it up, \"he says.\" We, \"he says,\" are able to stir ourselves, \"he says.\" We, \"he says,\" must be in the position we are in. \"\" We, \"he says,\" are in the position we are in. \"We,\" he says, \"are in the position we are in.\" We, \"he says,\" are in the position we are in. \""}, {"heading": "6. Checking the Constraints", "text": "We want that by using distributed vectors and two operations on vectors, addition (+) and binding (#), plus MBAT approach to the representation of complex structures, we are able to satisfy the constraints from Section 2.Constraints 1 (fixed length vector) and 2 (distributed representation) are obviously satisfied. We have seen how the binding operator can represent structure (constraint 3), including a practical solution to the binding problem, as well as sequences (constraint 5). Computations are uniquely linear in the number of objects, plus complexity (description size) of structures (constraint 6). Let us map Constraint 4 (similar objects / structures to similar representations)."}, {"heading": "7. Prior Research", "text": "It is not as if the people in this country have the same problems as the people in this country. \"But it is as if.\" \"It is as if.\" \"It is so.\" \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" \"It.\" \"\" It. \"\" \"It.\" \"\" It. \"\" \"It.\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \"\" \".\" \"\". \"\" \"\". \"\" \".\" \"\" \"\". \"\". \"\" \"\" \".\" \"\". \"\". \"\" \".\" \"\" \"\". \"\". \"\" \"\" \"\". \"\" \".\" \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\". \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\". \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\". \"\" \"\" \".\""}, {"heading": "8. Discussion", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "Appendix: Capacity for Vector Sums", "text": "It is possible to derive good approximation formulas for storage and recognition within a single distributed vector, as we show below. < Previous approximations appear in the appendices in Platte's book [2003] and Anderson [1973], which both look at vector components drawn from normal distributions rather than + 1 / -1. Their results for normally distributed components are consistent with Propositions 1-3, which we would like to distinguish from those summing up V T (x) = one-sided tail probability in a normal distribution of vectors S = number of vectors to form the vector V N = number of randomly generated vectors."}], "references": [{"title": "A theory for the recognition of items from short memorized lists. Psychological Review 80(6):417-438", "author": ["Anderson", "James A"], "venue": "Psychological Review,", "citeRegEx": "Anderson and A.,? \\Q1973\\E", "shortCiteRegEx": "Anderson and A.", "year": 1973}, {"title": "Simple Composition: A Magnetoencephalography investigation into the Comprehension of Minimal Linguistic Phrases", "author": ["D.K. Bemis", "L. Pylkk\u00e4nen"], "venue": "Journal of Neuroscience", "citeRegEx": "Bemis and Pylkk\u00e4nen,? \\Q2011\\E", "shortCiteRegEx": "Bemis and Pylkk\u00e4nen", "year": 2011}, {"title": "A Statistical Approach To Machine Translation", "author": ["Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Computational Linguistics Volume 16, Number", "citeRegEx": "Lafferty et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 1990}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Ronan", "Jason Weston"], "venue": "Information Processing and Management,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["M. T"], "venue": "Journal of Machine Learning Research", "citeRegEx": "T.,? \\Q2011\\E", "shortCiteRegEx": "T.", "year": 2011}, {"title": "Finding Structure in Time", "author": ["J.L. Elman"], "venue": "Cognitive Science", "citeRegEx": "Elman,? \\Q1990\\E", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Random Cells: An Idea Whose Time Has Come and Gone ... and Come Again?", "author": ["S.I. Gallant", "D. Smith"], "venue": "Proceedings of the IEEE International Conference on Neural Networks (San Diego 1987),", "citeRegEx": "Gallant and Smith,? \\Q1987\\E", "shortCiteRegEx": "Gallant and Smith", "year": 1987}, {"title": "Vector Symbolic Architectures answer Jackendoff\u2019s challenges for cognitive neuroscience. In Peter Slezak (Ed.), ICCS/ASCS International Conference on Cognitive Science (pp. 133138). Sydney, Australia: University of New South Wales", "author": ["R.W. Gayler"], "venue": null, "citeRegEx": "Gayler,? \\Q2003\\E", "shortCiteRegEx": "Gayler", "year": 2003}, {"title": "Implementing semantic networks", "author": ["G.E. Hinton"], "venue": "Parallel Models of Associative Memory. Hillsdale,", "citeRegEx": "Hinton,? \\Q1981\\E", "shortCiteRegEx": "Hinton", "year": 1981}, {"title": "Distributional representations for handling sparsity in supervised sequence labeling. In Meeting of the Association for Computational Linguistics (ACL), pages 495\u2013503", "author": ["F. Huang", "A. Yates"], "venue": "Jackendoff, R", "citeRegEx": "Huang and Yates.,? \\Q2009\\E", "shortCiteRegEx": "Huang and Yates.", "year": 2009}, {"title": "Fully distributed representation", "author": ["P. Kanerva"], "venue": "In Real World Computing Symposium (RWC97). Kanerva, Pentti", "citeRegEx": "Kanerva,? \\Q1997\\E", "shortCiteRegEx": "Kanerva", "year": 1997}, {"title": "Encoding structure in holographic reduced representations", "author": ["M.A. Kelly", "D. Blostein", "D.J.K. Mewhort"], "venue": "Canadian Journal of Experimental Psychology. Kohonen, T", "citeRegEx": "Kelly et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Kelly et al\\.", "year": 1977}, {"title": "Vector Symbolic Architectures: A new building material for Artificial General Intelligence", "author": ["S.D. Levy", "R.W. Gayler"], "venue": "Proceedings of the First Conference on Artificial General Intelligence", "citeRegEx": "Levy and Gayler,? \\Q2008\\E", "shortCiteRegEx": "Levy and Gayler", "year": 2008}, {"title": "Phrase clustering for discriminative learning. In Meeting of the Association for Computational Linguistics (ACL), pages 1030\u20131038", "author": ["D. Lin", "X. Wu"], "venue": "Neural Computation,", "citeRegEx": "Lin and Wu.,? \\Q2009\\E", "shortCiteRegEx": "Lin and Wu.", "year": 2009}, {"title": "Perceptrons: An Introduction to Computational Geometry, The MIT Press, Cambridge MA (Second edition", "author": ["Minsky", "Marvin", "Seymour Papert"], "venue": "Proceedings of ACL-08: HLT, pages 236\u2013244,Columbus, Ohio,", "citeRegEx": "Minsky et al\\.,? \\Q1969\\E", "shortCiteRegEx": "Minsky et al\\.", "year": 1969}, {"title": "Learning Distributed Representations of High-Arity Relational Data with Nonlinear Relational Embedding", "author": ["Paccanaro", "Alberto", "Geoffrey E. Hinton"], "venue": "Annual Meeting of the ACL,", "citeRegEx": "Paccanaro et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Paccanaro et al\\.", "year": 2003}, {"title": "Learning Distributed Representations of Concepts Using Linear Relational Embedding", "author": ["Paccanaro", "Alberto", "Geoffrey E. Hinton"], "venue": "IEEE Trans. Knowl. Data Eng. (TKDE)", "citeRegEx": "Paccanaro et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Paccanaro et al\\.", "year": 2001}, {"title": "Recursive distributed representations", "author": [], "venue": "Artificial Intelligence,", "citeRegEx": "Pollack,? \\Q1990\\E", "shortCiteRegEx": "Pollack", "year": 1990}, {"title": "Compositional Matrix-Space Models of Language", "author": ["Vol I", "H.M. Stationery Office", "London", "S. 1959. Rudolph", "E. Giesbrecht"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (pp. 907-916)", "citeRegEx": "1958 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "1958 et al\\.", "year": 2010}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["P. Smolensky"], "venue": "Artificial Intelligence", "citeRegEx": "Smolensky,? \\Q1990\\E", "shortCiteRegEx": "Smolensky", "year": 1990}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Socher", "Richard", "Cliff Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "The 28th International Conference on Machine Learning,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Learning distributed representations for the classification of terms", "author": ["A. Sperduti", "A. Starita", "C. Goller"], "venue": "Empirical Methods in Natural Language Processing,", "citeRegEx": "Sperduti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sperduti et al\\.", "year": 2011}, {"title": "A general framework for adaptive processing of data structures", "author": ["Sperduti", "Alessandro"], "venue": "Technical Report DSI-RT-15/97, Universita degli Studi di Firenze, Dipartimento di Sistemi e Informatica", "citeRegEx": "Sperduti and Alessandro.,? \\Q1997\\E", "shortCiteRegEx": "Sperduti and Alessandro.", "year": 1997}, {"title": "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities", "author": ["V.N. Vapnik", "A. Ya"], "venue": "Neuron,", "citeRegEx": "Vapnik and Ya.,? \\Q1999\\E", "shortCiteRegEx": "Vapnik and Ya.", "year": 1999}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Waibel et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Waibel et al\\.", "year": 1989}], "referenceMentions": [{"referenceID": 4, "context": "and Okaywe, T. W. (2013). Representing Objects, Relations, and Sequences.", "startOffset": 12, "endOffset": 25}, {"referenceID": 3, "context": "For example, Collobert et al. [2011] produce a system that outputs structure information (part of speech, chunks, semantic roles) for each word in a sentence.", "startOffset": 13, "endOffset": 37}, {"referenceID": 16, "context": "Vector Symbolic Architectures trace their origins to Smolensky\u2019s [1990] tensor product models, but avoid the exponential growth in vector size of those models.", "startOffset": 53, "endOffset": 72}, {"referenceID": 4, "context": "VSAs include Kanerva\u2019s Binary Spatter Codes (BSC) [1994, 1997], Plate\u2019s Holographic Reduced Representations (HRR) [1992, 2003], Rachkovskij and Kussul\u2019s Context Dependent Thinning (CDT) [2001], and Gayler\u2019s Multiply-Add-Permute coding (MAP) [1998].", "startOffset": 171, "endOffset": 193}, {"referenceID": 4, "context": "VSAs include Kanerva\u2019s Binary Spatter Codes (BSC) [1994, 1997], Plate\u2019s Holographic Reduced Representations (HRR) [1992, 2003], Rachkovskij and Kussul\u2019s Context Dependent Thinning (CDT) [2001], and Gayler\u2019s Multiply-Add-Permute coding (MAP) [1998].", "startOffset": 171, "endOffset": 248}, {"referenceID": 4, "context": "Here we encounter the \u201cBinding Encoding Problem\u201d in Cognitive Science and Artificial Intelligence surveyed by Treisman [1999]: for the word pair \u201csmart girl\u201d, we need to represent that \u201csmart\u201d refers to \u201cgirl\u201d, and not some other word in the sentence.", "startOffset": 110, "endOffset": 126}, {"referenceID": 4, "context": "Due to the commutativity of vector addition, multiple phrases such as in \u201cThe smart girl saw the gray elephant\u201d will have exactly the same vector sum as \u201cThe smart elephant saw the gray girl\u201d or even \u201celephant girl gray saw smart the the\u201d. In other words, vector addition gives us the \u201cbag of words\u201d used to create the sum, but no other structure information. Here we run into the classic \u201cBinding Encoding Problem\u201d in Cognitive Science and Artificial Intelligence, surveyed by Treisman [1999]. We need some way to bind \u201cgray\u201d to \u201celephant\u201d and not to \u201cgirl\u201d or to any other word, while retaining a distributed representation.", "startOffset": 74, "endOffset": 494}, {"referenceID": 4, "context": "\uf0b7 This formula for computing the next state also gives a way to represent input sequences. Kanerva [2009] and Plate [2003] previously employed this technique for sequence coding, using different binding operators.", "startOffset": 2, "endOffset": 106}, {"referenceID": 4, "context": "\uf0b7 This formula for computing the next state also gives a way to represent input sequences. Kanerva [2009] and Plate [2003] previously employed this technique for sequence coding, using different binding operators.", "startOffset": 2, "endOffset": 123}, {"referenceID": 4, "context": "These include the Perceptron Convergence Theorem [Rosenblatt 1959, see also Minsky & Papert 1969], Perceptron Cycling Theorem [Minsky & Papert 1969, Block & Levin 1970], Cover\u2019s theorem for the likelihood of a set of vectors to be separable [Cover 1965], and Vapnik-Chervonenkis generalization bounds [1971]. This body of theory permits us to prove learnability in many cases, as well as to set bounds on generalization.", "startOffset": 0, "endOffset": 308}, {"referenceID": 4, "context": "This paper presents characteristics, advantages and neural plausibility arguments. (These topics are reviewed in Gallant [1993].) In Information Retrieval, the use of high dimensional vectors to represent terms (words) was pioneered by Salton & McGill [1983].", "startOffset": 0, "endOffset": 128}, {"referenceID": 4, "context": "This paper presents characteristics, advantages and neural plausibility arguments. (These topics are reviewed in Gallant [1993].) In Information Retrieval, the use of high dimensional vectors to represent terms (words) was pioneered by Salton & McGill [1983]. Deerwester et al.", "startOffset": 0, "endOffset": 259}, {"referenceID": 4, "context": "This paper presents characteristics, advantages and neural plausibility arguments. (These topics are reviewed in Gallant [1993].) In Information Retrieval, the use of high dimensional vectors to represent terms (words) was pioneered by Salton & McGill [1983]. Deerwester et al. [1990] represented terms, documents and queries by starting with a document-by-term matrix, and then using Singular Value Decomposition to reduce dimensionality.", "startOffset": 0, "endOffset": 285}, {"referenceID": 4, "context": "1990], or computing the probability of a word given its surrounding window [Okanohara & Tsujii 2007, Collobert &Weston 2008]. See also the Brown clustering algorithm [1992], phrase clustering [Lin & Wu, 2009] and [Huang & Yates 2009].", "startOffset": 88, "endOffset": 173}, {"referenceID": 4, "context": "Then letting \u0278 be a constant placeholder for the target term \u201cKing\u201d, we would add HRRs for: \u201cLuther * \u0278\u201d, \u201c\u0278 * Jr\u201d, \u201cLuther * \u0278 * Jr\u201d, etc. The resulting order vector is then normalized and added to the semantic vector for King. The result is a vector that captures semantics as well as word order syntactic effects. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al.", "startOffset": 0, "endOffset": 373}, {"referenceID": 4, "context": "Then letting \u0278 be a constant placeholder for the target term \u201cKing\u201d, we would add HRRs for: \u201cLuther * \u0278\u201d, \u201c\u0278 * Jr\u201d, \u201cLuther * \u0278 * Jr\u201d, etc. The resulting order vector is then normalized and added to the semantic vector for King. The result is a vector that captures semantics as well as word order syntactic effects. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al. [2010]. Such vectors should provide interesting starting codings for terms in language systems, including MBAT.", "startOffset": 0, "endOffset": 453}, {"referenceID": 7, "context": "With respect to matrix multiplication bindings, Hinton\u2019s \u201ctriple memory\u201d system [1981] used random matrix connections in a subsidiary role while focusing on learning, rather than representation.", "startOffset": 48, "endOffset": 87}, {"referenceID": 4, "context": "Also, Plate\u2019s book [2003, page 22] later mentions in passing exactly the \u201cTwo Input\u201d version of the binding operator from Section 3, which he attributes to Hinton. Plate also lists matrix multiplication as an alternative binding possibility in Section 7.3, Table 26. In a Computational Linguistics setting, Rudolph & Giesbrecht [2010] proposed using only matrices (rather than vectors) to represent objects, and examined matrix multiplication as a composition operation.", "startOffset": 74, "endOffset": 335}, {"referenceID": 4, "context": "Also, Plate\u2019s book [2003, page 22] later mentions in passing exactly the \u201cTwo Input\u201d version of the binding operator from Section 3, which he attributes to Hinton. Plate also lists matrix multiplication as an alternative binding possibility in Section 7.3, Table 26. In a Computational Linguistics setting, Rudolph & Giesbrecht [2010] proposed using only matrices (rather than vectors) to represent objects, and examined matrix multiplication as a composition operation. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al.", "startOffset": 74, "endOffset": 527}, {"referenceID": 4, "context": "Also, Plate\u2019s book [2003, page 22] later mentions in passing exactly the \u201cTwo Input\u201d version of the binding operator from Section 3, which he attributes to Hinton. Plate also lists matrix multiplication as an alternative binding possibility in Section 7.3, Table 26. In a Computational Linguistics setting, Rudolph & Giesbrecht [2010] proposed using only matrices (rather than vectors) to represent objects, and examined matrix multiplication as a composition operation. Similar results were obtained by Sahlgren et al. [2008] by encoding order information with permutations; see also Recchia et al. [2010]. However, vector addition carried out by sparse matrices in D 2 dimensions rather than D dimensions is inefficient.", "startOffset": 74, "endOffset": 607}, {"referenceID": 4, "context": "Another related sequence approach, Time Delay Neural Networks of Waibel et al. [1989], has several layers of groups of hidden nodes.", "startOffset": 35, "endOffset": 86}, {"referenceID": 10, "context": "Gallant and Okaywe: Representing Objects, Relations and Sequences 26 For sequence representations that do not require learning, Kanerva [1988] represents sequences using pointer chains.", "startOffset": 128, "endOffset": 143}, {"referenceID": 10, "context": "Gallant and Okaywe: Representing Objects, Relations and Sequences 26 For sequence representations that do not require learning, Kanerva [1988] represents sequences using pointer chains. Later, Plate [2003] employs trajectory association, where the idea is to bind powers of a vector to sequence information.", "startOffset": 128, "endOffset": 206}, {"referenceID": 16, "context": "Another early work involving Reduced Representations is Pollack\u2019s RAAM (Recursive Auto Associative Memory) architecture [1990] and later extensions, for example LRAAM (Labeling RAAM) by Sperduti, Starita & Goller [1995].", "startOffset": 56, "endOffset": 127}, {"referenceID": 16, "context": "Another early work involving Reduced Representations is Pollack\u2019s RAAM (Recursive Auto Associative Memory) architecture [1990] and later extensions, for example LRAAM (Labeling RAAM) by Sperduti, Starita & Goller [1995]. These approaches use Backpropagation learning (or variants) on a network of inputs, hidden units, and outputs that attempt to reproduce inputs.", "startOffset": 56, "endOffset": 220}, {"referenceID": 4, "context": "These approaches use Backpropagation learning (or variants) on a network of inputs, hidden units, and outputs that attempt to reproduce inputs. The hidden units, after learning, encode the reduced representations of the inputs. A drawback of these approaches is the need for learning over all inputs to achieve the representations of the inputs. For example, adding additional input cases requires re-learning the representation for all previous input patterns using Backpropagation (violating the Efficient Coding Constraint). Improvements in capacity and generalization were reported by Voegtlin & Dominey [2005]. Although these approaches are all too slow (non-linear) for the Representation Generation Stage, their abilities to capture generalization may present good synergy as part of the Pre-processing Stage.", "startOffset": 0, "endOffset": 615}, {"referenceID": 4, "context": "As with RAAM architectures, generalization ability may prove useful in producing Pre-processing Stage inputs for MBAT or other approaches. Sperduti [1997] proposed a \u201cgeneralized recursive neuron\u201d architecture for classification of structures.", "startOffset": 116, "endOffset": 155}, {"referenceID": 21, "context": "It is worth noting that Sperduti et al. [1995] conduct simulations to show good performance for learning to discriminate presence of particular terms in the representation.", "startOffset": 24, "endOffset": 47}, {"referenceID": 3, "context": "In a later work, Collobert et al. [2011] develop a unified neural network architecture for these linguistic tasks, where all tasks are trained using two somewhat complex architectures based upon Time Delay Neural Networks.", "startOffset": 17, "endOffset": 41}, {"referenceID": 4, "context": "Socher\u2019s Recursive Neural Networks (RNN) are binary trees with distributed representations, which are structurally identical to \u201cTwo Input\u201d binding operators in Section 3. In particular, Socher\u2019s matrix [2011a] for combining two vectors is equivalent to concatenating rows from M Left and M Right to form a single \u201cdouble wide\u201d matrix for applying to pairs of concatenated column vectors.", "startOffset": 129, "endOffset": 211}, {"referenceID": 4, "context": "Finally, the MBAT architecture can motivate Cognitive Science hypotheses. For example, we can hypothesize that there are neural resources devoted to recognizing phrases in language at an early processing stage. This hypothesis is supported by the computational benefits we have seen, as well as by the help given for phrase recognition in both written and spoken language: punctuation, small sets of prepositions and conjunctions, typical word ordering for phrases, pauses in speaking, tone patterns, coordinated body language, etc. Recent Magnetoencephalography studies by Bemis & Pylkk\u00e4nen [2011] give additional support.", "startOffset": 16, "endOffset": 599}], "year": 2013, "abstractText": "Vector Symbolic Architectures (VSAs) are high-dimensional vector representations of objects (eg., words, image parts), relations (eg., sentence structures), and sequences for use with machine learning algorithms. They consist of a vector addition operator for representing a collection of unordered objects, a Binding operator for associating groups of objects, and a methodology for encoding complex structures. We first develop Constraints that machine learning imposes upon VSAs: for example, similar structures must be represented by similar vectors. The constraints suggest that current VSAs should represent phrases (\u201cThe smart Brazilian girl\u201d) by binding sums of terms, in addition to simply binding the terms directly. We show that matrix multiplication can be used as the binding operator for a VSA, and that matrix elements can be chosen at random. A consequence for living systems is that binding is mathematically possible without the need to specify, in advance, precise neuron-to-neuron connection properties for large numbers of synapses. A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms), is described that satisfies all Constraints. With respect to machine learning, for some types of problems appropriate VSA representations permit us to prove learnability, rather than relying on simulations. We also propose dividing machine (and neural) learning and representation into three Stages, with differing roles for learning in each stage. For neural modeling, we give \u201crepresentational reasons\u201d for nervous systems to have many recurrent connections, as well as for the importance of phrases in language processing. Sizing simulations and analyses suggest that VSAs in general, and MBAT in particular, are ready for real-world applications.", "creator": "Microsoft\u00ae Word 2010"}}}