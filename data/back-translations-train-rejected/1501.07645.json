{"id": "1501.07645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2015", "title": "Hyper-parameter optimization of Deep Convolutional Networks for object recognition", "abstract": "Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks. Specifically, we report three DCN networks generated by our proposed algorithm that produce &lt;9% test error rate, with the best network exhibiting a test error rate of 7.81% on the CIFAR-10 benchmark. Our results compare favorably to the current state-of-the-art of 7.97 % test error rate for CIFAR-10 that are obtained by hand tuning.", "histories": [["v1", "Fri, 30 Jan 2015 02:08:51 GMT  (126kb,D)", "https://arxiv.org/abs/1501.07645v1", "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015"], ["v2", "Sun, 17 May 2015 03:32:22 GMT  (127kb,D)", "http://arxiv.org/abs/1501.07645v2", "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015"]], "COMMENTS": "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["sachin s talathi"], "accepted": false, "id": "1501.07645"}, "pdf": {"name": "1501.07645.pdf", "metadata": {"source": "CRF", "title": "HYPER-PARAMETER OPTIMIZATION OF DEEP CONVOLUTIONAL NETWORKS FOR OBJECT RECOGNITION", "authors": ["Sachin S. Talathi", "Diego CA"], "emails": [], "sections": [{"heading": null, "text": "Index terms - hyperparameter optimization, deep folding networks, sequential model-based optimization"}, {"heading": "1. INTRODUCTION", "text": "The primary task for a supervised machine learning algorithm is to use training data sets {xtr, ytr} to find a function f: x \u2192 y that generalizes well about the test (or hold-out) data set {xh, yh}. Very often, f is achieved by optimizing a training criterion, C, in relation to a set of parameters. As a rule, the learning algorithm used to optimize C contains its own set of free parameters, called learning algorithms. These hyperparameters are often estimated by cross-sectional network search checks. In addition to learning algorithms that contain hyperparameters, neural network models such as deep convolutional networks (DCNs) will also consist of these learning parameters."}, {"heading": "2. METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Formulation of the problem", "text": "The DCN model, M, is parametrized by two parameters, the first being defined by optimizing a training criterion, C, using a learning algorithm of the type of gradient descent, such as the reverse propagation algorithm, and the second by the so-called hyperparameter. Hyper parameters \u03bba define the DCN architecture and hyperparameters \u03bbl, are associated with the learning algorithm used to optimize C. The target for the DCN hyperparameter optimization is Xiv: 150 1.07 645v 2 [cs.C V] on May 17, 201 5, to solve the problem of joint optimization, as stated below: {w} = argmin [yr] or yr (yr] (yr) (yr) (yr) (yr), where (yr), (yr), (yr), yr, yr, yr (yr), yr (yr), yr (yr), yr (yr), yr (yr), yr, yr (yr), yr (yr), yr (yr), yr (yr), yr (yr, yr, yr (yr), yr (yr), yr (yr), yr (yr, yr, yr (yr), yr (yr), yr (yr), yr (yr, yr, yr (yr), yr (yr), yr (yr, yr, yr (yr), yr (yr), yr (yr, yr (yr), yr (yr), yr (yr, yr, yr (yr), yr (yr, yr), yr (yr, yr (yr), yr (yr, yr, yr, yr, yr, yr, yr (yr), yr (yr), yr (yr, yr, yr, yr (yr), yr (yr, yr, yr, yr, yr), yr (yr, yr, yr (yr, yr, yr), yr (yr, yr, yr, yr, yr, yr, yr (yr, yr, yr, yr, yr, yr, yr), yr (yr, yr, yr, yr, yr, yr, yr, yr (yr), yr (yr, yr, yr, yr, yr, yr, yr, yr, yr, yr, yr, yr, yr, yr"}, {"heading": "2.2. Sequential model based optimization", "text": "SMBO is a direct search method for nonlinear optimization, starting with the selection of a meta-model of the function (for which an extreme is sought), then applying an active learning strategy to select a query point that offers the most potential to approach the extreme. Specifically, we assume that we have a database D1: t = {\u03bb1: t, e1: t} of t DCN models in which the extreme parameter and ei | ti = 1 are the validation errors on the hold-out dataset generated by each of the t DCN models. The basic idea underlying SMBO is to replace the original optimization problem of a given function, such as the time-consuming and computationally expensive, with an equivalent problem of optimizing the expected value of a utility function, u (e)."}, {"heading": "3. RESULTS", "text": "We evaluate our proposed SMBO algorithms on the CIFAR-10 benchmark, which consists of 60,000 32x32 color images. < The collection is divided into 50,000 training images and 10,000 test images. All DCNs generated by our proposed algorithm were trained using cudaconvnet21. We used a 3-step cooling process, starting with the learning rate l = 0.01, the dynamics m = 0.9, the weight drop parameter wc = 0.0005 for the first 120 epochs followed by a further 20 epochs, reducing the learning rate by a factor of 10 (keeping other parameters the same) and then producing training for 10 more epochs by reducing the learning rate by a factor of 10. As the primary focus for us in this work is further reduced, it is to be determined whether SMBO can be used to identify suitable DCN architectures, we have the DCN hypotheses we have associated with the learning algorithms."}, {"heading": "4. CONCLUSION", "text": "In this paper, we have proposed a simple SMBO algorithm and a recipe for hyperparameter optimization of DCN architectures. We have shown that SMBO can be used to create a large number of \"good\" DCN architectures, which can then form the backbone for further investigation. Our results suggest that SMBO can actually be used to identify superior DCNs. In summary, our work in this paper adds to those from previous work [1, 3] to the scope of models that can be realistically examined without the researchers having to limit themselves to manually evaluating some architectural parameters at any given time."}, {"heading": "5. REFERENCES", "text": "[1] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kegl, \"Algorithms for Hyperparameter Optimization,\" in NIPS, 2011, Volume 24. [2] Distributed asynchronous hyper parameter optimization. https: / / github.com / hyperopt / hyperopt. [3] J. Snoek, H. Larochelle, and R.P. Adam, \"Practical bayesian optimization for machine learning,\" in NIPS, 2012, vol. 25. [4] E. Brochu, V.M. Cora, and N. Freitas, \"A tutorial on bayesian cost functions, with application to active user modeling and hierarchical reinforcement learning,\" Arxiv, vol. arXiv, arXiv of Y.2599, 2010."}], "references": [{"title": "Algorithms for hyper-parameter optimization", "author": ["J. Bergstra", "R. Bardenet", "Y. Bengio", "B. Kegl"], "venue": "NIPS, 2011, vol. 24.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical bayesian optimization for machine learning", "author": ["J. Snoek", "H. Larochelle", "R.P. Adam"], "venue": "NIPS, 2012, vol. 25.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. Freitas"], "venue": "Arxiv, vol. arXiv:1012.2599, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, vol. 13, pp. 281\u2013305, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. Rep., Dept. Computer Science, University of Toronto, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Arxiv, vol. arXiv:1409.1556, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeply supervised nets", "author": ["C.Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "NIPS, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Striving for simplicity, all convolution net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "ICLR, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving deep into rectifiers, surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Arxiv:1502.01852, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1852}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "F. Warde", "D. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "ICML, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["M.F. Stollenga", "J. Masci", "F. Gomez", "J Schmidhuber"], "venue": "NIPS, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeply supervised nets", "author": ["C.Y. Lee", "S. Xi", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "NIPS, 2014. 5", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "As a result, many of the stateof-the-art DCNs are manually designed, making the task of tuning these hyper-parameters more of an art than a science [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 1, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 2, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 3, "context": "In recent years, there has been a concerted effort in the machine learning community to develop better algorithms to solve the hyper-parameter optimization problem [1, 2, 3, 4, 5].", "startOffset": 164, "endOffset": 179}, {"referenceID": 2, "context": "We then present the general strategy of sequential model based optimization (SMBO) [4] and summarize our approach to SMBO for designing DCN architectures.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "The basic idea underlying SMBO is to replace the original optimization problem of finding extrema of a given function, such as \u03a8(\u03bb), which is time consuming and computationally expensive, with an equivalent problem of optimization of expected value of an utility function, u(e) [4].", "startOffset": 278, "endOffset": 281}, {"referenceID": 2, "context": "A common choice for the utility function u(e), is the Expected Improvement function [4], u(e) = max ((e\u2217 \u2212 e), 0), in which case, Eq.", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": ", choose e\u2217 to be some quantile of observed e values, and we define two density functions; l(\u03bb) = p(\u03bb1:t|e) when e \u2264 e\u2217 and g(\u03bb) = p(\u03bb1:t|e) when e > e\u2217 as proposed in [1], then,", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": ", [1], proposed an adaptive Parson estimator algorithm to evaluate Eq.", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "For normalization layer; we only consider local response normalization across filter maps [6], with a scaling factor of 0.", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "It has been reported in the literature [7] that very deep networks are difficult to train primarily suffering from vanishing gradient problem at larger depths.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "For the results presented here, we consider t = 32 as the size of our initial database based on our analysis of random search hyper-parameter optimization [5] and we set p = 0.", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "error, shown in yellow) test error (evaluated in multi-view test mode, [6]) E(i) = \u3008ei\u221210:i\u3009 and in Figure 1b, we plot the minimum test error M(i) = min(e0:i) as function of the iteration number i, respectively.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "In comparison, the best hand-tuned DCN architecture, produced by [6] exhibits 11% test error in multi-view mode and requires a longer training time on the order of 500 epochs.", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "Since the state-of-the-art performance numbers for the CIFAR-10 benchmark dataset are usually reported in multi-view mode (with data-augmentation [6]), we report multi-view test error of 7.", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "97% [8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "At the time of writing of this manuscript for camera ready version, we came across a recent paper [9], that reported multi-view test error of 7.", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "Yet another paper [10], reported the utility of using parametric-relu neuron as opposed to the relu neurons.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "While none of the optimized DCN networks that we report in Table 3 generate better performance numbers than the latest state-of-the-art numbers reported in [9], we wanted to determine whether the use of parametric-relu neuron can boost the performance of the optimized DCN networks that we have identified through the hyper-parameter optimization approach.", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "Maxout [11] maxout 9.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "38 >6 M Dropconnect [12] relu 9.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "32 dasNet [13] maxout 9.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "22 >6 M Network in Network [14] relu 8.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "81 \u22481 M Deeply Supervised [15] relu 7.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "97 \u22481 M All-CNN [9] relu 7.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time.", "startOffset": 75, "endOffset": 81}, {"referenceID": 1, "context": "In summary, our work in this paper in addition to those from earlier works [1, 3] broaden the scope of the models that can be realistically investigated, without the need for the researchers to be restricted to manual evaluation of a few architectural parameters at any given time.", "startOffset": 75, "endOffset": 81}], "year": 2015, "abstractText": "Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks.", "creator": "LaTeX with hyperref package"}}}