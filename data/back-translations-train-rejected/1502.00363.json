{"id": "1502.00363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2015", "title": "Iterated Support Vector Machines for Distance Metric Learning", "abstract": "Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while many existing metric learning algorithms become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem, and solve it by iterated training of support vector machines (SVM). The new formulation is easy to implement, efficient in training, and tractable for large-scale problems. Two novel metric learning models, namely Positive-semidefinite Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained Metric Learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experimental results on UCI dataset classification, handwritten digit recognition, face verification and person re-identification demonstrate that the proposed metric learning methods achieve higher classification accuracy than state-of-the-art methods and they are significantly more efficient in training.", "histories": [["v1", "Mon, 2 Feb 2015 05:30:44 GMT  (147kb)", "http://arxiv.org/abs/1502.00363v1", "14 pages, 10 figures"]], "COMMENTS": "14 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wangmeng zuo", "faqiang wang", "david zhang", "liang lin", "yuchi huang", "deyu meng", "lei zhang"], "accepted": false, "id": "1502.00363"}, "pdf": {"name": "1502.00363.pdf", "metadata": {"source": "CRF", "title": "Iterated Support Vector Machines for Distance Metric Learning", "authors": ["Wangmeng Zuo"], "emails": ["cswmzuo@gmail.com;", "tshfqw@163.com)", "dzhang@comp.polyu.edu.hk;", "cslzhang@comp.polyu.edu.hk)", "linliang@ieee.org)", "yuchi@nec.cn)"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.00 363v 1 [cs.L G] 2F eb2 015 1Index Terms - metric learning, support vector machine, core method, lagrange duality, alternative optimization"}, {"heading": "1 INTRODUCTION", "text": "It is about the question of how to learn in a system in which the different learning methods and methods are interlinked. [4], how to learn in a system, how to learn in a system. [5], how to learn in a system. [6], how to learn in a system. [6], how to learn in a system. [6], how to learn in a system. [7], how to learn in a system. [8], how to learn in a system. [10], how to learn in a system. [7], how to learn in a system. [7], how to learn in a system. [8], how to learn in a system. [10], how to learn retrievally [11], [12] how to learn."}, {"heading": "2 RELATED WORK", "text": "This year, there is less than a year to go before an agreement can be reached."}, {"heading": "3 POSITIVE-SEMIDEFINITE CONSTRAINED METRIC LEARNING (PCML)", "text": "The question that arises is whether it is a PSD matrix, < A > B > tr (A T B) (xi \u2212 xj) T) = M (xi \u2212 xj) T s (xi \u2212 xj) T s, (2) where M is a PSD matrix, < B > tr (A T B) is defined as the inner product of two matrices A and B, and tr (xi \u2212 xj) stands for the matrix matrix operator. For each pair of xi and xj we define a matrix Xij = (xi \u2212 xj) T. With Xij, the Mahalanobis distance can be rewritten as d2M (xi \u2212 j)."}, {"heading": "3.2 Alternative Optimization Algorithm", "text": "In order to solve the dual problem efficiently, we propose an optimization approach by alternatively updating the problem of \u03bb and Y. Given Y, we introduce a new variable \u03b7 with \u03b7ij = 1 \u2212 hij < Xij, Y > = 1 \u2212 hij (xi \u2212 xj) T Y (xi \u2212 xj), and the subproblem on \u03bb can be formulated as follows: max \u03bb \u2212 12 \u2211 i, j \u00b2 k, l \u00b2 klhijhkl < Xij, Xkl > + \u2211 i, j \u00b2 ijs.t. \u2211 i, j \u00b2 ijhij = 0, 0 \u2264 \u03bbij \u2264 C, \u0432 i, j. (7) The subproblem (7) is a QP problem. We can define the core function of sample pairs as follows: K ((xi, xj), (xk, xl)), (Xij, Xkl > = (xi \u2212 explicit)."}, {"heading": "3.3 Optimality Condition", "text": "As shown in [54], [55], the general alternating minimization approach will approximate Y = Y. By alternatively updating (4) and Y, the proposed algorithm can achieve the global optimum of the problems in (4) and (5). The optimal state of the proposed algorithm can be verified by the duality gap in each iteration (1), which is defined as the difference between the primary and dual targets: DualGap (n) PCML (n). \u2212 n The optimal state of the proposed algorithm (n) can be verified by the duality gap in each iteration. \u2212 n The difference (n) is defined as the difference between the primary and dual targets: DualGap (n)."}, {"heading": "3.4 Remarks", "text": "Warm start: When updating \u03bb, we use a simple warm start strategy. We use the solution of the previous iteration as the initialization of the next iteration. Since the previous solution can serve as a good guess, warm start leads to a significant increase in efficiency. Construction of pairwise constraints: Based on the training set, we can introduce a total of pairwise N2 constraints. In practice, however, we only need a subset of pairwise constraints to reduce computational costs. For each sample, we find their k nearest neighbors to construct similar pairs, and their k farthest neighbors to construct unequal pairs. Therefore, we only need 2kN of pairwise constraints. By this strategy, we can reduce the scale of malwise constraints from O (N2) to O (kN). Since k is usually small in practice (= 1 \u0445 3), the computational costs of metric learning are greatly reduced."}, {"heading": "4 NONNEGATIVE-COEFFICIENT CONSTRAINED METRIC LEARNING (NCML)", "text": "For a series of rank-1 PSD matrices Mt = mtm T t (t = 1, \u00b7 \u00b7, T), a linear combination of Mt is defined as M = \u2211 t \u03b1tMt, where \u03b1t is the scalar combination coefficient. Theorem 1: If the scalar coefficient \u03b1t \u2265 0, \u0442t, the matrix M = \u2211 t \u03b1tMt is a PSD matrix, where Mt = mtm T is an arank-1 PSD matrix. Proof: Denote by u-Rd is a random vector. Based on the expression of M, we have: u T Mu = uT (\u0445 t-tmT t t) u = \u2211 t-T mtm T = \u0445 t-tm T (u T-mt) 2.5 Da (u T mt) 2 instead of 0 and ent-T-0, we have u Mu-mtm T excellent 0, hence a PM matrix."}, {"heading": "4.1 NCML and Its Dual Problem", "text": "Motivated by Theorem 1, we propose to convert the PSD constraint into (4) by re-parameterizing the distance metrix M and developing a non-negative coefficient method for each pair (xi, xj) to learn the PSD matrix M. Assuming that the learned matrix should be the linear combination of Xij with the non-negative coefficient constraint, the NCML model can be formulated as follows: min M, b, x, 1 2: appendix, 2: appendix. (< M, Xij > + b). hij (< M, Xij > + b)."}, {"heading": "4.2 Optimization Algorithm", "text": "There are two groups of variables, \u03b7 and \u03b2, in problem (19). We apply an alternative optimization approach to solve them. First, the variables \u03b2ij can be solved as follows: max \u03b2 \u2212 12 \u2211 i, j \u2211 k, l\u03b2ij\u03b2klhijhkl < Xij, Xkl > + \u2211 i, j \u03b4ij\u03b2ijs.t. 0 \u2264 \u03b2ij \u2264 C, i, j \u2211 l, l, l \u03b2ijhij = 0, (22) Algorithm 2 algorithm of NCMLInput: Training set {(xi, xj), hij}. Output: The matrix M. Initialize \u03b7 (0) with small random values,."}, {"heading": "4.3 Optimality Condition", "text": "We check the duality gap of NCML to determine the optimum state of the problem. NCML = 12 \u2211 i, j, l\u03b1 (n) ij \u03b1 (n) kl < Xij, Xkl > + C \u2211 n, jump (n) ij + 12 \u2211 i, j, k, l (n) ij hij + \u03b7 (n) ij (n) ij (n) ij < Xij, Xkl > \u2212 n, j\u03b2 (n) ij (n) digitality (n) ij (n) ij (n) ij (n) ij (n) ij (n) ij (n) ij (n) ij (n), ij (n) (ij) (ij) (n), ij (n) (ij), ij (n) (ij), ij (n), ij (n), ij (n), ij (n), ij (n), ij (n), ij (n), ij (n), ij (n), ij (n)."}, {"heading": "4.4 Remarks", "text": "The cost of constructing the matrix M is lower than O (kNd2), and this operation is required only once after the convergence of \u03b2 and XL to construct the pairwise constraints for NCML < k = > PCML. In each iteration, NCML calls the SVM solver twice, while PCML only needs it once. If the SMO-type SVO algorithm [34] is used for SVM education, the computational complexity of NCML O (k2N2d) is an additional advantage of NCML in its lower computing costs relative to d, the computation of < Xij, Xkl > and the construction of the matrix M. Since < Xij, Xkl > = (xi \u2212 xj) T (xk) 2, the cost of calculating < Xij, Xkl > is learning < Xilt; Xij, Xkl > and the cost of calculating the XkO is less than < the cost of calculating the XkO is < Xk >."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "We evaluate the proposed PCML and NCML models for k-NN classification (k = 1) using 9 UCI datasets, 4 handwritten digit datasets, 2 face verification datasets, and 2 person re-identification datasets. We compare PCML and NCML with Euclidean distance metrics and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML file [22]. If the breakdown of training set and test set is not defined, we evaluate the performance of each method using a 10x cross-validation, and the classification error rate and training time are determined by a 10x cross-validation averaging. PCML and NCML are evaluated using the LibSVM1 toolbox, the best results are NCMI-M5, NCM3, and NCMM3."}, {"heading": "5.1 Results on the UCI Datasets", "text": "First, we use 9 sets of data from the UCI Machine Learning Repository [57] to evaluate the proposed models; the information from the 9 UCI sets is summarized in Table 2; on the satellite, SPECTF heart and letter sets, the lowest error and lowest error set are defined; on the other sets, we use 10-fold cross-validation to evaluate the metric learning methods. http: / / www.csie.edu.edu.tw / listed cjlin / libsvm / 2. http: / www.cs.berkeley.edu / we fowlkes / software / nca / 3.: http: / www.csie.edu.tw / listed cjlin / libsvm / 2. http: / www.cs.berkeley.edu / software / nca / 3.: http: / / www.csie.edu.tw / listed cjlin / libsvm / 2."}, {"heading": "5.2 Handwritten Digit Recognition", "text": "We evaluate the proposed methods on four handwritten digit datasets: MNIST 110, Pen-based Recognition of Handwritten Digit Dataset (PenDigits), Semeion and USPS. Table 4 summarizes the basic information of these four handwritten digit datasets. On the NCT, PenDigits and USPS datasets, we use the defined training sets to train the metrics, and use the defined test sets to calculate the classification error rates. On the NCT dataset, we use 10-fold cross-validation to evaluate the metric learning methods, and the classification error rate and training time are achieved by averaging 10 passes of the 10-fold cross-validation. Since the sections of the images in the MNIST, Semeion and USPS datasets are relatively high, we use the main component analysis of the MPCL MPCL (PCA) to reduce the feature space of the MPCL to the 100-dimension of the MPL and we do not have the classification space of the MPC5 to the MPL and the MPL to the substrate the substrate the MPL."}, {"heading": "5.3 Face Verification", "text": "In this section, we evaluate the proposed methods for facial verification using two challenging facial databases: Labeled Faces in the Wild (LFW) [59] and Public Figures (PubFig) [60]."}, {"heading": "5.3.1 The LFW Database", "text": "The facial images in the LFW database were collected from the Internet and show large variations in pose, lighting, expression, etc. The database consists of 13,233 facial images of 5,749 people. As part of the image-limited setting, the performance of a facial verification method is evaluated by 10-fold cross-validation. For each of the 10 runs, the database offers 300 positive pairs and 300 negative pairs for testing and 5,400 image pairs for training. The verification rate and the receiver characteristic (ROC) curve of each method are obtained by an average over the 10 runs. In our experiments, we use the SIFT [61] characteristics and the attribute attributes provided by [8] and [60] to evaluate the metric learning methods. Since the dimension of the SIFT characteristics is high (i.e. 128 x 3 x 9), PCA is used to reduce the feature verification MISPCL checks to 150, we do not override the data base, or limit the FW setting to two."}, {"heading": "5.3.2 The PubFig Database", "text": "The PubFig database [60] contains 58,797 facial images of 200 people with large variations in pose, lighting, expression, scene, camera, image conditions and parameters, etc. In this database, the facial verification methods are also evaluated by 10-fold cross-validation. Out of the 20,000 image pairs given, we randomly select 18,000 pairs for training and use the remaining 2,000 pairs for testing in each run. ROC curves and verification rates are obtained by averaging over the 10 rounds.10 We use the attribute attributes provided by [60] to evaluate the competing methods. Fig. 8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attributes Classifiers [60] and Euclidean baseline distance. It can be seen that the performance of PCML and NCML methods is better than other methods and NCML methods."}, {"heading": "5.4 Person Re-identification", "text": "In this section, we will evaluate the performance of the proposed methods of person recognition, i.e. identification of a person in different places and at different times. [62] To evaluate the performance of the proposed methods, two challenging person recognition databases are used, the Viewpoint Invariant Pedestrian Recognition (VIPeR) database [63] and the Context Aware Vision using Image-based Active Recognition for ReIdentification (CAVIAR4REID) database [64]."}, {"heading": "5.4.1 The VIPeR Database", "text": "The VIPeR database contains 1,264 pedestrian images of 632 people from two camera perspectives (camera A and camera B). For each person, there are two images taken from different angles with a change of 90 degrees. In our experiments, we randomly select 316 people and use their images for training and use the images of the other 316 people for testing. For the test images, we use the images from camera B as a probe set and the images from camera A as a gallery set. Finally, 10 subdivisions of training and test sets are constructed, and the average accuracy over the 10 test sets is calculated as final accuracy. We report on the cumulative Matching Characteristic (CMC) curves of the competing methods in Fig. 9. We also compare their accuracy under different ranks in Table 8. From Fig. 9 and Table 8 you can see that both PCML as well as NCITKL and Euclidean distance are significantly better than MISL."}, {"heading": "5.4.2 The CAVIAR4REID Database", "text": "CAVIAR4REID consists of 1,220 pedestrian images of 72 people, with the images extracted from the shopping centre scenario of the CAVIAR database. [64] The database covers a wide range of image resolutions and pose variations. The minimum and maximum image sizes in the CAVIAR4REID database are 17 x 39 and 72 x 144, respectively. Following [65] and [10] we use the hierarchical Gaussian (HG) characteristics to evaluate the metric learning methods. In accordance with the assessment protocol in [10] we randomly select 36 people and use their images for training and use the remaining images for testing. For the test images, we randomly select one image for each person to construct a test set consisting of 36 images, and use the other test images as a gallery set. Finally, 10 MISMISL partitions are deconstructed for training and test sets, and the results are obtained by averaging 10."}, {"heading": "6 CONCLUSION", "text": "The proposed models can guarantee the positive semidefinitive property of the learned matrix M and can be efficiently solved by the existing SVM solvers. Experimental results on nine UCI machine learning repository datasets and four handwritten digital datasets showed that the proposed PCML and NCML methods can not only achieve higher classification accuracy compared to the most modern metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39] and DML-eig [22], but are also much faster in training. On average, they are 35 and 21 times faster than PLML, 12 the third-fastest metric learning method, and the proposed PCML and NCML methods indicate a higher efficiency in identifying people who perform FVIR-FREE tasks very well and the results of experimentation."}, {"heading": "APPENDIX A THE DUAL OF PCML", "text": "The original problem of PCML is formulated as follows: < < M, Xij > + b) < M, Xij > + b) < M, Xij + b) < M, Xij + c) < M, Xij + c) < M, Xij + c) < M, Xij + c). < M, Xij + b). (1). (3). (4). (4). (4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5. (5). (5). (5). (5). (5). (5.). (5.). (5. (5). (5). (5). (5.). (5.). (5. (5). (5). (5). (5.). (5.). (5. (5). (5.). (5.). (5.). (5. (5.). (5.). (5.). (5.). (5.). (5. (5.). (5.). (5. (5.). (5.). (5.). (5. (5.). (5. (5.). (5.). (5. (5.). (5.). (5.). (5. (5. (5.). (5.). (5.). (5.). (5. (5.). (5.). (5.). (5.). (5.). (5. (5.). (5.). (5. (5. (5.). (5. (5.).). (5.). (5"}, {"heading": "APPENDIX B THE DUAL OF NCML", "text": "The original problem of NCML is as follows: min \u03b1, b, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "APPENDIX C THE DUAL OF THE SUBPROBLEM ON \u03b7 IN", "text": "NCML The sub-problem on the one hand is formulated as follows: Min. \u2212 jp 63, Min. < Xij, Xkl > < Xij, Xkl >, (59), where it occurs, where it occurs. \u2212 jp < Xij, Xkl >, (64), where it occurs. < Xij, Xkl > +, jp. \u2212 jp < Xij, Xkl, k. \u2212 jp < jp < Xij, Xkl >, (60), where it represents the Lagrange multiplier that meets the requirements. < Xij, Xkl > +, jp. \u2212 jp & lt. \u2212 jp < jp & lt."}], "references": [{"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv:1306.6709, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res., vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2002, pp. 505\u2013512.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Metric learning to rank", "author": ["B. McFee", "G. Lanckriet"], "venue": "Proc. 27th Int. Conf. Mach. Learn. (ICML 2010), 2010, pp. 775\u2013782.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust structural metric learning", "author": ["D. Lim", "B. McFee", "G.R. Lanckriet"], "venue": "Proc. 30th Int. Conf. Mach. Learn. (ICML 2013), 2013, pp. 615\u2013623.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning semi-riemannian metrics for semisupervised feature extraction", "author": ["W. Zhang", "Z. Lin", "X. Tang"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 23, no. 4, pp. 600\u2013611, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Distance metric learning for kernel machines", "author": ["Z. Xu", "K.Q. Weinberger", "O. Chapelle"], "venue": "arXiv:1208.3422, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Proc. IEEE 12th Int. Conf. Comput. Vis. (ICCV 2009), 2009, pp. 498\u2013505.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P. Roth", "H. Bischof"], "venue": "Proc. 2012 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2012), 2012, pp. 2288\u20132295.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning locally-adaptive decision functions for person verification", "author": ["Z. Li", "S. Chang", "F. Liang", "T.S. Huang", "L. Cao", "J.R. Smith"], "venue": "Proc. 16th IEEE Int. Conf. Comput. Vis. (ICCV 2013), 2013, pp. 3610\u20133617.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["S. Hoi", "W. Liu", "S. Chang"], "venue": "Proc. 2008 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2008), 2008, pp. 1\u20137.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval", "author": ["L. Yang", "R. Jin", "L. Mummert", "R. Sukthankar", "A. Goode", "B. Zheng", "S.C.H. Hoi", "M. Satyanarayanan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 1, pp. 30\u201344, Jan. 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Human activity recognition with metric learning", "author": ["D. Tran", "A. Sorokin"], "venue": "Proc. 10th Eur. Conf. Comput. Vis. (ECCV 2008), 2008, pp. 548\u2013561.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Metric learning for text documents", "author": ["G. Lebanon"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 497\u2013 508, Apr. 2006.  14", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning a distance metric from a network", "author": ["B. Shaw", "B. Huang", "T. Jebara"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2011, pp. 1899\u20131907.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2004, pp. 513\u2013520.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Large margin component analysis", "author": ["L. Torresani", "K. Lee"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2006, pp. 1385\u20131392.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Neighborhood repulsed metric learning for kinship verification", "author": ["J. Lu", "X. Zhou", "Y. Tan", "Y. Shang", "J. Zhou"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 36, no. 2, pp. 331\u2013345, Feb. 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2005, pp. 1473\u20131480.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2005, pp. 451\u2013458.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Positive semidefinite metric learning using boosting-like algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. Hengel"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 1007\u20131036, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Y. Ying", "P. Li"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 1\u201326, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Informationtheoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "Proc. 24th Int. Conf. Mach. Learn. (ICML 2007), 2007, pp. 209\u2013216.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Metric learning for large scale image classification: generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "Proc. 2012 Eur. Conf. Comput. Vis. (ECCV 2012), 2012, pp. 488\u2013501.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Checkik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "J. Mach. Learn. Res., vol. 11, pp. 1109\u20131135, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 2002, pp. 561\u2013568.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "F. France", "M. Pontil"], "venue": "Proc. 10th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, 2004, pp. 109\u2013117.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Kernel discriminant analysis for positive definite and indefinite kernels", "author": ["E. Pekalska", "B. Haasdonk"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 6, pp. 1017\u20131032, Jun. 2009.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised kernel mean shift clustering", "author": ["S. Anand", "S. Mittal", "O. Tuzel", "P. Meer"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 36, no. 6, pp. 1201\u20131215, Jun. 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient classification for additive kernel SVMs", "author": ["S. Maji", "A.C. Berg", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 66\u201377, Jan. 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "New York: Springer,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1995}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Trans. Intell. Syst. Technol., vol. 2, pp. 1\u201327, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods: Support Vector Learning. Cambridge, MA: MIT Press, 1999, pp. 185\u2013208.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1999}, {"title": "Core vector machines: Fast SVM training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P.M. Cheung"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 363\u2013392, 2005.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Solving multiclass support vector machines with larank", "author": ["A. Bordes", "L. Bottou", "P. Gallinari", "J. Weston"], "venue": "Proc. 24th Int. Conf. Mach. Learn. (ICML 2007), 2007, pp. 89\u201396.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "A scalable modular convex solver for regularized risk minimization", "author": ["C.H. Teo", "Q. Le", "A. Smola", "S.V.N. Vishwanathan"], "venue": "Proc. 13th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, 2007, pp. 727\u2013736.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Pegasos: primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming, vol. 127, no. 1, pp. 3\u201330, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Parametric local metric learning for nearest neighbor classification", "author": ["J. Wang", "A. Woznica", "A. Kalousis"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2012, pp. 1610\u20131618.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Informationtheoretic semi-supervised metric learning via entropy regularization", "author": ["G. Niu", "B. Dai", "M. Yamada", "M. Sugiyama"], "venue": "Proc. 29th Int. Conf. Mach. Learn. (ICML 2012), 2012, pp. 89\u201396.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast solvers and efficient implementations for distance metric learning", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Proc. 25th Int. Conf. Mach. Learn. (ICML 2008), 2008, pp. 1160\u20131167.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2009, pp. 862\u2013870.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "An efficient sparse metric learning in high-dimensional space via l1penalized log-determinant regularization", "author": ["G.-J. Qi", "J. Tang", "Z.-J. Zha", "T.-S. Chua", "H.-J. Zhang"], "venue": "Proc. 26th Int. Conf. Mach. Learn., 2009, pp. 841\u2013848.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse metric learning via smooth optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2009, pp. 2214\u20132222.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Positive semidefinite metric learning with boosting", "author": ["C. Shen", "J. Kim", "L. Wang", "A. Hengel"], "venue": "Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2009, pp. 1651\u20131659.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaboost on low-rank psd matrices for metric learning", "author": ["J. Bi", "D. Wu", "L. Lu", "M. Liu", "Y. Tao", "M. Wolf"], "venue": "Proc. 2011 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2011), 2011, pp. 2617\u20132624.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "A scalable dual approach to semidefinite metric learning", "author": ["C. Shen", "J. Kim", "L. Wang"], "venue": "Proc. 2011 IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR 2011), 2011, pp. 2601\u20132608.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient dual approach to distance metric learning", "author": ["C. Shen", "J. Kim", "F. Liu", "L. Wang", "A. Hengel"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 25, no. 2, pp. 394\u2013406, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "A robust and efficient doubly regularized metric learning approach", "author": ["M. Liu", "B.C. Vemuri"], "venue": "Proc. 2012 Eur. Conf. Comput. Vis. (ECCV 2012), 2012, pp. 646\u2013659.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Metric learning: A support vector approach", "author": ["N. Nguyen", "Y. Guo"], "venue": "Proc. ECML/PKDD, 2008, pp. 125\u2013136.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}, {"title": "Pairwise support vector machines and their applications to large scale problems", "author": ["C. Brunner", "A. Fischer", "K. Luig", "T. Thies"], "venue": "J. Mach. Learn. Res., vol. 13, pp. 2279\u20132292, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "A metric learning perspective of SVM: on the relation of SVM and LMNN", "author": ["H. Do", "A. Kalousis", "J. Wang", "A. Woznica"], "venue": "arXiv:1201.4714, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "A kernel classification framework for metric learning", "author": ["F. Wang", "W. Zuo", "L. Zhang", "D. Meng", "D. Zhang"], "venue": "to be appear in IEEE Trans. Neural Netw. Learn. Syst.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 0}, {"title": "Information geometry and alternating minimization procedures", "author": ["I. Csiszar", "G. Tusnady"], "venue": "Statistics and decisions, Supplement Issue, vol. 1, pp. 205\u2013237, 1984.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1984}, {"title": "Convergence theorems for generalized alternating minimization procedures", "author": ["A. Gunawardana", "W. Byrne"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 2049\u20132073, 2005.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2005}, {"title": "Generalized sparse metric learning with relative comparisons", "author": ["K. Huang", "Y. Ying", "C. Campbell"], "venue": "Knowledge and Inf. Syst., vol. 28, no. 1, pp. 25\u201345, 2011.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "UCI machine learning repository [http://archive.ics.uci.edu/ml", "author": ["A. Frank", "A. Asuncion"], "venue": "2010.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Demsar"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 1\u201330, 2006.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Univ. of Massachusetts, Tech. Rep., 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "Proc. 2009 IEEE Int. Conf. Comput. Vis. (ICCV 2009), 2009, pp. 365\u2013372.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Comput. Vis., vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2004}, {"title": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "author": ["D. Gray", "H. Tao"], "venue": "Proc. 2008 Eur. Conf. Comput. Vis. (ECCV 2008), 2008, pp. 262\u2013275.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2008}, {"title": "Custom pictorial structures for re-identification", "author": ["D. Cheng", "M. Cristani", "M. Stoppa", "L. Bazzani", "V. Murino"], "venue": "Proc. British Machine Vision Conf., 2011.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical gaussianization for image classification", "author": ["X. Zhou", "N. Cui", "Z. Li", "F. Liang", "T. Huang"], "venue": "Proc. 12th IEEE Int. Conf. Comput. Vis. (ICCV 2009), 2009, pp. 1971\u20131977.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION D ISTANCE metric learning aims to train a valid distance metric which can enlarge the distances between samples of different classes and reduce the distances between samples of the same class [1].", "startOffset": 207, "endOffset": 210}, {"referenceID": 1, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 227, "endOffset": 230}, {"referenceID": 8, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 257, "endOffset": 260}, {"referenceID": 9, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 262, "endOffset": 266}, {"referenceID": 10, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 284, "endOffset": 288}, {"referenceID": 11, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 290, "endOffset": 294}, {"referenceID": 12, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 317, "endOffset": 321}, {"referenceID": 13, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 347, "endOffset": 351}, {"referenceID": 14, "context": "Metric learning is closely related to k-Nearest Neighbor (k-NN) classification [2], clustering [3], ranking [4], [5], feature extraction [6] and support vector machine (SVM) [7], and has been widely applied to face recognition [8], person re-identification [9], [10], image retrieval [11], [12], activity recognition [13], document classification [14], and link prediction [15], etc.", "startOffset": 373, "endOffset": 377}, {"referenceID": 15, "context": "Methods that learn L, including neighborhood components analysis (NCA) [16], large margin components analysis (LMCA) [17] and neighborhood repulsed metric learning (NRML) [18], are mostly formulated as nonconvex optimization problems, which are solved by gradient descent based optimizers.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Methods that learn L, including neighborhood components analysis (NCA) [16], large margin components analysis (LMCA) [17] and neighborhood repulsed metric learning (NRML) [18], are mostly formulated as nonconvex optimization problems, which are solved by gradient descent based optimizers.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Methods that learn L, including neighborhood components analysis (NCA) [16], large margin components analysis (LMCA) [17] and neighborhood repulsed metric learning (NRML) [18], are mostly formulated as nonconvex optimization problems, which are solved by gradient descent based optimizers.", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 288, "endOffset": 292}, {"referenceID": 2, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 313, "endOffset": 316}, {"referenceID": 20, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 332, "endOffset": 336}, {"referenceID": 21, "context": "Taking the PSD constraint into account, methods that learn M, including large margin nearest neighbor (LMNN) [19] and maximally collapsing metric learning (MCML) [20], are mostly formulated as convex semidefinite programming (SDP) problems, which can be optimized by standard SDP solvers [19], projected gradient [3], Boosting-like [21], or Frank-Wolfe [22] algorithms.", "startOffset": 353, "endOffset": 357}, {"referenceID": 22, "context": "[23] proposed an information-theoretic metric learning (ITML) model with an iterative Bregman projection algorithm, which does not need projections onto the PSD cone.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Besides, the use of online solvers for metric learning has been discussed in [9], [24], [25].", "startOffset": 77, "endOffset": 80}, {"referenceID": 23, "context": "Besides, the use of online solvers for metric learning has been discussed in [9], [24], [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Besides, the use of online solvers for metric learning has been discussed in [9], [24], [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "On the other hand, kernel methods [26]\u2013[31] have been widely studied in many learning tasks, e.", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "On the other hand, kernel methods [26]\u2013[31] have been widely studied in many learning tasks, e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "There are many open resources on kernel classification methods, and a variety of toolboxes and libraries have been released [32]\u2013[38].", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "There are many open resources on kernel classification methods, and a variety of toolboxes and libraries have been released [32]\u2013[38].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 195, "endOffset": 199}, {"referenceID": 1, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 238, "endOffset": 241}, {"referenceID": 15, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 276, "endOffset": 280}, {"referenceID": 19, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 319, "endOffset": 323}, {"referenceID": 22, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 366, "endOffset": 370}, {"referenceID": 7, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 414, "endOffset": 417}, {"referenceID": 21, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 464, "endOffset": 468}, {"referenceID": 38, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 528, "endOffset": 532}, {"referenceID": 8, "context": "Abbreviation Full Name PSD Positive semidefinite (matrix) SDP Semidefinite programming k-NN k-nearest neighbor (classification) KKT Karush-Kuhn-Tucker (condition) SVM Support vector machine LMCA [17] Large margin components analysis LMNN [2] Large margin nearest neighbor NCA [16] Neighborhood components analysis MCML [20] Maximally collapsing metric learning ITML [23] Information-theoretic metric learning LDML [8] Logistic discriminant metric learning DML-eig [22] Distance metric learning with eigenvalue optimization PLML [39] Parametric local metric learning KISSME [9] Keep it simple and straightforward metric learning PCML Positive-semidefinite constrained metric learning NCML Nonnegative-coefficient constrained metric learning", "startOffset": 573, "endOffset": 576}, {"referenceID": 32, "context": "The off-the-shelf SVM solvers such as LibSVM [33] can be employed to solve the metric learning problem.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 68, "endOffset": 72}, {"referenceID": 39, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 123, "endOffset": 126}, {"referenceID": 19, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "2 RELATED WORK Compared with nonconvex metric learning models [16], [17], [40], convex formulation of metric learning [2], [3], [20]\u2013[22] has drawn increasing attentions due to its desired properties such as global optimality.", "startOffset": 133, "endOffset": 137}, {"referenceID": 40, "context": "developed an efficient solver based on the sub-gradient descent and the active set techniques [41].", "startOffset": 94, "endOffset": 98}, {"referenceID": 22, "context": "[23] suggested an iterative Bregman projection algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Iterative projected gradient descent method [3], [42] has been widely employed for metric learning but it requires an eigenvalue decomposition in each iteration.", "startOffset": 44, "endOffset": 47}, {"referenceID": 41, "context": "Iterative projected gradient descent method [3], [42] has been widely employed for metric learning but it requires an eigenvalue decomposition in each iteration.", "startOffset": 49, "endOffset": 53}, {"referenceID": 42, "context": "Other algorithms such as block-coordinate descent [43], smooth optimization [44], and Frank-Wolfe [22] have also been studied for metric learning.", "startOffset": 50, "endOffset": 54}, {"referenceID": 43, "context": "Other algorithms such as block-coordinate descent [43], smooth optimization [44], and Frank-Wolfe [22] have also been studied for metric learning.", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "Other algorithms such as block-coordinate descent [43], smooth optimization [44], and Frank-Wolfe [22] have also been studied for metric learning.", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "derived the Lagrange dual of the exponential loss based metric learning model, and proposed a boostinglike approach, namely BoostMetric, where the matrix M is learned as a linear positive combination of rank-one matrices [21], [45].", "startOffset": 221, "endOffset": 225}, {"referenceID": 44, "context": "derived the Lagrange dual of the exponential loss based metric learning model, and proposed a boostinglike approach, namely BoostMetric, where the matrix M is learned as a linear positive combination of rank-one matrices [21], [45].", "startOffset": 227, "endOffset": 231}, {"referenceID": 45, "context": "MetricBoost [46] and FrobMetric [47], [48] were further proposed to improve the performance of BoostMetric.", "startOffset": 12, "endOffset": 16}, {"referenceID": 46, "context": "MetricBoost [46] and FrobMetric [47], [48] were further proposed to improve the performance of BoostMetric.", "startOffset": 32, "endOffset": 36}, {"referenceID": 47, "context": "MetricBoost [46] and FrobMetric [47], [48] were further proposed to improve the performance of BoostMetric.", "startOffset": 38, "endOffset": 42}, {"referenceID": 48, "context": "Liu and Vemuri incorporated two regularization terms in the duality for robust metric learning [49].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 22, "endOffset": 26}, {"referenceID": 44, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 28, "endOffset": 32}, {"referenceID": 45, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 46, "endOffset": 50}, {"referenceID": 46, "context": "Note that BoostMetric [21], [45], MetricBoost [46], and FrobMetric [47] are proposed for metric learning with triplet constraints, whereas in many applications such as verification, only pairwise constraints are available in the training stage.", "startOffset": 67, "endOffset": 71}, {"referenceID": 49, "context": "Several SVM-based metric learning approaches [50]\u2013 [53] have also been proposed.", "startOffset": 45, "endOffset": 49}, {"referenceID": 52, "context": "Several SVM-based metric learning approaches [50]\u2013 [53] have also been proposed.", "startOffset": 51, "endOffset": 55}, {"referenceID": 49, "context": "Using SVM, Nguyen and Guo [50] formulated metric learning as a quadratic semidefinite programming problem, and suggested a projected gradient descent algorithm.", "startOffset": 26, "endOffset": 30}, {"referenceID": 49, "context": "The formulations of the proposed PCML and NCML in this work are different from the model in [50], and they are solved by the dual problems with the off-the-shelf SVM solvers.", "startOffset": 92, "endOffset": 96}, {"referenceID": 50, "context": "[51] proposed a pairwise SVM method to learn a dissimilarity function rather than a distance metric.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Different from [51], the proposed PCML and NCML learn a distance metric and the matrix M is constrained to be a PSD matrix.", "startOffset": 15, "endOffset": 19}, {"referenceID": 51, "context": "[52] studied SVM from a metric learning perspective and presented an improved variant of SVM classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[53] developed a kernel classification framework for metric learning and proposed two learning models which can be efficiently implemented by the standard SVM solvers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "In this work, the proposed PCML and NCML models have different formulations from [53], and their solutions are globally optimal.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "Substituting (8) into (7), the subproblem on \u03bb becomes a kernel-based classification problem, and can be efficiently solved by using the existing SVM solvers such as LibSVM [33].", "startOffset": 173, "endOffset": 177}, {"referenceID": 53, "context": "3 Optimality Condition As shown in [54], [55], the general alternating minimization approach will converge.", "startOffset": 35, "endOffset": 39}, {"referenceID": 54, "context": "3 Optimality Condition As shown in [54], [55], the general alternating minimization approach will converge.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "Similar strategy for constructing pairwise or triplet constraints can be found in [2], [11].", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "Similar strategy for constructing pairwise or triplet constraints can be found in [2], [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 33, "context": "The computational complexity of SMO-type algorithms [34] is O(kNd).", "startOffset": 52, "endOffset": 56}, {"referenceID": 32, "context": ", LibSVM [33].", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "When the SMO-type algorithm [34] is adopted for SVM training, the computational complexity of NCML is O ( kNd )", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "vector data has been recently receiving considerable research interests [5], [56], and NCML can provide a new perspective on this topic.", "startOffset": 72, "endOffset": 75}, {"referenceID": 55, "context": "vector data has been recently receiving considerable research interests [5], [56], and NCML can provide a new perspective on this topic.", "startOffset": 77, "endOffset": 81}, {"referenceID": 34, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 23, "endOffset": 27}, {"referenceID": 35, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 47, "endOffset": 51}, {"referenceID": 37, "context": ", core vector machines [35], LaRank [36], BMRM [37], and Pegasos [38], which can be utilized for large scale metric learning.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Moreover, we can refer to the progresses in kernel methods [26]\u2013[28] for developing semi-supervised, multiple instance, and multitask metric learning approaches.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "Moreover, we can refer to the progresses in kernel methods [26]\u2013[28] for developing semi-supervised, multiple instance, and multitask metric learning approaches.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 7, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 173, "endOffset": 176}, {"referenceID": 38, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 183, "endOffset": 187}, {"referenceID": 21, "context": "We compare PCML and NCML with the baseline Euclidean distance metric and 7 state-of-the-art metric learning models, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22].", "startOffset": 201, "endOffset": 205}, {"referenceID": 56, "context": "1 Results on the UCI Datasets We first use 9 datasets from the UCI Machine Learning Repository [57] to evaluate the proposed models.", "startOffset": 95, "endOffset": 99}, {"referenceID": 57, "context": "The average rank is defined as the mean rank of one method over the nine datasets, which can provide a fair comparison of the learning methods [58].", "startOffset": 143, "endOffset": 147}, {"referenceID": 58, "context": "3 Face Verification In this subsection, we evaluate the proposed methods for face verification using two challenging face databases: Labeled Faces in the Wild (LFW) [59] and Public Figures (PubFig) [60].", "startOffset": 165, "endOffset": 169}, {"referenceID": 59, "context": "3 Face Verification In this subsection, we evaluate the proposed methods for face verification using two challenging face databases: Labeled Faces in the Wild (LFW) [59] and Public Figures (PubFig) [60].", "startOffset": 198, "endOffset": 202}, {"referenceID": 60, "context": "In our experiments, we use the SIFT [61] features and the attribute features provided by [8] and [60] to evaluate the metric learning methods.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "In our experiments, we use the SIFT [61] features and the attribute features provided by [8] and [60] to evaluate the metric learning methods.", "startOffset": 89, "endOffset": 92}, {"referenceID": 59, "context": "In our experiments, we use the SIFT [61] features and the attribute features provided by [8] and [60] to evaluate the metric learning methods.", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "55 DML-eig [22] 81.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "79 ITML [9] 82.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "40 LDML [8] 79.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "08 KISSME [9] 80.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "We report the ROC curves of PCML, NCML, DML-eig [22], ITML [23], KISSME [9], LDML [8] and Euclidean distance in Fig.", "startOffset": 82, "endOffset": 85}, {"referenceID": 59, "context": "2 The PubFig Database The PubFig database [60] contains 58,797 face images of 200 persons with large variations in pose, lighting, expression, scene, camera, imaging conditions and parameters, etc.", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "7: The ROC curves of different metric learning methods on the LFW-funneled dataset under the image restricted setting [8], [9], [22].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "7: The ROC curves of different metric learning methods on the LFW-funneled dataset under the image restricted setting [8], [9], [22].", "startOffset": 123, "endOffset": 126}, {"referenceID": 21, "context": "7: The ROC curves of different metric learning methods on the LFW-funneled dataset under the image restricted setting [8], [9], [22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 59, "context": "We use the attribute features provided by [60] to evaluate the competing methods.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 45, "endOffset": 48}, {"referenceID": 22, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 68, "endOffset": 72}, {"referenceID": 59, "context": "8 shows the ROC curves of PCML, NCML, KISSME [9], ITML [23], DMLeig [22], Attribute Classifiers [60] and the baseline Euclidean distance.", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "38 KISSME [9] 77.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "09 ITML [9] 69.", "startOffset": 8, "endOffset": 11}, {"referenceID": 59, "context": "50 Attribute Classifiers [60] 78.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "65 DML-eig [22] 77.", "startOffset": 11, "endOffset": 15}, {"referenceID": 61, "context": "databases, the Viewpoint Invariant Pedestrian Recognition (VIPeR) database [63] and the Context Aware Vision using Image-based Active Recognition for ReIdentification (CAVIAR4REID) database [64] are used to assess the performance of the proposed methods.", "startOffset": 75, "endOffset": 79}, {"referenceID": 62, "context": "databases, the Viewpoint Invariant Pedestrian Recognition (VIPeR) database [63] and the Context Aware Vision using Image-based Active Recognition for ReIdentification (CAVIAR4REID) database [64] are used to assess the performance of the proposed methods.", "startOffset": 190, "endOffset": 194}, {"referenceID": 8, "context": "05 KISSME [9] 19.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "LMNN [9] 16.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "ITML [9] 15.", "startOffset": 5, "endOffset": 8}, {"referenceID": 21, "context": "10 DML-eig [22] 8.", "startOffset": 11, "endOffset": 15}, {"referenceID": 62, "context": "2 The CAVIAR4REID Database CAVIAR4REID consists of 1,220 pedestrian images from 72 persons, where the images are extracted from the shopping center scenario of the CAVIAR database [64].", "startOffset": 180, "endOffset": 184}, {"referenceID": 63, "context": "Following [65] and [10], we use the hierarchical Gaussian (HG) features to evaluate the metric learning methods.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Following [65] and [10], we use the hierarchical Gaussian (HG) features to evaluate the metric learning methods.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "According to the evaluation protocol in [10], we randomly select 36 persons and use their images for training, and use the rest images for testing.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 61, "endOffset": 64}, {"referenceID": 22, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "We report the CMC curves of PCML, NCML, DML-eig [22], KISSME [9], ITML [23], LMNN [19] and Euclidean TABLE 9: Person re-identification accuracies (%) and training time (s) on the CAVIAR4REID dataset.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "23 DML-eig [22] 30.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "LMNN [9] 28.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "ITML [9] 31.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "18 KISSME [9] 29.", "startOffset": 10, "endOffset": 13}, {"referenceID": 15, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 215, "endOffset": 219}, {"referenceID": 7, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 226, "endOffset": 229}, {"referenceID": 1, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 236, "endOffset": 239}, {"referenceID": 38, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 246, "endOffset": 250}, {"referenceID": 21, "context": "Experimental results on nine UCI machine learning repository datasets and four handwritten digit datasets showed that, compared with the state-of-the-art metric learning methods, including NCA [16], ITML [23], MCML [20], LDML [8], LMNN [2], PLML [39], and DML-eig [22], the proposed PCML and NCML methods can not only achieve higher classification accuracy, but also are much faster in training.", "startOffset": 264, "endOffset": 268}], "year": 2015, "abstractText": "Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while many existing metric learning algorithms become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem, and solve it by iterated training of support vector machines (SVM). The new formulation is easy to implement, efficient in training, and tractable for large-scale problems. Two novel metric learning models, namely Positive-semidefinite Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained Metric Learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experimental results on UCI dataset classification, handwritten digit recognition, face verification and person re-identification demonstrate that the proposed metric learning methods achieve higher classification accuracy than state-of-the-art methods and they are significantly more efficient in training.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}