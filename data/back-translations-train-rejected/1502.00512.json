{"id": "1502.00512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2015", "title": "Scaling Recurrent Neural Network Language Models", "abstract": "This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction.", "histories": [["v1", "Mon, 2 Feb 2015 15:27:37 GMT  (19kb,D)", "http://arxiv.org/abs/1502.00512v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["will williams", "niranjani prasad", "david mrva", "tom ash", "tony robinson"], "accepted": false, "id": "1502.00512"}, "pdf": {"name": "1502.00512.pdf", "metadata": {"source": "CRF", "title": "SCALING RECURRENT NEURAL NETWORK LANGUAGE MODELS", "authors": ["Will Williams", "Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson"], "emails": ["willw@cantabResearch.com", "tonyr@cantabResearch.com"], "sections": [{"heading": null, "text": "Index Terms - recurring neural network, speech modeling, GPU, speech recognition, RNNLM"}, {"heading": "1. INTRODUCTION", "text": "Statistical language models are a critical component of many applications such as automatic speech recognition (ASR), machine translation (MT), and prediction for text input on mobile phones. One such class of models, Recurrent Neural Network Language Models (RNNLMs), provides a rich and powerful way to model sequential voice data. Despite an initial surge of interest in RNNNs in the early 1990s for acoustic modeling [1, 2, 3], the computing costs and storage costs of training large RNLMs proved prohibitive. Since the earliest days of large vocabulary recognition, n-grammar models have been the dominant paradigm. However, recent work by Mikolov [4] on RNLMs has shown that moderately large RNLMs can now be trained and have been shown to be competitive with NNLMs. Mikolov-trained RNLMs with 800 state-based state modelling units have been incorporated into Google's Benchmark [5]."}, {"heading": "2. DATA SETS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Training Data", "text": "The most important training corpus we use in this work is an internally compiled and processed data set. All data dates from before November 2013 and includes approximately 8 billion words, including: 1. [880m] Spoken News: Derived from transcripts of radio and news broadcasts. 2. [1.7 billion] Wikipedia: Copy of Wikipedia articles. 3. [6.1 billion] Written Messages: Derived from a web crawl of many popular news sites. We used a rolling buffer of a minimum size of 140 characters divided into sentences to deduplicate the corpora. We used several hash functions to efficiently store past commands and excluded a negligible portion of text due to false positives. We then inserted the corpus through a typical ASR normalization and tokenization pipeline - the numbers were expanded into sequences of digits, most punctuation characters were removed, lowercase letters were cast into punctuation marker."}, {"heading": "2.2. Test Data", "text": "Unless otherwise stated, all of our tests were performed on a standard test benchmark: TED test data IWSLT14.SLT.tst20101. We chose this out-of-domain test to eliminate any possibility of text overlapping between training and test kits, and also due to the availability of a publicly available KALDI reception2."}, {"heading": "2.3. Entropy Filtering", "text": "Typically, sentences are used as the natural choice for segments of this filter style. However, for the composite corpus, we wanted to keep the context between sentences in favor of RNNLM training, so we did not want to filter our corpus on a sentence-by-sentence basis. Instead, we implemented a rolling buffer solution: a score of difference between sentences was calculated using rolling batches of 16 sentences. If a sentence was ever part of a rolling buffer using 1http: / / hltshare.fbk.eu / IWSLT2014 / IWSLT14.SLT. tst2010.en-fr.en.tgz2Kaldi revision 4084, recipe: http: / / sourceforge.net / p / kaldi / code / IWSLT / tree / trunk / egs / tedlium / s5 / Xiv: 150 2.5 etc., with the difference determined below the threshold of 150C."}, {"heading": "3. SCALING PROPERTIES OF N-GRAMS", "text": "n-gram language models have maintained their dominance in statistical language modeling largely due to their simplicity and simplicity of calculation. Although n-gram can be modeled and queried relatively quickly, they show decreasing gains as they build on more data.n-gram can be calculated more easily than RNNLMs on larger datasets, but as the extrapolation in Figure 1 shows, any order of magnitude of increase in training data over 1012 words leads to a reduction in perplexity of less than 6%. The largest current text corpora such as Google n-grams4 and commonCrawl [8] are about 1012 orders of magnitude. The asymptote of an exponential curve corresponding to Figure 1 is approximately perplexity 73; these values represent a hard limit on the performance of 5 grams in this testset. n-gram also scale very poorly in terms of memory."}, {"heading": "4. RNNLMS ON GPUS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Background", "text": "Recent attempts to build very large networks (in the order of billions of parameters), however, require many CPU cores to work together [5, 10]. High-end Graphics Processing Units (GPUs) are a viable alternative, as they are both affordable and able to achieve extremely high computing throughput. GPUs have therefore been one of the key elements in the resurgence of neural networks over the last decade. RNLMs are publicly available for the highly optimized GPU-based libraries due to their predominant dependence on linear algebra operations (such as matrix multiplications). Unlike n-gram models, which simply count the occurrence of certain word combinations, RNNNs learn a distributed hidden representation for each context, from which a probability distribution across all words in a given vocabulary is possible."}, {"heading": "4.2. Implementation", "text": "s highly optimized linear algebra library cuBLAS to perform SGEMM calls. Where this was not possible, we wrote and optimized our own custom CUDA cores. We use floats everywhere; the precision that doubles the supply is unnecessary to learn good visualizations. We package our input data using a data offset scheme that indexes the input corpus at a number of different points called noffset. This includes managing noffsets by minibatch size of various hidden states during the training. Typically, we use noffset 128, similar to [16]. Empirically, we found a minibatch size of 9-perplexity, which represents a good trade-off between memory."}, {"heading": "4.3. Analysis", "text": "Figure 3 shows that while we are scaling nstate, we are observing an almost linear reduction in log perplexity with log training words. Given that n-grams scale very poorly in terms of their order, it is clear that on a fixed-size dataset RNNs scale much better in terms of model size. For network sizes above nstate 1024, our implementations on a Nvidia GeForce GTX Titan give 100x accelerations against the baseline single core Mikolov implementation5 on a 3.4 GHz Intel i7 CPU. Despite significantly improved training times on the GPU, our larger RNNs take in the order of days to train instead of hours that require n-grams. However, more computing power will favor RNNNNs, as larger nstate RNNNs use the additional calculation to scale the model size with nstate - which may provide slightly less performance."}, {"heading": "5. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Language Modelling", "text": "We present our RNNLM results on the recently released billion-word modeling benchmark [5]. Vocabulary for this task is very large (770K) - so we train the RNNs on a much smaller 64k vocabulary and interpolate it with a full 5g format to fill in rare word probabilities. RNN confusion in Table 1 is interpolated with the KN-5 gram. Nstate 4096 RNN is larger in state size than those in [5] and5http: / / rnnlm.org / we achieve better perplexity (42.4) with this single model than the combination of interpolated models in [5], while using only 3% of the total parameters."}, {"heading": "5.2. ASR", "text": "We evaluated the RNNLMs by rescording grids on three different systems, using the IWSLT14.SLT.tst2010 data both with and without the supplied segmentation; the \"IWSLT\" system uses the Kaldi TEDLIUM [20] recipe for acoustic models and speech models based on [5] 6; the nstate 2048 models overtrain on the small entropy-filtered corpus; the \"en-US\" system uses the same framework, but with the RNNLMs from Figure 3 and internal acoustic models; and the \"SM\" system also uses the RNLMs from Figure 3, but within the commercial service we provide on speechmatics.com."}, {"heading": "5.3. Machine Translation", "text": "In our MT experiments, we used the WFST grating from the WMT13 EN-RU system developed at Cambridge University Engineering Department [21]. The usual BLEU score in statistical machine translation was 32.34 with our base model n-gram and increased to 33.45 with an nstate 3520 RNNLM (greater is better for BLEU scores).The improvement of 1 BLEU point above the baseline shows the benefit of RNNLMs not only in ASR, but also in statistical MT systems. 6The 12.8% IWSLT result is a competitive baseline that complies with the IWSLT rules, uses freely redistributable sources and can be reproduced from the Kaldi TEDLIUM s5 recipe and http: / / cantabResearch / cantab-TEDLIUM.tar.bz2."}, {"heading": "5.4. Word prediction", "text": "Word prediction involves predicting the next word in a string, where performance is determined by the percentage times the target word is predicted in the top 3 words (known as the \"hit rate\").For this task, we train using 100M words from the BNC corpus [22] using a 10K vocabulary size and the RNN using a shortlist of 100 candidate words generated from a heavily truncated 2MB n-gram. For our task of word prediction on mobile phones, we have tight memory constraints and therefore choose nstate 1024 as the appropriate RNN size. We compress the RNN by inserting layers of 512 units above and below the hidden state and then train it end-to-end. Additionally, we bind the input and state output weights while transposing and quantifying all weights. At 10MB, the RN achieves a relative gain of 5 grams over the RN showing the limited benefit in RN 17 grams."}, {"heading": "6. CONCLUSION", "text": "We have shown that large RNNLMs can be efficiently trained on GPUs by exploiting data parallelism and minimizing the number of additional parameters required during training. Such RNNs reduce helplessness about standard benchmarks by over 40% compared to 5 grams, while using only a fraction of the parameters. We believe that RNNNs today offer less helplessness than 5 grams for a lot of training data. Furthermore, we have shown that state-of-the-art ASR systems with Kaldi and high-end GPUs can be trained by resorbing them with an RNNLM. Although RNLMs are bound to both computing power and GPU memory, they are currently comfortably n-gram ahead. We believe that future developments in computing power and storage capacity will further benefit them."}, {"heading": "7. REFERENCES", "text": "[1] Anthony J. Robinson, \"An application of recurrent nets to phone probability estimation,\" Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 298-305, 1994. [2] Ronald J. Williams, \"Complexity of exact gradient computation networks on, vol. 45, no. 11, pp. 2673-2681, 1997. [3] Mike Schuster and Kuldip K. Paliwal,\" Bidirectional recurrent neural networks, \"Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp. 2681-2681, 1997. [4] Toma. [s.] Mikolov, statistical language models based on neural networks, Ph.D. Levno University of Technology, 2012. [5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson,\" One Billion Word Benchmark for Progress Measuring Language in Klakow."}], "references": [{"title": "An application of recurrent nets to phone probability estimation", "author": ["Anthony J. Robinson"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 298\u2013305, 1994.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Complexity of exact gradient computation algorithms for recurrent neural networks", "author": ["Ronald J. Williams"], "venue": "Tech. Rep., 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Statistical language models based on neural networks, Ph.D", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": "thesis, Brno University of Technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "One Billion Word Benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": ".", "citeRegEx": "5", "shortCiteRegEx": null, "year": 0}, {"title": "Selecting articles from the language model training corpus", "author": ["Dietrich Klakow"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. IEEE, 2000, vol. 3, pp. 1695\u20131698.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Intelligent selection of language model training data", "author": ["Robert C. Moore", "William Lewis"], "venue": "Proceedings of the ACL 2010 Conference Short Papers. Association for Computational Linguistics, 2010, pp. 220\u2013224.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen"], "venue": "LREC, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "KenLM: Faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, 2011, pp. 187\u2013197.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg S. Corrado", "Jeff Dean", "Andrew Y. Ng"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8595\u20138598.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), 2010, pp. 1045\u2013 1048.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Advances in optimizing recurrent networks", "author": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8624\u20138628.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Tomas Mikolov", "Stefan Kombrink", "Anoop Deoras", "Lukar Burget", "J Cernocky"], "venue": ".", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating recurrent neural network training via two stage classes and parallelization", "author": ["Zhiheng Huang", "Geoffrey Zweig", "Michael Levit", "Benoit Dumoulin", "Barlas Oguz", "Shawn Chang"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 326\u2013331.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch", "author": ["Xie Chen", "Yongqiang Wang", "Xunying Liu", "Mark JF Gales", "Philip C Woodland"], "venue": "Proc. ISCA Interspeech, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Large scale recurrent neural network on gpu", "author": ["Boxun Li", "Erjin Zhou", "Bo Huang", "Jiayi Duan", "Yu Wang", "Ningyi Xu", "Jiaxing Zhang", "Huazhong Yang"], "venue": "Neural Networks (IJCNN), 2014 International Joint Conference on. IEEE, 2014, pp. 4062\u20134069.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2265\u20132273.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "TED-LIUM: an Automatic Speech Recognition dedicated corpus", "author": ["Anthony Rousseau", "Paul Del\u00e9glise", "Yannick Est\u00e8ve"], "venue": "LREC, 2012, pp. 125\u2013129.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "The University of Cambridge Russian-English system at WMT13", "author": ["J. Pino", "A. Waite", "T. Xiao", "A. de Gispert", "F. Flego", "W. Byrne"], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, 2013, pp. 198\u2013 203.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "The British National Corpus, version 3 (BNC XML Edition), distributed by Oxford University Computing Services on behalf of the BNC Consortium", "author": ["BNC Consortium"], "venue": "2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Despite an initial flurry of interest in RNNs in the early \u201990s for acoustic modelling [1, 2, 3], the computational cost and memory overheads of training large RNNs proved prohibitive.", "startOffset": 87, "endOffset": 96}, {"referenceID": 1, "context": "Despite an initial flurry of interest in RNNs in the early \u201990s for acoustic modelling [1, 2, 3], the computational cost and memory overheads of training large RNNs proved prohibitive.", "startOffset": 87, "endOffset": 96}, {"referenceID": 2, "context": "Despite an initial flurry of interest in RNNs in the early \u201990s for acoustic modelling [1, 2, 3], the computational cost and memory overheads of training large RNNs proved prohibitive.", "startOffset": 87, "endOffset": 96}, {"referenceID": 3, "context": "However, recent work by Mikolov [4] on RNNLMs has shown that modestly sized RNNLMs can now be trained and have been shown to be competitive with ngrams.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Mikolov trained RNNLMs with 800 hidden state units; Google\u2019s language modelling benchmark [5] subsequently established baseline RNNLM results with 1024 hidden state units.", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "To filter our corpus we used cross-entropy difference scoring [6, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 6, "context": "To filter our corpus we used cross-entropy difference scoring [6, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 7, "context": "The largest current text corpora such as Google n-grams4 and CommonCrawl [8] are about size 10.", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "At 8bn words the KN 5-gram already takes up 362GB in ARPA format and 69GB in KenLM [9] binary trie format - already impractically large for current commercial ASR.", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Recent attempts to train very large networks (on the order of billions of parameters) have required many CPU cores to work in concert [5, 10].", "startOffset": 134, "endOffset": 141}, {"referenceID": 9, "context": "Recent attempts to train very large networks (on the order of billions of parameters) have required many CPU cores to work in concert [5, 10].", "startOffset": 134, "endOffset": 141}, {"referenceID": 10, "context": "We use a standard RNN architecture [11] but dispense with bias units to maximise efficiency on the GPU and because, in our experience, they do not provide any practical benefit.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "Recent work has improved our understanding of how to effectively train RNNs [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 13, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 14, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 15, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 16, "context": "We believe our setup from 2013 in this paper gives better speedups than previously reported elsewhere [13, 14, 15, 16, 17].", "startOffset": 102, "endOffset": 122}, {"referenceID": 17, "context": "To achieve high throughput and utilisation on GPUs we train a standard RNN with stochastic gradient descent and rmsprop[18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "Typically we use noffset 128, similar to [16].", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "We train using Noise Contrastive Estimation [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "heldout-00000-of00050\u2019) from [5].", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Language Modelling We present our RNNLM results on the recently released billion word language modelling benchmark[5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "The nstate 4096 RNN is larger in state size than those in [5] and", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "4) with that one single model than the combination of interpolated models in [5] whilst also using only 3% of the total parameters.", "startOffset": 77, "endOffset": 80}, {"referenceID": 19, "context": "The \u2018IWSLT\u2019 system uses the Kaldi TEDLIUM [20] recipe for acoustic models and language models built on [5]6.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "The \u2018IWSLT\u2019 system uses the Kaldi TEDLIUM [20] recipe for acoustic models and language models built on [5]6.", "startOffset": 103, "endOffset": 106}, {"referenceID": 20, "context": "In our MT experiments we rescored the WFST lattices from the WMT13 EN-RU system developed at Cambridge University Engineering Department [21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "For this task we train on 100M words from the BNC corpus [22] using a vocabulary size of 10K and the RNN using a shortlist of 100 candidate words generated from a heavily pruned 2 MB n-gram.", "startOffset": 57, "endOffset": 61}], "year": 2015, "abstractText": "This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction.", "creator": "LaTeX with hyperref package"}}}