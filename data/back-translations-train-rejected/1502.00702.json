{"id": "1502.00702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2015", "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks", "abstract": "In this paper, we propose a universal model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modelling framework. The HOPE model itself can be learned unsupervisedly from un-labelled data based on the maximum likelihood estimation as well as trained discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each NN hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, it also provides several new learning algorithms to learn NNs either supervisedly or unsupervisedly. In this work, we have investigated the HOPE framework in learning NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results show that the HOPE framework yields significant performance gains over the current state-of-the-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning.", "histories": [["v1", "Tue, 3 Feb 2015 01:38:19 GMT  (954kb,D)", "http://arxiv.org/abs/1502.00702v1", "29 pages, 4 Figures, technical report"], ["v2", "Sat, 6 Jun 2015 01:57:42 GMT  (1172kb,D)", "http://arxiv.org/abs/1502.00702v2", "31 pages, 5 Figures, technical report"]], "COMMENTS": "29 pages, 4 Figures, technical report", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["shiliang zhang", "hui jiang"], "accepted": false, "id": "1502.00702"}, "pdf": {"name": "1502.00702.pdf", "metadata": {"source": "CRF", "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Probe and Learn Neural Networks", "authors": ["Shiliang Zhang", "Hui Jiang"], "emails": ["zsl2008@mail.ustc.edu.cn,", "hj@cse.yorku.ca"], "sections": [{"heading": null, "text": "In this paper, we propose a universal model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixing model under a unified generative modeling framework. HOPE model itself can be learned unsupervised from unlabeled data based on maximum probability estimation and trained discriminatively on labeled data. Interestingly, we have shown that the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE model can be used as a novel tool to investigate why and how NNNs work, and more importantly, it also provides several new learning algorithms to either monitor or unsupervised NNs. In this work, we have examined the standard framework for multiple image tasks, including the identification of significant MT and NT issues."}, {"heading": "1 Introduction", "text": "In fact, it is that most people are able to understand themselves and to understand how they have behaved. (...) It is not that they are able to identify themselves. \"(...)\" It is not that they are able to identify themselves. \"(...)\" It is that they are not able to identify themselves. \"(...)\" It is not that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...)\" It is that they do it. \"(...\" It is. \"(...)\" It is. \"(...\" It is. \"It is.\" (...) \"It is.\" It is. \"(...\" It is. \"It.\" It. \"It is.\" (... \"It is.\" It. \"It.\" It. \"It is. (...\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It. (...\" It. \"It.\" It. \"It.\" It. (It. \")\" It is. (It. (It. \")\" It. (It is. (It. (It. \"It.)\" It. (It. (It. \"It.)\" It is. (It. \"It. (It.). (It. (It. (It.)\" It is. (It.) \"It is. (It. (It. (It.) It is. (It.\" It. (It.) \"It is. (It. (It.)\" It is. (It. (It.) It. (It. (It. (It.) It is. (It is. (It.) It is. (It is. (It. (It.) It is. (It. (It.). (It is. (It. (It.). (It. (It.) It is. (It is."}, {"heading": "2 Related Work", "text": "Among many, PCA is the most popular technique in this category. PCA is defined as the orthogonal projection of high-dimensional data onto a low-dimensional linear space known as principal subspace, so that the variance of the projected data is maximized [6]. The nice property of PCA is that it can be formulated as the eigenvector problem of the data covariance matrix, where a simple solution exists in closed form. Furthermore, PCA can be expressed as the maximum probability solution of a probabilistic latent variable model. In this case, if the projected data assumes that it follows a mediocre unit-covariance-Gaussian distribution in principal subspace, the probabilistic PCA [24,26] can be expressed as a linear model."}, {"heading": "3 Hybrid Orthogonal Projection and Estimation (HOPE)", "text": "Consider a standard PCA setting in which each data sample is represented as a high-dimensional vector x with dimensionality D. Our goal is to learn a linear projection that is represented as a matrix U to map each data sample to a space with dimensionality M < D. Our proposed HOPE model is essentially a generative model in nature, but can also be considered a generalization to expand the probable PCA in [26] to take into account a complex data distribution that needs to be modeled by a finite mixing model in the latent feature space. This setting differs greatly from that in [25], where the original data x is modeled by mixing models in the original higher-dimensional raw data space."}, {"heading": "3.1 HOPE: combining generalized PCA with generative model", "text": "In fact, it is as if it were a way, as it has developed in recent years. (...) In fact, it is as if it were a way, as it has developed in recent years. (...) In fact, it is as if it were a way, as it has developed in recent years. (...) It is as if it were a way, as it has developed in recent years. (...) In fact, it is as if it were a way, as it has developed in recent years. (...) It is as if it were a way. (...) \"It is as if it were a way.\" (...) \"It is as if it were a way.\" (...) \"It is as if it were a way.\" (...) It is as if it were a way, and as if it were a way. (...) It is a way. (...) It is a way. (...) And it is a way. (...) It is a way. (...) It is a way."}, {"heading": "4 Unsupervised Learning of HOPE Models", "text": "It is obvious that the HOPE model is essentially a generative model that combines extraction and data modeling, and thus its model parameters, including the project matrix and the blending model, can be derived on the basis of maximum probability (ML). In this case, some iterative optimization algorithms, such as the stochastic gradient descent (SGD) [7,8], are applicable to maximize both the projection matrix U and the underlying blending model as a whole. In this section, we assume that the projection matrices, not only U, but also the entire U, are all orthonormal."}, {"heading": "4.2 Dealing with the noise model term L2", "text": "The protocol probability function in connection with the sound model, L2 (U, \u03c3), can be expressed as follows: L2 (U, \u03c3) = \u2212 N (D \u2212 M) 2 ln (\u03c32) \u2212 1 2\u03c32 N \u2211 n = 1 nTnnn. (19) And we have: nTnnn = (xn \u2212 UT zn) T (xn \u2212 UT zn) = (xn \u2212 UTUxn) T (xn \u2212 UTUxn) = xTnxn \u2212 2xTnUTUxn + xTnUTUxn (20) Therefore, L2 (U, \u03c3) = \u2212 N (D \u2212 M) 2 ln (\u03c32) \u2212 1 2\u03c32 N \u0445 n \u0445 n = 1 [xTnxTnUTUxn) noise (UTUxn \u2212 as a consequence of which derivation can be derived from the derivative form."}, {"heading": "4.3 Computing L1 for GMMs", "text": "In this section we will consider the calculation of partial derivatives of L1 (U) for the sub-components of GMP. Let us assume that each mini-batch X consists of a small subset of randomly selected training samples, whereby the log probability function of HOPE models can be represented with GMMs as follows: L1 (U, U) = N-ray n = 1 ln [K-ray k = 1 \u03c0k \u00b7 N (Uxn | \u00b5k, \u0441k)] (24) The partial derivative of L1 (U) w.r.t The mean vector, \u00b5k-Gaussian component can be calculated as follows: X-ray L1 (U, \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043dannicnicnynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynynyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy"}, {"heading": "4.4 Computing L1 for movMFs", "text": "Similarly, we derive all partial derivatives from L1 (U) \u00b7 k \u00b7 j \u00b7 k (U) for mixtures of vMF (movMFs). In this case, the log probability function of the HOPE model with movMFs can be expressed as follows: L1 (U, \u0432) = N \u00b2 n = 1 ln [K \u00b2 n = 1 \u03c0k \u00b7 CM (| \u00b5k |) \u00b7 ezn \u00b7 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "4.5 The SGD-based Learning Algorithm", "text": "Since all mixing weights, \u03c0k (k = 1, \u00b7 \u00b7 K) and all line vectors, ui (i = 1, \u00b7 \u00b7, M) of the projection matrix meet the constraints, \u03c0k and ui must be normalized during the SGD learning process after each update as follows: \u03c0k \u2190 \u03c0k \u0445 j \u03c0k (35) ui \u2190 ui | ui |. (36) Finally, we summarize the SGD algorithm to learn the HOPE models based on the maximum probability criterion (ML) in algorithm 1."}, {"heading": "5 Learning Neural Networks as HOPE", "text": "As described above, the HOPE model can be used as a universal model for high-dimensional data. Furthermore, the HOPE model itself can be learned efficiently and unsupervised from unlabeled databases based on the above-mentioned highest probability criterion. If data labels are available, a variety of discriminatory training methods, such as those in [15,16], can be used to learn the HOPE model based on other discriminatory learning criterias.What is more interesting is that, as we will explain here, there is a strong relationship between the HOPE models and neural networks (NNs). First, we will show that the HOPE models can be used as a new tool to investigate the mechanism why NNs work so well in practice. Under the new HOPE framework, we can explain why NNs can be characterized almost universally by a variety of normal data types, and how NNNs that treat zero-machine models as other highly correlated learning models can be fairly high-dimensional for many data."}, {"heading": "5.1 Linking HOPE to Neural Networks", "text": "The second level represents the underlying model of the underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying, underlying,"}, {"heading": "5.2 Unsupervised Learning of Neural Networks as HOPE", "text": "The method of maximum probability estimation for HOPE in Section 4 can be used to learn neural networks layer by layer in an unattended learning mode. All parameters of the HOPE model in Figure 1 (a) are first merged on the basis of the maximum probability criterion as in Section 4. Next, the two layers of the HOPE model are merged into a regular NN hidden layer, as in Figure 1 (b). In this case, class markers are not required to learn all network weights and neural networks in a theoretically solid framework, taking into account unmarked data. This is similar to Hebrew-style learning [23], but has a sound and convergent objective function in learning. Next, the reflected log probability values from the HOPE model, i.e., \u03b7k (1 \u2264 k \u2264 K), can be considered as a sensory map in the latent feature space, which can serve as a good representation of the original characteristic."}, {"heading": "5.3 Supervised Learning of Neural Networks as HOPE", "text": "The HOPE framework can also be applied to the supervised learning of neural networks when data labels are available. Let's take rectified linear (ReLU) neural networks as an example. Each hidden layer in a ReLU neural network, as shown in Figure 1 (b), is considered a HOPE model and can therefore be dissected as a combination of a projection layer and a model layer, as shown in Figure 1 (a). In this case, M must be selected correctly to avoid overfitting. In other words, each hidden layer in ReLU-Ns is presented as two layers during learning, namely a linear projection layer and a nonlinear model layer. If data labels are available, we can use the default minimal cross-entropy error criterion instead of the maximum probability criterion."}, {"heading": "5.4 HOPE for Deep Learning", "text": "In Figure 2 (a), a HOPE model can be used as the first layer primarily for feature extraction, and a deep neural network is concatenated above it as a powerful classifier to form a deep structure. In Figure 2 (a), the deep model can be learned either in supervised or semi-unsupervised mode. In semi-supervised learning, the HOPE model is learned based on the maximum probability estimate, and the upper deep NN is learned supervised. Alternatively, if we have enough labeled data, we can jointly learn both HOPE and DNN in supervised mode. In Figure 2 (b), we can even stack several HOPE models on top of each other to form another deep model structure. In this case, we can add each individual layer in HOPE mode to a normal HOPE card."}, {"heading": "6 Experiments", "text": "In this section, we will examine the proposed HOPE framework for learning neural networks for various standard image and speech recognition tasks under different learning conditions: i) unattended feature learning; ii) supervised learning; iii) semi-supervised learning. The tasks studied include image recognition tasks using the MNIST dataset and the speech recognition task using the TIMIT dataset."}, {"heading": "6.1 MNIST: Image Recognition", "text": "The MNIST dataset [19] consists of 28 x 28 pixel grayscale images of the handwritten digits 0-9 with 60,000 training examples and 10,000 test examples. In our experiments, we first evaluate the performance of unattended feature learning using the HOPE model with movable MFs. Second, we investigate the performance of supervised learning of DNNs within the HOPE framework and further investigate the effects of orthogonal constraint within the HOPE framework. Finally, we consider a semi-supervised learning scenario using the HOPE models, in which all training samples (without labels) are used to learn the presentation of features unsupervised, and then a portion of the training data (along with labels) is used to supervise gradual classification models."}, {"heading": "6.1.1 Unsupervised Feature Learning on MNIST", "text": "In this experiment, we first found many small patches from the original blank screen resolution (MNIST). (Each patch is 6 x 6 in dimension, represented as a vector in RD, with D = 36. In this thesis, we will randomly extract 400,000 patches in total from the MNIST training set for unattended learning algorithms. In addition, each patch is standardized by subtracting the mean and dividing it by the standard deviation of its elements. In the unattended learning situation, we follow the same experimental setting in [9], where an unattended learning algorithm is used to learn a \"black box\" to map each input vector in RD. In this thesis, we have examined various unattended learning algorithms for feature learning: (i) kmeans clustering; (ii) spherical kmeans (spherical kmeans) clustering."}, {"heading": "6.1.2 Supervised Learning of Neural Networks as HOPE on MNIST", "text": "In this experiment, we use the MNIST data to examine the supervised learning of rectified linear (ReLU) networks in the context of HOPE, as described in Section 5.3.Here, we follow the normalized initialization to align all NN weights without using the prior education. We assume a small modification of the method by adding a factor to control the dynamic range of the initial weights, as follows: \"It is about activating ReLU units,\" \"It is about identifying the number of units in the i-th layer.\" \"It is about the number of units in the i-th layer that could grow due to the unrestricted behavior of the activation functions of the ReLU units without us having to handle this numerical problem, we shrink the dynamic range of the initial weights using a small factor.\""}, {"heading": "6.2 Semi-supervised Learning on MNIST", "text": "In this experiment, we combine unattended feature learning with supervised model learning and examine classification performance when limited labeled data is available. Here, we also list the results using Convolutionary Deep Arbitrary Networks (CDBN) in [20] as the base system. In our experiments, we use the raw pixel characteristics and unattended learned (USL) characteristics in Section 6.1.1. As an example, we select the unattended learned characteristics from the HOPE-movMF model (K = 800) in Table 15. Next, we link a post-stage classifier that is supervised using only a portion of the training data from 1000 to 60000 (all). We look at many different types of classifiers, including linear SVM, regular DNNs, and HOPE-trained DNNs. Note that all classifiers are trained separately from feature learning. All results are summarized in Table 5."}, {"heading": "6.3 TIMIT: Speech Recognition", "text": "In this case, it is just one of many who are able to play by the rules, \"he said in an interview with the Deutsche Presse-Agentur.\" We have to play by the rules, \"he said,\" but we have to play by the rules we have set ourselves. \""}, {"heading": "7 Final Remarks", "text": "In this paper, we have proposed a universal model called hybrid orthogonal projection and estimation (HOPE) for high-dimensional data. HOPE model combines feature extraction and data modeling under a unified generative modeling framework, so that both feature extractor and data model can be jointly monitored or learned unattended. Interestingly, the HOPE models are so closely linked to neural networks that each hidden layer in NNNs can be reformulated as a HOPE model. Therefore, the proposed HOPE-related learning algorithms can be easily applied to perform either supervised or unsupervised learning for neural networks. We have evaluated the proposed HOPE models in learning NNNNs on several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have strongly supported that the HOPE models can provide a very net-effective, unattended learning framework for large-scale NHOPE."}, {"heading": "A Learning HOPE when U\u0302 is not orthonormal", "text": "In some cases, it is possible that we use the projection matrix U (= K = 11) to ensure signal projection in all dimensions, which is very important for many image and speech recognition tasks. In this case, we can impose orthogonal delimitations between all line vectors of U, i.e., ui, ui, uj = 0 (i, j = 1, \u00b7 \u00b7 M, i = j), but these lines cannot be of unit length, i.e., ui = 1, \u00b7 \u00b7, M). In addition, it is better not to amplify residual noise in the remaining D \u2212 M dimensions."}, {"heading": "B Derivatives of movMFs", "text": "The partial derivatives of the objective function in eq. (29) w.r.t all \u00b5k functions (k function) can be calculated as follows: mps L1 (U function).z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z"}, {"heading": "C Numerical Methods for Iv(\u00b7)", "text": "In the learning algorithm for moving MFs, we have to calculate the Bessel functions, Iv (\u00b7), in several places. First, when calculating the probability function of a vMF distribution, as in q. (9), we have to calculate the normalization term CM (\u0445). Second, we have to calculate the rations of the modified Bessel functions, Ad (\u0445) = IM / 2 (\u0445) IM / 2 \u2212 1 (\u0445), in eq. (55). As we know, the modified Bessel functions of the first type take the following form: Id (\u0445) = Bessel k \u2265 (d + 1 + k) k!"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we propose a universal model for high-dimensional data, called the Hybrid Orthogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modelling framework. The HOPE model itself can be learned unsupervisedly from un-labelled data based on the maximum likelihood estimation as well as trained discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, it also provides several new learning algorithms to learn NNs either supervisedly or unsupervisedly. In this work, we have investigated the HOPE framework in learning NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results show that the HOPE framework yields significant performance gains over the current stateof-the-art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning.", "creator": "LaTeX with hyperref package"}}}