{"id": "1502.01057", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2015", "title": "Personalized Web Search", "abstract": "Personalization is important for search engines to improve user experience. Most of the existing work do pure feature engineering and extract a lot of session-style features and then train a ranking model. Here we proposed a novel way to model both long term and short term user behavior using Multi-armed bandit algorithm. Our algorithm can generalize session information across users well, and as an Explore-Exploit style algorithm, it can generalize to new urls and new users well. Experiments show that our algorithm can improve performance over the default ranking and outperforms several popular Multi-armed bandit algorithms.", "histories": [["v1", "Tue, 3 Feb 2015 22:37:37 GMT  (112kb,D)", "http://arxiv.org/abs/1502.01057v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["li zhou"], "accepted": false, "id": "1502.01057"}, "pdf": {"name": "1502.01057.pdf", "metadata": {"source": "CRF", "title": "Personalized Web Search", "authors": ["Li Zhou"], "emails": ["lizhou@cs.cmu.edu"], "sections": [{"heading": null, "text": "Categories and topic descriptions H.4 [Information system applications]: Miscellaneous Terms and Conditions ApplicationKeywords Personalization, Web search, Multi-armed bandit, Hiddensemi-Markov model"}, {"heading": "1. BACKGROUND OF THIS IRLAB", "text": "However, my IRLab project started in the last semester. In the beginning, I wanted to do personalization on the web search because it is a hot topic in both the academic and the industrial world. I also found a personalization dataset that was published as part of the WSDM 2014 workshop. I implemented this method in the summer. However, it turned out that it did not work well: the average number of requests in each session period is about 3, so the data is really sparse; on the other hand, the Hidden Semi Markov model has a lot of parameters. As a result, the model I have revised the data. I have tried several techniques to fix this, such as reducing the number of parameters and holding only sessions that contain more than 5 questions. However, this problem is my problem that my personalization is not armed."}, {"heading": "2. INTRODUCTION", "text": "In fact, most of them are able to play by the rules that they have set themselves, and they are able to determine for themselves what they have to do."}, {"heading": "3. RELATED WORK", "text": "Personalization is a very popular topic. There have been a number of workshops on personalization [3] [1]. Most of the work focuses on the following issues: a) When should we do personalization and when not? b) How can we model long-term user behavior? c) How can we model short-term user behavior?"}, {"heading": "3.1 To Personalize or not to Personalize", "text": "Personalization does not guarantee good results. [25] and [26] showed that there are big differences between searches in terms of the benefits that can be achieved through personalization, with some searches everyone who makes the search looks for the same thing. In other searches, different people want very different results, although they express their needs in the same way. They characterize searches based on a variety of characteristics of the search query, the results that are issued for the search query, and the interaction history of people with the search query. Using these characteristics, they create predictive models to identify searches that can benefit from personalization. Experiments suggested that personalization strategies should not be applied to popular searches or searches with very similar clicking behavior between different people. [11] Likewise, the click drop was defined to measure the variation of users \"information needs under a search query. And experimental results showed that personalized web searches yielded significant improvements over high-end searches."}, {"heading": "3.2 Long Term User Behavior Modeling", "text": "Long-term behavioral modeling focuses on how to build a profile for each user based on historical data and then reclassify search results according to the user profile. The main problem is how to present and learn each user profile. Typically, there are two ways, the first is to present the user profile based on history queries, URLs clicked, and many other features. The second method is to classify users \"interests into a range of topics. [19] Build a Firefox add-on and capture features such as Unigram, Noun Phrase, and so on. Each term is rated by TF, TF-IDF, and BM25. These terms and ratings are treated as a user profile. Then, they use simple score features such as internal product and language models to calculate the similarity of the user profile and the Url snipet, and rearrange the search results further. [Build a user profile for each user's language model]"}, {"heading": "3.3 Short Term User Behavior Modeling", "text": "Short-term behavioral modeling focuses on the conclusion of a user's current intention based on behavior in the current session. [7] proposed a context-sensitive approach to query suggestions that takes place in two steps: in the offline step, queries are bundled by a clickable two-sided suffix tree, and a concept sequence tree for query suggestions is constructed as a suffix tree; in the online step, a user's search context is captured by mapping the query sequence submitted by the user to a suffix tree. [29] They examine the effectiveness of activity-based contexts. Features include queries, SERP clicks, post-SERP navigation. They assign URLs to ODP category labels and call it query model, they also build context models that occur in the search session before the current query. Finally, they build purpose models that represent the weighted average and context models."}, {"heading": "3.4 Existing Multi-armed Bandit Approach to Personalization", "text": "In the personalization of messages, we assume that there are a number of candidate articles, and each time we select only one of them to the user, and according to the user's feedback, we try to minimize confidence in our model, which in our case is defined as a reward for the best article and not clicking in our case. Here, we are talking about two algorithms that will be our basic algorithm: 3.4.1 LinUCB [16] LinUCB tries to minimize regret, which is defined as a reward for the best article, and the expectation of the reward of the article selected by the algorithm: R (T) = E [T) = D \u00b2 = 1 rt, a \u00b2 -UCB [T = 1 rt, at] (1) The reward in our case is the reward of the best article and the expectation of the reward of the article selected by the algorithm."}, {"heading": "4. ABANDONED PROPOSED METHOD", "text": "This is the first algorithm I came up with for short-term personalization. Search sessions record the behavior of the user within a short period of time. Session segmentation is usually based on time span or similarities between queries [13]. Within a session, we assume that there is finally a and perhaps more hidden intention. For example, during a session, a user may want to act in his \"IPhone 3GS,\" so he first searches for \"IPhone Trade-in\" and then for \"IPhone 3GS Trade-in Bestbuy Price.\" Another example is that a user may want to have a nice dinner with his friends in Mount Washington, Pittsburgh, he may first search for \"Mount Washington Restaurant\" and then for \"Seafood Mount Washington\" and then for \"IPhone 3GS Trade-in Bestbuy Price.\" Another example is that a user may want to have a nice dinner with his friends in Mount Washington, Pittsburgh, he may first search for \"St Mount Washington\" and then search for \"Seafood Mount Washington,\" then for \"IPhone Trade-in Bestbuy Price.\""}, {"heading": "4.1 Inference", "text": "Similar to the Hidden Markov model, we use the forward-backward algorithm to derive the MLE of the state sequence. Forward variables for HSMM are defined by \u03b2 (t, d) = P (St \u2212 d + t: t = j, o1: t | \u03bb) (8) and backward variables for HSMM by \u03b2 (t, d) = P (Ot + 1: T | S [t \u2212 d + 1: t] = j, \u03bb) (9), so that the forward-backward algorithm for HSMM [32] [21] is: \u03b1t (j, d) = [21]: Byt (j, d) = p\\ {j}."}, {"heading": "4.2 Estimation", "text": "We can re-evaluate based on expectations: a) (i, d) (j, d) = (i, d) t (i, d) / \u2211 j 6 = i, d (i, d) t (i, d) (16) b) j, d (o1: od) = \u2211 t [\u03b7t (j, d) I (ot + 1: t + d = O1: d)] / \u2211 t \u03b7t (j, d) (17) To reduce the number of parameters, we used the query cluster instead of the query unigram as an observation variable. We trained a ranking model for each query cluster offline. And then we use the HSMM model to predict the query cluster for the next query and then use the corresponding ranking model to re-evaluate the search results."}, {"heading": "4.3 Why Abandon This Method", "text": "I implemented this method over the summer. First, based on an open source project [2] from JHU, I wrote my own HSMM model code. However, I found that I am too optimistic for this model. First, most sessions have only one or two queries, so it is not enough to estimate the HSMM model. Then, I only maintained those sessions that contain more than 5 queries that slightly solved this problem. Second, it is difficult to estimate the stop state of the HSMM. However, general HSMM applications such as those in language or NLP do not have as much search space, so that we can conclude from sufficient data the probability that the current sequence will be stopped and forwarded to another hidden variable after remaining in state i for some time. In our case, however, users can quit because they are satisfied with the results, or because they simply give up and leave."}, {"heading": "5. PROPOSED METHOD", "text": "I suggested to use the multi-armed bandit algorithm for web search for personalization. As the multi-armed bandit selects one article after another, we focus on the results from rank 1 in the search results list and try to improve the click rate of the article from rank 1."}, {"heading": "5.1 Short term behavior modeling", "text": "So for each url clicked, we have a set of associated terms. Then, each session is represented by the query terms associated with all clicks. We will repeat the session with the same intention for the next two years. (For example, if people want to find a place to trade on their old computer, they can derive all search queries such as \"trade-in-bestbuy\" or \"trade-in-computer\" etc., which session that session is, this can be repeated by a generalized sampling session [15]. Algorithm has the following levels 5.1.1 step in the training data, for each session, we find the urls clicked on. Then we build a hash table: the key is the clicked url, and the value is the query term. So for each url clicked, we have a set of associated query terms."}, {"heading": "5.2 Long term behavior modeling", "text": "We still assume that there are 10 standard search results, and try to select one that we believe to be the user with the highest probability of clicking and placing it in the first position; the probability of being clicked on by a user is modeled as a linear regression with a Gaussian error; suppose that the probability of a reward ri (t) at the given time t (t) and the parameter u were given by the pdf of the Gaussian distribution N (bi (t) T\u00b5, v2) and LetB (t) = Id + t \u2212 1 Predicting a reward ri (t) in the given context bi (t) and parameter u; the pdf of the Gaussian distribution N (bi (t) T\u00b5, v2) and LetB (t) \u2212 1 prediction."}, {"heading": "6. DATASET", "text": "We use the data provided by Yandex during the WSDM 2014 Log-based Personalization Workshop [3]. The data set includes user sessions extracted from Yandex logs with user IDs, queries, query items, URLs, their domains, URLs and clicks. User data is fully anonymized. Only meaningless numerical IDs of users are released. Some features of the data set: 1. Unique queries: 21,073,5692. Unique Urls: 703,484,263. Unique Users: 5,736,3334. Training sessions: 34,573,6305. Test sessions: 797,8676. Clicks in the training data: 64,693,0547. Total number of records in the log: 167,413,039. The time span for the training data set is 27 days, and the time span for the test data is 3 days. However, we do not have the basic truth."}, {"heading": "7. EXPERIMENT SETTING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Metric", "text": "URLs are marked by 3 degrees of relevance: 0 (irrelevant), 1 (relevant), 2 (highly relevant). The marking is based on dwell time: 0 (irrelevant) corresponds to documents without clicks and clicks with dwell time of less than 50 units of time; 1 (relevant) corresponds to documents with clicks and dwell time of between 50 and 399 units of time (including); 2 (highly relevant) corresponds to documents with clicks and dwell time of no less than 400 units of time. In addition, relevance level 2 corresponds to the documents associated with the clicks that are the last actions in the corresponding sessions. Dwell, dwell time is the time elapsed between the click on the document and the next click or query. It is well known that dwell time correlates with the probability that the user satisfies his information needs with the document clicked on. Since we use a multi-arm bandit algorithm, we are only interested in the position of the CAP search result of the CAP rank 1 of the search results."}, {"heading": "7.2 Feature construction", "text": "I use the following 18 characteristics, which are described in Table 1. Note that our goal is not the feature engineering, so I did not spend much time developing new functions. These characteristics try to capture the url-related click statistics under a session, one user and all users. These characteristics are used by RankSVM models for both short-term and long-term modeling. Table 1: characteristics that are used by both long-term and short-term users, model an id fea name fea description 1 short-term relevance level 2 in this session, the number of times that url is used for both short-term and long-term modeling. Table 1: characteristics that are used by both long-term and short-term users, model an id fea name fea description 1 short-term relevance level 2 in this session, the number of times that url is displayed at level 2 short-term relevance level 1 in this session, the number of times that url is published at this session, the number of times at this level 1 relevance level 1 relevance of the user 3 short-term relevance level fea description 1 short-term relevance level 2 in this session, the number of times that url is displayed at this session 0 relevance level 0 relevance level 4 short-term show in this session l, the number of times that url is displayed at this session 1 short-term relevance level 1 in this session, the relevance level 1 in this session, the number of times that url is published at this level 1 in this level, the relevance level 1 in this level 1 relevance level 1 relevance level 1 relevance of the user 3 short-term relevance level 3 short-term relevance level fea description 1, the user 3 short-term relevance level 0 in this session 1 is displayed at this session 0 relevance level."}, {"heading": "7.3 Compared Algorithms", "text": "Note that our algorithm is a kind of Explore exploit algorithm, so it can be used above any monitored learning algorithm such as RankSVM, LambdaMART or RankNet. Therefore, here we focus on comparing our algorithm with other ExploreExploit algorithms: 7.3.1 Standard Ranking This is the default ranking provided by Yandex, which is a very high baseline, since it is a commercial search engine. NDCG of the standard ranking is 0.794379.7.3.2 LinUCB [16] This is one of the most popular UCB algorithms and has been treated as the basis for many essays. This algorithm has been described in the background area. Each time, the algorithm selects the articles that have the highest upper confidence limit. 7.3.3 Thompson Sampling [8] This is an algorithm used by Yahoo! News Personalization. It is a very efficient algorithm and has proven to be less durable than our general amplification algorithm based on B.4."}, {"heading": "8. EXPERIMENT RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Results on Short term session data", "text": "For short-term session data, we first collect all sessions in training data and then use Generalized Thompson Sampling to derive from which session group the current session is in, and then apply the corresponding ranking model. The result is Figure 2. From Figure 2, we can see that if the number of clusters is 7, Generalized Thompson Sampling has achieved the best CTR at No. 1, this is 3.4% higher than the standard ranking and 2.01% higher than LinUCB. We can also see that if the number of clusters is 1, Generalized Thompson Sampling is still better than the standard ranking, which means that the RankSVM model itself can help improve the CTR to No. 1. Figure 3 shows the performance of Generalized Thompson Sampling when a different percentage of training data is used. Because we perform session clustering in our proposed algorithm and need to build a ranking model on each of the clusters so that we can have more data, higher CR and higher TR."}, {"heading": "8.2 Results on Long term session data", "text": "For long-term modeling of user behavior, our proposed model uses Thompson sampling with linear payouts, and the advantage is that it can examine the CTR of URLs in the long tail, while the traditional monitored learning algorithm for long tail URLs has insufficient data. First, we show that our algorithm is indeed sensitive to the learning rate, which is a common problem for all online learning algorithms. Figure 4 shows that the algorithm reaches the highest CTR when the learning rate is \u03b1 = 2. Figure 5 shows that our proposed long-term and short-term modeling algorithm can improve the CTR of position 1 of the standard rankings. Also, note that long-term modeling can improve the CTR by 4.08%, which is higher than short-term modeling. Thus, we can see that long-term modeling is more efficient than short-term modeling, if we can combine short-term modeling with 4.59%."}, {"heading": "9. CONCLUSION", "text": "We proposed a novel method of modelling both long-term and short-term user behavior using the multi-arm bandit algorithm. In the short term, we first cluster based on session data and then use generalized thompson sampling to determine which session cluster the current session is in; in the long term, we use thompson sampling with linear payout. Experiments show that our algorithm can efficiently improve performance over standard ranking, outperforming LinUCB, a popular algorithm for multi-arm bandits."}, {"heading": "10. REFERENCES", "text": "[1] NIPS 2014 personalization workshop. https: / / sites.google.com / site / nips2014long information /. [2] Semi-markov sequence modeler. https: / / github.com / danielenglish / sm2. [3] WSCD personalization workshop. http: / / research. microsoft.com / en-us / um / people / nickcr / wscd2014 /. [4] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. arXiv preprint arXiv: 1209.3352, 2012. [5] P. N. Bennett, K. Svore, and S. T. Dumais. Classification-enhanced ranking. In Proceedings of the 19th international conference on world wide web, pages 111-120. ACM, 2010. [6] P. N. Bennett, R. W. White, W. Chu, S. T. Dumais."}], "references": [{"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "arXiv preprint arXiv:1209.3352,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification-enhanced ranking", "author": ["P.N. Bennett", "K. Svore", "S.T. Dumais"], "venue": "Proceedings of the 19th international conference on World wide web, pages 111\u2013120. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling the impact of short-and long-term behavior on search personalization", "author": ["P.N. Bennett", "R.W. White", "W. Chu", "S.T. Dumais", "P. Bailey", "F. Borisyuk", "X. Cui"], "venue": "Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 185\u2013194. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-aware query suggestion by mining click-through and session data", "author": ["H. Cao", "D. Jiang", "J. Pei", "Q. He", "Z. Liao", "E. Chen", "H. Li"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 875\u2013883. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in Neural Information Processing Systems, pages 2249\u20132257,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Using odp metadata to personalize search", "author": ["P.A. Chirita", "W. Nejdl", "R. Paiu", "C. Kohlsch\u00fctter"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178\u2013185. ACM,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Personalizing web search results by reading level", "author": ["K. Collins-Thompson", "P.N. Bennett", "R.W. White", "S. de la Chica", "D. Sontag"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Evaluating  the effectiveness of personalized web search", "author": ["Z. Dou", "R. Song", "J.-R. Wen", "X. Yuan"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 21(8):1178\u20131190,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Personalizing atypical web search sessions", "author": ["C. Eickhoff", "K. Collins-Thompson", "P.N. Bennett", "S. Dumais"], "venue": "Proceedings of the sixth ACM international conference on Web search and data mining, pages 285\u2013294. ACM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey on session detection methods in query logs and a proposal for future evaluation", "author": ["D. Gayo-Avello"], "venue": "Information Sciences, 179(12):1822\u20131843,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering temporal hidden contexts in web sessions for user trail prediction", "author": ["J. Kiseleva", "H. Thanh Lam", "M. Pechenizkiy", "T. Calders"], "venue": "Proceedings of the 22nd international conference on World Wide Web companion, pages 1067\u20131074. International World Wide Web Conferences Steering Committee,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized thompson sampling for contextual bandits", "author": ["L. Li"], "venue": "arXiv preprint arXiv:1310.7163,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "A vlhmm approach to context-aware search", "author": ["Z. Liao", "D. Jiang", "J. Pei", "Y. Huang", "E. Chen", "H. Cao", "H. Li"], "venue": "ACM Transactions on the Web (TWEB), 7(4):22,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Interest-based personalized search", "author": ["Z. Ma", "G. Pant", "O.R.L. Sheng"], "venue": "ACM Transactions on Information Systems (TOIS), 25(1):5,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Personalizing web search using long term browsing history", "author": ["N. Matthijs", "F. Radlinski"], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, pages 25\u201334. ACM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic identification of user interest for personalized search", "author": ["F. Qiu", "J. Cho"], "venue": "Proceedings of the 15th international conference on World Wide Web, pages 727\u2013736. ACM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE, 77(2):257\u2013286,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Context-sensitive information retrieval using implicit feedback", "author": ["X. Shen", "B. Tan", "C. Zhai"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 43\u201350. ACM,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic models for personalizing web search", "author": ["D. Sontag", "K. Collins-Thompson", "P.N. Bennett", "R.W. White", "S. Dumais", "B. Billerbeck"], "venue": "Proceedings of the fifth ACM international conference on Web search and data mining, pages 433\u2013442. ACM,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining long-term search history to improve search accuracy", "author": ["B. Tan", "X. Shen", "C. Zhai"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 718\u2013723. ACM,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Potential for personalization", "author": ["J. Teevan", "S.T. Dumais", "E. Horvitz"], "venue": "ACM Transactions on Computer-Human Interaction (TOCHI), 17(1):4,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "To personalize or not to personalize: modeling queries with variation in user intent", "author": ["J. Teevan", "S.T. Dumais", "D.J. Liebling"], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 163\u2013170. ACM,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Understanding and predicting personal navigation", "author": ["J. Teevan", "D.J. Liebling", "G. Ravichandran Geetha"], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, pages 85\u201394. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Personalization of web-search using short-term browsing context", "author": ["Y. Ustinovskiy", "P. Serdyukov"], "venue": "Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 1979\u20131988. ACM,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting short-term interests using activity-based search context", "author": ["R.W. White", "P.N. Bennett", "S.T. Dumais"], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1009\u20131018. ACM,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Enhancing personalized search by mining and modeling task behavior", "author": ["R.W. White", "W. Chu", "A. Hassan", "X. He", "Y. Song", "H. Wang"], "venue": "Proceedings of the 22nd international conference on World Wide Web, pages 1411\u20131420. International World Wide Web Conferences Steering Committee,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Context-aware ranking in web search", "author": ["B. Xiang", "D. Jiang", "J. Pei", "X. Sun", "E. Chen", "H. Li"], "venue": "Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 451\u2013458. ACM,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Hidden semi-markov models", "author": ["S.-Z. Yu"], "venue": "Artificial Intelligence, 174(2):215\u2013243,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Basically personalization techniques put a search in the context of the user\u2019s interests, and consider two cases [6]: the long term ar X iv :1 50 2.", "startOffset": 113, "endOffset": 116}, {"referenceID": 24, "context": "Search session is a series of intent-related user\u2019s requests to the search engine[28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "Studies[7] [22] has been shown that a user tends to refine the queries or explore related information about his or her search intent in a session.", "startOffset": 7, "endOffset": 10}, {"referenceID": 18, "context": "Studies[7] [22] has been shown that a user tends to refine the queries or explore related information about his or her search intent in a session.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "Most of the current work[19] [24] make use of the click data and try to infer user\u2019s interests.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "Most of the current work[19] [24] make use of the click data and try to infer user\u2019s interests.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "[25] and [26] showed that there is a lot of variation across queries in the benefits that can be achieved through personalization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] and [26] showed that there is a lot of variation across queries in the benefits that can be achieved through personalization.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "Similarly, [11] defined the click entropy to measure variation in information needs of users under a query.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "[19] build a Firefox add-on and capture features such as unigram, noun phrase etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[23] build a language model for each user as the user profile.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27] analyze the query logs by looking at navigation behavior across all users.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24] explored how to exploit longterm search history, which consists of past queries, result documents and clickthrough, as useful search context that can improve retrieval performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[18] map user interests to categories in the ODP taxonomy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[9] do re-rank based on the distance between a user profile defined using ODP topics and the sets of ODP topics covered by each URL returned in regular web search.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[5] [12] classify urls into 1 of the 219 topical categories from the top two levels of the ODP.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[5] [12] classify urls into 1 of the 219 topical categories from the top two levels of the ODP.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "[20] learn a user topic perference vec-", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[7] proposed a context-aware query suggestion approach which is in two steps: in the offline step, queries are clustered by a clickthrough bipartite, and a concept sequence suffix tree is constructed as the query suggestion model; in the online step, a user\u2019s search context is captured by mapping the query sequence submitted by the user to a sequence suffix tree.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "[29] investigate the effectiveness of activity-based context.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[28] [31] defined a set of features such as the cosine and jaccard distance between the search results and the query.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] [31] defined a set of features such as the cosine and jaccard distance between the search results and the query.", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "[17] adopted vlHMM to model session behavior, where each query in the session has a latent intention conditioned on the former n query, also states in the vlHMM are represented as feature vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[30] mine historic search-engine logs to find other users performing similar tasks to the current user and leverage their on-task behavior to identify web pages to promote in the current ranking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[14] decomposed web sessions into nonoverlapping segments and learned the temporal context for each segment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Finally, [6] and [6] make use of both long term and short term context to re-rank search results.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "Finally, [6] and [6] make use of both long term and short term context to re-rank search results.", "startOffset": 17, "endOffset": 20}, {"referenceID": 12, "context": "1 LinUCB[16] LinUCB try to minimize the regret which is defined as the expectation of reward of best article and the expectation of reward of article selected by the algorithm:", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "2 Thompson Sampling Thompson sampling [8] is a very old heuristic algorithm yet recently received a lot of attention.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "Normally the prior and posterior distribution are all Gaussian distribution [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "Session segmentation normally based on time span or similarities between queries [13].", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "Hidden-semi markov model[32] is a perfect model for modeling such sequential transition.", "startOffset": 24, "endOffset": 28}, {"referenceID": 28, "context": "\u03b2(t, d) = P (Ot+1:T |S[t\u2212d+1:t] = j, \u03bb) (9) so the forward-backward algorithm for HSMM is [32][21]:", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "\u03b2(t, d) = P (Ot+1:T |S[t\u2212d+1:t] = j, \u03bb) (9) so the forward-backward algorithm for HSMM is [32][21]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "When predicting online, we need to infer which session group this session is in, this can be done by a Generalized Thompson sampling [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "To do that, we adopt Generalized Thompson sampling [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "2 Long term behavior modeling We use thompson sampling with linear payoff [4] to model long term user behavior.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "It is well-known that dwell time is well correlated with the probability of the user to satisfy her information need with the clicked document [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "2 LinUCB [16] This is one of the most popular UCB style algorithm and have been treated as baseline for many papers.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "3 Thompson Sampling[8] This is an algorithm that has been used by Yahoo! news personalization.", "startOffset": 19, "endOffset": 22}], "year": 2015, "abstractText": "Personalization is important for search engines to improve user experience. Most of the existing work do pure feature engineering and extract a lot of session-style features and then train a ranking model. Here we proposed a novel way to model both long term and short term user behavior using Multi-armed bandit algorithm. Our algorithm can generalize session information across users well, and as an ExploreExploit style algorithm, it can generalize to new urls and new users well. Experiments show that our algorithm can improve performance over the default ranking and outperforms several popular Multi-armed bandit algorithms.", "creator": "LaTeX with hyperref package"}}}