{"id": "1502.01446", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2015", "title": "Beyond Word-based Language Model in Statistical Machine Translation", "abstract": "Language model is one of the most important modules in statistical machine translation and currently the word-based language model dominants this community. However, many translation models (e.g. phrase-based models) generate the target language sentences by rendering and compositing the phrases rather than the words. Thus, it is much more reasonable to model dependency between phrases, but few research work succeed in solving this problem. In this paper, we tackle this problem by designing a novel phrase-based language model which attempts to solve three key sub-problems: 1, how to define a phrase in language model; 2, how to determine the phrase boundary in the large-scale monolingual data in order to enlarge the training set; 3, how to alleviate the data sparsity problem due to the huge vocabulary size of phrases. By carefully handling these issues, the extensive experiments on Chinese-to-English translation show that our phrase-based language model can significantly improve the translation quality by up to +1.47 absolute BLEU score.", "histories": [["v1", "Thu, 5 Feb 2015 07:42:18 GMT  (161kb,D)", "http://arxiv.org/abs/1502.01446v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiajun zhang", "shujie liu", "mu li", "ming zhou", "chengqing zong"], "accepted": false, "id": "1502.01446"}, "pdf": {"name": "1502.01446.pdf", "metadata": {"source": "CRF", "title": "Beyond Word-based Language Model in Statistical Machine Translation", "authors": ["Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong"], "emails": ["jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn", "shujliu@microsoft.com", "muli@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": "Introduction", "text": "In fact, we are able to put ourselves in a situation in which we are able, in which we are able, in which we are able to assert ourselves, in which we are able, in which we are able, and in which we are able to achieve our objectives."}, {"heading": "Phrase-based Language Model", "text": "For the translation models that generate the output by rendering and composition of the target language sequence = millions of people, it does not matter whether the translation is achieved using the left-oriented algorithm (Koehn et al. 2007) or the bottom-up approach (Xiong, Liu and Lin 2006), the phrase-based language model is suggested to measure whether one output phrase sequence is grammatically more correct than another. In view of a partial translation candidate in the form of phrase sequence t = tp0tpn, the phrase-language model attempts to calculate the following probability: P (t) = P (tp0tpn) tpn... tpn (tpn) p (tpn = tp0) p (tpn = tp0), the phrase-language attempts to sequence-based the phrase-based sequence-sequence-sequence-sequence-sequence-based sequence-sequence-sequence-sequence-sequence-candidate-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-based-sequence-sequence-sequence-sequence-sequence-sequence-in-sequence-sequence-sequence-1-sequence-sequence-sequence-sequence-sequence-sequence-sequence-candidate-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-based-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-1-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-in-sequence-sequence-sequence-sequence-sequence-sequence-1-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequence-sequ"}, {"heading": "Definition of Minimal Phrase", "text": "\"It's the first time that we're able to hide,\" he said. \"It's the first time that we're able to hide,\" he said. \"It's the second time that we're able to get angry.\" \"It's the second time that we're able to get angry,\" he said. \"The second time that we're able to get angry,\" he said. \"The second time that we're able to get angry,\" he said. \"The second time that we're able to do it is.\""}, {"heading": "Identifying Minimal Phrases In Monolingual Data", "text": "We know from the previous section that identifying the minimum phrases is trivial given the verbatim parallel sentence pair. Of course, the corresponding unique partition for the target sentence can be used to estimate the parameters of the language model. However, bilingual resources are always limited, so the target part of the text is too limited to learn a powerful phrase-based language model. The first problem is to partition the monolingual sentences into the sequence of the minimum phrases. For traditional word-based language models, the target language can be used monolingual data immediately after preprocessing, but for our language model based on minimum phrases, the first problem is to divide the monolingual sentences into the sequence of the minimum phrases. Without source-language sentences and word alignment, identifying the minimum phrases is a difficult problem. Considering each target language monolingual sentence t1... tn, our goal is to find the best partition of the minimum phrases, tj + tti.. 1.."}, {"heading": "Parameter Estimation with Deep Learning", "text": "Given the extensive training data of minimal phrase distributions, we are able to train a language model based on phrases. Following the previous conventions, we apply a Markov model of order N-1 to calculate the probability of a minimum phrase sequence: P (t) = P (mp0mp1... mpn) \u0432\u0438\u0441\u0442ni = 0P (mpi | mpi \u2212 N + 1... mpi \u2212 1) (4) The standard number-based probability models, such as Kneser-Ney back off (Kneser and Ney 1995), are used to estimate the probability of a word against the preceding N1 words. It can also be used to estimate the probability of a minimum phrase versus previous minimal phrases N-1. However, this MP-KN model may encounter the problem of data sparseness because the granularity of minimum phrases is greater than the words in 2003 and the sequence of minimum phrases is less likely to treat it as a KN model of minimum complexes than the MP of this year."}, {"heading": "Neural Network Structure", "text": "Our neural network structure is a feed-forward neural network and is almost identical to that described in (Vaswani et al. 2013). As Figure 4 shows, the input vector is the concatenation of N-1 minimal phrase context vectors, in which each minimal phrase is mapped to a 128-dimensional vector using a common embedding matrix. By using two 256-dimensional hidden layers with reflected linear activation function (\u03c6 (x) = max (0, x))) (Gutmann and Hyvrinen 2010), we apply Softmax function in the output layer to calculate the probability of each minimal phrase in the vocabulary: P (x) = exp (dmp (x)))) Z (x) = \u2211 mp \u2032 V exp (x) exp (x))))) 5) mp3 (mp3) mp3 is the minimum mp3 (3) mp3 (3) mp3 the observed."}, {"heading": "Neural Network Training", "text": "Typically, the neural network is optimized to maximize the logging probability of training data. However, the Softmax layer must sum up all the minimal vocabulary phrases for each forward calculation, and it is too time consuming. In recent years, there has been progress in the development of self-normalized neural networks. Vaswani et al. (2013) adopt the Noisy Contrastive Estimation (NCE) (Gutmann and Hyvaurinen 2010) to avoid normalization in the output layer. In contrast, Devlin et al. (2014) explicitly add a limitation to the normalization term (logZ (x) = 0) in the objective function. In our work, we apply the NCE approach. The main idea behind NCE is that we select x k sounds for each training sample and train the network to classify the examples as logical."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Translation System", "text": "We have implemented a phrase-based translation system with a maximum entropy-based reordering model using bracket transduction grammatics (Wu 1997; Xiong, Liu and Lin 2006; Zhang and Zong 2013). In this translation system, the phrasal translation rule A \u2192 (x, y) converts a source-language phrase into a target-language phrase and forms a block. Monotonous merging rule A \u2192 [Al, Ar] combines the two consecutive blocks into a larger block by stringing two partial target translation candidates in sequence, while the swap rule A \u2192 < Al, Ar > reverses the two partial target candidates. Obviously, the phrase-based language model could play an important role in whether the result translation (phrase order) is fluid or not."}, {"heading": "Data Preparation", "text": "The bilingual training data from the LDC contain approximately 2.06 million sentence pairs with 27.7 million Chinese words and 31.9 million English words. The statistical significance test is carried out using the pair-by-pair re-sampling approach (Koehn 2004). As the language model is the focus of this work, we examine four language models. The target-side raw data comprises the English part of the bilingual data and the monolingual Xinhua portion of the English gigaword. The total target language data contains approximately 300 million words. Four different language models are detailed as follows: \u2022 W-KN: It is the conventional word-based language model Kooz-Basic Model. \u2022 W-NN: It is the word-based language model Kooz Basic Model."}, {"heading": "Experimental Results on Minimal Phrase Partition", "text": "Before detailing the translation performance, we first report on the performance of the minimal phrase partition in the large format monolingual data. We perform the word alignment for the Chinese and English reference sentences on NIST MT03. Based on the limitations of the word alignment, we obtain the English minimum phrase partitions used as gold test data. We apply the trained Perceptron algorithm to partition the English reference set of NIST MT03. We compare this result with the gold test data. Accuracy, recall, and F1 score are 0.83, 0.873, and 0.851, respectively. It shows that the performance of the minimum phrase partition for the monolingual data is quite good, and the minimum phrase partitions on the large format monolingual data are very reliable to use for the formation of the phrase-based language model."}, {"heading": "Experimental Results on Translation Quality", "text": "In order to have a comprehensive understanding of how different language models affect translation quality, we perform eight language model settings and compare them. Table 2 reports on the detailed results.For the first four lines, we use only one language model in the translation system. We plan to find out the following two questions: 1, can the neural language model replace the Kneser-Ney-based language model? 2, can the phrase-based language model replace the word-based language model in SMT? Comparing W-NN with W-KN in Table 2, we can find that the application of only the word-based neural language model does not work as well as the word-based Kneser-Ney language model. The similar phenomenon occurs for the word-based language model (MP-KN vs. MPNN). In theory, DNN can alleviate the problem of data scarcity that the word model of the language does not replace the strict vocabulary size models of the language (mularsize model)."}, {"heading": "Language Model MT05 MT06 MT08", "text": "We also calculate the perplexity of the four language models based on the English references of the test sentences. As shown in parentheses of Table 2, the perplexity has little relation to the translation quality. We can see that the phrase model of the neural language has a lower perplexity, but for the word levels the neural language model has a higher perplexity. We believe that they are not comparable because their basic units are different. Although the phrase model of the linguistic phrase levels, the neural language model has a lower perplexity, the neural language model has a higher perplexity. We will investigate this phenomenon further in our future work. Although the phrase-based language model cannot surpass the word-based language model when used independently, both of them should be indispensable in measuring the quality of the translation output. To prove this, we integrate multilingual models into the translation system."}, {"heading": "Related Work", "text": "In statistical translation, little has been done to design a phrase-threshing system. Many researchers are trying to go beyond the wordbased language model and expand the translation system with syntax-based language models. Charniak, Knight, and Yamada (2003) are designing a CFG-based syntax model for translation reranking. Shen, Xu, and Weischedel (2008) propose a dependency model for the hierarchical phrase-threshing system. Post and Gildea (2009), Zhu, and Zhu, and Zhang, and Zhao, and Zhao (2013) proclaim a dependency on the language based on the phrase."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we have presented a novel phrase-based language model for statistical machine translation. First, we have given the definition of the minimum phrases, then, in order to make full use of the extensive monolingual data for phrase-based language model training, we have developed a sequence markup algorithm to divide the monolingual data into minimal phrases, and finally, we have designed a deep neural network to better learn the parameters of the phrase-based language model. Extensive experiments on the translation from Chinese to English have shown that the proposed phrase-based language model significantly improves translation performance. In the future, we plan to explore our phrase-based language model in two directions: first, we will continue to address the shortage problem of minimal phrases, and second, we will integrate our phrase-based language model into other translation models, such as the MTU-based translation model and hierarchical translation model."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Language model is one of the most important modules in statistical machine translation and currently the wordbased language model dominants this community. However, many translation models (e.g. phrase-based models) generate the target language sentences by rendering and compositing the phrases rather than the words. Thus, it is much more reasonable to model dependency between phrases, but few research work succeed in solving this problem. In this paper, we tackle this problem by designing a novel phrase-based language model which attempts to solve three key sub-problems: 1, how to define a phrase in language model; 2, how to determine the phrase boundary in the large-scale monolingual data in order to enlarge the training set; 3, how to alleviate the data sparsity problem due to the huge vocabulary size of phrases. By carefully handling these issues, the extensive experiments on Chinese-to-English translation show that our phrase-based language model can significantly improve the translation quality by up to +1.47 absolute BLEU score.", "creator": "LaTeX with hyperref package"}}}