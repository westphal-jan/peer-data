{"id": "1502.01827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2015", "title": "Hierarchical Maximum-Margin Clustering", "abstract": "We present a hierarchical maximum-margin clustering method for unsupervised data analysis. Our method extends beyond flat maximum-margin clustering, and performs clustering recursively in a top-down manner. We propose an effective greedy splitting criteria for selecting which cluster to split next, and employ regularizers that enforce feature sharing/competition for capturing data semantics. Experimental results obtained on four standard datasets show that our method outperforms flat and hierarchical clustering baselines, while forming clean and semantically meaningful cluster hierarchies.", "histories": [["v1", "Fri, 6 Feb 2015 08:37:55 GMT  (412kb,D)", "http://arxiv.org/abs/1502.01827v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["guang-tong zhou", "sung ju hwang", "mark schmidt", "leonid sigal", "greg mori"], "accepted": false, "id": "1502.01827"}, "pdf": {"name": "1502.01827.pdf", "metadata": {"source": "META", "title": "Hierarchical Maximum-Margin Clustering", "authors": ["Guang-Tong Zhou", "Sung Ju Hwang", "Mark Schmidt"], "emails": ["GZA11@CS.SFU.CA", "SJHWANG@UNIST.AC.KR", "SCHMIDTM@CS.UBC.CA", "LSIGAL@DISNEYRESEARCH.COM", "MORI@CS.SFU.CA"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Related Work", "text": "It is a question of whether and to what extent people are able to survive themselves, and the question of how they should behave. (...) It is a question of how they should behave. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which they are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves."}, {"heading": "3. Hierarchical Maximum-Margin Clustering", "text": "We have a hierarchical clustering method based on the maximum margin, and we want to find the groups of data points with a large separation between them, while we focus on the different levels of the hierarchy to focus on the use of different characteristics, and (ii) we build the hierarchy iteratively from the coarse clusters to the fine-grained clusters (rather than forming all the clusters in a split). We first present the HMMC formulation in this section, and then describe the optimization method in Sec. 4.Suppose There are no empty nodes in the learned hierarchy."}, {"heading": "4. Optimization", "text": "The goal of Equation (1) is not convex because of the unknown hierarchical structure, and because we do not know the distribution between each node that jointly optimizes w and y. To solve the problem, we propose a greedy top-down algorithm to build the hierarchy (paragraph 4.1) and an alternating descent algorithm to divide a node (paragraph 4.2)."}, {"heading": "4.1. Building the Hierarchy", "text": "We build the cluster hierarchy in a top-down manner, where the challenge is to split the next leaf. (Algorithm 1 gives an overview of our greedy method.) We start with the root node n1, which contains all the data. (Note: n1 starts as a leaf node, because it has no children. (4) Each iteration tries to split the data to each leaf node (Step 4), and we define the splitting score (Step 5) as: S (nt) = \"xi\" Dt w > tyixi G (wt) + E (wt). (4) The splitting score measures how well and how easily the data can be clustered across the node. (4) Summarizes the scores of each instance to their assigned models."}, {"heading": "4.2. Splitting A Node", "text": "The accumulation of labor in a particular group is more regulating than ever before. (wt) We are not able to solve a problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem. (wt) We are not able to solve this problem."}, {"heading": "5. Experiments", "text": "In fact, it is the case that most people who have chosen politics in recent years are able to outdo themselves, \"he told the German Press Agency.\" I don't think people are able to outlive themselves, \"he said.\" I don't think they are able to outlive themselves. \"He added,\" I don't think they are able to outlive themselves. \"He added,\" I don't think they are able to outlive themselves. \"He added,\" I don't think they are able to outlive themselves and that they are able to outlive themselves. \""}, {"heading": "5.1. Results", "text": "The results are shown in Table 1, which shows that they perform best in the region, 6% in the region and 4% in the region."}, {"heading": "6. Conclusion", "text": "We have presented a hierarchical clustering method for the unattended construction of taxonomies. We develop a greedy top-down splitting criterion and use the grouping and exclusive regulatory mechanisms to build semantically significant hierarchies from unattended data. Our method uses maximum margin learning and we propose effective algorithms to solve the resulting non-convex goal. We test our method on four standard data sets, demonstrating the effectiveness of our method in clustering and the ability to grasp semantics across hierarchies."}], "references": [{"title": "Principal direction divisive partitioning", "author": ["Boley", "Daniel"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Boley and Daniel.,? \\Q1998\\E", "shortCiteRegEx": "Boley and Daniel.", "year": 1998}, {"title": "Representations of quasi-Newton matrices and their use in limited memory methods", "author": ["Byrd", "Richard H", "Nocedal", "Jorge", "Schnabel", "Robert B"], "venue": "Mathematical Programming,", "citeRegEx": "Byrd et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 1994}, {"title": "Likelihood based hierarchical clustering", "author": ["Castro", "Rui M", "Coates", "Mark", "Nowak", "Robert D"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Castro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2004}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "What does classifying more than 10, 000 image categories tell us", "author": ["Deng", "Jia", "Berg", "Alexander C", "Li", "Kai", "Fei-Fei"], "venue": "In ECCV,", "citeRegEx": "Deng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2010}, {"title": "LEMON - an open source C++ graph template library", "author": ["Dezs", "Bal\u00e1zs", "J\u00fcttner", "Alp\u00e1r", "Kov\u00e1cs", "P\u00e9ter"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "Dezs et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dezs et al\\.", "year": 2011}, {"title": "Theoretical improvements in algorithmic efficiency for network flow problems", "author": ["Edmonds", "Jack", "Karp", "Richard M"], "venue": "Journal of ACM,", "citeRegEx": "Edmonds et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Edmonds et al\\.", "year": 1972}, {"title": "Semantic label sharing for learning with many categories", "author": ["Fergus", "Robert", "Bernal", "Hector", "Weiss", "Yair", "Torralba", "Antonio"], "venue": "In ECCV,", "citeRegEx": "Fergus et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2010}, {"title": "A note on the group lasso and a sparse group", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "lasso. CoRR,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Hierarchical clustering of a mixture model", "author": ["Goldberger", "Jacob", "Roweis", "Sam T"], "venue": "In NIPS,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Semantic measures for the comparison of units of language, concepts or entities from text and knowledge base analysis", "author": ["Harispe", "S\u00e9bastien", "Ranwez", "Sylvie", "Janaqi", "Stefan", "Montmain", "Jacky"], "venue": "CoRR, abs/1310.1285,", "citeRegEx": "Harispe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Harispe et al\\.", "year": 2013}, {"title": "Learning a tree of metrics with disjoint visual features", "author": ["Hwang", "Sung Ju", "Grauman", "Kristen", "Sha", "Fei"], "venue": "In NIPS,", "citeRegEx": "Hwang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2011}, {"title": "Semantic kernel forests from multiple taxonomies", "author": ["Hwang", "Sung Ju", "Grauman", "Kristen", "Sha", "Fei"], "venue": "In NIPS,", "citeRegEx": "Hwang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Lampert", "Christoph H", "Nickisch", "Hannes", "Harmeling", "Stefan"], "venue": "In CVPR,", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Proximal Newton-type methods for convex optimization", "author": ["Lee", "Jason D", "Sun", "Yuekai", "Saunders", "Michael A"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Least squares quantization in PCM", "author": ["Lloyd", "Stuart P"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Lloyd and P.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd and P.", "year": 1982}, {"title": "Introduction to Information Retrieval", "author": ["Manning", "Christopher D", "Raghavan", "Prabhakar", "Sch\u00fctze", "Hinrich"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "The group lasso for logistic regression", "author": ["Meier", "Lukas", "Geer", "Sara Van De", "Buhlmann", "Peter"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Meier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2008}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Ng", "Andrew Y", "Jordan", "Michael I", "Weiss", "Yair"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Updating quasi-Newton matrices with limited storage", "author": ["Nocedal", "Jorge"], "venue": "Mathematics of computation,", "citeRegEx": "Nocedal and Jorge.,? \\Q1980\\E", "shortCiteRegEx": "Nocedal and Jorge.", "year": 1980}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["Rand", "William M"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rand and M.,? \\Q1971\\E", "shortCiteRegEx": "Rand and M.", "year": 1971}, {"title": "Graphical Model Structure Learning with `1Regularization", "author": ["Schmidt", "Mark"], "venue": "PhD thesis,", "citeRegEx": "Schmidt and Mark.,? \\Q2010\\E", "shortCiteRegEx": "Schmidt and Mark.", "year": 2010}, {"title": "A comparison of document clustering techniques", "author": ["Steinbach", "Michael", "Karypis", "George", "Kumar", "Vipin"], "venue": "In KDD Workshop on Text Mining,", "citeRegEx": "Steinbach et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Steinbach et al\\.", "year": 2000}, {"title": "Model-based hierarchical clustering", "author": ["Vaithyanathan", "Shivakumar", "Dom", "Byron"], "venue": "In UAI,", "citeRegEx": "Vaithyanathan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Vaithyanathan et al\\.", "year": 2000}, {"title": "Generalized maximum margin clustering and unsupervised kernel learning", "author": ["Valizadegan", "Hamed", "Jin", "Rong"], "venue": "In NIPS,", "citeRegEx": "Valizadegan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Valizadegan et al\\.", "year": 2006}, {"title": "On learning matrices with orthogonal columns or disjoint supports", "author": ["Vervier", "Kevin", "Mah\u00e9", "Pierre", "D\u2019Aspremont", "Alexandre", "Veyrieras", "Jean-Baptiste", "Vert", "Jean-Philippe"], "venue": "In ECML/PKDD,", "citeRegEx": "Vervier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vervier et al\\.", "year": 2014}, {"title": "Hierarchical classification via orthogonal transfer", "author": ["Xiao", "Lin", "Zhou", "Dengyong", "Wu", "Mingrui"], "venue": "In ICML,", "citeRegEx": "Xiao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2011}, {"title": "Unsupervised and semisupervised multi-class support vector machines", "author": ["Xu", "Linli", "Schuurmans", "Dale"], "venue": "In AAAI,", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}, {"title": "Maximum margin clustering", "author": ["Xu", "Linli", "Neufeld", "James", "Larson", "Bryce", "Schuurmans", "Dale"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2004}, {"title": "Maximum margin clustering made practical", "author": ["Zhang", "Kai", "Tsang", "Ivor W", "Kwok", "James T"], "venue": "In ICML,", "citeRegEx": "Zhang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "Efficient multiclass maximum margin clustering", "author": ["Zhao", "Bin", "Wang", "Fei", "Zhang", "Changshui"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}, {"title": "Latent maximum margin clustering", "author": ["Zhou", "Guang-Tong", "Lan", "Tian", "Vahdat", "Arash", "Mori", "Greg"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Exclusive lasso for multi-task feature selection", "author": ["Zhou", "Yang", "Jin", "Rong", "Hoi", "Steven C. H"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": "Popular clustering methods include k-means clustering (Lloyd, 1982) and spectral clustering (Ng et al., 2001).", "startOffset": 92, "endOffset": 109}, {"referenceID": 28, "context": "Recent progress in maximum-margin methods has led to the development of maximum-margin clustering (MMC) techniques (Xu et al., 2004), which aim to learn both the separating hyperplanes that separate clusters of data, and the label assignments of instances to the clusters.", "startOffset": 115, "endOffset": 132}, {"referenceID": 29, "context": "While efficient MMC methods have been proposed (Zhang et al., 2007; Zhao et al., 2008), even in such cases the time complexity is at least linear or quadratic with respect to the number of samples and clusters.", "startOffset": 47, "endOffset": 86}, {"referenceID": 30, "context": "While efficient MMC methods have been proposed (Zhang et al., 2007; Zhao et al., 2008), even in such cases the time complexity is at least linear or quadratic with respect to the number of samples and clusters.", "startOffset": 47, "endOffset": 86}, {"referenceID": 26, "context": "Such regularization has been made popular in semantic supervised learning in recent years (Xiao et al., 2011; Hwang et al., 2011), but here we apply the idea in an unsupervised hierarchical clustering framework.", "startOffset": 90, "endOffset": 129}, {"referenceID": 11, "context": "Such regularization has been made popular in semantic supervised learning in recent years (Xiao et al., 2011; Hwang et al., 2011), but here we apply the idea in an unsupervised hierarchical clustering framework.", "startOffset": 90, "endOffset": 129}, {"referenceID": 28, "context": "(Xu et al., 2004).", "startOffset": 0, "endOffset": 17}, {"referenceID": 28, "context": "Since this joint learning results in a non-convex formulation, unlike SVMs, it is often solved by a semidefinite relaxation (Xu et al., 2004; Valizadegan & Jin, 2006) or alternating optimization (Zhang et al.", "startOffset": 124, "endOffset": 166}, {"referenceID": 29, "context": ", 2004; Valizadegan & Jin, 2006) or alternating optimization (Zhang et al., 2007).", "startOffset": 61, "endOffset": 81}, {"referenceID": 30, "context": "While most of the MMC methods focus on efficient optimization of the nonconvex problems, the MMC formulation was also extended to handle the case of multi-cluster clustering problems (Xu & Schuurmans, 2005; Zhao et al., 2008), and to include latent variables (Zhou et al.", "startOffset": 183, "endOffset": 225}, {"referenceID": 31, "context": ", 2008), and to include latent variables (Zhou et al., 2013).", "startOffset": 41, "endOffset": 60}, {"referenceID": 16, "context": "Hierarchical clustering methods: Most hierarchical clustering methods employ either top-down clustering strategies that recursively split clusters into fine-grained clusters, or bottom-up clustering strategies that recursively group the smaller clusters into larger ones (Manning et al., 2008).", "startOffset": 271, "endOffset": 293}, {"referenceID": 22, "context": ", the bisecting k-means method (Steinbach et al., 2000)).", "startOffset": 31, "endOffset": 55}, {"referenceID": 2, "context": ", PDDP (Boley, 1998)) which performs the hierarchical clustering on the graph Laplacian of the similarity matrix, and modelbased hierarchical clustering (Vaithyanathan & Dom, 2000; Castro et al., 2004; Goldberger & Roweis, 2004) which fits probabilistic models at each split.", "startOffset": 153, "endOffset": 228}, {"referenceID": 17, "context": "The group lasso (Meier et al., 2008) employs a mixed `1,2-norm to promote sparsity among groups of features, identifying the groups that are most important for the task.", "startOffset": 16, "endOffset": 36}, {"referenceID": 8, "context": "A generalization of the group lasso is the sparse group lasso (Friedman et al., 2010), that further encourages sparsity within each individual model.", "startOffset": 62, "endOffset": 85}, {"referenceID": 32, "context": "The exclusive lasso (Zhou et al., 2010) encourages two models to use different features, by minimizing the `2-norm of their `1norms.", "startOffset": 20, "endOffset": 39}, {"referenceID": 26, "context": "Orthogonal transfer (Xiao et al., 2011) focuses on such exclusiveness between parent and child models in a", "startOffset": 20, "endOffset": 39}, {"referenceID": 11, "context": "The tree of metrics approach (Hwang et al., 2011) employs similar intuition, but learns Mahalanobis metrics instead of SVM weights, and focuses on selecting sparse and disjoint features.", "startOffset": 29, "endOffset": 49}, {"referenceID": 28, "context": "The proposed method builds on the standard flat MMC clustering (Xu et al., 2004), but extends MMC in the following two aspects: (i) we introduce regularizers to encourage the different layers of the hierarchy to focus on the use of different feature subsets, and (ii) we build the hierarchy iteratively from coarse clusters to fine-grained clusters (rather than forming all clusters in one split) using a greedy top-down algorithm with a novel splitting criterion.", "startOffset": 63, "endOffset": 80}, {"referenceID": 31, "context": "As suggested in (Zhou et al., 2013), we set Lt and Ut to 0.", "startOffset": 16, "endOffset": 35}, {"referenceID": 26, "context": "While the grouping and competition among features have proved useful for encoding semantic taxonomies in supervised learning problems (Xiao et al., 2011; Hwang et al., 2011), we apply these ideas for discovering semantically meaningful cluster hierarchies in an entirely unsupervised setting.", "startOffset": 134, "endOffset": 173}, {"referenceID": 11, "context": "While the grouping and competition among features have proved useful for encoding semantic taxonomies in supervised learning problems (Xiao et al., 2011; Hwang et al., 2011), we apply these ideas for discovering semantically meaningful cluster hierarchies in an entirely unsupervised setting.", "startOffset": 134, "endOffset": 173}, {"referenceID": 26, "context": "In (Xiao et al., 2011; Vervier et al., 2014), it is shown that Eq.", "startOffset": 3, "endOffset": 44}, {"referenceID": 25, "context": "In (Xiao et al., 2011; Vervier et al., 2014), it is shown that Eq.", "startOffset": 3, "endOffset": 44}, {"referenceID": 8, "context": "(Friedman et al., 2010).", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "In each iteration we fix the model parameters wt and optimize yt by solving a clustering assignment problem, and then we update wt while keeping yt fixed using a proximal quasi-Newton algorithm (Lee et al., 2012; Schmidt, 2010).", "startOffset": 194, "endOffset": 227}, {"referenceID": 31, "context": "Following (Zhou et al., 2013), we could solve Eq.", "startOffset": 10, "endOffset": 29}, {"referenceID": 5, "context": "To find this optimal flow, we apply the capacity scaling algorithm (Edmonds & Karp, 1972) implemented in the LEMON library (Dezs et al., 2011), which is an efficient dual solution method running in O ( |Dt| \u00b7 Kt \u00b7 log(|Dt| + Kt) \u00b7 log(Ut \u00b7 |Dt| \u00b7Kt) ) complexity.", "startOffset": 123, "endOffset": 142}, {"referenceID": 31, "context": "In practice, our MCF solver speeds up the ILP solver in (Zhou et al., 2013) by 10 to 100 times.", "startOffset": 56, "endOffset": 75}, {"referenceID": 14, "context": "Updating wt: With fixed yt, we solve for wt (a convex problem) using a proximal quasi-Newton method (Lee et al., 2012; Schmidt, 2010).", "startOffset": 100, "endOffset": 133}, {"referenceID": 1, "context": "w t , and \u2016z\u2016B = z>Bz is a divergence formed using the L-BFGS matrix B (Byrd et al., 1994; Nocedal, 1980).", "startOffset": 71, "endOffset": 105}, {"referenceID": 14, "context": "In the wt update, we fix the clustering yt and use a method that is guaranteed to find a global optimum (Lee et al., 2012; Schmidt, 2010).", "startOffset": 104, "endOffset": 137}, {"referenceID": 13, "context": "Datasets: We evaluate the performance of HMMC on four datasets from two public image collections: Animal With Attributes (AWA) (Lampert et al., 2009) and Ima-", "startOffset": 127, "endOffset": 149}, {"referenceID": 3, "context": "geNet (Deng et al., 2009).", "startOffset": 6, "endOffset": 25}, {"referenceID": 11, "context": "We use two datasets following the practice of (Hwang et al., 2011).", "startOffset": 46, "endOffset": 66}, {"referenceID": 13, "context": "The first one, AWA-ATTR, has 85 features consisting of the outputs of 85 linear SVMs trained to predict the presence/absence of the 85 nameable properties annotated by (Lampert et al., 2009), like red and furry.", "startOffset": 168, "endOffset": 190}, {"referenceID": 11, "context": "2 of (Hwang et al., 2011).", "startOffset": 5, "endOffset": 25}, {"referenceID": 11, "context": ", cab and canoe) and 26,624 images (Hwang et al., 2011), and IMAGENET consists of 28,957 images spanning 20 non-animal, nonvehicle classes (e.", "startOffset": 35, "endOffset": 55}, {"referenceID": 12, "context": ", lamp and drum) (Hwang et al., 2012).", "startOffset": 17, "endOffset": 37}, {"referenceID": 4, "context": "The raw image features are the provided bag-of-words histograms obtained by SIFT (Deng et al., 2010; 2009).", "startOffset": 81, "endOffset": 106}, {"referenceID": 11, "context": "3 of (Hwang et al., 2011) and Fig.", "startOffset": 5, "endOffset": 25}, {"referenceID": 12, "context": "2(e) of (Hwang et al., 2012), respectively.", "startOffset": 8, "endOffset": 28}, {"referenceID": 18, "context": "The first set is the flat clustering methods k-means (KM), spectral clustering (SC) (Ng et al., 2001), and an MMC approach implemented in (Zhou et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 31, "context": ", 2001), and an MMC approach implemented in (Zhou et al., 2013).", "startOffset": 44, "endOffset": 63}, {"referenceID": 16, "context": "We have tested a variety of methods including Single-Link (SL), Average-Link (AL) and Complete-Link (CL) (Manning et al., 2008).", "startOffset": 105, "endOffset": 127}, {"referenceID": 10, "context": "The first shortest path metric (Harispe et al., 2013) finds the shortest path linking the two image classes in the ground-truth hierarchy, normalizes the path distance by the maximum distance, and subtracts the distance from 1 as the semantic similarity.", "startOffset": 31, "endOffset": 53}, {"referenceID": 7, "context": "The second path sharing metric (Fergus et al., 2010) counts the number of nodes shared by the parent branches of the two image classes, normalized by the length of the longest of the two branches.", "startOffset": 31, "endOffset": 52}], "year": 2015, "abstractText": "We present a hierarchical maximum-margin clustering method for unsupervised data analysis. Our method extends beyond flat maximummargin clustering, and performs clustering recursively in a top-down manner. We propose an effective greedy splitting criteria for selecting which cluster to split next, and employ regularizers that enforce feature sharing/competition for capturing data semantics. Experimental results obtained on four standard datasets show that our method outperforms flat and hierarchical clustering baselines, while forming clean and semantically meaningful cluster hierarchies.", "creator": "LaTeX with hyperref package"}}}