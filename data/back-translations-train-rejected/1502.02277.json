{"id": "1502.02277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2015", "title": "Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches", "abstract": "Term frequency normalization is a serious issue since lengths of documents are various. Generally, documents become long due to two different reasons - verbosity and multi-topicality. First, verbosity means that the same topic is repeatedly mentioned by terms related to the topic, so that term frequency is more increased than the well-summarized one. Second, multi-topicality indicates that a document has a broad discussion of multi-topics, rather than single topic. Although these document characteristics should be differently handled, all previous methods of term frequency normalization have ignored these differences and have used a simplified length-driven approach which decreases the term frequency by only the length of a document, causing an unreasonable penalization. To attack this problem, we propose a novel TF normalization method which is a type of partially-axiomatic approach. We first formulate two formal constraints that the retrieval model should satisfy for documents having verbose and multi-topicality characteristic, respectively. Then, we modify language modeling approaches to better satisfy these two constraints, and derive novel smoothing methods. Experimental results show that the proposed method increases significantly the precision for keyword queries, and substantially improves MAP (Mean Average Precision) for verbose queries.", "histories": [["v1", "Sun, 8 Feb 2015 17:32:44 GMT  (34kb)", "http://arxiv.org/abs/1502.02277v1", "8 pages, conference paper, published in ECIR '08"]], "COMMENTS": "8 pages, conference paper, published in ECIR '08", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["seung-hoon na", "in-su kang", "jong-hyeok lee"], "accepted": false, "id": "1502.02277"}, "pdf": {"name": "1502.02277.pdf", "metadata": {"source": "CRF", "title": "Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches", "authors": ["Seung-Hoon Na", "In-Su Kang", "Jong-Hyeok Lee"], "emails": ["Korea@postech.ac.kr", "nsh1979@postech.ac.kr", "jhlee@postech.ac.kr", "Korea@kisti.re.kr", "dbaisk@kisti.re.kr"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.02 277v 1 [cs.I R] 8F eb"}, {"heading": "1 Introduction", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rfu the rfu the"}, {"heading": "2 Formal Constraints of New TF Normalization, and Analysis of Previous Language Modeling Approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Constraints", "text": "Dre rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc die rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "2.2 Analysis of Language Modeling Approaches", "text": "Our goal is to modify the language modeling approaches so that they better meet the proposed two constraints - VNC and TNC. We will examine two popular smoothing methods - Jelinek-Mercer Analysis (JM) and Dirichlet-previous smoothing (Dir). Before modifying them, we will begin to discuss whether each smoothing method meets VNC and TNC in this subset. Notations used in this paper are summarized as follows: Q A given query t fD (w) term frequency of W in document D t fC (w) term frequency of collection lC Total frequency of collection D Total term frequency of collection Model of D Smoothed Document Language D (MLE)."}, {"heading": "3 Modification of Previous Retrieval Models", "text": "In the previous section, we showed that two different smoothing methods do not satisfy two constraints well. In this section, we present the measurement of the number of topics and modify the previous call model to better satisfy VNC and TNC."}, {"heading": "3.1 Measurement of The Number of Topics", "text": "To find out what measurement \u03c4 (D) is acceptable to calculate the number of topics in document D, we propose two simple measurements for \u03c4 (D) - the first measurement is the vocabulary size and the second is the amount of information. Vocabulary size: Generally, because there are more terms, a particular document has more topics. Note: Although the vocabulary size is simple and reasonable, we cannot distinguish the mainly topic-related terms from the causally occurring terms. By using the vocabulary size D, the number of topics may be increased unreasonably due to the causally occurring terms. Quantity: Although the vocabulary size is simple and reasonable, it cannot distinguish the mainly topic-related terms from the causally occurring terms."}, {"heading": "3.2 Modification of JM", "text": "The basic idea of modifying JM is a pseudo-document model that is not really constructed in real time. We all need generative probabilities for query terms from the pseudo-document. To estimate the probability of query terms in a pseudo-document, we simplify the estimation problem by using probabilities in the original documentation. To determine the probability of query terms in a pseudo-document, we simplify the estimation problem in the original documentation."}, {"heading": "4 Modification of Dir", "text": "Our goal for the dir modification is to provide VNC. We introduce the concept of the pseudo-document model q to modify you (D). Unlike the pseudo-document for the JM modification, which consists only of query-relevant parts, the pseudo-document for the dir modification consists of all topics in the original document, but has a different length than the original length. We assume that the pseudo-document model is proportional to the original MLE document model. In addition, we determine the length of the pseudo-document based on the document length (D). Keep in mind that the informative detail - \u03c9 (D) - does not correspond to VNC. We assume that the pseudo-document model is proportional to the original MLE model."}, {"heading": "5 Experimentation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setting", "text": "s stammering. Table 1 summarizes the basic information of each test collection. In columns # Q, Topics, # R, # Doc, avglen, and # Terms, the number of topics, the corresponding query topic identities, the number of relevant documents, the number of documents, the average length of documents, and the number of terms, or the number of terms. According to Zhai's thesis [6], we used the following three different types of queries: 1) Short Keyword (SK): Let us only use the title of the topic description. 2) Short Verbose (SV): Let us only use the description field (usually one sentence). 3) Long Verbose (LV): Let us use the title, the description, and the narrative field (more than 50 words)."}, {"heading": "5.2 Experimental Results", "text": "Table 2 shows the best performance (MAP, Pr @ 5, Pr @ 10) of DirV and JMV2 compared to you. As with the subject measurement \u03c4 (D), we chose the amount of information (\u03b5 (D) because JMV2 and DirV are better than those using the amount of information than those using vocabulary size. We used MLE (Maximum Likelihood Estimation) for P (w | \u03b8D) to calculate the amount of information without smoothing. To check whether the proposed method (DirV and JMV2) significantly improves the superiority over JM in all test collections or not, we performed the Wilcoxon Sign Ranking Test to obtain the best performance of each run, we looked for 20 different values between 0.01 and 0.99 for \u03bb and 22 values between 100 and 30,000 for \u00b5. To check whether the proposed method (DirV and JMV2) significantly improves the baseline (DirV and JMV2)."}, {"heading": "6 Conclusion", "text": "This paper introduced a new edition for TF normalization by looking at two different types of complicated long-length documents - detailed documents and multi-topical documents. We proposed a novel TF normalization method that uses a partially axiomatic approach. To this end, we formulated two desirable constraints that the retrieval function should meet, and demonstrated that earlier approaches to speech modeling did not meet these constraints well. Then, we derived novel smoothing methods for approaches to speech modeling without losing basic principles, and showed that the proposed methods meet these constraints more effectively. Experimental results on five standard TREC collections show that the proposed methods are better than previous smoothing methods, especially for verbal types of queries. JMV2 significantly improved JM for all types of queries, and DirV eliminated the constraint on dir by providing the robustness of results @ based on the results of P7, as well as the complexity of results for improving the performance of robustness of P7."}], "references": [{"title": "Pivoted document length normalization", "author": ["A. Singhal", "C. Buckley", "M. Mitra"], "venue": "SIGIR \u201996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["S.E. Robertson", "S. Walker"], "venue": "SIGIR \u201994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "A formal study of information retrieval heuristics", "author": ["H. Fang", "T. Tao", "C. Zhai"], "venue": "SIGIR \u201904.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "An exploration of axiomatic approaches to information retrieval", "author": ["H. Fang", "C. Zhai"], "venue": "SIGIR \u201905.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "SIGIR \u201998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "SIGIR \u201901.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "A study of poisson query generation model for information retrieval", "author": ["Q. Mei", "H. Fang", "C. Zhai"], "venue": "SIGIR \u201907.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Effective ranking with arbitrary passages", "author": ["M. Kaszkiel", "J. Zobel"], "venue": "Journal of the American Society for Information Science and Technology (JASIST) 52(4)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Regarding this, Singhal observed the following two different types of reasons for making the length of a document long [1] 3.", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "3 Robertson and Walker mentioned two types of reasons as scope hypothesis and verbosity hypothesis, respectively [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "Singhal pre-assumed that long-length documents should be penalized regardless of whether or not their types are verbosity (or multi-topicality) [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "These constraints can be directly utilized to derive a new class of retrieval function as Fang\u2019s exploration [3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Originally, Fang formulated two constraints related to term frequency - LNC1 and LNC2 [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "In contrast, Fang\u2019s approach is the fully-axiomatic approach [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 3, "context": "In contrast, Fang\u2019s approach is the fully-axiomatic approach [3, 4].", "startOffset": 61, "endOffset": 67}, {"referenceID": 4, "context": "We selected the language modeling approaches as the backbone retrieval model [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "We investigate two popular smoothing methods - Jelinek-Mercer smoothing (JM) and Dirichlet-prior smoothing (Dir) [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Analysis of Jelinek-Mercer Smoothing In JM (Jeliner-Mercer Smoothing), a smoothed document model is obtained by the interpolation of MLE (Maximum Likelihood Estimation) of a document model and the collection model as follows [6]:", "startOffset": 225, "endOffset": 228}, {"referenceID": 5, "context": "Analysis of Dirichlet-Prior Smoothing In Dir (Dirichlet-prior smoothing), a smoothed document model is estimated as posterior model when taking \u03bcP(w|\u03b8C) as a prior probability of term w as follows [6]:", "startOffset": 197, "endOffset": 200}, {"referenceID": 0, "context": "penalized even multi-topical documents, as well as verbose documents [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "According to Zhai\u2019s work [6], we used the following three different types of queries: 1) Short keyword (SK): Using only the title of the topic description.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "This is comparable to recent results using more complicated query-specific smoothing based on Poisson language model [7].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "To handle long-length documents, passage-based retrieval could be applied [8].", "startOffset": 74, "endOffset": 77}], "year": 2015, "abstractText": "Term frequency normalization is a serious issue since lengths of documents are various. Generally, documents become long due to two different reasons verbosity and multi-topicality. First, verbosity means that the same topic is repeatedly mentioned by terms related to the topic, so that term frequency is more increased than the well-summarized one. Second, multi-topicality indicates that a document has a broad discussion of multi-topics, rather than single topic. Although these document characteristics should be differently handled, all previous methods of term frequency normalization have ignored these differences and have used a simplified length-driven approach which decreases the term frequency by only the length of a document, causing an unreasonable penalization. To attack this problem, we propose a novel TF normalization method which is a type of partially-axiomatic approach. We first formulate two formal constraints that the retrieval model should satisfy for documents having verbose and multi-topicality characteristic, respectively. Then, we modify language modeling approaches to better satisfy these two constraints, and derive novel smoothing methods. Experimental results show that the proposed method increases significantly the precision for keyword queries, and substantially improves MAP (Mean Average Precision) for verbose queries.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}