{"id": "1502.02410", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Out-of-sample generalizations for supervised manifold learning for classification", "abstract": "Supervised manifold learning methods for data classification map data samples residing in a high-dimensional ambient space to a lower-dimensional domain in a structure-preserving way, while enhancing the separation between different classes in the learned embedding. Most nonlinear supervised manifold learning methods compute the embedding of the manifolds only at the initially available training points, while the generalization of the embedding to novel points, known as the out-of-sample extension problem in manifold learning, becomes especially important in classification applications. In this work, we propose a semi-supervised method for building an interpolation function that provides an out-of-sample extension for general supervised manifold learning algorithms studied in the context of classification. The proposed algorithm computes a radial basis function (RBF) interpolator that minimizes an objective function consisting of the total embedding error of unlabeled test samples, defined as their distance to the embeddings of the manifolds of their own class, as well as a regularization term that controls the smoothness of the interpolation function in a direction-dependent way. The class labels of test data and the interpolation function parameters are estimated jointly with a progressive procedure. Experimental results on face and object images demonstrate the potential of the proposed out-of-sample extension algorithm for the classification of manifold-modeled data sets.", "histories": [["v1", "Mon, 9 Feb 2015 09:56:57 GMT  (713kb,D)", "http://arxiv.org/abs/1502.02410v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["elif vural", "christine guillemot"], "accepted": false, "id": "1502.02410"}, "pdf": {"name": "1502.02410.pdf", "metadata": {"source": "CRF", "title": "Out-of-sample generalizations for supervised manifold learning for classification", "authors": ["Elif Vural", "Christine Guillemot"], "emails": ["(elif.vural@inria.fr,", "tine.guillemot@inria.fr)."], "sections": [{"heading": null, "text": "This year it has come to the point that it will be able to drown the aforementioned lcihsrteeSe in order to drown them."}, {"heading": "II. OVERVIEW OF MANIFOLD LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Manifold learning for classification", "text": "Considering a number of data samples {xi} Ni = 1 \"Rn that are located in a high-dimensional space, the problem is Rn = W.\" Various learning methods generally assume that the samples {xi} come from a low intrinsic dimension model and look for an embedding that significantly reduces the dimension of the data (d \"n) while preserving certain geometric properties. Different methods aim at different goals in the calculation of embedding Y.\" The ISOMAP method calculates such an embedding that the Euclidean distances in the low-dimensional domain are significantly reduced and the geographical distances in the original domain are proportional, while LLE searches for an embedding that preserves local reconstruction weights of data samples in the original domain."}, {"heading": "B. Out-of-sample extensions", "text": "This year it has come to the point where we will be able to put ourselves at the top, \"he said in an interview with\" Welt am Sonntag, \"in which he said yes.\" We have been able, \"he said.\" We have to be in the position we are in, \"he said.\" We have to be in the position we are in, \"he said.\" We have to be able to be in the position we are in, \"he said.\" We have to be in the position we are in, \"he said."}, {"heading": "III. OUT-OF-SAMPLE EXTENSIONS FOR CLASSIFICATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Formulation of the out-of-sample problem", "text": "We begin with a formalization of the point x to the manifold Mm, which is based on a minimum of spatial expansion."}, {"heading": "B. Construction of the interpolation function", "text": "In this study, we choose the sentence H of interpolation (x) = 1 (x).1 (x).2 (x).2 (x).2 (x).3 (x).3 (x).3 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).5 (x).5 (x).5 (x).5 (x).5 (x).5 (x).6).5 (x).5 (x).6 (x).6 (x).6 (x).6).6 (x).6 (.6).6 (x).6 (.6).6 (x).6 (x).6 (x).6 (x).6 (x).6 (x).6 (x).6 (x).6 (x).6 (x).6 (x).6).6 (x).6 (x).6).6 (x).6).6 (x).6 (x).6).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4 (x).4"}, {"heading": "IV. DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Complexity analysis", "text": "We now derive the complexity of the proposed method, which is essentially determined by the complexity of steps 11-13 in the main loop of the algorithm. (In step 11, the determination of the closest neighbors in XT for each test image is of the complexity O (nN), and the solution of the square program in (19) has a polynomial time complexity O (poly (K))) in the number of neighbors K [24]. The complexity dK of (20) can be neglected as d. Since the embedding of the projection of individual points in X\\ XT is calculated only once in the total complexity of the algorithm, we obtain the total complexity of step 11 as O (poly (K) + nN)."}, {"heading": "B. Relation to kernel ridge regression", "text": "In this section, we discuss how to adjust the weighting of regulatory problems."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In this section, we evaluate the performance of the proposed method in classification experiments. We apply the presented out-of-sample extension algorithm to two different monitored manifold learning methods. First, we consider the monitored Laplacian eigenmaps algorithm used in [9], [7], and [21], which calculates an embedding by solving (1). Then, we evaluate our algorithm for embedding using the Fisher-like objective function in (2) used by methods such as [5], [7], and [21]. However, we calculate a nonlinear embedding by removing the linear projection constraint zT = vTX, so that the out-of-sample extension problem is of interest. We compare the following methods in the experiments, the first four of which provide out-of-sample extension solutions for multiple embedding."}, {"heading": "VI. CONCLUSIONS", "text": "The proposed method of generalization outside the sample is based on the construction of an RBF interpolation function, in which the parameters of the interpolation function are optimized to minimize the embedding error over 13a of initially blank data samples, whose class names are gradually estimated along with the parameters of the interpolation function. We have shown that the regularity of the interpolation function can be controlled by optimizing the RBF scale parameters to minimize a regulatory goal that controls the total gradient of the interpolation function while simultaneously promoting sufficiently strong derivatives along the guidelines of the class boundaries to ensure effective separation between the different classes. The proposed generalization method, which controls the total gradient of the interpolation function and distinguishes the results of the interpolation classification in the way that the proposed interalgorithms effectively classify the results."}, {"heading": "VII. ACKNOWLEDGMENT", "text": "The authors thank Pascal Frossard and Alhussein Fawzi for the helpful discussions that contributed to this study."}], "references": [{"title": "A global geometric framework for nonlinear dimensionality reduction.", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, vol. 290,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, pp. 2323\u20132326, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, vol. 15, no. 6, pp. 1373\u20131396, Jun. 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "H.J. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 1, pp. 40\u201351, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Local similarity and diversity preserving discriminant projection for face and handwriting digits recognition.", "author": ["Q. Hua", "L. Bai", "X.Z. Wang", "Y. Liu"], "venue": "Neurocomputing, vol", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "A multi-manifold discriminant analysis method for image feature extraction", "author": ["W. Yang", "C. Sun", "L. Zhang"], "venue": "Pattern Recognition, vol. 44, no. 8, pp. 1649\u20131657, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Marginal semi-supervised submanifold projections with informative constraints for dimensionality reduction and recognition", "author": ["Z. Zhang", "M. Zhao", "T. Chow"], "venue": "Neural Networks, vol. 36, pp. 97\u2013111, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Stable orthogonal local discriminant embedding for linear dimensionality reduction.", "author": ["Q. Gao", "J. Ma", "H. Zhang", "X. Gao", "Y. Liu"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "A supervised non-linear dimensionality reduction approach for manifold learning", "author": ["B. Raducanu", "F. Dornaika"], "venue": "Pattern Recognition, vol. 45, no. 6, pp. 2432\u20132444, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Out-of-sample extensions for LLE, ISOMAP, MDS, Eigenmaps, and Spectral Clustering", "author": ["Y. Bengio", "J.F. Paiement", "P. Vincent", "O. Delalleau", "N. Le Roux", "M. Ouimet"], "venue": "Adv. Neural Inf. Process. Syst. MIT Press, 2004, pp. 177\u2013184.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Sparse projections of medical images onto manifolds", "author": ["G.H. Chen", "C. Wachinger", "P. Golland"], "venue": "Proc. Information Processing in Medical Imaging - 23rd International Conference, 2013, pp. 292\u2013303.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "An explicit nonlinear mapping for manifold learning", "author": ["H. Qiao", "P. Zhang", "D. Wang", "B. Zhang"], "venue": "IEEE T. Cybernetics, vol. 43, no. 1, pp. 51\u201363, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A sparse-grid-based out-of-sample extension for dimensionality reduction and clustering with laplacian eigenmaps", "author": ["B. Peherstorfer", "D. Pfl\u00fcger", "H.J. Bungartz"], "venue": "AI 2011: Proc. Advances in Artificial Intelligence - 24th Australasian Joint Conference, 2011, pp. 112\u2013121.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "A generalised solution to the out-ofsample extension problem in manifold learning", "author": ["H. Strange", "R. Zwiggelaar"], "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "The out-of-sample problem for classical multidimensional scaling", "author": ["M.W. Trosset", "C.E. Priebe"], "venue": "Computational Statistics & Data Analysis, vol. 52, no. 10, pp. 4635\u20134642, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Out-of-sample extrapolation of learned manifolds", "author": ["T.J. Chin", "D. Suter"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 9, pp. 1547\u20131556, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised learning of image manifolds by semidefinite programming", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "International Journal of Computer Vision, vol. 70, no. 1, pp. 77\u201390, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality Preserving Projections", "author": ["X. He", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems 16. Cambridge, MA: MIT Press, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Face recognition using Laplacianfaces", "author": ["X. He", "S. Yan", "Y. Hu", "P. Niyogi", "H. Zhang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 3, pp. 328\u2013340, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Orthogonal Laplacianfaces for face recognition", "author": ["D. Cai", "X. He", "J. Han", "H. Zhang"], "venue": "IEEE Transactions on Image Processing, vol. 15, no. 11, pp. 3608\u20133614, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Manifold discriminant analysis", "author": ["R. Wang", "X. Chen"], "venue": "CVPR, 2009, pp. 429\u2013436.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Marginal fisher analysis and its variants for human gait recognition and content- based image retrieval", "author": ["D. Xu", "S. Yan", "D. Tao", "S. Lin", "H. Zhang"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 11, pp. 2811\u20132821, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Radial Basis Functions", "author": ["M.D. Buhmann"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "The polynomial solvability of convex quadratic programming", "author": ["M.K. Kozlov", "S.P. Tarasov", "L. Khachiyan"], "venue": "USSR Computational Mathematics and Mathematical Physics, vol. 20, no. 5, pp. 223 \u2013 228, 1980.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1980}, {"title": "Ridge regression learning algorithm in dual variables", "author": ["C. Saunders", "A. Gammerman", "V. Vovk"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning, 1998, pp. 515\u2013521.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "Machine Learning, Proceedings of the Twentieth International Conference, 2003, pp. 912\u2013 919.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Analyzing appearance and contour based methods for object categorization", "author": ["B. Leibe", "B. Schiele"], "venue": "2003 IEEE Computer Society 14 Conference on Computer Vision and Pattern Recognition (CVPR 2003), 2003, pp. 409\u2013415.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Columbia Object Image Library (COIL-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "Tech. Rep., Feb 1996.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "unsupervised manifold learning methods such as [1], [2], [3], which only take the geometric structure of data samples into account when learning a low-dimensional embedding, many recent supervised manifold learning methods seek a representation that not only preserves the manifold structure in each class, but also enhances the separation between differ-", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "unsupervised manifold learning methods such as [1], [2], [3], which only take the geometric structure of data samples into account when learning a low-dimensional embedding, many recent supervised manifold learning methods seek a representation that not only preserves the manifold structure in each class, but also enhances the separation between differ-", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "unsupervised manifold learning methods such as [1], [2], [3], which only take the geometric structure of data samples into account when learning a low-dimensional embedding, many recent supervised manifold learning methods seek a representation that not only preserves the manifold structure in each class, but also enhances the separation between differ-", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "Linear methods such as [4], [5], [6], [7], and [8] learn a linear projection that maps data into a lower-dimensional space such that the proximity of neighboring samples from the same class is preserved, while the distance between samples from different classes is increased.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Linear methods such as [4], [5], [6], [7], and [8] learn a linear projection that maps data into a lower-dimensional space such that the proximity of neighboring samples from the same class is preserved, while the distance between samples from different classes is increased.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Linear methods such as [4], [5], [6], [7], and [8] learn a linear projection that maps data into a lower-dimensional space such that the proximity of neighboring samples from the same class is preserved, while the distance between samples from different classes is increased.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "Linear methods such as [4], [5], [6], [7], and [8] learn a linear projection that maps data into a lower-dimensional space such that the proximity of neighboring samples from the same class is preserved, while the distance between samples from different classes is increased.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Linear methods such as [4], [5], [6], [7], and [8] learn a linear projection that maps data into a lower-dimensional space such that the proximity of neighboring samples from the same class is preserved, while the distance between samples from different classes is increased.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "Nonlinear methods such as [9] have a similar classification-driven objective, while the new coordinates of data samples in the low-dimensional space are computed with a nonlinear learning process based on a graph representation of data.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "In fact, nonlinear methods such as [9], or nonlinear adaptations of the above linear methods, typically learn data representations where different classes become even linearly separable.", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "The study in [10] focuses on the extension", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "formula as proposed in [10] can also be derived from the kernel ridge regression framework, by removing the regularization term and imposing the constraint that the data coordinates of training samples be given by the eigenvectors of the data kernel matrix [11].", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "formula as proposed in [10] can also be derived from the kernel ridge regression framework, by removing the regularization term and imposing the constraint that the data coordinates of training samples be given by the eigenvectors of the data kernel matrix [11].", "startOffset": 257, "endOffset": 261}, {"referenceID": 11, "context": "interpolation functions used in manifold learning extensions are polynomials [12], sparse linear combinations of functions in a reproducing kernel Hilbert space (RKHS) [11], and sparse grid functions [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "interpolation functions used in manifold learning extensions are polynomials [12], sparse linear combinations of functions in a reproducing kernel Hilbert space (RKHS) [11], and sparse grid functions [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "interpolation functions used in manifold learning extensions are polynomials [12], sparse linear combinations of functions in a reproducing kernel Hilbert space (RKHS) [11], and sparse grid functions [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 13, "context": "In [14], the out-of-sample extension of general manifold learning methods is achieved by computing a local projection of the high-dimensional space to the lowdimensional domain with a similarity transformation of the local PCA bases.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "The study in [15] proposes an out-of-sample generalization of the multidimensional scaling (MDS) method, which is based on an interpretation of MDS as a least squares problem.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Similarly, the method proposed in [16] presents a generalization for maximum variance unfolding [17].", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Similarly, the method proposed in [16] presents a generalization for maximum variance unfolding [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": "For instance, the popular Nystr\u00f6m extension [10] considers embeddings given by the eigenvectors of a symmetric kernel matrix.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": ", as in [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "The ISOMAP method computes an embedding such that Euclidean distances in the low-dimensional domain are proportional to the geodesic distances in the original domain [1], while LLE looks for an embedding that preserves local reconstruction weights of data samples in the original domain [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "The ISOMAP method computes an embedding such that Euclidean distances in the low-dimensional domain are proportional to the geodesic distances in the original domain [1], while LLE looks for an embedding that preserves local reconstruction weights of data samples in the original domain [2].", "startOffset": 287, "endOffset": 290}, {"referenceID": 2, "context": "The Laplacian eigenmaps algorithm [3] first constructs a graph from the data samples where nearest neighbors are typically connected with an edge.", "startOffset": 34, "endOffset": 37}, {"referenceID": 17, "context": "projection of X onto R in [18], which is applied to face", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "recognitions problems in [19] and [20].", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "recognitions problems in [19] and [20].", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "The algorithm proposed in [9] formalizes this idea by defining two graphs that respectively capture the within-class and betweenclass neighborhoods.", "startOffset": 26, "endOffset": 29}, {"referenceID": 20, "context": "The method proposed in [21] employs an alternative Fisher-like formulation for the supervised manifold learning problem where the embedding is obtained by solving", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Variations over this formulation can be found in several other works such as [4], [5], [6], [7], [8] and [22].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "Variations over this formulation can be found in several other works such as [4], [5], [6], [7], [8] and [22].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "Variations over this formulation can be found in several other works such as [4], [5], [6], [7], [8] and [22].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "Variations over this formulation can be found in several other works such as [4], [5], [6], [7], [8] and [22].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Variations over this formulation can be found in several other works such as [4], [5], [6], [7], [8] and [22].", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "Variations over this formulation can be found in several other works such as [4], [5], [6], [7], [8] and [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "A popular out-of-sample generalization algorithm is presented in [10], based on the Nystr\u00f6m formula.", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "The out-ofsample extension proposed in [10] is then given by the function f(x) = [f(x) .", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "The reason is that, although the data kernel matrix M is assumed to be a general symmetric matrix (not necessarily positive semi-definite) in [10], the entries of this matrix in supervised methods are not only dependent on the data samples xi, but also on their class labels.", "startOffset": 142, "endOffset": 146}, {"referenceID": 10, "context": "Several out-of-sample extension methods such as those based on fitting a particular type of interpolation function as in [11], [12], and [13] can be applied for generalizing supervised embeddings by fitting a function f to the priorly learned (xi, yi) pairs.", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "Several out-of-sample extension methods such as those based on fitting a particular type of interpolation function as in [11], [12], and [13] can be applied for generalizing supervised embeddings by fitting a function f to the priorly learned (xi, yi) pairs.", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "Several out-of-sample extension methods such as those based on fitting a particular type of interpolation function as in [11], [12], and [13] can be applied for generalizing supervised embeddings by fitting a function f to the priorly learned (xi, yi) pairs.", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "The square matrix \u03a6 is invertible if the points xi are distinct and \u03c6 is chosen as the Gaussian kernel [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 23, "context": "nation of the nearest neighbors in XT for each test image is of complexity O(nN), and the solution of the quadratic program in (19) has a polynomial-time complexity O(poly(K)) in the number of neighbors K [24].", "startOffset": 205, "endOffset": 209}, {"referenceID": 24, "context": "An alternative formulation of ridge regression is proposed in [25] that is based on a dual version of the above problem.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "First, we consider the supervised Laplacian eigenmaps algorithm presented in [9], which computes an embedding by solving (1).", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "Next, we evaluate our algorithm on embeddings obtained with the Fisher-like objective function in (2), which is used by methods such as [5], [6], [7], and [21].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Next, we evaluate our algorithm on embeddings obtained with the Fisher-like objective function in (2), which is used by methods such as [5], [6], [7], and [21].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Next, we evaluate our algorithm on embeddings obtained with the Fisher-like objective function in (2), which is used by methods such as [5], [6], [7], and [21].", "startOffset": 146, "endOffset": 149}, {"referenceID": 20, "context": "Next, we evaluate our algorithm on embeddings obtained with the Fisher-like objective function in (2), which is used by methods such as [5], [6], [7], and [21].", "startOffset": 155, "endOffset": 159}, {"referenceID": 1, "context": "\u2022 Locally linear embedding (LLE): Test points in R are mapped to R with an adaptation of the LLE algorithm [2] to the out-of-sample problem.", "startOffset": 107, "endOffset": 110}, {"referenceID": 25, "context": "We test the performance of SSL with the algorithm proposed in [26], which is a state-of-the-art semi-supervised classifier based on the computation of a smooth function on the data graph that coincides with the class labels when evaluated at data samples of known class labels.", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": "We first evaluate the proposed method on a data set consisting of the face images of 12 individuals from the extended Yale face database [27], which includes 58 images", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "The first experiment is conducted on the images of 8 objects from the ETH-80 database [28], where 41 images are available for each object (in particular, the images of the first object in each object category are used so that the images in each class belong to the same manifold).", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "The second experiment is done on the images of 20 objects from the COIL-20 database [29] with 71 images for each object, which are normalized, converted to grayscale, and downsampled to a resolution of 32 \u00d7 32 pixels.", "startOffset": 84, "endOffset": 88}], "year": 2015, "abstractText": "Supervised manifold learning methods for data classification map data samples residing in a high-dimensional ambient space to a lower-dimensional domain in a structurepreserving way, while enhancing the separation between different classes in the learned embedding. Most nonlinear supervised manifold learning methods compute the embedding of the manifolds only at the initially available training points, while the generalization of the embedding to novel points, known as the out-of-sample extension problem in manifold learning, becomes especially important in classification applications. In this work, we propose a semi-supervised method for building an interpolation function that provides an out-of-sample extension for general supervised manifold learning algorithms studied in the context of classification. The proposed algorithm computes a radial basis function (RBF) interpolator that minimizes an objective function consisting of the total embedding error of unlabeled test samples, defined as their distance to the embeddings of the manifolds of their own class, as well as a regularization term that controls the smoothness of the interpolation function in a direction-dependent way. The class labels of test data and the interpolation function parameters are estimated jointly with a progressive procedure. Experimental results on face and object images demonstrate the potential of the proposed out-of-sample extension algorithm for the classification of manifold-modeled data sets.", "creator": "LaTeX with hyperref package"}}}