{"id": "1502.02476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "An Infinite Restricted Boltzmann Machine", "abstract": "We present a mathematical construction for the restricted Boltzmann machine (RBM) in which the hidden layer size is adaptive and can grow during training. This is obtained by first extending the RBM to be sensitive to the ordering of its hidden units. Then, thanks to a carefully chosen definition of the energy function, we show that the limit of infinitely many hidden units is well defined. As in a regular RBM, approximate maximum likelihood training can be performed, resulting in an algorithm that naturally and adaptively adds trained hidden units during learning. We empirically study the behaviour of this infinite RBM, showing that its performance is competitive to that of the RBM.", "histories": [["v1", "Mon, 9 Feb 2015 13:18:24 GMT  (644kb,D)", "http://arxiv.org/abs/1502.02476v1", "9 pages"], ["v2", "Tue, 10 Feb 2015 03:44:17 GMT  (646kb,D)", "http://arxiv.org/abs/1502.02476v2", "9 pages"], ["v3", "Thu, 11 Jun 2015 16:05:59 GMT  (1767kb,D)", "http://arxiv.org/abs/1502.02476v3", "12 pages"], ["v4", "Fri, 18 Mar 2016 14:14:04 GMT  (863kb,D)", "http://arxiv.org/abs/1502.02476v4", "25 pages, 8 figures"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marc-alexandre c\\^ot\\'e", "hugo larochelle"], "accepted": false, "id": "1502.02476"}, "pdf": {"name": "1502.02476.pdf", "metadata": {"source": "META", "title": "An Infinite Restricted Boltzmann Machine", "authors": ["Marc-Alexandre C\u00f4t\u00e9", "Hugo Larochelle"], "emails": ["MARC-ALEXANDRE.COTE@USHERBROOKE.CA", "HUGO.LAROCHELLE@USHERBROOKE.CA"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "2. Restricted Boltzmann Machine", "text": "We begin with the description of the basic RBM model, which we will use to derive its ordered and infinite units. (RBM) We begin with the description of the basic RBM model, which we will apply to the derivation of its ordered and infinite units. (RBM) We begin with the description of the basic RBM model, which we will apply to the derivation of its ordered and infinite units. (RBM model) We begin with the description of the basic RBM models. (RBM model) We begin with the description of the basic RBM models. (RBM model) We begin with the derivation of the basic RBM models. (RBM model) We have an associated energy value, which is defined by the following function: E (v, h) = \u2212 hTWv \u2212 vTbv \u2212 vTbh (1) We begin with the description of the basic RBM models (BM model)."}, {"heading": "3. Ordered Restricted Boltzmann Machine", "text": "The model we propose is a variant of the RBM, in which the hidden units h = = are arranged from left to right, taking into account this order of the energy function. We refer to this model as an ordered RBM (oRBM). The oRBM takes into account hidden units by introducing a random variable z, which can be understood as the effective number of hidden units involved in the energy distribution. Hidden units are from the left and the selection of each hidden unit with incremental costs in energy. Specifically, we define the energy function of the oRBM asE (v, h, z) = - vTbv \u2212 z, i = 1 hi (Wi \u00b7 v + b h i) \u2212 \u03b2i, where z represents the number of selected hidden units that are active and \u03b2i is an energy penalty for the selection of each hidden unit. Different choices for the per unit of energy penalty could be used."}, {"heading": "4. Infinite Restricted Boltzmann Machine", "text": "() () () () () () () () () () ()) () () () ()) () ()) () ()) () ()) () ()) ()) () ()) () ()) () ()) () () ()) () ()) () ()) () ()) () ()) () ()) () ()) () () () ()) () () () () () () () () ()) () () ()) () () ()) () () ()) () () () ()) () () () () () () () ()) () () () () ()) () () () ()) () () ()) () () () () ()) () () () ()) () () () ()) () () () ()) () () ()) () () ()) () () ()) () ()) () () ()) () () () ()) () ()) () () () () () ()) ()) () () () () () () ()) () () () ()) () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ("}, {"heading": "5. Related Work", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have never lost as much time as this year, \"he said.\" But we are not yet at the point where we can afford it. \""}, {"heading": "6. Experiments", "text": "We compare the performance of the oRBM and the iRBM with the classical RBM on two datasets: the binarized MNIST (Salakhutdinov & Murray, 2008) and the silhouettes CalTech101 (Marlin et al., 2010). All NLL results of this section were obtained by estimating the log partition function ln Z (Annealed Importance Sampling (AIS) (Salakhutdinov & Murray, 2008) with 100,000 intermediate distributions and 5,000 chains. As an additional validation step, samples were generated from the best models and visually verified. The code for reproducing the paper experiments is at github1.1http: / / github.com / MarcCote / iRBMEach model was stochastic gradient descent using a mini-batch using the CD (k) or PCD RCD (k), with k \u2212 10, 25} representing the number of steps between parameters."}, {"heading": "6.1. Binarized MNIST", "text": "The MNIST dataset2 is composed of 70,000 28x28 pixel images representing handwritten digits (0-9), stochastically binarized according to their pixel intensity as in Salakhutdinov & Murray (2008), using the same distribution as in Larochelle & Murray (2011), corresponding to 50,000 examples for training, 10,000 for validation and 10,000 for testing, each model has been reassessed for 500 epochs and the best results for RBM, oRBM and iRBM are reported in Table 1, the best RBM with 500 hidden units use CD (25), and has been trained by Salakhutdinov & Murray (2008) 3, but we have reassessed its partitioning function to follow the rest of our experiments. the best oRBM with 500 units have been trained using PCD (10) and the following hyper parameters: Size 1.1 = 0."}, {"heading": "6.2. CalTech101 Silhouettes", "text": "The CalTech101 Silhouettes 4 dataset (Marlin et al., 2010) consists of 8,671 28x28 binary pixel images representing the silhouettes of objects (101 classes), and is divided into three subsets: 4,100 training examples, 2,264 for validation, and 2,307 for test.Each model has been trained for 1000 epochs, and the best results for RBM, oRBM, and iRBM are reported in Table 2.Again, the oRBM and iRBM models achieve competitive performance compared to RBM. The best RBM with 500 hidden units was trained by Marlin et al. (2010) and uses the PCD instead of the CD. We have reassessed its partition function to follow the same procedure as the rest of our experiments. The best oRBM with 500 units were trained with PCD (25) and the following parameters: 1.01 = 1.10 \u2212 3 units of learning rate, and a best RM \u2212 1.1 \u2212 1."}, {"heading": "7. Conclusion", "text": "We proposed novel extensions of the RBM, the ordered RBM, and the infinite RBM, the latter being derived from the former by taking the infinite limit of its hidden layer size. We introduced a training method derived from Contrastive Divergence, so that the training of the iRBM leads to a learning process in which the effective hidden layer size 4http: / / people.cs.umass.edu / \u0445 marlin / data.shtmlcan grow with the training. In future work, we are interested in generalizing the idea of a growing latent representation to structures other than a flat vector representation. We are currently exploring extensions of the RBM that allow tree-structured latent representation. We believe that a similar construction that includes a similar z-random variable should allow us to derive a training algorithm that also learns the size of the latent representation."}, {"heading": "Acknowledgments", "text": "We thank NSERC for supporting this research and Stanislas Lauly for creating the iRBM training video."}], "references": [{"title": "Training Restricted Boltzmann Machines on Word Observations", "author": ["Dahl", "George E", "Adams", "Ryan P", "Larochelle", "Hugo"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "The cascadecorrelation learning architecture", "author": ["Fahlman", "Scott E", "Lebiere", "Christian"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Fahlman et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Fahlman et al\\.", "year": 1989}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Hinton", "GE"], "venue": "Neural computation,", "citeRegEx": "Hinton and GE.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and GE.", "year": 2002}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": null, "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Inductive Principles for Restricted Boltzmann Machine Learning", "author": ["Marlin", "Benjamin M", "Swersky", "Kevin", "Chen", "Bo", "de Freitas", "Nando"], "venue": "Proc. Intl. Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Marlin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2010}, {"title": "Implicit Mixtures of Restricted Boltzmann Machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey"], "venue": "NIPS, pp", "citeRegEx": "Nair et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2008}, {"title": "Bayesian Nonparametric Models", "author": ["Orbanz", "Peter", "Teh", "Yee Whye"], "venue": "In Encyclopedia of Machine Learning. Springer,", "citeRegEx": "Orbanz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Orbanz et al\\.", "year": 2010}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["Ranzato", "MarcAurelio", "Krizhevsky", "Alex", "Hinton", "Geoffrey E"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Learning Ordered Representations with Nested Dropout", "author": ["O Rippel", "MA Gelbart", "Adams", "RP"], "venue": "arXiv preprint arXiv:1402.0915,", "citeRegEx": "Rippel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2014}, {"title": "On the quantitative analysis of Deep Belief Networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "Proceedings of the 25th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Restricted Boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["Taylor", "Graham W", "Hinton", "Geoffrey E", "Roweis", "Sam T"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["Tieleman", "Tijmen"], "venue": "ICML; Vol. 307, pp", "citeRegEx": "Tieleman and Tijmen.,? \\Q2008\\E", "shortCiteRegEx": "Tieleman and Tijmen.", "year": 2008}, {"title": "Self supervised boosting", "author": ["Welling", "Max", "Zemel", "Richard S", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Welling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "The RBM (and its extensions to non-binary vectors) have been successfully applied to a large variety of problems and data, such as images (Ranzato et al., 2010), movie user preferences (Salakhutdinov et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 10, "context": ", 2010), movie user preferences (Salakhutdinov et al., 2007), motion capture data (Taylor et al.", "startOffset": 32, "endOffset": 60}, {"referenceID": 11, "context": ", 2007), motion capture data (Taylor et al., 2011), text data (Dahl et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 0, "context": ", 2011), text data (Dahl et al., 2012) and many others.", "startOffset": 19, "endOffset": 38}, {"referenceID": 8, "context": "The oRBM also bears some similarity with autoencoders trained by a nested version of dropout (Rippel et al., 2014).", "startOffset": 93, "endOffset": 114}, {"referenceID": 8, "context": "Rippel et al. (2014) showed that this defines a learning objective that makes the solution identifiable and no longer invariant to hidden unit permutation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "Rippel et al. (2014) showed that this defines a learning objective that makes the solution identifiable and no longer invariant to hidden unit permutation. In addition to being concerned with a different type of neural network model, this work doesn\u2019t discuss the case of an unbounded and adaptive hidden layer size. Welling et al. (2003) proposed a self supervised boosting approach, which is applicable to the RBM and in which hidden units are sequentially added and trained.", "startOffset": 0, "endOffset": 339}, {"referenceID": 8, "context": "Rippel et al. (2014) showed that this defines a learning objective that makes the solution identifiable and no longer invariant to hidden unit permutation. In addition to being concerned with a different type of neural network model, this work doesn\u2019t discuss the case of an unbounded and adaptive hidden layer size. Welling et al. (2003) proposed a self supervised boosting approach, which is applicable to the RBM and in which hidden units are sequentially added and trained. However, like boosting in general and unlike the iRBM, this procedure trains each hidden unit greedily instead of jointly, which could lead to much larger networks than necessary. Moreover, the procedure is not easily generalizable to online learning. While the work on unsupervised neural networks with adaptive hidden layer size is otherwise relatively scarse, there\u2019s been much more work in the context of supervised learning. There is the well known work of Fahlman & Lebiere (1990) on Cascade-Correlation networks.", "startOffset": 0, "endOffset": 965}, {"referenceID": 8, "context": "Rippel et al. (2014) showed that this defines a learning objective that makes the solution identifiable and no longer invariant to hidden unit permutation. In addition to being concerned with a different type of neural network model, this work doesn\u2019t discuss the case of an unbounded and adaptive hidden layer size. Welling et al. (2003) proposed a self supervised boosting approach, which is applicable to the RBM and in which hidden units are sequentially added and trained. However, like boosting in general and unlike the iRBM, this procedure trains each hidden unit greedily instead of jointly, which could lead to much larger networks than necessary. Moreover, the procedure is not easily generalizable to online learning. While the work on unsupervised neural networks with adaptive hidden layer size is otherwise relatively scarse, there\u2019s been much more work in the context of supervised learning. There is the well known work of Fahlman & Lebiere (1990) on Cascade-Correlation networks. More recently, Zhou et al. (2012) proposed a procedure for learning discriminative features with a denoising autoencoder (a model related to the RBM).", "startOffset": 0, "endOffset": 1032}, {"referenceID": 8, "context": "Rippel et al. (2014) showed that this defines a learning objective that makes the solution identifiable and no longer invariant to hidden unit permutation. In addition to being concerned with a different type of neural network model, this work doesn\u2019t discuss the case of an unbounded and adaptive hidden layer size. Welling et al. (2003) proposed a self supervised boosting approach, which is applicable to the RBM and in which hidden units are sequentially added and trained. However, like boosting in general and unlike the iRBM, this procedure trains each hidden unit greedily instead of jointly, which could lead to much larger networks than necessary. Moreover, the procedure is not easily generalizable to online learning. While the work on unsupervised neural networks with adaptive hidden layer size is otherwise relatively scarse, there\u2019s been much more work in the context of supervised learning. There is the well known work of Fahlman & Lebiere (1990) on Cascade-Correlation networks. More recently, Zhou et al. (2012) proposed a procedure for learning discriminative features with a denoising autoencoder (a model related to the RBM). The procedure is also applicable to the online setting. It relies on invoking two heuristics that either add or merge hidden units during training. We note that the iRBM framework could easily be generalized to discriminative and hybrid training as in Zhou et al. (2012). The corresponding mecanisms for adding and merging units would then be implicitly derived from gradient descent on the corresponding supervised training objective.", "startOffset": 0, "endOffset": 1420}, {"referenceID": 4, "context": "We compare the performance of the oRBM and the iRBM with the classic RBM on two datasets: binarized MNIST (Salakhutdinov & Murray, 2008) and CalTech101 Silhouettes (Marlin et al., 2010).", "startOffset": 164, "endOffset": 185}, {"referenceID": 4, "context": "CalTech101 Silhouettes The CalTech101 Silhouettes dataset4 (Marlin et al., 2010) is composed of 8,671 images of size 28x28 binary pixels, representing object silhouettes (101 classes).", "startOffset": 59, "endOffset": 80}, {"referenceID": 4, "context": "CalTech101 Silhouettes The CalTech101 Silhouettes dataset4 (Marlin et al., 2010) is composed of 8,671 images of size 28x28 binary pixels, representing object silhouettes (101 classes). The dataset is divided in three subsets: 4,100 examples for training, 2,264 for validation and 2,307 for testing. Each model was trained for 1000 epochs and the best results for the RBM, oRBM and iRBM are reported in Table 2. Again, the oRBM and the iRBM models reach competitive performance compared to the RBM. The best RBM with 500 hidden units was trained by Marlin et al. (2010) and uses the PCD instead of CD.", "startOffset": 60, "endOffset": 569}], "year": 2017, "abstractText": "We present a mathematical construction for the restricted Boltzmann machine (RBM) in which the hidden layer size is adaptive and can grow during training. This is obtained by first extending the RBM to be sensitive to the ordering of its hidden units. Then, thanks to a carefully chosen definition of the energy function, we show that the limit of infinitely many hidden units is well defined. As in a regular RBM, approximate maximum likelihood training can be performed, resulting in an algorithm that naturally and adaptively adds trained hidden units during learning. We empirically study the behaviour of this infinite RBM, showing that its performance is competitive to that of the RBM.", "creator": "LaTeX with hyperref package"}}}