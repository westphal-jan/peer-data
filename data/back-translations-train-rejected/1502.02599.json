{"id": "1502.02599", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction", "abstract": "We present a novel adaptive random subspace learning algorithm (RSSL) for prediction purpose. This new framework is flexible where it can be adapted with any learning technique. In this paper, we tested the algorithm for regression and classification problems. In addition, we provide a variety of weighting schemes to increase the robustness of the developed algorithm. These different wighting flavors were evaluated on simulated as well as on real-world data sets considering the cases where the ratio between features (attributes) and instances (samples) is large and vice versa. The framework of the new algorithm consists of many stages: first, calculate the weights of all features on the data set using the correlation coefficient and F-statistic statistical measurements. Second, randomly draw n samples with replacement from the data set. Third, perform regular bootstrap sampling (bagging). Fourth, draw without replacement the indices of the chosen variables. The decision was taken based on the heuristic subspacing scheme. Fifth, call base learners and build the model. Sixth, use the model for prediction purpose on test set of the data. The results show the advancement of the adaptive RSSL algorithm in most of the cases compared with the synonym (conventional) machine learning algorithms.", "histories": [["v1", "Mon, 9 Feb 2015 18:49:29 GMT  (144kb,D)", "http://arxiv.org/abs/1502.02599v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohamed elshrif", "ernest fokoue"], "accepted": false, "id": "1502.02599"}, "pdf": {"name": "1502.02599.pdf", "metadata": {"source": "META", "title": "Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction", "authors": ["Mohamed Elshrif", "Ernest Fokou\u00e9"], "emails": ["MME4362@RIT.EDU", "EPFEQA@RIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. \"(...)"}, {"heading": "2. Related Work", "text": "In fact, it is as if most people are able to understand themselves and understand what they are doing. (...) It is as if people are able to understand themselves and understand themselves. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. \"(...) It is as if they do not understand the world.\" (...) It is as if they cannot understand the world. \"(...)\" It is as if they do not understand the world. \""}, {"heading": "3. Adaptive RSSL", "text": "In this section, we present an adaptive random learning algorithm for the predictive problem. We begin by formulating the problem, followed by our proposed solution (proposed algorithm) to tackle (handle) it. A crucial step in assessing the candidate characteristics for building the models is explained in detail. Finally, we explain the strength of the new algorithm from a theoretical perspective."}, {"heading": "3.1. Problem formulation", "text": "As we have said before, our proposed method belongs to the category of random learning in the subspace, where each base learner is constructed based on a sample and a subset of the original p-characteristics, the main difference being that we use base learners, which usually do not lead to improvement when aggregated, and we also select characteristics based on weighting schemes inspired by the strength of the relationship between each characteristic and the answer (goal). Each base learner is driven by the subset {j (l) 1, \u00b7, j (l) d \u00b2 of the predictors randomly selected to build them, and the subsample D (l), which is substituted from D. For notoriously convenience, we use vectors of the indicator variables to name these two important quantities. The sample indicator \u03b4 (l) = (l) = (l), (l), (l), (l), (l), (l), (l), (l), (l), where we give (l), (l)."}, {"heading": "4. Experiments", "text": "For our experiments, we used a collection of simulated and real data sets. In addition, we used real data sets from previous work to solve the same problem for comparison purposes. We specified the mean square error (MSE) for each algorithm and task purpose, i.e. regression or classification."}, {"heading": "4.1. Simulated data sets", "text": "We have designed our artificial data sets based on the factors dimensionality of the data (number of features), number of samples (number of instances) and correlation of the data for six scenarios."}, {"heading": "4.2. Real data sets", "text": "For consistency and completeness, we select the real datasets, which have different characteristics in terms of number of instances and number of characteristics, as well as a variety of applications, and the real datasets can be represented by the task as follows: To show the performance of our developed model, we compare the accuracy of the RSSL with random forests and... with the same real datasets that they have previously used."}, {"heading": "5. Discussion", "text": "The reason for this is that only when the number of voters is greater, the random process of attribute selection can bring a good but unstable procedure to a significant step toward an optimal evaluation. Why was the training set in real data sets chosen so that it is large and in simulated data sets the test set so large that it can be large? The bootstrap sample was repeated 50 times. Table 1. Regression analysis: Mean Square Error (MSE) for different machine learning algorithms on different scenarios of synthetic datasets. WEIGHTING P MUNADIR 16.0 \u00b1 69 \u00b1 FORIVM \u00b1 69 \u00b1 92.0 \u00b1 91 (for our special dataset) yields \u00b1 92.81 \u00b1 92.0 \u00b1 82-3 \u00b1 82.0 ensembles that are highly precise and stable. The reason is that only when the number of voters is \"large enough\u00d3 67 scenario of attribute selection sucient quality number of different classifiers, how accurate are the classifiers and the accuracy?"}, {"heading": "6. Conclusion and Future Work", "text": "We presented a detailed quantitative analysis of the adaptive RSSL algorithm for an ensemble prediction problem. We support this analysis with an in-depth theoretical (mathematical) explanation (formulation). The main questions for the developed algorithm are based on four fundamental factors: generalization, flexibility, speed and accuracy. We explain each of these four factors. We present a rigorous theoretical justification for our proposed algorithm. Currently, we select a fixed number of attribute subsets. However, the algorithm should be evaluated based on performance (accuracy) to determine the appropriate number (dimension) for individual classifiers used in ensemble learning. In addition, the adaptive RSSL algorithm is tested on relatively small data sets. Our next step will be the application of the developed algorithm to large amounts of data. In addition, we show that adaptive RSSL works better than widely used ensemble algorithms, even taking into account the dependence on feature subsects."}], "references": [{"title": "Bio-molecular cancer prediction with random subspace ensembles of support vector machines", "author": ["Bertoni", "Alberto", "Folgieri", "Raffaella", "Valentini", "Giorgio"], "venue": null, "citeRegEx": "Bertoni et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bertoni et al\\.", "year": 2005}, {"title": "ISSN 0885-6125. doi: 10.1023/A: 1018054314350", "author": ["Breiman", "Leo"], "venue": "Bagging predictors. Machine Learning,", "citeRegEx": "Breiman and Leo.,? \\Q1996\\E", "shortCiteRegEx": "Breiman and Leo.", "year": 1996}, {"title": "Narrowing the gap: Random forests in theory and in practice", "author": ["Denil", "Misha", "Matheson", "David", "de Freitas", "Nando"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Experiments with a new boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": null, "citeRegEx": "Freund et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1996}, {"title": "A decisiontheoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Extremely randomized trees", "author": ["Geurts", "Pierre", "Ernst", "Damien", "Wehenkel", "Louis"], "venue": "Machine Learning,", "citeRegEx": "Geurts et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geurts et al\\.", "year": 2006}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Hansen and Salamon,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon", "year": 1990}, {"title": "Classifier combining: Analytical results and implications", "author": ["Tumer", "Kagan", "Ghosh", "Joydeep"], "venue": "Proceedings of the AAAI-96 Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms,", "citeRegEx": "Tumer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Tumer et al\\.", "year": 1995}, {"title": "Decimated input ensembles for improved generalization", "author": ["Tumer", "Kagan", "Oza", "Nikunj C"], "venue": null, "citeRegEx": "Tumer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Tumer et al\\.", "year": 1999}, {"title": "Improved customer choice predictions using ensemble methods", "author": ["van Wezel", "Michiel", "Potharst", "Rob"], "venue": "European Journal of Operational Research,", "citeRegEx": "Wezel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wezel et al\\.", "year": 2007}, {"title": "Stacked generalization", "author": ["Wolpert", "David H"], "venue": "Neural Networks,", "citeRegEx": "Wolpert and H.,? \\Q1992\\E", "shortCiteRegEx": "Wolpert and H.", "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": ", 2010) and DNA microarrays (Bertoni et al., 2005) data sets.", "startOffset": 28, "endOffset": 50}, {"referenceID": 5, "context": "(4) algorithms that manipulate the learning algorithm such as random forests (Breiman, 2001), neural networks ensemble (Hansen & Salamon, 1990), and extra-trees ensemble (Geurts et al., 2006).", "startOffset": 170, "endOffset": 191}, {"referenceID": 2, "context": "As (Denil et al., 2014) mentions that when building a random tree, there are three issues that should be decided in advance; (1) the leafs splitting method, (2) the type of predictor, and 3- the randomness method.", "startOffset": 3, "endOffset": 23}], "year": 2015, "abstractText": "We present a novel adaptive random subspace learning algorithm (RSSL) for prediction purpose. This new framework is flexible where it can be adapted with any learning technique. In this paper, we tested the algorithm for regression and classification problems. In addition, we provide a variety of weighting schemes to increase the robustness of the developed algorithm. These different wighting flavors were evaluated on simulated as well as on real-world data sets considering the cases where the ratio between features (attributes) and instances (samples) is large and vice versa. The framework of the new algorithm consists of many stages: first, calculate the weights of all features on the data set using the correlation coefficient and F-statistic statistical measurements. Second, randomly draw n samples with replacement from the data set. Third, perform regular bootstrap sampling (bagging). Fourth, draw without replacement the indices of the chosen variables. The decision was taken based on the heuristic subspacing scheme. Fifth, call base learners and build the model. Sixth, use the model for prediction purpose on test set of the data. The results show the advancement of the adaptive RSSL algorithm in most of the cases compared with the synonym (conventional) machine learning algorithms.", "creator": "LaTeX with hyperref package"}}}