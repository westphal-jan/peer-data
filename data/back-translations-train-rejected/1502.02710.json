{"id": "1502.02710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2015", "title": "Scalable Multilabel Prediction via Randomized Methods", "abstract": "We propose an efficient technique for multilabel classification based on calibration, a term we use to mean learning a link function that maps independent predictions to joint predictions. Though a naive implementation of our proposal would require training individual classifiers for each label, we show that for natural datasets and linear classifiers we can sidestep this by leveraging techniques from randomized linear algebra. Moreover, our algorithm applies equally well to multiclass classification. The end result is an algorithm that scales to very large multilabel and multiclass problems, and offers state of the art accuracy on many datasets.", "histories": [["v1", "Mon, 9 Feb 2015 22:18:26 GMT  (16kb)", "http://arxiv.org/abs/1502.02710v1", null], ["v2", "Mon, 20 Apr 2015 21:08:19 GMT  (44kb)", "http://arxiv.org/abs/1502.02710v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nikos karampatziakis", "paul mineiro"], "accepted": false, "id": "1502.02710"}, "pdf": {"name": "1502.02710.pdf", "metadata": {"source": "CRF", "title": "Multilabel Prediction via Calibration", "authors": ["Nikos Karampatziakis"], "emails": ["nikosk@microsoft.com", "pmineiro@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.02 710v 1 [cs.L G"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Background and Notation", "text": "At this point, we will present the notation and review RPCA, a technique that we will use and adapt in the sequel. (Halko et al., 2011) provides a thorough and formal presentation."}, {"heading": "2.1 Notation", "text": "We use n examples as matrices X, Rn and Y, Rn and C. We assume that the labels Y are sparse: each example is typically associated with only a handful of labels. We use | | X | | F for the Frobenius standard, E [\u00b7] for the expectation operator. Algorithm 1 Randomized PCA 1: Input: X, k,, q 2: Q borders (d, k +) 3: for i-Q {1,.., q} do 4: D-X XQ 5: Q \u00b2 orthogonize () 6: End for 7: F-Q (XQ) (XQ) 8: (V, 2) \u2190 eig (F, k) 9: V-QV-10: Return: (V,)"}, {"heading": "2.2 Randomized PCA", "text": "The algorithm is insensitive to the parameters and q as long as they are large enough (in our experiments we use = 20 and q = 1). We start with a set of k + random vectors and use them to probe the range of X X. Since fundamental eigenvectors can be thought of as \"common directions\" (Liberty, 2013), the range of iteration tends to align fairly with the space spanned by the uppermost eigenvectors of X X. We calculate an orthogonal basis for the range of and refine it by repeating q times. This can also be thought of as orthogonal (aka subspace) iteration to find eigenvectors with an early stop (i.e. q is small)."}, {"heading": "3 From Link Functions to Calibration", "text": "We start with the premise that in many cases certain label combinations often occur in common. We would like to take advantage of such co-occurrences, but we cannot go directly into modeling the output space, because the classifier can only reach this space by the characteristics it uses. Therefore, we will try to model correlations between the predictions. The easiest way to do this is to introduce a link function as an operator that enables a vector of activation p to a vector of prediction y = Cp. A very popular operator in setting the multiclass classification is softmax: g (p) = an unknown operator whose effect is to amplify the largest predictions while ensuring a probability distribution. In multi-label classification, we would like to introduce a non-oblivious operator, because we do not know which labels are often together.A first possibility is a linear operator."}, {"heading": "4 Dealing with Statistical Issues", "text": "Although we have started by assuming that certain combinations of labels (and activations) are more correlated than others, we have not yet clarified what the exact property we will exploit is. Correlations between activations are captured in their empirical second moment and our assumption is that this matrix has a low rank and can therefore be described by k-c eigenvectors. Furthermore, similar ideas, such as the assumption that the second moment of the labels is a low rank, have been used in other techniques for multi-label problems."}, {"heading": "5 Dealing with Computational Issues", "text": "There are three reactionary driving forces that are able to hide, and that are able to hide."}, {"heading": "XW = X(X\u22a4X)\u22121X\u22a4Y U = PU.", "text": "A few remarks are in order: To improve generalization performance, we use a regularized minimum square suitable for the RPCA (2014). Regularization is less critical (and sometimes harmful) for learning the final V matrix, but this is not surprising given the correspondence between label embedding and low-grade regularization Mineiro and Karampatziakis (2014). Another (implicit) parameter of the algorithm is the random distribution of vectors rt. This distribution defines the choice of the shift-invariant kernel that we use to measure similarities between activations p and Labels y. Fortunately, we can offer some guidance here by using the spectral properties of different shift-invariant nuclei (see also Le et al. (2013)."}, {"heading": "6 Related Work", "text": "Many calibration procedures Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al. (2011) have focused on binary classification and are now widely used in applications along with diagnostic tools such as calibration plots. Calibration for multicultural and multilabel classification has received little empirical attention. A notable exception for multicultural approaches is Zadrozny and Elkan (2002), which first produces calibrated probability estimates for induced binary problems and then combines these estimates into a definitive estimate of the posterior probability of each class. However, it is not clear how well the latest multicultural estimates are calibrated. In high dimensions, given the hardness results Hazan and Kakade (2012) and a lack of diagnostic tools, our approach follows a more pragmatic route: Choose a family of flexible linkage functions via a kernel machine parameter, then learn a good parameterization in this family."}, {"heading": "7 Experiments", "text": "We demonstrate the effectiveness of our approach on a variety of multi-label issues, as well as a 1000-class multiclass dataset, demonstrating the versatility of our approach. We use four small but standardized benchmark multi-label datasets (Bibtex, Delicious, Mediamill, and Corel5k), as well as RCV1 industries with high-dimensional inputs. Our multiclass dataset is ALOI Geusebroek et al. (2005). Table 1 lists the details of these datasets."}, {"heading": "7.1 Multilabel benchmarks", "text": "Our first four datasets are standard multi-label benchmarks. We compare our approach with PLST. For the datasets also used in Chen and Lin (2012), we also compare with CPLST. We set our parameters as follows: We use a k large enough to capture at least 99% of the variance of the predictions. We sample s = 4000 random vectors and distortions to approximate a Gaussian core with a bandwidth equivalent to the median distance between the projected activations of 300 examples. We use square loss for the final adjustment (in our experience, logistic loss works at least as well). Table 2 shows the average hammer loss on the test set for each dataset. The numbers in the first two columns are from Tai and Lin (2012) as well as Chen and Lin (2012). We also list the training times for these datasets in Table 3 on a standard PC. We see that calibration is not only a consistent, but also an improvement over the basic methods."}, {"heading": "7.2 Is it Just About Flexibility?", "text": "Our approach seems to work well in practice, but a reasonable question at this point is, where does the better performance come from? Since in the second step we have a two-step procedure with nonlinear characteristics, could we have obtained the same results just by using a much simpler method? Here, we illustrate that simple methods such as blindly using a core (approximation) directly on the inputs and independent prediction of the labels can lead to very different results from our prudent use of flexibility to model interlabel dependencies. We focus on the data set of the RCV1 industry, which has a small training set compared to its relatively larger number of characteristics. On this basis, we should expect that the naive application of flexible modeling can lead to a reduction in generalization performance. In fact, we conducted three experiments: learn individual logistic regressions to predict each of the 354 categories, learn individual logistic regressions regardless of the kernel approximation selected (as in the bandwidth of the experiment)."}, {"heading": "7.3 Comparison with Tree Approaches", "text": "In addition, FastXML is a multi-label ranking algorithm and not a multi-label classification algorithm, so the authors report only on precision at-k metrics. In Table 4, we list precision-1 on the test set for the subset of data sets used in this paper as well as in Prabhu and Varma (2014). For comparison with the prior state of the art, we also list another method, the MultiLabel Random Forest Agrawal et al. (2013). In Bibtex and Delicious, calibration beats MLRF and statistically ties in with FastXML. In Mediamill, calibration is not competitive with these methods, but it performs better than the other baselines listed in Prabhu and Varma (2014)."}, {"heading": "7.4 Multiclass Prediction", "text": "In this section, we present the results of a multi-class dataset to demonstrate the applicability of algorithm 3 to these settings. ALOI is a color image collection of one thousand small objects recorded for scientific purposes (Geusebroek et al., 2005). For these experiments, we consider the accuracy of the test classification using the same tensile-test split and the characteristics of Choromanska and Langford (2014). Specifically, there is a fixed tensile-test split of 90: 10 for all experiments and the representation is linear in 128 raw pixel values. We use logistic losses for the equation (3) of algorithm 3 and a dimensionality k of 50. The results, from top to bottom, indicate an improvement through the use of logistic regression over all; advantageous regulation effects through the composition of dimensionality reduction with logistic regression; and a significant improvement through the modelling of the core of pre-correlation with a flexible correlation."}, {"heading": "8 Conclusions", "text": "We call it calibration because it is inspired by binary classification calibration methods, although our method does not guarantee that the final predictions are actually calibrated or that the mapping learned is a monotonous operator. Nevertheless, our method works empirically better than many strong baselines, it is fast and scales to very large output ranges. In the future, we plan to investigate the applicability and effectiveness of calibration methods in more complex output ranges where the dependency structure of output variables is specified by a graphical model."}], "references": [{"title": "Least squares revisited: Scalable approaches for multi-class prediction", "author": ["A. Agarwal", "S.M. Kakade", "N. Karampatziakis", "L. Song", "G. Valiant"], "venue": "arXiv preprint arXiv:1310.1949.", "citeRegEx": "Agarwal et al\\.,? 2013", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma"], "venue": "Proceedings of the 22nd international conference on World Wide Web, pages 13\u201324. International World Wide Web Conferences Steering Committee.", "citeRegEx": "Agrawal et al\\.,? 2013", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Label embedding trees for large multiclass tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "Advances in Neural Information Processing Systems, pages 163\u2013171.", "citeRegEx": "Bengio et al\\.,? 2010", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "An extensive evaluation of decision tree\u2013based hierarchical multilabel classification methods and performance measures", "author": ["R. Cerri", "G.L. Pappa", "A.C.P. Carvalho", "A.A. Freitas"], "venue": "Computational Intelligence.", "citeRegEx": "Cerri et al\\.,? 2014", "shortCiteRegEx": "Cerri et al\\.", "year": 2014}, {"title": "Feature-aware label space dimension reduction for multi-label classification", "author": ["Chen", "Y.-N.", "Lin", "H.-T."], "venue": "Pereira, F., Burges, C., Bottou, L., and Weinberger, K., editors, Advances in Neural Information Processing Systems 25, pages 1529\u20131537. Curran Associates, Inc.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Logarithmic time online multiclass prediction", "author": ["A. Choromanska", "J. Langford"], "venue": "arXiv preprint arXiv:1406.1822.", "citeRegEx": "Choromanska and Langford,? 2014", "shortCiteRegEx": "Choromanska and Langford", "year": 2014}, {"title": "Efficient svm training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "The Journal of Machine Learning Research, 2:243\u2013264.", "citeRegEx": "Fine and Scheinberg,? 2002", "shortCiteRegEx": "Fine and Scheinberg", "year": 2002}, {"title": "The amsterdam library of object images", "author": ["Geusebroek", "J.-M.", "G.J. Burghouts", "A.W. Smeulders"], "venue": "International Journal of Computer Vision, 61(1):103\u2013112.", "citeRegEx": "Geusebroek et al\\.,? 2005", "shortCiteRegEx": "Geusebroek et al\\.", "year": 2005}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "Martinsson", "P.-G.", "J.A. Tropp"], "venue": "SIAM review, 53(2):217\u2013288.", "citeRegEx": "Halko et al\\.,? 2011", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "weak) calibration is computationally hard", "author": ["E. Hazan", "S. Kakade"], "venue": "arXiv preprint arXiv:1202.4478.", "citeRegEx": "Hazan and Kakade,? 2012", "shortCiteRegEx": "Hazan and Kakade", "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S. Kakade", "J. Langford", "T. Zhang"], "venue": "NIPS, volume 22, pages 772\u2013780.", "citeRegEx": "Hsu et al\\.,? 2009", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["S.M. Kakade", "V. Kanade", "O. Shamir", "A. Kalai"], "venue": "Advances in Neural Information Processing Systems, pages 927\u2013935.", "citeRegEx": "Kakade et al\\.,? 2011", "shortCiteRegEx": "Kakade et al\\.", "year": 2011}, {"title": "Fastfood\u2013approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "Proceedings of the international conference on machine learning.", "citeRegEx": "Le et al\\.,? 2013", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Simple and deterministic matrix sketching", "author": ["E. Liberty"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 581\u2013588. ACM.", "citeRegEx": "Liberty,? 2013", "shortCiteRegEx": "Liberty", "year": 2013}, {"title": "Fast label embeddings for extremely large output spaces", "author": ["P. Mineiro", "N. Karampatziakis"], "venue": "CoRR, abs/1412.6547.", "citeRegEx": "Mineiro and Karampatziakis,? 2014", "shortCiteRegEx": "Mineiro and Karampatziakis", "year": 2014}, {"title": "Predicting good probabilities with supervised learning", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 625\u2013632. ACM.", "citeRegEx": "Niculescu.Mizil and Caruana,? 2005", "shortCiteRegEx": "Niculescu.Mizil and Caruana", "year": 2005}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J Platt"], "venue": "Advances in large margin classifiers, 10(3):61\u201374.", "citeRegEx": "Platt,? 1999", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Y. Prabhu", "M. Varma"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.", "citeRegEx": "Prabhu and Varma,? 2014", "shortCiteRegEx": "Prabhu and Varma", "year": 2014}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in neural information processing systems, pages 1177\u20131184.", "citeRegEx": "Rahimi and Recht,? 2007", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Multilabel classification with principal label space transformation", "author": ["F. Tai", "Lin", "H.-T."], "venue": "Neural Computation, 24(9):2508\u20132542.", "citeRegEx": "Tai et al\\.,? 2012", "shortCiteRegEx": "Tai et al\\.", "year": 2012}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["P. Vincent"], "venue": "CoRR, abs/1412.7091.", "citeRegEx": "Vincent,? 2014", "shortCiteRegEx": "Vincent", "year": 2014}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "Proceedings of the 14th Annual Conference on Neural Information Processing Systems, number EPFL-CONF-161322, pages 682\u2013688.", "citeRegEx": "Williams and Seeger,? 2001", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers", "author": ["B. Zadrozny", "C. Elkan"], "venue": "ICML, volume 1, pages 609\u2013616. Citeseer.", "citeRegEx": "Zadrozny and Elkan,? 2001", "shortCiteRegEx": "Zadrozny and Elkan", "year": 2001}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 694\u2013699. ACM.", "citeRegEx": "Zadrozny and Elkan,? 2002", "shortCiteRegEx": "Zadrozny and Elkan", "year": 2002}], "referenceMentions": [{"referenceID": 14, "context": "For binary classification, several calibration algorithms have been proposed Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 14, "context": "For binary classification, several calibration algorithms have been proposed Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 11, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) and they all work by learning a link function that transforms the outputs of generic classifiers to calibrated probability estimates.", "startOffset": 35, "endOffset": 56}, {"referenceID": 11, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) and they all work by learning a link function that transforms the outputs of generic classifiers to calibrated probability estimates. Unlike binary classification where calibration has been well studied Niculescu-Mizil and Caruana (2005),", "startOffset": 35, "endOffset": 294}, {"referenceID": 9, "context": "As calibration in the sense of E[y|p] = p is generally computationally hard in high dimensions Hazan and Kakade (2012), for the rest of the paper we use the term calibration to only mean that we are learning a link function that jointly maps vectors to vectors.", "startOffset": 95, "endOffset": 119}, {"referenceID": 8, "context": "(Halko et al., 2011) provides a thorough and formal exposition.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Since principal eigenvectors can be thought as \u201cfrequent directions\u201d (Liberty, 2013), the range of \u03a8 will tend to align fairly with the space spanned by the top eigenvectors of XX .", "startOffset": 69, "endOffset": 84}, {"referenceID": 19, "context": "There are several techniques for doing this, including the Nystr\u00f6m method Williams and Seeger (2001), Incomplete Cholesky Factorization Fine and Scheinberg (2002), and Random Fourier Features (RFFs) Rahimi and Recht (2007).", "startOffset": 74, "endOffset": 101}, {"referenceID": 6, "context": "There are several techniques for doing this, including the Nystr\u00f6m method Williams and Seeger (2001), Incomplete Cholesky Factorization Fine and Scheinberg (2002), and Random Fourier Features (RFFs) Rahimi and Recht (2007).", "startOffset": 136, "endOffset": 163}, {"referenceID": 6, "context": "There are several techniques for doing this, including the Nystr\u00f6m method Williams and Seeger (2001), Incomplete Cholesky Factorization Fine and Scheinberg (2002), and Random Fourier Features (RFFs) Rahimi and Recht (2007). Here we will use RFFs which are especially easy to work with shift invariant kernels (i.", "startOffset": 136, "endOffset": 223}, {"referenceID": 14, "context": "We call the procedure of estimating the parameters vj calibration because it learns a link function similar to calibration techniques for binary classification including Platt scaling Platt et al. (1999) and isotonic regression Zadrozny and Elkan (2001).", "startOffset": 170, "endOffset": 204}, {"referenceID": 14, "context": "We call the procedure of estimating the parameters vj calibration because it learns a link function similar to calibration techniques for binary classification including Platt scaling Platt et al. (1999) and isotonic regression Zadrozny and Elkan (2001). We reiterate that, unlike isotonic regression for binary classification, we cannot guarantee that g(p) is calibrated in the sense of E[y|g(p)] = g(p) because of hardness results Hazan and Kakade (2012) when y is high dimensional.", "startOffset": 170, "endOffset": 254}, {"referenceID": 8, "context": "We reiterate that, unlike isotonic regression for binary classification, we cannot guarantee that g(p) is calibrated in the sense of E[y|g(p)] = g(p) because of hardness results Hazan and Kakade (2012) when y is high dimensional.", "startOffset": 178, "endOffset": 202}, {"referenceID": 0, "context": "Under some hard to verify conditions, specialized algorithms Agarwal et al. (2013) can yield a truly calibrated g(p) and these algorithms essentially perform many rounds of our procedure.", "startOffset": 61, "endOffset": 83}, {"referenceID": 0, "context": "Under some hard to verify conditions, specialized algorithms Agarwal et al. (2013) can yield a truly calibrated g(p) and these algorithms essentially perform many rounds of our procedure. In this work, however, we use a single stage of calibration which substantially simplifies the algorithm of Agarwal et al. (2013) and, as we will show later, already provides empirically superior performance over many strong baselines.", "startOffset": 61, "endOffset": 318}, {"referenceID": 0, "context": "Under some hard to verify conditions, specialized algorithms Agarwal et al. (2013) can yield a truly calibrated g(p) and these algorithms essentially perform many rounds of our procedure. In this work, however, we use a single stage of calibration which substantially simplifies the algorithm of Agarwal et al. (2013) and, as we will show later, already provides empirically superior performance over many strong baselines. At this point we have the two-stage procedure outlined in Algorithm 2. The loss function in the final fit can be either independent logistic regressions or least squares. The former usually requires a smaller s to attain the same result, while the latter admits a fast fitting procedure Vincent (2014) that is independent of the size of the output as long as the output is sparse.", "startOffset": 61, "endOffset": 726}, {"referenceID": 19, "context": "For square loss we can alternatively use the recent technique of Vincent (2014) that shows how to perform stochastic gradient updates for least squares problems when the output is large but sparse.", "startOffset": 65, "endOffset": 80}, {"referenceID": 8, "context": "Our procedure implicitly performs randomized PCA Halko et al. (2011) on the predictions without ever fitting individual classifiers.", "startOffset": 49, "endOffset": 69}, {"referenceID": 13, "context": "Regularization is less crucial (and sometimes detrimental) for learning the final V matrix, but this is not surprising given the correspondence between label embeddings and low-rank regularization Mineiro and Karampatziakis (2014). Another (implicit) parameter of the algorithm is the sampling distribution of the vectors rt.", "startOffset": 197, "endOffset": 231}, {"referenceID": 12, "context": "Fortunately, we can offer some guidance here using the spectral properties of various shift-invariant kernels (see also Le et al. (2013) for details).", "startOffset": 120, "endOffset": 137}, {"referenceID": 12, "context": "Fortunately, we can offer some guidance here using the spectral properties of various shift-invariant kernels (see also Le et al. (2013) for details). We recommend using Gaussian and Cauchy respectively for low and high dimensional y. These are special cases of the multivariate Student distribution with \u03bd = \u221e and \u03bd = 1 degrees of freedom. Intermediate values such as \u03bd = 3 and \u03bd = 5 can offer better results for medium dimensional y. The corresponding kernels are from the Mat\u00e9rn family. Some empirical support for their superiority on medium to high dimensional vectors is offered in Le et al. (2013).", "startOffset": 120, "endOffset": 604}, {"referenceID": 12, "context": "Many calibration procedures Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 12, "context": "Many calibration procedures Platt et al. (1999); Zadrozny and Elkan (2001); Kakade et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 8, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) have focused on binary classification and they are now widely used in applications together with diagnostic tools such as calibration plots.", "startOffset": 35, "endOffset": 56}, {"referenceID": 8, "context": "(1999); Zadrozny and Elkan (2001); Kakade et al. (2011) have focused on binary classification and they are now widely used in applications together with diagnostic tools such as calibration plots. Calibration for multiclass and multilabel classification has received little empirical attention. A notable exception for multiclass is Zadrozny and Elkan (2002) which first produces calibrated probability estimates for induced binary problems and then combines these estimates to a final estimate of the posterior probability of each class.", "startOffset": 35, "endOffset": 359}, {"referenceID": 8, "context": "In high dimensions, given hardness results Hazan and Kakade (2012) and a lack of diagnostic tools, our approach follows a more pragmatic route: select a family of flexible link functions via a kernel machine parameterization, then learn an efficient approximation to a good link function in that family using random features.", "startOffset": 43, "endOffset": 67}, {"referenceID": 8, "context": "In high dimensions, given hardness results Hazan and Kakade (2012) and a lack of diagnostic tools, our approach follows a more pragmatic route: select a family of flexible link functions via a kernel machine parameterization, then learn an efficient approximation to a good link function in that family using random features. Exploitation of the low-rank structure of predictions via dimensionality reduction is a popular technique in the multilabel literature. Hsu et al. (2009), motivated by advances in compressed sensing, utilized a random embedding of the labels along with sparse decoding strategy.", "startOffset": 43, "endOffset": 480}, {"referenceID": 2, "context": "Bengio et al. (2010) combined a tree based decomposition with a low-dimensional label embedding.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bengio et al. (2010) combined a tree based decomposition with a low-dimensional label embedding. For the multilabel case, the principal label space transform (PLST) Tai and Lin (2012) constructs a low-dimensional embedding using principal components on the empirical label covariance, which is then utilized along with a greedy sparse decoding strategy.", "startOffset": 0, "endOffset": 184}, {"referenceID": 3, "context": "Due to the richness of the literature, we refer the reader to a survey paper Cerri et al. (2014). Here we discuss FastXML Prabhu and Varma (2014), an multilabel tree ensemble method for which we have direct experimental comparisons.", "startOffset": 77, "endOffset": 97}, {"referenceID": 3, "context": "Due to the richness of the literature, we refer the reader to a survey paper Cerri et al. (2014). Here we discuss FastXML Prabhu and Varma (2014), an multilabel tree ensemble method for which we have direct experimental comparisons.", "startOffset": 77, "endOffset": 146}, {"referenceID": 7, "context": "Our multiclass dataset is ALOI Geusebroek et al. (2005). Table 1 lists the details of these datasets.", "startOffset": 31, "endOffset": 56}, {"referenceID": 16, "context": "Here we compare against a state of the art tree based approach, namely FastXML Prabhu and Varma (2014). FastXML uses tree ensembles so the cost of inference can be substantial.", "startOffset": 79, "endOffset": 103}, {"referenceID": 16, "context": "Here we compare against a state of the art tree based approach, namely FastXML Prabhu and Varma (2014). FastXML uses tree ensembles so the cost of inference can be substantial. Furthermore FastXML is a multilabel ranking algorithm not a multilabel classification algorithm, so the authors only report precision-at-k metrics. In table 4 we list precisionat-1 on the test set for the subset of datasets that were used in this work as well as in Prabhu and Varma (2014). For comparison with previous state of the art we also list another method, the MultiLabel Random Forest Agrawal et al.", "startOffset": 79, "endOffset": 467}, {"referenceID": 1, "context": "For comparison with previous state of the art we also list another method, the MultiLabel Random Forest Agrawal et al. (2013). On bibtex and delicious, calibration beats MLRF and statistically ties with FastXML.", "startOffset": 104, "endOffset": 126}, {"referenceID": 1, "context": "For comparison with previous state of the art we also list another method, the MultiLabel Random Forest Agrawal et al. (2013). On bibtex and delicious, calibration beats MLRF and statistically ties with FastXML. On Mediamill, calibration is not competitive with these methods but it does outperform the other baselines listed in Prabhu and Varma (2014). For these experiments we used logistic loss for the final fit as it optimizes a tighter bound on Hamming loss than squared loss.", "startOffset": 104, "endOffset": 353}, {"referenceID": 7, "context": "ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes (Geusebroek et al., 2005).", "startOffset": 97, "endOffset": 122}, {"referenceID": 5, "context": "For these experiments we will consider test classification accuracy utilizing the same train-test split and features from Choromanska and Langford (2014). Specifically there is a fixed train-test split of 90:10 for all experiments and the representation is linear in 128 raw pixel values.", "startOffset": 122, "endOffset": 154}], "year": 2017, "abstractText": "We propose an efficient technique for multilabel classification based on calibration, a term we use to mean learning a link function that maps independent predictions to joint predictions. Though a naive implementation of our proposal would require training individual classifiers for each label, we show that for natural datasets and linear classifiers we can sidestep this by leveraging techniques from randomized linear algebra. Moreover, our algorithm applies equally well to multiclass classification. The end result is an algorithm that scales to very large multilabel and multiclass problems, and offers state of the art accuracy on many datasets.", "creator": "LaTeX with hyperref package"}}}