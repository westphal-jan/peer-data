{"id": "1502.03248", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Off-Policy Reward Shaping with Ensembles", "abstract": "Potential-based reward shaping (PBRS) is an effective and popular technique to speed up reinforcement learning by leveraging domain knowledge. While PBRS is proven to always preserve optimal policies, its effect on learning speed is determined by the quality of its potential function, which, in turn, depends on both the underlying heuristic and the scale. Knowing which heuristic will prove effective requires testing the options beforehand, and determining the appropriate scale requires tuning, both of which introduce additional sample complexity. We formulate a PBRS framework that reduces learning speed, but does not incur extra sample complexity. For this, we propose to simultaneously learn an ensemble of policies, shaped w.r.t. many heuristics and on a range of scales. The target policy is then obtained by voting. The ensemble needs to be able to efficiently and reliably learn off-policy: requirements fulfilled by the recent Horde architecture, which we take as our basis. We demonstrate empirically that (1) our ensemble policy outperforms both the base policy, and its single-heuristic components, and (2) an ensemble over a general range of scales performs at least as well as one with optimally tuned components.", "histories": [["v1", "Wed, 11 Feb 2015 10:27:15 GMT  (510kb,D)", "http://arxiv.org/abs/1502.03248v1", "Short version at AAMAS-15, in submission to ALA-15"], ["v2", "Mon, 23 Mar 2015 13:35:59 GMT  (510kb,D)", "http://arxiv.org/abs/1502.03248v2", "To be presented at ALA-15. Short version to appear at AAMAS-15"]], "COMMENTS": "Short version at AAMAS-15, in submission to ALA-15", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["anna harutyunyan", "tim brys", "peter vrancx", "ann nowe"], "accepted": false, "id": "1502.03248"}, "pdf": {"name": "1502.03248.pdf", "metadata": {"source": "CRF", "title": "Off-Policy Reward Shaping with Ensembles", "authors": ["Anna Harutyunyan", "Tim Brys", "Peter Vrancx"], "emails": ["aharutyu@vub.ac.be", "tbrys@vub.ac.be", "pvrancx@vub.ac.be", "anowe@vub.ac.be"], "sections": [{"heading": null, "text": "We are formulating a PBRS framework that reduces the speed of learning but does not entail additional sample complexity. To this end, we propose to simultaneously learn an overall set of strategies that is characterized by many heuristics and on different scales. Target policy is then achieved through coordination; the overall ensemble must be able to learn efficiently and reliably outside politics: requirements that are met by the latest Horde architecture that we base on. We demonstrate empirically that (1) our ensemble policy outperforms both basic policy and its individual heuristic components, and (2) an overall ensemble functions at least as well on a general scale as one with optimally coordinated components. Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: LearningGeneral Terms Algorithms, Experimental Keywords reinforcement learning, potential-based reward shaping, offlearning policy, horde"}, {"heading": "1. INTRODUCTION", "text": "eDi eeisrcnlhsrteeSrlteeeeoiuiiiiiiietlrcnlhsrteeoiiiiiiiiiiiiiiiiiiiiiiiiteteteteteteetersrsrsrsrsrsrrsrsrsrrsrrsrrrrsrsrrsrsrrrsrrrrsrsrrrrrsrrrrsrrrrsrrsrrrsrrsrrrsreteeteeteeterrrsrrsrrrrrrrrrsrrsrsreteeteeteeteeteeteerrrrrsrrsrsrreteeteeteeteeteeteeteeteerrrrsrsrsrrsrrrreteeteeteeteeteeteeteeteeteeteerrrrrrsrsrsrrsrrrsreteeteeteeteeteeteeteeteeteerrrrrsreteeteeteerrrrrrsrsrsrrsrsrrsrsreteeteeteeteeteerrrrrsrsrsrsrsrrsrsrsreteeteeteeteeteerrrrrsrsrsrrsrsrsrsrrsrsrsreteeteeteeteeteeteerrrrrrrsrsrsrsrsrsrsreteeteeteeteeteerrrrrrrrrsrsrrrrrrrrrrreteeteeteeteerrrrrrrrreteeteeteerrrrrre"}, {"heading": "2. BACKGROUND", "text": "We assume that the usual RL framework [25], in which the agent interacts with his (typically) Markovian environment in discrete time steps = Q = Q function = Q function in discrete time steps t = 1, 2,... Formally, a Markov decision process (MDP) [22] is a tuple M = < S, A, \u03b3, T, R >, where: S is a set of states, A is a set of measures, \u03b3 [0, 1] is the discount factor, T = {Psa (\u00b7) | s, A} are the next state transition probabilities with Psa (s \u00b2), specifying the probability of the state probability of states s \u00b2 s occurring after actions are taken, R: S \u00d7 A \u00d7 S \u2192 R is the reward function with R (s, s \u00b2), which gives the expected value of the reward we receive when a reward is received."}, {"heading": "2.1 Horde", "text": "It is well known that off-policy bootstrapping methods (such as Q-learning) cause even simple technical problems to deviate [2, 28]. The family of gradient time difference (GTD) methods provides a solution to this problem and guarantees off-policy convergence under FA, as a fixed (or slowly changing behavior) [26]. Previously, similar guarantees were provided only by second-order batch methods (e.g. LSTD [3]) that are unsuitable for online learning. GTD methods are the first to maintain these guarantees, while the (time and space) complexity remains linear in the size of government space. Note that linearity represents a lower limit to what is feasible because it is required to easily store and access learning vectors. As a consequence, GTD methods scale well to the number of value functions (policies) that are learned, and due to the inherent off-policy setting, can do so."}, {"heading": "2.2 Reward Shaping", "text": "The reward form expands the true reward signal R with an additional reward provided by the designer F. The reward form is designed to guide the agent when the reward function is low or less informative in order to speed up learning. In its most general form: R \u00b2 = R + F (9) Since tasks are identified by their reward function, the reward function must be performed cautiously so as not to change the task, or else the reward design may slow down or even prevent finding the optimal policy [23]. Ng et al. [20] show that the grounding of the reward function in state potentials is both necessary and sufficient to ensure the preservation of the (optimal) policy of the original MDP. Potential reward design (PBRS) maintains a potential function: S \u2192 R, and defines the additional reward function F as: F (s, a, s \u2032) = interchangeable ()."}, {"heading": "3. A HORDE OF SHAPINGS", "text": "The key findings in the field of ensemble creation are that the strength of an ensemble lies in the diversity of its components [11]. In the context of the RL, this diversity can be expressed through several aspects related to the dimensions of the learning process: (1) diversity of experience, (3) diversity of reward signals, (4) high complexity and (5) the assumption that it is a multi-level setup or that it is a multi-level learning. (5)"}, {"heading": "4. ARCHITECTURE", "text": "We are now ready to describe the architecture of our ensemble (Fig. 1). We maintain our horde of design possibilities as a series of greed GQ (\u03bb) learners [14]. In view of a number of potential functions, the ensemble reward is a vector: R = R + < F\u03a61 c11, F\u03a61 c12,..., F \u0445 \"c'k\" > (11), where F\u03a6i cij is the potential design reward given by equation. (10) w.r.t. the potential function \u03a6i and scaled with the factor cij. To get the notorious clarity, we use F i'j to mean that F\u0441i'k '> (11) is the potential design reward given by equation. (10) w.r.t. the potential function and scaled with the factor cij."}, {"heading": "4.1 Ensemble Policy", "text": "To the best of my knowledge, both voting methods were first used in the context of the Wiering and Van Hasselt RL agents [31]. In both methods, each demon d casts one vote vd: S \u00b7 A \u2192 N0, s.t. vd (s, a) is the preference value of the action a. The voting scheme is then defined for policies and not for value functions, which mitigates the size differences. [5] The voting scheme determines the way vd is allocated. Majority voting Each demon casts a vote of 1 for its most preferred action, and a vote of 0 for the others. [i.e.: vd (s, a), a vote of A (12). The voting scheme determines the way vd is allocated."}, {"heading": "5. EXPERIMENTS", "text": "We now present the empirical studies that confirm the effectiveness of our ensemble architecture with appropriate scaling factors. Experiments show that ensemble policy works at least as well as the best heuristics. We then turn to the problem of scaling and show that ensembles on narrow and wide scale ranges work at least as well as the optimal scaling factors. We conduct our experiments on two common benchmark problems. In both problems, the behavioural policy is a uniform distribution across all measures in each time step. The evaluation is made by interrupting each z episode and executing the requested greedy policy once. No learning is permitted during the evaluation. We evaluated the ensembles with the same voting scheme from paragraph 4.1 and found that the (sum) performance is not significantly different (p > 0.05), with the emphasis not solely on voting."}, {"heading": "5.1 Mountain Car", "text": "The task is to drive an underpowered car up a hill (= 1). The (continuous) state of the system is composed of the current position (in [\u2212 1,2, 0,6]) and the current speed (in [\u2212 0,07, 0,07]) of the car."}, {"heading": "5.2 Cart-Pole", "text": "The task is to balance a pole on the top of a moving carriage for as long as possible. There are two actions: a small positive force and a small negative force applied to the carriage. A pole falls when the sequence ends. The track is limited within [\u2212 4], but the sides are \"soft\"; the art does not crash when it is hit. The reward function penalizes a pole drop and is 0 elsewhere. An episode ends successfully when the pole is balanced for 1000 steps. State space is approached with tile coding, using ten tilts of 10 \u00d7 10 across all 4 dimensions, learning a parameter vector for each activity."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we have described a novel non-political PBRS ensemble architecture that is able to reduce the learning speed in a latent environment without requiring the additional sampling complexity introduced by the steps to align heuristics and their scale, typical of PBRS. We avoid these steps by simultaneously learning an overall political ensemble with many heuristic and scaling factors. Our ensemble is the first of its kind that has general convergence guarantees while remaining efficient, using the latest Horde architecture to learn a single task well. Our experiments confirm the use of PBRS in the latent environment and demonstrate the effectiveness of the proposed ensemble."}, {"heading": "7. REFERENCES", "text": "[1] J. Asmuth, M. L. Littman, and R. Zinkov.Potential-based shaping in model-based reinforcement learning. In Proceedings of AAAI, pp. 604-609, 2008. [2] L. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proceedings of ICML, pp. 30-37, 1995. [3] S. J. Bradtke, and M. G. Barto, and P. Kaelbling. Linear least-squares algorithms for temporal difference learning. In Machine Learning, pp. 22-33, 1996. [4] T. Brys, A. Nowe, D. Kudenko, and M. Taylor. Combining multiple correlated reward shaping signals by measuring confidence. In Proceedings of AAAI, 2014. [5] S. Devlin, D. Kudenko, and M. Grzes. An empirical study of potential-based reward shaping and advice in complex, multi-agent systems."}], "references": [{"title": "Potential-based shaping in model-based reinforcement learning", "author": ["J. Asmuth", "M.L. Littman", "R. Zinkov"], "venue": "Proceedings of AAAI, pages 604\u2013609", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Proceedings of ICML, pages 30\u201337", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto", "P. Kaelbling"], "venue": "Machine Learning, pages 22\u201333", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Combining multiple correlated reward shaping signals by measuring confidence", "author": ["T. Brys", "A. Now\u00e9", "D. Kudenko", "M.E. Taylor"], "venue": "Proceedings of AAAI", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical study of potential-based reward shaping and advice in complex", "author": ["S. Devlin", "D. Kudenko", "M. Grzes"], "venue": "multi-agent systems. Advances in Complex Systems (ACS), 14(02):251\u2013278", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving Exploration in Reinforcement Learning through Domain Knowledge and Parameter Analysis", "author": ["M. Grzes"], "venue": "PhD thesis, University of York", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning of shaping rewards in reinforcement learning", "author": ["M. Grzes", "D. Kudenko"], "venue": "Neural Networks, 23(4):541 \u2013 550", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Off-policy shaping ensembles in reinforcement learning", "author": ["A. Harutyunyan", "T. Brys", "P. Vrancx", "A. Now\u00e9"], "venue": "Proceedings of ECAI, pages 1021\u20131022", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M.I. Jordan", "S.P. Singh"], "venue": "Neural computation, 6(6):1185\u20131201", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "The fixed points of off-policy td", "author": ["J.Z. Kolter"], "venue": "Advances in Neural Information Processing Systems, pages 2169\u20132177", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural network ensembles", "author": ["A. Krogh", "J. Vedelsby"], "venue": "cross validation, and active learning. In Advances in Neural Information Processing Systems, pages 231\u2013238. MIT Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "The influence of reward on the speed of reinforcement learning: An analysis of shaping", "author": ["A. Laud", "G. DeJong"], "venue": "Proceedings of ICML. AAAI Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H. Maei"], "venue": "PhD thesis, University of Alberta", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H. Maei", "R. Sutton"], "venue": "Proceedings of the Third Conf. on Artificial General Intelligence.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward off-policy learning control with function approximation", "author": ["H. Maei", "C. Szepesv\u00e1ri", "S. Bhatnagar", "R. Sutton"], "venue": "J. F\u00fcrnkranz and T. Joachims, editors, Proceedings of ICML 2010, pages 719\u2013726", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "An ensemble of linearly combined reinforcement-learning agents", "author": ["V. Marivate", "M. Littman"], "venue": "AAAI Workshops", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic shaping and decomposition of reward functions", "author": ["B. Marthi"], "venue": "Proceedings of ICML, ICML \u201907, pages 601\u2013608, New York, NY, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Boxes: An experiment in adaptive control", "author": ["D. Michie", "R.A. Chambers"], "venue": "E. Dale and D. Michie, editors,  Machine Intelligence. Oliver and Boyd, Edinburgh, UK", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1968}, {"title": "Acquiring a broad range of empirical knowledge in real time by temporal-difference learning", "author": ["J. Modayil", "A. White", "P.M. Pilarski", "R.S. Sutton"], "venue": "Systems, Man, and Cybernetics (SMC), 2012 IEEE International Conference on, pages 1903\u20131910. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "In Proceedings of ICML, pages 278\u2013287. Morgan Kaufmann", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Adaptive artificial limbs: a real-time approach to prediction and anticipation", "author": ["P. Pilarski", "M. Dawson", "T. Degris", "J. Carey", "K. Chan", "J. Hebert", "R. Sutton"], "venue": "Robotics Automation Magazine,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": "John Wiley & Sons, Inc., New York, NY, USA, 1st edition", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning to drive a bicycle using reinforcement learning and shaping", "author": ["J. Randl\u00f8v", "P. Alstr\u00f8m"], "venue": "Proceedings of ICML", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning potential functions and their representations for multi-task reinforcement learning", "author": ["M. Snel", "S. Whiteson"], "venue": "Autonomous Agents and Multi-Agent Systems, 28(4):637\u2013681", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement learning: An introduction", "author": ["R. Sutton", "A. Barto"], "venue": "volume 116. Cambridge Univ Press", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R. Sutton", "H. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "In Proceedings of ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P. Pilarski", "A. White", "D. Precup"], "venue": "Proceedings of AAMAS, pages 761\u2013768, Richland, SC", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B.V. Roy"], "venue": "Technical report, IEEE Transactions on Automatic Control", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Insights in reinforcement learning : formal analysis and empirical evaluation of temporal-difference learning algorithms", "author": ["H. van Hasselt"], "venue": "PhD thesis, Utrecht University,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Q-learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3):272\u2013292", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Ensemble algorithms in reinforcement learning. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["M. Wiering", "H. van Hasselt"], "venue": "IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "The powerful ability of reinforcement learning (RL) [25] to find optimal policies tabula rasa, is also the source of its main weakness: infeasibly long running times.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "One paradigm to inject such knowledge into the reinforcement learning problem is potential-based reward shaping (PBRS) [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 4, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 3, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 23, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 19, "context": "Moreover, it is the only reward shaping scheme that is guaranteed to do so [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "The pursuit of designing a potential function that accurately encapsulates the \u201ctrue\u201d desirability is meaningless, as it would solve the task at hand [20], and remove the need for learning altogether.", "startOffset": 150, "endOffset": 154}, {"referenceID": 3, "context": "Moreover, heuristics may contribute complementary knowledge that cannot be leveraged in isolation [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 25, "context": "The recently introduced Horde architecture [26] is well-suited to be the basis of our ensemble, due to its general off-policy convergence guarantees and computational efficiency.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "In contrast to the previous uses of Horde [21], we exploit its power to learn a single task, but from multiple viewpoints.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "The convergence guarantees of Horde require a latent learning scenario [15], i.", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "Note that the conventional interpretation of PBRS to steer exploration [6], does not apply here, as the behavior is unaffected by the target policy, and is kept fixed.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "This work (and its precursor [8]) provides, to our knowledge, the first validation of PBRS effective in such a latent setting.", "startOffset": 29, "endOffset": 32}, {"referenceID": 24, "context": "We assume the usual RL framework [25], in which the agent interacts with its (typically) Markovian environment at discrete time steps t = 1, 2, .", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "Formally, a Markov Decision Process (MDP) [22] is a tuple M = \u3008S,A, \u03b3, T , R\u3009, where: S is a set of states, A is a set of actions, \u03b3 \u2208 [0, 1] is the discounting factor, T = {Psa(\u00b7)|s \u2208 S, a \u2208 A} are the next state transition probabilities with Psa(s \u2032) specifying the probability of state s\u2032 occuring upon taking action a from state s, R : S \u00d7 A \u00d7 S \u2192 R is the reward function with R(s, a, s\u2032) giving the expected value of the reward that will be received when a is taken in state s, and rt+1 denoting the component of R at time t.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "Formally, a Markov Decision Process (MDP) [22] is a tuple M = \u3008S,A, \u03b3, T , R\u3009, where: S is a set of states, A is a set of actions, \u03b3 \u2208 [0, 1] is the discounting factor, T = {Psa(\u00b7)|s \u2208 S, a \u2208 A} are the next state transition probabilities with Psa(s \u2032) specifying the probability of state s\u2032 occuring upon taking action a from state s, R : S \u00d7 A \u00d7 S \u2192 R is the reward function with R(s, a, s\u2032) giving the expected value of the reward that will be received when a is taken in state s, and rt+1 denoting the component of R at time t.", "startOffset": 135, "endOffset": 141}, {"referenceID": 0, "context": "A (stochastic) Markovian policy \u03c0 : S \u00d7 A \u2192 [0, 1] is a probability distribution over actions at each state, s.", "startOffset": 44, "endOffset": 50}, {"referenceID": 29, "context": "Given \u03c0b, the values of the optimal greedy policy can be learned incrementally through the following Q-learning [30] update:", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "Qt in st+1 Given tabular representation, this process is shown to converge to the correct value estimates (the TD-fixpoint) in the limit under standard approximation conditions [9].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "FA is known to cause off-policy bootstrapping methods (such as Q-learning) to diverge even on simple problems [2, 28].", "startOffset": 110, "endOffset": 117}, {"referenceID": 27, "context": "FA is known to cause off-policy bootstrapping methods (such as Q-learning) to diverge even on simple problems [2, 28].", "startOffset": 110, "endOffset": 117}, {"referenceID": 25, "context": "The family of gradient temporal difference (GTD) methods provides a solution for this issue, and guarantees offpolicy convergence under FA, given a fixed (or slowly changing behavior) [26].", "startOffset": 184, "endOffset": 188}, {"referenceID": 2, "context": "LSTD [3]),", "startOffset": 5, "endOffset": 8}, {"referenceID": 18, "context": "As a consequence, GTD methods scale well to the number of value functions (policies) learnt [19], and due to the inherent off-policy setting, can do so from a single stream of environment interactions (or experience).", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "[27] formalize this idea in a framework of parallel offpolicy learners, called Horde.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "There have been further successful applications of Horde in realistic robotic setups [21].", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "On the technical level, GTD methods are based on the idea of performing gradient descent on a reformulated objective function, which ensures convergence to the projected TD-fixpoint, by introducing a gradient bias into the TDupdate [26].", "startOffset": 232, "endOffset": 236}, {"referenceID": 25, "context": "This is a simpler form of the GTD-update, namely that of TDC [26].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "GQ(\u03bb) [14] augments this update with eligibility traces.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "In, to our knowledge, the only work that addresses this issue, Kolter [10] gives a way of constraining the solution space to achieve stronger qualitative guarantees, but his algorithm has quadratic complexity and thus is not scalable.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Because tasks are identified by their reward function, modifying the reward function needs to be done with care, in order to not alter the task, or else reward shaping can slow down or even prevent finding the optimal policy [23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 19, "context": "[20] show that grounding the shaping rewards in state potentials is both necessary and sufficient for ensuring preservation of the (optimal) policies of the original MDP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Please refer to Maei\u2019s dissertation for the full details [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "The key insight in ensemble learning is that the strength of an ensemble lies in the diversity its components contribute [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "Maintaining the shapings separately has recently been shown to be a more robust and effective approach [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 26, "context": "Horde\u2019s demonstrated ability to learn thousands of policies in parallel in real time [27, 19] allows to consider large ensembles, at little computational cost.", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "Horde\u2019s demonstrated ability to learn thousands of policies in parallel in real time [27, 19] allows to consider large ensembles, at little computational cost.", "startOffset": 85, "endOffset": 93}, {"referenceID": 5, "context": "The effects of PBRS on the learning process are usually considered to lie in the guidance of exploration during learning [6, 17, 20].", "startOffset": 121, "endOffset": 132}, {"referenceID": 16, "context": "The effects of PBRS on the learning process are usually considered to lie in the guidance of exploration during learning [6, 17, 20].", "startOffset": 121, "endOffset": 132}, {"referenceID": 19, "context": "The effects of PBRS on the learning process are usually considered to lie in the guidance of exploration during learning [6, 17, 20].", "startOffset": 121, "endOffset": 132}, {"referenceID": 11, "context": "Laud and DeJong [12] formalize this by showing that the difficulty of learning is most dependent on the reward horizon, a measure of the number of decisions a learning agent must make before experiencing accurate feedback, and that reward shaping artificially reduces this horizon.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "2 of van Hasselt\u2019s dissertation [29].", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "We maintain our Horde of shapings as a set D of Greedy-GQ(\u03bb)-learners [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "[27], and refer to individual agents within Horde as demons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Note that because PBRS preserves all of the optimal policies from the original problem [20], the ensemble policy does too.", "startOffset": 87, "endOffset": 91}, {"referenceID": 30, "context": "To the best of our knowledge, both voting methods were first used in the context of RL agents by Wiering and Van Hasselt [31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "We slightly modify the formulation from [31], by ranking Q-values, instead of policy probabilities.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "We begin with the classical benchmark domain of mountain car [25].", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "The state space is approximated with the standard tile-coding technique [25], using ten tilings of 10\u00d710, with a parameter vector learnt for each action.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "Here x = \u3008x, \u1e8b\u3009 is the state (position and velocity), and \u0101 denotes the normalization of a onto [0, 1].", "startOffset": 96, "endOffset": 102}, {"referenceID": 7, "context": "This experiment first appeared in the early version of this work [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 17, "context": "We now validate our framework on the problem of cartpole [18].", "startOffset": 57, "endOffset": 61}], "year": 2017, "abstractText": "Potential-based reward shaping (PBRS) is an effective and popular technique to speed up reinforcement learning by leveraging domain knowledge. While PBRS is proven to always preserve optimal policies, its effect on learning speed is determined by the quality of its potential function, which, in turn, depends on both the underlying heuristic and the scale. Knowing which heuristic will prove effective requires testing the options beforehand, and determining the appropriate scale requires tuning, both of which introduce additional sample complexity. We formulate a PBRS framework that reduces learning speed, but does not incur extra sample complexity. For this, we propose to simultaneously learn an ensemble of policies, shaped w.r.t. many heuristics and on a range of scales. The target policy is then obtained by voting. The ensemble needs to be able to efficiently and reliably learn off-policy: requirements fulfilled by the recent Horde architecture, which we take as our basis. We demonstrate empirically that (1) our ensemble policy outperforms both the base policy, and its single-heuristic components, and (2) an ensemble over a general range of scales performs at least as well as one with optimally tuned components.", "creator": "LaTeX with hyperref package"}}}