{"id": "1502.03505", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices", "abstract": "Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for Symmetric Positive Definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.", "histories": [["v1", "Thu, 12 Feb 2015 01:38:36 GMT  (1831kb,D)", "http://arxiv.org/abs/1502.03505v1", "19 pages, 6 figures, 3 tables"]], "COMMENTS": "19 pages, 6 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["florian yger", "masashi sugiyama"], "accepted": false, "id": "1502.03505"}, "pdf": {"name": "1502.03505.pdf", "metadata": {"source": "CRF", "title": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices", "authors": ["Florian Yger", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is an important problem that occurs in many mechanical tutorials. < p > p > p > p > p > p (0) p (0) p > p (0) p (0) p (0) p (0) p (0) p (0) p (0) p (0) p) p (0) p (0) p) p (0) p (0) p (0) p (0) p (0) p (p) p (0) p p p p (0) p p p) p (0) p p p p) p (0) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "2 Proposed approach", "text": "In this section we describe our proposed metric learning method."}, {"heading": "2.1 Formulation of metric learning under LogEuclidean distance", "text": "A matrix A is positively determined (A 0) when x > Ax > 0 is parameterized for all non-zero x-SPD matrices by Pd.In order to learn a metric, we have to parameterize a distance. In this paper, according to Bhatia (2009), we focus on the congruent transformation, i.e. for all A, G-Pd, {Pd 7 \u2192 Pd-G \u2212 1 2: A \u2192 G-12AG \u2212 12. Thus, the logEuclidean distance (2) is parameterized using the G-Pd-asGl (A, B) = 3-Pd-Pd (A) = 2 (A)."}, {"heading": "2.2 Metric learning with kernel-target alignment", "text": "For metric learning, we use the (centered) Core Target Alignment (KTA) > Problem (KTA) (X \u2032 r) naive (.eef et al. (2001); Cortes et al. (2012): A (K, K?) = < UKU, UK? U > F * UKU * F, where K is a matrix to align, K? = yy T is a target matrix, y = (y1,.., yn) T * Rn, U = Id \u2212 1d1 > dd is the centering matrix, Id is the identity matrix of size d \u2212 d, and 1d is the d \u2212 dimensional vector with all. To make it easier, we assume that y is centered, i.e., UyTU \u2192 yT \u2192 yT below.Let kG (X, X \u2032 r) Uliv is the logeuclimatic metric from which we would derive the climatic distance Eudean \u00b2 G \u00b2 (G-G \u00b2 2)."}, {"heading": "2.3 Riemanian Gradient Optimization", "text": "In this year it is so far that it is able to put itself at the top, namely in the manner and manner in which it has been the case in recent years: in the way and manner in which it is able to do so, in the way in which it is able to put itself at the top, in the way in which it is able to move, in the way in which it is able to be in the position in which it is able to put itself at the top, in the way in which it is able to be in the position in which it is able to move, in the way in which it is able to be in the position in which it is able, in the way in which it is able to put itself at the top, in the way in which it is in the way in which it is able to be in the way in which it is in the way in which it is able to move, in the way in which it is able to be in the way in which it is in the way in which it is able to be in the way in which it is in the way in which it is able to be in the way in which it is in the way in which it is in the way in which it is in the way in which it is in the way in which it is in the way in which it is in the way in which it is in which it is able to be in the it in the way in the way in which it is in which it is in the way in which it is in the way in which it is in which it is able to be in the way in which it in the way in which it is in the way in which it in which it is in which it is in the it in the way in which it is in the it in the way it in which it is in which it is in which it is in the it is in which it is in the way it in which it is in the way it in which it is in which it in the it is in the way it is in which it is in which it is in the way it in which it is in which it is in the way it in which it is in the way it is in which it is in which it is in the way it in which it is in the way it in which it is in the way it is in which it is in the way it in which it is in which it in which it is in the way it is in which it"}, {"heading": "3 Discussions", "text": "In this section we encourage you to use the logEuclidean distance and compare it with other distances in Pd."}, {"heading": "3.1 Geometries on Pd", "text": "As stated in the literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al. (2011); Cherian et al. (2011); various tools come with various implicit invariance properties and computational complexities.3Further details on optimization for Riemannian matrix manifolds and the geometry of Pd can be found in Absil et al. (2009) and Bhatia (2009). As explained in Bhatia (2009) when the geometry is implied in Eq. 7, the distance between two SPD matrices A and B along a curve is compiled (referred to as geodesic) and the associated affine-invariant Riemannian metric (AIRM)."}, {"heading": "3.2 Interpretations of LogEuclidean metric learning", "text": "In fact, we will be able to find the path to the future that will lead us into the future, \"he said."}, {"heading": "3.3 Geometrical motivation", "text": "The deformation implied by the imaging of points in a tangent space was first stated as an exponential metric increasing (EMI) property in Bhatia (2009). It gives inequalities between the Rieman distance \u03b4r and the Euclidean distance \u03b4e in the tangent space in the identity Id.Theorem 1. Bhatia (2009) For each pair of points A, B in Pd we have an inequality between the points A, B in Pd. This property, inherited from the nonpositive curvature of Pd, implies a deformation of the shapes when using the logarithmic map. The use of the properties of the bijection group G \u2212 1 2. Equality occurs when A and Id belong to the same geodesy. \u2212 This property, inherited from the nonpositive curvature of Pd, implies a deformation of the shapes when using the logarithmic map."}, {"heading": "4 Numerical experiments", "text": "In this section we report on experimental results."}, {"heading": "4.1 Setup", "text": "In order to implement our approach, we used a Rieman fiduciary region5 Boumal & Absil (2011); Absil et al. (2009). In practice, we use the following regulated optimization problem: {max G-Pd f (G) s.t. \u03b4r (G0, G) \u2264 for any G0. Since we have found no guarantee regarding convexity or geodetic-convective convexity Wiesel (2012) of the centered KTA, we decided to use the Rieman mean Moakher (2005); Jeuris et al. (2012) as G0. Below, we set = 10 for all datasets.5 with the implementation of the Manopt toolbox provided by Boumal et al. (2014). In our numerical experiments, we report accuracy on balanced datasets based on a 1-nearest neighbor (1-NN) equipped with different distances."}, {"heading": "4.2 Toy dataset", "text": "First, in order to gain some insight into the behaviour of our approach, we have designed a simple experiment in which the covariance matrices are generated as follows: X = Qdiag (\u03bb1,.., \u03bbr, \u00b51,..., \u00b5r) Q > + V diag (| 1 |,.., 2r |) V > with Q and V, two random orthonormal square matrices, and \u03bbi \u0445 N (5, 0.2) and \u03bbi \u0445 N (4, 0.1) respectively for the positive and negative classes and \u00b5i \u0445 U ([1, 6]) and i \u0445 N (0, 1) independently of the class. The data set consists of 50 samples in the training set and 500 samples in the test set. It is re-sampled to repeat the experiment ten times and the results are shown in Tab. 1. In this simple experiment, the euclimatic distance between the individual classes is predictably separated by the rivalry matrices."}, {"heading": "4.3 Brain Computer-Interface", "text": "In fact, most of them are not a purely formal matter, but a purely formal matter, which is about finding a political solution."}, {"heading": "4.4 Texture classification", "text": "As another example of real problems, we consider a subset of four textures (shown in Fig. 6) of the Brodatz dataset Brodatz (1966). Similar to the work of Mairal et al. (2009) and Yger & Rakotomamonjy (2011), we extracted 16 x 16 patches from each texture pair. For each texture pair, the training set consists of patches from the left half of each texture and the test set consists of patches from the right half. In each set, the patches may overlap, but the training and test sets do not overlap. As suggested in Tou et al. (2009), each method was trained on 50 patches from the training set and tested on 300 patches selected from the test set. Classification rates were calculated as an average of 50 runs after re-sampling the training and test sets."}, {"heading": "5 Conclusion and perspectives", "text": "In this paper, we presented a novel approach to selecting a logEuclidean metric based on theoretical observations of distortion by a logEuclidean metric, and then proposed to address the problem of selecting a relevant metric as a core learning algorithm with a criterion for aligning the core goal. Finally, we solved this problem in a variety of ways using optimization tools and applied it to synthetic and real data. Although we limited ourselves to binary classification problems, extending our approach to multi-class problems Ramona et al. (2012) is straightforward. When it comes to huge data sets, our optimization algorithm might not be well scalable. Stochastic setting on manifold bonuses (2013) could be a promising expansion. We considered only complete covariance matrices, but extending our approach to low-level matrices is a challenging and interesting work to consider as this fits with the proposed explanatory structure."}, {"heading": "A Appendix", "text": "G & H (G & H): D (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G & H).G (G).G (G).G (G).G (G).H (G).H).H (G).G (.G).G (.G).G (.G).G).G (.G).G (.G).G).G (.G).G (.G).G).G (.G).G (.G).G (.G).G).G (.G).G (.G).G (.G).G (.G) (.G).G (.G).G (.G).G (.G).G (.G).G (.G) (.G).G (.G).G (.G).G (.G).G (.G) (.G).G).G (.G).G (.G).G (.G) (.G) (.G) (.G).G).G (.G).G (.G) (.G) (.G) (.G) (.G).G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G).G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G) (.G (.G) (.G) (.G) (.G) (.G) (.G) (."}], "references": [{"title": "Optimization algorithms on matrix manifolds", "author": ["Absil", "P-A", "Mahony", "Robert", "Sepulchre", "Rodolphe"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2009}, {"title": "Computing the fr\u00e9chet derivative of the matrix exponential, with an application to condition number estimation", "author": ["Al-Mohy", "Awad H", "Higham", "Nicholas J"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Al.Mohy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Al.Mohy et al\\.", "year": 2009}, {"title": "Log-euclidean metrics for fast and simple calculus on diffusion tensors", "author": ["Arsigny", "Vincent", "Fillard", "Pierre", "Pennec", "Xavier", "Ayache", "Nicholas"], "venue": "Magnetic resonance in medicine,", "citeRegEx": "Arsigny et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2006}, {"title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices", "author": ["Arsigny", "Vincent", "Fillard", "Pierre", "Pennec", "Xavier", "Ayache", "Nicholas"], "venue": "SIAM journal on matrix analysis and applications,", "citeRegEx": "Arsigny et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2007}, {"title": "Riemannian geometry applied to bci classification", "author": ["Barachant", "Alexandre", "Bonnet", "St\u00e9phane", "Congedo", "Marco", "Jutten", "Christian"], "venue": "In LVA/ICA,", "citeRegEx": "Barachant et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barachant et al\\.", "year": 2010}, {"title": "Classification of covariance matrices using a riemannian-based kernel for bci applications", "author": ["Barachant", "Alexandre", "Bonnet", "St\u00e9phane", "Congedo", "Marco", "Jutten", "Christian"], "venue": null, "citeRegEx": "Barachant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barachant et al\\.", "year": 2013}, {"title": "Learning good edit similarities with generalization guarantees", "author": ["Bellet", "Aur\u00e9lien", "Habrard", "Amaury", "Sebban", "Marc"], "venue": "In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD),", "citeRegEx": "Bellet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2011}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["Bellet", "Aur\u00e9lien", "Habrard", "Amaury", "Sebban", "Marc"], "venue": "arXiv preprint arXiv:1306.6709,", "citeRegEx": "Bellet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2013}, {"title": "Positive definite matrices", "author": ["Bhatia", "Rajendra"], "venue": null, "citeRegEx": "Bhatia and Rajendra.,? \\Q2009\\E", "shortCiteRegEx": "Bhatia and Rajendra.", "year": 2009}, {"title": "Stochastic gradient descent on riemannian manifolds", "author": ["Bonnabel", "Silvere"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Bonnabel and Silvere.,? \\Q2013\\E", "shortCiteRegEx": "Bonnabel and Silvere.", "year": 2013}, {"title": "Discrete regression methods on the cone of positive-definite matrices", "author": ["Boumal", "Nicolas", "Absil", "P-A"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Boumal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2011}, {"title": "Manopt, a matlab toolbox for optimization on manifolds", "author": ["Boumal", "Nicolas", "Mishra", "Bamdev", "Absil", "P-A", "Sepulchre", "Rodolphe"], "venue": "The Journal of Machine Learning Research (JMLR),", "citeRegEx": "Boumal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2014}, {"title": "Textures: a photographic album for artists and designers", "author": ["Brodatz", "Phil"], "venue": "Dover Pubns,", "citeRegEx": "Brodatz and Phil.,? \\Q1966\\E", "shortCiteRegEx": "Brodatz and Phil.", "year": 1966}, {"title": "Efficient similarity search for covariance matrices via the jensen-bregman logdet divergence", "author": ["Cherian", "Anoop", "Sra", "Suvrit", "Banerjee", "Arindam", "Papanikolopoulos", "Nikolaos"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Cherian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cherian et al\\.", "year": 2011}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "The Journal of Machine Learning Research (JMLR),", "citeRegEx": "Cortes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2012}, {"title": "Nearest neighbor pattern classification", "author": ["Cover", "Thomas", "Hart", "Peter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1967}, {"title": "On kernel target alignment", "author": ["Cristianini", "Nello", "Shawe-Taylor", "John", "Elisseeff", "Andre", "Kandola", "Jaz"], "venue": "In NIPS,", "citeRegEx": "Cristianini et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2001}, {"title": "Ground metric learning", "author": ["Cuturi", "Marco", "Avis", "David"], "venue": "The Journal of Machine Learning Research (JMLR),", "citeRegEx": "Cuturi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cuturi et al\\.", "year": 2014}, {"title": "Non-euclidean statistics for covariance matrices, with applications to diffusion tensor imaging", "author": ["Dryden", "Ian L", "Koloydenko", "Alexey", "Zhou", "Diwei"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Dryden et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dryden et al\\.", "year": 2009}, {"title": "Principal geodesic analysis on symmetric spaces: Statistics of diffusion tensors", "author": ["Fletcher", "P Thomas", "Joshi", "Sarang"], "venue": "In Computer Vision and Mathematical Methods in Medical and Biomedical Image Analysis,", "citeRegEx": "Fletcher et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fletcher et al\\.", "year": 2004}, {"title": "Learning with distance substitution kernels", "author": ["Haasdonk", "Bernard", "Bahlmann", "Claus"], "venue": "Pattern Recognition, pp", "citeRegEx": "Haasdonk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Haasdonk et al\\.", "year": 2004}, {"title": "From manifold to manifold: geometry-aware dimensionality reduction for spd matrices", "author": ["Harandi", "Mehrtash", "Salzmann", "Mathieu", "Hartley", "Richard"], "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),", "citeRegEx": "Harandi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Harandi et al\\.", "year": 2014}, {"title": "Kernel methods on the riemannian manifold of symmetric positive definite matrices", "author": ["Jayasumana", "Sadeep", "Hartley", "Richard", "Salzmann", "Mathieu", "Li", "Hongdong", "Harandi", "Mehrtash"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Jayasumana et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jayasumana et al\\.", "year": 2013}, {"title": "A survey and comparison of contemporary algorithms for computing the matrix geometric mean", "author": ["Jeuris", "Ben", "Vandebril", "Raf", "Vandereycken", "Bart"], "venue": "Electronic Transactions on Numerical Analysis,", "citeRegEx": "Jeuris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jeuris et al\\.", "year": 2012}, {"title": "Non-linear metric learning", "author": ["Kedem", "Dor", "Tyree", "Stephen", "Sha", "Fei", "Lanckriet", "Gert R", "Weinberger", "Kilian Q"], "venue": "In NIPS, pp", "citeRegEx": "Kedem et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kedem et al\\.", "year": 2012}, {"title": "Robust structural metric learning", "author": ["Lim", "Daryl", "Lanckriet", "Gert", "McFee", "Brian"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Regularizing common spatial patterns to improve bci designs: unified theory and new algorithms", "author": ["Lotte", "Fabien", "Guan", "Cuntai"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "Lotte et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lotte et al\\.", "year": 2011}, {"title": "Neighborhood repulsed metric learning for kinship verification", "author": ["Lu", "Jiwen", "Hu", "Junlin", "Zhou", "Xiuzhuang", "Shang", "Yuanyuan", "Tan", "Yap-Peng", "Wang", "Gang"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2012}, {"title": "Supervised dictionary learning", "author": ["Mairal", "Julien", "Ponce", "Jean", "Sapiro", "Guillermo", "Zisserman", "Andrew", "Bach", "Francis"], "venue": "In NIPS,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["Mensink", "Thomas", "Verbeek", "Jakob", "Perronnin", "Florent", "Csurka", "Gabriela"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Mensink et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mensink et al\\.", "year": 2012}, {"title": "A differential geometric approach to the geometric mean of symmetric positive-definite matrices", "author": ["Moakher", "Maher"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Moakher and Maher.,? \\Q2005\\E", "shortCiteRegEx": "Moakher and Maher.", "year": 2005}, {"title": "Seperability of four-class motor imagery data using independent components analysis", "author": ["M Naeem", "C Brunner", "R Leeb", "B Graimann", "G. Pfurtscheller"], "venue": "Journal of neural engineering,", "citeRegEx": "Naeem et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Naeem et al\\.", "year": 2006}, {"title": "Hamming distance metric learning", "author": ["Norouzi", "Mohammad", "Blei", "David M", "Salakhutdinov", "Ruslan R"], "venue": "In NIPS, pp", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "A riemannian framework for tensor computing", "author": ["Pennec", "Xavier", "Fillard", "Pierre", "Ayache", "Nicholas"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Pennec et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pennec et al\\.", "year": 2006}, {"title": "Motor imagery and direct brain-computer communication", "author": ["Pfurtscheller", "Gert", "Neuper", "Christa"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Pfurtscheller et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Pfurtscheller et al\\.", "year": 2001}, {"title": "Multiclass feature selection with kernel gram-matrix-based criteria", "author": ["Ramona", "Mathieu", "Richard", "Ga\u00ebl", "David", "Bertrand"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Ramona et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ramona et al\\.", "year": 2012}, {"title": "Learning a distance metric from a network", "author": ["Shaw", "Blake", "Huang", "Bert", "Jebara", "Tony"], "venue": "In NIPS, pp. 1899\u20131907,", "citeRegEx": "Shaw et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shaw et al\\.", "year": 2011}, {"title": "Positive definite matrices and the s-divergence", "author": ["Sra", "Suvrit"], "venue": "arXiv preprint arXiv:1110.1773,", "citeRegEx": "Sra and Suvrit.,? \\Q2011\\E", "shortCiteRegEx": "Sra and Suvrit.", "year": 2011}, {"title": "A new metric on the manifold of kernel matrices with application to matrix geometric means", "author": ["Sra", "Suvrit"], "venue": "In NIPS, pp", "citeRegEx": "Sra and Suvrit.,? \\Q2012\\E", "shortCiteRegEx": "Sra and Suvrit.", "year": 2012}, {"title": "Gabor filters as feature images for covariance matrix on texture classification problem", "author": ["Tou", "Jing Yi", "Tay", "Yong Haur", "Lau", "Phooi Yee"], "venue": "In Advances in Neuro-Information Processing,", "citeRegEx": "Tou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tou et al\\.", "year": 2009}, {"title": "Region covariance: A fast descriptor for detection and classification", "author": ["Tuzel", "Oncel", "Porikli", "Fatih", "Meer", "Peter"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Tuzel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tuzel et al\\.", "year": 2006}, {"title": "Pedestrian detection via classification on riemannian manifolds", "author": ["Tuzel", "Oncel", "Porikli", "Fatih", "Meer", "Peter"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Tuzel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tuzel et al\\.", "year": 2008}, {"title": "Prodis-contshc: learning protein dissimilarity measures and hierarchical context coherently for protein-protein comparison in protein database retrieval", "author": ["Wang", "Jingyan", "Gao", "Xin", "Quanquan", "Li", "Yongping"], "venue": "BMC bioinformatics,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Geodesic convexity and covariance estimation", "author": ["Wiesel", "Ami"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Wiesel and Ami.,? \\Q2012\\E", "shortCiteRegEx": "Wiesel and Ami.", "year": 2012}, {"title": "A review of kernels on covariance matrices for bci applications", "author": ["Yger", "Florian"], "venue": "In IEEE International Workshop on Machine Learning for Signal Processing (MLSP),", "citeRegEx": "Yger and Florian.,? \\Q2013\\E", "shortCiteRegEx": "Yger and Florian.", "year": 2013}, {"title": "Wavelet kernel learning", "author": ["Yger", "Florian", "Rakotomamonjy", "Alain"], "venue": "Pattern Recognition,", "citeRegEx": "Yger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yger et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al.", "startOffset": 231, "endOffset": 249}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al.", "startOffset": 231, "endOffset": 284}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al. (2012), link prediction in networks Shaw et al.", "startOffset": 231, "endOffset": 328}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al. (2012), link prediction in networks Shaw et al. (2011) and bioinformatics Wang et al.", "startOffset": 231, "endOffset": 376}, {"referenceID": 20, "context": "So far, various methods have been developed for learning Mahalanobis distances in the Euclidean setting, and such metric learning methods have been successfully applied to diverse real-world problems including music recommendation Lim et al. (2013), face recognition Lu et al. (2012), image classification Mensink et al. (2012), link prediction in networks Shaw et al. (2011) and bioinformatics Wang et al. (2012). For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al.", "startOffset": 231, "endOffset": 414}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects.", "startOffset": 83, "endOffset": 104}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al.", "startOffset": 83, "endOffset": 313}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al. (2012), binary codes Norouzi et al.", "startOffset": 83, "endOffset": 334}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al. (2012), binary codes Norouzi et al. (2012) and strings Bellet et al.", "startOffset": 83, "endOffset": 370}, {"referenceID": 4, "context": "For the survey of recent advances, issues and perspectives in metric learning, see Bellet et al. (2013). Classical metric learning approaches focused on an Euclidean setting which do not apply to complex or structured objects. Recently, metric learning for complex objects such as histograms Cuturi & Avis (2014); Kedem et al. (2012), binary codes Norouzi et al. (2012) and strings Bellet et al. (2011) have been actively explored, to which the Euclidean distance is not relevant.", "startOffset": 83, "endOffset": 403}, {"referenceID": 16, "context": "(2006); Dryden et al. (2009), brain-computer interfaces Barachant et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "(2009), brain-computer interfaces Barachant et al. (2010, 2013), pedestrian detection Tuzel et al. (2008) and texture classification Tuzel et al.", "startOffset": 34, "endOffset": 106}, {"referenceID": 4, "context": "(2009), brain-computer interfaces Barachant et al. (2010, 2013), pedestrian detection Tuzel et al. (2008) and texture classification Tuzel et al. (2006); Tou et al.", "startOffset": 34, "endOffset": 153}, {"referenceID": 4, "context": "(2009), brain-computer interfaces Barachant et al. (2010, 2013), pedestrian detection Tuzel et al. (2008) and texture classification Tuzel et al. (2006); Tou et al. (2009). In these applications, covariance matrices are either extracted from a physical model of the studied phenomenon (for diffusion tensor imaging) or as an empirical estimator from observations (for signal processing and computer-vision tasks).", "startOffset": 34, "endOffset": 172}, {"referenceID": 2, "context": "However, the Euclidean geometry for averaging SPD matrices can result in a swelling effect Arsigny et al. (2007), i.", "startOffset": 91, "endOffset": 113}, {"referenceID": 2, "context": "However, the Euclidean geometry for averaging SPD matrices can result in a swelling effect Arsigny et al. (2007), i.e., the determinant of the average can be bigger than the determinant of each matrix. As also remarked in Fletcher & Joshi (2004) and illustrated in Fig.", "startOffset": 91, "endOffset": 246}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al.", "startOffset": 94, "endOffset": 116}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013).", "startOffset": 94, "endOffset": 141}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013). In this paper, we propose a supervised approach to learning the LogEuclidean metric.", "startOffset": 94, "endOffset": 218}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013). In this paper, we propose a supervised approach to learning the LogEuclidean metric. More specifically, we formulate our LogEuclidean metric learning problem as the kernel-target alignment problem Cristianini et al. (2001); Cortes et al.", "startOffset": 94, "endOffset": 442}, {"referenceID": 2, "context": "The LogEuclidean metric (and the derived distance and kernel) has been used in the literature Arsigny et al. (2006); Barachant et al. (2013), but its parameterization remained underestimated until recently Yger (2013). In this paper, we propose a supervised approach to learning the LogEuclidean metric. More specifically, we formulate our LogEuclidean metric learning problem as the kernel-target alignment problem Cristianini et al. (2001); Cortes et al. (2012), and solve the non-trivial optimization problem using the Riemannian geometry for SPD matrices.", "startOffset": 94, "endOffset": 464}, {"referenceID": 2, "context": "G was implicitly chosen as the identity matrix in Arsigny et al. (2006), while G was heuristically chosen as the Riemannian mean of training samples in Barachant et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 2, "context": "G was implicitly chosen as the identity matrix in Arsigny et al. (2006), while G was heuristically chosen as the Riemannian mean of training samples in Barachant et al. (2013) and Yger (2013).", "startOffset": 50, "endOffset": 176}, {"referenceID": 2, "context": "G was implicitly chosen as the identity matrix in Arsigny et al. (2006), while G was heuristically chosen as the Riemannian mean of training samples in Barachant et al. (2013) and Yger (2013). Although these choices sound reasonable, we argue that they are sub-optimal when \u03b4 l is used for nearest neighbor classification.", "startOffset": 50, "endOffset": 192}, {"referenceID": 15, "context": "2 Metric learning with kernel-target alignment For metric learning, we employ the (centered) kernel target alignment (KTA) criterion Cristianini et al. (2001); Cortes et al.", "startOffset": 133, "endOffset": 159}, {"referenceID": 14, "context": "(2001); Cortes et al. (2012):", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "2 For more sophisticated optimization methods, see Absil et al. (2009) and Boumal et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 0, "context": "2 For more sophisticated optimization methods, see Absil et al. (2009) and Boumal et al. (2014).", "startOffset": 51, "endOffset": 96}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al.", "startOffset": 35, "endOffset": 79}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al.", "startOffset": 35, "endOffset": 101}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al.", "startOffset": 35, "endOffset": 126}, {"referenceID": 1, "context": "As already discussed in literature Arsigny et al. (2006); Pennec et al. (2006); Dryden et al. (2009); Fletcher & Joshi (2004); Cherian et al. (2011), different tools come along with various implicit invariance properties and computational complexities.", "startOffset": 35, "endOffset": 149}, {"referenceID": 0, "context": "3More details about optimization on Riemannian matrix manifolds and the geometry of Pd can be found in Absil et al. (2009) and in Bhatia (2009).", "startOffset": 103, "endOffset": 123}, {"referenceID": 0, "context": "3More details about optimization on Riemannian matrix manifolds and the geometry of Pd can be found in Absil et al. (2009) and in Bhatia (2009).", "startOffset": 103, "endOffset": 144}, {"referenceID": 2, "context": "As already stated in Arsigny et al. (2007), this distance is immune to the swelling effect.", "startOffset": 21, "endOffset": 43}, {"referenceID": 2, "context": "As already stated in Arsigny et al. (2007), this distance is immune to the swelling effect. Thus, it could be a good candidate for distance metric learning for covariance matrices. However, the AIRM distance comes along with an invariance to a class of congruent transforms Bhatia (2009) and it leads to the following isometry for any M \u2208 Pd: \u03b4r(\u0393M (A),\u0393M (B)) = \u03b4r(A,B).", "startOffset": 21, "endOffset": 288}, {"referenceID": 4, "context": "Apart from their difference in terms of invariance properties, it should be highlighted that the AIRM distance is not negative-definite and then, contrary to the LogEuclidean distance, cannot be used for defining positive-definite kernels on SPD matrices Haasdonk & Bahlmann (2004); Sra (2011); Barachant et al. (2013). Using information geometry and extending divergences to the matrix case Cherian et al.", "startOffset": 295, "endOffset": 319}, {"referenceID": 4, "context": "Apart from their difference in terms of invariance properties, it should be highlighted that the AIRM distance is not negative-definite and then, contrary to the LogEuclidean distance, cannot be used for defining positive-definite kernels on SPD matrices Haasdonk & Bahlmann (2004); Sra (2011); Barachant et al. (2013). Using information geometry and extending divergences to the matrix case Cherian et al. (2011); Sra (2011, 2012), a symmetrized LogDeterminant divergence (also called the symmetrized Stein loss) can also be used.", "startOffset": 295, "endOffset": 414}, {"referenceID": 4, "context": "Apart from their difference in terms of invariance properties, it should be highlighted that the AIRM distance is not negative-definite and then, contrary to the LogEuclidean distance, cannot be used for defining positive-definite kernels on SPD matrices Haasdonk & Bahlmann (2004); Sra (2011); Barachant et al. (2013). Using information geometry and extending divergences to the matrix case Cherian et al. (2011); Sra (2011, 2012), a symmetrized LogDeterminant divergence (also called the symmetrized Stein loss) can also be used. This divergence can be seen as an approximation of the AIRM distance and as such, there exist some bounds between the AIRM distance and this symmetrized divergence Sra (2011). Moreover, this divergence is invariant to the same transformation as the AIRM distance, but can lead under some conditions to a definite-positive kernel.", "startOffset": 295, "endOffset": 707}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning.", "startOffset": 46, "endOffset": 67}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning. In this view, the parameter G can be seen as the center of the tangent plane, and it has been empirically observed Yger (2013) that the choice of G has a strong impact on the metric behaviour.", "startOffset": 46, "endOffset": 333}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning. In this view, the parameter G can be seen as the center of the tangent plane, and it has been empirically observed Yger (2013) that the choice of G has a strong impact on the metric behaviour. So far, G has been heuristically tuned using either the identity matrix or the Riemannian mean Moakher (2005); Jeuris et al.", "startOffset": 46, "endOffset": 509}, {"referenceID": 4, "context": "Hence, according to the taxonomy developed in Bellet et al. (2013), since we optimizeG through a logarithmic mapping on a Riemannian manifold, our approach is an instance of non-linear similarity learning. In this view, the parameter G can be seen as the center of the tangent plane, and it has been empirically observed Yger (2013) that the choice of G has a strong impact on the metric behaviour. So far, G has been heuristically tuned using either the identity matrix or the Riemannian mean Moakher (2005); Jeuris et al. (2012) of data.", "startOffset": 46, "endOffset": 531}, {"referenceID": 22, "context": "(2013); Yger (2013); Jayasumana et al. (2013). In this regard, the optimization of the LogEuclidean metric can also be interpreted as a kernel learning approach.", "startOffset": 21, "endOffset": 46}, {"referenceID": 0, "context": "1 Setup Concerning the implementation of our approach, we employed a Riemannian trust-region5 Boumal & Absil (2011); Absil et al. (2009) .", "startOffset": 117, "endOffset": 137}, {"referenceID": 21, "context": "As we have not found any guarantee concerning the convexity or the geodesic-convexity Wiesel (2012) of the centered KTA, we decided to use the Riemannian mean Moakher (2005); Jeuris et al. (2012) as G0.", "startOffset": 175, "endOffset": 196}, {"referenceID": 10, "context": "5with the implementation provided in the Manopt toolbox provided by Boumal et al. (2014).", "startOffset": 68, "endOffset": 89}, {"referenceID": 31, "context": "3 Brain Computer-Interface We tested our approach on Dataset 2a from the BCI Competition IV7 Naeem et al. (2006). The dataset consists of EEG signals (recorded from 22 electrodes) on 9 subjects who were asked to perform left hand, right hand, foot 6by comparing the two best results with the two-sided Wilcoxon signed rank test at significance level 0.", "startOffset": 93, "endOffset": 113}, {"referenceID": 4, "context": "As shown in literature Barachant et al. (2010, 2013); Yger (2013), for motor imagery signals, the spatial covariance matrix of the trials is a very promising feature.", "startOffset": 23, "endOffset": 66}, {"referenceID": 4, "context": "As shown in literature Barachant et al. (2010, 2013); Yger (2013), for motor imagery signals, the spatial covariance matrix of the trials is a very promising feature. Based on this observation, we extracted the covariance matrix between sensors (i.e., the spatial covariance matrix) from every filtered time segment (i.e., every example to be classified). Although we could directly use our approach and try to classify samples, the performance would be poor since this dataset is known to show some non-stationarity between sessions. In order to cope with this problem, we employ an adaptive kernel formulation proposed in Barachant et al. (2013). This method cancels the non-stationarity in the data by whitening the covariance matrices in the training session and test session by subtracting the Riemannian mean of the training and test datasets, respectively.", "startOffset": 23, "endOffset": 648}, {"referenceID": 28, "context": "Similarly to the works of Mairal et al. (2009) and Yger & Rakotomamonjy (2011), we extracted 16\u00d7 16 patches from every texture.", "startOffset": 26, "endOffset": 47}, {"referenceID": 28, "context": "Similarly to the works of Mairal et al. (2009) and Yger & Rakotomamonjy (2011), we extracted 16\u00d7 16 patches from every texture.", "startOffset": 26, "endOffset": 79}, {"referenceID": 39, "context": "As proposed in Tou et al. (2009), we built covariance matrices using a bank of 8 Gabor filters (4 angles and 2 scales).", "startOffset": 15, "endOffset": 33}, {"referenceID": 34, "context": "Although we restricted ourselves to binary classification problems, the extension of our approach to multi-class problems Ramona et al. (2012) is straightforward.", "startOffset": 122, "endOffset": 143}, {"referenceID": 34, "context": "Although we restricted ourselves to binary classification problems, the extension of our approach to multi-class problems Ramona et al. (2012) is straightforward. When huge datasets are involved, our optimization algorithm might not scale well. A stochastic setting on manifold Bonnabel (2013) could be a promising extention.", "startOffset": 122, "endOffset": 294}, {"referenceID": 21, "context": "Exploration along this line of research would bridge the gap between our approach and the one proposed in Harandi et al. (2014).", "startOffset": 106, "endOffset": 128}], "year": 2015, "abstractText": "Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for symmetric positive definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.", "creator": "LaTeX with hyperref package"}}}