{"id": "1502.03581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Web spam classification using supervised artificial neural network algorithms", "abstract": "Due to the rapid growth in technology employed by the spammers, there is a need of classifiers that are more efficient, generic and highly adaptive. Neural Network based technologies have high ability of adaption as well as generalization. As per our knowledge, very little work has been done in this field using neural network. We present this paper to fill this gap. This paper evaluates performance of three supervised learning algorithms of artificial neural network by creating classifiers for the complex problem of latest web spam pattern classification. These algorithms are Conjugate Gradient algorithm, Resilient Backpropagation learning, and Levenberg-Marquardt algorithm.", "histories": [["v1", "Thu, 12 Feb 2015 09:58:23 GMT  (364kb)", "http://arxiv.org/abs/1502.03581v1", "10 Pages in Advanced Computational Intelligence: An International Journal (ACII), Vol.2, No.1, January 2015"]], "COMMENTS": "10 Pages in Advanced Computational Intelligence: An International Journal (ACII), Vol.2, No.1, January 2015", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["ashish chandra", "mohammad suaib", "dr rizwan beg"], "accepted": false, "id": "1502.03581"}, "pdf": {"name": "1502.03581.pdf", "metadata": {"source": "CRF", "title": "WEB SPAM CLASSIFICATION USING SUPERVISED ARTIFICIAL NEURAL NETWORK ALGORITHMS", "authors": ["Ashish Chandra", "Mohammad Suaib", "Rizwan Beg"], "emails": [], "sections": [{"heading": null, "text": "Due to the rapid technological growth of spammers, there is a need for more efficient, generic and highly adaptive classifiers. Neural network technologies have a high capacity for adaptation and generalization, and to our knowledge very little work has been done in this area using neural networks. To fill this gap, we present this paper, which evaluates the performance of three monitored learning algorithms of artificial neural networks by creating classifiers for the complex problem of the latest web spam pattern classification. These algorithms are Conjugate Gradient Algorithm, Resilient Backpropagation Learning and Levenberg-Marquardt Algorithm. KEYWORDSWeb Spam, Artificial Neural Network, Back Propagation Algorithms, Conjugate Gradient, Resilient Backpropagation, Levenberg-Marquardt, Web Spam Classification."}, {"heading": "1. INTRODUCTION", "text": "In this thesis, we examine three artificial neural network algorithms (ANN), namely the Conjugate Gradient Learning Algorithm, the Resilient Back-propagation Algorithm, and the LevenbergMarquardt Algorithm. All of these algorithms are monitored learning algorithms. We evaluate the performance of these algorithms based on classification results and computational requirements in training. The application on which we evaluate these algorithms is web spam classification. Web spam is one of the most important challenges for the search engine industry. Web spam is the web pages that are created or manipulated to guide users from the search engine results page to a target web page and also to manipulate the search engine ranking of the target page."}, {"heading": "2. RELATED WORK", "text": "Svore (2007) [1] developed a method for web spam detection based on content-based characteristics and rank time using SVM classifiers with a linear core. Noi (2010) [2] proposed a combination of graph neural network and probability mapping of graphs that were self-organizing and organized in a layered architecture. It was a mixture of unattended and supervised learning approaches, but the training time was computationally very time consuming. Erdelyi (2011) [3] achieved superior classification results in experiments using LogitBoost and RandomForest learning methods with less computation-hungry content features. They used 100,000 hosts from WebspamUK2007 and 190,000 hosts from DC2010 datasets and investigated the target conflict between feature generation and spam classification accuracy. They demonstrated that more features can improve performance, but more complex features can be manipulated like Ximarin 2011 pairing accuracy."}, {"heading": "3. ARTIFICIAL NEURAL NETWORK", "text": "An ANN is a collection of simple processing units that communicate with each other via a large number of weighted connections. Each unit receives input from neighboring units or external sources and calculates output that spreads to other neighbors. There is also a mechanism for adjusting connection weights. Normally, there are 3 types of units. \u2022 Input Unit: receives signals from external sources. \u2022 Output Unit: sends data from the network. \u2022 Hidden Unit: receives and sends signals within the network. Many of the units can work in parallel in the system. \u2022 ANN can be customized to accommodate a series of inputs and produce a desired amount of outputs. This process is known as learning or training."}, {"heading": "3.1. Multi-Layer Perceptron (MLP)", "text": "MLP is a nonlinear network model that maps a series of inputs x into a series of outputs y. It has 3 types of levels: input layer, output layer, and hidden layers. Standard perceptron calculates a discontinuous function: \u2192 (+ (, (1) smoothing is done using a logistic function to obtain \u2192 (+ (, (2)), where: (= Advanced Computational Intelligence: An International Journal (ACII), Vol.MLP is a finite acyclic diagram where the nodes are neurons with logistic activation. Neurons of the i layer serve as input for neurons of the (i + 1) network, which contains many neurons. All connections are weighted with real number.Weight of connections i-output layer neurons have a bias weight. Each node of the network gives an activation function that is applied by the weighted sum of the network outputs.The output is given by the neurons."}, {"heading": "3.2. Neural Network Supervised Learning Algorithms", "text": "Very complex functions can be calculated when neurons of a plane are connected to all neurons of \u2192 j. Everything hidden and di0. (w + w,!) \"X aError E defined% & E ()\" thits all (3) (4) Where P is the total size of the training data set and Ep is the error for the training pattern p. And also% * + (O \u2212 t * / \"(5), where N is the total number of output nodes, Oi the output of the i-th output node and ti the target output at the ith output node. Each learning algorithm tries to reduce the global error by adjusting weights and distortions."}, {"heading": "3.3. Conjugate Gradient (CG) Algorithm", "text": "It is a basic reverse propagation algorithm. It adjusts the weights in the steepest downward direction, i.e. in the most negative of the slopes. In this direction, the function decreases the fastest. It is observed that, although the function decreases the fastest along the negative direction of the slope, it does not always offer the fastest convergence. In the conjugate gradient algorithm, a search is performed in conjugate directions that generally provide the faster convergence than the steepest downward direction [6]. The conjugate gradient algorithm adjusts the step size in each iteration along the conjugate gradient direction to minimize the performance function. It seeks the steepest downward direction in the first iteration. p0 = - g0 (6) It then performs line search to determine the optimal distance to move along the current search direction by combining new steepest downward with the previous one."}, {"heading": "3.4. Resilient Back-propagation (RB) Algorithm", "text": "Multi-layer Perceptron networks typically use the sigmoid transfer function in the hidden layer. It is also known as the pinch function because it compresses an infinite input range into a finite output range. As you enter, the sigmoid function approaches zero. It causes problems when we use the steepest descent to train the network. As the slope can have a very small value, it can cause small changes in weights and distortions, even though the weight and distortions are quite far from the optimum values. The purpose of resilient reverse propagation (RB) learning is to eliminate the harmful effects of the magnitude of partial derivatives. It uses only the sign of the partial derivative to determine the direction of the weight change. The magnitude of the weight change is calculated as the following rule [8]: \u2022 Values of each weight and distortion are increased if the derivative shows the same in relation to the weight."}, {"heading": "3.5. Levenberg-Marquardt (LM) Algorithm", "text": "It provides a numerical solution to the problem of minimizing a generally non-linear function via a parameter space for the function. It is a popular alternative to the Gaussian-Newton method of finding the minimum of a function. It approaches the training speed of second order without calculating the Hessian matrix [9]. If the performance function has the shape of a sum of squares, then the Hessian matrix can be approximated as follows: H = JT J (9) and the gradient is: g = J T e (10), where J is the Jacobin matrix containing the first derivatives of network errors in terms of weights and distortions, and e is the vector of network errors. The Jacobin matrix can be calculated using a standard backpropagation technique that is much more complex than the calculation of the Hessian matrix. This approximate matrix is used in accordance with Newton as datexk = Jessik - J + T [J + J] Ziff function."}, {"heading": "3.6. Bayesian Regularization (BR)", "text": "Mackay (1992) proposed a Bayesian framework that can be applied directly to the NN learning problem. It allows to estimate the effective number of parameters actually used by the model, i.e. the number of network weights that are actually needed to solve a particular problem. Bayesian regulation expands the cost function to look not only for minimal errors, but also for minimal errors with minimal weights. By using Bayesian regulation, costly cross-validation can be avoided. It is particularly useful for problems that would not or would not suffer if a portion of the available data were reserved to a validation set. Furthermore, regulation also reduces or eliminates the need to test different numbers of hidden neurons for a problem. bayesian regulated ANN are severely overtrained and overfit."}, {"heading": "4. CONFUSION MATRIX", "text": "We used four attributes of the confusion matrix in evaluating the performance of the algorithms. These attributes are sensitivity, specificity, efficiency and accuracy. These attributes are defined as follows: Sensitivity or True Positive Rate (TPR), also known as Recall Rate is given by: Sensitivity =) (=) > / (12) Specificity or True Negative Rate (TNR) is given by: SpeciAicity = / (>) = / (13) Efficiency is given by: EfAiciency CDEF G H GI C (DJ A J GI * =) K = / K * (14) Accuracy is given by: Accuracy =) = / () / () / (15), where P is the number of positive instances, N the number of negative instances, TP the number of correctly positive instances, TN the number of positive instances, TN the number of incorrectly classified and FN the number of negative instances."}, {"heading": "5. EXPERIMENT", "text": "We checked the performance of this algorithm to automatically detect web spam. These algorithms are Conjugate Gradient, Resilient Back-propagation and Levenberg-Marquardt Learning. We created a neural network with a single hidden layer and used a neuron in the output layer. We used bipolar sigmoid function with an output range of [-1, 1]. (* OP \u2212 1 (16) We selected in the experiment: Stopped criteria as number of iterations \u03b8 = 100, Learning rate \u03b1 = 0.1 Number of neurons in hidden layers = 10 (or 20, where specified). We created a corpus of 368 instances of manually selected websites where about 30% of the instances were designated as spam and the rest of the pages as ham. To create training data, we randomly selected from the data sets we used to check the remaining 31% expensive factors."}, {"heading": "6. RESULT", "text": "We have created tables (Table II to Table VIII) to show the performance results of each algorithm. Consider the number of neurons in the hidden layer as 10, unless otherwise specified. Values in the tables represent an average of 10 experimental readings per category. The values in underlined show the best results."}, {"heading": "LM 0.8115 0.9352 0.8734 0.8816 3.744", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.8076 0.9823 0.8950 0.9066 0.187", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.7230 0.9088 0.8159 0.8282 1.923", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.6692 0.9117 0.7904 0.8066 0.157", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.9230 0.6382 0.7806 0.7616 0.113", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.6615 0.8441 0.7528 0.7650 9.685", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.7269 0.9205 0.8237 0.8366 0.191", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.7384 0.8823 0.8104 0.8200 0.198", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.6884 0.7647 0.7265 0.7316 2.723", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.4653 0.9117 0.6885 0.7183 0.168", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.9153 0.3088 0.6121 0.5716 0.149", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.8846 0.9264 0.9055 0.9083 0.266", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.8538 0.9764 0.9151 0.9233 0.190", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.8807 0.8823 0.8815 0.8816 10.743", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.8730 0.9558 0.9144 0.9200 0.539", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.8576 0.9823 0.9200 0.9283 0.312", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.8884 0.9205 0.9045 0.9066 32.789", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.7269 0.8705 0.7987 0.8083 0.230", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.7593 0.9088 0.8340 0.8438 0.188", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.7038 0.8441 0.7739 0.7833 7.940", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.8500 0.9676 0.9088 0.9166 0.297", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.8769 0.9735 0.9252 0.9316 0.214", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.8653 0.9029 0.8841 0.8866 13.146", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CG 0.8384 0.9823 0.9104 0.9200 0.575", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "RB 0.8807 0.9588 0.9197 0.9250 0.377", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "LM 0.8653 0.9264 0.8959 0.9000 45.499", "text": "The data from the tables indicate that the conjugate gradient (CG + BR) algorithm delivered the best TNR (sensitivity) in most categories, while the Levenberg-Marquardt algorithm with Bayesian regularization (LM + BR) delivered the best TNR (specificity) in most categories. Overall, the best classification performance was achieved in most categories by the Resilient Back-propagation (RB) algorithm in terms of both efficiency and accuracy. Training time was considered to be the lowest for Resilient Back-propagation (RB) in most cases, so we can say that it is not only the best classification, but also the fastest algorithm. ConjugateGradient (CG) works quickly when the number of inputs is lower, but when the number of inputs is increased, it becomes slower than RB. The slowest algorithm has been observed with Bayesian Regarization (LBR)."}, {"heading": "7. CONCLUSION", "text": "In the experiment, we can conclude that the resilient back propagation algorithm is the fastest and performs best in both efficiency and accuracy measurements; the conjugate gradient algorithm provides the best sensitivity, while the Levenberg-Marquardt algorithm with Bayesian regularization provides the best specificity, but is the slowest when training time is factored.The classification performance of each algorithm improves with increased input factors. If the number of factors is high, an increased number of neurons improves the performance of the algorithm; the training time increases for all algorithms when either the number of inputs is increased or the number of neurons in hidden layers is increased."}], "references": [{"title": "Improving web spam classification using rank-time features", "author": ["K.M. Svore", "Q. Wu", "C.J. Burges"], "venue": "Proc. of the 3rd AIRWeb, Banff, Alberta, Canada (2007) 9\u201316.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Web spam detection by probability mapping graphsoms and graph neural networks", "author": ["L.D. Noi", "M. Hagenbuchner", "F. Scarselli", "A. Tsoi"], "venue": "Proc. of the 20th ICANN, Thessaloniki, Greece (2010) 372\u2013381.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Web spam classification: a few features worth more", "author": ["M. Erdelyi", "A. Garzo", "A.A. Benczur"], "venue": "Proceedings of the 2011 Joint WICOW/AIRWeb Workshop on Web Quality, WebQuality'11, Hyderabad, India, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Support vector machines under adversarial label noise", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "JMLR: Workshop and Conference Proceedings 20, Taoyuan, Taiwan, 2011, pp. 97\u2013112.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Adversarial label flips attack on support vector machines", "author": ["H. Xiao", "H. Xiao", "C. Eckert"], "venue": "presented at the 20th European Conference on Artificial Intelligence (ECAI), Montpellier, France, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning neural networks genetic algorithms and fuzzy systems", "author": ["H Adeli", "SL Hong"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "A direct adaptive method for faster back-propagation learning: The RPROP algorithm", "author": ["M Reidmiller", "H Brain"], "venue": "Proc IEEE Int. Conf. Neural Networks, 1993.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Numerical Analysis\", edited by Watson GA, Lecture Notes in Mathematics 630, (Springer Verlog, Germany", "author": ["JJ More"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Svore (2007) [1] devised a method for web spam detection based on content-based features and the rank-time.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "Noi (2010) [2] proposed a combination of graph neural network and probability mapping graph self organizing maps organized into a layered architecture.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "22 Erdelyi (2011) [3] achieved superior classification results in experiment using learning methods LogitBoost and RandomForest with less computation hungry content features.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "According to Biggio (2011) [4], SVM can be manipulated in adversarial classification tasks such as spam filtering.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "Similarly, Xiao (2012) [5] showed that injection of contaminated data in training dataset significantly degrades the accuracy of the SVM.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "In the Conjugate Gradient algorithm a search is done in conjugate directions, which generally provides the faster convergence than the steepest descent direction[6].", "startOffset": 161, "endOffset": 164}, {"referenceID": 6, "context": "The magnitude of the weight change is calculated as following rule [8]:", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "It approaches second order training speed without computing the Hessian Matrix [9], If the performance function is of the form of a sum of squares, then Hessian matrix can be approximated as: H = J J (9)", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "We employed bipolar sigmoid function which has the output range of [ -1 , 1 ].", "startOffset": 67, "endOffset": 77}], "year": 2015, "abstractText": "Due to the rapid growth in technology employed by the spammers, there is a need of classifiers that are more efficient, generic and highly adaptive. Neural Network based technologies have high ability of adaption as well as generalization. As per our knowledge, very little work has been done in this field using neural network. We present this paper to fill this gap. This paper evaluates performance of three supervised learning algorithms of artificial neural network by creating classifiers for the complex problem of latest web spam pattern classification. These algorithms are Conjugate Gradient algorithm, Resilient Backpropagation learning, and Levenberg-Marquardt algorithm.", "creator": "PScript5.dll Version 5.2.2"}}}