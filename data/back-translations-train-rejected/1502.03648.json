{"id": "1502.03648", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Over-Sampling in a Deep Neural Network", "abstract": "Deep neural networks (DNN) are the state of the art on many engineering problems such as computer vision and audition. A key factor in the success of the DNN is scalability - bigger networks work better. However, the reason for this scalability is not yet well understood. Here, we interpret the DNN as a discrete system, of linear filters followed by nonlinear activations, that is subject to the laws of sampling theory. In this context, we demonstrate that over-sampled networks are more selective, learn faster and learn more robustly. Our findings may ultimately generalize to the human brain.", "histories": [["v1", "Thu, 12 Feb 2015 13:29:03 GMT  (122kb)", "http://arxiv.org/abs/1502.03648v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["andrew j r simpson"], "accepted": false, "id": "1502.03648"}, "pdf": {"name": "1502.03648.pdf", "metadata": {"source": "META", "title": "Over-Sampling in a Deep Neural Network_arxiv", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "The reason for this, however, is that the networks mentioned are more selective and that they are able to do more than they do."}, {"heading": "II. METHOD", "text": "It is indeed the case that we will be able to go in search of a solution that is capable of solving the problems that we have to solve."}, {"heading": "III. RESULTS AND DISCUSSION", "text": "Figure 2a Graphs Classification Errors for each model, applied to the classification of individual test data (10,000 examples), as a function of the number of iterations of training. Therefore, the 16x rammed models (784x784x10 units) showed the best overall results, converged fastest, and showed no signs of overmatching. However, the models on the 1x and 2x over sampling showed poorer overall results and showed no signs of convergence sufficient to determine whether they would ultimately provide a non-monotonic error function. 4x and 8x models were competitive during the adjustment (low numbers of iterations), but later showed extreme non-monotonicity indicating overmatching. So, contrary to conventional expectations, the largest (16x) model showed the least sign of overmatching, and the 8x model showed less severe overmatching than the 4x model. Contrary to popular belief, these results show that larger models can be adjusted."}, {"heading": "IV. CONCLUSION", "text": "In this paper, we have presented a sampling interpretation of deep neural networks, which interprets the system in terms of learned filters and management of nonlinear distortions. We have shown that over-sampling networks perform better, are more selective, learn faster, and fit less. These results have far-reaching implications for how deep neural networks are interpreted, designed, and understood. Finally, our results suggest that a hugely over-sampled system would learn robustly and quickly, so it could be that our results are generalized to the vast scale of the biological brain."}, {"heading": "ACKNOWLEDGMENT", "text": "The AJRS was supported by funding EP / L027119 / 1 from the UK Engineering and Physical Sciences Research Council (EPSRC)."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["E. Hinton G", "S. Osindero", "Y. Teh"], "venue": "Neural Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition", "author": ["Ciresan C. D", "U. Meier", "M. Gambardella L", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors,", "author": ["E. Hinton G", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Computing Research Repository (CoRR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["E. Dahl G", "N. Sainath T", "E. Hinton G"], "venue": "in Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The simplest form of feed-forward deep neural network (DNN) features at least two layers of neurons that, via a full complement of weights, are fully connected to inputs from the layer below [1], [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 0, "context": "A typical problem for a DNN is image classification [1], whereby some rectangular matrix of pixel intensities is unpacked into a vector which constitutes the input to the first layer of the network.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "We chose the well-known computer vision problem of hand-written character classification using the MNIST dataset [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "At an image resolution of 28x28 pixels, this problem has been thoroughly solved using DNN [1], [3], [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "At an image resolution of 28x28 pixels, this problem has been thoroughly solved using DNN [1], [3], [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "At an image resolution of 28x28 pixels, this problem has been thoroughly solved using DNN [1], [3], [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": "[1], we used sigmoid activation functions [ = 1/(1 + exp(\u2212 ] in a fully connected network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1]), we kept the first two layers at the same sample rate (i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "examples from the MNIST dataset [3] using stochastic gradient descent (SGD).", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "These results suggest that over-sampling may be a useful alternative to regularization by dropout [5], [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "These results suggest that over-sampling may be a useful alternative to regularization by dropout [5], [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Practically speaking, dropout is well known for increasing the training time necessary [5].", "startOffset": 87, "endOffset": 90}], "year": 2015, "abstractText": "Deep neural networks (DNN) are the state of the art on many engineering problems such as computer vision and audition. A key factor in the success of the DNN is scalability \u2013 bigger networks work better. However, the reason for this scalability is not yet well understood. Here, we interpret the DNN as a discrete system, of linear filters followed by nonlinear activations, that is subject to the laws of sampling theory. In this context, we demonstrate that over-sampled networks are more selective, learn faster and learn more robustly. Our findings may ultimately generalize to the human brain.", "creator": "PDFCreator Version 1.7.1"}}}