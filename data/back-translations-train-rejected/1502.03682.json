{"id": "1502.03682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2015", "title": "Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation", "abstract": "BACKGROUND: The amount of biomedical literature is rapidly growing and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up-to-date. In this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for identifying relationships from unstructured text. We evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora available on the web. Properties included relationships to diseases ('may treat') or physiological processes ('has physiological effect'). We compared the relationships identified by word2vec with manually curated information from the National Drug File - Reference Terminology (NDF-RT) ontology as a gold standard. RESULTS: Our results revealed a maximum accuracy of 49.28% which suggests a limited ability of word2vec to capture linguistic regularities on the collected medical corpora compared with other published results. We were able to document the influence of different parameter settings on result accuracy and found and unexpected trade-off between ranking quality and accuracy. Pre-processing corpora to reduce syntactic variability proved to be a good strategy for increasing the utility of the trained vector models. CONCLUSIONS: Word2vec is a very efficient implementation for computing vector representations and for its ability to identify relationships in textual data without any prior domain knowledge. We found that the ranking and retrieved results generated by word2vec were not of sufficient quality for automatic population of knowledge bases and ontologies, but could serve as a starting point for further manual curation.", "histories": [["v1", "Thu, 12 Feb 2015 14:44:15 GMT  (1025kb)", "http://arxiv.org/abs/1502.03682v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG cs.NE", "authors": ["jose antonio mi\\~narro-gim\\'enez", "oscar mar\\'in-alonso", "matthias samwald"], "accepted": false, "id": "1502.03682"}, "pdf": {"name": "1502.03682.pdf", "metadata": {"source": "CRF", "title": "Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation", "authors": ["Jose Antonio Mi\u00f1arro-Gim\u00e9nez", "Oscar Mar\u00edn-Alonso", "Matthias Samwald"], "emails": [], "sections": [{"heading": null, "text": "Background: The amount of biomedical literature is growing rapidly and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up-to-date. In this study, we applied the word2vec Deep Learning Toolkit to medical corporations to test their potential for identifying relationships from unstructured text. We evaluated the efficiency of word2vec in identifying characteristics of pharmaceuticals based on medium-sized, unstructured medical text corpora available on the Internet. Properties included relationships with diseases (\"can treat\") or physiological processes (\"has physiological effects\"). We compared the relationships identified by word2vec with manually curated information from the National Drug File - Reference Terminology (NDF-RT) ontology as the gold standard. We used various word2vec parameter settings and models to compare their impact on outcome quality. Results: Our results showed a maximum accuracy of 428% indicating a 29.2% relationship."}, {"heading": "Introduction", "text": "This year, we have reached the point where we are able to move to a country where most people are able to leave a country, where they are able to move, and where they are able to move."}, {"heading": "Material and Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Word2vec", "text": "Word2vec is an efficient implementation of deep learning techniques based on two architectures, Continuous Bag-of-Words (CBOW) and Skip-gram (SG) [5], to calculate a continuous, distributed vector representation of words from large datasets (up to hundreds of billions of words).Word2vec requires training of the corpora using one of these architectures. The training tool offers the following options: (i) type of architecture: Continuous Bag-of-Words or Skip-gram; (ii) the dimensionality of the vector space; (iii) the size of the context window in word count; (iv) the training algorithm: hierarchical softmax and / or negative sampling; (v) the threshold for scanning the common words; (vi) the number of threads to be used; and (vii) the output word time table format is an example of 1."}, {"heading": "Medical text corpora", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "Evaluation", "text": "To perform the pilot evaluation of word2vec vector tools for the medical text, we have defined the following workflow for analyzing and evaluating the terms generated by the word2vec tools: (1) capturing and processing openly available medical text corpora; (2) computing the results from the word2vec distance and analogy tools to assess how well the models captured the medical relationships, such as \"can\" and \"can prevent the medical references vector vector vector vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-calculation of statistics to evaluate the effects of various parameter configurations. (3) Capturing and processing medical corporation required vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vectors-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vectors-vectors-vectors-vectors-vectors-vectors-vectors-"}, {"heading": "Assessment system", "text": "In order to evaluate the vector models of the trained corpora, the results of the word2vec distance and analogy tools were compared with the curated content of the NDF-RT ontology as the gold standard. To perform this evaluation, we developed a system that automatically queried a trained vector model using the distance and analogy tools of word2vec and matched the resulting wordlist with the content of the NDF-RT ontology.Figure 3 shows the software architecture of the evaluation system.The module \"RESTful Server\" made the word2vec analogy and distance tools accessible through RESTful Services. RESTful Services were used through the Java-based Jersey framework [16] and facilitated access to the vector space models from external applications. Both services provided forty words with their cosmic similarity values. The module \"RESTful Client\" (Figure 3) was responsible for collecting the information required from the NDF by automatically consulting the results from the RT-DF."}, {"heading": "Results and Discussion", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "Conclusions", "text": "The ability of word2vec to retrieve the expected terms from the size-limited corpora we use is not suitable for applications that require high precision, as we have only achieved an accuracy of 49.28%. These modest results could be explained by the limited size of the medical corpora collected and the complexity of the medical knowledge area. We may be interested in future research to test this tool with larger, commercially available medical corpora. Due to the complexity of the medical terminology, we found the pre-processing of corpora necessary to reduce syntactical variability. We also found that many relevant medical terms were composed of several words and that the ontological pre-processing of these terms resulted in a significant improvement in the results from the word2vec tools. As expected, the analogy tool led to better results in identifying related entities for a certain type of relationship than the distance terms, and the larger corpora pre-processing based on these corpora result."}, {"heading": "Acknowledgments", "text": "We would like to thank the word2vec team for their support in adjusting the parameters of the word2vec tools."}], "references": [{"title": "Corpus annotation for mining biomedical events from literature", "author": ["J-D Kim", "T Ohta", "J Tsujii"], "venue": "BMC Bioinformatics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y Bengio", "A Courville", "P Vincent"], "venue": "IEEE Trans Pattern Anal Mach Intell", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T Mikolov", "K Chen", "G Corrado", "J Dean"], "venue": "Cs. Available: http://arxiv.org/abs/1301.3781. Accessed", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J Pennington", "R Socher", "CD Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["O Levy", "Y Goldberg"], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A Frome", "GS Corrado", "J Shlens", "S Bengio", "J Dean", "T Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Wikipedia: a key tool for global public health promotion", "author": ["JM Heilman", "E Kemmann", "M Bonert", "A Chatterjee", "B Ragar"], "venue": "J Med Internet Res 13:", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": " Trusted Medical and Scientific Information", "author": ["THE MERCK MANUAL"], "venue": "Available: http://www.merckmanuals.com/. Accessed 15 October", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The large amount of biomedical information in databases such as PubMed [1] is a valuable source for automated information extraction [2] that facilitates the development of more efficient biomedical information retrieval systems.", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "It refers to unsupervised learning algorithms which automatically discover data without the need of supplying specific domain knowledge [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "3 Word2vec [4] implements an efficient deep learning algorithm for computing high-dimensional vector representations of words and their relationships [5] based on unstructured text data.", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "While similar approaches such as GloVe [6] were recently claimed to outperform word2vec tools for the unsupervised learning of word representations and word analogy, we utilized word2vec because it has been widely studied [6-8] and, therefore, our results can be easily compared with others.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "While similar approaches such as GloVe [6] were recently claimed to outperform word2vec tools for the unsupervised learning of word representations and word analogy, we utilized word2vec because it has been widely studied [6-8] and, therefore, our results can be easily compared with others.", "startOffset": 222, "endOffset": 227}, {"referenceID": 4, "context": "While similar approaches such as GloVe [6] were recently claimed to outperform word2vec tools for the unsupervised learning of word representations and word analogy, we utilized word2vec because it has been widely studied [6-8] and, therefore, our results can be easily compared with others.", "startOffset": 222, "endOffset": 227}, {"referenceID": 5, "context": "While similar approaches such as GloVe [6] were recently claimed to outperform word2vec tools for the unsupervised learning of word representations and word analogy, we utilized word2vec because it has been widely studied [6-8] and, therefore, our results can be easily compared with others.", "startOffset": 222, "endOffset": 227}, {"referenceID": 2, "context": "Word2vec Word2vec is an efficient implementation of deep learning techniques based on two architectures, continuous bag-of-words (CBOW) and skip-gram (SG) [5], for computing continuous distributed vector representation of words from large datasets (up to hundreds of billions of words).", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "We created a corpus of medically relevant content from Wikipedia by selecting all articles that were associated with Wikiproject Medicine or Wikiproject Pharmacology [12] through manual curation of Wikipedia editors.", "startOffset": 166, "endOffset": 170}, {"referenceID": 7, "context": "We also included two popular publicly available websites with content for medical professionals: Medscape [13] and Merck Manual [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "This is also consistent with other evaluations such as [6] which obtained an accuracy of 61% with SG, a dimension vector of 300 and 1B word corpus.", "startOffset": 55, "endOffset": 58}], "year": 2015, "abstractText": null, "creator": "Microsoft\u00ae Word 2010"}}}