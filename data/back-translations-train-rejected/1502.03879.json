{"id": "1502.03879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "Semi-supervised Data Representation via Affinity Graph Learning", "abstract": "We consider the general problem of utilizing both labeled and unlabeled data to improve data representation performance. A new semi-supervised learning framework is proposed by combing manifold regularization and data representation methods such as Non negative matrix factorization and sparse coding. We adopt unsupervised data representation methods as the learning machines because they do not depend on the labeled data, which can improve machine's generation ability as much as possible. The proposed framework forms the Laplacian regularizer through learning the affinity graph. We incorporate the new Laplacian regularizer into the unsupervised data representation to smooth the low dimensional representation of data and make use of label information. Experimental results on several real benchmark datasets indicate that our semi-supervised learning framework achieves encouraging results compared with state-of-art methods.", "histories": [["v1", "Fri, 13 Feb 2015 03:35:15 GMT  (518kb)", "http://arxiv.org/abs/1502.03879v1", "10 pages,2 Tables. Written in Aug,2013"]], "COMMENTS": "10 pages,2 Tables. Written in Aug,2013", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["weiya ren"], "accepted": false, "id": "1502.03879"}, "pdf": {"name": "1502.03879.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Data Representation via Affinity Graph Learning", "authors": ["Weiya Ren"], "emails": ["weiyren.phd@gmail.com"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Graph-based Semi-supervised Learning (GSSL)", "text": "GSSL techniques begin with the calculation of a similarity value between all node pairs using a similarity function or a nucleus, and then an algorithm is selected to find a low-weighted subgraph from the fully connected similarity graph. There are two typical methods for creating a sparse graph: neighborhood approaches, including nearest and adjacent algorithms, and matching approaches, such as b-matching [10]. After a graph has been sparsely labeled, several methods can be used to update graph weights, such as binary weighting and Gaussian kernel weighting. Finally, GSSL algorithms distribute labels on the known part of the graph to unknown nodes, taking into account the final affinity graph and some initial marking information.Among the best GSSL techniques currently are the greedy Max-Cut method [1], the lacquer support method [8] and the global conversion method [5]."}, {"heading": "B. Metric learning", "text": "Metric learning expects the learned metric to make the distances between similar samples small and the distances between unequal samples large. In general, a Mahalanobis distance metric measures the square distance between two data points and: () () () (1), which is a positive semi-definitive matrix and is a sample pair. Currently, the best metric learning methods include Large Margin Next Neighbor Learning (LMNN) [11], Information Theory Metric Learning (ITML) [12] and KISS Metric Learning [7].III. PROPOSED FRAMEWORK"}, {"heading": "A. Framwork", "text": "Traditional semi-supervised methods of unattended data representation usually use the label Weight as affinity [15]. Label Weight is constructed as follows: {(2) In this way, the unsupervised data are completely ignored. In the proposed framework, prior information is embedded in the triple regulation to control the smoothness of the data representation. Independent and identically distributed samples of characteristic dimensions and instances are assumed, and the first samples have labels. The learned weight matrix forms the following laplac regulator, which is used to measure the smoothness of the low-dimensional representation: \u2022 (3) Where is the data representation of data, \u2022 the trace of a matrix andis denotes a diagonal matrix whose inputs are column totals. Subsequently, the loss function is defined in the proposed framework as follows: (4) Where is the loss function of a measurement unsupervised method (the pararization of the pararization of the measurement method)."}, {"heading": "B. Affinity graph learning", "text": "We use the metric KISS learning method [7] to generate a kernel matrix to measure the similarity between all sample pairs. Then, the similarity between all node pairs generates a complete adjacence matrix in which ().Then, -next neighbor algorithm or matching algorithm can be applied to create a sparse graph ().If the graph is sparsified, Gaussian kernel weighting is used to update the graph weights, generate a finality graph in which the edge weight between two connected samples is calculated and as follows: (5) Where is the parameter of the kernel bandwidth and we always \u2211 \u221a."}, {"heading": "C. Algorithms under the proposed framework", "text": "We use two powerful, uncontrolled data representation methods: Graph Regularized Nonnegative MatrixFactorization (GNMF) [4] and Graph Regularized Sparse Coding (GSC) [6] to illustrate our framework. [1] Semi-supervised Graph Regularized Nonnegative Matrix FactorizationGNMF expands the NMF by explicitly taking into account the multiple assumptions [3] and the locally invariant idea, i.e. thenearby points are likely to have similar embedding. GNMF is expanded into a semi-supervised method by using the proposed affinity graph. In the face of a non-negative datamatrix, the base matrix is andbe the coefficient matrix."}, {"heading": "IV. EXPERIMENTS", "text": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the areas of clustering and image clustering tasks [1], [4], [6]. We examine the clustering performance of the proposed semi-supervised framework on three real image datasets, i.e., the Yale Database 1, the ORL Database 2, and the FEI Database consists of 10 different images for each of 40 different subjects, taken at different times, under different lighting conditions, with different facial expressions, and without glasses. Yale Database contains 165 Grayscale images of 15 individuals. There are 11 images per subject, one per different facial expression or configuration. The FEI Facial Database is set against a white homogeneous background in a upright frontal position with profile rotation of up to about 180 degrees."}, {"heading": "V. CONCLUSIONS", "text": "We present a novel semi-supervised framework that explicitly takes into account the generatability of learning machines and previous information. By learning the affinity graph, the laplactic regulator is integrated into the method of unattended data representation. Experimental results of clustering have shown that our proposed framework can be more discerning and significantly improve the performance of data representation."}, {"heading": "VI. ACKNOWLEDGMENTS", "text": "This work is supported by the College of Information System and Management of the National University of Defense Technology and is subsidized by the National Natural Science Foundation of China Grant No. 61170158 and the Open Project Program of the State Key Laboratory of Mathematical Engineering and Advanced Computing Grant 2013A08."}, {"heading": "VII. REFERENCES", "text": "[1] Jun Wang, Tony Jebara, and Shih-Fu Chang, \"Semi-Supervised Learning Using Greedy Max-Cut,\" Journal of Machine Learning Research, 2013. [2] Rui Zhao, Wanli Ouyang, and Xiaogang Wang, \"Unsupervised Salience Learning for Person Re-Identification,\" Proc. IEEE Intern. Conf.on Computer Vision and Pattern Recognition, 2013. [3] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani, \"Manifold Regularization: A Geometric Framework for Learning from Labeled andlabeled Examples and Pattern Recognition, 2006. [4] D. Cai, X. Wu, and J. Han,\" Graph Regularized Non-negative Matrix Factorization."}], "references": [{"title": "Semi-Supervised Learning Using Greedy Max -Cut", "author": ["Jun Wang", "Tony Jebara", "Shih-Fu Chang"], "venue": "Journal of Machine Learning Research, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised Salience Learning for Person Re-identification", "author": ["Rui Zhao", "Wanli Ouyang", "Xiaogang Wang"], "venue": "Proc. IEEE Intern. Conf. on Computer Vision and Pattern Recognition, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "Journal of Machine Learning Research, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Graph Regularized Non-negative Matrix Factorization for Data Representation", "author": ["D. Cai", "X. He", "X. Wu", "J. Han"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel, vol.33, no.8, 1548-1560, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch \u00f6lkopf"], "venue": "S. Thrun, L. Saul, and B. Sch \u00f6lkopf, editors, Advances in Neural Information Processing Systems, volume 16, pages 321\u2013328. 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Graph regularized sparse coding for image representation", "author": ["M. Zheng", "J. Bu", "C.A. Chen", "C. Wang", "L. Zhang", "G. Qiu", "D. Cai"], "venue": "IEEE Trans. Image Process., vol. 20, pp. 1327-1336, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale metric learning from equivalence constraints,", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "in Proc. IEEE Conf. Comput. Vision Pattern Recogn.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "On manifold regularization", "author": ["M. Belkin", "P. Niyogi", "V Sindhwani"], "venue": "Int. Workshop on Artificial Intelligence and Statistics, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Semi-supervised learning with max-margin graph cuts", "author": ["B. Kveton", "M. Valko", "A. Rahimi", "L. Huang"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, pages 421\u2013428, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph Construction and b-Matching for Semi-Supervised Learning", "author": ["Tony Jebara", "Jun Wang", "Shih-Fu Chang"], "venue": "Proceedings of the 26th International Conference on Machine Learning, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proc. IEEE Intern. Conf. on Machine Learning, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Information-theoretic metric learning[C]//Proceedings of the 24th international conference on Machine learning", "author": ["V Davis J", "B Kulis", "P Jain"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Label propagation through linear neighborhoods", "author": ["F. Wang", "C.S. Zhang"], "venue": "IEEE Trans. Knowl. Data Eng., 20, 55\u201367, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Document Clustering Based on Non-Negative Matrix Factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "Proc. Ann. ACM SIGIR Conf. Research and Development in Information Retrieval, 2003. 10", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Constrained nonnegative matrix factorization for image representation", "author": ["H Liu", "Z Wu", "X Li", "D Cai", "TS Huang"], "venue": "IEEE Trans Pattern Anal Mach Intell 34(7):1299\u20131311, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 7, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 8, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 9, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 12, "context": "A variety of graph-based semi-supervised learning (GSSL) [1,5,8,9,10,13] have recently become popular due to their high accuracy and computational efficiency.", "startOffset": 57, "endOffset": 72}, {"referenceID": 2, "context": "Inspired by graph based method [3,4,6], which usually use an affinity graph to smooth the representation of data.", "startOffset": 31, "endOffset": 38}, {"referenceID": 3, "context": "Inspired by graph based method [3,4,6], which usually use an affinity graph to smooth the representation of data.", "startOffset": 31, "endOffset": 38}, {"referenceID": 5, "context": "Inspired by graph based method [3,4,6], which usually use an affinity graph to smooth the representation of data.", "startOffset": 31, "endOffset": 38}, {"referenceID": 2, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 183, "endOffset": 192}, {"referenceID": 10, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 183, "endOffset": 192}, {"referenceID": 11, "context": "Different from GSSL framework and manifold regularization framework [3], which always use unsupervised Gussian kernel to construct similarity graph, we utilize metric learning method [7,11,12] to produce a kernel matrix (also known as a \u201cGram matrix\u201d) to measure the similarity between all pairs of samples, subsequently, the similarity matrix is sparsified and reweighted to produce the final affinity graph.", "startOffset": 183, "endOffset": 192}, {"referenceID": 9, "context": "There are two typical ways to build a sparse graph: neighborhood approaches including the k-nearest and neighbors algorithms, and matching approaches such as b-matching [10].", "startOffset": 169, "endOffset": 173}, {"referenceID": 0, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 150, "endOffset": 153}, {"referenceID": 12, "context": "The current best GSSL techniques include the greedy max-cut method [1], Laplacian support vector machine [8], the local and global consistency method [5] and the alternating graph transduction method [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 10, "context": "The current best Metric learning techniques include Large margin nearest neighbor learning (LMNN) [11], Information theoretic metric learning (ITML) [12], and KISS metric learning [7].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "The current best Metric learning techniques include Large margin nearest neighbor learning (LMNN) [11], Information theoretic metric learning (ITML) [12], and KISS metric learning [7].", "startOffset": 149, "endOffset": 153}, {"referenceID": 6, "context": "The current best Metric learning techniques include Large margin nearest neighbor learning (LMNN) [11], Information theoretic metric learning (ITML) [12], and KISS metric learning [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 14, "context": "Traditional semi-supervised manner for unsupervised data representation usually use the Label Weight as the affinity [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "We utilize KISS metric learning method [7] to produce a kernel matrix to measure the similarity between all pairs of samples.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "We adopt two powerful unsupervised data representation methods: Graph Regularized Nonnegative Matrix Factorization (GNMF) [4] and Graph Regularized Sparse Coding (GSC) [6] to display our framework.", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "We adopt two powerful unsupervised data representation methods: Graph Regularized Nonnegative Matrix Factorization (GNMF) [4] and Graph Regularized Sparse Coding (GSC) [6] to display our framework.", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "1) Semi-supervised Graph Regularized Nonnegative Matrix Factorization GNMF extend NMF by explicitly considering the manifold assumption [3] and the locally invariant idea , i.", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Following the iteratively optimization method in [6], the GSC algorithm can learn the graph regularized sparse codes and the learning dictionary iteratively.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the document clustering and image clustering tasks [1], [4], [6].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the document clustering and image clustering tasks [1], [4], [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "Previous studies show that GNMF, GSC and GMC are very powerful for clustering, especially in the document clustering and image clustering tasks [1], [4], [6].", "startOffset": 154, "endOffset": 157}, {"referenceID": 14, "context": "\uf06c Constrained Nonnegative Matrix Factorization (CNMF) [15].", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "\uf06c The Local and Global Consistency method (LGC) [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "\uf06c Greedy Max\u2013Cut (GMC) [1].", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "\uf06c Graph regularized Nonnegative Matrix Factorization (GNMF)[4] +K\u2013means.", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "\uf06c Graph regularized Sparse Coding (GSC) [6]+ K\u2013means.", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "For other methods, we use Accuracy Metric (AC) [14] to evaluate the clustering performance.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "We follow the parameter settings for all algorithms to achieve their best performance, see details in [1,4,6].", "startOffset": 102, "endOffset": 109}, {"referenceID": 3, "context": "We follow the parameter settings for all algorithms to achieve their best performance, see details in [1,4,6].", "startOffset": 102, "endOffset": 109}, {"referenceID": 5, "context": "We follow the parameter settings for all algorithms to achieve their best performance, see details in [1,4,6].", "startOffset": 102, "endOffset": 109}, {"referenceID": 14, "context": "42 CNMF [15] 55.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "69 LGC [5] 41.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "54 GMC [1] 45.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "43 GNMF [4] 48.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "65 LGNMF [15] 48.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "99 GSC [6] 51.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "ORL, k=5 Yale, k=5 FEI, k=5 Number of labeled samples of each subject 2 5 8 2 5 8 2 5 8 10 Test Runs 100 100 100 100 100 100 100 100 100 100 CNMF [15] 74.", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "65 LGC [5] 75.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "53 GMC [1] 81.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "69 LGNMF [15] 67.", "startOffset": 9, "endOffset": 13}], "year": 2015, "abstractText": "We consider the general problem of utilizing both labeled and unlabeled data to improve data representation performance. A new semi-supervised learning framework is proposed by combing manifold regularization and data representation methods such as Non negative matrix factorization and sparse coding. We adopt unsupervised data representation methods as the learning machines because they do not depend on the labeled data, which can improve machine\u2019s generation ability as much as possible. The proposed framework forms the Laplacian regularizer through learning the affinity graph. We incorporate the new Laplacian regularizer into the unsupervised data representation to smooth the low dimensional representation of data and make use of label information. Experimental results on several real benchmark datasets indicate that our semi-supervised learning framework achieves encouraging results compared with state-of-art methods.", "creator": "Microsoft\u00ae Word 2010"}}}