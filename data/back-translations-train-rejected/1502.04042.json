{"id": "1502.04042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "Abstract Learning via Demodulation in a Deep Neural Network", "abstract": "Inspired by the brain, deep neural networks (DNN) are thought to learn abstract representations through their hierarchical architecture. However, at present, how this happens is not well understood. Here, we demonstrate that DNN learn abstract representations by a process of demodulation. We introduce a biased sigmoid activation function and use it to show that DNN learn and perform better when optimized for demodulation. Our findings constitute the first unambiguous evidence that DNN perform abstract learning in practical use. Our findings may also explain abstract learning in the human brain.", "histories": [["v1", "Fri, 13 Feb 2015 16:09:41 GMT  (90kb)", "http://arxiv.org/abs/1502.04042v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["andrew j r simpson"], "accepted": false, "id": "1502.04042"}, "pdf": {"name": "1502.04042.pdf", "metadata": {"source": "CRF", "title": "Abstract Learning via Demodulation in a Deep Neural Network", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "This is not the first time that DNN has learned abstract representations through a process of demodulation. We present a biased sigmoid activation function and use it to show that DNN learns and works better when optimized for demodulation. Our results are the first clear evidence that DNN performs abstract learning in practice. Our findings may also explain abstract learning in the human brain."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["E. Hinton G", "S. Osindero", "Y. Teh"], "venue": "Neural Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Deep big simple neural nets excel on handwritten digit recognition", "author": ["Ciresan C. D", "U. Meier", "M. Gambardella L", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors,", "author": ["E. Hinton G", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Computing Research Repository (CoRR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["E. Dahl G", "N. Sainath T", "E. Hinton G"], "venue": "in Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems", "author": ["P Dayan", "LF Abbott"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Over-Sampling in a Deep Neural Network, arxiv.org abs/1502.03648", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNN) are state of the art for many machine learning problems [1], [2], [3], [4], [5], [6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "Deep neural networks (DNN) are state of the art for many machine learning problems [1], [2], [3], [4], [5], [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Deep neural networks (DNN) are state of the art for many machine learning problems [1], [2], [3], [4], [5], [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "Deep neural networks (DNN) are state of the art for many machine learning problems [1], [2], [3], [4], [5], [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "Deep neural networks (DNN) are state of the art for many machine learning problems [1], [2], [3], [4], [5], [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "The architecture of deep neural networks is inspired by the hierarchical structure of the brain [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "In this paper, we take a sampling theory perspective [8] and we interpret the nonlinear activation function of the DNN as a demodulation device within the context of the well-known MNIST [4] hand-written character classification problem (example image in Fig.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "In this paper, we take a sampling theory perspective [8] and we interpret the nonlinear activation function of the DNN as a demodulation device within the context of the well-known MNIST [4] hand-written character classification problem (example image in Fig.", "startOffset": 187, "endOffset": 190}, {"referenceID": 0, "context": "The sigmoidal activation function comprises a zero-centred sigmoid which is mapped to the range [0, 1].", "startOffset": 96, "endOffset": 102}, {"referenceID": 2, "context": "1) to a typical feed-forward DNN, which we used to solve the well-known problem of hand-written character recognition on the MNIST dataset [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "We chose the well-known computer vision problem of hand-written character classification using the MNIST dataset [3], [1], [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "We chose the well-known computer vision problem of hand-written character classification using the MNIST dataset [3], [1], [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "We chose the well-known computer vision problem of hand-written character classification using the MNIST dataset [3], [1], [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "We trained the various instances of the model on the 60,000 training examples from the MNIST dataset [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "In this context, the alternate nonlinear activation functions (abs, tanh, rectified linear unit - ReLU, etc [6]) may be understood as demodulators with different properties.", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "It has been observed that models employing the ReLU function outperform sigmoid activated models [6] in ways that are similar to the results in Fig.", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "However, it has also been observed that such models require far greater degrees of regularization (such as dropout [6]), which the biased sigmoid activation function does not appear to require.", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "Assuming that demodulation performance is the reason for the improved performance in both cases, the difference in terms of need for regularization is likely due to the high order distortion generated by the abrupt nonlinearity (of the ReLU) and its increased potential for aliasing [8].", "startOffset": 283, "endOffset": 286}, {"referenceID": 5, "context": "More generally, given that the DNN takes its inspiration from the brain, it is interesting to note that the bias scheme described here is not necessary in order for the equivalent brain network to perform abstract learning because the sigmoid activated neurons of the brain are not signed or zeromean operated [7].", "startOffset": 310, "endOffset": 313}], "year": 2015, "abstractText": "Learning via Demodulation in a Deep Neural Network Andrew J.R. Simpson #1 # Centre for vision, speech and signal processing (CVSSP), University of Surrey, Guildford, Surrey, UK 1 Andrew.Simpson@Surrey.ac.uk Abstract\u2014Inspired by the brain, deep neural networks (DNN) are thought to learn abstract representations through their hierarchical architecture. However, at present, how this happens is not well understood. Here, we demonstrate that DNN learn abstract representations by a process of demodulation. We introduce a biased sigmoid activation function and use it to show that DNN learn and perform better when optimized for demodulation. Our findings constitute the first unambiguous evidence that DNN perform abstract learning in practical use. Our findings may also explain abstract learning in the human brain.Inspired by the brain, deep neural networks (DNN) are thought to learn abstract representations through their hierarchical architecture. However, at present, how this happens is not well understood. Here, we demonstrate that DNN learn abstract representations by a process of demodulation. We introduce a biased sigmoid activation function and use it to show that DNN learn and perform better when optimized for demodulation. Our findings constitute the first unambiguous evidence that DNN perform abstract learning in practical use. Our findings may also explain abstract learning in the human brain.", "creator": "PDFCreator Version 1.7.1"}}}