{"id": "1502.04149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2015", "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation", "abstract": "Monaural source separation is important for many real world applications. It is challenging in that, given only single channel information is available, there is an infinite number of solutions without proper constraints. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including the monaural speech separation task, monaural singing voice separation task, and speech denoising task. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative training criterion for the neural networks to further enhance the separation performance. We evaluate our proposed system on TSP, MIR-1K, and TIMIT dataset for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30~4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30~2.48 dB GNSDR gain and 4.32~5.42 dB GSIR gain compared to previous models in the singing voice separation task, and outperform NMF and DNN baseline in the speech denoising task.", "histories": [["v1", "Fri, 13 Feb 2015 23:22:16 GMT  (8542kb,D)", "http://arxiv.org/abs/1502.04149v1", "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. 11 pages"], ["v2", "Tue, 2 Jun 2015 04:22:20 GMT  (8630kb,D)", "http://arxiv.org/abs/1502.04149v2", "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. 12 pages"], ["v3", "Thu, 13 Aug 2015 04:20:33 GMT  (1892kb,D)", "http://arxiv.org/abs/1502.04149v3", "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015 12 pages"], ["v4", "Thu, 1 Oct 2015 02:58:01 GMT  (1754kb,D)", "http://arxiv.org/abs/1502.04149v4", null]], "COMMENTS": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing. 11 pages", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.LG cs.MM", "authors": ["po-sen huang", "minje kim", "mark hasegawa-johnson", "paris smaragdis"], "accepted": false, "id": "1502.04149"}, "pdf": {"name": "1502.04149.pdf", "metadata": {"source": "CRF", "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation", "authors": ["Po-Sen Huang", "Minje Kim", "Mark Hasegawa-Johnson"], "emails": ["huang146@illinois.edu;", "jhasegaw@illinois.edu)", "minje@illinois.edu)", "paris@illinois.edu)"], "sections": [{"heading": null, "text": "In this paper, we focus on the source separation of monaural language separations, vocal voices and language denocializations tasks.P.-S. Huang and M. Hasegawa Johnson are the University's electricity systems with electricity separation and electricity separation. There are various approaches in which different types of signals have been mixed together and the goal is to restore the original signals from the combined signal. Source separation is important for several real applications of real language. For example, the accuracy of chord recognition and tone appreciation can be improved by separating vocal voice and music. [1] The accuracy of automatic speech recognition (ASR) can be improved by separating speech signals [2]. In this paper, we focus on source separation of monaural speech signals, i.e., source separation of monaural speech recordings is more challenging, without prior knowledge, there are only a few solutions available."}, {"heading": "II. PROPOSED METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Deep Recurrent Neural Networks", "text": "To capture the contextual information between the audio signals, one possibility is to bundle adjacent features together as input features to form the deep neural network = 1. However, the number of parameters increases proportionally to the input dimension and the number of neighbors in time. Therefore, the size of the concatenated window is limited. A recursive neural network (RNN) can be considered as a DNN with an unlimited number of layers that introduce memory from earlier time steps. [24] The potential weakness for RNs is that RNs have no hierarchical processing of the input in the current time step. To provide the hierarchical information through multiple time scales, deep neural networks (DRNNs) are explored [24]. DRNNs can be explored in different schemes, as shown in Figure 1. The left side of Figure 1 is a standard RNN folded in time."}, {"heading": "B. Model Architecture", "text": "In fact, we are able to separate one of the sources from another, rather than recognizing one of the sources as the target, and model the framework from all the other sources at the same time. Figure 2 shows an example of the architecture. In addition, we find it useful to combine source separation with time-frequency masking, for example, by modeling the framework from all the other sources at the same time. Figure 2 shows that it is an example of the architecture."}, {"heading": "C. Training Objectives", "text": "Given the output predictions y \u2212 1t and y \u2212 2t (or y \u2212 1t and y \u2212 2t) of the original sources y1t and y2t, we examine the optimization of the parameters of the neural network by minimizing the square error as follows: JMSE = square error 1t \u2212 y1t \u00b2 22 + square error 2t \u2212 y2t \u00b2 22 (7) Equation measures the difference between predicted and actual targets. If the targets have similar spectra, it is possible for the DNN to minimize the equation (7) by being too conservative: If a feature could be attributed to either source 1 or source 2, the neural network attributes them both."}, {"heading": "III. EXPERIMENTS", "text": "In this section, we evaluate our proposed models against a speech separation task, a voice separation task, and a speech separation task. Source separation is evaluated using three quantitative values: Source to Interference Ratio (SIR), Source to Artifacts Ratio (SAR), and Source to Distortion Ratio (SDR) according to BSS-EVAL metrics [28]. Higher values of SDR, SAR, and SIR represent better separation quality. Interference suppression is reflected in SIR. Artifacts introduced by the separation process are reflected in SAR. Overall performance is reflected in SDR. For the language separation task, we also calculate the short-term objective comprehensibility measure (STOI), which is a quantitative estimate of speech intelligibility."}, {"heading": "A. Speech Separation Setting", "text": "We evaluate the performance of the proposed approaches to monaural speech separation with the TSP corpus [31]. In the TSP dataset, we select four speakers, FA, FB, MC and MD, from the TSP language database. After concatenating all 60 sets per speaker, we use 90% of the signal for training and 10% for testing. Note that in the neural network experiments, we divide the training into 8: 1 to set aside 10% of the data for validation, the signals are sampled down to 16 kHz and then transformed with 1024 points DFT with 50% overlap for generating spectra. Neural networks are trained in three different mixed cases: FA vs. MC, FA vs. FB and MC vs. MD. As FA and FB are female speakers, while MC and MD are male, the latter two cases become more difficult because the frequencies of the same sex are compared."}, {"heading": "B. Speech Separation Results", "text": "In the second half of the last decade, in the second half of the last decade, in the zeroes in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 19th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 19th century, in the second half of the 19th century, in the second half of the 19th century, in the second half of the 20th century, in the second half of the 19th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 19th century, in the second half of the 19th century, and in the second half of the 19th century, in the second half of the 19th century, in the second half of the 19th century, in the second half of the second half of the 20th century, and in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the second half of the 20th century, and in the second half of the 20th century, in the second half of the 20th century, in the second half of the second half of the 20th century, in the second half of the second half of the 20th century, in the 20th century, in the second half, in the second half of the second half of the second half of the 20th century, in the second half of the 20th century, in the second half, in the second half of the 20th century, in the second half of the second half, in the second half, in the second half of the 20th century, and in the second half of the second half of the 20th century, in the second half, and in the second half of the second half, in the second half of the second half of the second half of the 20th century, in the second half, in the second half of the second half of the second half"}, {"heading": "C. Singing Voice Separation Setting", "text": "We evaluate our proposed system using the MIR-1K dataset [33].2 thousand song clips are encoded at a sampling rate of 16 KHz, with a duration of 4 to 13 seconds. Clips are extracted from 110 Chinese karaoke songs played by male and female amateurs. There is manual annotation of pitch contours, texts, indexes and types for voiceless frames, and the indices of vocal and non-vocal frames are used in our experiments. Each clip contains the singing voice and background music in various channels. According to the rating framework in [3], [4] we use 175 clips sung by a male and a female singer."}, {"heading": "D. Singing Voice Separation Results", "text": "In this section, we compare different deep learning models under different aspects, including the effects of different input context parameters, the effects of different circular layer steps, the effects of different output formats, the effects of different deep recursive neural network structures, and the effects of discriminatory training targets. For convenience, we report the results using 3 hidden layers of 1000 hidden units with the mean square error criterion, common masking training, and 10K samples as circular step size using features with context window size 3.Table I presents the results with different output layer formats. We compare the use of a single source as a target (line 1) and the use of two sources as targets in the output layer (line 2 and line 3). We find that modeling two sources simultaneously provides better performance. If we compare lines 2 and 3 in Table I, we find that the use of two sources as targets in the output layer (line 2 and line 3) further improves the use of masking results."}, {"heading": "E. Speech Denoising Setting", "text": "In the experiments, we use size spectra as input characteristics for the neural network, and the spectral representation is extracted using a 1024-point short-term transformation (STFT) with 50% overlap. Empirically, we have found that log-mel filter banking functions perform worse. We use 2 hidden layers of 1000 hidden units of neural networks with the mean square error criterion, shared masking training, and 10K samples as circular step size. Results of different architectures are shown in Figure 10. We can observe that deep, recurring networks produce similar results compared to deep neural networks. With discriminatory training, although SDRs and SIRs are noisy, STOIs are similar and SARs are slightly worrisome."}, {"heading": "F. Speech Denoising Results", "text": "In the following experiments we examine the effect of the proposed methods under different scenarios. We can observe that the recurring neural network architectures (DRNN-1, DRNN-2, sRNN) achieve a similar performance compared to the DNN models."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "In this paper, we examine various deep learning architectures, including deep neural networks and deep recurrent neural networks for monaural source separation problems. We continue to improve the results by jointly optimizing a soft mask layer with the networks and examining the discriminatory training criteria. We evaluate our proposed method for speech separation, voice separation during singing and speech denocialization tasks. Overall, our proposed models achieve an SDR gain of 2.30 x 4.98 dB compared to the NMF baseline while maintaining better SIRs and SARs in the TSP task for speech separation. In addition, our proposed method exceeds the NMF and DNN baselines in the MIR-1K singing task at different imbalance conditions in TIMIT language separation. To further improve performance, one direction is to investigate similar GSARs while maintaining similar GSARs at the same time."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by the US ARL and ARO under grant number W911NF-09-1-0383. This work was carried out with the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by grant number ACI-1053575 of the National Science Foundation."}], "references": [{"title": "Singing-voice separation from monaural recordings using robust principal component analysis", "author": ["P.-S. Huang", "S.D. Chen", "P. Smaragdis", "M. Hasegawa-Johnson"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 57\u201360.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural networks for noise reduction in robust ASR", "author": ["A.L. Maas", "Q.V. Le", "T.M. O\u2019Neil", "O. Vinyals", "P. Nguyen", "A.Y. Ng"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time online singing voice separation from monaural recordings using robust low-rank modeling", "author": ["P. Sprechmann", "A. Bronstein", "G. Sapiro"], "venue": "Proceedings of the 13th International Society for Music Information Retrieval Conference, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank representation of both singing voice and music accompaniment via learned dictionaries", "author": ["Y.-H. Yang"], "venue": "Proceedings of the 14th International Society for Music Information Retrieval Conference, November 4-8 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "On sparse and low-rank matrix decomposition for singing voice separation", "author": ["Y.-H. Yang"], "venue": "ACM Multimedia, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Suppression of acoustic noise in speech using spectral subtraction", "author": ["S. Boll"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 27, no. 2, pp. 113\u2013120, Apr 1979.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1979}, {"title": "Speech enhancement using a minimummean square error short-time spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 32, no. 6, pp. 1109\u20131121, Dec 1984.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1984}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999, pp. 50\u201357.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "A probabilistic latent variable model for acoustic modeling", "author": ["P. Smaragdis", "B. Raj", "M. Shashanka"], "venue": "Advances in models for acoustic processing, NIPS, vol. 148, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, pp. 82\u201397, Nov. 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. PP, no. 99, pp. 1\u20131, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Single-channel speech separation with memory-enhanced recurrent neural networks", "author": ["F. Weninger", "F. Eyben", "B. Schuller"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp. 3709\u20133713.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Experiments on deep learning for speech denoising", "author": ["D. Liu", "P. Smaragdis", "M. Kim"], "venue": "Proceedings of the annual conference of the International Speech Communication Association (INTERSPEECH), 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep stacking networks with time series for speech separation", "author": ["S. Nie", "H. Zhang", "X. Zhang", "W. Liu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, May 2014, pp. 6667\u20136671.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Ideal ratio mask estimation using deep neural networks for robust speech recognition", "author": ["A. Narayanan", "D. Wang"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing. IEEE, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards scaling up classification-based speech separation", "author": ["Y. Wang", "D. Wang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7, pp. 1381\u20131390, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "On training targets for supervised speech separation", "author": ["Y. Wang", "A. Narayanan", "D. Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1849\u20131858, Dec 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1849}, {"title": "Deep neural network based speech separation for robust speech recognition", "author": ["Y. Tu", "J. Du", "Y. Xu", "L.-R. Dai", "C.-H. Lee"], "venue": "International Symposium on Chinese Spoken Language Processing, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for single channel source separation", "author": ["E. Grais", "M. Sen", "H. Erdogan"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, May 2014, pp. 3734\u20133738.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning for monaural speech separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 1562\u20131566.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Singing-voice separation from monaural recordings using deep recurrent neural networks", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "International Society for Music Information Retrieval (ISMIR), 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Training and analysing deep recurrent neural networks", "author": ["M. Hermans", "B. Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "International Conference on Learning Representations, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "JMLR W&CP: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2011), 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning for monaural speech separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 14, no. 4, pp. 1462 \u20131469, July 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "An algorithm for intelligibility prediction of time-frequency weighted noisy speech", "author": ["C. Taal", "R. Hendriks", "R. Heusdens", "J. Jensen"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2125\u20132136, Sept 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "TIMIT: acoustic-phonetic continuous speech corpus", "author": ["J.S. Garofolo", "L.D. Consortium"], "venue": "Linguistic Data Consortium,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1993}, {"title": "Improving wideband speech recognition using mixed-bandwidth training data in CD-DNN-HMM", "author": ["J. Li", "D. Yu", "J.-T. Huang", "Y. Gong"], "venue": "IEEE Spoken Language Technology Workshop (SLT). IEEE, 2012, pp. 131\u2013136.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "On the improvement of singing voice separation for monaural recordings using the MIR-1K dataset", "author": ["C.-L. Hsu", "J.-S. Jang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 2, pp. 310 \u2013319, Feb. 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997. PLACE PHOTO HERE Po-Sen Huang Biography text here. Minje Kim Biography text here. Mark Hasegawa-Johnson Biography text here. Paris Smaragdis Biography text here.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "For example, the accuracy of chord recognition and pitch estimation can be improved by separating singing voice from music [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "The accuracy of automatic speech recognition (ASR) can be improved by separating noise from speech signals [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "For example, in the singing voice separation task, several approaches have been proposed to utilize the assumption of the low rank and sparsity of the music and speech signals, respectively [1], [3]\u2013[5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "For example, in the singing voice separation task, several approaches have been proposed to utilize the assumption of the low rank and sparsity of the music and speech signals, respectively [1], [3]\u2013[5].", "startOffset": 195, "endOffset": 198}, {"referenceID": 4, "context": "For example, in the singing voice separation task, several approaches have been proposed to utilize the assumption of the low rank and sparsity of the music and speech signals, respectively [1], [3]\u2013[5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 5, "context": "In the speech denoising task, spectral subtraction [6] subtracts a short-term noise spectrum estimate to generate the spectrum of clean speech.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "By assuming the underlying properties of speech and noise, statistical modelbased methods infer speech spectral coefficients given noisy observations [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "Non-negative matrix factorization (NMF) [8] and probabilistic latent semantic indexing (PLSI) [9], [10] learn the non-negative reconstruction bases and weights of different sources and use them to factorize time-frequency spectral representations.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "Non-negative matrix factorization (NMF) [8] and probabilistic latent semantic indexing (PLSI) [9], [10] learn the non-negative reconstruction bases and weights of different sources and use them to factorize time-frequency spectral representations.", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "Non-negative matrix factorization (NMF) [8] and probabilistic latent semantic indexing (PLSI) [9], [10] learn the non-negative reconstruction bases and weights of different sources and use them to factorize time-frequency spectral representations.", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "Recently, deep learning based methods have been used in many applications, including automatic speech recognition [11], image classification [12], etc.", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "Recently, deep learning based methods have been used in many applications, including automatic speech recognition [11], image classification [12], etc.", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "proposed using a deep recurrent neural network (DRNN) for robust automatic speech recognition tasks [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 12, "context": "proposed a deep neural network (DNN)-based speech enhancement system, including global variance equalization and noise-aware training, to predict clean speech spectra for speech enhancement tasks [13].", "startOffset": 196, "endOffset": 200}, {"referenceID": 13, "context": "[14] trained two long-short term memory (LSTM) RNNs for predicting speech and noise, respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "explored using a deep neural network for predicting clean speech signals in various denoising settings [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "utilized deep stacking networks with time series inputs and a re-threshold method to predict the ideal binary mask [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Narayanan and Wang [17] and Wang and Wang [18] proposed a two-stage framework using deep neural networks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "Narayanan and Wang [17] and Wang and Wang [18] proposed a two-stage framework using deep neural networks.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "[19] recently proposed using deep neural networks to train different targets, including ideal ratio mask and FFT-mask, for speech separation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "proposed modeling two sources as the output targets for a robust ASR task [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "[21] proposed using a deep neural network to predict two scores corresponding to the probabilities of two different sources respectively for a given frame of normalized magnitude spectrum.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In this paper, we further extend our previous work in [22] and [23] and propose a general framework for the monaural source separation task for speech separation, singing voice separation, and speech denoising.", "startOffset": 54, "endOffset": 58}, {"referenceID": 22, "context": "In this paper, we further extend our previous work in [22] and [23] and propose a general framework for the monaural source separation task for speech separation, singing voice separation, and speech denoising.", "startOffset": 63, "endOffset": 67}, {"referenceID": 23, "context": "To further provide the hierarchical information through multiple time scales, deep recurrent neural networks (DRNNs) are explored [24], [25].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "To further provide the hierarchical information through multiple time scales, deep recurrent neural networks (DRNNs) are explored [24], [25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 24, "context": "The right of Figure 1 is an L intermediate layer DRNN with full temporal connections (called stacked RNN (sRNN) in [25]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 25, "context": "Function \u03c6l(\u00b7) is a nonlinear function, and we empirically found that using the rectified linear unit f(x) = max(0,x) [26] performs better compared to using a sigmoid or tanh function.", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "Since our goal is to separate one of the sources from a mixture, instead of learning one of the sources as the target, we adapt the framework from [27] to model all different sources simultaneously.", "startOffset": 147, "endOffset": 151}, {"referenceID": 0, "context": "Moreover, we find it useful to further smooth the source separation results with a time-frequency masking technique, for example, binary time-frequency masking or soft timefrequency masking [1], [27].", "startOffset": 190, "endOffset": 193}, {"referenceID": 26, "context": "Moreover, we find it useful to further smooth the source separation results with a time-frequency masking technique, for example, binary time-frequency masking or soft timefrequency masking [1], [27].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "The source separation evaluation is measured by using three quantitative values: Source to Interference Ratio (SIR), Source to Artifacts Ratio (SAR), and Source to Distortion Ratio (SDR), according to the BSS-EVAL metrics [28].", "startOffset": 222, "endOffset": 226}, {"referenceID": 28, "context": "For speech denoising task, we additionally compute the shorttime objective intelligibility measure (STOI) which is a quantitative estimate of the intelligibility of the denoised speech [29].", "startOffset": 185, "endOffset": 189}, {"referenceID": 30, "context": "In the speech recognition literature [32], the log-mel filterbank is found to provide better results compared to mel-frequency cepstral coefficients (MFCC) and log FFT bins.", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "We evaluate our proposed system using the MIR-1K dataset [33].", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": "Following the evaluation framework in [3], [4], we use 175 clips sung by one male and one female singer (\u2018abjones\u2019 and \u2018amy\u2019) as the training and development set.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Following the evaluation framework in [3], [4], we use 175 clips sung by one male and one female singer (\u2018abjones\u2019 and \u2018amy\u2019) as the training and development set.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "42 dB GSIR gain with similar GSAR performance, compared with the RNMF model [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "4 4We thank the authors in [3] for providing their trained model for comparison.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Unsupervised Model GNSDR GSIR GSAR RPCA [1] 3.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "09 RPCAh [5] 3.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "10 RPCAh + FASST [5] 3.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "Supervised Model GNSDR GSIR GSAR MLRR [4] 3.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "70 RNMF [3] 4.", "startOffset": 8, "endOffset": 11}, {"referenceID": 14, "context": "The left/back, middle, right/front bars in each pairs show the results of NMF, DNN without joint optimization of a masking layer [15], and DNN with joint optimization of a masking layer, respectively.", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "Speech denoising experimental results comparison between NMF, NN (without jointly optimizing masking function [15]), and DNN (with jointly optimizing masking function), when used on data that is not represented in training.", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "We compare our proposed approach with NMF model and DNN without joint optimizing the masking layer [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 32, "context": "To further improve the performance, one direction is to further explore using long short-term memory (LSTM) to model longer temporal information [34], which has shown great performance compared to conventional recurrent neural network as avoiding vanishing gradient properties.", "startOffset": 145, "endOffset": 149}], "year": 2015, "abstractText": "Monaural source separation is important for many real world applications. It is challenging in that, given only single channel information is available, there is an infinite number of solutions without proper constraints. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including the monaural speech separation task, monaural singing voice separation task, and speech denoising task. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative training criterion for the neural networks to further enhance the separation performance. We evaluate our proposed system on TSP, MIR-1K, and TIMIT dataset for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30\u223c4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30\u223c2.48 dB GNSDR gain and 4.32\u223c5.42 dB GSIR gain compared to previous models in the singing voice separation task, and outperform NMF and DNN baseline in the speech denoising task.", "creator": "LaTeX with hyperref package"}}}