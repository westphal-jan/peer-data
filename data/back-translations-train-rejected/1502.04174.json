{"id": "1502.04174", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2015", "title": "Probabilistic Models for High-Order Projective Dependency Parsing", "abstract": "This paper presents generalized probabilistic models for high-order projective dependency parsing and an algorithmic framework for learning these statistical models involving dependency trees. Partition functions and marginals for high-order dependency trees can be computed efficiently, by adapting our algorithms which extend the inside-outside algorithm to higher-order cases. To show the effectiveness of our algorithms, we perform experiments on three languages---English, Chinese and Czech, using maximum conditional likelihood estimation for model training and L-BFGS for parameter estimation. Our methods achieve competitive performance for English, and outperform all previously reported dependency parsers for Chinese and Czech.", "histories": [["v1", "Sat, 14 Feb 2015 06:47:34 GMT  (2100kb)", "http://arxiv.org/abs/1502.04174v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xuezhe ma", "hai zhao"], "accepted": false, "id": "1502.04174"}, "pdf": {"name": "1502.04174.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Models for High-Order Projective Dependency Parsing", "authors": ["Xuezhe Ma", "Hai Zhao"], "emails": ["xuezhe.ma@gmail.com,", "zhaohai@cs.sjtu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.04 174v 1 [cs.C L] 14 Feb 2015Probabilistic Models for High-Order Projective Dependency ParsingXuezhe Ma \u0445 Shanghai Jiao Tong UniversityHai Zhao Shanghai Jiao Tong UniversityThis paper presents generalized probability models for high-order projective dependency analysis and an algorithmic framework for learning these statistical models with dependency trees. Partition functions and margins for high-order dependency trees can be efficiently calculated by adapting our algorithms, which extend the indoor / outdoor algorithm to higher-order cases. To demonstrate the effectiveness of our algorithms, we conduct experiments in three languages - English, Chinese and Czech, using maximum conditional probability estimates for model training and L-BFGS for parameter estimation. Our methods achieve competitive performance for English and exceed all previously reported dependency parameters for Czech and Chinese."}, {"heading": "1. Introduction", "text": "In recent years, the number of those who are able to take to the streets has multiplied. (...) In recent years, the number of those who take to the streets has multiplied. (...) The number of those who take to the streets has multiplied. (...) The number of those who take to the streets has increased. (...) The number of those who take to the streets has increased. (...) The number of those who take to the streets has increased. (...) The number of those who take to the streets has increased. (...) The number of those who take to the streets has increased. (...) The number of those who take to the streets has risen. (...) The number of those who take to the streets has increased. (...) The number of those who take to the streets has increased. (...)"}, {"heading": "2. Dependency Parsing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Background of Dependency Parsing", "text": "Dependency trees represent syntactic relationships through labeled directed word edges and their syntactical modifiers. For example, Figure 1 shows a dependency tree for the sentence, business news had little impact on financial markets, with the root symbol of the sentence acting as the root.1 http: / / sourceforge.net / projects / maxparser / RoothadnewsEconomiceffectlittle onmarketsfinancialrootsbj objnmodnmodnmodnpc Business news had little impact on the financial marketRoot (a) (b) Figure 1 An example of dependency trees. Taking into account the point of crossing dependency trees, dependency trees fall into two categories - projective and non-projective dependency trees. An equivalent and more convenient formulation of the projectivity constraint is that if a dependency tree can be written with all words in a predefined linear order, and all edges drawn at the level, without typing edges."}, {"heading": "2.2 Probabilistic Model", "text": "The symbols we used in this work are described as follows: x represents a general input set and y represents a general dependency tree. T (x) is used to set the set of possible dependency trees for set x. The probability model for the analysis of dependencies defines a family of conditional probabilities Pr (y | x) over the entire y-given set x, with a log-linear form: Pr (y | x) = 1Z (x) exp {\u2211 jFj (y, x)}, where Fj are attribute functions, \u03bb = (\u03bb1, \u03bb2,...) are parameters of the model, and Z (x) is a normalization factor commonly referred to as a partition function: Z (x) = \u0445 y-jFj (y, x) exp {\u0445 jFj (y, x)}."}, {"heading": "2.3 Maximum Likelihood Parameter Inference", "text": "For a series of training data {(xk, yk)} the logarithm of the probability known as a logarithm probability is given by the following scales: L (\u03bb) = Logarithm-probability-probability-probability-probability-probability-probability-probability-probability-probability-probability-value-value-value-value-value-value-value-value-value-value-value-value-value-probability-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value"}, {"heading": "2.4 Problems of Training and Decoding", "text": "In order to train and decode the dependency parsers, we need to solve three problems that are central to the algorithms proposed in this publication. (The first problem is the decoding problem, which has the best parse for a set, if all the parameters of the probability model can be considered a problem, so decoding a dependency parameter is equivalent to searching for the dependency tree y y, which has the maximum conditional probability: y x x x. (2) The second and third problems are the calculation of partition Z (x). (3) Decoding a dependency parameter is equivalent to searching for the dependency tree y, which has the maximum conditional probability: y x x. (2) The second and third problems are the calculation of partition Z (x) and the strategy we can pursue."}, {"heading": "2.5 Discussion", "text": "It should be noted that for parsers trained by online learning algorithms such as AP or MIRA, only the algorithm for solving the decoding problem is required. However, in order to train parsers that use offline methods for estimating parameters such as the maximum probability described above, we need to carefully design algorithms for inference problems 2 and 3. The proposed probabilistic model is capable of generalizing to all types of parts p and can be learned from the framework that solves the three inference problems. For different types of factor models, the algorithms for solving the three inference problems are different. Following Koo and Collins (2010), the order of a part is defined as the number of dependencies contained therein, and the order of a factorisation or analysis algorithm is the maximum of ordering the parts used. In this essay, we focus on three factoring children, or grandchildren, and three factoring siblings:"}, {"heading": "2.6 Labeled Parsing", "text": "Our probability model can easily be extended by dependency names. We call L a set of all valid dependency names. We change the feature functions to include markup functions: Fj (y, x) = number of labels l contains, and we call them o (l). It should be noted that the order l is not necessarily equal to the order p, since l can contain labels of edge parts in p. For example, for the second order sibling model and the part (s, r, t) l only the labeling of the edge from word xs to word x.The weight function of each part is changed to: w (p, l, x) = exp {jumpf\u00e4nj (p, l, x) gg.4) The labeling of the edge from word xs to word x.The weight function of each part is changed to: w (p, l, x) = p = exp (r, x) gg.4)."}, {"heading": "3. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Grammatical Bigram Probability Model", "text": "The probabilistic model described in Section 2.2 is a generalized formulation of the grammatical bigram probabilistic model proposed in Eisner (1996), which is used by several papers (Paskin 2001; Koo et al. 2007; Smith and Smith 2007). Indeed, the grammatical bigram probabilistic model is a special case of our probabilistic model, specifying the parts p as individual edges. The grammatical bigram model is based on a strong assumption of independence: that all the dependency edges of a tree are independent of each other, since the sentence x. For the first-order model (part p is an individual edge), a variant of the inner-outer algorithm proposed by Baker (1979) for probable context-free grammars can be used to calculate the partition function and the margins for projective dependency structures. This inner-outer algorithm is based on the semiring parsing framework (Goodman 1999)."}, {"heading": "3.2 Algorithms of Decoding Problem for Different Factored Models", "text": "It should be noted that when the score of voices is defined as the logarithm of their weight: score (p, x) = logw (p, x) = \u2211 j\u03bbjfj (p, x), then the decoding problem is synonymous with the form of graph-based dependence parsing with the global linear model (GLM) and several parsing algorithms for different factorizations proposed in previous work. Figure 2 provides graphical specifications of these parsing algorithms. McDonald et al. (2005) presented the first-order dependency saver, which splits a dependency tree into a series of individual edges. A widely used dynamic programming algorithm (Eisner 2000) was used to decode these parsing algorithms. This algorithm introduces two related types of dynamic programming structures: complete spans and incomplete spans (Crammer, Persian, and two adjacent spinters)."}, {"heading": "3.3 Transition-based Parsing", "text": "Another category of dependency sparsing systems is \"transition-based\" parsing (Nivre and Scholz 2004; Attardi 2006; McDonald and Nivre 2007), which parameterizes models of transitions from one state to another in an abstract state machine. In these models, dependency trees are constructed by performing the transition with the highest score in each state until a state is entered for termination. Parameters in these models are typically learned using standardized classification techniques to predict a transition from a number of possible transitions in the face of a state history. Recently, several approaches have been proposed to improve transition-based dependency sparers. In terms of decoding, beam search (Johansson and Nugues 2007; Huang, Jiang and Liu 2009) and partial dynamic programming (Huang and Sagae 2010) have been applied to improve the best possible search. In terms of decoding, beam search (Johansson and Nugues 2007; Huang, Jiang and Liu 2009) and partial dynamic programming (Huang 2010) have been applied to improve the best search."}, {"heading": "4. Algorithms for High-order Models", "text": "In this section, we describe our algorithms for problem 2 and 3 of three highly factored models: grandchild and sibling, two second-order models; and grandsibling, which is third-order. Our algorithms are based on the idea of the inner-outer algorithm (Paskin 2001) for the first-order projective parsing model. Afterwards, we define the inner probabilities \u03b2 and the outer probabilities \u03b1 over a period of \u03c6: \u03b2 (\u03c6) = \u2211 t; p; p; x; p; p; p; p; p; p; p; p; p; x; p; p; p; p; p; p; x; p; x; algorithm 1 Calculating the inner probability \u03b2 for second-order grandchild ModelRequire: \u03b2 (Cgs; s) = 1.0 g, s"}, {"heading": "1: for k = 1 to n", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2: for s = 0 to n\u2212 k", "text": "3: t = s + k4: for g < s or g > t5: \u03b2 (Igs, t) = \u2211 s \u2264 r < t \u03b2 (Cgs, r) \u00b7 \u03b2 (C s t, r + 1) \u00b7 w g s, t \u03b2 (I g t, s) = \u2211 s \u2264 r < t \u03b2 (Cts, r) \u00b7 \u03b2 (C g t, r + 1) \u00b7 w g t, s6: \u03b2 (Cgs, t) = \u2211 s < r \u2264 t \u03b2 (Igs, r) \u00b7 \u03b2 (C s r, t) \u03b2 (C g t, s) = \u2211 s \u2264 r < t \u03b2 (Igt, r) \u00b7 \u03b2 (C t r, s)"}, {"heading": "7: end for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8: end for", "text": "Required: \u03b2 (Cs, s) = 1.0"}, {"heading": "9: for k = 1 to n", "text": "10: s = n \u2212 k, t = k11: \u03b2 (I0, t) = \u2211 0 \u2264 r < t \u03b2 (C0, r) \u00b7 \u03b2 (C0 t, r + 1) \u00b7 w 0 0, t \u03b2 (In, s) = \u2211 s \u2264 r < n \u03b2 (Cns, r) \u00b7 \u03b2 (Cn, r + 1) \u00b7 w n, s12: \u03b2 (C0, t) = \u2211 0 < r \u2264 t \u03b2 (I0, r) \u00b7 \u03b2 (C0 r, t) \u03b2 (Cn, s) = \u2211 s \u2264 r < n \u03b2 (In, r) \u00b7 \u03b2 (C n r, s)"}, {"heading": "13: end for", "text": "where t is a substructure of a tree and y (\u03c6) is the substructure of tree y, which belongs to span \u03c6."}, {"heading": "4.1 Model of Grandchild Factorization", "text": "In the second-order grandchild model, each dependency tree is included in a series of grandchild parts - pairs of dependencies connected head-to-tail. Formally, a grandchild part is a triple index (g, s, t), with both (g, s) and (s, t) being dependencies. To calculate the partition function Z (x) and marginals m (g, s, t) for this factorization, we extend both incomplete and complete spans with the indexes of grandparents. This is similar to Koo and Collins (2010) for the decryption algorithm of this grandchild factorization. Following Koo and Collins (2010), we refer to these extended structures as g-spans and designate an incomplete g-span as Igs, t, where Is, t is a normal complete span and g is the index of a grandparent outside the range [s, t] with the implication that (the span) is an Ig."}, {"heading": "1: for k = n to 1", "text": "2: s = n \u2212 k, t = k 3: \u03b1 (C0, t) = \u2211 t < r \u2264 n\u03b2 (C0r, t + 1) \u00b7 \u03b1 (I0, r) \u00b7 w 0 0, r \u03b1 (Cn, s) = \u2211 0 \u2264 r < s\u03b2 (Cnr, s \u2212 1) \u00b7 \u03b1 (In, r) \u00b7 w n, r4: \u03b1 (I0, t) = \u2211 t \u2264 r \u2264 n\u03b2 (C0t, r) \u00b7 \u03b1 (C0, r) \u03b1 (In, s) = \u2211 0 \u2264 r \u2264 s\u03b2 (Cns, r) \u00b7 \u03b1 (Cn, r)"}, {"heading": "5: end for", "text": "Required: \u03b1 (I00, n) = 1.0, \u03b1 (I n, 0) = 1.0"}, {"heading": "6: for k = n to 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7: for s = 0 to n\u2212 k", "text": "8: t = s + k9: for g < s 10: \u03b1 (Cgs, t) = t < r \u2264 n\u03b2 (Csr, t + 1) \u00b7 \u03b1 (I g, r) \u00b7 w g s, r + p > t\u03b2 (Irg, s) \u00b7 \u03b1 (C r g, t) 11: \u03b1 (Cgt, s) = p < r < s\u03b2 (Ctr, s \u2212 1) \u00b7 \u03b1 (I g t, r) \u00b7 w g t, r + p < g > t\u03b2 (Crg, s \u2212 1) \u00b7 \u03b1 (Ig, tr) \u00b7 w r g, t12: if g = 013: \u03b1 (Cgs, t) + = \u03b2 (I0, s) \u00b7 \u03b1 (C0, s) \u00b7 \u03b1 (C0, s) \u00b7 \u03b1 (C0, t) \u00b7 g (T) \u00b7 g (g, T), T (T)."}, {"heading": "16: end for", "text": "17: for g > t 18: \u03b1 (Cgs, t) = \u2211 t < r < g\u03b2 (Csr, t + 1) \u00b7 \u03b1 (I g, r) \u00b7 w g, s, r < s, r > g\u03b2 (Crg, t + 1) \u00b7 \u03b1 (I r g, s) \u00b7 w r g, s19: \u03b1 (Cgt, s) = \u2211 0leqr < s\u03b2 (Ctr, s \u2212 1) \u00b7 \u03b1 (I g, r) \u00b7 w g t, r + \u2211 r < s, r < s, r > g\u03b2 (Irg, t) \u00b7 \u03b1 (Ig, sr) 20: if g = n21: \u03b1 (Cgs, t) + = \u03b2 (In, t + 1) \u00b7 \u03b1 (Cn, s) \u00b7 \u03b1 (Cn, s) \u00b7 w n, s \u03b1 (C g t, s) + = \u03b2 (In, s) 22: end if 23: \u03b1 (Ig, + 1), \u03b1 (T), (T), (T)"}, {"heading": "24: end for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "25: end for", "text": "26: End of the range defined for the two algorithms is the same. Since our algorithm takes into account multiple root dependency trees, we should take another recursive step to calculate the internal probability \u03b2 for the entire range C0, t after calculating \u03b2 for all g span. Algorithm 2 illustrates the algorithm for calculating external probabilities \u03b1. This is a dynamic programming algorithm from top to bottom, and the key of this algorithm is to determine all contributions to the final Z (x) for each g span; fortunately, this can be done deterministically in all cases. For example, the full g span Cgs, t with g < s < t has two different contributions: combined with a g-span Csr, t + 1 of which r > t, on the right, to build a larger g span, g s; or combined with a g-span error."}, {"heading": "4.2 Model of Sibling Factorization", "text": "To analyze sibling factorization, a new type of span is defined: sibling span = \u03b2-weight. We call a sibling span Ss, t, where s and t are successive modifiers with a common head. Formally, a sibling span \u00b7 \u00b7 \u00b7 \u00b7 span Ss, t represents the region between successive modifiers s and t of a head. The key finding is that an incomplete span is constructed by combining a smaller incomplete span with a sibling span covering the region between the two successive modifiers. The new method allows the capture of sibling dependencies in a single state. It is no surprise that the dynamic programming of structures and derivatives of the algorithm for calculating \u03b2 is the same as that of the decryption algorithm, and we use the pseudo-part of this algorithm."}, {"heading": "1: for k = n to 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2: for s = 0 to n\u2212 k", "text": "3: t = s + k4: \u03b1 (Ss, t) = \u2211 0 \u2264 r < s \u03b2 (Ir, s) \u00b7 \u03b1 (Ir, t) \u00b7 wr, s, t + \u2211 t < r \u2264 n \u03b2 (Ir, t) \u00b7 wr, t, s5: \u03b1 (Cs, t) = \u2211 t < r \u2264 n \u03b2 (Cr, t + 1) \u00b7 \u03b1 (Ss, r) + \u2211 0 \u2264 r < s \u03b2 (Ir, s) \u00b7 \u03b1 (Cr, s) \u00b7 \u03b1 (Cr, t): + \u03b2 (Ct + 1, t + 1) \u00b7 \u03b1 (It + 1, s) \u00b7 wt + 1, \u2212, s6: \u03b1 (Ct, s) = 0 \u2264 r < s (Cr, s) \u00b7 \u03b1 (Cr, s) \u00b7 s (p, s) \u00b7 \u03b1 (Cr, s) (p p, s) \u00b7 s \u03b2 (Cr, p p p p, s, s, s) \u00b7 s (It + 1, p + 1) \u00b7 wt + 1, \u2212, \u2212, \u2212, s6: \u03b1 (Ct, s) = 0 \u2264 r < s \u03b2 (Ct, s) \u00b7 \u03b1 (Cr, s) \u00b7 \u03b1 (Cr, s (Cr, s) \u00b7 \u03b1 (Cr, p, t, t, t, t): \u03b1 (Cr, p, t) \u00b7 p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, s) \u00b7 s) \u00b7 s (It (s, p, p, p, p, p, p, p, p, (s, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p (s), p, p, p, p, p, p, p, p, p (s, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "9: end for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10: end for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3 Model of Grand-Sibling Factorization", "text": "We now describe the algorithms of the third-order sibling model by comparing it with each grandparent index. In this model, each tree is divided into two large sibling parts, which include grandchildren and sibling parts. Formally, a grandchild-sibling model is a 4-tuple of indexes (g, s, r, t) in which (s, r, t) is a sibling part and (g, s, t) is a grandchild part. The algorithm of this factorization can be designed on the basis of the algorithms for grandchildren and sibling models. Like extending the second-order sibling model to the first-order model, we define the sibling models g-spans Sgs, t, where t is a normal sibling span and g is the index of the head s and t, which lies outside the region [s, t] with the implication that forms (g, s, t) a valid sibling part. This model can also be treated as an extension of the sibling model."}, {"heading": "5. Experiments for Dependency Parsing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data Sets", "text": "We implement and evaluate the proposed algorithms of the three factor models (siblings, grandchildren, and grand siblings) at Penn English Treebank (PTB version 3.0) (Marcus, Santorini, and Marcinkiewicz 1993), Penn Chinese Treebank (CTB version 5.0) (Xue et al. 2005), and Prague Dependency Treebank (PDT) (Hajic, 1998; Hajic, et al. 2001). For English, the PTB data is prepared using the standard split: Sections 2-21 are used for training, Section 22 for development, and Section 23 for testing. Dependencies are extracted using the Penn2Malt2 tool with standard header rules (Yamada and Matsumoto 2003). For Chinese, we take over the data sharing from Zhang and Clark (2009), and we also use the Penn2Malt tool to convert the data into dependency structures."}, {"heading": "5.2 Feature Space", "text": "After previous work on high-level dependency analysis (McDonald and Pereira 2006; Carreras 2007; Koo and Collins 2010), high-level factored models are associated not only with the corresponding higher-order parts, but also with the features of relevant lower-order parts contained in their factorization. For example, third-order parts are evaluated for dependencies, siblings, grandchildren, and grandsiblings, so that the function of dependency savings is given by the following features: F (y, x) = (s, t), yfdep (s, x) + (s, r, t), yfsib (s, t, x), the function of dependency savings is given by: F (g, s, x) + (s, yfdep (s, x) + (g, r, x)."}, {"heading": "5.3 Model Training", "text": "The method of parameter estimation for our models is the limited memory BFGS algorithm (L-BFGS) (Nash and Nocedal 1991), with L2 regularization. L-BFGS algorithm is widely used for large-scale optimization, as it combines fast training time with low memory requirements, which is particularly important for large-scale optimization problems. In the meantime, L-BFGS can achieve highly competitive performance. Development sets are used to adjust hyperparameter C, which dictates the level of regularization in the model. For comparison purposes, we also conduct experiments with graph-based dependency parameters of the three different factorizations, using two online learning methods: The best version of the margin infused relaxed algorithm (MIRA) (Crammer and Singer 2003; Crammer al; Czech AP 2006), both of which are best (MIRK 1999 and MIRK) (both best structured)."}, {"heading": "5.4 Results and Analysis", "text": "Table 3 shows the results of three different factor analysis models trained by three different learning algorithms on the three tree banks of the PTB, CTB and PDT. Our analysis models trained by the L-BFGS method achieve a significant improvement in the analysis performance of the parser models trained by AP for all three tree banks and obtain a parser performance that competes with the parser models trained by the MIRA. For example, the parsers trained by the L-BFGS method improve the UAS by 0.6% for the PTB, 2.1% for the CTB and 1.1% for the PDT, compared with the parser models trained by the MIRA. For the parsers trained by the UAS for the PTB, our higher parser accuracy (about 0.2% better) achieves for both CTBand PDT. Furthermore, it should be noted that our algorithms achieve a significant improvement in RA and CM on all three treads."}, {"heading": "5.5 Comparison with Previous Works", "text": "Our experimental results show an improvement in the performance of English and Chinese compared to the results of Zhang and Clark (2008), which combine graph-based and transition-based dependencies in a single parser using the beam search framework, and Zhang and Nivre (2011), which are based on a transition-based dependency sparser with rich non-local characteristics. For English and Czech, our results are better than the results of the two third-place table 5 Accuracy comparisons of different dependency sparsers on PTB, CTB and PDT.Eng Chn Cze UAS CM UAS CMMcDonald et al. (2005) 90.9 36.7 79.7 27.2 84.4 32.2 McDonald and Pereira (2006) 81.5 42.1 82.5 32.6 85.2 Zhang and Clark (2008) - CMMcDonald et al. (2005) - 92.4 Koki 36.7 74.7 34.4 Zhang 84.9 - Zhang (2011) (94.9) (2011)."}, {"heading": "6. Conclusion", "text": "In this article, we described probabilistic models for high-level projective dependency analysis achieved by relaxing the independent assumption of the previous grammatical bigram model, and presented algorithms for calculating partition functions and margins for three factorized parsing models - second-order siblings and grandsons, and third-order grandsiblings. Our methods achieve competitive or state-of-the-art performance across three tree banks of English, Chinese, and Czech languages. By analyzing errors in structural properties of length factors, we showed that, despite very similar parsing performance of the UAS, parsers trained by online and offline learning methods generally exhibit a pronounced error distribution. We also demonstrated that our parsing models can be trained much faster than those using online training methods by using parallel computation techniques."}], "references": [{"title": "Experiments with a multilanguage non-projective dependency parser", "author": ["Attardi2006]Attardi", "Giuseppe"], "venue": "In Proceedings of the Tenth Conference on Natural Language Learning", "citeRegEx": "Attardi2006.Attardi and Giuseppe.,? \\Q2006\\E", "shortCiteRegEx": "Attardi2006.Attardi and Giuseppe.", "year": 2006}, {"title": "Trainable grammars for speech recognition", "author": ["K. James"], "venue": "In Proceedings of 97th meeting of the Acoustical Society of America,", "citeRegEx": "James,? \\Q1979\\E", "shortCiteRegEx": "James", "year": 1979}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006]Buchholz", "Sabine", "Erwin Marsi"], "venue": "In Proceeding of the 10th Conference on Computational Natural Language Learning", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "Experiments with a higher-order projective dependency parser", "author": ["Carreras2007]Carreras", "Xavier"], "venue": "In Proceedings of the CoNLL Shared Task Session of EMNLP-CONLL,", "citeRegEx": "Carreras2007.Carreras and Xavier.,? \\Q2007\\E", "shortCiteRegEx": "Carreras2007.Carreras and Xavier.", "year": 2007}, {"title": "Improving graph-based dependency parsing with decision history", "author": ["Chen et al.2010]Chen", "Wenliang", "Jun\u2019ichi Kazama", "Yoshimasa Tsuruoka", "Kentaro Torisawa"], "venue": "In Proceeding of the 23rd International Conference on Computional Linguistics", "citeRegEx": "al.2010.Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al.2010.Chen et al\\.", "year": 2010}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Collins2002]Collins", "Michael"], "venue": "In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Collins2002.Collins and Michael.,? \\Q2002\\E", "shortCiteRegEx": "Collins2002.Collins and Michael.", "year": 2002}, {"title": "Online passive-aggressive algorithms", "author": ["Crammer et al.2006]Crammer", "Koby", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "Jornal of Machine Learning Research,", "citeRegEx": "al.2006.Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al.2006.Crammer et al\\.", "year": 2006}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Crammer", "Singer2003]Crammer", "Koby", "Yoram Singer"], "venue": "Journal of Machine Learining Research,", "citeRegEx": "Crammer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2003}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Eisner1996]Eisner", "Jason"], "venue": "In Proceedings of the 9th International Conference on Computational Linguistics", "citeRegEx": "Eisner1996.Eisner and Jason.,? \\Q1996\\E", "shortCiteRegEx": "Eisner1996.Eisner and Jason.", "year": 1996}, {"title": "Bilexical grammars and their cubic-time parsing algorithms", "author": ["Eisner2000]Eisner", "Jason"], "venue": "Advances in Probabilistic and Other Parsing Technologies. Kluwer Academic Publishers,", "citeRegEx": "Eisner2000.Eisner and Jason.,? \\Q2000\\E", "shortCiteRegEx": "Eisner2000.Eisner and Jason.", "year": 2000}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Freund", "Schapire1999]Freund", "Yoav", "Robert E. Schapire"], "venue": null, "citeRegEx": "Freund et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1999}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Gimenez", "Marquez2004]Gimenez", "Marquez"], "venue": "In Proceedings of the 4th International Conference of Language Resources and Evaluation", "citeRegEx": "Gimenez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gimenez et al\\.", "year": 2004}, {"title": "Building a syntactically annotated corpus: The Prague Dependency Treebank", "author": ["Haji\u010d1998]Haji\u010d", "Jan"], "venue": "Issues of Valency and Meaning Studies in Honor of Jarmila Panevov,", "citeRegEx": "Haji\u010d1998.Haji\u010d and Jan.,? \\Q1998\\E", "shortCiteRegEx": "Haji\u010d1998.Haji\u010d and Jan.", "year": 1998}, {"title": "The Prague Dependency Treebank 1.0 CD-ROM", "author": ["Haji\u010d et al.2001]Haji\u010d", "Jan", "Eva Haji\u010dov\u00e1", "Petr Pajas", "Jarmila Panevov\u00e1", "Petr Sgall"], "venue": "Linguistic Data Consortium, Cat. No. LDC2001T10", "citeRegEx": "al.2001.Haji\u010d et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al.2001.Haji\u010d et al\\.", "year": 2001}, {"title": "Bilingually-constrained (monolingual) shift-reduce parsing", "author": ["Huang", "Jiang", "Liu2009]Huang", "Liang", "Wenbin Jiang", "Qun Liu"], "venue": "In Proceeding of EMNLP", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Sagae2010]Huang", "Liang", "Kenji Sagae"], "venue": "In Proceeding of ACL 2010, page 1077l\u0301C1086,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Incremental dependency parsing using online learning", "author": ["Johansson", "Nugues2007]Johansson", "Richard", "Pierre Nugues"], "venue": "In Proceedings of CoNLL/EMNLP", "citeRegEx": "Johansson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2007}, {"title": "Training a parser for machine translation reordering", "author": ["Katz-Brown et al.2011]Katz-Brown", "Jason", "Slav Petrov", "Ryan McDonald", "Franz Och", "David Talbot", "Hiroshi Ichikawa", "Masakazu Seno", "Hideto Kazawa"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al.2011.Katz.Brown et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Katz.Brown et al\\.", "year": 2011}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo", "Carreras", "Collins2008]Koo", "Terry", "Xavier Carreras", "Michael Collins"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010]Koo", "Terry", "Michael Collins"], "venue": "In Proceedings of 48th Meeting of the Association for Computional Linguistics", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Structured predicition models via the matrix-tree theorem", "author": ["Koo et al.2007]Koo", "Terry", "Amir Globerson", "Xavier Carreras", "Michael Collins"], "venue": "In Proceedings of EMNLP-CONLL", "citeRegEx": "al.2007.Koo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al.2007.Koo et al\\.", "year": 2007}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["Marcus", "Santorini", "Marcinkiewicz1993]Marcus", "Mitchell", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online large-margin training of dependency parsers", "author": ["McDonald", "Crammer", "Pereira2005]McDonald", "Ryan", "Koby Crammer", "Fernando Pereira"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Characterizing the errors of data-driven dependency parsing models", "author": ["McDonald", "Nivre2007]McDonald", "Ryan", "Joakim Nivre"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL", "citeRegEx": "McDonald et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2007}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["McDonald", "Pereira2006]McDonald", "Ryan", "Fernando Pereira"], "venue": "In European Association for Computational Linguistics", "citeRegEx": "McDonald et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2006}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["McDonald et al.2005]McDonald", "Ryan", "Fernando Pereira", "Kiril Ribarov", "Jan Hajic"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language (HLT/EMNLP", "citeRegEx": "al.2005.McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al.2005.McDonald et al\\.", "year": 2005}, {"title": "On the complexity of non-projective data-driven dependency parsing", "author": ["McDonald", "Satta2007]McDonald", "Ryan", "Giorgio Satta"], "venue": "In Proceedings of the 10th International Conference on Parsing Technologies,", "citeRegEx": "McDonald et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2007}, {"title": "A numerical study of the limited memory bfgs method and truncated-newton method for large scale optimization", "author": ["Nash", "Nocedal1991]Nash", "Stephen G", "Jorge Nocedal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nash et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Nash et al\\.", "year": 1991}, {"title": "Convolution kernels on constituent, dependency and sequential structures for relation extraction", "author": ["Nguyen", "Moschitti", "Riccardi2009]Nguyen", "Truc-Vien T", "Alessandro Moschitti", "Giuseppe Riccardi"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Nguyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2009}, {"title": "Deterministic dependency parsing of english text", "author": ["Nivre", "Scholz2004]Nivre", "Joakim", "Mario Scholz"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics", "citeRegEx": "Nivre et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2004}, {"title": "Cubic-time parsing and learning algorithms for grammatical bigram models", "author": ["A. Mark"], "venue": "Technical Report,", "citeRegEx": "Mark,? \\Q2001\\E", "shortCiteRegEx": "Mark", "year": 2001}, {"title": "Automatic paraphrase acquisition from news articles", "author": ["Shinyama", "Sekine", "Sudo2002]Shinyama", "Yusuke", "Satoshi Sekine", "Kiyoshi Sudo"], "venue": "In Proceeding of the 2nd International Conference on Human Language Technology Research", "citeRegEx": "Shinyama et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shinyama et al\\.", "year": 2002}, {"title": "Probabilistic models of nonporjective dependency trees", "author": ["Smith", "Smith2007]Smith", "David A", "Noah A. Smith"], "venue": "In Proceedings of EMNLP-CONLL", "citeRegEx": "Smith et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2007}, {"title": "An empirial study of semi-supervised structured conditional models for dependency parsing", "author": ["Suzuki et al.2009]Suzuki", "Jun", "Hideki Isozaki", "Xavier Carreras", "Micheal Collins"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "al.2009.Suzuki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Suzuki et al\\.", "year": 2009}, {"title": "A novel dependency-to-string model for statistical machine translation", "author": ["Xie", "Mi", "Liu2011]Xie", "Jun", "Haitao Mi", "Qun Liu"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Xie et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2011}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Xue et al.2005]Xue", "Naiwen", "Fei Xia", "Fu-Dong Chiou", "Marta Palmer"], "venue": "Natural Language Engineering,", "citeRegEx": "al.2005.Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al.2005.Xue et al\\.", "year": 2005}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003]Yamada", "Hiroyasu", "Yuji Matsumoto"], "venue": "In Proceedings of the 8th International Workshop on Parsing Technologies", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beam search", "author": ["Zhang", "Clark2008]Zhang", "Yue", "Stephen Clark"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Transition-based parsing of Chinese treebank using a global discriminative model", "author": ["Zhang", "Clark2009]Zhang", "Yue", "Stephen Clark"], "venue": "In Proceedings of the 11th International Conference on Parsing Technologies", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011]Zhang", "Yue", "Joakim Nivre"], "venue": "In Proceeding of ACL", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 22, "context": "2007), there are currently two dominant approaches for data-driven dependency parsing: local-and-greedy transitionbased algorithms (Yamada and Matsumoto 2003; Nivre and Scholz 2004; Attardi 2006; McDonald and Nivre 2007), and globally optimized graph-based algorithms (Eisner 1996; McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; Carreras 2007; Koo and Collins 2010), and graph-based parsing models have achieved state-ofthe-art accuracy for a wide range of languages.", "startOffset": 268, "endOffset": 404}, {"referenceID": 7, "context": "There have been several existing graph-based dependency parsers, most of which employed online learning algorithms such as the averaged structured perceptron (AP) (Freund and Schapire 1999; Collins 2002) or Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006; McDonald 2006) for learning parameters. However, One shortcoming of these parsers is that learning parameters of these models usually takes a long time (several hours for an iteration). The primary reason is that the training step cannot be performed in parallel, since for online learning algorithms, the updating for a new training instance depends on parameters updated with the previous instance. Paskin (2001) proposed a variant of the inside-outside algorithm (Baker 1979), which were applied to the grammatical bigram model (Eisner 1996).", "startOffset": 273, "endOffset": 709}, {"referenceID": 7, "context": "There have been several existing graph-based dependency parsers, most of which employed online learning algorithms such as the averaged structured perceptron (AP) (Freund and Schapire 1999; Collins 2002) or Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006; McDonald 2006) for learning parameters. However, One shortcoming of these parsers is that learning parameters of these models usually takes a long time (several hours for an iteration). The primary reason is that the training step cannot be performed in parallel, since for online learning algorithms, the updating for a new training instance depends on parameters updated with the previous instance. Paskin (2001) proposed a variant of the inside-outside algorithm (Baker 1979), which were applied to the grammatical bigram model (Eisner 1996). Using this algorithm, the grammatical bigram model can be learning by off-line learning algorithms. However, the grammatical bigram model is based on a strong independence assumption that all the dependency edges of a tree are independent of one another. This assumption restricts the model to first-order factorization (single edge), losing much of the contextual information in dependency tree. Chen et.al (2010) illustrated that a wide range of decision history can lead to significant improvements in accuracy for graph-based dependency parsing models.", "startOffset": 273, "endOffset": 1255}, {"referenceID": 18, "context": "2 is a generalized formulation of the grammatical bigram probabilistic model proposed in Eisner (1996), which is used by several works (Paskin 2001; Koo et al. 2007; Smith and Smith 2007). In fact, the grammatical bigram probabilistic model is a special case of our probabilistic model, by specifying the parts p as individual edges. The grammatical bigram model is based on a strong independence assumption: that all the dependency edges of a tree are independent of one another, given the sentence x. For the first-order model (part p is an individual edge), a variant of the inside-outside algorithm, which was proposed by Baker (1979) for probabilistic context-free grammars, can be applied for the computation of partition function and marginals for projective dependency structures.", "startOffset": 149, "endOffset": 639}, {"referenceID": 22, "context": "McDonald et al. (2005) presented the first-order dependency parser, which decomposes a dependency tree into a set of individual edges.", "startOffset": 0, "endOffset": 23}, {"referenceID": 22, "context": "McDonald et al. (2005) presented the first-order dependency parser, which decomposes a dependency tree into a set of individual edges. A widely-used dynamic programming algorithm (Eisner 2000) was used for decoding. This algorithm introduces two interrelated types of dynamic programming structures: complete spans, and incomplete spans (McDonald, Crammer, and Pereira 2005). Larger spans are created from two smaller, adjacent spans by recursive combination in a bottom-up procedure. The second-order sibling parser (McDonald and Pereira 2006) breaks up a dependency tree into sibling parts\u2014pairs of adjacent edges with shared head. Koo and Collins (2010) proposed a parser that factors each dependency tree into a set of grandchild parts.", "startOffset": 0, "endOffset": 657}, {"referenceID": 22, "context": "McDonald et al. (2005) presented the first-order dependency parser, which decomposes a dependency tree into a set of individual edges. A widely-used dynamic programming algorithm (Eisner 2000) was used for decoding. This algorithm introduces two interrelated types of dynamic programming structures: complete spans, and incomplete spans (McDonald, Crammer, and Pereira 2005). Larger spans are created from two smaller, adjacent spans by recursive combination in a bottom-up procedure. The second-order sibling parser (McDonald and Pereira 2006) breaks up a dependency tree into sibling parts\u2014pairs of adjacent edges with shared head. Koo and Collins (2010) proposed a parser that factors each dependency tree into a set of grandchild parts. Formally, a grandchild part is a triple of indices (g, s, t) where g is the head of s and s is the head of t. In order to parse this factorization, it is necessary to augment both complete and incomplete spans with grandparent indices. Following Koo and Collins (2010), we refer to these augmented structures as g-spans.", "startOffset": 0, "endOffset": 1010}, {"referenceID": 22, "context": "McDonald et al. (2005) presented the first-order dependency parser, which decomposes a dependency tree into a set of individual edges. A widely-used dynamic programming algorithm (Eisner 2000) was used for decoding. This algorithm introduces two interrelated types of dynamic programming structures: complete spans, and incomplete spans (McDonald, Crammer, and Pereira 2005). Larger spans are created from two smaller, adjacent spans by recursive combination in a bottom-up procedure. The second-order sibling parser (McDonald and Pereira 2006) breaks up a dependency tree into sibling parts\u2014pairs of adjacent edges with shared head. Koo and Collins (2010) proposed a parser that factors each dependency tree into a set of grandchild parts. Formally, a grandchild part is a triple of indices (g, s, t) where g is the head of s and s is the head of t. In order to parse this factorization, it is necessary to augment both complete and incomplete spans with grandparent indices. Following Koo and Collins (2010), we refer to these augmented structures as g-spans. The second-order parser proposed in Carreras (2007) is capable to score both sibling and grandchild parts with complexities of O(n) time and O(n) space.", "startOffset": 0, "endOffset": 1114}, {"referenceID": 22, "context": "McDonald et al. (2005) presented the first-order dependency parser, which decomposes a dependency tree into a set of individual edges. A widely-used dynamic programming algorithm (Eisner 2000) was used for decoding. This algorithm introduces two interrelated types of dynamic programming structures: complete spans, and incomplete spans (McDonald, Crammer, and Pereira 2005). Larger spans are created from two smaller, adjacent spans by recursive combination in a bottom-up procedure. The second-order sibling parser (McDonald and Pereira 2006) breaks up a dependency tree into sibling parts\u2014pairs of adjacent edges with shared head. Koo and Collins (2010) proposed a parser that factors each dependency tree into a set of grandchild parts. Formally, a grandchild part is a triple of indices (g, s, t) where g is the head of s and s is the head of t. In order to parse this factorization, it is necessary to augment both complete and incomplete spans with grandparent indices. Following Koo and Collins (2010), we refer to these augmented structures as g-spans. The second-order parser proposed in Carreras (2007) is capable to score both sibling and grandchild parts with complexities of O(n) time and O(n) space. However, the parser suffers an crucial limitation that it can only evaluate events of grandchild parts for outermost grandchildren. The third-order grand-sibling parser, which encloses grandchild and sibling parts into a grand-sibling part, was described in Koo and Collins (2010). This factorization defines all grandchild and sibling parts and still requires O(n) time and O(n) space.", "startOffset": 0, "endOffset": 1496}, {"referenceID": 7, "context": "For the purpose of comparison, we also run experiments on graph-based dependency parsers of the three different factorizations, employing two online learning methods: The kbest version of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006; McDonald 2006) with k = 10, and averaged structured perceptron (AP) (Freund and Schapire 1999; Collins 2002). Both the two learning methods are used in previous work for training graph-based dependency parsers and achieved highly competitive parsing accuracies\u2014k-best MIRA is used in McDonald et al. (2005), McDonald and Pereira (2006), and McDonald and Nivre (2007), and AP is used in Carreras (2007) and Koo and Collins (2010).", "startOffset": 258, "endOffset": 586}, {"referenceID": 7, "context": "For the purpose of comparison, we also run experiments on graph-based dependency parsers of the three different factorizations, employing two online learning methods: The kbest version of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006; McDonald 2006) with k = 10, and averaged structured perceptron (AP) (Freund and Schapire 1999; Collins 2002). Both the two learning methods are used in previous work for training graph-based dependency parsers and achieved highly competitive parsing accuracies\u2014k-best MIRA is used in McDonald et al. (2005), McDonald and Pereira (2006), and McDonald and Nivre (2007), and AP is used in Carreras (2007) and Koo and Collins (2010).", "startOffset": 258, "endOffset": 615}, {"referenceID": 7, "context": "For the purpose of comparison, we also run experiments on graph-based dependency parsers of the three different factorizations, employing two online learning methods: The kbest version of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006; McDonald 2006) with k = 10, and averaged structured perceptron (AP) (Freund and Schapire 1999; Collins 2002). Both the two learning methods are used in previous work for training graph-based dependency parsers and achieved highly competitive parsing accuracies\u2014k-best MIRA is used in McDonald et al. (2005), McDonald and Pereira (2006), and McDonald and Nivre (2007), and AP is used in Carreras (2007) and Koo and Collins (2010).", "startOffset": 258, "endOffset": 646}, {"referenceID": 7, "context": "For the purpose of comparison, we also run experiments on graph-based dependency parsers of the three different factorizations, employing two online learning methods: The kbest version of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006; McDonald 2006) with k = 10, and averaged structured perceptron (AP) (Freund and Schapire 1999; Collins 2002). Both the two learning methods are used in previous work for training graph-based dependency parsers and achieved highly competitive parsing accuracies\u2014k-best MIRA is used in McDonald et al. (2005), McDonald and Pereira (2006), and McDonald and Nivre (2007), and AP is used in Carreras (2007) and Koo and Collins (2010).", "startOffset": 258, "endOffset": 681}, {"referenceID": 7, "context": "For the purpose of comparison, we also run experiments on graph-based dependency parsers of the three different factorizations, employing two online learning methods: The kbest version of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006; McDonald 2006) with k = 10, and averaged structured perceptron (AP) (Freund and Schapire 1999; Collins 2002). Both the two learning methods are used in previous work for training graph-based dependency parsers and achieved highly competitive parsing accuracies\u2014k-best MIRA is used in McDonald et al. (2005), McDonald and Pereira (2006), and McDonald and Nivre (2007), and AP is used in Carreras (2007) and Koo and Collins (2010). Each parser is trained for 10 iterations and selects parameters from the iteration that achieves the highest parsing performance on the development set.", "startOffset": 258, "endOffset": 708}, {"referenceID": 20, "context": "Eng Chn Cze UAS CM UAS CM UAS CM McDonald et al. (2005) 90.", "startOffset": 33, "endOffset": 56}, {"referenceID": 20, "context": "Eng Chn Cze UAS CM UAS CM UAS CM McDonald et al. (2005) 90.9 36.7 79.7 27.2 84.4 32.2 McDonald and Pereira (2006) 91.", "startOffset": 33, "endOffset": 114}, {"referenceID": 20, "context": "Eng Chn Cze UAS CM UAS CM UAS CM McDonald et al. (2005) 90.9 36.7 79.7 27.2 84.4 32.2 McDonald and Pereira (2006) 91.5 42.1 82.5 32.6 85.2 35.9 Zhang and Clark (2008) 92.", "startOffset": 33, "endOffset": 167}, {"referenceID": 20, "context": "Eng Chn Cze UAS CM UAS CM UAS CM McDonald et al. (2005) 90.9 36.7 79.7 27.2 84.4 32.2 McDonald and Pereira (2006) 91.5 42.1 82.5 32.6 85.2 35.9 Zhang and Clark (2008) 92.1 45.4 85.7 34.4 - Zhang and Nivre (2011) 92.", "startOffset": 33, "endOffset": 212}, {"referenceID": 20, "context": "Eng Chn Cze UAS CM UAS CM UAS CM McDonald et al. (2005) 90.9 36.7 79.7 27.2 84.4 32.2 McDonald and Pereira (2006) 91.5 42.1 82.5 32.6 85.2 35.9 Zhang and Clark (2008) 92.1 45.4 85.7 34.4 - Zhang and Nivre (2011) 92.9 48.0 86.0 36.9 - Koo and Collins (2010), model2 92.", "startOffset": 33, "endOffset": 257}, {"referenceID": 20, "context": "Eng Chn Cze UAS CM UAS CM UAS CM McDonald et al. (2005) 90.9 36.7 79.7 27.2 84.4 32.2 McDonald and Pereira (2006) 91.5 42.1 82.5 32.6 85.2 35.9 Zhang and Clark (2008) 92.1 45.4 85.7 34.4 - Zhang and Nivre (2011) 92.9 48.0 86.0 36.9 - Koo and Collins (2010), model2 92.9 - - - 87.4 Koo and Collins (2010), model1 93.", "startOffset": 33, "endOffset": 304}, {"referenceID": 18, "context": "3 Koo et al. (2008) 93.", "startOffset": 2, "endOffset": 20}, {"referenceID": 18, "context": "3 Koo et al. (2008) 93.2 - - - 87.1 Suzuki et al. (2009) 93.", "startOffset": 2, "endOffset": 57}, {"referenceID": 18, "context": "3 Koo et al. (2008) 93.2 - - - 87.1 Suzuki et al. (2009) 93.8 - - - 88.1 Zhang and Clark (2009) - - 86.", "startOffset": 2, "endOffset": 96}], "year": 2015, "abstractText": "This paper presents generalized probabilistic models for high-order projective dependency parsing and an algorithmic framework for learning these statistical models involving dependency trees. Partition functions and marginals for high-order dependency trees can be computed efficiently, by adapting our algorithms which extend the inside-outside algorithm to higher-order cases. To show the effectiveness of our algorithms, we perform experiments on three languages\u2014 English, Chinese and Czech, using maximum conditional likelihood estimation for model training and L-BFGS for parameter estimation. Our methods achieve competitive performance for English, and outperform all previously reported dependency parsers for Chinese and Czech.", "creator": "LaTeX with hyperref package"}}}