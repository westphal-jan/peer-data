{"id": "1502.04492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "Towards Building Deep Networks with Bayesian Factor Graphs", "abstract": "We propose a Multi-Layer Network based on the Bayesian framework of the Factor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional lattice. The Latent Variable Model (LVM) is the basic building block of a quadtree hierarchy built on top of a bottom layer of random variables that represent pixels of an image, a feature map, or more generally a collection of spatially distributed discrete variables. The multi-layer architecture implements a hierarchical data representation that, via belief propagation, can be used for learning and inference. Typical uses are pattern completion, correction and classification. The FGrn paradigm provides great flexibility and modularity and appears as a promising candidate for building deep networks: the system can be easily extended by introducing new and different (in cardinality and in type) variables. Prior knowledge, or supervised information, can be introduced at different scales. The FGrn paradigm provides a handy way for building all kinds of architectures by interconnecting only three types of units: Single Input Single Output (SISO) blocks, Sources and Replicators. The network is designed like a circuit diagram and the belief messages flow bidirectionally in the whole system. The learning algorithms operate only locally within each block. The framework is demonstrated in this paper in a three-layer structure applied to images extracted from a standard data set.", "histories": [["v1", "Mon, 16 Feb 2015 11:01:25 GMT  (2038kb,D)", "http://arxiv.org/abs/1502.04492v1", "Submitted for journal publication"]], "COMMENTS": "Submitted for journal publication", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amedeo buonanno", "francesco a n palmieri"], "accepted": false, "id": "1502.04492"}, "pdf": {"name": "1502.04492.pdf", "metadata": {"source": "CRF", "title": "Towards Building Deep Networks with Bayesian Factor Graphs", "authors": ["Amedeo Buonanno", "Francesco A.N. Palmieri"], "emails": ["amedeo.buonanno@unina2.it", "francesco.palmieri@unina2.it"], "sections": [{"heading": null, "text": "Keywords: Bavarian networks, factor graphics, Deep Belief Networks"}, {"heading": "1. Introduction", "text": "The problem is that most people are able to recognize themselves and understand what they are doing."}, {"heading": "2. Related Work", "text": "The large literature on the deep representation of people (see also the detailed overview of the year 2000) can usually be divided into two main lines of research: the first is based on probable graphical models such as the \"RBM\" (RBM), the second on the \"RBM\" (RBM), the third on the \"RBM\" (RBM), the third on the \"RBM\" (RBM), the third on the \"RBM\" (RBM), the third on the \"RBM\" (RBM), the third on the \"RBM\" (RBM), the third on the \"RBM\" (RBM)."}, {"heading": "3. Factor Graphs in Reduced Normal Form", "text": "In the FGrn framework (Palmieri, 2013), the Bayean graph is reduced to a simplified form (X = 1) composed only of variables, replicators (or diverters), single input / single output (SISO), and source blocks (Y = 1) (X = 1). Although various architectures have been suggested in the literature for Bayesian graphs (Loeliger, 2004), we have found that the FGrn framework is much easier to use, it is better suited to define unique learning equations (Palmieri, 2013) and it is better suited for distributed implementations. The blocks needed to compose each architecture are shown in Figure 1. In our notation, we avoid the upper arrays for the messages and assign one direction for each variable definition of forward and backward messages.For a variable X (Figure 1), the values in the discrete {X = 2}."}, {"heading": "4. Bayesian Clustering", "text": "For the architectures that will follow, the basic building block is the latent-variable model (LVM) shown in Figure 2. At the bottom of each LVM there are N \u00b7 M variables X [n, m], n = 1: M that belong to a finite alphabet, messages that follow in the application that follows the values of the same alphabet. Here, the variables are organized on one level (as an image) because they will compose the layers of a multi-layered architecture. N \u00b7 M variables code of multiple discrete labels that take values that follow the alphabet in the application, but they could have slightly different cardinalities if we fuse information from different sources (the combination of heterogeneous variables is one of the most powerful features of the FGrn paradigm). Generally, the complexity of the entire system increases with the cardinality of alphabets.The N \u00b7 M variables below are connected to the variables."}, {"heading": "5. Multi-layer FGrn", "text": "In this thesis we build a multi-layered structure like in Figure 3 (a) on a bottom layer of random variables. They can be pixels of an image, a characteristic map or more generally a collection of spatially distributed discrete variables. In the following we refer to the bottom variables as an image. The architecture that lies above the image is the quadtree. In Figure 3 the cyanspheres are represented as image variables and the others (red, green and blue) are the embedded (latent or hidden) variables of the LVM blocks. In Figure 3 (b) the same architecture is presented as a FGrn.A network with L + 1 layers (layer 0,..., layer L) comprising a bottom image (layer 0) S0 [n, m] n = 1: N \u00b7 2L \u2212 1, m = Variables: 1 = 2b = 1, divided into (2L \u2212 1) \u00b7 (2L \u2212 1) \u2212 1."}, {"heading": "5.1 Inference Modes", "text": "Once the network parameters have been learned, the system can be used in the following main derivative modes: Generative: A latent variable Si [n, m] is fixed to a value \u03c3 i k, i.e. its forward distribution is a delta fSi (s \u2212 \u03c3ik), \u03c3i1, \u03c3i2,.. After the message propagation downwards, forward messages to the terminal variables S0 [n, m] in the form submitted by Si are the k-th \"hierarchical distributions\" associated with Si. This generation could be carried out at level 1 to check the clusters in the image patches, or at higher levels to visualize the coding role of the various hierarchical representations. Of course, the propagation of variables in the form of a generic node can also occur upwards with a backward distribution."}, {"heading": "5.2 Learning", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "6. Simulations", "text": "In this series of simulations, we have taken 50 automotive images from the Caltech101 dataset. Each image is cropped, filtered with an anisotropic diffusion algorithm (Kovesi), filtered white, and finally filtered with a canny filter to obtain images that have only the edges of the vehicle. Our input alphabet is binary (dS0 = 2). 500 image fields of 32 \u00d7 32 pixels are randomly extracted from the 50 filtered images."}, {"heading": "6.1 Learning", "text": "The steps for the learning phase are described above and use the following variables: P = 10, T = 50, N = 8, M = 8, L = 3, dS0 = 2, dS1 = 100, dS2 = 300, dS3 = 300."}, {"heading": "6.2 Inference", "text": "Once the matrices are learned, we use the network in various inference modes: Generative mode: we obtain forward distributions at the bottom by injecting a delta distribution at the top of the various structures (the grayscale images show the probability of one of the two symbols at each pixel); more specifically, to visualize the conditional distributions corresponding to layer 1, we look only at the latent model in Figure 2; for layer 2, we look at the 3-layer architecture, which consists of 4 LVM blocks connected to an LVM block; for layer 3, we look at the complete architecture; Figures 6, 7, and 8 respectively show the forward distributions that are stored by injection of deltas in layers 1, 2, and 3.The network has stored the complex structures quite well. The forward distributions of layer 1 represent simple orientation patterns that resemble those to which the early human visual system responds in layers 1, 2, and 3."}, {"heading": "7. Discussion and Conclusions", "text": "The layers retain the information about the clusters contained in the data and form a hierarchical internal representation. Each layer successfully learns to compose the objects provided by the lower layers. We chose the boundary of the automotive images extracted from Caltech101 because we wanted to see if the paradigm is suitable for patching together the prominent structures of an object. Other experiments have been conducted on characters and different patterns that reveal very similar results.We believe that the FGrn paradigm is a promising complement to the various proposals for deep networks that appear in the literature. It can offer great flexibility and modularity. The network can be easily expanded by introducing new and different variables (in cardinality and type). Prior knowledge and supervised information can be invised at the scales: new \"label variables\" can be added in one or more of the different junctions."}], "references": [{"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "Barber.,? \\Q2012\\E", "shortCiteRegEx": "Barber.", "year": 2012}, {"title": "Variational Algorithms for Approximate Bayesian Inference", "author": ["M.J. Beal"], "venue": "PhD thesis, University of London,", "citeRegEx": "Beal.,? \\Q2003\\E", "shortCiteRegEx": "Beal.", "year": 2003}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "On the expressive power of deep architectures", "author": ["Yoshua Bengio", "Olivier Delalleau"], "venue": "Algorithmic Learning Theory,", "citeRegEx": "Bengio and Delalleau.,? \\Q2011\\E", "shortCiteRegEx": "Bengio and Delalleau.", "year": 2011}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "URL http://arxiv.org/abs/1206.5538. cite arxiv:1206.5538", "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Deep learning", "author": ["Yoshua Bengio", "Ian J. Goodfellow", "Aaron Courville"], "venue": "Book in preparation for MIT Press,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Latent variable models. In Learning in Graphical Models, pages 371\u2013403", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1999\\E", "shortCiteRegEx": "Bishop.", "year": 1999}, {"title": "A multiscale random field model for bayesian image segmentation", "author": ["Charles A. Bouman", "Michael Shapiro"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Bouman and Shapiro.,? \\Q1994\\E", "shortCiteRegEx": "Bouman and Shapiro.", "year": 1994}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Yuri Boykov", "Olga Veksler", "Ramin Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 1999}, {"title": "Simulink implementation of belief propagation in normal factor graphs", "author": ["A. Buonanno", "F.A.N. Palmieri"], "venue": "In Proceedings of the 24th Workshop on Neural Networks,", "citeRegEx": "Buonanno and Palmieri.,? \\Q2014\\E", "shortCiteRegEx": "Buonanno and Palmieri.", "year": 2014}, {"title": "Bayesian classification (autoclass)", "author": ["Peter Cheeseman", "John Stutz"], "venue": "Theory and results,", "citeRegEx": "Cheeseman and Stutz.,? \\Q1996\\E", "shortCiteRegEx": "Cheeseman and Stutz.", "year": 1996}, {"title": "Learning latent tree graphical models", "author": ["M.J. Choi", "V.Y.F. Tan", "A. Anandkumar", "A.S. Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Learning feature representations with k-means", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "Neural Networks: Tricks of the Trade (2nd ed.),", "citeRegEx": "Coates and Ng.,? \\Q2012\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2012}, {"title": "How the brain might work: A hierarchical and temporal model for learning and recognition", "author": ["G. Dileep"], "venue": "PhD thesis,", "citeRegEx": "Dileep.,? \\Q2008\\E", "shortCiteRegEx": "Dileep.", "year": 2008}, {"title": "Codes on graphs: Normal realizations", "author": ["G.D. Forney"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Forney.,? \\Q2001\\E", "shortCiteRegEx": "Forney.", "year": 2001}, {"title": "Inference in loopy graphs", "author": ["Marcus Frean"], "venue": "Class Notes in Machine Learning COMP 431,", "citeRegEx": "Frean.,? \\Q2008\\E", "shortCiteRegEx": "Frean.", "year": 2008}, {"title": "Sampling-Based Approaches to Calculating Marginal Densities", "author": ["Alan E. Gelfand", "Adrian F.M. Smith"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gelfand and Smith.,? \\Q1990\\E", "shortCiteRegEx": "Gelfand and Smith.", "year": 1990}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["Stuart Geman", "D. Geman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Geman and Geman.,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman.", "year": 1984}, {"title": "On Intelligence (with Sandra Blakeslee)", "author": ["Jeff Hawkins"], "venue": "Times Books,", "citeRegEx": "Hawkins.,? \\Q2004\\E", "shortCiteRegEx": "Hawkins.", "year": 2004}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G E Hinton", "R R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi Jaakkola", "Lawrence K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": null, "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Loeliger. Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Discrete markov image modeling and inference on the quadtree", "author": ["J.M. Laferte", "P. Perez", "F. Heitz"], "venue": "Trans. Img. Proc.,", "citeRegEx": "Laferte et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Laferte et al\\.", "year": 2000}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Hardware-efficient belief propagation", "author": ["Chia-Kai Liang", "Chao-Chung Cheng", "Yen-Chieh Lai", "Liang-Gee Chen", "Homer H. Chen"], "venue": "In CVPR, pages 80\u201387", "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "An introduction to factor graphs", "author": ["H.A. Loeliger"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Loeliger.,? \\Q2004\\E", "shortCiteRegEx": "Loeliger.", "year": 2004}, {"title": "Multiscale representations of markov random fields", "author": ["M.R. Luettgen", "W.C. Karl", "A.S. Willsky", "R.R. Tenney"], "venue": "Trans. Sig. Proc.,", "citeRegEx": "Luettgen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Luettgen et al\\.", "year": 1993}, {"title": "Task parallel implementation of belief propagation in factor graphs. In IPDPS Workshops, pages 1944\u20131953", "author": ["Nam Ma", "Yinglong Xia", "Viktor K. Prasanna"], "venue": "IEEE Computer Society,", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "A survey on latent tree models and applications", "author": ["Rapha\u00ebl Mourad", "Christine Sinoquet", "N.L. Zhang", "T. Liu", "Philippe Leray"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Mourad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2013}, {"title": "Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series)", "author": ["Kevin P. Murphy"], "venue": null, "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "Multiscale hidden markov models for bayesian image analysis", "author": ["Robert D. Nowak"], "venue": null, "citeRegEx": "Nowak.,? \\Q1999\\E", "shortCiteRegEx": "Nowak.", "year": 1999}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "Olshausen and Field.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1996}, {"title": "Belief propagation and learning in convolution multi-layer factor graph", "author": ["F. Palmieri", "A. Buonanno"], "venue": "In Proceedings of the the 4th International Workshop on Cognitive Information Processing, Copenhagen - Denmark,", "citeRegEx": "Palmieri and Buonanno.,? \\Q2014\\E", "shortCiteRegEx": "Palmieri and Buonanno.", "year": 2014}, {"title": "A comparison of algorithms for learning hidden variables in normal graphs", "author": ["F.A.N. Palmieri"], "venue": null, "citeRegEx": "Palmieri.,? \\Q2013\\E", "shortCiteRegEx": "Palmieri.", "year": 2013}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Marc\u2019Aurelio Ranzato", "Christopher S. Poultney", "Sumit Chopra", "Yann LeCun"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "A neuromorphic approach to computer vision", "author": ["Thomas Serre", "Tomaso Poggio"], "venue": "Commun. ACM,", "citeRegEx": "Serre and Poggio.,? \\Q2010\\E", "shortCiteRegEx": "Serre and Poggio.", "year": 2010}, {"title": "Efficient computation of sum-products on gpus through software-managed cache", "author": ["Mark Silberstein", "Assaf Schuster", "Dan Geiger", "Anjul Patney", "John D. Owens"], "venue": "Proceedings of the 22nd annual international conference on Supercomputing,", "citeRegEx": "Silberstein et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silberstein et al\\.", "year": 2008}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "ICML, volume 307 of ACM International Conference Proceeding Series,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J. Wainwright", "Michael I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan.", "year": 2008}, {"title": "Multiresolution markov models for signal and image processing", "author": ["Alan S Willsky"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Willsky.,? \\Q2002\\E", "shortCiteRegEx": "Willsky.", "year": 2002}, {"title": "Inference and parameter estimation on hierarchical belief networks for image segmentation", "author": ["Christian Wolf", "G\u00e9rald Gavin"], "venue": "Neurocomput.,", "citeRegEx": "Wolf and Gavin.,? \\Q2010\\E", "shortCiteRegEx": "Wolf and Gavin.", "year": 2010}, {"title": "Multilevel belief propagation for fast inference on markov random fields. In ICDM, pages 371\u2013380", "author": ["Liang Xiong", "Fei Wang", "Changshui Zhang"], "venue": "IEEE Computer Society,", "citeRegEx": "Xiong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2007}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "Recent striking results with \u201cdeep networks\u201d have generated much attention in machine learning on what is known as Representation Learning (see (Bengio et al., 2012) for a review).", "startOffset": 144, "endOffset": 165}, {"referenceID": 3, "context": "The creation of a feature hierarchy permits to the structure inside the data to emerge at different scales combining more and more complex features as we go upward in the hierarchy (Bengio and Delalleau, 2011), (Bengio et al.", "startOffset": 181, "endOffset": 209}, {"referenceID": 5, "context": "The creation of a feature hierarchy permits to the structure inside the data to emerge at different scales combining more and more complex features as we go upward in the hierarchy (Bengio and Delalleau, 2011), (Bengio et al., 2014).", "startOffset": 211, "endOffset": 232}, {"referenceID": 39, "context": "The neurons become selective for stimuli that are increasingly complex, from simple oriented bars and edges to moderately complex features, such as a combination of orientations, to complex objects (Serre and Poggio, 2010).", "startOffset": 198, "endOffset": 222}, {"referenceID": 22, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 69, "endOffset": 96}, {"referenceID": 0, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 98, "endOffset": 112}, {"referenceID": 14, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 155, "endOffset": 169}, {"referenceID": 27, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 171, "endOffset": 187}, {"referenceID": 35, "context": "Various architectures have been proposed as adaptive Bayesian graphs (Koller and Friedman, 2009), (Barber, 2012), but in our case the use of Factor Graphs (Forney, 2001), (Loeliger, 2004), specially in the simplified Reduced Normal Form (Palmieri, 2013), allows better modularity.", "startOffset": 237, "endOffset": 253}, {"referenceID": 31, "context": "The Latent Variable Model (LVM) (Murphy, 2012), (Bishop, 1999), also known as Autoclass (Cheeseman and Stutz, 1996) is the basic building block of a quadtree hierarchy.", "startOffset": 32, "endOffset": 46}, {"referenceID": 6, "context": "The Latent Variable Model (LVM) (Murphy, 2012), (Bishop, 1999), also known as Autoclass (Cheeseman and Stutz, 1996) is the basic building block of a quadtree hierarchy.", "startOffset": 48, "endOffset": 62}, {"referenceID": 10, "context": "The Latent Variable Model (LVM) (Murphy, 2012), (Bishop, 1999), also known as Autoclass (Cheeseman and Stutz, 1996) is the basic building block of a quadtree hierarchy.", "startOffset": 88, "endOffset": 115}, {"referenceID": 30, "context": "The complete system can be seen as a partitioned type of Latent Tree Model (Mourad et al., 2013).", "startOffset": 75, "endOffset": 96}, {"referenceID": 38, "context": "The vast literature on the deep representation learning (see the extensive overview in (Schmidhuber, 2015)) can be mostly divided in two main lines of research: the first one is based on probabilistic graphical models such as the Restricted Boltzmann Machine (RBM)", "startOffset": 87, "endOffset": 106}, {"referenceID": 20, "context": "(Hinton et al., 2006), (Hinton and Salakhutdinov, 2006), (Lee et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": ", 2006), (Hinton and Salakhutdinov, 2006), (Lee et al.", "startOffset": 9, "endOffset": 41}, {"referenceID": 25, "context": ", 2006), (Hinton and Salakhutdinov, 2006), (Lee et al., 2008) and the second one is based on neural network models as the autoencoder (Bengio et al.", "startOffset": 43, "endOffset": 61}, {"referenceID": 2, "context": ", 2008) and the second one is based on neural network models as the autoencoder (Bengio et al., 2007), (Ranzato et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 37, "context": ", 2007), (Ranzato et al., 2006).", "startOffset": 9, "endOffset": 31}, {"referenceID": 33, "context": "At the same time several unsupervised feature learning algorithms have been proposed: Sparse Coding (Olshausen and Field, 1996),(Lee et al.", "startOffset": 100, "endOffset": 127}, {"referenceID": 25, "context": "At the same time several unsupervised feature learning algorithms have been proposed: Sparse Coding (Olshausen and Field, 1996),(Lee et al., 2008), RBM (Hinton et al.", "startOffset": 128, "endOffset": 146}, {"referenceID": 20, "context": ", 2008), RBM (Hinton et al., 2006), Autoencoders (Bengio et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 2, "context": ", 2006), Autoencoders (Bengio et al., 2007), (Ranzato et al.", "startOffset": 22, "endOffset": 43}, {"referenceID": 37, "context": ", 2007), (Ranzato et al., 2006), (Vincent et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 41, "context": ", 2006), (Vincent et al., 2008), K-means (Coates and Ng, 2012).", "startOffset": 9, "endOffset": 31}, {"referenceID": 12, "context": ", 2008), K-means (Coates and Ng, 2012).", "startOffset": 17, "endOffset": 38}, {"referenceID": 18, "context": "Other models based on the memory-prediction theory of brain have also been proposed (Hawkins, 2004), (Dileep, 2008).", "startOffset": 84, "endOffset": 99}, {"referenceID": 13, "context": "Other models based on the memory-prediction theory of brain have also been proposed (Hawkins, 2004), (Dileep, 2008).", "startOffset": 101, "endOffset": 115}, {"referenceID": 42, "context": "Confining our interest to probabilistic graphical models, the most natural choice for modeling the spatial interactions between pixels (or patches) in the image is a two-dimensional lattice (Markov Random Field - MRF) where the nodes represent the pixels (or patches) and the potential functions are associated to the edges between adjacent nodes (Wainwright and Jordan, 2008).", "startOffset": 347, "endOffset": 376}, {"referenceID": 17, "context": "For these models convergence of the inference is not guaranteed and even if for large-scale models it is intractable, approximate and sub-optimal methods have been often used: Markov Chain Monte Carlo methods (Geman and Geman, 1984), (Gelfand and Smith, 1990), variational methods (Jordan et al.", "startOffset": 209, "endOffset": 232}, {"referenceID": 16, "context": "For these models convergence of the inference is not guaranteed and even if for large-scale models it is intractable, approximate and sub-optimal methods have been often used: Markov Chain Monte Carlo methods (Geman and Geman, 1984), (Gelfand and Smith, 1990), variational methods (Jordan et al.", "startOffset": 234, "endOffset": 259}, {"referenceID": 21, "context": "For these models convergence of the inference is not guaranteed and even if for large-scale models it is intractable, approximate and sub-optimal methods have been often used: Markov Chain Monte Carlo methods (Geman and Geman, 1984), (Gelfand and Smith, 1990), variational methods (Jordan et al., 1999), (Beal, 2003), graph cut (Boykov et al.", "startOffset": 281, "endOffset": 302}, {"referenceID": 1, "context": ", 1999), (Beal, 2003), graph cut (Boykov et al.", "startOffset": 9, "endOffset": 21}, {"referenceID": 8, "context": ", 1999), (Beal, 2003), graph cut (Boykov et al., 1999) and Belief Propagation (Xiong et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 45, "context": ", 1999) and Belief Propagation (Xiong et al., 2007).", "startOffset": 31, "endOffset": 51}, {"referenceID": 28, "context": "These have the advantages of allowing the application of efficient tree algorithms to perform exact inference with the trade off that the model is imperfect (Luettgen et al., 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al.", "startOffset": 157, "endOffset": 180}, {"referenceID": 7, "context": ", 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al.", "startOffset": 9, "endOffset": 35}, {"referenceID": 32, "context": ", 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al.", "startOffset": 37, "endOffset": 50}, {"referenceID": 24, "context": ", 1993), (Bouman and Shapiro, 1994), (Nowak, 1999), (Laferte et al., 2000), (Willsky, 2002).", "startOffset": 52, "endOffset": 74}, {"referenceID": 43, "context": ", 2000), (Willsky, 2002).", "startOffset": 9, "endOffset": 24}, {"referenceID": 44, "context": "have proposed a Markov cube adding additional connections at the different levels (Wolf and Gavin, 2010).", "startOffset": 82, "endOffset": 104}, {"referenceID": 36, "context": "On the quadtree structure inference can be performed using the belief propagation algorithm that was originally proposed for inferences on trees where exact solutions are guaranteed (Pearl, 1988).", "startOffset": 182, "endOffset": 195}, {"referenceID": 46, "context": "When the graph has loops, open issues still remain about the accuracy of inferences, even though often the bare application of standard belief propagation may already provide satisfactory results (loopy belief propagation) (Yedidia et al., 2005), (Frean, 2008).", "startOffset": 223, "endOffset": 245}, {"referenceID": 15, "context": ", 2005), (Frean, 2008).", "startOffset": 9, "endOffset": 22}, {"referenceID": 11, "context": "When the problem can be reduced to a tree, belief propagation provides exact marginalization and algorithms for learning latent trees have been proposed (Choi et al., 2011) with successful applications to computer vision.", "startOffset": 153, "endOffset": 172}, {"referenceID": 14, "context": "A very appealing approach to directed Bayesian graphs for visualization and manipulation, that has not found its full way in the applications, is the Factor Graph (FG) representation and in particular the so-called Normal Form (FGn) (Forney, 2001), (Loeliger, 2004).", "startOffset": 233, "endOffset": 247}, {"referenceID": 27, "context": "A very appealing approach to directed Bayesian graphs for visualization and manipulation, that has not found its full way in the applications, is the Factor Graph (FG) representation and in particular the so-called Normal Form (FGn) (Forney, 2001), (Loeliger, 2004).", "startOffset": 249, "endOffset": 265}, {"referenceID": 35, "context": "Furthermore, in the Reduced Normal Form (FGrn), through the use of replicator units (or equal constraints), the graph is reduced to an architecture in which each variable is connected to two factors at most (Palmieri, 2013).", "startOffset": 207, "endOffset": 223}, {"referenceID": 9, "context": "This is the framework on which this paper is focused because the designed network resembles a circuit diagram with belief messages more easily visualized as they flow into SISO blocks and travel through replicator nodes (Buonanno and Palmieri, 2014).", "startOffset": 220, "endOffset": 249}, {"referenceID": 34, "context": "In our previous work (Palmieri and Buonanno, 2014) we have reported some preliminary results on a multi-layer convolution Bayesian Factor Graph built as a stack of HMM-like trees.", "startOffset": 21, "endOffset": 50}, {"referenceID": 35, "context": "In the FGrn framework (Palmieri, 2013) the Bayesian graph is reduced to a simplified form composed only by Variables, Replicators (or Diverters), Single-Input/Single-Output (SISO) blocks and Source blocks.", "startOffset": 22, "endOffset": 38}, {"referenceID": 27, "context": "Even though various architectures have been proposed in the literature for Bayesian graphs (Loeliger, 2004), we have found that the FGrn framework is much easier to handle, it is more suitable to define unique learning equations (Palmieri, 2013) and it is more suited for distributed implementations.", "startOffset": 91, "endOffset": 107}, {"referenceID": 35, "context": "Even though various architectures have been proposed in the literature for Bayesian graphs (Loeliger, 2004), we have found that the FGrn framework is much easier to handle, it is more suitable to define unique learning equations (Palmieri, 2013) and it is more suited for distributed implementations.", "startOffset": 229, "endOffset": 245}, {"referenceID": 35, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al.", "startOffset": 54, "endOffset": 70}, {"referenceID": 9, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al.", "startOffset": 72, "endOffset": 101}, {"referenceID": 27, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al.", "startOffset": 130, "endOffset": 146}, {"referenceID": 23, "context": "For a more detailed review, refer to our recent works (Palmieri, 2013), (Buonanno and Palmieri, 2014) (or to the classical papers (Loeliger, 2004) (Kschischang et al., 2001)).", "startOffset": 147, "endOffset": 173}, {"referenceID": 35, "context": "We set the learning problem as an EM algorithm to maximize global likelihood (Palmieri, 2013).", "startOffset": 77, "endOffset": 93}, {"referenceID": 35, "context": "After adding a stabilizing term to the cost function and applying KKT conditions we obtain the following algorithm (Palmieri, 2013).", "startOffset": 115, "endOffset": 131}, {"referenceID": 35, "context": "The algorithm has been discussed and compared to other similar updates in (Palmieri, 2013).", "startOffset": 74, "endOffset": 90}, {"referenceID": 22, "context": "This architecture can be seen also as a Mixture of Categorical Distributions (Koller and Friedman, 2009).", "startOffset": 77, "endOffset": 104}, {"referenceID": 0, "context": "Each element of the alphabet S represents a \u201dBayesian cluster\u201d for the N \u00b7M dimensional stochastic image, X = [X[n,m]]m=1:M n=1:N (similar to the Naive Bayes classifier (Barber, 2012)).", "startOffset": 169, "endOffset": 183}, {"referenceID": 22, "context": "Essentially each bottom variable is independent from the others given the Hidden Variable (Koller and Friedman, 2009).", "startOffset": 90, "endOffset": 117}, {"referenceID": 35, "context": "All SISO blocks and the source block adapt their parameters using an iterative Maximum Likelihood Algorithm (Palmieri (2013)) outlined in Section 3.", "startOffset": 109, "endOffset": 125}, {"referenceID": 26, "context": "Some studies have been carried out for other deep network frameworks (Liang et al., 2009), (Silberstein et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 40, "context": ", 2009), (Silberstein et al., 2008), (Ma et al.", "startOffset": 9, "endOffset": 35}, {"referenceID": 29, "context": ", 2008), (Ma et al., 2012)) and we are confident that similarly the FGrn paradigm may present new interesting opportunities to approach some of the most challenging tasks in computer vision.", "startOffset": 9, "endOffset": 26}], "year": 2015, "abstractText": "We propose a Multi-Layer Network based on the Bayesian framework of the Factor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional lattice. The Latent Variable Model (LVM) is the basic building block of a quadtree hierarchy built on top of a bottom layer of random variables that represent pixels of an image, a feature map, or more generally a collection of spatially distributed discrete variables. The multi-layer architecture implements a hierarchical data representation that, via belief propagation, can be used for learning and inference. Typical uses are pattern completion, correction and classification. The FGrn paradigm provides great flexibility and modularity and appears as a promising candidate for building deep networks: the system can be easily extended by introducing new and different (in cardinality and in type) variables. Prior knowledge, or supervised information, can be introduced at different scales. The FGrn paradigm provides a handy way for building all kinds of architectures by interconnecting only three types of units: Single Input Single Output (SISO) blocks, Sources and Replicators. The network is designed like a circuit diagram and the belief messages flow bidirectionally in the whole system. The learning algorithms operate only locally within each block. The framework is demonstrated in this paper in a three-layer structure applied to images extracted from a standard data set.", "creator": "LaTeX with hyperref package"}}}