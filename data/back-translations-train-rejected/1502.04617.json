{"id": "1502.04617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2015", "title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis", "abstract": "Errors in data are usually unwelcome and so some means to correct them is useful. However, it is difficult to define, detect or correct errors in an unsupervised way. Here, we train a deep neural network to re-synthesize its inputs at its output layer for a given class of data. We then exploit the fact that this abstract transformation, which we call a deep transform (DT), inherently rejects information (errors) existing outside of the abstract feature space. Using the DT to perform probabilistic re-synthesis, we demonstrate the recovery of data that has been subject to extreme degradation.", "histories": [["v1", "Mon, 16 Feb 2015 16:41:26 GMT  (197kb)", "http://arxiv.org/abs/1502.04617v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1502.04617"}, "pdf": {"name": "1502.04617.pdf", "metadata": {"source": "CRF", "title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "This year, it is closer than ever before to being able to take the lead."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["E. Hinton G", "S. Osindero", "Y. Teh"], "venue": "Neural Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning deep architectures for AI, Foundations and Trends in Machine Learning 2:1\u2013127", "author": ["Y Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition", "author": ["Ciresan C. D", "U. Meier", "M. Gambardella L", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors,", "author": ["E. Hinton G", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Computing Research Repository (CoRR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Over-Sampling in a Deep Neural Network, arxiv.org abs/1502.03648", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "A DNN that is trained to replicate its inputs at its output layer provides a oneto-one mapping and is known as an autoencoder [2].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "We chose the well-known MNIST hand-written digit classification problem [3], [1], [4] and trained a typical feed-forward DNN to classify the digits.", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "We chose the well-known MNIST hand-written digit classification problem [3], [1], [4] and trained a typical feed-forward DNN to classify the digits.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "We chose the well-known MNIST hand-written digit classification problem [3], [1], [4] and trained a typical feed-forward DNN to classify the digits.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "Both networks used biased sigmoid activation functions [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Both models were independently trained on the 60,000 training examples from the MNIST dataset [4] using stochastic gradient descent (SGD).", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "We call this a deep transform because it is an abstract transformation learned by a deep neural network [6].", "startOffset": 104, "endOffset": 107}], "year": 2015, "abstractText": "Errors in data are usually unwelcome and so some means to correct them is useful. However, it is difficult to define, detect or correct errors in an unsupervised way. Here, we train a deep neural network to re-synthesize its inputs at its output layer for a given class of data. We then exploit the fact that this abstract transformation, which we call a deep transform (DT), inherently rejects information (errors) existing outside of the abstract feature space. Using the DT to perform probabilistic resynthesis, we demonstrate the recovery of data that has been subject to extreme degradation.", "creator": "PDFCreator Version 1.7.1"}}}