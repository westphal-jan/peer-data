{"id": "1502.04843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2015", "title": "Generalized Gradient Learning on Time Series under Elastic Transformations", "abstract": "The majority of machine learning algorithms assumes that objects are represented as vectors. But often the objects we want to learn on are more naturally represented by other data structures such as sequences and time series. For these representations many standard learning algorithms are unavailable. We generalize gradient-based learning algorithms to time series under dynamic time warping. To this end, we introduce elastic functions, which extend functions on time series to matrix spaces. Necessary conditions are presented under which generalized gradient learning on time series is consistent. We indicate how results carry over to arbitrary elastic distance functions and to sequences consisting of symbolic elements. Specifically, four linear classifiers are extended to time series under dynamic time warping and applied to benchmark datasets. Results indicate that generalized gradient learning via elastic functions have the potential to complement the state-of-the-art in statistical pattern recognition on time series.", "histories": [["v1", "Tue, 17 Feb 2015 10:08:48 GMT  (797kb)", "http://arxiv.org/abs/1502.04843v1", null], ["v2", "Tue, 9 Jun 2015 10:50:41 GMT  (435kb,D)", "http://arxiv.org/abs/1502.04843v2", "accepted for publication in Machine Learning"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["brijnesh jain"], "accepted": false, "id": "1502.04843"}, "pdf": {"name": "1502.04843.pdf", "metadata": {"source": "CRF", "title": "Generalized Gradient Learning on Time Series under Elastic Transformations", "authors": ["Brijnesh J. Jain"], "emails": ["brijnesh.jain@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.04 843v 1 [cs.L G] 17 February 2015Generalized Gradient Learning on TimeSeries under Elastic TransformationsBrijnesh J. JainTechnische Universita \u00bc t BerlinBerlin, Germany-mail: brijnesh.jain @ gmail.com The majority of machine learning algorithms assume that objects are represented as vectors. To this end, we introduce elastic functions that extend the functions of time series to matrix spaces. Necessary conditions are presented under which generalized gradient learning is consistent on time series. We show how the results are transferred to arbitrary elastic distance functions and to sequences of symbolic elements."}, {"heading": "1. Introduction", "text": "In this context, it should be noted that the solution to problems that have arisen in the past is not a problem, but a problem that has arisen in the past."}, {"heading": "2. Background", "text": "This section summarizes the necessary background material and provides an introduction to the terminology and notation used in this essay. In Section 2.1 we define the DTW distance. Section 2.2 presents the problem of learning from examples, Section 2.3 introduces smooth functions step by step, and Section 2.4 describes the generalized gradient method for minimizing smooth functions step by step."}, {"heading": "2.1. Dynamic Time Warping Distance", "text": "With [n] we denote the set {1,.., n} for some n,. A time series of length n is an ordered sequence x = (x1,.., xn) with properties xi xi =, which are sampled at individual points in time i, [n]. To define the DTW distances between time series x and y of length n andm, resp. we construct a grid G = [n] \u00b7 [m]. A distorting path in the grid G is a sequence \u03c6 = (t1,.., tp), consisting of points tk = (ik, jk) and such g. t1 = (1, 1) and tp = (n, m) (boundary conditions) 2. tk + 1 \u2212 tk. (1, 0), (0, 1), (1, 1) and y (distortion conditions)."}, {"heading": "2.2. The Problem of Learning", "text": "We consider example learning to be the problem of minimizing a functional risk. To present the main ideas, it is sufficient to focus on supervised learning. Let's consider an input space X and an output space Y. The problem of supervised learning is to estimate an unknown function f \u0445: X \u2192 Y, which according to an underlying common probability distribution P (x, y) on X \u00b7 Y.To measure how well a function f: X \u2192 Y predicts the output values y from x, we run the risk R [f] = x \u00b7 Y (y, y) on an underlying risk distribution P (x, y) on X \u00b7 Y.To measure how well a function f: X \u2192 Y predicts the output values y from x, we run the risk R [f] on a mical probability f (x).The goal of learning is to find a function: X \u2192 empirical task in which the risk cannot be minimized."}, {"heading": "2.3. Piecewise Smooth Functions", "text": "A function f: X \u2192 R defined in a Euclidean space is piecemeal smooth if f is continuous and there is a finite collection of continuously differentiable functionsR (f) = {fi: E \u2192 R: i-I} indexed by set I, so that f (x): i-fi (x): i-I} for all x-X. We refer to the collection R (f) as a representation of f. A function fi (f) = f (x) is an active function of f at x. By Af (x) = i-fi (x) = f (x)} we refer to the active index set of f at x. Piecemeal smooth functions are closed under composition, scalar multiplication, finite sums, pointedly maximum and minimum operations. Specifically, the maximum and minimum operation density of a finite collection of differentiable functions allow us to construct piecemeal smooth functions."}, {"heading": "2.4. Generalized Gradient Methods", "text": "Consider the minimization problem in x-Z f (x), (1) where f is piecemeal smooth. Consider the subset of solutions that fulfills the necessary condition of optimality, and f-Z = {f (x): x-Z-Z is the set of solution values. Consider the following iterative method, which is henceforth called the generalized gradient method: x0-Z (2) xt + 1-Z (xt \u2212 \u03b7t \u00b7 fi (xt)) (3) i-Af (xt), (4) where \"Z\" is the polyvalent projection on Z and \"Z\" is the learning rate that satisfies the conditional lim t."}, {"heading": "3. Generalized Gradient Learning on Time Series Spaces", "text": "In this section we generalize gradient-based learning to time series spaces. Section 3.1 introduces elastic functions as a key concept. Section 3.2 describes a generic scheme of supervised general gradient learning. Section 3.3 discusses model selection. In Section 3.4 we examine elastic linear classifiers. Then we move on to unattended generalized gradient learning in Section 3.5 and conclude with generalizations to other elastic proximity functions and arbitrary sequence data in Section 3.6."}, {"heading": "3.1. Elastic Functions", "text": "The derivative of a function is undefined outside standardized vector spaces. To provide derivatives for functions on time series, we present the concept of elastic function, which links functions on time series with matrix spaces X = Rn \u00b7 m. The inner product induces the Euclidean space of all real (n \u00b7 m) matrices with inner product < X > M of the X series has the following meaning: the number of rows n referring to the maximum length of all time series. The inner product induces the number of columns is a problem."}, {"heading": "3.2. Supervised Generalized Gradient Learning", "text": "This section introduces a generic scheme of generalized gradient learning for time series under dynamic time courses. Suppose D = (x1, y1),..., (xN), yN}, T \u00b7 Y with parameters \u03b8 = (W1,.., Wr, b) that D = {(x1, y1),.., (xN), T \u00b7 Y is a training set. According to the empirical risk minimization principle, the goal is to minimize RN = (x, y), and the loss (y, fTB (x) as a function of the risk minimization principle can be written in relation to the parameter z = (x, y), y)."}, {"heading": "3.3. Model Selection", "text": "If the hypotheses space F is too complex for the given training set D, the learned model fN may primarily have poor generalization performance (overmatch). A common way to avoid overmatch is to minimize the regulated risk RN [f] = RN [f] + overmatch (f), where \"f\" is the regulator that penalizes excessively complex functions and \"f\" is the regulation parameter that quantifies the trade-off between minimizing the empirical risk RN [f] and the complexity of f (f). \"Piecewise\" smooth regulators for primitive functions F on trait vectors directly to piecewise smooth regulators via elastic functions f [f] on time series. \"Piecewise smooth\" is the regularized risk piecewise smooth regulator for primitive functions on piecewise smooth regulators. \""}, {"heading": "3.4. Elastic Linear Classifiers", "text": "An elastic linear classifier is a function of formh\u03b8: T \u2192 Y, x 7 \u2192 {+ 1: f\u03b8 (x) \u2265 0 \u2212 1: f\u03b8 (x) < 0 (11), where f\u03b8 (x) margin (x) = b + \u03c3 (x, W), is an elastic linear function and \u03b8 = (W, b), summarizes the parameters. We assign a time series x to the positive class if f\u03b8 (x) \u2265 0 and to the negative class otherwise. Depending on the choice of the loss function (y, f\u03b8), we obtain various elastic linear classifiers as shown in Table 1. The loss function of elastic logistic regression is differentiable as a function of fordable and b, but piecewise smooth as a function of W. All other loss functions are piecewise smooth as a function of fledge, b and W."}, {"heading": "3.5. Unsupervised Generalized Gradient Learning", "text": "Multiple unsupervised learning algorithms such as k-mean, self-organizing maps, major component analysis and the mix of Gaussians are based on the concept of (weighted) mean value. Once we know how to calculate a set of time series on average, the extension of mean-based learning methods to time series follows the same rules as for vectors. Therefore, it is sufficient to focus on the problem of averaging a set of time series. Let's assume, D = {x1,.., xN}. T is a set of blank time series. Consider the sum of squared distances (Y) = N \u00b2 (xi, Y), (14) where Y-X is a matrix and vice versa the elastic Euclidean distance. A matrix Y, which minimizes f-value, is an elastic mean value of set D and the minimum value f-value f-value f (Y-point) is the variation of D. The updating rule of point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point is the point-point point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point is the point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point-point"}, {"heading": "3.6. Generalizations", "text": "This section identifies some generalizations of the concept of elastic functions."}, {"heading": "3.6.1. Generalization to other Elastic Distance Functions", "text": "The elastic functions presented here are based on the DTW distance over embedded distances along a series of realizable distortion distances with square differences as local transformation costs. The choice of the distance function and local transformation costs is arbitrary. We may as well define elastic functions based on distances other than the DTW distance. Learning results are transmitted whenever a proximity in terms of time series satisfies the following sufficient conditions: (1) \u03c1 minimizes the cost over a series of realizable paths, (2) the cost of a realizable path is a piecemeal smooth function as a function of local transformation costs, and (3) the local transformation costs are piecemeal. Regarding the DTW distance, these generalizations include euclidean distance and the DTW distances with additional limitations such as the Sakoe-Chiba band [21]. Furthermore, absolute differences as local transformation costs are feasible, as the value function is gradual."}, {"heading": "3.6.2. Generalization to Multivariate Time Series", "text": "A multivariate time series is an ordered sequence x = (x1,.., xn) consisting of character vectors xi-Rd. We can define the DTW distance between multivariate time series x and y as in the univariate case, but the local transformation costs c (xi, yj) = (xi \u2212 yj) 2 can be replaced by c (xi, yj) = xi \u2212 yj \u04452. To define elastic functions, we embed multivariate time series in the set X = (Rd) n \u00b7 m vector value matrices X = (xij) with elements xij-Rd. This adaptation preserves the gradual smoothness, since Euclidean space X is a direct product of low-dimensional Euclidean spaces."}, {"heading": "3.6.3. Generalization to Sequences with Symbolic Attributes", "text": "We consider sequences x = (x1,., xn) with attributes xi from any finite group A of d attributes (symbols). Since A is finite, we can render its attributes a-A by d-dimensional binary vectors a-dimensional, all but one of which are zero. The unique non-zero element has the value one and stands with attribute a. In this way, we can reduce the case of the assigned sequences to the case of multivariate time series. We can introduce the following local transformation costs (xi, yj) = {0: xi = yj 1: xi 6 = yj. More generally, we can introduce the local transformation costs of Formc (xi, yj) = k (xi, xi) \u2212 2k (xi, yj) + k (yj, yj) + k (yj), where k: A \u00d7 A \u2192 R is a positively defined kernel. Provided that the kernel is an inner product in any two-dimensional unit."}, {"heading": "4. Relationship to Previous Approaches", "text": "To place these approaches within the framework of elastic functions, it is sufficient to consider the problem of calculating an average of a series of time series. Suppose that D = {x1,.. xN} is a series of time series. Consider the sum of squared distances g (y) = N \u00b2 i = 1d2 (xi, y), where d is the DTW distance. Algorithm 1 outlines a uniform minimization method of g. The quantity Z in line 1 of the method consists of all matrices with n identical rows, where n is the maximum length of all time series of D. Thus, there is a one-to-one correspondence between time series of T and matrices of Z. By construction, we have g (y) = f."}, {"heading": "5. Experiments", "text": "The aim of this section is to evaluate the performance of all four elastic linear classifiers presented in section 3.4."}, {"heading": "5.1. Experimental Setup.", "text": "We looked at all the two-class problems of the UCR time series data [8]. Table 2 summarizes characteristic features of the data sets. We compared the four elastic linear classifiers of section 3.4 with different variants of the closest neighbour (NN) classifier with the DTW distance. The variants of the NN classifiers differ in the selection of prototypes. The first variant uses all training examples as prototypes. The second and third variants learned one prototype per class from the training set using the k-mean as the second variant and agglomerative hierarchical clustering as the third variant [16]. The settings of the elastic linear classifiers were as follows: The dimension of matrix space X was set to n \u00d7 0.1n, with n being the length of the shortest (= longest) time series in the training parameter."}, {"heading": "5.2. Results and Discussion.", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6. Conclusion", "text": "This approach combines (a) the novel concept of elastic functions linking elastic proximities on time series with step-by-step smooth functions, with (b) generalized gradient methods for uneven optimization. Using the proposed diagram, we demonstrated (1) how a broad class of gradient-based learning can be applied to time series under elastic transformations, (2) derived general convergence statements that justify the generalizations, and (3) put existing adaptive methods in the right context. Examples include elastic logistic regression, elastic (edge) perception learning, and elastic linear SVM on two-class problems, and compared with neighboring classifiers that use the DTW distance. Despite the simplicity of decision limits and compression efficiency, elastic linear classifiers can provide compelling performance on the basis of elastic functions, there is still room for improvements in the number of shapes on the xelastic functions."}, {"heading": "A. Proof of Convergence Results for Elastic Linear", "text": "Since affine functions are convex and the maximum of convex functions is also convex, the elastic inner product is convex. Furthermore, the composition of convex functions is convex. Therefore, the loss functions of elastic linear classifiers are convex. Then, the first convergence result is shown in [22]. To show the other two convergence statements, we assume that | D | = N. For each training example (xi, yi) - D the loss functions i (\u03b8) - i (yi, b + \u03c3 (xi, W))) - is real and convex, where \u03b8 = (W, b) -. Then there is a positive scalar Ci limiting the subdifferential of i for all i [N]. Let us note that vesatC = max i = 1,..., N Ci.Then follows Prop. 2.2. - Prop."}], "references": [{"title": "A Complexity-Invariant Distance Measure for Time Series", "author": ["G.E. Batista", "X. Wang", "E.J. Keogh"], "venue": "SIAM International Conference on Data Mining, SDM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Ensemble methods in machine learning", "author": ["T.G. Dietterich"], "venue": "Proceedings of the First International Workshop on Multiple Classifier Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization", "author": ["Y. Ermoliev", "V. Norkin"], "venue": "Cybernetics and Systems Analysis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "A review on time series data mining", "author": ["T. Fu"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Pattern extraction for time series classification", "author": ["P. Geurts"], "venue": "Principles of Data Mining and Knowledge Discovery, pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "The elements of statistical learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Time-series clustering by approximate prototypes", "author": ["V. Hautamaki", "P. Nykanen", "P. Franti"], "venue": "International Conference on Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "The UCR Time Series Classification/Clustering Homepage: www.cs.ucr.edu/~eamonn/time_series_data", "author": ["E. Keogh", "Q. Zhu", "B. Hu", "Y. Hao", "X. Xi", "L. Wei", "C.A. Ratanamahatana"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "The symmetric time-warping problem: From continuous to discrete Time Warps, String Edits and Macromolecules: The Theory and Practice of Sequence Comparison", "author": ["J.B. Kruskal", "M. Liberman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Time series classification with ensembles of elastic distance measures", "author": ["J. Lines", "A. Bagnall"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Incremental subgradient methods for nondifferentiable optimization", "author": ["A. Nedic", "D.P. Bertsekas"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Inaccuracies of shape averaging method using dynamic time warping for time series data", "author": ["V. Niennattrakul", "C.A. Ratanamahatana"], "venue": "Computational Science \u2014 ICCS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "On Clustering Multimedia Time Series Data Using K-Means and Dynamic Time Warping", "author": ["V. Niennattrakul", "C.A. Ratanamahatana"], "venue": "International Conference on Multimedia and Ubiquitous Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization", "author": ["V. Norkin"], "venue": "Cybernetics and Systems Analysis,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "A global averaging method for dynamic time warping, with applications to clustering", "author": ["F. Petitjean", "A. Ketterlin", "P. Gancarski"], "venue": "Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Dynamic Time Warping Averaging of Time Series allows Faster and more Accurate Classification", "author": ["F. Petitjean", "G. Forestier", "G.I. Webb", "A.E. Nicholson", "Y. Chen", "E. Keogh"], "venue": "International Conference on Data Mining, ICDM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Considerations in applying clustering techniques to speaker?independent word recognition", "author": ["L.R. Rabiner", "J.G. Wilpon"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1979}, {"title": "Making time- series classification more accurate using learned constraints", "author": ["C.A. Ratanamahatana", "E.J. Keogh"], "venue": "SIAM International Conference on Data Mining,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Three myths about dynamic time warping data mining", "author": ["C.A. Ratanamahatana", "E.J. Keogh"], "venue": "SIAM International Conference on Data Mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1978}, {"title": "Minimization Methods for Nondifferentiable Functions", "author": ["N.Z. Shor"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1985}, {"title": "Self-organizing maps and learning vector quantization for feature sequences", "author": ["P. Somervuo", "T. Kohonen"], "venue": "Neural Processing Letters,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["X. Wang", "A. Mueen", "H. Ding", "G. Trajcevski", "P. Scheuermann", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "A modified K-means clustering algorithm for use in isolated work recognition", "author": ["J.P. Wilpon", "L.R. Rabiner"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1985}, {"title": "Fast time series classification using numerosity reduction", "author": ["X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana"], "venue": "Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "Statistical pattern recognition on time series finds many applications in diverse domains such as speech recognition, medical signal analysis, and recognition of gestures [4, 5].", "startOffset": 171, "endOffset": 177}, {"referenceID": 4, "context": "Statistical pattern recognition on time series finds many applications in diverse domains such as speech recognition, medical signal analysis, and recognition of gestures [4, 5].", "startOffset": 171, "endOffset": 177}, {"referenceID": 0, "context": "The weak mathematical structure of time series spaces bears two consequences: (a) there are only few learning algorithms that directly operate on time series under elastic transformation; and (b) simple methods like the nearest neighbor classifier together with the DTW distance belong to the state-of-the-art and are reported to be exceptionally difficult to beat [1, 10, 26].", "startOffset": 365, "endOffset": 376}, {"referenceID": 9, "context": "The weak mathematical structure of time series spaces bears two consequences: (a) there are only few learning algorithms that directly operate on time series under elastic transformation; and (b) simple methods like the nearest neighbor classifier together with the DTW distance belong to the state-of-the-art and are reported to be exceptionally difficult to beat [1, 10, 26].", "startOffset": 365, "endOffset": 376}, {"referenceID": 24, "context": "The weak mathematical structure of time series spaces bears two consequences: (a) there are only few learning algorithms that directly operate on time series under elastic transformation; and (b) simple methods like the nearest neighbor classifier together with the DTW distance belong to the state-of-the-art and are reported to be exceptionally difficult to beat [1, 10, 26].", "startOffset": 365, "endOffset": 376}, {"referenceID": 8, "context": "They mainly devise or apply different measures of central tendency of a set of time series under dynamic time warping [9, 17, 18, 15].", "startOffset": 118, "endOffset": 133}, {"referenceID": 16, "context": "They mainly devise or apply different measures of central tendency of a set of time series under dynamic time warping [9, 17, 18, 15].", "startOffset": 118, "endOffset": 133}, {"referenceID": 14, "context": "They mainly devise or apply different measures of central tendency of a set of time series under dynamic time warping [9, 17, 18, 15].", "startOffset": 118, "endOffset": 133}, {"referenceID": 6, "context": "The individual approaches reported in the literature are k-means [7, 12, 13, 16, 25], self-organizing maps [23], and learning vector quantization [23].", "startOffset": 65, "endOffset": 84}, {"referenceID": 11, "context": "The individual approaches reported in the literature are k-means [7, 12, 13, 16, 25], self-organizing maps [23], and learning vector quantization [23].", "startOffset": 65, "endOffset": 84}, {"referenceID": 12, "context": "The individual approaches reported in the literature are k-means [7, 12, 13, 16, 25], self-organizing maps [23], and learning vector quantization [23].", "startOffset": 65, "endOffset": 84}, {"referenceID": 15, "context": "The individual approaches reported in the literature are k-means [7, 12, 13, 16, 25], self-organizing maps [23], and learning vector quantization [23].", "startOffset": 65, "endOffset": 84}, {"referenceID": 23, "context": "The individual approaches reported in the literature are k-means [7, 12, 13, 16, 25], self-organizing maps [23], and learning vector quantization [23].", "startOffset": 65, "endOffset": 84}, {"referenceID": 21, "context": "The individual approaches reported in the literature are k-means [7, 12, 13, 16, 25], self-organizing maps [23], and learning vector quantization [23].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "The individual approaches reported in the literature are k-means [7, 12, 13, 16, 25], self-organizing maps [23], and learning vector quantization [23].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "Then learning on time series amounts in minimizing piecewise smooth risk functionals using generalized gradient methods proposed by [3, 14].", "startOffset": 132, "endOffset": 139}, {"referenceID": 13, "context": "Then learning on time series amounts in minimizing piecewise smooth risk functionals using generalized gradient methods proposed by [3, 14].", "startOffset": 132, "endOffset": 139}, {"referenceID": 7, "context": "We tested the four elastic linear classifiers to all two-class problems of the UCR time series benchmark dataset [8].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "This procedure converges to a solution satisfying the necessary condition of optimality [3, 14]:", "startOffset": 88, "endOffset": 95}, {"referenceID": 13, "context": "This procedure converges to a solution satisfying the necessary condition of optimality [3, 14]:", "startOffset": 88, "endOffset": 95}, {"referenceID": 2, "context": "Ermoliev and Norkin also presented a consistency theorem under similar conditions when the problem is posed as that of a stochastic optimization problem [3, 14].", "startOffset": 153, "endOffset": 160}, {"referenceID": 13, "context": "Ermoliev and Norkin also presented a consistency theorem under similar conditions when the problem is posed as that of a stochastic optimization problem [3, 14].", "startOffset": 153, "endOffset": 160}, {"referenceID": 19, "context": "Other options for model selection are imposing global constraints such as the SakoeChiba band [21] that restricts the set of feasible warping paths to a band of certain width along the main diagonal of the grid.", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "With regard to the DTW distance, these generalizations include the Euclidean distance and DTW distances with additional constraints such as the Sakoe-Chiba band [21].", "startOffset": 161, "endOffset": 165}, {"referenceID": 8, "context": "Most of the literature is summarized in [9, 15, 16, 23].", "startOffset": 40, "endOffset": 55}, {"referenceID": 14, "context": "Most of the literature is summarized in [9, 15, 16, 23].", "startOffset": 40, "endOffset": 55}, {"referenceID": 15, "context": "Most of the literature is summarized in [9, 15, 16, 23].", "startOffset": 40, "endOffset": 55}, {"referenceID": 21, "context": "Most of the literature is summarized in [9, 15, 16, 23].", "startOffset": 40, "endOffset": 55}, {"referenceID": 7, "context": "We considered all two-class problems of the UCR time series datasets [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 15, "context": "The second and third variant learned one prototype per class from the training set using k-means as second variant and agglomerative hierarchical clustering as third variant [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 0, "context": "Comparison of elastic linear classifiers and nearest neighbor methods is motivated by the following reasons: First, nearest neighbor classifiers belong to the state-of-the-art and are considered to be exceptionally difficult to beat [1, 10, 26].", "startOffset": 233, "endOffset": 244}, {"referenceID": 9, "context": "Comparison of elastic linear classifiers and nearest neighbor methods is motivated by the following reasons: First, nearest neighbor classifiers belong to the state-of-the-art and are considered to be exceptionally difficult to beat [1, 10, 26].", "startOffset": 233, "endOffset": 244}, {"referenceID": 24, "context": "Comparison of elastic linear classifiers and nearest neighbor methods is motivated by the following reasons: First, nearest neighbor classifiers belong to the state-of-the-art and are considered to be exceptionally difficult to beat [1, 10, 26].", "startOffset": 233, "endOffset": 244}, {"referenceID": 5, "context": "In contrast, nearest neighbor methods make very mild assumption about the data and therefore often yield accurate but possibly unstable predictions [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "As reported by [10], ensemble classifiers of different elastic distance measures are assumed to be first approach that significantly outperformed the NN+ALL classifier on the UCR time series dataset.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "This result is not surprising, because in machine learning it is well known for a long time hat ensemble classifiers often perform better than their base classifiers for reasons explained in [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 15, "context": "Evidence for this finding is provided by two results: first, AHC and KME performed best among several prototype selection methods for NN classification [16]; and second, error rates of EL classifiers are significantly better than those of NN+AHC and NN+KME for eight, comparable for two, and significantly worse for two datasets.", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "Findings reported by [24]", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "For NN classifiers, two common techniques to decrease computation time are global constraints such as the Sakoe-Chiba band [21] and diminishing the number of DTW distance calculations by applying lower bounding technique [19, 20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "For NN classifiers, two common techniques to decrease computation time are global constraints such as the Sakoe-Chiba band [21] and diminishing the number of DTW distance calculations by applying lower bounding technique [19, 20].", "startOffset": 221, "endOffset": 229}, {"referenceID": 18, "context": "For NN classifiers, two common techniques to decrease computation time are global constraints such as the Sakoe-Chiba band [21] and diminishing the number of DTW distance calculations by applying lower bounding technique [19, 20].", "startOffset": 221, "endOffset": 229}], "year": 2017, "abstractText": "The majority of machine learning algorithms assumes that objects are represented as vectors. But often the objects we want to learn on are more naturally represented by other data structures such as sequences and time series. For these representations many standard learning algorithms are unavailable. We generalize gradient-based learning algorithms to time series under dynamic time warping. To this end, we introduce elastic functions, which extend functions on time series to matrix spaces. Necessary conditions are presented under which generalized gradient learning on time series is consistent. We indicate how results carry over to arbitrary elastic distance functions and to sequences consisting of symbolic elements. Specifically, four linear classifiers are extended to time series under dynamic time warping and applied to benchmark datasets. Results indicate that generalized gradient learning via elastic functions have the potential to complement the state-of-the-art in statistical pattern recognition on time series.", "creator": "LaTeX with hyperref package"}}}