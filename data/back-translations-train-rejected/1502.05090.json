{"id": "1502.05090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2015", "title": "Real time clustering of time series using triangular potentials", "abstract": "Motivated by the problem of computing investment portfolio weightings we investigate various methods of clustering as alternatives to traditional mean-variance approaches. Such methods can have significant benefits from a practical point of view since they remove the need to invert a sample covariance matrix, which can suffer from estimation error and will almost certainly be non-stationary. The general idea is to find groups of assets which share similar return characteristics over time and treat each group as a single composite asset. We then apply inverse volatility weightings to these new composite assets. In the course of our investigation we devise a method of clustering based on triangular potentials and we present associated theoretical results as well as various examples based on synthetic data.", "histories": [["v1", "Wed, 18 Feb 2015 00:27:39 GMT  (1703kb)", "http://arxiv.org/abs/1502.05090v1", "AIFU15"]], "COMMENTS": "AIFU15", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aldo pacchiano", "oliver williams"], "accepted": false, "id": "1502.05090"}, "pdf": {"name": "1502.05090.pdf", "metadata": {"source": "CRF", "title": "REAL TIME CLUSTERING OF TIME SERIES USING TRIANGULAR POTENTIALS", "authors": ["Aldo Pacchiano", "Oliver J. Williams"], "emails": ["aldopacchiano@gmail.com", "oliver.williams@markhamrae.com"], "sections": [{"heading": null, "text": "KEYWORDS clustering, expected benefits, graphic models, K-clique problem"}, {"heading": "1. INTRODUCTION", "text": "A common problem in the financial world is the question of how to build a diversified portfolio of investment opportunities over multiple timeframes. This problem is ubiquitous in fund management, banking and insurance, and has led to a wealth of evolving literature, both theoretically and empirically. From an informal mathematical perspective, the central challenge is to develop a method for determining the weighting of a number of random variables so that ex-post realizations of the weighted sum optimize a certain objective function on average. The objective function most commonly used in financial economics is a conceptual usefulness function that focuses on the portfolio construction problem, which is a question of optimizing the so-called expected usefulness of the horizon. Koller and Friedman provide a detailed discussion of horizontal utility functions and decision-making theory in the general machine learning context."}, {"heading": "2. PROBLEM SPECIFICATION", "text": "Definition 1 Let us define the set of natural numbers from to. Let us define time series where for.Definition 2 clustering.A clustering of is an equivalence relationship \u20ac~ about as follows: 1. Reflexivity: If \u20aci ~ j then \u20acj ~ i. 2. Transitivity: If \u20aci ~ j and \u20acj ~ k then \u20ack ~ i. Definition 3 time-dependent clustering. We say \u20aci ~ k j when and are clustered in due time.Our goal is to find a sequence \u20ac{~ k} k = 1 m, i.e. we leave the nature of the cluster relationship over time.We define the distance between rows at a given time as for all and the similarity at a given time.The functions are specified by the user of the algorithm and can be selected on the basis of previous domain-specific knowledge, or perhaps through a more systematic process of searching for alternative specifications, which Distance function is guided by time function.4."}, {"heading": "3. SPECTRAL CLUSTERING", "text": "Here we present the Spectral Clustering Algorithm, which is suitable for data where the cluster structure does not change over time. Later in the work, we will compare the performance of our proposed approach with this benchmark method. Definition 7 The laplac matrix of a similarity matrix is defined as: Where. The most basic spectral clustering algorithm for the bipartition of data is the Shi Malik Bipartition Algorithm, which we describe below."}, {"heading": "3.1. Shi Malik algorithm", "text": "Faced with items and a similarity matrix, the Shi Malik algorithm divides the data into two groups based on the eigenvector, which corresponds to the second smallest eigenvalue of the normalized laplac matrix. Algorithm 1 The Shi Malik bipartition algorithm: 1. Calculate the laplaker from a similarity matrix. 2. Calculate the second smallest eigenvalue and the corresponding eigenvector. 3. Calculate the median of its corresponding eigenvector. 4. All points whose component is larger than it is assigned are assigned to the remaining points. Unfortunately, the Shi Malik algorithm is not a dynamic method, i.e. it is not intended to identify an underlying cluster structure that varies in time."}, {"heading": "3.2. A generalized spectral clustering approach", "text": "The following algorithm is an extension of the Shi Malik algorithm, which can handle two or more clusters. It can be found in [7]. In the face of objects and a similarity matrix, the goal of the Dynamic Spectral Cluster is to find a cluster number of \u20ac~. Algorithm 2 Dynamic Spectral Clusters 1. Calculate the laplaker of the similarity matrix. 2. Calculate the eigenvalues and eigenvectors of the laplaker 3. Allow a desired number of clusters. 4. Find eigenvectors of the corresponding eigenvalues found on the previous step. Let the corresponding matrix.5. Rotate them by multiplying them with a suitable rotation matrix so that each of the corresponding rows (ideally) has only one nonzero entry. In reality, we find the resulting matrix, which we call the largest (in absolute value) matrix."}, {"heading": "3.2.1. A dynamic clustering algorithm", "text": "Given a family of time-dependent similarity functions that define a family of similarity matrices, an optimal time-varying cluster structure can be estimated by applying algorithm 2 in due course using an input similarity matrix. Therefore, we propose the following algorithm for time series data: Algorithm 3 Let time series be. Where. Allow a window parameter to be a distance function and be a similarity function. 1. Let the distance matrix be present for each pair. 2. Let the similarity matrix be present for each pair. 3. Let \u20ac~ mbe the cluster formation resulting from executing algorithm 2 with input similarity matrix. 4. Edition, which includes \u20ac~ m. Extensions to this approach include consideration of a geometric decay factor in distance calculation, alternative distance functions and various similarity functions. We have tried different combinations, but found no significant cluster stability improvement."}, {"heading": "3.3. Overview", "text": "We present the performance of this algorithm in Figure 1. Some of the observed characteristics of this method are the following: \u2022 The resulting cluster values are particularly sensitive to the similarity function used in the model. \u2022 The cluster structure estimated using this method tends to be relatively unstable over time. Although this may be plausible in some applications, in the context of financial time series, we assume that clusters typically arise from common factors related to economic fundamentals (e.g. similar commodities, currency pairs of close trading partners, etc.) that would change relatively slowly relative to the frequency of market data."}, {"heading": "4. GRAPHICAL MODEL APPROACH", "text": "Instead of presenting clustering as a binary matrix, as the authors of [8] do, we approach the problem in a different way. Consider a symmetrical () family of Bernoulli random variables, so that: \u20acCi, j = 1 if i, j are in the same cluster, or 0 others.We want to learn a distribution over the whole ensemble. The model we will use in this essay is this: where is a similarity matrix; in other words, we consider that the observed similarity between a pair of points will come from one of two distributions, depending on whether the two points belong to the same cluster or not. In the following, it will be useful to consider the matrix as an adjacency matrix."}, {"heading": "4.1. Exponential model", "text": "As a starting point, we propose the following model for the ensemble, in which we impose conditional assumptions of independence between observed similarities. Therefore, we assume the following factorization: In this model, we assume and. This corresponds to the assumption of complete pairwise independence of the variables and the conditions. For implementation purposes, we assume that the variables are exponentially distributed and are Bernoulli random variables. 4.1.1. Training \u20ac{~ k} can be translated into a training sequence by transforming ensemble values if \u20aci ~ k. Based on the assumptions of independence underlying this model, the ML estimate for the posterior distribution of the ensemble can be expressed by determining the ML estimate for each of the distributions and. The ML estimate for the rate parameter corresponds to the reversal of the sample mean, and the ML estimate for the mean of the sample frequency of the serial."}, {"heading": "4.1.2. Prediction", "text": "The prediction according to this model is made by determining the MAP assignment for the ensemble and converting it into a cluster formation. The prediction is made by maximizing each probability independently of each other: For the ensemble assignment, we output a cluster assignment consisting of a cluster for each connected component of the graph. The results are shown in Figure 2.The prediction algorithm is linear."}, {"heading": "4.1.3. Limitations", "text": "Consider the following common posterior distribution across clusters: \u20acp (\u22c5) = 0.1 if \u22c5 = (1,2,3) 0.41 if \u22c5 = (1,2), (3) 0.41 if \u22c5 = (1,3), (2) 0 if \u22c5 = (2,3), (1) 0.17 if \u22c5 = (1), (2), (3), (3), (2), (1), (2), (2), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (0.5."}, {"heading": "4.2. Triangular Potentials", "text": "The main limitation of the approach described in the previous section is that the potential exists for false large clusters to arise solely from the independent optimization of the potentials. If the marginal probability is high, it is likely that the MAP of the ensemble will occur regardless of the values of the other similarities or cluster assignments. It is also possible that the algorithm proposes cluster shapes that are intuitively implausible (and do not agree with previous notions of cluster structure that might be suitable for a particular domain); we illustrate this in Figure 3. We will therefore proceed to address these problems by modifying the basic model as described by the following observations: Observation 2 is a valid cluster distribution \u20ac~, if for all triplets of different numbers. Observation 3 is a valid cluster formation, if the graph whose adjacency matrix is composed of a dissimilar association of cliques. In this section we proceed from the following factorization: Observation 2 is a valid cluster distribution \u20ac~, if for all triplets of different numbers. Observation 3 is a valid cluster formation, if the graph is composed of which adjacency matrix is composed of cliques."}, {"heading": "4.2.1. Training algorithm", "text": "We use the same construction for the univariate and bivariate potentials as in the previous section. Distribution via clusters will vary, as the triangular potentials limit the mass of distribution to the space of valid clusters. Of course, it is also possible to add other potentials that affect different groups of cluster variables, although we leave this direction to future research."}, {"heading": "4.2.2. Prediction algorithms", "text": "This model can be thought of as an undirected graphical model with variables for and against edges, for all. If the variable is identified with the point, then there is an edge between all two variables on the same vertical line and between all two variables on the same horizontal line. We solve the problem of obtaining the MAP mapping via clustering in this model either with the elimination algorithm or with the MCMC. To get an estimate for the MAP mapping using MCMC, we take the posterior and output the most common cluster arrangement. MCMC chain construction is described in the next paragraph. By constructing it, there is a magnitude clique along the horizontal line, because as a result, the elimination algorithm has an exponential runtime over this graphical model. Likewise, there are no simple theoretical guarantees for the performance of the MCMCMC method. Specifically, it is possible that the mass proposed for the optimum mapping is not the optimal mold reference."}, {"heading": "4.3. Results and Limitations", "text": "Next, we use the classical sumproduct algorithm or the MAP elimination algorithm to find the best cluster formation, with results shown in Figure 4, but the disadvantages are that this solution becomes insoluble as the number of products increases, and the elimination algorithm could at worst become insoluble very quickly."}, {"heading": "4.3.1. Theoretical limitations", "text": "It is as if it is a reactionary project, able to retaliate, to retaliate and to retaliate."}, {"heading": "5. EXTENSIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. HMM", "text": "We propose a generalization of the previous models by means of an HMM. In this model, each hidden state is a cluster and the transition probabilities result from the sampled frequencies of the transitions in the training phase. If the hidden states of the training data are known, the ML estimate of the transition probabilities of an HMM corresponds to the transition frequencies. The results of the application of this method are shown in Figure 5, where it is obvious that relatively good performances are achieved. The version implemented here is hard coded only for series and therefore only possible cluster states."}, {"heading": "5.2. Coagulation Fragmentation", "text": "At each step, the chain either selects a random cluster and divides it into two, or it selects two random clusters and connects them to each other. Acceptance / rejection probabilities can be calculated with respect to each clotting process. In our implementation, we choose either a uniform random cluster and a random dichotomy of it (fragmentation) or a uniform random cluster pair (coagulation). We believe that the mixing time of this process should be fast, as it is related to a coagulation fragmentation process known as a random transposition pathway. Diaconis and Shahshahani provided a polynomial upper limit for the mixing time of this process [9]."}, {"heading": "5.2.1. Alternative model", "text": "We believe that a sensible alternative to the ideas described above is to present the cluster evolution as HMM on the basis of fragmentation and coagulation parameters: the simplest model with only two parameters, one controlling the probability of fragmentation and the other the probability of coagulation. If the number of fragmentation and coagulation parameters is low, one could draw conclusions from this."}, {"heading": "6. CONCLUSIONS", "text": "We have documented the process by which we analyzed the problem and considered a method for identifying clusters using triangular potentials, which can be computationally intensive, and we have presented some preliminary theoretical results regarding their limitations. However, despite these considerations, we have found promising empirical results from applying the method to simulated data sets and look forward to extending them to real data in due course. In future work, we aim to extend the idea to an environment where we give clustering a non-uniform priority, for example where expert knowledge suggests that a group of investments may have similar return characteristics, then we can configure potentials to create appropriate weighted linkages between these products."}], "references": [{"title": "Probabilistic Graphical Models", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Portfolio Selection", "author": ["H. Markowitz"], "venue": "Journal of Finance,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1952}, {"title": "Theory of Financial Decision Making, Rowman and Littlefield", "author": ["J.E. Ingersoll Jr."], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1987}, {"title": "Naive diversification strategies in defined contribution saving plans", "author": ["S. Benartzi", "R.H. Thaler"], "venue": "American Economic Review,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Optimal versus naive diversification: How inefficient is the portfolio strategy?", "author": ["V. De Miguel", "L. Garlappi", "R. Uppal"], "venue": "Review of Financial Studies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Dynamic Spectral Clustering", "author": ["A. LaViers", "A. Rahmani", "M. Egerstedt"], "venue": "Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems \u2013 MTNS 2010,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Evolutionary Spectral Clustering by Incorporating Temporal Smoothness", "author": ["Y. Chi", "X. Song", "D. Zhou", "K. Hino", "B.L. Tseng"], "venue": "Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Generating a Random Permutation with Random Transpositions", "author": ["P. Diaconis", "M. Shahshahani"], "venue": "Zeitschrift fu\u0308r Wahrscheinlichkeitstheorie und Verwandte Gebiete,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1981}], "referenceMentions": [{"referenceID": 0, "context": "Koller and Friedman provide a detailed discussion of utility functions and decision theory in the general machine learning context [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "One of the most prominent theoretical results is the concept of mean-variance efficiency which has its roots in the work of Markowitz [3]: the idea is that in a one period model (under certain restrictive assumptions) if investors seek to maximize return and minimise portfolio variance, the optimal ex ante weighting vector is given by", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "where is the covariance matrix of future returns, is the mean vector of expected returns, is a risk-aversion parameter and is the risk-free rate of return [4].", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "In these circumstances one strand of literature considers simpler weighting schemes which are predicated on relatively few assumptions; one prominent example, popular with practitioners, is the self-explanatory equally-weighted (or ) approach [5].", "startOffset": 243, "endOffset": 246}, {"referenceID": 4, "context": "Although this may be far from the truth it may be more innocuous to assume this than to suffer potentially negative effects of erroneous statistical forecasts and there is a body of empirical literature which demonstrates the efficiency of the approach [6].", "startOffset": 253, "endOffset": 256}, {"referenceID": 5, "context": "It can be found at [7].", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "Following [7] we set .", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "As suggested by [7], the optimal number of clusters can be obtained by choosing the value of that maximizes a scoring function given by", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "GRAPHICAL MODEL APPROACH Instead of representing clusterings as a binary matrix such that if cluster as the authors of [8] do, we approach the problem in a different way.", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "Diaconis and Shahshahani provided a polynomial upper bound for this walk\u2019s mixing time [9].", "startOffset": 87, "endOffset": 90}], "year": 2015, "abstractText": "Motivated by the problem of computing investment portfolio weightings we investigate various methods of clustering as alternatives to traditional mean-variance approaches. Such methods can have significant benefits from a practical point of view since they remove the need to invert a sample covariance matrix, which can suffer from estimation error and will almost certainly be non-stationary. The general idea is to find groups of assets which share similar return characteristics over time and treat each group as a single composite asset. We then apply inverse volatility weightings to these new composite assets. In the course of our investigation we devise a method of clustering based on triangular potentials and we present associated theoretical results as well as various examples based on synthetic data.", "creator": "Word"}}}