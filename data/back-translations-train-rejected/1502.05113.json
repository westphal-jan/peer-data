{"id": "1502.05113", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2015", "title": "Temporal Embedding in Convolutional Neural Networks for Robust Learning of Abstract Snippets", "abstract": "The prediction of periodical time-series remains challenging due to various types of data distortions and misalignments. Here, we propose a novel model called Temporal embedding-enhanced convolutional neural Network (TeNet) to learn repeatedly-occurring-yet-hidden structural elements in periodical time-series, called abstract snippets, for predicting future changes. Our model uses convolutional neural networks and embeds a time-series with its potential neighbors in the temporal domain for aligning it to the dominant patterns in the dataset. The model is robust to distortions and misalignments in the temporal domain and demonstrates strong prediction power for periodical time-series.", "histories": [["v1", "Wed, 18 Feb 2015 04:25:23 GMT  (395kb,D)", "http://arxiv.org/abs/1502.05113v1", "a submission to kdd 15'"]], "COMMENTS": "a submission to kdd 15'", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jiajun liu", "kun zhao", "brano kusy", "ji-rong wen", "raja jurdak"], "accepted": false, "id": "1502.05113"}, "pdf": {"name": "1502.05113.pdf", "metadata": {"source": "CRF", "title": "Temporal Embedding in Convolutional Neural Networks for Robust Learning of Abstract Snippets", "authors": ["\u2020Jiajun Liu", "\u2020Kun Zhao", "\u2020Brano Kusy", "\u2217Ji-rong Wen"], "emails": ["raja.jurdak}@csiro.au", "jrwen@ruc.edu.cn"], "sections": [{"heading": null, "text": "We conduct extensive experiments and find that the proposed model has significant and consistent advantages over existing methods in terms of a variety of data modalities ranging from human mobility to household power consumption records. Empirical results suggest that the model is robust against various factors such as sample count, data variance, numerical data ranges, etc. Experiments also confirm that the intuition behind the model can be generalized to multiple data types and applications, promising a significant improvement in predictive outcomes across the data sets studied."}, {"heading": "1. INTRODUCTION", "text": "The behaviour of many people in the world is fundamentally tied to the cycle of the sun and moon, which creates day and night."}, {"heading": "3. THE MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Intuition", "text": "This year, the time has come for us to be able to find a new home in a country where most people are able to find a new home, a new home, where they are able to find a new home, where they are able to find a new home, where they are able to find a new home, where they are able to find a new home, where they are able to find a new home, where they are able to find a new home, where they are able to find a new home, where they are able to find a new home, where they are able to find a new home, where they are looking for a new home, to find a new home."}, {"heading": "3.2 Model Overview", "text": "We propose a revolutionary neural network to learn the snippets from the periodic time series as shown in Figure 1. The model has three invisible layers, namely the temporal embedding layer, the convolution / max pooling layer, and the sigmoid layer. The output layer is a l1-regulated regression layer with the least squares. The illustrated model is an example instance of the proposed model with the input size, the embedding window size, the snippet size, the maximum pool size, and the sigmoid layer, which is 6, 1, 2, (1.3), and 3. The model implements the following workflow: It takes an input sample and applies the time embedding. This layer transforms the sample into a more dense representation with the potential temporal neighbors."}, {"heading": "3.3 Temporal Embedding", "text": "rE \"s rf\u00fc land rf\u00fc rf\u00fc rf\u00fc rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the r"}, {"heading": "3.4 Convolution, Max-pooling and Sigmoid", "text": "The convolution / pooling layer is only associated with three neurons from the connecting layer."}, {"heading": "3.6 Backpropagation", "text": "The parameters in the network are updated through stochastic gradient descent. Specifically W (4) can be learned through (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development)) (development) (development) (development) (development) (development) (development)) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) ((development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) (development) ("}, {"heading": "4. EXPERIMENTS", "text": "In the experiments, we perform extensive tests on the proposed model, with 15 individual data sets and 4 competitive methods. The objectives of the experimental studies are quadruple: 1) the evaluation of the predicted performance of the proposed model in terms of its predictive accuracy and its comparison with competitive models; 2) the evaluation of the behavior and sensitivity of the model to characteristics of different data sets; 3) the investigation of the isolated effects of time embedding; and 4) the visualization of the snippets and the presentation of how they work with intermediate values from the learning process."}, {"heading": "4.1 Datasets", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.2 Evaluation Settings", "text": "For evaluation, we consider the periodic accumulation prediction problem, where each input x \"Rd\" (d \"< d) is a header segment of a complete x and corresponds to a target y = \u2211 d i = 0 xi representing the periodic accumulation. Clearly, the model can be used to perform other types of prediction, such as time series forecasting or k-ahead forecasting. Due to the space limit here, we use periodic accumulation forecasting as a showcase for TeNet's performance advances. TeNet is implemented with the Theano Framework4. For comparison, we consider four competing methods, namely supporting vector regression with linear kernel (SVLN), supporting vector regression with radial base kernel (SVSIG), supporting vector regression with polynomial kernel (SVPOLY) and multiple kernel regression (MKR)."}, {"heading": "4.3 Prediction for Periodical Accumulation", "text": "In fact, it is that most people are able to determine themselves what they want, and that they are able to determine themselves. (...) It is not as if they were able to determine themselves. (...) \"It is not as if.\" (...) \"It is not as if.\" (...) \"It is not as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is as if.\" (...) \"It is.\" (...). \"(...\" It is. \"(...).\" (...). \"(...).\" (... \"(...).\" (...). \"(It is.\" (...). \"(...).\" (It is. \"(...).\" (...). \"(It is.\" (...). \"(...\" (...). \"It is.\" (... \"(...).\" (... \"It is.\" (...). \"(...\" It is. \"(...).\" (... \"It is.\" (...). \"(...\" (...). \"It is.\" (... \"(...).\" (... \"(...).\" It is. \"(...\" (...). \"(...\" It is. \"(...\" (...). \"(...\" It is. \"(...).\" (). \"(...\" It is. \"().\" (... \"It is.\" (). (). (it is. (). \"(it is. (). (It is. (). (...\" is. (). (it. (). (It is. (it. (...). (It is. (). (It is. (). (It is. (). (). (it. (It is. (). (It is. (). (It is. (). (It is. (). (It is. (). (It is. (It is. (). (. (). (it. (it.). (it. (it.).). (it. ("}, {"heading": "4.4 The Effect of d", "text": "Figure 4 illustrates the influence of attribute dimensionality d on predictive accuracy. Here, we use HPC-AU-8 as a case study. Figure 4 (a) shows the changes in MRE and normalized MSE on a growing scale. Unsurprisingly, both errors decrease monotonously when d increases, from 1, 0.35 at d = 8 to 0.08, 0.07 at d = 44. Figure 4 (b) shows how the HR responds to a growing one. Again, we see monotonous growths (almost, except d = 16) at HR @ 20% and HR @ 30%. These results confirm that TeNet can use the additional information effectively and in the meantime has received little effect from noise in the additional dimensions."}, {"heading": "4.5 The Effect of Temporal Embedding", "text": "In Section 3.3, we discussed how the hypothetical time embedding would increase the performance of the model by automatically adapting the distorted time series to the prevailing patterns in a dataset, and verifying them with a case study using a synthetic example. To further confirm this hypothesis on real data, we create a designated dataset from HPC-AU-8 by performing the following procedure: 1. We perform a clustering using the affinity propagation method in [10] and find the top 10 examples. 2. We take the examples and generate 300 synthetic samples (30 for each example) by distorting the examples using randomly selected operations, such as swapping two adjacent segments or shifting the data forward and backward. They are equally in training, validation and testset.3. We train a model using a modified classical neural network prior to regression (CNN, Prediction \u2192 Conlineation / Resolation \u2192 1), with no half of the regression."}, {"heading": "4.6 Discussion", "text": "We present a visualization of the random snippets and learned snippets for the first cross-validation iteration on HPC-AU-8 in Figure 5. Each cell is a snippet, a segment of the time series that the model deems representative. The numbers show some remarkable characteristics. First, the random snippets are quite dense, while scholars are much more sparse, which means that in most cases there is only a smaller number of spikes and valleys in each snippet learned. Second, the thrift of the learned snippets is also accompanied by a visually identifiable high distinctiveness over the acquired snippets, which means that snippets have learned to differ from each other because they effectively capture different patterns in the training data. Both characteristics indicate that the snippets really learn from the patterns in the datasets and both properties we have a positive effect on the selection of the hyper 6.hereby 6.6."}, {"heading": "5. CONCLUSION", "text": "Motivated by the observation that regularities in periodic time series sometimes manifest at different times and at different speeds, in this paper we propose a technical time embedding and develop a revolutionary neural network-based learning model called TeNet that is resilient to time distortions and misalignments in order to learn abstract features. First, we present TeNet and discuss the underlying intuition based on a case study, and then we describe the technical details for the entire network architecture and solve the back propagation problem for the proposed model. In the experiments, we use an extensive range of real periodic data covering three modalities to compare the performance of the proposed model with competing methods. We find that TeNet achieves an average advantage of 8% to 33% over other methods in differential metrics, and the advantage scales with an increasing sample size used in training."}, {"heading": "6. REFERENCES", "text": "[1] O. Abdel-Hamid, A. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu. Convolutional neural networks for speech recognition. IEEE / ACM Transactions on Audio, Speech & Language Processing, 22 (10): H. H. Williams networks, and D. Yu. Convolutional neural networks for AI. Foundations and Trends in Machine Learning, 2 (1): 1-127, 2009. [3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. NIPS, 19: 153, 2007. Y. Chon, E. Talipov, H. Shin, and H. Cha. CRAWDAD data set yonsei / lifemap (v. 2012-01-03). Downloaded from http: / crawdad.org / lifemap, Jan."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech & Language Processing, 22(10):1533\u20131545,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, 2(1):1\u2013127,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS, 19:153,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "CRAWDAD data set yonsei/lifemap (v", "author": ["Y. Chon", "E. Talipov", "H. Shin", "H. Cha"], "venue": "2012-01-03). Downloaded from http://crawdad.org/yonsei/lifemap/, Jan.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "IJCAI, pages 1237\u20131242,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML, pages 160\u2013167. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493\u20132537,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Recent advances in deep learning for speech research at microsoft", "author": ["L. Deng", "J. Li", "J. Huang", "K. Yao", "D. Yu", "F. Seide", "M.L. Seltzer", "G. Zweig", "X. He", "J. Williams", "Y. Gong", "A. Acero"], "venue": "ICASSP, pages 8604\u20138608,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Training restricted boltzmann machines: An introduction", "author": ["A. Fischer", "C. Igel"], "venue": "Pattern Recognition, 47(1):25\u201339,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, 315(5814):972\u2013976, February", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Understanding individual human mobility patterns", "author": ["M.C. Gonzalez", "C.A. Hidalgo", "A.-L. Barabasi"], "venue": "Nature, 453(7196):779\u2013782,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A.Y. Hannun", "C. Case", "J. Casper", "B.C. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G.E. Hinton"], "venue": "G. Montavon, G. B. Orr, and K. M\u00fcller, editors, Neural Networks: Tricks of the Trade - Second Edition, volume 7700 of Lecture Notes in Computer Science, pages 599\u2013619. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Camazotz: Multimodal activity-based gps sampling", "author": ["R. Jurdak", "P. Sommer", "B. Kusy", "N. Kottege", "C. Crossman", "A. Mckeown", "D. Westcott"], "venue": "IPSN,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "Journal of Machine Learning Research, 10:1\u201340,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, Nov", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "NPIS, pages 873\u2013880,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "ICML, pages 609\u2013616,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "NIPS, pages 1096\u20131104,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Subject independent facial expression recognition with robust face detection using a convolutional neural network", "author": ["M. Matsugu", "K. Mori", "Y. Mitari", "Y. Kaneda"], "venue": "Neural Networks, 16(5-6):555\u2013559,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature selection, l 1 vs", "author": ["A.Y. Ng"], "venue": "l 2 regularization, and rotational invariance. In ICML, page 78,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, pages 689\u2013696,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["T. Rakthanmanon", "B.J.L. Campana", "A. Mueen", "G.E.A.P.A. Batista", "M.B. Westover", "Q. Zhu", "J. Zakaria", "E.J. Keogh"], "venue": "SIGKDD, pages 262\u2013270,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Online multiple kernel regression", "author": ["D. Sahoo", "S.C.H. Hoi", "B. Li"], "venue": "SIGKDD, pages 293\u2013302,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Least squares optimization with l1-norm regularization", "author": ["M. Schmidt"], "venue": "CS542B Project Report,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Unravelling daily human mobility motifs", "author": ["C.M. Schneider", "V. Belik", "T. Couronn\u00e9", "Z. Smoreda", "M.C. Gonz\u00e1lez"], "venue": "Journal of The Royal Society Interface, 10(84):20130246,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Daily travel behavior: Lessons from a week-long survey for the extraction of human mobility motifs related information", "author": ["C.M. Schneider", "C. Rudloff", "D. Bauer", "M.C. Gonz\u00e1lez"], "venue": "ACM SIGKDD International Workshop on Urban Computing, page 3,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty", "author": ["Y. Tsuruoka", "J. Tsujii", "S. Ananiadou"], "venue": "ACL 2009, pages 477\u2013485,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P. Manzagol"], "venue": "ICML, pages 1096\u20131103,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "Journal of Machine Learning Research, 11:3371\u20133408,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Quantifying long-term scientific impact", "author": ["D. Wang", "C. Song", "A.-L. Barabasi"], "venue": "Science, 342(6154):127\u2013132, October", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Response to comment on \u201dquantifying long-term scientific impact", "author": ["D. Wang", "C. Song", "H.-W. Shen", "A.-L. Barabasi"], "venue": "Science, 345(6193), July", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Accuracy of iphone locations: A comparison of assisted gps, wifi and cellular positioning", "author": ["P.A. Zandbergen"], "venue": "Transactions in GIS, 13(s1):5\u201325,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal l\u00e9vy-flight foraging in a finite landscape", "author": ["K. Zhao", "R. Jurdak", "J. Liu", "D. Westcott", "B. Kusy", "H. Parry", "P. Sommer", "A. McKeown"], "venue": "Journal of The Royal Society Interface, 12(104),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "It is the reason why across the days of an average person, there often exist periodical patterns for their mobility or more generally, their behavior [26, 27].", "startOffset": 150, "endOffset": 158}, {"referenceID": 26, "context": "It is the reason why across the days of an average person, there often exist periodical patterns for their mobility or more generally, their behavior [26, 27].", "startOffset": 150, "endOffset": 158}, {"referenceID": 13, "context": "While in the scenario of smart location tracking [14, 34], with a replenish-able energy budget the system either aims to minimize the energy efficiency of location tracking, or attempts to maximize the tracking accuracy given a fixed energy budget.", "startOffset": 49, "endOffset": 57}, {"referenceID": 33, "context": "While in the scenario of smart location tracking [14, 34], with a replenish-able energy budget the system either aims to minimize the energy efficiency of location tracking, or attempts to maximize the tracking accuracy given a fixed energy budget.", "startOffset": 49, "endOffset": 57}, {"referenceID": 2, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 1, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 14, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 17, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 8, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 18, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 272, "endOffset": 280}, {"referenceID": 21, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 272, "endOffset": 280}, {"referenceID": 12, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 16, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 12, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 28, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 284, "endOffset": 295}, {"referenceID": 2, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 284, "endOffset": 295}, {"referenceID": 29, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 284, "endOffset": 295}, {"referenceID": 17, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 411, "endOffset": 418}, {"referenceID": 4, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 411, "endOffset": 418}, {"referenceID": 19, "context": "Among the variations of neural networks, inspired by biological processes [20], convolutional networks in particular excel in finding such abstract features that are robust to geometric variations in images [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Among the variations of neural networks, inspired by biological processes [20], convolutional networks in particular excel in finding such abstract features that are robust to geometric variations in images [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 133, "endOffset": 143}, {"referenceID": 7, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 133, "endOffset": 143}, {"referenceID": 11, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 133, "endOffset": 143}, {"referenceID": 5, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 176, "endOffset": 182}, {"referenceID": 6, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 176, "endOffset": 182}, {"referenceID": 22, "context": "While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or \u201cmotif\u201d discovery [23, 26, 27].", "startOffset": 223, "endOffset": 235}, {"referenceID": 25, "context": "While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or \u201cmotif\u201d discovery [23, 26, 27].", "startOffset": 223, "endOffset": 235}, {"referenceID": 26, "context": "While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or \u201cmotif\u201d discovery [23, 26, 27].", "startOffset": 223, "endOffset": 235}, {"referenceID": 20, "context": "the high-level neuron\u2019s responses to the sample, as high-level pattern recognizers responses to the signal, a sparse solution will utilize the most significant responses and hence will be less sensitive to noise [21, 25].", "startOffset": 212, "endOffset": 220}, {"referenceID": 24, "context": "the high-level neuron\u2019s responses to the sample, as high-level pattern recognizers responses to the signal, a sparse solution will utilize the most significant responses and hence will be less sensitive to noise [21, 25].", "startOffset": 212, "endOffset": 220}, {"referenceID": 20, "context": "The advantage of using the l1 regularizer over l2 is that the l1 regularizer forces the optimization to find a sparse solution that only uses the most distinctive high-level features to conjure the final prediction [21, 25].", "startOffset": 215, "endOffset": 223}, {"referenceID": 24, "context": "The advantage of using the l1 regularizer over l2 is that the l1 regularizer forces the optimization to find a sparse solution that only uses the most distinctive high-level features to conjure the final prediction [21, 25].", "startOffset": 215, "endOffset": 223}, {"referenceID": 27, "context": "One can speed up this optimization process using the methods proposed in [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "With the convolution layer\u2019s back propagated error being \u03b4 (which can be calculated by the method described in [16]), \u2202J(W,b;x,y) \u2202W (1) can therefore be updated with the gradient:", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "Both modalities are extracted from the LifeMap [4]) that contains human mobility traces collected from eight individuals, spanning from a few months to around two years.", "startOffset": 47, "endOffset": 50}, {"referenceID": 32, "context": "network positioning are reported to be 8, 74 and 600 m [33] 2https://archive.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "For comparison, we consider four competitive methods, namely Support Vector regression with Linear kernel (SVLN), Support Vector regression with Radial Basis kernel (SVSIG), Support Vector regression with Polynomial kernel (SVPOLY), and Multiple Kernel Regression (MKR) [24].", "startOffset": 270, "endOffset": 274}, {"referenceID": 10, "context": "Using four metrics is due to that for datasets with long-tailed values (which human behaviors can often be characterized to be [11]), as an absolute measurement, MSE alone is not an ideal metric to evaluate a regression method\u2019s performance because it is heavily biased by samples in the long tail [31, 32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 30, "context": "Using four metrics is due to that for datasets with long-tailed values (which human behaviors can often be characterized to be [11]), as an absolute measurement, MSE alone is not an ideal metric to evaluate a regression method\u2019s performance because it is heavily biased by samples in the long tail [31, 32].", "startOffset": 298, "endOffset": 306}, {"referenceID": 31, "context": "Using four metrics is due to that for datasets with long-tailed values (which human behaviors can often be characterized to be [11]), as an absolute measurement, MSE alone is not an ideal metric to evaluate a regression method\u2019s performance because it is heavily biased by samples in the long tail [31, 32].", "startOffset": 298, "endOffset": 306}, {"referenceID": 9, "context": "We run a clustering with the affinity propagation method in [10], and find the top 10 exemplars.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "As an issue often posed to complex learning models including neural networks, how to select the hyperparameters is an open question studied by many [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "One can also use the greedy hyperparameter selection processed described in [15].", "startOffset": 76, "endOffset": 80}], "year": 2015, "abstractText": "The prediction of periodical time-series remains challenging due to various types of data distortions and misalignments. Here, we propose a novel model called Temporal embeddingenhanced convolutional neural Network (TeNet) to learn repeatedly-occurring-yet-hidden structural elements in periodical time-series, called abstract snippets, for predicting future changes. Our model uses convolutional neural networks and embeds a time-series with its potential neighbors in the temporal domain for aligning it to the dominant patterns in the dataset. The model is robust to distortions and misalignments in the temporal domain and demonstrates strong prediction power for periodical time-series. We conduct extensive experiments and discover that the proposed model shows significant and consistent advantages over existing methods on a variety of data modalities ranging from human mobility to household power consumption records. Empirical results indicate that the model is robust to various factors such as number of samples, variance of data, numerical ranges of data etc. The experiments also verify that the intuition behind the model can be generalized to multiple data types and applications and promises significant improvement in prediction performances across the datasets studied.", "creator": "LaTeX with hyperref package"}}}