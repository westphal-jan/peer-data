{"id": "1502.05375", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2015", "title": "On learning k-parities with and without noise", "abstract": "We first consider the problem of learning $k$-parities in the on-line mistake-bound model: given a hidden vector $x \\in \\{0,1\\}^n$ with $|x|=k$ and a sequence of \"questions\" $a_1, a_2, ...\\in \\{0,1\\}^n$, where the algorithm must reply to each question with $&lt; a_i, x&gt; \\pmod 2$, what is the best tradeoff between the number of mistakes made by the algorithm and its time complexity? We improve the previous best result of Buhrman et al. by an $\\exp(k)$ factor in the time complexity.", "histories": [["v1", "Wed, 18 Feb 2015 20:36:19 GMT  (16kb)", "http://arxiv.org/abs/1502.05375v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["arnab bhattacharyya", "ameet gadekar", "ninad rajgopal"], "accepted": false, "id": "1502.05375"}, "pdf": {"name": "1502.05375.pdf", "metadata": {"source": "CRF", "title": "On learning k-parities with and without noise", "authors": ["Arnab Bhattacharyya", "Ameet Gadekar", "Ninad Rajgopal"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 2.05 375v 1 [cs.D S] 1 8Fe bWe first consider the problem of learning k-parities in the online error-bound model: If the algorithm must answer each question with < ai, x > (mod 2), what is the best compromise between the number of errors made by the algorithm and its time complexity? We improve the previous best result of Buhrman et. al. [BGM10] by an Exp (k) factor in the time complexity. Second, we consider the problem of learning k-parities in the presence of classification noise levels (0, 1 / 2)."}, {"heading": "1 Introduction", "text": "The question is simple enough, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve, twelve,"}, {"heading": "1.1 Our Results", "text": "s first study the noiseless setting. Our most important technical result is an improved compromise between the sample complexity and the runtime for learning parities.Theorem 1: (1) We assume that there is an algorithm that executes the concept class of k parities on n variables with confidence parameters with confidence parameters. (1 / 4.01) There is an algorithm that executes the concept class of k parameters with confidence parameters. In fact, we prove our result in the error-bound model [Lit89], which is stronger than the PAC model discussed above (in fact, strictly stronger assuming the existence of disposable functions [Blu94]."}, {"heading": "1.2 Our Techniques", "text": "We use the same approach as [BGM10] (which was itself inspired by [APY09]). The idea is to look at a family S of subsets of {0, 1} n in such a way that the hidden k-sparse vector is contained in one of the elements of S. We maintain this invariant throughout the algorithm. Each time an example comes along, it specifies a half-space H of {0, 1} n within which the hidden vector is located. So, we can update S by taking the section of each of its elements with H. If we can ensure that the set of points covered by the elements of S decreases by a constant factor every turn, then, according to O (log S | S |) n examples, that the hidden vector is learned."}, {"heading": "2 Preliminaries", "text": "Let PAR (k) be the class of all f-PAR (k). The error of hamming functions is a fixed number of malfunctions. So, | PAR (k) | = (n k). With each vector f-PAR (k) we associate a parity function f: {0, 1} n \u2192 {0, 1} defined by f (a) = n \u00b2 n (mod 2).Let C be a concept class of Boolean functions on n variables, such as PAR (k). We discuss two models of learning in this thesis. One is Littstone's online error-bound model [Lit89]. Here, learning proceeds in a series of rounds in which the learner reveals an unlabeled Boolean sample example of a 0, 1} n and must predict the value f (a) of an unknown target function f (a). Once the learner predicts the value of f (a), the true value of f (a) is revealed to the teacher."}, {"heading": "3 In the absence of noise", "text": "Theorem 3.1. Let k, t: N \u2192 N two functions be such that log log log log log log \"k (n) k (n) t (n) n. Then there is an algorithm that learns PAR (k) in the error-bound model, with errors at most bound (1 + o (1) knt + log (t k) and runtime per round e \u2212 k / 4.01 \u00b7 (t k) \u00b7 O (kn / t) 2). With theorem 1.3 we get directly theorem 1.1. Since theorem 1.3 generates a PAClearner about each distribution, a statement in the form of theorem 1.1 applies to examples from each distribution. For comparison, we cite the relevant result of [BGM10] in the error-bound model theorem 3.2 (theorem 2.1 of [BGM10])."}, {"heading": "3.1 The Algorithm", "text": "Let f = 0, 1) n be the hidden vector of parsimony k that the learning algorithm tries to learn. (Let e = 1, e2, \u00b7 \u00b7, en) the set of the standard base of vector space [0, 1} n. Let \u03b1 be a large constant that we set later, and let T = \u03b1t. (Note: T = 1,) Note: We define an arbitrary subset of [T], each of the size of vector space [0, C2, \u00b7 \u00b7, CT on the sentence e in T parts, each of size to no more than n / T. Next, let S1,.) Sm [T] is a random subset of [T], each of size. We choose m to ensure the following: m = O parts (T\u0430k) (T \u2212 k\u0430k)) (T \u2212 k\u0430k \u2212 k)) (T \u2212 k \u2212 k), with the ultimate probability."}, {"heading": "3.2 Analysis", "text": "Before we analyze the algorithm, we first make a combinatorial assertion that is the crux of our improvement: \"We assume that there is a constant.\" (\"We assume that the constant is a constant.\") (\"We assume that the constant is a constant.\") (\"We assume that the constant is a constant.\") (\"We assume that the constant is a constant.\") (\"We assume that the constant is a constant.\") (\"We assume that the constant is a constant.\") (\"We assume that the constant is a constant.\") (\"We assume that the constant is a constant.\" (\") (\" We assume that the constant is a constant. \") (\" We assume that the constant is a constant. \"(\" We assume that the constant is a constant. \") (\" We assume that the constant is a constant. \") (\" We assume that the constant is a constant is. \"(\" We assume. \") (\" We assume. \"We.\" We. \"We.\" We assume.. \"We........\" We assume..... that. that... that... \"(\" We assume... \"that...... we. we. we assume... we.\" (\"that..... we... we. we. we assume.. we. we.... we. we.\""}, {"heading": "4 In the presence of noise", "text": "Let us remember the k-LPN problem. In this section we show a reduction from k-LPN to silent learning of PAR (k) and its applications."}, {"heading": "4.1 The Reduction", "text": "We focus on the case when the noise rate is limited by a constant of less than half. Theorem 1.3 (recalled) Given an algorithm A that solves the k-LPN problem with the noise rate. (0, 1 / 3), using O (s / 2), Log (1 / 2), Examples and runtime exp. \"(O (H (3) \u00b7 s (3), \u00b7 Examples A.\" (1), \u00b7 Examples A. \"(1 / 2), using O (s / 2), Log.\" (1 / 2), and with reliability parameters exp. \"(O (H (3)), s.\""}, {"heading": "4.2 Applications", "text": "Immediate application of theorem 1.3 is obtained by letting A be the currently fastest known attribute-efficient algorithm for learning PAR (k), the algorithm based on gameman4 [KS06] that generates O (k logn) samples and takes O (n k / 2) time (for constant confidence parameters \u03b4). (We ignore the confidence parameter in this context for simplicity.) Consequently, 1.4 (retrieved) For each procedure there is an algorithm for k-LPN with sample complexity O (k logn) and runtime (n k / 2) 1 + O (H (1.5\u03b7)))) proof. Our next application of theorem 1.1 uses our improved PAR (k) learning algorithm with sample complexity O (k logn)."}], "references": [{"title": "Queries and concept learning", "author": ["Dana Angluin"], "venue": "Mach. Learn.,", "citeRegEx": "Angluin.,? \\Q1988\\E", "shortCiteRegEx": "Angluin.", "year": 1988}, {"title": "Deterministic approximation algorithms for the nearest codeword problem", "author": ["Noga Alon", "Rina Panigrahy", "Sergey Yekhanin"], "venue": "Algorithms and Techniques,", "citeRegEx": "Alon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2009}, {"title": "Learning parities in the mistakebound model", "author": ["Harry Buhrman", "David Gar\u0107\u0131a-Soriano", "Arie Matsliah"], "venue": "Inform. Process. Lett.,", "citeRegEx": "Buhrman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buhrman et al\\.", "year": 2010}, {"title": "Noise tolerant learning, the parity problem, and the statistical query model", "author": ["Avrim Blum", "Adam Kalai", "Hal Wasserman"], "venue": "J. ACM,", "citeRegEx": "Blum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2003}, {"title": "Separating distribution-free and mistake-bound learning models over the boolean domain", "author": ["Avrim Blum"], "venue": "SIAM J. on Comput.,", "citeRegEx": "Blum.,? \\Q1994\\E", "shortCiteRegEx": "Blum.", "year": 1994}, {"title": "On-line algorithms in machine learning", "author": ["Avrim Blum"], "venue": "In Workshop on on-line algorithms,", "citeRegEx": "Blum.,? \\Q1996\\E", "shortCiteRegEx": "Blum.", "year": 1996}, {"title": "Efficient fully homomorphic encryption from (standard) LWE", "author": ["Zvika Brakerski", "Vinod Vaikuntanathan"], "venue": "In Proc. 52nd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Brakerski and Vaikuntanathan.,? \\Q2011\\E", "shortCiteRegEx": "Brakerski and Vaikuntanathan.", "year": 2011}, {"title": "On agnostic learning of parities, monomials, and halfspaces", "author": ["Vitaly Feldman", "Parikshit Gopalan", "Subhash Khot", "Ashok Kumar Ponnuswami"], "venue": "SIAM J. on Comput.,", "citeRegEx": "Feldman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2009}, {"title": "On noise-tolerant learning of sparse parities and related problems. In Algorithmic Learning Theory, pages 413\u2013424", "author": ["Elena Grigorescu", "Lev Reyzin", "Santosh Vempala"], "venue": null, "citeRegEx": "Grigorescu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grigorescu et al\\.", "year": 2011}, {"title": "Space efficient learning algorithms", "author": ["David Haussler"], "venue": "Technical Report UCSC-CRL-88-2, University of California at Santa Cruz,", "citeRegEx": "Haussler.,? \\Q1988\\E", "shortCiteRegEx": "Haussler.", "year": 1988}, {"title": "Secure human identification protocols", "author": ["Nicholas J Hopper", "Manuel Blum"], "venue": "In Advances in cryptology\u2013ASIACRYPT", "citeRegEx": "Hopper and Blum.,? \\Q2001\\E", "shortCiteRegEx": "Hopper and Blum.", "year": 2001}, {"title": "Toward attribute efficient learning of decision lists and parities", "author": ["Adam R Klivans", "Rocco A Servedio"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Klivans and Servedio.,? \\Q2006\\E", "shortCiteRegEx": "Klivans and Servedio.", "year": 2006}, {"title": "From on-line to batch learning", "author": ["Nick Littlestone"], "venue": "In Proc. 2nd Annual ACM Workshop on Computational Learning Theory,", "citeRegEx": "Littlestone.,? \\Q1989\\E", "shortCiteRegEx": "Littlestone.", "year": 1989}, {"title": "On lattices, learning with errors, random linear codes, and cryptography", "author": ["Oded Regev"], "venue": "J. ACM,", "citeRegEx": "Regev.,? \\Q2009\\E", "shortCiteRegEx": "Regev.", "year": 2009}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Comm. Assn. Comp. Mach.,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "Finding correlations in subquadratic time, with applications to learning parities and juntas", "author": ["Gregory Valiant"], "venue": "In Proc. 53rd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Valiant.,? \\Q2012\\E", "shortCiteRegEx": "Valiant.", "year": 2012}], "referenceMentions": [], "year": 2015, "abstractText": "We first consider the problem of learning k-parities in the on-line mistake-bound model: given a hidden vector x \u2208 {0, 1} with |x| = k and a sequence of \u201cquestions\u201d a1, a2, \u00b7 \u00b7 \u00b7 \u2208 {0, 1} , where the algorithm must reply to each question with \u3008ai, x\u3009 (mod 2), what is the best tradeoff between the number of mistakes made by the algorithm and its time complexity? We improve the previous best result of Buhrman et. al. [BGM10] by an exp(k) factor in the time complexity. Second, we consider the problem of learning k-parities in the presence of classification noise of rate \u03b7 \u2208 (0, 1/2). A polynomial time algorithm for this problem (when \u03b7 > 0 and k = \u03c9(1)) is a longstanding challenge in learning theory. Grigorescu et al. [GRV11] showed an algorithm running in time ( n k/2 )1+4\u03b7+o(1) . Note that this algorithm inherently requires time ( n k/2 ) even when the noise rate \u03b7 is polynomially small. We observe that for sufficiently small noise rate, it is possible to break the ( n k/2 ) barrier. In particular, if for some function f(n) = \u03c9(1) and \u03b1 \u2208 [1/2, 1), k = n/f(n) and \u03b7 = o(f(n)/ log n), then there is an algorithm for the problem with running time poly(n) \u00b7 ( n k )1\u2212\u03b1 \u00b7 e.", "creator": "LaTeX with hyperref package"}}}