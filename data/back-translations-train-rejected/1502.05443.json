{"id": "1502.05443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2015", "title": "Influence-Optimistic Local Values for Multiagent Planning --- Extended Version", "abstract": "Recent years have seen the development of a number of methods for multiagent planning under uncertainty that scale to tens or even hundreds of agents. However, most of these methods either make restrictive assumptions on the problem domain, or provide approximate solutions without any guarantees on quality. To allow for meaningful benchmarking through measurable quality guarantees on a very general class of problems, this paper introduces a family of influence-optimistic upper bounds for factored Dec-POMDPs. Intuitively, we derive bounds on very large multiagent planning problems by subdividing them in sub-problems, and at each of these sub-problems making optimistic assumptions with respect to the influence that will be exerted by the rest of the system. We numerically compare the different upper bounds and demonstrate how, for the first time ever, we can achieve a non-trivial guarantee that the heuristic solution of problems with hundreds of agents is close to optimal. Furthermore, we provide evidence that the upper bounds may improve the effectiveness of heuristic influence search, and discuss further potential applications to multiagent planning.", "histories": [["v1", "Wed, 18 Feb 2015 23:42:24 GMT  (275kb)", "http://arxiv.org/abs/1502.05443v1", "long version of extended abstract at AAMAS 2015"], ["v2", "Mon, 20 Jul 2015 08:58:50 GMT  (193kb,D)", "http://arxiv.org/abs/1502.05443v2", "Long version of IJCAI 2015 paper (and extended abstract at AAMAS 2015)"]], "COMMENTS": "long version of extended abstract at AAMAS 2015", "reviews": [], "SUBJECTS": "cs.AI cs.SY", "authors": ["frans a oliehoek", "matthijs t j spaan", "stefan witwicki"], "accepted": false, "id": "1502.05443"}, "pdf": {"name": "1502.05443.pdf", "metadata": {"source": "CRF", "title": "Influence-Optimistic Local Values for Multiagent Planning \u2014 Extended Version", "authors": ["Frans A. Oliehoek"], "emails": ["fao@liverpool.ac.uk", "m.t.j.spaan@tudelft.nl", "stefan.witwicki@epfl.ch"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.05 443v 1 [cs.A I] 1 8Fe b"}, {"heading": "1. INTRODUCTION", "text": "Dre eeisrVnlrsrteeoiiiiiiiiiuztlrsrrteeoiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiirrteeVnrsrteeVnrrsrteeoiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiieteeteeteeteeteerrrrrteeVnrteeoiueoiueeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeteeteeteeteeteeteeeteeteeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "2. BACKGROUND", "text": "In this paper, we focus on factorized Dec-POMDPs [26], which are Dec-POMDPs = X functions, where the transition and Obser-1In the paper, we use the word \"tight\" for its (empirical) meaning of \"close to optimum,\" not for its (theoretical CS) meaning of \"agreement with the best possible limitation.\" Term models can be compactly represented as a two-level Bayesian Dynamic Network (2DBN): definition 1. Factored Dec-POMDP is a tuple M = < D, A, O, O, R, b0 >, where: \u2022 D = {1,. n} is the group of agents. \u2022 A = i D Ai is the group of joint actions a. \u2022 O = i D Oi is the group of joint observations or"}, {"heading": "3. SUB-PROBLEMS AND INFLUENCES", "text": "The general approach we take is to divide the problem into sub-problems (defined here), to calculate overestimates of the achievable value for each of these sub-problems (discussed in Section 4) and to summarize them into a global upper limit (Section 5)."}, {"heading": "3.1 Sub-Problems (SPs)", "text": "The concept of a sub-problem generalizes the concept of a local form model (LFM) [28] to several factors and reward components. We give a relatively precise description of this formalization, for further details see [28].Definition 2. A sub-problem (SP) Mc of a factor Dec-POMDP M is a tuple Mc = < M, D, X \u2032, R \u2032 >, where D \u2032 D, X \u2032 X, R \"designates subgroups of agents, state factors, and local reward functions.An SP inherits many features from M: We can locate states xc, X, X \u2032, and the subsets D\" Xi, R \"induce local joint actions Ac = i D\" Ai, observations Oc = i D \"Oi, and reward factors in houses xc.\" The considerations Oc \"c\" (xc, ac, xc \"), l\" kai \"l\" (Ai \"), and\" c \"c\""}, {"heading": "3.2 Structural Assumptions", "text": "In the most general form, the observation and reward model could also be under-specified. To simplify the exposure, we assume two assumptions about the structure of an SP: 1. For all contained agents, the state factors that can influence its observations (i.e., ancestors of oi in the 2DBN), in Mc.2. For all contained reward components, the state factors and actions that affect Rl are in Mc.That is, we assume that SPs have generalized forms of observation independence, Oc (oc | ac, xc \u2032), Pr (oc | ac, xc \u2032) = Pr (oc | a, s \u2032), and reward independence (cf. (1). These are more general terms of observation and reward independence than in previous work on TOI-Dec-MDPs [3] and ND-POMDPs [22], since we allow assumptions that cannot be influenced by the state factors that influence each other."}, {"heading": "3.3 Influence-Augmented SPs", "text": "An LFM is transformed to a so-called influenceaugmented local model that captures the influence of policies and parts of the environment that are not modeled in the local model. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p # p # p # p # p # p # p # p # p # p # p # p # p # p # p p p p p p p p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "4. LOCAL UPPER BOUNDS", "text": "In this section, we present our most important technical contribution: the machinery for calculating a number of influential optimistic upper limits (IO-UBs) for the value of partial problems. To properly define this class of upper limits, we first define the locally optimal value: Definition 4. The locally optimal value for a SP c, V LO c, max\u03c06 = c VBR c (\u03c0 6 = c) = max\u03c06 = c V \u0445 c (I \u2192 c (\u03c0 6 = c)), (5) is the local value (taking into account only the rewards Rc) that can be achieved if all actors use a policy chosen to optimize this local value. We will call the maximization argument \u03c0LO6 = c. Note that V LOc \u2265 Vc (\u03c0) - the value for the rewards Rc under the optimal common political influence Rc - can be achieved that they turn out to be optimal, but that they are unfeasible under the general value LOc."}, {"heading": "4.1 A Q-MMDP Approach", "text": "Like all heuristics we present, he assumes that the SP under consideration will receive the most optimistic (possibly unfeasible) influences. Furthermore, he assumes that the SP is fully observable, that it is reduced to a local multiagent MDP (MMDP). In other words, this approach is like Q-MMDP [35, 25], but is limited to one SP and is influence optimistic.4 This means that IO-Q-MDP makes a further overestimate in addition to influence optimism. While this negatively affects the tightness of the upper limit, it has the advantage that the computory complexity is relatively low. Formally, we can describe IO-Q-MDP as follows."}, {"heading": "4.2 A Q-MPOMDP Approach", "text": "The IO-Q-MDP approach of the previous section introduces overestimates through both influence optimism and the assumption of full observability, where we draw the upper limit by watering down the second assumption. In particular, we propose an upper limit based on the underlying multiagent POMDP (MPOMDP). However, a multiagent POMDP [21, 1] is partially observable, but assumes that the agents can freely communicate their observations, so that the problem is reduced to a special type of centralized model in which the decision maker (representing the entire team of agents) takes joint action and receives joint observations. As a result, the optimal value for an MPOMDP is analogous to that of a POMDP: Q (bt, at) = R (bt, at) + 1Pr (ot + 1)."}, {"heading": "4.3 A Dec-POMDP Approach", "text": "The previous approaches that we never need to build to calculate an IO-UB that we refer to as an IO-Q-POMDP. (As in the previous two subsections, we will be able to calculate an IO-UB that we never need to construct.) The previous approach optimization in terms of an influence-augmented model that we never need to construct, we will be able to calculate an IO-UB that we refer to as an IO-Q-POMDP. (As in the previous two subsections, we will be able to construct it in terms of an influence-augmented model that we never need to construct.)"}, {"heading": "4.4 Complexity Analysis", "text": "Due to the maximization in (6), (11) and (12), IO rear projections are more expensive than regular (non-IO) rear projections. In particular, the complexity of each backup is multiplied by the number of instances of sources of influence resulting from the time of instantiation. Therefore, the relative overhead is the same for all methods compared to solving the SPs as regular (non-IO) MMDPs, MPOMDPs and Dec POMDPs."}, {"heading": "5. GLOBAL UPPER BOUNDS", "text": "Next, we will discuss how the methods for calculating local upper limits can be applied to calculate a global upper limit for faktored Dec-POMDPs. Basically, the idea is to apply a non-overlapping upper limit for decomposition C (i.e. a division) of the reward functions {Rl} of the original faktored Dec-POMDP in SPs c \u00b2 C and calculate an IO upper limit V \u00b2 IOc for each of the three IO-UBs proposed in Section 4. Our global, optimistic upper limit is then represented by: V \u00b2 IO upper limit in SPs c \u00b2 C. (13) We illustrate the construction of a global upper limit V \u00b2 for the 6-agent FFG in Fig. 3, which represent the original problem (top row) and two possible decompositions in SPs. The second row specifies a decomposition in two SPs, while the third row uses three SPs."}, {"heading": "6. EMPIRICAL EVALUATION", "text": "In order to test the potential impact of the proposed influential optimistic limits, we present numerical results in the context of a number of benchmark problems. In this evaluation, we focus on the (relative) values found by this heuristic, as we hope that it will trigger a number of interesting ideas for further research (e.g. the notion of \"influence,\" its relationship to the approximation of factored Dec POMDPs, and the key idea that reasonable limits could be possible for very large problems). We do not examine timing results, as the analysis in Section 4.4 indicates that relative timing results follow those of regular (non-IO) MMDP, MPOMDP, and Dec POMDP methods; see, for example, [25] for a comparison of such timing results."}, {"heading": "6.1 Comparison of Different Bounds", "text": "The limits we propose are narrow, V-Dc \u2264 V-P-M-c, similar to the regular (nonIO) Dec-POMDP, Q-MPOMDP and Q-MDP values [25]. To get an understanding of how these differences turn out in practice, compare the various upper limits described in the paper with general ones, in the numerical evaluation here we take advantage of the property that the optimistic influences are easily identified outside the line, which allows the construction of small \"optimistic Dec-POMDPs\" without sacrificing in the bound quality. For example, for a 3-house FFG margin \"SP we define a regular 3-house POMDP, where the transition chances for the first house (say Xi in Fig-2) are modified."}, {"heading": "6.2 The Effect of Influence Strength", "text": "Fig. 4 (center) shows that the IO assumption in FFG is quite strong and leads to a significant overestimation of the local value compared to the \"full problem.\" We say that FFG has a high influence power. In fact, this indicates a new dimension in the qualification of weak coupling [44], which takes into account the variance of the NLAF probabilities (depending on the change in the value of the source of influence) and their effect on the local value. As a preliminary investigation of this concept, we are designing a modification of FFG, in which the influence power can be controlled. In particular, we parameterise the probability that a fire will be completely extinguished if 2 agents visit the same house, which is set to 1 in the original problem definition. Lower values of this probability mean that it is optimistically assumed that there is another agent in a house that leads to a lower advantage and thus to a lower influence power. Fig. 4 (right) shows the experiment's lower POP value, since there is a clear relationship between the fire extinguishing agents and the POP."}, {"heading": "6.3 Bounding Heuristic Methods", "text": "In this case, the analysis is not necessarily informative for overestimating the internal margins of major problems (where other agents exist, even if they are not superheroes). As such, in addition to investigating the upper limit capacity, the analysis also provides a better understanding of such internal overestimates. We use the narrowest upper limits that we could find by examining various SP partitions with sizes from n = 2-5, and examine the guarantees that can exist for transfer planning (TP) that are able to provide solutions for large Factored Dec POMDPs."}, {"heading": "6.4 Improved Heuristic Influence Search", "text": "In addition to analyzing the solution quality of approximate methods, our limitations can also be used in optimal methods. In particular, A * -OIS [41] solves TDPOMDPs (a subclass of factor-based Dec-POMDPs) by splitting them into 1-agent PLCs, scanning the space of influences and truncating them using optimistic heuristics. However, existing A * -OIS heuristics treat the unspecified influence stages of the PLCs as fully observable. In contrast, IO-Q-MPOMDP models the partial observability of the PLCs. We now present results that suggest an additional computational advantage in treating partial observability in A * -OIS heuristics. Table 2 illustrates the differences in cropping provided by four different A * -OIS heuristics: M is the baseline MDP-based heuristics of [41], P is the abbreviation for IO-OQ-OMA and the four different styles are caused by MPODP *."}, {"heading": "7. RELATED WORK", "text": "In recent years, we have developed many scalable approaches with no guarantees that can be useful for many of these methods. [14, 46, 48, 16, 40, 37, 27, 47, 36] The upper limit mechanism we propose could be useful for many of these methods. [14, 48, 16, 40, 37, 47, 36] The use of partial problems (SPs) is conceptually similar to the use of source problems in transfer planning (TP) [27]. Differences are that in our partition-based upper limit scheme, the SPs are selected so that they do not contain overlapping source problems. Furthermore, TP does not consider optimistic influences, but implicit influences. Finally, TP is used as a way to calculate a heuristic problem for the original problem."}, {"heading": "8. CONCLUSIONS", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "9. REFERENCES", "text": "[1] C. Amato and F. A. Oliehoek. Bayesianreinforcement learning for multiagent Intelligence with state uncertainty. In AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains (MSDM), pp. 76-83, 2013. [2] R. Becker, S. Zilberstein, and V. Lesser, and C. Markov decision processes with event-driven interactions. In Proc. of the International Conference on Autonomous Agents and Multiagent Systems, pp. 302-309, 2004. [3] R. Becker, S. Zilberstein, V. Lesser, and C. Goldman. Transition-independent decentralized Markov decision processes. In Proc. of the International Conference on Autonomous Agents and Multiagent systems, pp. 302-48, 2003. [4] C. Boutilier, T. Dean, and S. Hanks. Decision-theoretic planning."}, {"heading": "A. PROOFS", "text": "Lemma 1: h \u2212 1c is a (h \u2212 t) step-to-go policy."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recent years have seen the development of a number<lb>of methods for multiagent planning under uncertainty<lb>that scale to tens or even hundreds of agents. However,<lb>most of these methods either make restrictive assump-<lb>tions on the problem domain, or provide approximate<lb>solutions without any guarantees on quality. To allow<lb>for meaningful benchmarking through measurable qual-<lb>ity guarantees on a very general class of problems, this<lb>paper introduces a family of influence-optimistic upper<lb>bounds for factored Dec-POMDPs. Intuitively, we de-<lb>rive bounds on very large multiagent planning problems<lb>by subdividing them in sub-problems, and at each of<lb>these sub-problems making optimistic assumptions with<lb>respect to the influence that will be exerted by the rest<lb>of the system. We numerically compare the different<lb>upper bounds and demonstrate how, for the first time<lb>ever, we can achieve a non-trivial guarantee that the<lb>heuristic solution of problems with hundreds of agents<lb>is close to optimal. Furthermore, we provide evidence<lb>that the upper bounds may improve the effectiveness of<lb>heuristic influence search, and discuss further potential<lb>applications to multiagent planning.", "creator": "LaTeX with hyperref package"}}}