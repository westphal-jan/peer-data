{"id": "1502.05491", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2015", "title": "Optimizing Text Quantifiers for Multivariate Loss Functions", "abstract": "We address the problem of \\emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \\emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \\emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods.", "histories": [["v1", "Thu, 19 Feb 2015 08:06:54 GMT  (237kb,D)", "https://arxiv.org/abs/1502.05491v1", "In press in ACM Transactions on Knowledge Discovery from Data, 2015"], ["v2", "Wed, 15 Apr 2015 14:45:59 GMT  (237kb,D)", "http://arxiv.org/abs/1502.05491v2", "In press in ACM Transactions on Knowledge Discovery from Data, 2015"]], "COMMENTS": "In press in ACM Transactions on Knowledge Discovery from Data, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["andrea esuli", "fabrizio sebastiani"], "accepted": false, "id": "1502.05491"}, "pdf": {"name": "1502.05491.pdf", "metadata": {"source": "CRF", "title": "Optimizing Text Quantifiers for Multivariate Loss Functions", "authors": ["ANDREA ESULI"], "emails": ["andrea.esuli@isti.cnr.it.", "fsebastiani@qf.org.qa.", "permissions@acm.org."], "sections": [{"heading": null, "text": "In fact, in recent years, the number of people who are able to climb has skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed and skyrocketed, skyrocketed and skyrocketed."}, {"heading": "1. INTRODUCTION", "text": "In recent years, it has been pointed out that in a number of fields of application relating to classification, the ultimate goal is not to determine which class (or classes) contains individual unlabelled data, but rather the prevalence (or \"relative frequency\") of each class in the unlabelled data. However, the latter task is known as quantification [Forman 2005; Forman et al.), which we are discussing here applies to any type of data that we are largely interested in text quantification, i.e. quantification if the data points are textual documents. To see the importance of text quantification, we need to examine the task of classifying textual responses returned in questions. [Esuli and Sebastiani 2010a; Gamon 2004; Giorgetti and Sebastiani 2003], and let us discuss two important such scenarios. In the first scenario, a telecommunications company asks the question \"How satisfied are you with our mobile services?\""}, {"heading": "2. PRELIMINARIES", "text": "In this paper, we will focus on quantification at the binary level. That is, given a domain of documents D and a class c, we assume the existence of an unknown target function (or down-to-earth truth): D \u2192 {\u2212 1, + 1}, which indicates which members of D belong to c; as usual, + 1 and \u2212 1 represent membership and non-membership of c, respectively. The approaches we will focus on are based on aggregate quantification, i.e., they are based on generating a classifier (c) by supervised learning from a training set Tr. We will use Te to specify the test set on which quantification effectiveness is tested. We define the prevalence (or relative frequency) of class c in a set of documents Te as a fraction of the members of Te belonging to c, i.e., asecutive quantification Te (c) is a quantification effectiveness (or relative)."}, {"heading": "2.1. Evaluation measures for quantification", "text": "The simplest such measure is the tendency to overestimate the prevalence of c, while negative prevalence indicates a tendency, it.Absolute Error (AE) - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy and in [Barranquero et al.]."}, {"heading": "2.2. Existing quantification methods", "text": "A number of methods have been proposed for quantification in the (still short) literature; below we list the most important ones that we will use as baselines in the experiments discussed in Section 4.Classify and Count (CC). An obvious method of quantification is to generate a classifier of Tr, classify and estimate the documents in Te. (6) Forman [2008] calls this the fraction of the documents in Te that are predicted to be positive, i.e., a variant of the above consists in generating a classifier (dj) = + 1} | Te (6) Forman [2008] calls this the classification and counting (CC).Probabilistic Classify and Count it (C).A variant of the above consists in generating a classifier of Tr that classifies the documents in Te, and calculating it."}, {"heading": "3. OPTIMIZING QUANTIFICATION ACCURACY", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4. EXPERIMENTS", "text": "To do this, we carried out all of the experiments we used as the basis for our SVM (KLD) method, which we received from the author (this guarantees that the baselines reach their full potential), and instead implemented PCC and PACC themselves. At the heart of the implementation of all baselines, it is a standard linear SVM with its default values; where quantities (such as fprTe and tprTe - see Equation 8) we had to cross-validate 50 times as it was done and recommended."}, {"heading": "4.1. Datasets", "text": "In fact, it is the case that most of them are in a position to enter another world, in which they are in a position to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which they, in which, in which, in which they, in which they,"}, {"heading": "4.2. Testing quantification accuracy", "text": "We have conducted our experiments by assigning quantifiers for each class to a specific interval (SUV2 classes) and testing the quantifiers separately on each of the test sets, using KLD as the yardstick. We have done this for all 99 classes \u00d7 52 weeks in RCV1-V2 and for all 88 classes \u00d7 4 years in OHSUMED-S, and for all 10 basic methods discussed in \u00a7 2.2 plus our SVM (KLD) method. 4.2.1. We will first discuss the results according to the class dimension, i.e. by evaluating the results for each RCV1-V2 class over the 52 test weeks and for each OHSUMED-S class over the 4 years. As this would leave no less than 99 RCV1-V2 classes, we will discuss the average results for all RCV1-V2 classes."}, {"heading": "4.3. Testing classification accuracy", "text": "As discussed in Section 1, quantification accuracy is related to the classifier's ability to balance false positives and false negatives, but is not related to its ability to keep its total number low, which is instead a key requirement in the standard classification. However, it is quite natural to expect that a user will trust a quantification method only to the extent that his good quantification performance results from a reasonable classification performance. In other words, it is unlikely that a user will accept a classifier with good quantification accuracy but poor classification accuracy. Therefore, we have used SVM (KLD) against the traditional classification-oriented SVM (i.e. SVMorg - see Section 3) with the aim of determining whether the former has adequate classification accuracy. Default parameters were used both for the reasons already explained in the first paragraph of Section 4. We have compared the two systems by comparing the same user classes to the training experts in the quantification unit 79."}, {"heading": "4.4. Testing efficiency", "text": "SVM (KLD) also has good characteristics in terms of sheer efficiency. In terms of training, Joachims [2005] proves that training a classifier is with SVMperf O (n2), with n the number of training examples, for each loss function that can be calculated from a contingency table (as KLD actually is). This is certainly more expensive than training a classifier by means of a standard, linear SVM (which is the core of implementing all the quantification methods of \u00a7 2.2), since this latter is well known to be O (sn) (with s the average number of non-zero features in the training objects). However, note that SVM (KLD) only requires training a classifier by means of SVMperperf, since setting up a quantifier (sn) O (sn is the average number of non-zero features in the training objects)."}, {"heading": "5. RELATED WORK", "text": "An early mention of quantification can be found in [Lewis 1995, section 7], where this task is simply called counting; however, the author does not propose a specific solution- 16All times given in this section were measured on a raw material machine equipped with an Intel Centrino Duo 2 x 2Ghz processor and 2 GB RAM.ACM transactions for discovering knowledge from data, Vol. VV, No. NN, Article AA, Date of publication: YYYY."}, {"heading": "5.1. Applications of quantification", "text": "Chan and Ng [2005; 2006] use quantification (which they call \"class prediction\") to determine the prevalence of different senses of a word in a text corpus, with the aim of improving the accuracy of literal disambiguation algorithms applied to this corpus. Forman [2008] uses quantification to determine the prevalence of various support-related questions in incoming phone calls received at customer service points. Esuli and Sebastiani [2010a] use quantification methods to estimate the prevalence of different response classes in open answers received in the context of market research surveys (they do not use the term \"quantification\" but rather speak of \"measurement of classification accuracy at the overall level\"). Hopkins and King [2010] classify blog posts with the aim of assessing the prevalence of different political candidates in the prevalence of bloggers al ez al al al al al al al al."}, {"heading": "5.2. Quantification methods", "text": "Bella et al. [2010] compare many of the methods discussed in \u00a7 2.2 and note that CC \u2022 PCC \u2022 ACC \u2022 PACC (where \"underperforms\" means \"underperforms\"). Also, Tang et al. [2010] compare experimentally several of the methods discussed in \u00a7 2.2 and note that CC \u2022 PCC \u2022 ACC \u2022 PACC \u2022 MS. They also propose a method (specific to linked data) that does not require classification of individual items, but they find that it maintains a robust classification-based quantification method such as MS. However, the experimental comparisons of [Bella et al. 2010] and [Tang et al. 2010] are both made with absolute errors, which seems to be a below-average evaluation measure for this task (see \u00a7 2.1); furthermore, the data sets they test do not show the imbalance typical for many binary text classification tasks, so it is not surprising that their results are not confirmed by our MS."}, {"heading": "5.3. Other related work", "text": "In fact, it is not the case that it is an isolated case, but a case in which it is a case in which it is a case in which it is an isolated case in which it is an isolated case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is"}, {"heading": "6. CONCLUSIONS", "text": "We have introduced SVM (KLD), a new method of performing quantification, an important (though hardly studied) task in the field of supervised learning, where the goal is to estimate class prevalence rather than classify individual items. It is very different from most of the other methods presented in the literature. While most such methods use a general purpose classifier (where the decision threshold may have been adjusted according to some heuristics) and adjust the result of the \"classify and count\" phase, we adopt a simple \"classify and count\" approach (without threshold tuning and / or post-adjustment), but generate a classifier that is directly optimized for the evaluation measure used to estimate quantification accuracy. This is not easy as an evaluation measure for quantification is by nature not linear and multivariate, and is therefore not suitable for optimizing the standard class."}, {"heading": "ACKNOWLEDGMENTS", "text": "This year, it has reached the point where it will be able to solve the problems mentioned, but not yet able to solve the problems mentioned, in order to get to grips with them."}], "references": [{"title": "Class and subclass probability re-estimation to adapt a classifier in the presence of concept drift", "author": ["Roc\u0131\u0301o Ala\u0131\u0301z-Rodr\u0131\u0301guez", "Alicia Guerrero-Curieses", "Jes\u00fas Cid-Sueiro"], "venue": "Neurocomputing 74,", "citeRegEx": "Ala\u0131\u0301z.Rodr\u0131\u0301guez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ala\u0131\u0301z.Rodr\u0131\u0301guez et al\\.", "year": 2011}, {"title": "Variable-Constraint Classification and Quantification of Radiology Reports under the ACR Index", "author": ["Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani."], "venue": "Expert Systems and Applications 40, 9 (2013), 3441\u20133449.", "citeRegEx": "Baccianella et al\\.,? 2013", "shortCiteRegEx": "Baccianella et al\\.", "year": 2013}, {"title": "Quantification-oriented learning based on reliable classifiers", "author": ["Jose Barranquero", "Jorge D\u0131\u0301ez", "Juan Jos\u00e9 del Coz"], "venue": "Pattern Recognition 48,", "citeRegEx": "Barranquero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barranquero et al\\.", "year": 2015}, {"title": "On the study of nearest neighbor algorithms for prevalence estimation in binary problems", "author": ["Jose Barranquero", "Pablo Gonz\u00e1lez", "Jorge D\u0131\u0301ez", "Juan Jos\u00e9 del Coz"], "venue": "Pattern Recognition 46,", "citeRegEx": "Barranquero et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barranquero et al\\.", "year": 2013}, {"title": "Quantification via Probability Estimators", "author": ["Antonio Bella", "C\u00e8sar Ferri", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Mar\u0131\u0301a Jos\u00e9 Ram\u0131\u0301rez-Quintana"], "venue": "In Proceedings of the 11th IEEE International Conference on Data Mining (ICDM 2010). Sydney,", "citeRegEx": "Bella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bella et al\\.", "year": 2010}, {"title": "Word Sense Disambiguation with Distribution Estimation", "author": ["Yee Seng Chan", "Hwee Tou Ng."], "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005). Edinburgh, UK, 1010\u20131015.", "citeRegEx": "Chan and Ng.,? 2005", "shortCiteRegEx": "Chan and Ng.", "year": 2005}, {"title": "Estimating Class Priors in Domain Adaptation for Word Sense Disambiguation", "author": ["Yee Seng Chan", "Hwee Tou Ng."], "venue": "Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL 2006). Sydney, AU, 89\u201396.", "citeRegEx": "Chan and Ng.,? 2006", "shortCiteRegEx": "Chan and Ng.", "year": 2006}, {"title": "Elements of information theory", "author": ["Thomas M. Cover", "Joy A. Thomas."], "venue": "John Wiley & Sons, New York, US.", "citeRegEx": "Cover and Thomas.,? 1991", "shortCiteRegEx": "Cover and Thomas.", "year": 1991}, {"title": "Information Theory and Statistics: A Tutorial", "author": ["Imre Csisz\u00e1r", "Paul C. Shields."], "venue": "Foundations and Trends in Communications and Information Theory 1, 4 (2004), 417\u2013528.", "citeRegEx": "Csisz\u00e1r and Shields.,? 2004", "shortCiteRegEx": "Csisz\u00e1r and Shields.", "year": 2004}, {"title": "Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter", "author": ["Peter Sheridan Dodds", "Kameron Decker Harris", "Isabel M. Kloumann", "Catherine A. Bliss", "Christopher M. Danforth."], "venue": "PLoS ONE 6, 12 (2011).", "citeRegEx": "Dodds et al\\.,? 2011", "shortCiteRegEx": "Dodds et al\\.", "year": 2011}, {"title": "Machines that Learn how to Code Open-Ended Survey Data", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "International Journal of Market Research 52, 6 (2010), 775\u2013800.", "citeRegEx": "Esuli and Sebastiani.,? 2010a", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2010}, {"title": "Sentiment quantification", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "IEEE Intelligent Systems 25, 4 (2010), 72\u201375.", "citeRegEx": "Esuli and Sebastiani.,? 2010b", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2010}, {"title": "Training Data Cleaning for Text Classification", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "ACM Transactions on Information Systems 31, 4 (2013).", "citeRegEx": "Esuli and Sebastiani.,? 2013", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2013}, {"title": "A response to Webb and Ting\u2019s \u2018On the application of ROC analysis to predict classification performance under varying class distributions", "author": ["Tom Fawcett", "Peter Flach."], "venue": "Machine Learning 58, 1 (2005), 33\u201338.", "citeRegEx": "Fawcett and Flach.,? 2005", "shortCiteRegEx": "Fawcett and Flach.", "year": 2005}, {"title": "Counting Positives Accurately Despite Inaccurate Classification", "author": ["George Forman."], "venue": "Proceedings of the 16th European Conference on Machine Learning (ECML 2005). Porto, PT, 564\u2013575.", "citeRegEx": "Forman.,? 2005", "shortCiteRegEx": "Forman.", "year": 2005}, {"title": "Quantifying trends accurately despite classifier error and class imbalance", "author": ["George Forman."], "venue": "Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (KDD 2006). Philadelphia, US, 157\u2013166.", "citeRegEx": "Forman.,? 2006a", "shortCiteRegEx": "Forman.", "year": 2006}, {"title": "Tackling concept drift by temporal inductive transfer", "author": ["George Forman."], "venue": "Proceedings of the 29th ACM International Conference on Research and Development in Information Retrieval (SIGIR 2006). Seattle, US, 252\u2013259.", "citeRegEx": "Forman.,? 2006b", "shortCiteRegEx": "Forman.", "year": 2006}, {"title": "Quantifying counts and costs via classification", "author": ["George Forman."], "venue": "Data Mining and Knowledge Discovery 17, 2 (2008), 164\u2013206.", "citeRegEx": "Forman.,? 2008", "shortCiteRegEx": "Forman.", "year": 2008}, {"title": "Pragmatic text mining: Minimizing human effort to quantify many issues in call logs", "author": ["George Forman", "Evan Kirshenbaum", "Jaap Suermondt."], "venue": "Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (KDD 2006). Philadelphia, US, 852\u2013861.", "citeRegEx": "Forman et al\\.,? 2006", "shortCiteRegEx": "Forman et al\\.", "year": 2006}, {"title": "Sentiment classification on customer feedback data: Noisy data, large feature vectors, and the role of linguistic analysis", "author": ["Michael Gamon."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004). Geneva, CH, 841\u2013847.", "citeRegEx": "Gamon.,? 2004", "shortCiteRegEx": "Gamon.", "year": 2004}, {"title": "Automating Survey Coding by Multiclass Text Categorization Techniques", "author": ["Daniela Giorgetti", "Fabrizio Sebastiani."], "venue": "Journal of the American Society for Information Science and Technology 54, 14 (2003), 1269\u20131277.", "citeRegEx": "Giorgetti and Sebastiani.,? 2003", "shortCiteRegEx": "Giorgetti and Sebastiani.", "year": 2003}, {"title": "Class distribution estimation based on the Hellinger distance", "author": ["V\u0131\u0301ctor Gonz\u00e1lez-Castro", "Roc\u0131\u0301o Alaiz-Rodr\u0131\u0301guez", "Enrique Alegre"], "venue": "Information Sciences", "citeRegEx": "Gonz\u00e1lez.Castro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonz\u00e1lez.Castro et al\\.", "year": 2013}, {"title": "OHSUMED: An interactive retrieval evaluation and new large text collection for research", "author": ["William Hersh", "Christopher Buckley", "T.J. Leone", "David Hickman."], "venue": "Proceedings of the 17th ACM International Conference on Research and Development in Information Retrieval (SIGIR 1994). Dublin, IE, 192\u2013201.", "citeRegEx": "Hersh et al\\.,? 1994", "shortCiteRegEx": "Hersh et al\\.", "year": 1994}, {"title": "A Method of Automated Nonparametric Content Analysis for Social Science", "author": ["Daniel J. Hopkins", "Gary King."], "venue": "American Journal of Political Science 54, 1 (2010), 229\u2013247.", "citeRegEx": "Hopkins and King.,? 2010", "shortCiteRegEx": "Hopkins and King.", "year": 2010}, {"title": "A support vector method for multivariate performance measures", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 22nd International Conference on Machine Learning (ICML 2005). Bonn, DE, 377\u2013384. DOI:http://dx.doi.org/10.1145/1102351.1102399", "citeRegEx": "Joachims.,? 2005", "shortCiteRegEx": "Joachims.", "year": 2005}, {"title": "Training Linear SVMs in Linear Time", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (KDD 2006). Philadelphia, US, 217\u2013226.", "citeRegEx": "Joachims.,? 2006", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Cutting-plane training of structural SVMs", "author": ["Thorsten Joachims", "Thomas Finley", "Chun-Nam Yu."], "venue": "Machine Learning 77, 1 (2009), 27\u201359.", "citeRegEx": "Joachims et al\\.,? 2009a", "shortCiteRegEx": "Joachims et al\\.", "year": 2009}, {"title": "Predicting Structured Objects with Support Vector Machines", "author": ["Thorsten Joachims", "Thomas Hofmann", "Yisong Yue", "Chun-Nam Yu."], "venue": "Commun. ACM 52, 11 (2009), 97\u2013104.", "citeRegEx": "Joachims et al\\.,? 2009b", "shortCiteRegEx": "Joachims et al\\.", "year": 2009}, {"title": "The Impact of Changing Populations on Classifier Performance", "author": ["Mark G. Kelly", "David J. Hand", "Niall M. Adams."], "venue": "Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 1999). San Diego, US, 367\u2013371.", "citeRegEx": "Kelly et al\\.,? 1999", "shortCiteRegEx": "Kelly et al\\.", "year": 1999}, {"title": "Verbal Autopsy Methods with Multiple Causes of Death", "author": ["Gary King", "Ying Lu."], "venue": "Statist. Sci. 23, 1 (2008), 78\u201391.", "citeRegEx": "King and Lu.,? 2008", "shortCiteRegEx": "King and Lu.", "year": 2008}, {"title": "Partially identified prevalence estimation under misclassification using the kappa coefficient", "author": ["Helmut K\u00fcchenhoff", "Thomas Augustin", "Anne Kunz."], "venue": "International Journal of Approximate Reasoning 53, 8 (2012), 1168\u20131182.", "citeRegEx": "K\u00fcchenhoff et al\\.,? 2012", "shortCiteRegEx": "K\u00fcchenhoff et al\\.", "year": 2012}, {"title": "A three-population model for sequential screening for bacteriuria", "author": ["Paul S. Levy", "E.H. Kass."], "venue": "American Journal of Epidemiology 91, 2 (1970), 148\u2013154.", "citeRegEx": "Levy and Kass.,? 1970", "shortCiteRegEx": "Levy and Kass.", "year": 1970}, {"title": "Estimation of prevalence on the basis of screening tests", "author": ["Robert A. Lew", "Paul S. Levy."], "venue": "Statistics in Medicine 8, 10 (1989), 1225\u20131230.", "citeRegEx": "Lew and Levy.,? 1989", "shortCiteRegEx": "Lew and Levy.", "year": 1989}, {"title": "Representation and learning in information retrieval", "author": ["David D. Lewis."], "venue": "Ph.D. Dissertation. Department of Computer Science, University of Massachusetts, Amherst, US. http://www.research.att.com/\u223clewis/ papers/lewis91d.ps", "citeRegEx": "Lewis.,? 1992", "shortCiteRegEx": "Lewis.", "year": 1992}, {"title": "Evaluating and optimizing autonomous text classification systems", "author": ["David D. Lewis."], "venue": "Proceedings of the 18th ACM International Conference on Research and Development in Information Retrieval (SIGIR 1995). Seattle, US, 246\u2013254.", "citeRegEx": "Lewis.,? 1995", "shortCiteRegEx": "Lewis.", "year": 1995}, {"title": "RCV1: A New Benchmark Collection for Text Categorization Research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li."], "venue": "Journal of Machine Learning Research 5 (2004), 361\u2013397.", "citeRegEx": "Lewis et al\\.,? 2004", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Handling Concept Drift via Ensemble and Class Distribution Estimation Technique", "author": ["Nachai Limsetto", "Kitsana Waiyamai."], "venue": "Proceedings of the 7th International Conference on Advanced Data Mining (ADMA 2011). Bejing, CN, 13\u201326.", "citeRegEx": "Limsetto and Waiyamai.,? 2011", "shortCiteRegEx": "Limsetto and Waiyamai.", "year": 2011}, {"title": "A Demographic Analysis of Online Sentiment during Hurricane Irene", "author": ["Benjamin Mandel", "Aron Culotta", "John Boulahanis", "Danielle Stark", "Bonnie Lewis", "Jeremy Rodrigue."], "venue": "Proceedings of the NAACL/HLT Workshop on Language in Social Media. Montreal, CA, 27\u201336.", "citeRegEx": "Mandel et al\\.,? 2012", "shortCiteRegEx": "Mandel et al\\.", "year": 2012}, {"title": "Quantification Trees", "author": ["Letizia Milli", "Anna Monreale", "Giulio Rossetti", "Fosca Giannotti", "Dino Pedreschi", "Fabrizio Sebastiani."], "venue": "Proceedings of the 13th IEEE International Conference on Data Mining (ICDM 2013). Dallas, US, 528\u2013536.", "citeRegEx": "Milli et al\\.,? 2013", "shortCiteRegEx": "Milli et al\\.", "year": 2013}, {"title": "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series", "author": ["Brendan O\u2019Connor", "Ramnath Balasubramanyan", "Bryan R. Routledge", "Noah A. Smith"], "venue": "In Proceedings of the 4th AAAI Conference on Weblogs and Social Media (ICWSM", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "Estimating the prevalence of a rare disease: Adjusted maximum likelihood", "author": ["Elham Rahme", "Lawrence Joseph."], "venue": "The Statistician 47 (1998), 149\u2013158.", "citeRegEx": "Rahme and Joseph.,? 1998", "shortCiteRegEx": "Rahme and Joseph.", "year": 1998}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley."], "venue": "Information Processing and Management 24, 5 (1988), 513\u2013523.", "citeRegEx": "Salton and Buckley.,? 1988", "shortCiteRegEx": "Salton and Buckley.", "year": 1988}, {"title": "Concept drift", "author": ["Claude Sammut", "Michael Harries."], "venue": "Encyclopedia of Machine Learning, Claude Sammut and Geoffrey I. Webb (Eds.). Springer, Heidelberg, DE, 202\u2013205.", "citeRegEx": "Sammut and Harries.,? 2011", "shortCiteRegEx": "Sammut and Harries.", "year": 2011}, {"title": "Classification and Quantification Based on Image Analysis for Sperm Samples with Uncertain Damaged/Intact Cell Proportions", "author": ["Lidia S\u00e1nchez", "V\u0131\u0301ctor Gonz\u00e1lez", "Enrique Alegre", "Roc\u0131\u0301o Alaiz"], "venue": "In Proceedings of the 5th International Conference on Image Analysis and Recognition (ICIAR 2008). Po\u0301voa de Varzim,", "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2008}, {"title": "BoosTexter: A boosting-based system for text categorization", "author": ["Robert E. Schapire", "Yoram Singer."], "venue": "Machine Learning 39, 2/3 (2000), 135\u2013168.", "citeRegEx": "Schapire and Singer.,? 2000", "shortCiteRegEx": "Schapire and Singer.", "year": 2000}, {"title": "Collective Classification in Network Data", "author": ["Prithviraj Sen", "Galileo Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Gallagher", "Tina Eliassi-Rad."], "venue": "AI Magazine 29, 3 (2008), 93\u2013106.", "citeRegEx": "Sen et al\\.,? 2008", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Density estimation for statistics and data analysis", "author": ["Bernard W. Silverman."], "venue": "Chapman and Hall, London, UK.", "citeRegEx": "Silverman.,? 1986", "shortCiteRegEx": "Silverman.", "year": 1986}, {"title": "Network Quantification Despite Biased Labels", "author": ["Lei Tang", "Huiji Gao", "Huan Liu."], "venue": "Proceedings of the 8th Workshop on Mining and Learning with Graphs (MLG 2010). Washington, US, 147\u2013154.", "citeRegEx": "Tang et al\\.,? 2010", "shortCiteRegEx": "Tang et al\\.", "year": 2010}, {"title": "Support Vector Machine Learning for Interdependent and Structured Output Spaces", "author": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun."], "venue": "Proceedings of the 21 st International Conference on Machine Learning (ICML 2004). Banff, CA.", "citeRegEx": "Tsochantaridis et al\\.,? 2004", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "WagTag: A Dog Collar Accessory for Monitoring Canine Activity Levels", "author": ["Gary M. Weiss", "Ashwin Nathan", "J.B. Kropp", "Jeffrey W. Lockhart."], "venue": "Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing (UBICOMP 2013). Zurich, CH, 405\u2013414.", "citeRegEx": "Weiss et al\\.,? 2013", "shortCiteRegEx": "Weiss et al\\.", "year": 2013}, {"title": "Quantification and semi-supervised classification methods for handling changes in class distribution", "author": ["Jack Chongjie Xue", "Gary M. Weiss."], "venue": "Proceedings of the 15th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD 2009). Paris, FR, 897\u2013906.", "citeRegEx": "Xue and Weiss.,? 2009", "shortCiteRegEx": "Xue and Weiss.", "year": 2009}, {"title": "Statistical Language Models for Information Retrieval: A Critical Review", "author": ["ChengXiang Zhai."], "venue": "Foundations and Trends in Information Retrieval 2, 3 (2008), 137\u2013213.", "citeRegEx": "Zhai.,? 2008", "shortCiteRegEx": "Zhai.", "year": 2008}, {"title": "Transfer estimation of evolving class priors in data stream classification", "author": ["Zhihao Zhang", "Jie Zhou."], "venue": "Pattern Recognition 43, 9 (2010), 3151\u20133161.", "citeRegEx": "Zhang and Zhou.,? 2010", "shortCiteRegEx": "Zhang and Zhou.", "year": 2010}, {"title": "Statistical Methods in Diagnostic Medicine", "author": ["Xiao-Hua Zhou", "Donna K. McClish", "Nancy A. Obuchowski."], "venue": "Wiley, New York, US.", "citeRegEx": "Zhou et al\\.,? 2002", "shortCiteRegEx": "Zhou et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 18, "context": "The latter task is known as quantification [Forman 2005; 2006a; 2008; Forman et al. 2006].", "startOffset": 43, "endOffset": 89}, {"referenceID": 1, "context": ", predicting election results by estimating the prevalence of blog posts (or tweets) supporting a given candidate or party [Hopkins and King 2010], or planning the amount of human resources to allocate to different types of issues in a customer support center by estimating the prevalence of customer calls related to a given issue [Forman 2005], or supporting epidemiological research by estimating the prevalence of medical reports in which a specific pathology is diagnosed [Baccianella et al. 2013].", "startOffset": 477, "endOffset": 502}, {"referenceID": 28, "context": "Concept drift usually comes in one of three forms [Kelly et al. 1999]: (a) the class priors p(ci) may change, i.", "startOffset": 50, "endOffset": 69}, {"referenceID": 4, "context": ", [Bella et al. 2010; Forman 2005; 2006a; 2008; Forman et al. 2006; Hopkins and King 2010; Xue and Weiss 2009]) employ general-purpose supervised learning methods, i.", "startOffset": 2, "endOffset": 110}, {"referenceID": 18, "context": ", [Bella et al. 2010; Forman 2005; 2006a; 2008; Forman et al. 2006; Hopkins and King 2010; Xue and Weiss 2009]) employ general-purpose supervised learning methods, i.", "startOffset": 2, "endOffset": 110}, {"referenceID": 47, "context": "The simplest such measure is bias (B), defined as B(\u03bbTe, \u03bb\u0302Te) = \u03bb\u0302Te(c)\u2212 \u03bbTe(c) and used in [Forman 2005; 2006a; Tang et al. 2010]; positive bias indicates a tendency to overestimate the prevalence of c, while negative bias indicates a tendency to underestimate it.", "startOffset": 93, "endOffset": 131}, {"referenceID": 3, "context": "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; Gonz\u00e1lez-Castro et al. 2013; S\u00e1nchez et al. 2008; Tang et al. 2010]), de-", "startOffset": 114, "endOffset": 246}, {"referenceID": 4, "context": "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; Gonz\u00e1lez-Castro et al. 2013; S\u00e1nchez et al. 2008; Tang et al. 2010]), de-", "startOffset": 114, "endOffset": 246}, {"referenceID": 21, "context": "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; Gonz\u00e1lez-Castro et al. 2013; S\u00e1nchez et al. 2008; Tang et al. 2010]), de-", "startOffset": 114, "endOffset": 246}, {"referenceID": 43, "context": "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; Gonz\u00e1lez-Castro et al. 2013; S\u00e1nchez et al. 2008; Tang et al. 2010]), de-", "startOffset": 114, "endOffset": 246}, {"referenceID": 47, "context": "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; Gonz\u00e1lez-Castro et al. 2013; S\u00e1nchez et al. 2008; Tang et al. 2010]), de-", "startOffset": 114, "endOffset": 246}, {"referenceID": 13, "context": "The most convincing among the evaluation measures proposed so far is certainly Forman\u2019s [2005], who uses normalized cross-entropy, better known as Kullback-Leibler Divergence (KLD \u2013 see e.", "startOffset": 79, "endOffset": 95}, {"referenceID": 47, "context": "and also used in [Esuli and Sebastiani 2010b; Forman 2006a; 2008; Tang et al. 2010], is a measure of the error made in estimating a true distribution \u03bbTe over a set C of classes by means of a distribution \u03bb\u0302Te; this means that KLD is in principle suitable for evaluating quantification, since quantifying exactly means predicting how the test items are distributed across the classes.", "startOffset": 17, "endOffset": 83}, {"referenceID": 4, "context": "The PCC method is dismissed as unsuitable in [Forman 2005; 2008], but is shown to perform better than CC in [Bella et al. 2010] (where it is called \u201cProbability Average\u201d) and in [Tang et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 47, "context": "2010] (where it is called \u201cProbability Average\u201d) and in [Tang et al. 2010].", "startOffset": 56, "endOffset": 74}, {"referenceID": 4, "context": "The PACC method (proposed in [Bella et al. 2010], where it is called \u201cScaled Probability Average\u201d) is a probabilistic variant of ACC, i.", "startOffset": 29, "endOffset": 48}, {"referenceID": 13, "context": "lead Forman [2008] to \u201cclip\u201d the results of the estimation (i.", "startOffset": 5, "endOffset": 19}, {"referenceID": 14, "context": "Forman [2008] points out that the ACC method is very sensitive to the decision threshold of the classifier, which may yield unreliable values of \u03bb Te (c) (or lead to \u03bb Te (c) being undefined when tprTe = fprTe).", "startOffset": 0, "endOffset": 14}, {"referenceID": 26, "context": "SVMperf is a specialization to the problem of binary classification of the structural SVM (SVM) learning algorithm [Joachims et al. 2009a; Joachims et al. 2009b; Tsochantaridis et al. 2004] for \u201cstructured prediction\u201d, i.", "startOffset": 115, "endOffset": 189}, {"referenceID": 27, "context": "SVMperf is a specialization to the problem of binary classification of the structural SVM (SVM) learning algorithm [Joachims et al. 2009a; Joachims et al. 2009b; Tsochantaridis et al. 2004] for \u201cstructured prediction\u201d, i.", "startOffset": 115, "endOffset": 189}, {"referenceID": 48, "context": "SVMperf is a specialization to the problem of binary classification of the structural SVM (SVM) learning algorithm [Joachims et al. 2009a; Joachims et al. 2009b; Tsochantaridis et al. 2004] for \u201cstructured prediction\u201d, i.", "startOffset": 115, "endOffset": 189}, {"referenceID": 14, "context": "In order to sidestep this problem, we adopt the SVM for Multivariate Performance Measures (SVMperf ) learning algorithm proposed by Joachims [2005]4.", "startOffset": 72, "endOffset": 148}, {"referenceID": 27, "context": "As discussed in [Joachims et al. 2009b], SVM can be adapted to a specific task by defining four components:", "startOffset": 16, "endOffset": 39}, {"referenceID": 24, "context": "4In [Joachims 2005] SVMperf is actually called SVM\u2206multi, but the author has released its implementation under the name SVMperf . We will use this latter name because it uniquely identifies the algorithm on the Web, while searching for \u201cSVM multi\u201d often returns the SVMmulticlass package, which addresses a different problem. 5For this formulation of \u03a8, and when error rate is the chosen loss function, Joachims [2005] shows that SVMperf coincides with the traditional univariate SVM model (called SVMorg in [Joachims 2005]).", "startOffset": 5, "endOffset": 419}, {"referenceID": 35, "context": "Consistently with the evaluation presented in [Lewis et al. 2004], also classes placed at internal nodes in the hierarchically organized classification scheme are considered in the evaluation; as", "startOffset": 46, "endOffset": 65}, {"referenceID": 35, "context": "html 10This is the standard \u201cLYRL2004\u201d split between training and test data, originally defined in [Lewis et al. 2004].", "startOffset": 99, "endOffset": 118}, {"referenceID": 22, "context": "The OHSUMED-S dataset [Esuli and Sebastiani 2013] is a subset of the wellknown OHSUMED test collection [Hersh et al. 1994].", "startOffset": 103, "endOffset": 122}, {"referenceID": 20, "context": "Concerning training, Joachims [2005] proves that training a classifier with SVMperf is O(n), with n the number of training examples, for any loss function that can be computed from a contingency table (such as KLD indeed is).", "startOffset": 21, "endOffset": 37}, {"referenceID": 14, "context": "This is certainly more expensive than training a classifier by means of a standard, linear SVM (which is at the heart of Forman\u2019s implementation of all the quantification methods of \u00a72.2), since this latter is well-known to be O(sn) (with s the average number of non-zero features in the training objects) [Joachims 2006]. However, note that, while SVM(KLD) only requires training a classifier by means of SVMperf , setting up a quantifier with any of the methods of \u00a72.2 (with the only exception of the simple CC method) requires more than simply training a classifier. For instance, ACC (together with the methods derived from it, such as T50, X, MAX, MS, PACC) also requires estimating tprTe and fprTe on the training set via k-fold cross validation, which may be expensive; analogously, both MM(KS) and MM(PP) require estimatingD c andD c via k-fold cross-validation, and the same considerations apply. In practice, using SVMperf turns out to be affordable. On RCV1-V2, training the 99 binary classifiers described in the previous sections via SVMperf required on average about 4.7 seconds each16. By contrast, training the analogous classifiers via a standard linear SVM required on average only 2.1 seconds each. However, this means that, if k-fold cross-validation is used for the estimation of parameters with a value of k \u2265 2 (meaning that, for each class, additional k classifiers need to be trained), the computational advantage of using a linear SVM instead of the more expensive SVMperf is completely lost. Forman [2008] recommends choosing k = 50 in order to obtain more accurate estimates of tprTe and fprTe for use in ACC and derived methods; this means making the training phase roughly (2.", "startOffset": 121, "endOffset": 1534}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies.", "startOffset": 0, "endOffset": 26}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.", "startOffset": 0, "endOffset": 140}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.", "startOffset": 0, "endOffset": 319}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.", "startOffset": 0, "endOffset": 349}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.", "startOffset": 0, "endOffset": 371}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.", "startOffset": 0, "endOffset": 398}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification \u201cwithout knowingly doing so\u201d; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain \u201cclassify and count\u201d. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O\u2019Connor et al.", "startOffset": 0, "endOffset": 969}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification \u201cwithout knowingly doing so\u201d; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain \u201cclassify and count\u201d. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O\u2019Connor et al. [2010] analyse the correlation between public opinion as measured via tweet sentiment quantification and via traditional opinion polls; Dodds et al.", "startOffset": 0, "endOffset": 1174}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification \u201cwithout knowingly doing so\u201d; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain \u201cclassify and count\u201d. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O\u2019Connor et al. [2010] analyse the correlation between public opinion as measured via tweet sentiment quantification and via traditional opinion polls; Dodds et al. [2011] use tweet sentiment quantification in order to infer spatio-temporal happiness patterns; and Weiss et al.", "startOffset": 0, "endOffset": 1323}, {"referenceID": 1, "context": "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification \u201cwithout knowingly doing so\u201d; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain \u201cclassify and count\u201d. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O\u2019Connor et al. [2010] analyse the correlation between public opinion as measured via tweet sentiment quantification and via traditional opinion polls; Dodds et al. [2011] use tweet sentiment quantification in order to infer spatio-temporal happiness patterns; and Weiss et al. [2013] use quantification in order to measure the prevalence of different types of pets\u2019 activity as detected by wearable devices.", "startOffset": 0, "endOffset": 1436}, {"referenceID": 4, "context": "However, the experimental comparisons of [Bella et al. 2010] and [Tang et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 47, "context": "2010] and [Tang et al. 2010] are both framed in terms of absolute error, which seems a sub-standard evaluation measure for this task (see \u00a72.", "startOffset": 10, "endOffset": 28}, {"referenceID": 38, "context": "The first published work that implements and tests the idea of directly optimizing a quantification-specific loss function is [Milli et al. 2013], whose authors propose variants of decision trees and decision forests that directly optimize a loss combining classification accuracy and quantification accuracy.", "startOffset": 126, "endOffset": 145}, {"referenceID": 2, "context": "At the time of going to print we have become aware of a related paper [Barranquero et al. 2015] whose authors, following [Esuli and Sebastiani 2010b], use SVMperf to perform quantification; differently from the present paper, and similarly to [Milli et al.", "startOffset": 70, "endOffset": 95}, {"referenceID": 38, "context": "2015] whose authors, following [Esuli and Sebastiani 2010b], use SVMperf to perform quantification; differently from the present paper, and similarly to [Milli et al. 2013], they use an evaluation function that combines classification accuracy and quantification accuracy.", "startOffset": 153, "endOffset": 172}, {"referenceID": 45, "context": "A research area that might seem related to quantification is collective classification (CoC) [Sen et al. 2008].", "startOffset": 93, "endOffset": 110}, {"referenceID": 30, "context": "Quantification bears strong relations with prevalence estimation from screening tests, an important task in epidemiology (see [Levy and Kass 1970; Lew and Levy 1989; K\u00fcchenhoff et al. 2012; Rahme and Joseph 1998; Zhou et al. 2002]).", "startOffset": 126, "endOffset": 230}, {"referenceID": 53, "context": "Quantification bears strong relations with prevalence estimation from screening tests, an important task in epidemiology (see [Levy and Kass 1970; Lew and Levy 1989; K\u00fcchenhoff et al. 2012; Rahme and Joseph 1998; Zhou et al. 2002]).", "startOffset": 126, "endOffset": 230}], "year": 2015, "abstractText": "Authors\u2019 address: Andrea Esuli, Istituto di Scienza e Tecnologie dell\u2019Informazione, Consiglio Nazionale delle Ricerche, Via Giuseppe Moruzzi 1, 56124 Pisa, Italy. E-mail: andrea.esuli@isti.cnr.it. Fabrizio Sebastiani, Qatar Computing Research Institute, PO Box 5825, Doha, Qatar. E-mail: fsebastiani@qf.org.qa. Fabrizio Sebastiani is on leave from Consiglio Nazionale delle Ricerche. The order in which the authors are listed is purely alphabetical; each author has given an equally important contribution to this work. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c \u00a9 YYYY ACM 1556-4681/YYYY/-ARTAA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000", "creator": "TeX"}}}