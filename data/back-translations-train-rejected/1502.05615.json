{"id": "1502.05615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2015", "title": "Forgetting and consolidation for incremental and cumulative knowledge acquisition systems", "abstract": "The application of cognitive mechanisms to support knowledge acquisition is, from our point of view, crucial for making the resulting models coherent, efficient, credible, easy to use and understandable. In particular, there are two characteristic features of intelligence that are essential for knowledge development: forgetting and consolidation. Both plays an important role in knowledge bases and learning systems to avoid possible information overflow and redundancy, and in order to preserve and strengthen important or frequently used rules and remove (or forget) useless ones. We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The Stability-Plasticity dilemma. In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way. The metrics are not only used to forget some of the worst rules, but also to set a consolidation process to promote those selected rules to the knowledge base, which is also mirrored by a demotion system. We evaluate the framework with a series of tasks in a chess rule learning domain.", "histories": [["v1", "Thu, 19 Feb 2015 16:25:49 GMT  (5384kb,D)", "http://arxiv.org/abs/1502.05615v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["fernando mart\\'inez-plumed", "c\\`esar ferri", "jos\\'e hern\\'andez-orallo", "mar\\'ia jos\\'e ram\\'irez-quintana"], "accepted": false, "id": "1502.05615"}, "pdf": {"name": "1502.05615.pdf", "metadata": {"source": "CRF", "title": "Forgetting and consolidation for incremental and cumulative knowledge acquisition systems", "authors": ["Fernando Mart\u0301\u0131nez-Plumed", "C\u00e8sar Ferri", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["fmartinez@dsic.upv.es", "cferri@dsic.upv.es", "jorallo@dsic.upv.es", "mramirez@dsic.upv.es"], "sections": [{"heading": null, "text": "Keywords: cognitive skills, forgetting, consolidation, lifelong machine learning, knowledge acquisition, declarative learning, MML."}, {"heading": "1 Introduction", "text": "In this context, it is worth mentioning that this is a case in which a person has been put in a predicament, who is in a predicament."}, {"heading": "2 Coverage graph", "text": "We believe that \"rules\" are used to express examples, hypotheses, and background knowledge. Rules are referred to as e, where class (e) = c, c \u00b2 C, and C is the set of classes, such as {false, true}. The set of all possible rules is referred to as R, where W \u00b2 R is the workspace or memory, and K \u00b2 R is the background or consolidated knowledge base. Rules are presented as vertexes or nodes V (and we refer them vaguely) in a directed acyclic diagram G (V, A) we refer to as a cover diagram (which is the DAG representation of a particular area of work), because directed edges A represent the coverage relationship between the various rules determined by the deductive motor. We say that one rule \u03c1a is covered by another rule when (K \u00b2 b) | = a negative coverage diagram (c \u00b2): The precise understanding of the semantic sequential operators will depend on the language and the deductive motor."}, {"heading": "3 Basic Metrics for Discovered Knowledge Assessment", "text": "In order to select and arrange the rulebook in the workspace, different measures of usefulness, relevance and consistency must be derived from the coverage chart. Starting from the idea that the relevance or usefulness of a rule is determined by the relationship between 3For the sake of simplicity, the coverage charts do not include the edges for the temporary conclusion of the coverage relationship, i.e. if a node \u00b5 covers the nodes \u03bd and \u03b3, but \u03bd, only the edges \u00b5 \u2192 \u03bd and \u03bd \u2192 \u03b3 are included in the chart. Its own complexity and the complexity of the rules it covers, a general criterion such as the Minimum Measure Length [2] (MML) can be used as a starting criterion from which new metrics can be derived."}, {"heading": "3.1 Minimum Message Length", "text": "The minimum message length is one of the most popular selection criteria in inductive deduction (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]). It includes an interpretation of the Razor principle of Occam: The model produces the shortest general message (composed of the model and the proofs that are concisely encoded using this principle), which can be expressed in a Bayesian form [2] with the length of the first part of the message (the model) and the length of the second part (evidence covered). Bayesian theory, which is the primary concern of the Bayesian conclusion, is shown in Equation 1: P (H | E) = P (H) P (E) = P (E) = P (H) P (E) (E) (E) (E), with P (H) being the previous probability of the modelH (H) (H)."}, {"heading": "3.2 MML goes hierarchical: Support", "text": "Following Equation 3, we will understand the terms L (W) \u2212 L (W) \u2212 W (W) as conservative (W) (W) as conservative (W) as adapted to coverage charts and multi-class settings. (W) Formally, we define the support for a rule (W) as: S (R), L (R) \u2212 L (W) = L (W) \u2212 L (W) (5), where L (W) \u2212 L) represents the coverage of a rule (W) \u2212 L (W) represents the coverage of a rule (W), that is, the length of all rules in W minus the rules that are not covered by the rules. (W) Supporting a rule represents the length of the rules it covers: S (W) = the coverage of the rules it covers: S (W) = L (6), which results in an alternative expression for L (W)."}, {"heading": "ID L(\u03c1) class S\u0307+ S\u0307\u2212 \u2212L\u0307+ \u2212L\u0307\u2212", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3 Optimality", "text": "However, if there is more than one class in W, we must take into account the purity or trust of the rules. In the same sense of the MML principle, we define optimality as the difference between the cost of encoding a rule according to Eq.13 for a given class and the cost of encoding the exceptions, i.e. supporting the rules belonging to the other classes. We use a factor \u03b2, which indicates the relevance of rules that are as pure as possible. Formally: optc (\u03c1, W), \u2212 \u03b2 \u00b7 L \u00b2 c (\u03c1 | W) \u2212 (1 \u2212 \u03b2) \u00b7 x \u00b2 c \u00b2 c \u00b2 s c \u00b2 s \u00b2 s \u00b2 c \u00b2 (\u03c1, W) (14), which leads to a general optimality of a rule as follows: opt (\u03c1, W), max c \u00b2 C (\u03c1, W) (15). Following the family example, Table 4 shows the optimum values for the respective class."}, {"heading": "4 Structuring knowledge: forgetting, promotion and", "text": "demotionIn our environment, rules are generated time and again by the inductive motor and added to the work area W. In response to the possible infinite growth of W, it is necessary to have mechanisms to forget or revise useless parts of the knowledge acquired. On the basis of the indicators just introduced, we need a mechanism to discard those rules that are not useful, inconsistent, or do not receive sufficient support."}, {"heading": "4.1 Forgetting mechanism", "text": "The optimality of a rule is a central measure for determining its usefulness, but it is also important to see if it could be considered superfluous because it is covered (transitively or directly) by another rule of higher optimality. If it is the case, it is largely redundant and it could certainly be discarded. This idea leads to the following definition of the durability of a rule: Permc (r, W), optc (r) \u2212 max. (0, max: 1). (16) with a general durability: Perm (r, W), max c (r, C). (r, W). (17) The lower value of the durability of a rule has the probability that it must be forgotten. If we perform a forgotten step, the coverage graph is affected and the covers are also affected. To get so much information about the past support, each rule is provided with a trace of its old support."}, {"heading": "4.2 Consolidated knowledge: promotion and demotion", "text": "Finally, some of the rules with good indicators in the workspace must eventually be promoted to consolidated knowledge (or belief). This must be a careful process, as the consolidated knowledge of the deductive motor is used to calculate the range. This means that an inconsistent rule promoted to consolidated knowledge can have important effects on the behavior of the system. The promotion function can be adapted for application, but a general choice is the use of a threshold effect to consolidate or promote a rule to belief status in B. When a rule is promoted to consolidated knowledge, it cannot be the target of the forgetting mechanism and therefore forgotten. It may happen that this rule can eventually be removed from consolidated knowledge. Therefore, the promotion system is reflected by a demolition system, exceeding another threshold. The original background knowledge (B0) cannot be downgraded (and forgotten). In the example of Figure 6, we have a threshold that matches the average optimum working space in all rules."}, {"heading": "5 Experiments", "text": "As mentioned in Section 1, one of the problems in many cognitive systems (particularly connectionist, artificial or biological) is the dilemma of stability-plasticity. We claim that our approach is capable of addressing this problem in a long-term learning process. To this end, we have conducted an experimental evaluation to examine the following questions: (a) Is it possible to gradually create a large repository of consolidated knowledge, in which the usefulness of the rules is evaluated? (b) Is our approach capable of forgetting or revising the existing knowledge in order to create a rich and reusable knowledge base? and (c) how are the process and the resulting knowledge structure understood in relation to cognitive systems that need to gradually discover and develop knowledge? We want to illustrate these characteristics in a single area and find out whether the ultimate goal of these experiments is general enough to work with inductive and deductive engines to better understand how the metrics and other processes may work, and whether they may require some problems."}, {"heading": "5.1 Methodology", "text": "We will focus on the problem of learning the rules of chess by observation. In particular, we will focus on learning a model of legal moves of different pieces from a series of legal and illegal moves (extracted from [58]). In our framework, the legal moves are the positive ones and the illegal moves are the negative ones (so that we have two classes). Each example represents a move of a certain piece on an empty board. Therefore, a move is represented by a triple from the chess domain - Pos \u00b7 Pos, with the second and third components each representing the starting position of the piece and its destination on a chessboard. Positions are represented by a tuple from the domain, with pieces (a-h) representing columns and ranks (1-8) representing rows. For example, Figure 7 illustrates all possible moves of a knight from a certain starting position (K) to several other positions (K \u2032). We will use a prologue notation (as in the example in the previous section).The only background we will use is the Y (we use)."}, {"heading": "5.2 Consolidation without forgetting", "text": "In a first experiment, we try to show what would happen without applying the forgetfulness mechanism and to check whether the MML-based rules work successfully for knowledge acquisition: Is the final consolidated knowledge useful in light of the evidence to solve the problem? Figure 8 shows the evolution of the learning process in 500 steps. Since no rules are forgotten, the rule population (dashed brown line) reaches its maximum value (100) and stagnates without ignoring new evidence entering the system (because they are already placed in W) starting from step 180. In this case, we assumed that all the evidence for the chess problem can be mapped to W, but it could be the case that all the knowledge of a problem does not fit into W (memory limitations) and therefore collapses without improvement. The same applies to the average optimality of all rules (dashed blue line) and the consolidated rules (dashed green line), which, since no new rules are mapped to W, we cannot see any further step consolidated - 7 or green line can only take place for another learning improvement)."}, {"heading": "5.3 Consolidation with forgetting", "text": "In other words, we must abide by the rules we have imposed on ourselves. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules. (...) We must abide by the rules."}, {"heading": "5.4 Incremental knowledge acquisition", "text": "Finally, a final experiment tries to show the ability of our approach to gradually learning new knowledge from previously consolidated concepts. This experiment is divided into two phases: in the first, we have only rules and examples of moves of the tower and the bishop pawns (15 and 30 rules, respectively) that provide the system with them in the same way as in the previous experiment. The consolidation criterion has not been changed, but the maximum number of rules in the workspace has been set at 15 (to make the forgetting mechanism work) and the percentage of meaningless rules that are forgotten up to 25% for each forgetting process, due to the smaller size of the work set. In Table 10 we can see the set of consolidated rules after 100 steps. This sentence contains the rules that perfectly generalize all the legal steps of the tower and the bishop. In the first 100 steps of Figure 10 we can see how forgetting and the consolidation mechanisms work."}, {"heading": "5.5 Discussion", "text": "In an effort to facilitate an understanding of whether our approach is capable of effectively and progressively expanding a knowledge base by using appropriate assessment standards and useful cognitive skills to manage the acquired knowledge, we have conducted some experiments on a well-known scientific area, the chess problem. As we have said, the ultimate goal is not to validate the approach, but to provide insight into both its generality, efficiency, and the urgently needed use of forgetting and consolidation of cognitive practices in incremental and developmental approaches to the discovery of knowledge. However, in order to shed some light on these aspects, we refer to the questions raised at the beginning of this section. From the above experiments we see that the rulebook can be well structured and ordered by metrics, and that the system consolidates those rules that are appropriate, and therefore responds positively to the first question (a) with respect to the question (we see) that even the restriction in the rules is moderate."}, {"heading": "6 Conclusions", "text": "In fact, the fact is that most of them are able to move around without being able to move."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the EU (FEDER) and the Spanish MINECO within the framework of the grants TIN 2010-21062-C02-02, TIN 2013-45732-C4-1-P and FPI-ME BES-2011045099 as well as by the Generalitat Valenciana PROMETEO2011 / 552. Part of this work is examined under Pattern Recognition Letters."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "The art of adaptive pattern recognition by a selforganizing neural network", "author": ["G. Carpenter", "S. Grossberg"], "venue": "Computer, vol. 21, no. 3, pp. 77\u201388, March 1988.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1988}, {"title": "An information measure for classification", "author": ["C.S. Wallace", "D.M. Boulton"], "venue": "The Computer Journal, vol. 11, no. 2, pp. 185\u2013194, 1968. 27", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1968}, {"title": "Catastrophic forgetting, rehearsal and pseudorehearsal", "author": ["A. Robins"], "venue": "Connection Science, vol. 7, pp. 123\u2013146, 1995.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Pseudo-recurrent connectionist networks: An approach to the \u201dsensitivity-stability\u201d dilemma", "author": ["R.M. French"], "venue": "Connection Science, vol. 9, pp. 353\u2013379, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Avoiding catastrophic forgetting by coupling two reverberating neural networks", "author": ["B. Ans", "S. Rousset"], "venue": "Comptes Rendus de l\u2019Acadmie des Sciences - Series {III} - Sciences de la Vie, vol. 320, no. 12, pp. 989 \u2013 997, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Adaptive resonance theory: How a brain learns to consciously attend, learn, and recognize a changing world", "author": ["S. Grossberg"], "venue": "Neural Netw., vol. 37, pp. 1\u201347, Jan. 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["S. Thrun"], "venue": "Advances in Neural Information Processing Systems, vol. 8, 1996, pp. 640\u2013646.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowl. and Data Eng., vol. 22, no. 10, pp. 1345\u20131359, Oct. 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A model of inductive bias learning", "author": ["J. Baxter"], "venue": "Journal of Artificial Intelligence Research, vol. 12, pp. 149\u2013198, 2000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Multitask learning: A knowledge-based source of inductive bias", "author": ["R. Caruana"], "venue": "Proceedings of the Tenth International Conference on Machine Learning. Morgan Kaufmann, 1993, pp. 41\u201348.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Ella: An efficient lifelong learning algorithm", "author": ["E. Eaton", "P.L. Ruvolo"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), vol. 28, 2013, pp. 507\u2013515.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward an architecture for never-ending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka Jr.", "T.M. Mitchell"], "venue": "Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010), 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Lessons from theory revision applied to constructive induction", "author": ["L.A. Rendell"], "venue": "Machine Learning Proceedings 1995: Proceedings of the Twelfth International Conference on Machine Learning, Tahoe City, California, July 9-12 1995, vol. 51. Morgan Kaufmann, 1995, p. 185.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "Probabilistic first-order theory revision from examples", "author": ["A. Paes", "K. Revoredo", "G. Zaverucha", "V.S. Costa"], "venue": "Inductive Logic Programming. Springer, 2005, pp. 295\u2013311.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "A survey on concept drift adaptation", "author": ["J. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Computing Surveys (CSUR), vol. 46, no. 4, p. 44, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Inductive logic programming: Theory and methods", "author": ["S.H. Muggleton", "L. De Raedt"], "venue": "The Journal of Logic Programming, vol. 19, pp. 629\u2013679, 1994. 28", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Scientific knowledge discovery using inductive logic programming", "author": ["S.H. Muggleton"], "venue": "Communications of the ACM, vol. 42, no. 11, pp. 42\u201346, 1999.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "An introduction to inductive programming", "author": ["P. Flener", "U. Schmid"], "venue": "Artificial Intelligence Review, vol. 29, no. 1, pp. 45\u201362, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Inductive programming meets the real world", "author": ["S. Gulwani", "J. Hern\u00e1ndez-Orallo", "E. Kitzelmann", "S.H. Muggleton", "U. Schmid", "B. Zorn"], "venue": "2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental learning of functional logic programs", "author": ["C. Ferri-Ram\u0131\u0301rez", "J. Hern\u00e1ndez-Orallo", "M.J. Ram\u0131\u0301rez-Quintana"], "venue": "Functional and Logic Programming. Springer, 2001, pp. 233\u2013247.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Cumulative learning in the lambda calculus", "author": ["R. Henderson"], "venue": "Ph.D. dissertation, Imperial College London, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical and Inductive Inference by Minimum Message Length (Information Science and Statistics)", "author": ["C. Wallace"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Hypothesis selection and testing by the mdl principle", "author": ["J. Rissanen"], "venue": "The Computer Journal, vol. 42, no. 4, pp. 260\u2013269, 1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Advances in minimum description length: Theory and applications", "author": ["P.D. Gr\u00fcnwald", "I.J. Myung", "M.A. Pitt"], "venue": "MIT press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Constructive reinforcement learning", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "International Journal of Intelligent Systems, vol. 15, no. 3, pp. 241\u2013264, 2000.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Explanatory and creative alternatives to the MDL priciple", "author": ["J. Hern\u00e1ndez-Orallo", "I. Gar\u0107\u0131a-Varea"], "venue": "Foundations of Science, vol. 5, no. 2, pp. 185\u2013207, 2000.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J.M. Kleinberg"], "venue": "J. ACM, vol. 46, no. 5, pp. 604\u2013632, Sep. 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "The anatomy of a large-scale hypertextual web search engine", "author": ["S. Brin", "L. Page"], "venue": "Comput. Netw. ISDN Syst., vol. 30, no. 1-7, pp. 107\u2013117, Apr. 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "The stochastic approach for link-structure analysis (salsa) and the tkc effect", "author": ["R. Lempel", "S. Moran"], "venue": "Comput. Netw., vol. 33, no. 1-6, pp. 387\u2013401, Jun. 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "A fuzzy cognitive structure for pattern recognition", "author": ["W. Pedrycz"], "venue": "Pattern Recognition Letters, vol. 9, no. 5, pp. 305 \u2013 313, 1989.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning to learn: From smart machines to intelligent machines", "author": ["B. Raducanu", "J. Vitri"], "venue": "Pattern Recognition Letters, vol. 29, no. 8, pp. 1024 \u2013 1032, 2008, pattern Recognition in Interdisciplinary Perception and Intelligence {PRintPerclntel}.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling human color categorization", "author": ["E. van den Broek", "T. Schouten", "P. Kisters"], "venue": "Pattern Recognition Letters, vol. 29, no. 8, pp. 1136 \u2013 1144, 2008, pattern Recognition in Interdisciplinary Perception and Intelligence {PRintPerclntel}. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0167865507002759 29", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Common-sense reasoning for human action recognition", "author": ["J.M. del Rincn", "M.J. Santofimia", "J.-C. Nebel"], "venue": "Pattern Recognition Letters, vol. 34, no. 15, pp. 1849 \u2013 1860, 2013, smart Approaches for Human Action Recognition. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0167865512003509", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1849}, {"title": "From visual patterns to semantic description: A cognitive approach using artificial curiosity as the foundation", "author": ["D.M. Ramk", "K. Madani", "C. Sabourin"], "venue": "Pattern Recognition Letters, vol. 34, no. 14, pp. 1577 \u2013 1588, 2013, innovative Knowledge Based Techniques in Pattern Recognition.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledge discovery from data?", "author": ["M.J. Pazzani"], "venue": "Intelligent systems and their applications, IEEE,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "Concept cells: the building blocks of declarative memory functions", "author": ["R.Q. Quiroga"], "venue": "Nature Reviews Neuroscience, vol. 13, pp. 587\u2013597, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Forget it!", "author": ["F. Lin", "R. Reiter"], "venue": "Proceedings of the AAAI Fall Symposium on Relevance,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Propositional independence: Formula-variable independence and forgetting", "author": ["J. Lang", "P. Liberatore"], "venue": "Journal of Artificial Intelligence Research, vol. 18, p. 2003, 2003.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Reasoning under inconsistency: A forgetting-based approach", "author": ["J. Lang", "P. Marquis"], "venue": "Artif. Intell., vol. 174, no. 12-13, pp. 799\u2013823, Aug. 2010.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Solving logic program conflict through strong and weak forgettings", "author": ["Y. Zhang", "N.Y. Foo"], "venue": "Artificial Intelligence, vol. 170, no. 89, pp. 739 \u2013 778, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Semantic forgetting in answer set programming", "author": ["T. Eiter", "K. Wang"], "venue": "Artificial Intelligence, vol. 172, no. 14, pp. 1644 \u2013 1672, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Forgetting in logic programs under strong equivalence.", "author": ["Y. Wang", "Y. Zhang", "Y. Zhou", "M. Zhang"], "venue": "in KR,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Knowledge forgetting: Properties and applications", "author": ["Y. Zhang", "Y. Zhou"], "venue": "Artificial Intelligence, vol. 173, no. 1617, pp. 1525 \u2013 1537, 2009.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Variable forgetting in reasoning about knowledge", "author": ["K. Su", "A. Sattar", "G. Lv", "Y. Zhang"], "venue": "Journal of Artificial Intelligence Research, vol. 35, no. 2, p. 677, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "On the progression of knowledge in the situation calculus", "author": ["Y. Liu", "X. Wen"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, p. 976.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Forgetting for knowledge bases in dl-lite", "author": ["Z. Wang", "K. Wang", "R. Topor", "J. Pan"], "venue": "Annals of Mathematics and Artificial Intelligence, vol. 58, no. 1-2, pp. 117\u2013151, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Forgetting actions in domain descriptions", "author": ["E. Erdem", "P. Ferraris"], "venue": "Proc. of the 22nd AAAI Conference on Artificial Intelligence, 2007, pp. 409\u2013414. 30", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Online sequential extreme learning machine with forgetting mechanism", "author": ["J. Zhao", "Z. Wang", "D.S. Park"], "venue": "Neurocomputing, vol. 87, pp. 79\u201389, 2012.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Biological grounding of recruitment learning and vicinal algorithms in longterm potentiation", "author": ["L. Shastri"], "venue": "Emergent Neural Computational Architectures Based on Neuroscience, ser. Lecture Notes in Computer Science, S. Wermter, J. Austin, and D. Willshaw, Eds. Springer Berlin Heidelberg, 2001, vol. 2036, pp. 348\u2013367.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2001}, {"title": "Handbook of behavioral neuroscience", "author": ["L.N. Ekrem Dere", "Alexander Easton", "J.P. Huston"], "venue": "Handbook of Episodic Memory, ser. Handbook of Behavioral Neuroscience. Elsevier, 2008, vol. 18.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with configurable operators and RL-based heuristics", "author": ["F. Mart\u0301\u0131nez-Plumed", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "M. Ram\u0131\u0301rez-Quintana"], "venue": "New Frontiers in Mining Complex Patterns, ser. LNCS, 2013, vol. 7765, pp. 1\u201316.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "A knowledge growth and consolidation framework for lifelong machine learning systems", "author": ["F. Mart\u0131nez-Plumed", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "M.J. Ram\u0131rez-Quintana"], "venue": "Proceedings of the 13th International Conference on Machine Learning and Applications. IEEE, 2014, pp. 111\u2013116.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications, 3rd ed", "author": ["M. Li", "P.M. Vit\u00e1nyi"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}, {"title": "Refinements of MDL and MML coding", "author": ["C.S. Wallace", "D.L. Dowe"], "venue": "Comput. J., vol. 42, no. 4, pp. 330\u2013337, 1999.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1999}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "Bell system technical journal, vol. 27, 1948.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1948}, {"title": "An experimental comparison of human and machine learning formalisms", "author": ["S.H. Muggleton", "M. Bain", "J. Hayes-Michie", "D. Michie"], "venue": "In Proc. of 6th International Workshop on Machine Learning. Morgan Kaufmann, 1989, pp. 113\u2013118.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The Stability-Plasticity dilemma [1].", "startOffset": 219, "endOffset": 222}, {"referenceID": 1, "context": "In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle [2] to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way.", "startOffset": 160, "endOffset": 163}, {"referenceID": 0, "context": "This lead us to one of the well-known constraints for AI systems: The StabilityPlasticity dilemma [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.", "startOffset": 118, "endOffset": 127}, {"referenceID": 3, "context": "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.", "startOffset": 118, "endOffset": 127}, {"referenceID": 4, "context": "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.", "startOffset": 118, "endOffset": 127}, {"referenceID": 5, "context": "Some of the proposed solutions include: (a) dual-memory systems simulating the presence of short and long-term memory [3, 4, 5], and (b) cognitive architectures such as the Adaptive Resonance Theory (ART) [6] emulating how the brain processes information.", "startOffset": 205, "endOffset": 208}, {"referenceID": 6, "context": "For instance, Lifelong Machine Learning (LML)[7] is concerned with the persistent and cumulative nature of learning, namely: (a) capable of retaining and using prior knowledge, and (b) capable of acquiring new knowledge over a series of prediction tasks.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Similarly, Transfer Learning [8] and multitask learning [9, 10] take a similar perspective, where it is more explicit that the process is task-oriented, and knowledge and its structure does not always play a central role in these systems.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "Similarly, Transfer Learning [8] and multitask learning [9, 10] take a similar perspective, where it is more explicit that the process is task-oriented, and knowledge and its structure does not always play a central role in these systems.", "startOffset": 56, "endOffset": 63}, {"referenceID": 9, "context": "Similarly, Transfer Learning [8] and multitask learning [9, 10] take a similar perspective, where it is more explicit that the process is task-oriented, and knowledge and its structure does not always play a central role in these systems.", "startOffset": 56, "endOffset": 63}, {"referenceID": 10, "context": "ELLA (Efficient Lifelong Learning Algorithm) [11] and NELL (Never-Ending Language Learner) [12] are two more recent approaches to LML, which are able to integrate many capabilities.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "ELLA (Efficient Lifelong Learning Algorithm) [11] and NELL (Never-Ending Language Learner) [12] are two more recent approaches to LML, which are able to integrate many capabilities.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "Other related topics are concept drift and theory revision [13, 14, 15], where some rules are replaced by new rules that are consistent with new experience.", "startOffset": 59, "endOffset": 71}, {"referenceID": 13, "context": "Other related topics are concept drift and theory revision [13, 14, 15], where some rules are replaced by new rules that are consistent with new experience.", "startOffset": 59, "endOffset": 71}, {"referenceID": 14, "context": "Other related topics are concept drift and theory revision [13, 14, 15], where some rules are replaced by new rules that are consistent with new experience.", "startOffset": 59, "endOffset": 71}, {"referenceID": 15, "context": "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].", "startOffset": 41, "endOffset": 49}, {"referenceID": 16, "context": "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].", "startOffset": 41, "endOffset": 49}, {"referenceID": 17, "context": "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 18, "context": "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 19, "context": "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "The areas of inductive logic programming [16, 17] or general inductive programming [18, 19] have seen several approaches for incremental [20] or cumulative systems [21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 1, "context": "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 21, "context": "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 22, "context": "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].", "startOffset": 203, "endOffset": 211}, {"referenceID": 23, "context": "When the theory or hypothesis is considered as a whole and separated from the evidence, we have many well-founded proposal, such as the MML principle [2, 22] or the similar (but posterior) MDL principle [23, 24].", "startOffset": 203, "endOffset": 211}, {"referenceID": 24, "context": "However, for knowledge integration and consolidation it is necessary to assess each part of the theory independently, where different parts of the theory can have different degrees of validity, probability or reinforcement [25, 26].", "startOffset": 223, "endOffset": 231}, {"referenceID": 25, "context": "However, for knowledge integration and consolidation it is necessary to assess each part of the theory independently, where different parts of the theory can have different degrees of validity, probability or reinforcement [25, 26].", "startOffset": 223, "endOffset": 231}, {"referenceID": 26, "context": "The perspective of a network or hierarchy of nodes that get support from other nodes is more common in the area of link analysis in web graphs such as the HITS algorithm [27], PageRank [28] or SALSA [29], or in infometrics.", "startOffset": 170, "endOffset": 174}, {"referenceID": 27, "context": "The perspective of a network or hierarchy of nodes that get support from other nodes is more common in the area of link analysis in web graphs such as the HITS algorithm [27], PageRank [28] or SALSA [29], or in infometrics.", "startOffset": 185, "endOffset": 189}, {"referenceID": 28, "context": "The perspective of a network or hierarchy of nodes that get support from other nodes is more common in the area of link analysis in web graphs such as the HITS algorithm [27], PageRank [28] or SALSA [29], or in infometrics.", "startOffset": 199, "endOffset": 203}, {"referenceID": 29, "context": "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].", "startOffset": 83, "endOffset": 103}, {"referenceID": 30, "context": "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].", "startOffset": 83, "endOffset": 103}, {"referenceID": 31, "context": "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].", "startOffset": 83, "endOffset": 103}, {"referenceID": 32, "context": "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].", "startOffset": 83, "endOffset": 103}, {"referenceID": 33, "context": "Finally, knowledge acquisition has much to learn from the study of human cognition [30, 31, 32, 33, 34].", "startOffset": 83, "endOffset": 103}, {"referenceID": 34, "context": "We can fully realise the benefits of knowledge acquisition by paying attention to the cognitive factors that simplify the learning and processing of the knowledge which make the resulting models coherent, efficient, credible, easy to use and understandable [35].", "startOffset": 257, "endOffset": 261}, {"referenceID": 35, "context": "The ability to focus on what to discard what is not relevant is becoming more relevant not only in cognitive science and neuroscience [36], but also in artificial intelligence (e.", "startOffset": 134, "endOffset": 138}, {"referenceID": 36, "context": "The notion of forgetting, also known as variable elimination, has been widely investigated in the context of classical logic (propositional and first-order logic) [37, 38, 39] and developed under the notion of logical equivalence, that is, logically equivalent formulas (theories) will remain equivalent after forgetting the same set of propositional variables or literals.", "startOffset": 163, "endOffset": 175}, {"referenceID": 37, "context": "The notion of forgetting, also known as variable elimination, has been widely investigated in the context of classical logic (propositional and first-order logic) [37, 38, 39] and developed under the notion of logical equivalence, that is, logically equivalent formulas (theories) will remain equivalent after forgetting the same set of propositional variables or literals.", "startOffset": 163, "endOffset": 175}, {"referenceID": 38, "context": "The notion of forgetting, also known as variable elimination, has been widely investigated in the context of classical logic (propositional and first-order logic) [37, 38, 39] and developed under the notion of logical equivalence, that is, logically equivalent formulas (theories) will remain equivalent after forgetting the same set of propositional variables or literals.", "startOffset": 163, "endOffset": 175}, {"referenceID": 38, "context": "A similar approach but for reasoning from inconsistent propositional bases is proposed in [39].", "startOffset": 90, "endOffset": 94}, {"referenceID": 39, "context": "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.", "startOffset": 145, "endOffset": 157}, {"referenceID": 40, "context": "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.", "startOffset": 145, "endOffset": 157}, {"referenceID": 41, "context": "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.", "startOffset": 145, "endOffset": 157}, {"referenceID": 42, "context": "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.", "startOffset": 274, "endOffset": 286}, {"referenceID": 43, "context": "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.", "startOffset": 274, "endOffset": 286}, {"referenceID": 44, "context": "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.", "startOffset": 274, "endOffset": 286}, {"referenceID": 45, "context": "Recently, the concept of forgetting has been widespread in other non-classical logic systems from various perspectives such as in logic programs [40, 41, 42] where a semantic forgetting is used instead of developing a number of criteria for forgetting atoms; in modal logic [43, 44, 45] where variable elimination is applied in the context of intelligent agents; and in description logic (DLs) [46] for omitting concepts and roles in knowledge bases.", "startOffset": 394, "endOffset": 398}, {"referenceID": 46, "context": "Forgetting (abstracting from) actions in planning has been also investigated in [47].", "startOffset": 80, "endOffset": 84}, {"referenceID": 47, "context": "Finally, in [48] is proposed a forgetting mechanism for an online learning algorithm to learn sequential data with timelines able to gradually expel the outdated data that could become a possible source of misleading information.", "startOffset": 12, "endOffset": 16}, {"referenceID": 48, "context": "Some studies about episodic memory in humans [49, 50] claim that memory traces in the hippocampus are not permanent and are occasionally transferred to neocortical areas in the brain through a consolidation processes.", "startOffset": 45, "endOffset": 53}, {"referenceID": 49, "context": "Some studies about episodic memory in humans [49, 50] claim that memory traces in the hippocampus are not permanent and are occasionally transferred to neocortical areas in the brain through a consolidation processes.", "startOffset": 45, "endOffset": 53}, {"referenceID": 50, "context": "In fact, this research started when developing our system gErl [52, 53].", "startOffset": 63, "endOffset": 71}, {"referenceID": 51, "context": "In fact, this research started when developing our system gErl [52, 53].", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.", "startOffset": 62, "endOffset": 70}, {"referenceID": 16, "context": ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.", "startOffset": 62, "endOffset": 70}, {"referenceID": 17, "context": ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.", "startOffset": 111, "endOffset": 119}, {"referenceID": 18, "context": ", a rule learner, an inductive logic programming (ILP) system [16, 17] or an inductive programming (IP) system [18, 19]) and an off-the-shelf deductive engine (e.", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "Figure 2 shows an example of Coverage Graph of a well-known ILP problem [16]: the family relationship.", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "its own complexity and the complexity of the rules it covers, a general criterion such as the Minimum Message Length [2] (MML) can be used as a starting criterion from which to derive new metrics.", "startOffset": 117, "endOffset": 120}, {"referenceID": 52, "context": "The Minimum Message Length is one of the most popular selection criterion in inductive inference (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]).", "startOffset": 202, "endOffset": 214}, {"referenceID": 53, "context": "The Minimum Message Length is one of the most popular selection criterion in inductive inference (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]).", "startOffset": 202, "endOffset": 214}, {"referenceID": 21, "context": "The Minimum Message Length is one of the most popular selection criterion in inductive inference (for a formal justification and its relation to Kolmogorov complexity and the related MDL principle, see [54, 55, 22]).", "startOffset": 202, "endOffset": 214}, {"referenceID": 1, "context": "This message can be re-stated in a Bayesian form [2] with the length of the first part of the message (the model) and the length of the second part (evidence covered).", "startOffset": 49, "endOffset": 52}, {"referenceID": 54, "context": "An information-theoretic interpretation of MML is that a given evidence E of probability P (E) can be coded by a message of length L(E) = \u2212log2(P (E)) [56].", "startOffset": 151, "endOffset": 155}, {"referenceID": 55, "context": "In particular, we focus on learning a model of legal moves of different pieces from a set of legal and illegal move examples (extracted from [58]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 50, "context": "We plan to apply the setting to some other applications, by using the same or other deductive and inductive engines, and keep on with the integration into our learning system gErl [52].", "startOffset": 180, "endOffset": 184}], "year": 2015, "abstractText": "The application of cognitive mechanisms to support knowledge acquisition is, from our point of view, crucial for making the resulting models coherent, efficient, credible, easy to use and understandable. In particular, there are two characteristic features of intelligence that are essential for knowledge development: forgetting and consolidation. Both plays an important role in knowledge bases and learning systems to avoid possible information overflow and redundancy, and in order to preserve and strengthen important or frequently used rules and remove (or forget) useless ones. We present an incremental, long-life view of knowledge acquisition which tries to improve task after task by determining what to keep, what to consolidate and what to forget, overcoming The Stability-Plasticity dilemma [1]. In order to do that, we rate rules by introducing several metrics through the first adaptation, to our knowledge, of the Minimum Message Length (MML) principle [2] to a coverage graph, a hierarchical assessment structure which treats evidence and rules in a unified way. The metrics are not only used to forget some of the worst rules, but also to set a consolidation process to promote those selected rules to the knowledge base, which is also mirrored by a demotion system. We evaluate the framework with a series of tasks in a chess rule learning domain.", "creator": "LaTeX with hyperref package"}}}