{"id": "1502.05767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Automatic differentiation in machine learning: a survey", "abstract": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD) is a technique for calculating derivatives efficiently and accurately, established in fields such as computational fluid dynamics, nuclear engineering, and atmospheric sciences. Despite its advantages and use in other fields, machine learning practitioners have been little influenced by AD and make scant use of available tools. We survey the intersection of AD and machine learning, cover applications where AD has the potential to make a big impact, and report on the recent developments in the adoption this technique. We also aim to dispel some misconceptions that we think have impeded the widespread awareness of AD within the machine learning community.", "histories": [["v1", "Fri, 20 Feb 2015 04:20:47 GMT  (70kb,D)", "http://arxiv.org/abs/1502.05767v1", "28 pages, 5 figures"], ["v2", "Sun, 19 Apr 2015 16:49:13 GMT  (79kb,D)", "http://arxiv.org/abs/1502.05767v2", "29 pages, 5 figures"], ["v3", "Thu, 17 Aug 2017 16:45:07 GMT  (69kb)", "http://arxiv.org/abs/1502.05767v3", "34 pages, 5 figures"]], "COMMENTS": "28 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.SC cs.LG", "authors": ["atilim gunes baydin", "barak a pearlmutter", "alexey", "reyevich radul", "jeffrey mark siskind"], "accepted": false, "id": "1502.05767"}, "pdf": {"name": "1502.05767.pdf", "metadata": {"source": "CRF", "title": "Automatic differentiation in machine learning: a survey", "authors": ["At\u0131l\u0131m G\u00fcne\u015f Baydin", "Barak A. Pearlmutter", "Alexey Andreyevich Radul"], "emails": ["atilimgunes.baydin@nuim.ie;", "barak@cs.nuim.ie", "axofch@gmail.com"], "sections": [{"heading": null, "text": "Keywords Automatic differentiation \u00b7 Optimisation \u00b7 Gradient methods \u00b7 Backpropagation"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "2 What AD is not", "text": "It is only a matter of time before such a problem occurs in the United States and other countries of the world. It is only a matter of time before such a problem occurs. (It is only a matter of time before such a problem occurs.) (It is only a matter of time before such a problem occurs.) (It is only a matter of time before such a problem occurs.) (It is a matter of time before such a problem occurs.) (It is a matter of time before such a problem occurs.) (It is a matter of time before such a problem occurs. (It is a matter of time before such a problem occurs.) (It is a matter of time before such a problem occurs.) (It is a matter of time before such a problem occurs.) (It is a matter of time before such a problem occurs.) In its simplest form, it is based on the standard definition of a derivative approach \u2192 (For example, we can approximate a function for each other."}, {"heading": "3 Preliminaries", "text": "In its most basic description, AD relies on the fact that all calculations are ultimately compositions of a finite series of elementary operations for which derivatives are known (Verma, 2000).The combination of derivatives of constituent operations by the chain rule gives the derivative of the general composition. Normally, these elementary operations include the binary operations + and \u00b7 the unspoken character inversion \u2212 the reciprocal, and the standard univariate functions such as exp, sin, and the Like.On the left side of Table 2 we see the representation of the arithmetic operations y = f (x1, x2) = ln (x1).We take the \"tripartite notation\" used by Griewank and Walther (2008), where a function f: Rn \u2192 Rm is constructed using intermediate variables."}, {"heading": "3.1.1 Dual numbers", "text": "A common way to handle the derived operations together with the derived value works as expected in two ways: as an example of using duplicate numbers (x x x), as an example of using duplicate numbers (x x x x), as an example of using duplicate numbers (x x x), as an example of using non-duplicate numbers (x), as an example of using entities such as (x + x) in which the coefficients of symbolic differentiation rules are conveniently mirrored (e.g. Eq. 3), we can use this by setting up a regime in which we use (x) x) x (x) x (x) x (5) and double numbers as data structures to perform the derived value."}, {"heading": "4 Derivatives and machine learning", "text": "\"We must investigate the most important uses of the gradients in the direction of negative gradients and how they can be used by the method AD.D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D \"D.\" D \"D.\" D. \"D.\" D \"D.\" D \"D\" D. \"D\" D \"D\" D. \"D\" D \"D\" D \"D.\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D. \"D\" D \"D\" D. \"D\" D \"D\" D \"D\" D \"D\" D. \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D. \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D. \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D"}, {"heading": "5 Implementations", "text": "Dre rf\u00fc ide eeisrVnlrteeeeoiiiiiiiiiiugnnlrsrrteeoiiiiiiiiiiiiiiiuiuiuuuuuuuuuuuuuuuuuuuuuuuuuueueueueueueueueteeteeceteeceeeceeeceeeceeeceeeceeeeceerrsrteeersrsrsrsrsrteeoioioioioioiueceeceeeceeeeeceeeeeceeeeeceeeeeeceeeeeeceeeeeeceeeeeeeceeeeeeeeerrrrrrsrsrrrsrsrlrteeeoioioioioioioioioioilllllllleeeeceececececeecececececeeceeececeeceeeceeeeceeceeeeeeceeeceeeeceeeeceeeeeeceeeeeceeeeeeeeeeeeeeeeeeeeceeeeeeceeeeeeeeeeceeeeeeceeeeeeeeeerrrrrrrrrreeeerrrreeeerrreeeeeeeeeeeerrrrreeeeeeeeeeeeeeeerrrreeeeeeeeeeeeereeeeeeeeeeee"}, {"heading": "6 Conclusions", "text": "For all its advantages, AD has been remarkably little used by the machine learning community, and we argue that this is mainly because it is poorly understood and often confused with the more well-known symbolic and numerical differentiation methods. By comparison, increasing awareness of AD in areas such as technical design optimization (Hascoe, et al, 2003), computational fluid dynamics (Mu M\u00fcller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence of its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti et al, 2011) showing performance increases of several orders of magnitude. In most cases, machine learning articles tend to present / present the calculation of analytical derivatives for novel models as an important technical achievement."}], "references": [{"title": "A SLANG simulation of an initially strong shock wave downstream of an infinite area change", "author": ["DS Adamson", "CW Winant"], "venue": "Proceedings of the Conference on Applications of Continuous-System Simulation Languages,", "citeRegEx": "Adamson and Winant,? \\Q1969\\E", "shortCiteRegEx": "Adamson and Winant", "year": 1969}, {"title": "Nonlinear system identification for predictive control using continuous time recurrent neural networks and automatic differentiation", "author": ["RK Al Seyab", "Y Cao"], "venue": "Journal of Process Control 18(6):568\u2013581,", "citeRegEx": "Seyab and Cao,? \\Q2008\\E", "shortCiteRegEx": "Seyab and Cao", "year": 2008}, {"title": "A memoryless BFGS neural network training algorithm", "author": ["MS Apostolopoulou", "DG Sotiropoulos", "IE Livieris", "P Pintelas"], "venue": "IEEE International Conference on Industrial Informatics,", "citeRegEx": "Apostolopoulou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Apostolopoulou et al\\.", "year": 2009}, {"title": "Felzenszwalb-Baum-Welch: Event detection by changing appearance", "author": ["DP Barrett", "JM Siskind"], "venue": null, "citeRegEx": "Barrett and Siskind,? \\Q2013\\E", "shortCiteRegEx": "Barrett and Siskind", "year": 2013}, {"title": "Computational graphs and rounding error", "author": ["FL Bauer"], "venue": "SIAM Journal on Numerical Analysis", "citeRegEx": "Bauer,? \\Q1974\\E", "shortCiteRegEx": "Bauer", "year": 1974}, {"title": "Programs for automatic differentiation for the machine BESM (in russian", "author": ["LM Beda", "LN Korolev", "NV Sukkikh", "TS Frolova"], "venue": null, "citeRegEx": "Beda et al\\.,? \\Q1959\\E", "shortCiteRegEx": "Beda et al\\.", "year": 1959}, {"title": "Algorithmic differentiation of implicit functions and optimal values", "author": ["BM Bell", "JV Burke"], "venue": "J (eds) Advances in Automatic Differentiation, Lecture Notes in Computational Science and Engineering,", "citeRegEx": "Bell and Burke,? \\Q2008\\E", "shortCiteRegEx": "Bell and Burke", "year": 2008}, {"title": "FADBAD, a flexible C++ package for automatic differentiation", "author": ["C Bendtsen", "O Stauning"], "venue": "Technical Report IMM-REP-1996-17, Department of Mathematical Modelling,", "citeRegEx": "Bendtsen and Stauning,? \\Q1996\\E", "shortCiteRegEx": "Bendtsen and Stauning", "year": 1996}, {"title": "Differential quadrature method in computational mechanics: A review", "author": ["CW Bert", "M Malik"], "venue": "Applied Mechanics Reviews 49,", "citeRegEx": "Bert and Malik,? \\Q1996\\E", "shortCiteRegEx": "Bert and Malik", "year": 1996}, {"title": "Ill-posed problems in early vision", "author": ["M Bertero", "T Poggio", "V Torre"], "venue": "Proceedings of the IEEE", "citeRegEx": "Bertero et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Bertero et al\\.", "year": 1988}, {"title": "COSY INFINITY and its applications in nonlinear dynamics", "author": ["M Berz", "K Makino", "K Shamseddine", "GH Hoffst\u00e4tter", "W Wan"], "venue": null, "citeRegEx": "Berz et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berz et al\\.", "year": 1996}, {"title": "ADIC: An extensible automatic differentiation tool for ANSI-C. Software Practice and Experience", "author": ["C Bischof", "L Roh", "A Mauer"], "venue": null, "citeRegEx": "Bischof et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bischof et al\\.", "year": 1997}, {"title": "Automatic differentiation for computational finance", "author": ["CH Bischof", "HM B\u00fccker", "B Lang"], "venue": "Computational Methods in DecisionMaking, Economics and Finance, Applied Optimization,", "citeRegEx": "Bischof et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bischof et al\\.", "year": 2002}, {"title": "On the implementation of automatic differentiation tools. Higher-Order and Symbolic Computation", "author": ["CH Bischof", "PD Hovland", "B Norris"], "venue": null, "citeRegEx": "Bischof et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bischof et al\\.", "year": 2008}, {"title": "Online learning and stochastic approximations. On-line learning in neural networks", "author": ["L Bottou"], "venue": null, "citeRegEx": "Bottou,? \\Q1998\\E", "shortCiteRegEx": "Bottou", "year": 1998}, {"title": "Extrapolation Methods: Theory and Practice", "author": ["C Brezinski", "MR Zaglia"], "venue": null, "citeRegEx": "Brezinski and Zaglia,? \\Q1991\\E", "shortCiteRegEx": "Brezinski and Zaglia", "year": 1991}, {"title": "A steepest ascent method for solving optimum programming problems", "author": ["Bryson AE Jr."], "venue": "Journal of Applied Mechanics", "citeRegEx": "Jr,? \\Q1962\\E", "shortCiteRegEx": "Jr", "year": 1962}, {"title": "Applied optimal control", "author": ["Bryson AE Jr.", "Ho YC"], "venue": null, "citeRegEx": "Jr and YC,? \\Q1969\\E", "shortCiteRegEx": "Jr and YC", "year": 1969}, {"title": "Fast greeks by algorithmic differentiation", "author": ["L Capriotti"], "venue": "Journal of Computational Finance", "citeRegEx": "Capriotti,? \\Q2011\\E", "shortCiteRegEx": "Capriotti", "year": 2011}, {"title": "Sensitivity analysis for atmospheric chemistry models via automatic differentiation", "author": ["GR Carmichael", "A Sandu"], "venue": "Atmospheric Environment", "citeRegEx": "Carmichael and Sandu,? \\Q1997\\E", "shortCiteRegEx": "Carmichael and Sandu", "year": 1997}, {"title": "Inverse problems in image processing and image segmentation: some mathematical and numerical aspects", "author": ["A Chambolle"], "venue": null, "citeRegEx": "Chambolle,? \\Q2000\\E", "shortCiteRegEx": "Chambolle", "year": 2000}, {"title": "Efficient adjoint derivatives: application to the meteorological model meso-nh. Optimization Methods and Software", "author": ["I Charpentier", "M Ghemires"], "venue": null, "citeRegEx": "Charpentier and Ghemires,? \\Q2000\\E", "shortCiteRegEx": "Charpentier and Ghemires", "year": 2000}, {"title": "Application of differentiation arithmetic", "author": ["GC Corliss"], "venue": "Perspectives in Computing,", "citeRegEx": "Corliss,? \\Q1988\\E", "shortCiteRegEx": "Corliss", "year": 1988}, {"title": "Histograms of oriented gradients for human detection", "author": ["N Dalal", "B Triggs"], "venue": "Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "Dalal and Triggs,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs", "year": 2005}, {"title": "The data-flow equations of checkpointing in reverse automatic differentiation", "author": ["B Dauvergne", "L Hasco\u00ebt"], "venue": "J (eds) Computational Science \u2013 ICCS", "citeRegEx": "Dauvergne and Hasco\u00ebt,? \\Q2006\\E", "shortCiteRegEx": "Dauvergne and Hasco\u00ebt", "year": 2006}, {"title": "Numerical Methods for Unconstrained Optimization and Nonlinear Equations", "author": ["JE Dennis", "RB Schnabel"], "venue": "Classics in Applied Mathematics,", "citeRegEx": "Dennis and Schnabel,? \\Q1996\\E", "shortCiteRegEx": "Dennis and Schnabel", "year": 1996}, {"title": "Use of automatic differentiation for calculating hessians and newton steps. In: Griewank A, Corliss GF (eds) Automatic Differentiation of Algorithms: Theory, Implementation, and Application", "author": ["LC Dixon"], "venue": null, "citeRegEx": "Dixon,? \\Q1991\\E", "shortCiteRegEx": "Dixon", "year": 1991}, {"title": "Arbitrary-order density functional response theory from automatic differentiation", "author": ["U Ekstr\u00f6m", "L Visscher", "R Bast", "AJ Thorvaldsen", "K Ruud"], "venue": "Journal of Chemical Theory and Computation 6:1971\u201380,", "citeRegEx": "Ekstr\u00f6m et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ekstr\u00f6m et al\\.", "year": 2010}, {"title": "Regularization tools for training large feed-forward neural networks using automatic differentiation", "author": ["J Eriksson", "M Gulliksson", "P Lindstr\u00f6m", "P Wedin"], "venue": "Optimization Methods and Software 10(1):49\u201369,", "citeRegEx": "Eriksson et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 1998}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["JR Finkel", "A Kleeman", "CD Manning"], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Finkel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Numerical differentiation of analytic functions", "author": ["B Fornberg"], "venue": "ACM Transactions on Mathematical Software 7(4):512\u201326,", "citeRegEx": "Fornberg,? \\Q1981\\E", "shortCiteRegEx": "Fornberg", "year": 1981}, {"title": "An efficient overloaded implementation of forward mode automatic differentiation in MATLAB", "author": ["SA Forth"], "venue": "ACM Transactions on Mathematical Software", "citeRegEx": "Forth,? \\Q2006\\E", "shortCiteRegEx": "Forth", "year": 2006}, {"title": "AMPL: A Modeling Language for Mathematical Programming", "author": ["R Fourer", "DM Gay", "BW Kernighan"], "venue": null, "citeRegEx": "Fourer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Fourer et al\\.", "year": 2002}, {"title": "Automatically finding and exploiting partially separable structure in nonlinear programming problems", "author": ["DM Gay"], "venue": null, "citeRegEx": "Gay,? \\Q1996\\E", "shortCiteRegEx": "Gay", "year": 1996}, {"title": "Efficient computation of sparse hessians using coloring and automatic differentiation", "author": ["A Gebremedhin", "A Pothen", "A Tarafdar", "A Walther"], "venue": "INFORMS Journal on Computing", "citeRegEx": "Gebremedhin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gebremedhin et al\\.", "year": 2009}, {"title": "Recipes for adjoint code construction", "author": ["R Giering", "T Kaminski"], "venue": "ACM Transactions on Mathematical Software 24:437\u201374,", "citeRegEx": "Giering and Kaminski,? \\Q1998\\E", "shortCiteRegEx": "Giering and Kaminski", "year": 1998}, {"title": "Smoking adjoints: fast monte carlo greeks", "author": ["M Giles", "P Glasserman"], "venue": null, "citeRegEx": "Giles and Glasserman,? \\Q2006\\E", "shortCiteRegEx": "Giles and Glasserman", "year": 2006}, {"title": "Distributed asynchronous online learning for natural language processing", "author": ["K Gimpel", "D Das", "NA Smith"], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Gimpel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2010}, {"title": "Pontryagin LS (1960) The theory of optimal processes I: The maximum principle", "author": ["VG Goltyanskii", "RV Gamkrelidze"], "venue": "Invest Akad Nauk SSSR Ser Mat", "citeRegEx": "Goltyanskii and Gamkrelidze,? \\Q1960\\E", "shortCiteRegEx": "Goltyanskii and Gamkrelidze", "year": 1960}, {"title": "The principles and practice of probabilistic programming", "author": ["ND Goodman"], "venue": "Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,", "citeRegEx": "Goodman,? \\Q2013\\E", "shortCiteRegEx": "Goodman", "year": 2013}, {"title": "Optimal convergence of on-line backpropagation", "author": ["M Gori", "M Maggini"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "Gori and Maggini,? \\Q1996\\E", "shortCiteRegEx": "Gori and Maggini", "year": 1996}, {"title": "Computer Algebra Handbook: Foundations", "author": ["J Grabmeier", "E Kaltofen", "VB Weispfenning"], "venue": null, "citeRegEx": "Grabmeier et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Grabmeier et al\\.", "year": 2003}, {"title": "Automatic differentiation for gpu-accelerated", "author": ["T Pock", "T Gross", "B Kainz"], "venue": null, "citeRegEx": "Grabner et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grabner et al\\.", "year": 2008}, {"title": "The Tapenade Automatic Differentiation tool: Principles", "author": ["L Hasco\u00ebt", "V Pascual"], "venue": null, "citeRegEx": "Hasco\u00ebt and Pascual,? \\Q2013\\E", "shortCiteRegEx": "Hasco\u00ebt and Pascual", "year": 2013}, {"title": "Automatic differentiation for optimum design", "author": ["L Hasco\u00ebt", "M V\u00e1zquez", "A Dervieux"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "A (2014) The no-U-turn sampler: Adaptively setting path lengths", "author": ["Tech. rep", "Lawrence Berkeley Lab", "CA Hoffman MD", "Gelman"], "venue": null, "citeRegEx": "rep. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "rep. et al\\.", "year": 2014}, {"title": "Journal of Machine Learning Research 15:1593\u20131623", "author": ["Monte Carlo"], "venue": "Horwedel JE, Worley BA, Oblow EM, Pin FG", "citeRegEx": "Carlo.,? \\Q1988\\E", "shortCiteRegEx": "Carlo.", "year": 1988}, {"title": "PADRE2, version 1\u2014user\u2019s manual", "author": ["K Kubo", "M Iri"], "venue": "Industrial and Applied Mathematics, Philadelphia,", "citeRegEx": "Kubo and Iri,? \\Q1990\\E", "shortCiteRegEx": "Kubo and Iri", "year": 1990}, {"title": "On the performance of discrete adjoint cfd codes using automatic", "author": ["Laboratory", "Batavia", "IL M\u00fcller JD", "Cusdin P"], "venue": null, "citeRegEx": "Laboratory et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Laboratory et al\\.", "year": 2005}, {"title": "Computing adjoints with the NAGWare Fortran 95 compiler", "author": ["U Naumann", "J Riehme"], "venue": null, "citeRegEx": "Naumann and Riehme,? \\Q2005\\E", "shortCiteRegEx": "Naumann and Riehme", "year": 2005}, {"title": "Probabilistic inference using markov chain monte carlo methods", "author": ["R Neal"], "venue": "Science and Engineering,", "citeRegEx": "Neal,? \\Q1993\\E", "shortCiteRegEx": "Neal", "year": 1993}, {"title": "Analytical differentiation on a digital computer", "author": ["JF Nolan"], "venue": "Master\u2019s thesis Ostiguy JF, Michelotti L", "citeRegEx": "Nolan,? \\Q1953\\E", "shortCiteRegEx": "Nolan", "year": 1953}, {"title": "Reverse-mode AD in a functional framework: Lambda", "author": ["BA Pearlmutter", "JM Siskind"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Algorithmic differentiation: Application to variational", "author": ["T Pock", "M Pock", "H Bischof"], "venue": null, "citeRegEx": "Pock et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pock et al\\.", "year": 2007}, {"title": "Optimization of neural network feedback control systems using automatic", "author": ["E Rollins"], "venue": "Engineering,", "citeRegEx": "Rollins,? \\Q2009\\E", "shortCiteRegEx": "Rollins", "year": 2009}, {"title": "A general framework for parallel", "author": ["DE Rumelhart", "GE Hinton", "JL McClelland"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "The Image Processing Handbook", "author": ["M Sambridge", "P Rickwood", "N Rawlinson", "S Sommacal"], "venue": "JC", "citeRegEx": "Sambridge et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sambridge et al\\.", "year": 2010}, {"title": "Perturbation confusion and referential transparency", "author": ["At\u0131l\u0131m G\u00fcne\u015f Baydin"], "venue": "Siskind JM, Pearlmutter BA", "citeRegEx": "Baydin,? \\Q2005\\E", "shortCiteRegEx": "Baydin", "year": 2005}, {"title": "Optimization for Machine Learning", "author": ["S Sra", "S Nowozin", "SJ Wright"], "venue": "PhD thesis,", "citeRegEx": "Sra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sra et al\\.", "year": 2011}, {"title": "An introduction to automatic differentiation", "author": ["SVN Vishwanathan", "NN Schraudolph", "MW Schmidt", "KP Murphy"], "venue": "Current Science", "citeRegEx": "Vishwanathan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Vishwanathan et al\\.", "year": 2000}, {"title": "Getting started with ADOL-C", "author": ["A Walther", "A Griewank"], "venue": null, "citeRegEx": "Walther and Griewank,? \\Q2012\\E", "shortCiteRegEx": "Walther and Griewank", "year": 2012}, {"title": "Nonstandard interpretations", "author": ["ND Goodman", "A Stuhlm\u00fcller", "JM Siskind"], "venue": null, "citeRegEx": "Wingate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wingate et al\\.", "year": 2011}, {"title": "Application of PID controller based on BP neural", "author": ["W Yang", "Y Zhao", "L Yan", "X Chen"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2008}, {"title": "Grounded language learning from video described with sentences", "author": ["H Yu", "JM Siskind"], "venue": null, "citeRegEx": "Yu and Siskind,? \\Q2013\\E", "shortCiteRegEx": "Yu and Siskind", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "When introducing new models, machine learning researchers spend considerable effort on the manual derivation of analytical derivatives and subsequently plug these into standard optimization procedures such as L-BFGS (Zhu et al, 1997) or stochastic gradient descent (Bottou, 1998).", "startOffset": 265, "endOffset": 279}, {"referenceID": 22, "context": "Symbolic differentiation addresses the weaknesses of both the manual and numerical methods, but often results in complex and cryptic expressions plagued with the problem of \u201cexpression swell\u201d (Corliss, 1988).", "startOffset": 192, "endOffset": 207}, {"referenceID": 19, "context": "Because of its generality, AD is an already established tool in applications including realparameter optimization (Walther, 2007), sensitivity analysis (Carmichael and Sandu, 1997), physical modeling (Ekstr\u00f6m et al, 2010), and probabilistic inference (Neal, 2011).", "startOffset": 152, "endOffset": 180}, {"referenceID": 40, "context": "In simplest terms, backpropagation models learning as gradient descent in neural network weight space, looking for the minimum of an error function (Gori and Maggini, 1996).", "startOffset": 148, "endOffset": 172}, {"referenceID": 14, "context": "When introducing new models, machine learning researchers spend considerable effort on the manual derivation of analytical derivatives and subsequently plug these into standard optimization procedures such as L-BFGS (Zhu et al, 1997) or stochastic gradient descent (Bottou, 1998). Manual differentiation is evidently time consuming and prone to error. Of the other alternatives, numerical differentiation is simple to implement, but susceptible to round-off and truncation errors that make it inherently unstable (Jerrell, 1997). Symbolic differentiation addresses the weaknesses of both the manual and numerical methods, but often results in complex and cryptic expressions plagued with the problem of \u201cexpression swell\u201d (Corliss, 1988). Furthermore, manual and symbolic methods require the model to be expressed as a closed-form mathematical formula, ruling out algorithmic control flow and severely limiting expressivity. We are concerned with the powerful fourth technique, automatic differentiation (AD), which works by systematically applying the chain rule of differential calculus at the elementary operator level. AD allows the accurate evaluation of derivatives in machine precision, with only a small constant factor of overhead and ideal asymptotic efficiency. In contrast with the effort involved in arranging code into closed-form expressions for symbolic differentiation, AD can usually be applied to existing code with minimal change. Because of its generality, AD is an already established tool in applications including realparameter optimization (Walther, 2007), sensitivity analysis (Carmichael and Sandu, 1997), physical modeling (Ekstr\u00f6m et al, 2010), and probabilistic inference (Neal, 2011). Despite its widespread use in other fields, AD has been underused, if not unknown, by the machine learning community. As it happens, AD and machine learning practice are conceptually very closely interconnected: consider the backpropagation method for training neural networks, which has a colorful history of being rediscovered several times by independent researchers (Widrow and Lehr, 1990). It has been one of the most studied and used training algorithms since the day it became popular mainly through the work of Rumelhart et al (1986). In simplest terms, backpropagation models learning as gradient descent in neural network weight space, looking for the minimum of an error function (Gori and Maggini, 1996).", "startOffset": 266, "endOffset": 2258}, {"referenceID": 30, "context": "Numerical approximations of derivatives are inherently ill-conditioned and unstable, with the exception of complex variable methods that are applicable to a limited set of holomorphic functions (Fornberg, 1981).", "startOffset": 194, "endOffset": 210}, {"referenceID": 15, "context": "Other techniques for improving numerical differentiation, including higherorder finite differences, Richardson extrapolation to the limit (Brezinski and Zaglia, 1991), and differential quadrature methods using weighted sums (Bert and Malik, 1996), also increase rapidly in programming complexity, do not completely eliminate approximation errors, and remain highly susceptible to floating point truncation.", "startOffset": 138, "endOffset": 166}, {"referenceID": 8, "context": "Other techniques for improving numerical differentiation, including higherorder finite differences, Richardson extrapolation to the limit (Brezinski and Zaglia, 1991), and differential quadrature methods using weighted sums (Bert and Malik, 1996), also increase rapidly in programming complexity, do not completely eliminate approximation errors, and remain highly susceptible to floating point truncation.", "startOffset": 224, "endOffset": 246}, {"referenceID": 4, "context": "A given trace of elementary operations can be also represented using a computational graph (Bauer, 1974), as shown in Fig.", "startOffset": 91, "endOffset": 104}, {"referenceID": 25, "context": "Compared with the straightforward simplicity of the forward mode, reverse mode AD can, at first, appear somewhat \u201cmysterious\u201d (Dennis and Schnabel, 1996).", "startOffset": 126, "endOffset": 153}, {"referenceID": 25, "context": "Compared with the straightforward simplicity of the forward mode, reverse mode AD can, at first, appear somewhat \u201cmysterious\u201d (Dennis and Schnabel, 1996). Griewank and Walther (2008) argue that this is in part because of the common acquaintance with the chain rule as a mechanical procedure propagating derivatives forward.", "startOffset": 127, "endOffset": 183}, {"referenceID": 24, "context": "It is an active area of research to improve storage requirements in implementations, by methods such as checkpointing strategies and data-flow analysis (Dauvergne and Hasco\u00ebt, 2006).", "startOffset": 152, "endOffset": 181}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959).", "startOffset": 57, "endOffset": 88}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD.", "startOffset": 58, "endOffset": 206}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD.", "startOffset": 58, "endOffset": 359}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation.", "startOffset": 58, "endOffset": 940}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation.", "startOffset": 58, "endOffset": 961}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation.", "startOffset": 58, "endOffset": 1267}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation.", "startOffset": 58, "endOffset": 1306}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation.", "startOffset": 58, "endOffset": 1324}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation. Within the machine learning community, the method has been reinvented several times, such as by Parker (1985), until it was eventually brought to fame by Rumelhart et al (1986) of the Parallel Distributed Processing (PDP) group.", "startOffset": 58, "endOffset": 1490}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation. Within the machine learning community, the method has been reinvented several times, such as by Parker (1985), until it was eventually brought to fame by Rumelhart et al (1986) of the Parallel Distributed Processing (PDP) group.", "startOffset": 58, "endOffset": 1557}, {"referenceID": 25, "context": "ular such method is the BFGS algorithm, together with its limited-memory variant L-BFGS (Dennis and Schnabel, 1996).", "startOffset": 88, "endOffset": 115}, {"referenceID": 26, "context": "This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009).", "startOffset": 118, "endOffset": 131}, {"referenceID": 33, "context": "This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009).", "startOffset": 154, "endOffset": 165}, {"referenceID": 25, "context": "ular such method is the BFGS algorithm, together with its limited-memory variant L-BFGS (Dennis and Schnabel, 1996). AD here provides a way of computing the exact Hessian in an efficient way. However, in many cases, one does not need the full Hessian but only a Hessian-vector product H r, which can be computed very efficiently using a combination of the forward and reverse modes of AD. This computes H r with O(n) complexity, even though the H is a n\u00d7n matrix. Moreover, Hessians arising in large-scale applications are typically sparse. This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009). Another approach for improving the rate of convergence of gradient methods is to use gain adaptation methods such as stochastic meta-descent (SMD) (Schraudolph, 1999), where stochastic sampling is introduced to avoid local minima. An example using SMD with AD Hessian-vector products is given by Vishwanathan et al (2006) on conditional random fields (CRF), a probabilistic method for labeling and segmenting data.", "startOffset": 89, "endOffset": 1093}, {"referenceID": 25, "context": "ular such method is the BFGS algorithm, together with its limited-memory variant L-BFGS (Dennis and Schnabel, 1996). AD here provides a way of computing the exact Hessian in an efficient way. However, in many cases, one does not need the full Hessian but only a Hessian-vector product H r, which can be computed very efficiently using a combination of the forward and reverse modes of AD. This computes H r with O(n) complexity, even though the H is a n\u00d7n matrix. Moreover, Hessians arising in large-scale applications are typically sparse. This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009). Another approach for improving the rate of convergence of gradient methods is to use gain adaptation methods such as stochastic meta-descent (SMD) (Schraudolph, 1999), where stochastic sampling is introduced to avoid local minima. An example using SMD with AD Hessian-vector products is given by Vishwanathan et al (2006) on conditional random fields (CRF), a probabilistic method for labeling and segmenting data. Similarly, Schraudolph and Graepel (2003) use Hessian-vector products in their model combining conjugate gradient techniques with stochastic gradient descent.", "startOffset": 89, "endOffset": 1228}, {"referenceID": 54, "context": "Similarly, Rollins (2009) uses reverse mode AD in conjunction with neural networks for the problem of optimal feedback control.", "startOffset": 11, "endOffset": 26}, {"referenceID": 1, "context": "An example is given for continuous time recurrent neural networks (CTRNN) by Al Seyab and Cao (2008), where they apply AD for the training of CTRNNs predicting dynamic behavior of nonlinear processes in real time.", "startOffset": 80, "endOffset": 101}, {"referenceID": 20, "context": "On the other hand, in computer vision, many problems are formulated as the minimization of an appropriate energy functional (Bertero et al, 1988; Chambolle, 2000).", "startOffset": 124, "endOffset": 162}, {"referenceID": 19, "context": "On the other hand, in computer vision, many problems are formulated as the minimization of an appropriate energy functional (Bertero et al, 1988; Chambolle, 2000). This minimization is usually accomplished via calculus of variations and the Euler-Lagrange equation. In this area, the first study introducing AD to computer vision is given by Pock et al (2007), where they address the problems of denoising, segmentation, and recovery of information from stereoscopic image pairs, and note the usefulness of AD in identifying sparsity patterns in large Jacobian and Hessian matrices.", "startOffset": 146, "endOffset": 360}, {"referenceID": 19, "context": "On the other hand, in computer vision, many problems are formulated as the minimization of an appropriate energy functional (Bertero et al, 1988; Chambolle, 2000). This minimization is usually accomplished via calculus of variations and the Euler-Lagrange equation. In this area, the first study introducing AD to computer vision is given by Pock et al (2007), where they address the problems of denoising, segmentation, and recovery of information from stereoscopic image pairs, and note the usefulness of AD in identifying sparsity patterns in large Jacobian and Hessian matrices. In another study, Grabner et al (2008) use reverse mode AD for GPUaccelerated medical 2D/3D registration, a task involving the alignment of data from different sources such as X-ray images or computed tomography.", "startOffset": 146, "endOffset": 622}, {"referenceID": 3, "context": "Barrett and Siskind (2013) present a use of AD for the task of video event detection.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "Barrett and Siskind (2013) present a use of AD for the task of video event detection. Compared with general computer vision tasks focused on recognizing objects and their properties (which can be thought of as nouns in a narrative), an important aspect of this work is that it aims to recognize and reason about events and actions (i.e. verbs). The method uses Hidden Markov Models (HMMs) and Dalal and Triggs (2005) object detectors, and performs training on a corpus of pre-tracked video by an adaptive step size naive gradient descent algorithm, where gradient computations are done with reverse mode AD.", "startOffset": 0, "endOffset": 417}, {"referenceID": 44, "context": "Improvements in training time can be realized by using online or distributed training algorithms (Gimpel et al, 2010). An example using stochastic gradient descent for NLP is given by Finkel et al (2008) optimizing conditional random field parsers through an objective function.", "startOffset": 114, "endOffset": 204}, {"referenceID": 44, "context": "Improvements in training time can be realized by using online or distributed training algorithms (Gimpel et al, 2010). An example using stochastic gradient descent for NLP is given by Finkel et al (2008) optimizing conditional random field parsers through an objective function. Related with the work on video event detection in the previous section, Yu and Siskind (2013) report their work on sentence tracking, representing an instance of grounded language learning paired with computer vision, where the system learns word meanings from short video clips paired with descriptive sentences.", "startOffset": 114, "endOffset": 373}, {"referenceID": 39, "context": "Probabilistic programming has been experiencing a recent resurgence thanks to new algorithmic advances for probabilistic inference and new areas of application in machine learning (Goodman, 2013).", "startOffset": 180, "endOffset": 195}, {"referenceID": 50, "context": "When model parameters are continuous, the Hamiltonian\u2014or, hybrid\u2014 Monte Carlo (HMC) algorithm provides improved convergence characteristics avoiding the slow exploration of random sampling, by simulating Hamiltonian dynamics through auxiliary \u201cmomentum variables\u201d (Neal, 1993).", "startOffset": 264, "endOffset": 276}, {"referenceID": 39, "context": "Probabilistic programming has been experiencing a recent resurgence thanks to new algorithmic advances for probabilistic inference and new areas of application in machine learning (Goodman, 2013). A probabilistic programming language provides primitive language constructs for random choice and allows the automatic probabilistic inference of distributions specified by programs. Inference techniques can be static, such as compiling programs to Bayesian networks and using algorithms such as belief propagation for inference; or they can be dynamic, executing programs several times and computing statistics on observed values to infer distributions. Markov chain Monte Carlo (MCMC) methods are typically used for dynamic inference, such as the MetropolisHastings algorithm based on random sampling. Meyer et al (2003) give an example of how AD can be used for speeding up Bayesian posterior inference in MCMC, with an application in stochastic volatility.", "startOffset": 181, "endOffset": 820}, {"referenceID": 47, "context": "Classical instances of source code transformation include the Fortran preprocessors GRESS (Horwedel et al, 1988) and PADRE2 (Kubo and Iri, 1990), which transform AD-enabled variants of Fortran into standard Fortran 77 before compiling.", "startOffset": 124, "endOffset": 144}, {"referenceID": 47, "context": "Classical instances of source code transformation include the Fortran preprocessors GRESS (Horwedel et al, 1988) and PADRE2 (Kubo and Iri, 1990), which transform AD-enabled variants of Fortran into standard Fortran 77 before compiling. Similarly, the ADIFOR tool by Bischof et al (1996), given a", "startOffset": 125, "endOffset": 287}, {"referenceID": 43, "context": "A recent and popular tool also utilizing this approach is Tapenade (Pascual and Hasco\u00ebt, 2008; Hasco\u00ebt and Pascual, 2013), implementing forward and reverse mode AD for Fortran and C programs.", "startOffset": 67, "endOffset": 121}, {"referenceID": 0, "context": "Some of the earliest AD tools such as SLANG (Adamson and Winant, 1969) and PROSE (Pfeiffer, 1987) belong to this category.", "startOffset": 44, "endOffset": 70}, {"referenceID": 49, "context": "The NAGWare Fortran 95 compiler (Naumann and Riehme, 2005) is a more recent example, where the use of ADrelated extensions triggers automatic generation of derivative code at compile time.", "startOffset": 32, "endOffset": 58}, {"referenceID": 60, "context": "A highly popular tool implemented with operator overloading in C++ is ADOL-C (Walther and Griewank, 2012).", "startOffset": 77, "endOffset": 105}, {"referenceID": 7, "context": "The FADBAD++ library (Bendtsen and Stauning, 1996) implements AD for C++ using templates and oper-", "startOffset": 21, "endOffset": 50}, {"referenceID": 21, "context": "In comparison, increasing awareness of AD in fields such as engineering design optimization (Hasco\u00ebt et al, 2003), computational fluid dynamics (M\u00fcller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence for its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti, 2011) reporting performance increases of several orders of magnitude.", "startOffset": 183, "endOffset": 215}, {"referenceID": 36, "context": "In comparison, increasing awareness of AD in fields such as engineering design optimization (Hasco\u00ebt et al, 2003), computational fluid dynamics (M\u00fcller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence for its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti, 2011) reporting performance increases of several orders of magnitude.", "startOffset": 331, "endOffset": 399}, {"referenceID": 18, "context": "In comparison, increasing awareness of AD in fields such as engineering design optimization (Hasco\u00ebt et al, 2003), computational fluid dynamics (M\u00fcller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence for its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti, 2011) reporting performance increases of several orders of magnitude.", "startOffset": 331, "endOffset": 399}], "year": 2015, "abstractText": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD) is a technique for calculating derivatives efficiently and accurately, established in fields such as computational fluid dynamics, nuclear engineering, and atmospheric sciences. Despite its advantages and use in other fields, machine learning practitioners have been little influenced by AD and make scant use of available tools. We survey the intersection of AD and machine learning, cover applications where AD has the potential to make a big impact, and report on the recent developments in the adoption this technique. We also aim to dispel some misconceptions that we think have impeded the widespread awareness of AD within the machine learning community.", "creator": "LaTeX with hyperref package"}}}