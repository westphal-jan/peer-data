{"id": "1502.05777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Spike Event Based Learning in Neural Networks", "abstract": "A scheme is derived for learning connectivity in spiking neural networks. The scheme learns instantaneous firing rates that are conditional on the activity in other parts of the network. The scheme is independent of the choice of neuron dynamics or activation function, and network architecture. It involves two simple, online, local learning rules that are applied only in response to occurrences of spike events. This scheme provides a direct method for transferring ideas between the fields of deep learning and computational neuroscience. This learning scheme is demonstrated using a layered feedforward spiking neural network trained self-supervised on a prediction and classification task for moving MNIST images collected using a Dynamic Vision Sensor.", "histories": [["v1", "Fri, 20 Feb 2015 05:26:09 GMT  (4442kb,D)", "http://arxiv.org/abs/1502.05777v1", "Figure 4 can be viewed as a movie in a separate file"]], "COMMENTS": "Figure 4 can be viewed as a movie in a separate file", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["james a henderson", "tingting a gibson", "janet wiles"], "accepted": false, "id": "1502.05777"}, "pdf": {"name": "1502.05777.pdf", "metadata": {"source": "CRF", "title": "Spike Event Based Learning in Neural Networks", "authors": ["J. A. Henderson", "T. A. Gibson", "J. Wiles"], "emails": [], "sections": [{"heading": null, "text": "The scheme is independent of the choice of neuron dynamics or activation function and the network architecture. It includes two simple, online applied, local learning rules that are applied only in response to the occurrence of spike events. This scheme provides a direct method of transferring ideas between deep learning and computer neuroscience. This learning scheme is demonstrated using a layered feedback forward spiking neural network that is trained on the basis of a prediction and classification task for moving MNIST images collected using a Dynamic Vision Sensor. Keywords: Spiking Neural Networks, Learning, Vision, Prediction"}, {"heading": "1. Introduction", "text": "This year, it is time for us to set out in search of new ways to travel the world, to travel the world."}, {"heading": "2. Learning Theory", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is not a country in which it is a country in which it is a country, a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "3. Application to Learning Layers of Autoencoders", "text": "A standard method for forming an unattended deep feedback network is to train each pair of layers one after the other as autoencoders [6], so that each layer encodes the activity of the underlying layer, see Fig. 3. The learning rules described in Fig. 2 can be used to learn layers of autoencoders by replacing the monitored output neuron with an i input neuron that monitors itself, and by reversing the direction of connectivity so that I learn to output Q (i | H), with H now being the future activity of the hidden neurons in the layer above i, as the causality from the previous case is reversed; the input layer causes activity in the hidden layer above, see Fig. 2.With this method, the hidden layers learn so that by observing a period of hidden layer activity the activity of the layer below the layer, the activity below the layer of the prediction can be deactivated."}, {"heading": "3.1. Weight Update Rules", "text": "We have developed rules for learning a value Qo in order to approximate Q (o | H); however, we have not yet discussed any rules for changing W that are necessary for implementation in a network. Before these rules can be established, the formula for calculating Qo of H and W. The most common choice is to use the product of H and W, which is summarized across all neurons in the layer below and over time in the case of time-delay connections. We use the same choice hereQo (t) wj (t) wj (t) dt \u2032), (17) where hj is a hidden neuron connected to o and wj, the corresponding connection is between them. Other choices are possible and may have advantages over this choice, although this will be left for future studies. We also have to choose a parameterization for W."}, {"heading": "3.2. DVS MNIST Event Based Dataset", "text": "In fact, the majority of people living in the USA live in the USA, in the USA and in Europe, in Europe, in Europe and in the USA, in the USA, in Europe and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in Europe and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA and in the USA"}, {"heading": "4. Summary", "text": "This paper presents an event-based learning scheme for neural networks, which does not depend on the specific form of neural dynamics or activation function, and while this paper focuses on the formation of spiking neural networks, this scheme can also be used to train traditional artificial neural networks, especially those that incorporate discontinuous activation functions that inhibit gradient descendance methods, and the scheme can also be applied to networks of neurons that contain biologically inspired dynamics. Future work in this direction could influence theories of brain dynamics and learning, and the broad applicability of this learning scheme provides an opportunity to directly apply ideas from deep learning and computer-assisted neuroscience, thereby strengthening and influencing theoretical progress in both areas."}], "references": [{"title": "L", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg"], "venue": "Fei-Fei, Imagenet large scale visual recognition challenge ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on 20 (1) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "New types of deep neural network learning for speech recognition and related applications: an overview", "author": ["L. Deng", "G. Hinton", "B. Kingsbury"], "venue": "in: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE 29 (6) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 35 (8) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Learning", "author": ["Y. Bengio", "I. Goodfellow", "A. Courville"], "venue": "MIT Press ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61 (0) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "in: Proceedings of the 27th International Conference on Machine Learning (ICML-10)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML 28 (3) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res. 15 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res. 11 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": "Nature 323 (6088) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1986}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput. 18 (7) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86 (11) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "B", "author": ["T. Serrano-Gotarredona"], "venue": "Linares-Barranco, A 128 \u00d7 128 1.5% contrast sensitivity 0.9% FPN 3 \u03bcs latency 4 mW asynchronous frame-free dynamic vision sensor using transimpedance preamplifiers, Solid-State Circuits, IEEE Journal of 48 (3) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Spiking activity propagation in neuronal networks: reconciling different perspectives on neural coding", "author": ["A. Kumar", "S. Rotter", "A. Aertsen"], "venue": "Nat. Rev. Neurosc. 11 (9) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamical Systems in Neuroscience", "author": ["E. Izhikevich"], "venue": "MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].", "startOffset": 207, "endOffset": 210}, {"referenceID": 0, "context": "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].", "startOffset": 229, "endOffset": 232}, {"referenceID": 1, "context": "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].", "startOffset": 256, "endOffset": 265}, {"referenceID": 2, "context": "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].", "startOffset": 256, "endOffset": 265}, {"referenceID": 3, "context": "Methods in deep learning for training neural networks (NNs) have been very successfully applied to a range of datasets, performing tasks at levels approaching human performance, such as image classification [1], object detection [1] and speech recognition [2, 3, 4].", "startOffset": 256, "endOffset": 265}, {"referenceID": 4, "context": "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].", "startOffset": 140, "endOffset": 149}, {"referenceID": 5, "context": "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].", "startOffset": 140, "endOffset": 149}, {"referenceID": 6, "context": "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].", "startOffset": 140, "endOffset": 149}, {"referenceID": 7, "context": "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].", "startOffset": 256, "endOffset": 259}, {"referenceID": 8, "context": "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].", "startOffset": 386, "endOffset": 393}, {"referenceID": 9, "context": "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].", "startOffset": 386, "endOffset": 393}, {"referenceID": 10, "context": "Along with these experimental successes, the field of deep learning is rapidly developing theoretical frameworks in representation learning [5, 6, 7] including understanding the benefits of different types of non-linearities in neuron activation functions [8], disentanglement of inputs by projecting onto hidden layer manifolds, model averaging with techniques like maxout and dropout [9, 10] and assisting generalization through corruption of input with denoising autoencoders [11].", "startOffset": 479, "endOffset": 483}, {"referenceID": 11, "context": "However, our scheme is fundamentally different to most methods used in deep learning as the learning rules are based solely on the activity of the neurons in the network and are the same, independent of the choice of neuron dynamics or activation function unlike gradient descent methods [12], and they can be implemented online and do not require periods of statistical sampling from the model unlike energy based methods [13].", "startOffset": 288, "endOffset": 292}, {"referenceID": 12, "context": "However, our scheme is fundamentally different to most methods used in deep learning as the learning rules are based solely on the activity of the neurons in the network and are the same, independent of the choice of neuron dynamics or activation function unlike gradient descent methods [12], and they can be implemented online and do not require periods of statistical sampling from the model unlike energy based methods [13].", "startOffset": 423, "endOffset": 427}, {"referenceID": 11, "context": "In addition, the learning scheme is local, meaning that modifying a connection only requires knowledge of the activity of the neurons it connects, not neurons from a distant part of the network, unlike gradient descent and energy based methods [12, 13].", "startOffset": 244, "endOffset": 252}, {"referenceID": 12, "context": "In addition, the learning scheme is local, meaning that modifying a connection only requires knowledge of the activity of the neurons it connects, not neurons from a distant part of the network, unlike gradient descent and energy based methods [12, 13].", "startOffset": 244, "endOffset": 252}, {"referenceID": 13, "context": "An event based dataset of moving MNIST digits collected using an DVS camera [14, 15, 16] is used to train the network for both prediction and classification tasks.", "startOffset": 76, "endOffset": 88}, {"referenceID": 14, "context": "An event based dataset of moving MNIST digits collected using an DVS camera [14, 15, 16] is used to train the network for both prediction and classification tasks.", "startOffset": 76, "endOffset": 88}, {"referenceID": 15, "context": "This interpretation is important in connecting the focus on probability distributions in machine learning with the focus on spike and rate coded networks in computational neuroscience [17].", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "A standard method for training an unsupervised deep feedforward network is to train each pair of layers successively as autoencoders [6] so that each layer encodes the activity of the layer below it, see Fig.", "startOffset": 133, "endOffset": 136}, {"referenceID": 16, "context": "Spiking neuron models in computational neuroscience are often dynamical systems modeled using differential equations [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 5, "context": "In contrast, neurons in machine learning are typically characterized by an activation function of the neuron\u2019s input [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "Any of these types of neuron models could be employed here; however, we choose rectified linear units (ReLUs) that are commonly used in deep learning networks [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 14, "context": "DVS MNIST Event Based Dataset To demonstrate this learning scheme we use a dataset collected using a Dynamic Vision Sensor (DVS) [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "The MNIST database [15] has been used extensively in the development of deep learning [6].", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "The MNIST database [15] has been used extensively in the development of deep learning [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 13, "context": "With the view of linking this work to previous work in deep learning, we demonstrate this learning scheme using a DVS version of the MNIST database [14, 15] in which the handwritten digits are displayed and moved on an LCD screen that is being recorded by a DVS camera.", "startOffset": 148, "endOffset": 156}, {"referenceID": 9, "context": "To demonstrate that many ideas used in deep learning are directly transferable to a spiking neural network that learns using this scheme, during training we use 50% dropout [10] for each hidden layer.", "startOffset": 173, "endOffset": 177}], "year": 2015, "abstractText": "A scheme is derived for learning connectivity in spiking neural networks. The scheme learns instantaneous firing rates that are conditional on the activity in other parts of the network. The scheme is independent of the choice of neuron dynamics or activation function, and network architecture. It involves two simple, online, local learning rules that are applied only in response to occurrences of spike events. This scheme provides a direct method for transferring ideas between the fields of deep learning and computational neuroscience. This learning scheme is demonstrated using a layered feedforward spiking neural network trained self-supervised on a prediction and classification task for moving MNIST images collected using a Dynamic Vision Sensor.", "creator": "LaTeX with hyperref package"}}}