{"id": "1502.05934", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Achieving All with No Parameters: Adaptive NormalHedge", "abstract": "We study the classic online learning problem of predicting with expert advice, and propose a truly parameter-free and adaptive algorithm that achieves several objectives simultaneously without using any prior information. The main component of this work is an improved version of the NormalHedge.DT algorithm (Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new algorithm ensures small regret when the competitor has small loss and almost constant regret when the losses are stochastic. On the other hand, the algorithm is able to compete with any convex combination of the experts simultaneously, with a regret in terms of the relative entropy of the prior and the competitor. This resolves an open problem proposed by Chaudhuri et al. (2009) and Chernov and Vovk (2010). Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen (2014) on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses.", "histories": [["v1", "Fri, 20 Feb 2015 16:58:36 GMT  (34kb)", "http://arxiv.org/abs/1502.05934v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "robert e schapire"], "accepted": false, "id": "1502.05934"}, "pdf": {"name": "1502.05934.pdf", "metadata": {"source": "CRF", "title": "Achieving All with No Parameters: Adaptive NormalHedge", "authors": ["Haipeng Luo"], "emails": ["haipengl@cs.princeton.edu", "schapire@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.05 934v 1 [cs.L G] 20 Feb 20"}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2 The Expert Problem and NormalHedge.DT", "text": "In the expert question, in which each round t = 1,., T: the player first chooses a distribution over N experts, then the opponent decides on the loss of each expert. (...), and then he shows these losses to the player. (...) At the end of this round, the player suffers from the weighted average loss. (...) The cumulative regret of Rt, i = 1, and the cumulative loss of Lt, i = 1, and the cumulative loss of Lt, i = 1,. (...) This is the best strategy we have. (...) Hedt, Rt and Lt represent (...), (...) We are not able to represent the strategy of Lt, i = 1,. (...) The paper, a courageous letter denotes a vector with N coordinates. (...) For example, Rt and Lt we represent (...), (...)."}, {"heading": "3 A New Algorithm: AdaNormalHedge", "text": "We start by applying several possible algorithms in a general form. We define potential function \u03a6 (R, C) = exp (R) 2 + 3C (R + 1, C + 1).Then, the prediction of NormalHedge.DT is easy to set pt, i to be proportional to w (R \u2212 1) where Ct = t for all t (R \u2212 1, C + 1).Note that Ct = t for all t (R \u2212 1, C + 1).Note that Ct = t for all t. Note that Ct = t for all t. Note that Ct is closely related to regret. In fact, the prediction of NormalHedge.DT is easy to set pt to get an expert-wise and more refined access, we replace Ct from Ct, i for each expert, so that there is some useful information for each expert."}, {"heading": "4 Confidence-rated Advice and Sleeping Experts", "text": "In this section, we generalize AdaNormalHedge to deal with experts who give confidence advice, an attitude that many interesting applications such as Blum (1997) and Freund (1997) are explored. (That is, the player must ignore these experts who refrain from giving advice (by expressing zero confidence). (That is, the loss of experts who do not abide by the rules (i.e.) It is as if the player is still suffering losses. (We define the instantaneous regrets rt, i to be it, i to be it, i to be it, i to be it, i for those experts who are not able to abide by the rules (i.e.) It is, i = 0 = 0 the player is still suffering losses. (We define the instaneous regrets, i to be it, i to be it, i to be it, i to be it, i to be it, i to be it, i to it, i to it, i to it, i to be it. \"(i.e.) It is the difference between the loss of the player and the expert weighted."}, {"heading": "5 Time-Varying Competitors", "text": "In this section, we examine a more challenging goal of competing in a standard expert environment with time-varying competitors (i.e., each expert is always awake and always rt, i = t \u2212 t, i), which can be reduced to a sleep problem. The results of this section are summarized in Table 1."}, {"heading": "5.1 Special Cases: Adaptive Regret and Tracking the Best Expert", "text": "The goal of the player is to get a relatively small regret about any kind of interval. [1997] Essentially, this problem is reduced to a sleep problem, which was later improved by Adamskiy et al. [2012] Specifically, for each pair of time t and expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i, a sleeping expert i."}, {"heading": "5.2 General Time-Varying Competitors", "text": "Finally, we discuss the most general objective: \"There is no solution.\" (u1: T) \"There is no solution\" (u1: T) \"There is no solution\" (u1: T) \"There is no solution\" (u1: T) \"There is no solution\" (u1: T) \"There is no solution\" (u1: T) \"There is no solution\" (u1: T). \"(u1: T)\" There is no solution \"(u1: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \"(2: T)\" There is no solution. \""}, {"heading": "6 Competing with the Best Pruning Tree", "text": "The problem is also that we provide much better results than previous work (summarized in Table 2). Specifically, one considers a setting in which the predictor must make a decision that depends on some convex set Y values, and then a convex loss function: \"0, 1.\" The predictor will consult a template tree G."}, {"heading": "A Complete proofs of Theorem 1 and 3", "text": "The first is an improved version of Lemma 2 by Luo and Schapire (2014).Lemma 1. For all R-R, C-0 and r-1).Proof. We first argue that \u03a6 (R + r, C + | r |), as a function of r, is piecewise-convex to [\u2212 1, 0] and [0, 1]. Since the value of the function is 1 if R + r < 0 and otherwise at least 1, it is sufficient to consider the case if R + r is \u2265 0. At the interval [0, 1] we can paraphrase the exponent (ignoring constant 13) as follows: (R + r) 2C + r = (R \u2212 C) 2 + C + 2. \""}, {"heading": "B Proof of Theorem 2", "text": "Evidence. For the first result, the key observation is u \u00b7 CT = R (u) + 2u \u00b7 LT. We only consider the case if R (u) \u2265 0 is used, otherwise the statement is trivial. Thus, due to the condition, we have R (u) 2 \u2264 (R) + 2u \u00b7 LT) A (u), which is confirmed by the solution for R (u) \u2264 12 (A (u) + \u221a A (u) 2 + 8 (u \u00b7 LT) A (u))) 2 (u \u00b7 L (T) A (u) + A (u), which confirms the bond we want. For the second result, Et is intended to denote the expectation conditioning on all coincidences up to round t. Thus, under the condition Et [rt, i] = DE = N i = 1 pt, iEt [t, i \u2212 T, i \u00b2] \u2265 (1 \u2212 pt, i \u00b2) and thus E [RT, i \u00b2), where we have the exact, S \u00b2, S \u00b2, S \u00b2 i \u00b2 i, S \u00b2 i (1)."}, {"heading": "C Proof of Theorem 4", "text": "We assume that this is always possible, with a trivial choice, the aj = ut = ut, irt, i. Note: T is always possible, with a trivial choice, the aj = ut, j, sj = tj. Note: T is always possible, j, sj = tj. Note: T is always possible, with a trivial choice, the aj = ut, j, sj = j and M = T; however, we will need a more complex construction, which will be specified later. With the adaptive repentance guarantee, we have haveT = 1ut, irt, i = M. Note: J is always possible, with a trivial choice, aj, j, sj = j."}], "references": [{"title": "A closer look at adaptive regret", "author": ["Dmitry Adamskiy", "Wouter M Koolen", "Alexey Chernov", "Vladimir Vovk"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Adamskiy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Adamskiy et al\\.", "year": 2012}, {"title": "Empirical support for Winnow and Weighted-Majority algorithms: Results on a calendar scheduling domain", "author": ["Avrim Blum"], "venue": "Machine Learning,", "citeRegEx": "Blum.,? \\Q1997\\E", "shortCiteRegEx": "Blum.", "year": 1997}, {"title": "From external to internal regret", "author": ["Avrim Blum", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blum and Mansour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Mansour.", "year": 2007}, {"title": "Tracking a small set of experts by mixing past posteriors", "author": ["Olivier Bousquet", "Manfred K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet and Warmuth.", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "How to use expert advice", "author": ["Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P. Helmbold", "Robert E. Schapire", "Manfred K. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Mirror descent meets fixed share (and feels no regret)", "author": ["Nicol\u00f2 Cesa-Bianchi", "Pierre Gaillard", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "A parameter-free hedging algorithm", "author": ["Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Prediction with advice of unknown number of experts", "author": ["Alexey Chernov", "Vladimir Vovk"], "venue": "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Chernov and Vovk.,? \\Q2010\\E", "shortCiteRegEx": "Chernov and Vovk.", "year": 2010}, {"title": "Follow the leader if you can, hedge if you must", "author": ["Steven de Rooij", "Tim van Erven", "Peter D. Gr\u00fcnwald", "Wouter M. Koolen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rooij et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rooij et al\\.", "year": 2014}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Manfred K. Warmuth"], "venue": "In Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "A second-order bound with excess losses", "author": ["Pierre Gaillard", "Gilles Stoltz", "Tim Van Erven"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "Gaillard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gaillard et al\\.", "year": 2014}, {"title": "Adaptive algorithms for online decision problems", "author": ["Elad Hazan", "C. Seshadhri"], "venue": "In Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Hazan and Seshadhri.,? \\Q2007\\E", "shortCiteRegEx": "Hazan and Seshadhri.", "year": 2007}, {"title": "Predicting nearly as well as the best pruning of a decision tree", "author": ["David P. Helmbold", "Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Helmbold and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Helmbold and Schapire.", "year": 1997}, {"title": "Tracking the best expert", "author": ["Mark Herbster", "Manfred Warmuth"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Herbster and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 1995}, {"title": "Tracking the best expert", "author": ["Mark Herbster", "Manfred Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Herbster and Warmuth.,? \\Q1998\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 1998}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Herbster and Warmuth.,? \\Q2001\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 2001}, {"title": "Online optimization: Competing with dynamic comparators", "author": ["Ali Jadbabaie", "Alexander Rakhlin", "Shahin Shahrampour", "Karthik Sridharan"], "venue": "In The 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Jadbabaie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jadbabaie et al\\.", "year": 2015}, {"title": "Putting bayes to sleep", "author": ["Wouter M Koolen", "Dmitry Adamskiy", "Manfred K Warmuth"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koolen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2012}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting", "author": ["Haipeng Luo", "Robert E. Schapire"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Luo and Schapire.,? \\Q2014\\E", "shortCiteRegEx": "Luo and Schapire.", "year": 2014}, {"title": "Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations", "author": ["H Brendan McMahan", "Francesco Orabona"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "McMahan and Orabona.,? \\Q2014\\E", "shortCiteRegEx": "McMahan and Orabona.", "year": 2014}, {"title": "Dimension-free exponentiated gradient", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Orabona.,? \\Q2013\\E", "shortCiteRegEx": "Orabona.", "year": 2013}, {"title": "Simultaneous model selection and optimization through parameter-free stochastic learning", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Orabona.,? \\Q2014\\E", "shortCiteRegEx": "Orabona.", "year": 2014}, {"title": "Exploiting easy data in online optimization", "author": ["Amir Sani", "Gergely Neu", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "Drifting games", "author": ["Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Schapire.,? \\Q2001\\E", "shortCiteRegEx": "Schapire.", "year": 2001}, {"title": "No-regret algorithms for unconstrained online convex optimization", "author": ["Matthew Streeter", "Brendan Mcmahan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Streeter and Mcmahan.,? \\Q2012\\E", "shortCiteRegEx": "Streeter and Mcmahan.", "year": 2012}, {"title": "Follow the leader with dropout perturbations", "author": ["Tim Van Erven", "Wojciech Kotlowski", "Manfred K Warmuth"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}, {"title": "A game of prediction with expert advice", "author": ["V.G. Vovk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vovk.,? \\Q1998\\E", "shortCiteRegEx": "Vovk.", "year": 1998}, {"title": "Open problem: Shifting experts on easy data", "author": ["Manfred K. Warmuth", "Wouter M. Koolen"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "Warmuth and Koolen.,? \\Q2014\\E", "shortCiteRegEx": "Warmuth and Koolen.", "year": 2014}, {"title": "Context tree weighting: A sequential universal source coding procedure for FSMX sources", "author": ["Frans M.J. Willems", "Yuri M. Shtarkov", "Tjalling J. Tjalkens"], "venue": "In Proceedings", "citeRegEx": "Willems et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Willems et al\\.", "year": 1993}, {"title": "The context tree weighting method: Basic properties", "author": ["Frans M.J. Willems", "Yuri M. Shtarkov", "Tjalling J. Tjalkens"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Willems et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Willems et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 22, "context": "DT algorithm [Luo and Schapire, 2014], called AdaNormalHedge.", "startOffset": 13, "endOffset": 37}, {"referenceID": 7, "context": "This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010].", "startOffset": 42, "endOffset": 66}, {"referenceID": 7, "context": "This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010]. Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree.", "startOffset": 42, "endOffset": 94}, {"referenceID": 7, "context": "This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010]. Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen [2014] on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses.", "startOffset": 42, "endOffset": 546}, {"referenceID": 2, "context": ", 2012]; learning with experts who provide confidence-rated advice [Blum and Mansour, 2007]; and achieving much smaller regret when the problem is \u201ceasy\u201d while still ensuring worst-case robustness [de Rooij et al.", "startOffset": 67, "endOffset": 91}, {"referenceID": 8, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al.", "startOffset": 83, "endOffset": 114}, {"referenceID": 3, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al.", "startOffset": 115, "endOffset": 142}, {"referenceID": 3, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al. [1997], Vovk [1998] and others two decades ago.", "startOffset": 143, "endOffset": 170}, {"referenceID": 3, "context": "1 Introduction The problem of predicting with expert advice was first pioneered by Littlestone and Warmuth [1994], Freund and Schapire [1997], Cesa-Bianchi et al. [1997], Vovk [1998] and others two decades ago.", "startOffset": 143, "endOffset": 183}, {"referenceID": 22, "context": "DT [Luo and Schapire, 2014].", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007].", "startOffset": 133, "endOffset": 157}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.", "startOffset": 30, "endOffset": 54}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.DT [Luo and Schapire, 2014]. We call it Adaptive NormalHedge (or AdaNormalHedge for short). NormalHedge and NormalHedge.DT provide guarantees for the so-called \u01eb-quantile regret simultaneously for any \u01eb, which essentially corresponds to competing with a uniform distribution over the top \u01eb-fraction of experts. Our new algorithm improves NormalHedge.DT from two aspects (Section 3): 1. AdaNormalHedge can compete with not just the competitor of the specific form mentioned above, but indeed any unknown fixed competitor simultaneously, with a regret in terms of the relative entropy between the competitor and the player\u2019s prior belief of the experts. 2. AdaNormalHedge ensures a new regret bound in terms of the cumulative magnitude of the instantaneous regrets, which is always at most the bound for NormalHedge.DT (or NormalHedge). Moreover, the power of this new form of regret is almost the same as the second order bound introduced in a recent work by Gaillard et al. [2014]. Specifically, it implies 1) a small regret when the loss of the competitor is small and 2) an almost constant regret when the losses are generated randomly with a gap in expectation.", "startOffset": 30, "endOffset": 1120}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.DT [Luo and Schapire, 2014]. We call it Adaptive NormalHedge (or AdaNormalHedge for short). NormalHedge and NormalHedge.DT provide guarantees for the so-called \u01eb-quantile regret simultaneously for any \u01eb, which essentially corresponds to competing with a uniform distribution over the top \u01eb-fraction of experts. Our new algorithm improves NormalHedge.DT from two aspects (Section 3): 1. AdaNormalHedge can compete with not just the competitor of the specific form mentioned above, but indeed any unknown fixed competitor simultaneously, with a regret in terms of the relative entropy between the competitor and the player\u2019s prior belief of the experts. 2. AdaNormalHedge ensures a new regret bound in terms of the cumulative magnitude of the instantaneous regrets, which is always at most the bound for NormalHedge.DT (or NormalHedge). Moreover, the power of this new form of regret is almost the same as the second order bound introduced in a recent work by Gaillard et al. [2014]. Specifically, it implies 1) a small regret when the loss of the competitor is small and 2) an almost constant regret when the losses are generated randomly with a gap in expectation. Our results resolve the open problem asked in Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether a better \u01eb-quantile regret in terms of the loss of the expert instead of the horizon can be achieved.", "startOffset": 30, "endOffset": 1374}, {"referenceID": 2, "context": "Our algorithm is a variant of Chaudhuri et al. [2009]\u2019s NormalHedge algorithm, and more specifically is an improved version of NormalHedge.DT [Luo and Schapire, 2014]. We call it Adaptive NormalHedge (or AdaNormalHedge for short). NormalHedge and NormalHedge.DT provide guarantees for the so-called \u01eb-quantile regret simultaneously for any \u01eb, which essentially corresponds to competing with a uniform distribution over the top \u01eb-fraction of experts. Our new algorithm improves NormalHedge.DT from two aspects (Section 3): 1. AdaNormalHedge can compete with not just the competitor of the specific form mentioned above, but indeed any unknown fixed competitor simultaneously, with a regret in terms of the relative entropy between the competitor and the player\u2019s prior belief of the experts. 2. AdaNormalHedge ensures a new regret bound in terms of the cumulative magnitude of the instantaneous regrets, which is always at most the bound for NormalHedge.DT (or NormalHedge). Moreover, the power of this new form of regret is almost the same as the second order bound introduced in a recent work by Gaillard et al. [2014]. Specifically, it implies 1) a small regret when the loss of the competitor is small and 2) an almost constant regret when the losses are generated randomly with a gap in expectation. Our results resolve the open problem asked in Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether a better \u01eb-quantile regret in terms of the loss of the expert instead of the horizon can be achieved.", "startOffset": 30, "endOffset": 1402}, {"referenceID": 1, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007]. We then focus on a special case of this setting called the sleeping expert problem [Blum, 1997, Freund et al., 1997], where the number of \u201cawake\u201d experts is dynamically changing and the total number of underlying experts is indeed unknown. AdaNormalHedge is thus a very suitable algorithm for this problem. To show the power of all the abovementioned properties of AdaNormalHedge, we study the following two examples of the sleeping expert problem and use AdaNormalHedge to significantly improve previous work. The first example is adaptive regret, that is, regret on any time interval, introduced by Hazan and Seshadhri [2007]. This can be reduced to a sleeping expert problem by adding a new copy of each original expert on each round [Freund et al.", "startOffset": 134, "endOffset": 787}, {"referenceID": 1, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007]. We then focus on a special case of this setting called the sleeping expert problem [Blum, 1997, Freund et al., 1997], where the number of \u201cawake\u201d experts is dynamically changing and the total number of underlying experts is indeed unknown. AdaNormalHedge is thus a very suitable algorithm for this problem. To show the power of all the abovementioned properties of AdaNormalHedge, we study the following two examples of the sleeping expert problem and use AdaNormalHedge to significantly improve previous work. The first example is adaptive regret, that is, regret on any time interval, introduced by Hazan and Seshadhri [2007]. This can be reduced to a sleeping expert problem by adding a new copy of each original expert on each round [Freund et al., 1997, Koolen et al., 2012]. Thus, the total number of sleeping experts is not fixed. When some information on this interval is known (such as the length, the loss of the competitor on this interval, etc), several algorithms achieve optimal regret [Hazan and Seshadhri, 2007, Cesa-Bianchi et al., 2012]. However, when no prior information is available, all previous work gives suboptimal bounds. We apply AdaNormalHedge to this problem. The resulting algorithm, which we called AdaNormalHedge.TV, enjoys the optimal adaptive regret in not only the adversarial case but also the stochastic case due to the properties of AdaNormalHedge. We then extend the results to the problem of tracking the best experts where the player needs to compete with the best partition of the whole process and the best experts on each of these partitions [Herbster and Warmuth, 1995, Bousquet and Warmuth, 2003]. This resolves one of the open problems in Warmuth and Koolen [2014] on whether a single algorithm can achieve optimal shifting regret for both adversarial and stochastic losses.", "startOffset": 134, "endOffset": 1871}, {"referenceID": 1, "context": "To illustrate this idea, in Section 4 we extend the algorithm and results to a setting where experts provide confidence-rated advice [Blum and Mansour, 2007]. We then focus on a special case of this setting called the sleeping expert problem [Blum, 1997, Freund et al., 1997], where the number of \u201cawake\u201d experts is dynamically changing and the total number of underlying experts is indeed unknown. AdaNormalHedge is thus a very suitable algorithm for this problem. To show the power of all the abovementioned properties of AdaNormalHedge, we study the following two examples of the sleeping expert problem and use AdaNormalHedge to significantly improve previous work. The first example is adaptive regret, that is, regret on any time interval, introduced by Hazan and Seshadhri [2007]. This can be reduced to a sleeping expert problem by adding a new copy of each original expert on each round [Freund et al., 1997, Koolen et al., 2012]. Thus, the total number of sleeping experts is not fixed. When some information on this interval is known (such as the length, the loss of the competitor on this interval, etc), several algorithms achieve optimal regret [Hazan and Seshadhri, 2007, Cesa-Bianchi et al., 2012]. However, when no prior information is available, all previous work gives suboptimal bounds. We apply AdaNormalHedge to this problem. The resulting algorithm, which we called AdaNormalHedge.TV, enjoys the optimal adaptive regret in not only the adversarial case but also the stochastic case due to the properties of AdaNormalHedge. We then extend the results to the problem of tracking the best experts where the player needs to compete with the best partition of the whole process and the best experts on each of these partitions [Herbster and Warmuth, 1995, Bousquet and Warmuth, 2003]. This resolves one of the open problems in Warmuth and Koolen [2014] on whether a single algorithm can achieve optimal shifting regret for both adversarial and stochastic losses. Note that although recent work by Sani et al. [2014] also solves this open problem in some sense, their method requires knowing the number of partitions and other information ahead of time and also gives a worse bound for stochastic losses, while AdaNormalHedge.", "startOffset": 134, "endOffset": 2034}, {"referenceID": 15, "context": "The second example we provide is predicting almost as well as the best pruning tree [Helmbold and Schapire, 1997], which was also shown to be reducible to a sleeping expert problem [Freund et al.", "startOffset": 84, "endOffset": 113}, {"referenceID": 12, "context": "The second example we provide is predicting almost as well as the best pruning tree [Helmbold and Schapire, 1997], which was also shown to be reducible to a sleeping expert problem [Freund et al., 1997].", "startOffset": 181, "endOffset": 202}, {"referenceID": 11, "context": "The well-known exponential weights algorithm gives the optimal results only when the learning rate is optimally tuned in terms of the competitor [Freund and Schapire, 1999].", "startOffset": 145, "endOffset": 172}, {"referenceID": 8, "context": "This problem was introduced in Herbster and Warmuth [2001] and later generalized by Cesa-Bianchi et al.", "startOffset": 31, "endOffset": 59}, {"referenceID": 4, "context": "This problem was introduced in Herbster and Warmuth [2001] and later generalized by Cesa-Bianchi et al. [2012]. Their algorithm (fixed share) also requires knowing some information on the sequence of competitors to optimally tune parameters.", "startOffset": 84, "endOffset": 111}, {"referenceID": 4, "context": "This problem was introduced in Herbster and Warmuth [2001] and later generalized by Cesa-Bianchi et al. [2012]. Their algorithm (fixed share) also requires knowing some information on the sequence of competitors to optimally tune parameters. We avoid this issue by showing that while this problem seems more general and difficult, it is in fact equivalent to its special case: achieving adaptive regret. This equivalence theorem is independent of the concrete algorithms and may be of independent interest. Applying this result, we show that without any parameter tuning, AdaNormalHedge.TV automatically achieves a bound comparable to the one achieved by the optimally tuned fixed share algorithm when competing with time-varying competitors. Concrete results and detailed comparisons on this first example can be found in Section 5. To sum up, AdaNormalHedge.TV is an algorithm that is simultaneously adaptive in the number of experts, the competitors and the way the losses are generated. The second example we provide is predicting almost as well as the best pruning tree [Helmbold and Schapire, 1997], which was also shown to be reducible to a sleeping expert problem [Freund et al., 1997]. Previous work either only considered the log loss setting, or assumed prior information on the best pruning tree is known. Using AdaNormalHedge, we again provide better or comparable bounds without knowing any prior information. In fact, due to the adaptivity of AdaNormalHedge in the number of experts, our regret bound depends on the total number of distinct traversed edges so far, instead of the total number of edges of the decision tree as in Freund et al. [1997] which could be exponentially larger.", "startOffset": 84, "endOffset": 1665}, {"referenceID": 0, "context": "While competing with any unknown competitor simultaneously is relatively easy in the log loss setting [Littlestone and Warmuth, 1994, Adamskiy et al., 2012, Koolen et al., 2012], it is much harder in the bounded loss setting studied here. The well-known exponential weights algorithm gives the optimal results only when the learning rate is optimally tuned in terms of the competitor [Freund and Schapire, 1999]. Chernov and Vovk [2010] also studied \u01eb-quantile regret, but no concrete algorithm was provided.", "startOffset": 134, "endOffset": 437}, {"referenceID": 0, "context": "While competing with any unknown competitor simultaneously is relatively easy in the log loss setting [Littlestone and Warmuth, 1994, Adamskiy et al., 2012, Koolen et al., 2012], it is much harder in the bounded loss setting studied here. The well-known exponential weights algorithm gives the optimal results only when the learning rate is optimally tuned in terms of the competitor [Freund and Schapire, 1999]. Chernov and Vovk [2010] also studied \u01eb-quantile regret, but no concrete algorithm was provided. Several work considers competing with unknown competitors in a different unconstrained linear optimization setting [Streeter and Mcmahan, 2012, Orabona, 2013, McMahan and Orabona, 2014, Orabona, 2014]. Jadbabaie et al. [2015] studied general adaptive online learning algorithms against time-varying competitors, but with different and incomparable measurement of the hardness of the problem.", "startOffset": 134, "endOffset": 735}, {"referenceID": 22, "context": "DT [Luo and Schapire, 2014] (a variant of NormalHedge [Chaudhuri et al.", "startOffset": 3, "endOffset": 27}, {"referenceID": 7, "context": "DT [Luo and Schapire, 2014] (a variant of NormalHedge [Chaudhuri et al., 2009]), before we introduce our new improved variants.", "startOffset": 54, "endOffset": 78}, {"referenceID": 15, "context": "In fact, in Section 5, we will consider an even more general notion of regret introduced in Herbster and Warmuth [2001], where we allow the competitor to vary over time and to have different scales.", "startOffset": 92, "endOffset": 120}, {"referenceID": 7, "context": "This is the first concrete algorithm with this kind of adaptive property (the original NormalHedge [Chaudhuri et al., 2009] still has a weak dependence on N ).", "startOffset": 99, "endOffset": 123}, {"referenceID": 7, "context": "This is the first concrete algorithm with this kind of adaptive property (the original NormalHedge [Chaudhuri et al., 2009] still has a weak dependence on N ). In fact, as we will show later, one can even extend the results to any competitor u. Moreover, we will improve NormalHedge.DT so that it has a much smaller regret when the problem is \u201ceasy\u201d in some sense. Notation. We use [N ] to denote the set {1, . . . , N}, \u2206N to denote the simplex of all distributions over [N ], and RE(\u00b7 || \u00b7) to denote the relative entropy between two distributions, Also define L\u0303t,i = \u2211t \u03c4=1[l\u03c4,i\u2212 l\u0302\u03c4 ]+. Many bounds in this work will be in terms of L\u0303T,i, which is always at most LT,i since trivially [lt,i\u2212 l\u0302t]+ \u2264 lt,i. We consider \u201clog log\u201d terms to be nearly constant, and use \u00d4() notation to hide these terms. Indeed, as pointed out by Chernov and Vovk [2010], ln lnx is smaller than 4 even when x is as large as the age of the universe expressed in microseconds (\u2248 4.", "startOffset": 100, "endOffset": 853}, {"referenceID": 13, "context": "Gaillard et al. [2014] introduced a new second order bound that implies much smaller regret when the problem is easy.", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "Gaillard et al. [2014] introduced a new second order bound that implies much smaller regret when the problem is easy. It turns out that our seemingly weaker first order bound is also enough to get the exact same results! We state these implications in the following theorem which is essentially a restatement of Theorems 9 and 11 of Gaillard et al. [2014] with weaker conditions.", "startOffset": 0, "endOffset": 356}, {"referenceID": 13, "context": "The proof of Theorem 2 is based on the same idea as in Gaillard et al. [2014], and is included in Appendix B for completeness.", "startOffset": 55, "endOffset": 78}, {"referenceID": 7, "context": "This answers the open question (in the affirmative) asked by Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether an improvement for small loss can be obtained for \u01eb-quantile regret without knowing \u01eb.", "startOffset": 61, "endOffset": 85}, {"referenceID": 7, "context": "This answers the open question (in the affirmative) asked by Chaudhuri et al. [2009] and Chernov and Vovk [2010] on whether an improvement for small loss can be obtained for \u01eb-quantile regret without knowing \u01eb.", "startOffset": 61, "endOffset": 113}, {"referenceID": 13, "context": "Comparison to Adapt-ML-Prod [Gaillard et al., 2014].", "startOffset": 28, "endOffset": 51}, {"referenceID": 13, "context": "Comparison to Adapt-ML-Prod [Gaillard et al., 2014]. Adapt-ML-Prod enjoys a second order bound in terms of \u2211T t=1 r 2 t,i, which is always at most the term \u2211T t=1 |rt,i| appeared in our bounds.4 However, on one hand, as discussed above, these two bounds have the same improvements when the problem is easy in several senses; on the other hand, Adapt-ML-Prod does not provide a bound in terms of RE(u || q) for an unknown u. In fact, as discussed at the end of Section A.3 of Gaillard et al. [2014], Adapt-ML-Prod cannot improve by exploiting a good prior q (or at least its current analysis cannot).", "startOffset": 29, "endOffset": 498}, {"referenceID": 13, "context": "Comparison to Adapt-ML-Prod [Gaillard et al., 2014]. Adapt-ML-Prod enjoys a second order bound in terms of \u2211T t=1 r 2 t,i, which is always at most the term \u2211T t=1 |rt,i| appeared in our bounds.4 However, on one hand, as discussed above, these two bounds have the same improvements when the problem is easy in several senses; on the other hand, Adapt-ML-Prod does not provide a bound in terms of RE(u || q) for an unknown u. In fact, as discussed at the end of Section A.3 of Gaillard et al. [2014], Adapt-ML-Prod cannot improve by exploiting a good prior q (or at least its current analysis cannot). Specifically, while the regret for AdaNormalHedge does not have an explicit dependence on N and is much smaller when the prior q is close to the competitor u, the regret for Adapt-ML-Prod always has a lnN multiplicative term for \u2211T t=1 r 2 t,i, which means even a good prior results in the same regret as a uniform prior! More advantages of AdaNormalHedge over Adapt-ML-Prod will be discussed in concrete examples in following sections. Proof sketch of Theorem 1. The analysis of NormaHedge.DT is based on the idea of converting the expert problem into a drifting game [Schapire, 2001, Luo and Schapire, 2014]. Here, we extract and simplify the key idea of their proof and also improve it to form our analysis. The main idea is to show that the weighted sum of potentials does not increase much on each round using an improved version of Lemma 2 of Luo and Schapire [2014]. In fact, we show that the final potential \u2211N i=1 qi\u03a6(RT,i, CT,i) is exactly bounded by B (defined in Theorem 1).", "startOffset": 29, "endOffset": 1473}, {"referenceID": 1, "context": "4 Confidence-rated Advice and Sleeping Experts In this section, we generalize AdaNormalHedge to deal with experts that make confidence-rated advice, a setting that subsumes many interesting applications as studied by Blum [1997] and Freund et al.", "startOffset": 217, "endOffset": 229}, {"referenceID": 1, "context": "4 Confidence-rated Advice and Sleeping Experts In this section, we generalize AdaNormalHedge to deal with experts that make confidence-rated advice, a setting that subsumes many interesting applications as studied by Blum [1997] and Freund et al. [1997]. In this general setting, on each round t, each expert first reports its confidence It,i \u2208 [0, 1] for the current task.", "startOffset": 217, "endOffset": 254}, {"referenceID": 12, "context": "Previously, Gaillard et al. [2014] studied a general reduction from an expert algorithm to a confidencerated expert algorithm.", "startOffset": 12, "endOffset": 35}, {"referenceID": 7, "context": "This is indeed the case for most algorithms (including Adapt-ML-Prod and even the original NormalHedge by Chaudhuri et al. [2009]).", "startOffset": 106, "endOffset": 130}, {"referenceID": 12, "context": "1 Special Cases: Adaptive Regret and Tracking the Best Expert We start from a special case: adaptive regret, introduced by Hazan and Seshadhri [2007] to better capture changing environments.", "startOffset": 123, "endOffset": 150}, {"referenceID": 11, "context": "Freund et al. [1997] essentially introduced a way to reduce this problem to a sleeping expert problem, which was later improved by Adamskiy et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "[1997] essentially introduced a way to reduce this problem to a sleeping expert problem, which was later improved by Adamskiy et al. [2012]. Specifically, for every pair of time t and expert i, we create a sleeping expert, denoted by (t, i), who is only awake after (and including) round t and since then suffers the same loss as the original expert i.", "startOffset": 117, "endOffset": 140}, {"referenceID": 14, "context": "However, the data streaming technique used in Hazan and Seshadhri [2007] can be directly applied here to reduce the time and space complexity to O(N ln t) and O(N lnT ) respectively, with only an extra multiplicative O( \u221a ln(t2 \u2212 t1)) factor in the regret.", "startOffset": 46, "endOffset": 73}, {"referenceID": 14, "context": "However, the data streaming technique used in Hazan and Seshadhri [2007] can be directly applied here to reduce the time and space complexity to O(N ln t) and O(N lnT ) respectively, with only an extra multiplicative O( \u221a ln(t2 \u2212 t1)) factor in the regret. Tracking the best expert. In fact, AdaNormalHedge.TV is a solution for one of the open problems proposed by Warmuth and Koolen [2014]. Adaptive regret immediately implies the so-called K-shifting regret for the problem of tracking the best expert in a changing environment.", "startOffset": 46, "endOffset": 391}, {"referenceID": 14, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007].", "startOffset": 51, "endOffset": 78}, {"referenceID": 10, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007]. This is exactly what was asked in Warmuth and Koolen [2014]: whether there is an algorithm that can do optimally for both adversarial and stochastic losses in the problem of tracking the best expert.", "startOffset": 52, "endOffset": 140}, {"referenceID": 10, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007]. This is exactly what was asked in Warmuth and Koolen [2014]: whether there is an algorithm that can do optimally for both adversarial and stochastic losses in the problem of tracking the best expert. AdaNormalHedge.TV achieves this goal without knowing K or any other information, while the solution provided by Sani et al. [2014] needs to know K , LK-Shift and \u03b1 to get the same adversarial bound and a worse stochastic bound of order O(1/\u03b12).", "startOffset": 52, "endOffset": 411}, {"referenceID": 10, "context": "These bounds are optimal up to logarithmic factors [Hazan and Seshadhri, 2007]. This is exactly what was asked in Warmuth and Koolen [2014]: whether there is an algorithm that can do optimally for both adversarial and stochastic losses in the problem of tracking the best expert. AdaNormalHedge.TV achieves this goal without knowing K or any other information, while the solution provided by Sani et al. [2014] needs to know K , LK-Shift and \u03b1 to get the same adversarial bound and a worse stochastic bound of order O(1/\u03b12). Comparison to previous work. For adaptive regret, the FLH algorithm by Hazan and Seshadhri [2007] treats any standard expert algorithm as a sleeping expert, and has an additive term O( \u221a t2 ln t2) in addition to the base algorithm\u2019s regret (when no prior information is available), which adds up to a large O(K \u221a T lnT ) term for K-shifting regret.", "startOffset": 52, "endOffset": 623}, {"referenceID": 0, "context": "Several works on fixed share for the simpler \u201clog loss\u201d setting were studied before [Herbster and Warmuth, 1998, Bousquet and Warmuth, 2003, Adamskiy et al., 2012, Koolen et al., 2012]. Cesa-Bianchi et al. [2012] studied a generalized fixed share algorithm for the bounded loss setting considered here.", "startOffset": 141, "endOffset": 213}, {"referenceID": 14, "context": "V (u1:T )L\u0303(u1:T ) ln (NT ) FLH8 [Hazan and Seshadhri, 2007] \u221a t2 ln t2 K \u221a T lnT unknown Fixed Share [Cesa-Bianchi et al.", "startOffset": 33, "endOffset": 60}, {"referenceID": 6, "context": "V (u1:T )L\u0303(u1:T ) ln (NT ) FLH8 [Hazan and Seshadhri, 2007] \u221a t2 ln t2 K \u221a T lnT unknown Fixed Share [Cesa-Bianchi et al., 2012] \u221a", "startOffset": 102, "endOffset": 129}, {"referenceID": 6, "context": "Fixed share is shown to ensure the following regret [Cesa-Bianchi et al., 2012]: R(u1:T ) = O ( \u221a", "startOffset": 52, "endOffset": 79}, {"referenceID": 5, "context": "Cesa-Bianchi et al. [2012] introduced a distance measurement to capture this variation: V (u1:T ) = \u2211T t=1 \u2211N i=1[ut,i\u2212ut\u22121,i]+ where we define u0,i = 0 for all i.", "startOffset": 0, "endOffset": 27}, {"referenceID": 5, "context": "Moreover, while the results in Cesa-Bianchi et al. [2012] are specific for the fixed share algorithm, we prove the following results which are independent of the concrete algorithms and may be of independent interest.", "startOffset": 31, "endOffset": 58}, {"referenceID": 5, "context": "3 of Cesa-Bianchi et al. [2012], the authors mentioned online tuning technique for the parameters, it only works for special cases (e.", "startOffset": 5, "endOffset": 32}, {"referenceID": 13, "context": "This problem was studied in the context of online learning by Helmbold and Schapire [1997] using the approach of Willems et al.", "startOffset": 62, "endOffset": 91}, {"referenceID": 4, "context": "It is also called the tree expert problem in Cesa-Bianchi and Lugosi [2006]. Freund et al.", "startOffset": 45, "endOffset": 76}, {"referenceID": 4, "context": "It is also called the tree expert problem in Cesa-Bianchi and Lugosi [2006]. Freund et al. [1997] proposed a generic reduction from a tree expert problem to a sleeping expert problem.", "startOffset": 45, "endOffset": 98}, {"referenceID": 4, "context": "It is also called the tree expert problem in Cesa-Bianchi and Lugosi [2006]. Freund et al. [1997] proposed a generic reduction from a tree expert problem to a sleeping expert problem. Using this reduction with our new algorithm, we provide much better results compared to previous work (summarized in Table 2). Specifically, consider a setting where on each round t, the predictor has to make a decision yt from some convex set Y given some side information xt, and then a convex loss function ft : Y \u2192 [0, 1] is revealed and the player suffers loss ft(yt). The predictor is given a template tree G to consult. Starting from the root, each node of G performs a test on xt to decide which of its child should perform the next test, until a leaf is reached. In addition to a test, each node (except the root) also makes a prediction based on xt. A pruning tree P is a tree induced by replacing zero or more nodes (and associated subtrees) of G by leaves. The prediction of a pruning tree P given xt, denoted by P(xt), is naturally defined as the prediction of the leaf that xt reaches by traversing P. The player\u2019s goal is thus to predict almost as well as the best pruning tree in hindsight, that is, to minimize RG = \u2211T t=1 ft(yt)\u2212minP \u2211T t=1 ft(P(xt)). The idea of the reduction introduced by Freund et al. [1997] is to view each edge of G as a sleeping expert (indexed by e), who is awake only when traversed by xt, and in that case predicts yt,e, the same prediction as the child node that it connects to.", "startOffset": 45, "endOffset": 1315}, {"referenceID": 12, "context": "The work by Freund et al. [1997] considers a variant of the exponential weights algorithm in a \u201clog loss\u201d setting, and is not directly applicable here (specifically it is not clear how 11", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "O (\u2016xt\u2016G) O (NT ) No Adapt-ML-Prod [Gaillard et al., 2014] \u00d4 ( \u221a", "startOffset": 35, "endOffset": 58}, {"referenceID": 15, "context": "O (\u2016xt\u2016G) O (NT ) No Exponential Weights [Helmbold and Schapire, 1997] O (\u221a mL )", "startOffset": 41, "endOffset": 70}, {"referenceID": 13, "context": "A better choice is the Adapt-ML-Prod algorithm by Gaillard et al. [2014] (the version for the sleeping expert problem).", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "We finally compare with a totally different approach [Helmbold and Schapire, 1997], where one simply treats each pruning tree as an expert and run the exponential weights algorithm.", "startOffset": 53, "endOffset": 82}, {"referenceID": 12, "context": "As discussed in Freund et al. [1997], the linear dependence on m in this bound is much better than the one of the form O (\u221a mL\u2217 lnN )", "startOffset": 16, "endOffset": 37}, {"referenceID": 12, "context": "Finally, as mentioned in Freund et al. [1997], the sleeping expert approach can be easily generalized to predicting with a decision graph.", "startOffset": 25, "endOffset": 46}, {"referenceID": 13, "context": "3 of Gaillard et al. [2014] that we already mentioned at Section 3.", "startOffset": 5, "endOffset": 28}], "year": 2015, "abstractText": "We study the classic online learning problem of predicting with expert advice, and propose a truly parameter-free and adaptive algorithm that achieves several objectives simultaneously without using any prior information. The main component of this work is an improved version of the NormalHedge.DT algorithm [Luo and Schapire, 2014], called AdaNormalHedge. On one hand, this new algorithm ensures small regret when the competitor has small loss and almost constant regret when the losses are stochastic. On the other hand, the algorithm is able to compete with any convex combination of the experts simultaneously, with a regret in terms of the relative entropy of the prior and the competitor. This resolves an open problem proposed by Chaudhuri et al. [2009] and Chernov and Vovk [2010]. Moreover, we extend the results to the sleeping expert setting and provide two applications to illustrate the power of AdaNormalHedge: 1) competing with time-varying unknown competitors and 2) predicting almost as well as the best pruning tree. Our results on these applications significantly improve previous work from different aspects, and a special case of the first application resolves another open problem proposed by Warmuth and Koolen [2014] on whether one can simultaneously achieve optimal shifting regret for both adversarial and stochastic losses.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}