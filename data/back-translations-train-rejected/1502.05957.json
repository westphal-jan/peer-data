{"id": "1502.05957", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Web Similarity", "abstract": "Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or any other large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a similarity on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity according to all (upper semi)computable properties. We develop the theory and give applications. The derivation of the NWD method is based on Kolmogorov complexity.", "histories": [["v1", "Fri, 20 Feb 2015 17:55:58 GMT  (205kb,D)", "http://arxiv.org/abs/1502.05957v1", "LaTeX 25 pages, 3 tables. A precursor isarXiv:1308.3177"]], "COMMENTS": "LaTeX 25 pages, 3 tables. A precursor isarXiv:1308.3177", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.CV", "authors": ["andrew r cohen", "paul m b vitanyi"], "accepted": false, "id": "1502.05957"}, "pdf": {"name": "1502.05957.pdf", "metadata": {"source": "CRF", "title": "Web Similarity", "authors": ["Andrew R. Cohen", "M.B. Vit\u00e1nyi"], "emails": ["acohen@coe.drexel.edu", "Paul.Vitanyi@cwi.nl."], "sections": [{"heading": null, "text": "\"Normalized Web Distance\" (NWD) is a similarity or normalized semantic distance that affects either the object itself or the persons involved. \"We either specify the type of objects we are looking for.\" \"We estimate the distance between objects on the basis of the World Wide Web or other large electronic databases, for example Wikipedia, and a search engine that returns reliable aggregated page numbers.\" For sets of search terms, the NWD gives a similarity on a scale from 0 (identical) to 1 (completely different). The NWD approaches similarity according to all (upper half) calculable properties. We develop the theory and give applications. The derivation of the NWD method is based on searching for objects on Kolmogorov Complexity. Index terms - reliable web distance, pattern recognition, data mining, similarity, classification, Kolmogorov complexity. Objects are computer files that carry all their properties. However, there are also objects that carry with them."}, {"heading": "A. Related Work", "text": "The similarity or relative semantics between the pairs of search terms was defined in [5] and demonstrated in practice, using the World Wide Web as a database and Google as a search engine. The proposed Normalized Google Distance (NGD) works for any search engine that specifies an aggregate number of pages for search terms. See, for example, [2], [7], [21], [20], [3] and the many references to [5] in Google scholars. [11] introduces the notion that the information required to move from any object in a finite multi-sentence (a sentence in which an element may occur more than once) from objects to any other object in the set."}, {"heading": "B. Results", "text": "The NWD is a similarity (a common semantics) between all the search terms in a sentence. (We use sentence instead of multi-sentence, since a sentence is subsequently more appropriate in the context of the search terms.) It can be thought of as a diameter of the sentence. This is simpler and mathematically much easier that constructing the classes from the pairwise distances. In the latter solution, information is inevitably lost. The basic concepts such as the web events, web distribution and web code are given in Section II. We determine the length of a single binary program to calculate from each web event of an individual member in a sentence. (The latter solution will inevitably be lost.) The basic concepts such as the web events, web distribution, and web code are described in Section II."}, {"heading": "II. WEB DISTRIBUTION AND WEB CODE", "text": "We give a derivation that applies to idealized search engines that return reliable aggregate numbers of pages from their databases (here called the Web consisting of web pages), and then apply the idealized theory to real problems that real search engines apply to real databases."}, {"heading": "A. Web Event", "text": "The set of singleton search terms is designated by S, a set of search terms is X = {x1,.., xn} with the definition II.1. We define the web product e (X) as the set of web pages returned by the search engine that searches for X. Unless otherwise specified, we consider all singleton search terms that define the same web product to be the same term. Therefore, we actually treat equivalence classes [x] and not the equivalence class [x] = {y-S: y-x}. Unless otherwise specified, we consider all singleton search terms that define the same web product as the same term."}, {"heading": "B. The Web Code", "text": "It is natural to look at the codewords for the events. We base these codewords on the probabilities of the event. (That is) We have to look at the number of searched terms per page (the number of searched terms per page). (That is) We can imagine that the probability in the sense of an instantaneous snapshot.February 23, 2015 DRAFT5 allows a probability mass function on a known sentence to define the associated prefix codeword length (information content) is equal, [13].Such a prefix code is a correct prefix word length."}, {"heading": "III. THEORY", "text": "(0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f) f (0) f (0) f) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f (0) f) f (0) f (0) f) f (0) f (0) f (0) f) f (0) f (0) f (0) f) f (0) f (0) f (0) f) f (0) f) f (0) f (0) f) f (0) f) f) f (0) f (0) f) f (0) f) f (0) f) f (0) f) f (0) f (0) f) f) f) f (0) f) f (0) f) f (0) f) f (0) f) f) f (0) f) f (0) f) f (0) f) f) f (0) f (0) f) f (0) f (0) f) f (0) f (0) f (0) f) f) f (0) f) f (0) f (0) f) f) f (0) f (0) f (0) f (0) f (0) f) f (0) f (0) f (0) f (0) f) f) f (0) f (0) f (0) f (0) f) f (0) f) f (0) f (0) f"}, {"heading": "IV. APPLICATIONS", "text": "The applicability of the approach presented here requires the ability to question a database for the number of occurrences and side effects of the elements in the database we want to analyze. The challenge is to find a database that has a sufficient gap to contain a significant number of nodes for searching for the underlying terms. The first question is that Google limits the number of programmatic searches in a single day to a maximum of 100 queries, and charges for queries to the tune of up to $50 per thousand. The second problem with using Google search pages is that the numbers are not exact, but are generated using an approximate algorithm that Google has not disclosed."}, {"heading": "V. CONCLUSION", "text": "Consider queries to a search engine using a database divided into blocks called web pages. On each query, the search engine returns a series of web pages. We propose a method called Normalized Web Distance (NWD) for query records, which quantifies in a single number between 0 and 1 the way in which queries in the set are similar: 0 means that all queries in the set are equal (the set has cardinality one) and 1 means that all queries in the set are maximally dissimilar to each other. February 23, 2015 DRAFT17The similarity between the queries uses the frequency of web pages returned for each query and the set of queries. The method can be applied to any large database and a search engine that provides reliable aggregate numbers of pages. As this method uses names for the object and not the objects themselves, we can consider the common similarity of queries as common semantics between these names (words or phrases)."}, {"heading": "A. Strings and the Self-Delimiting Property", "text": "We write a string to mean a finite binary string, and designate the empty string. (If the string is above a larger finite alphabet, it is converted to a binary one.) The length of a string x (the number of bits in it) is denoted by | x |. Thus, the self-delimiting code for x of length n is x = 1 | x | 0x of length 2n + 1 or even shorter x \u2032 = 1x x of length n + 2 log n + 1 (see [12] for even shorter self-delimiting codes). Self-delimiting codewords encode where they end. The advantage is that when many strings of different length are self-delimiting using the same code, their concatenation in their associated code words can be parsed in one pass (from left to right)."}, {"heading": "B. Computability Notions", "text": "A pair of integers such as (p, q) can be interpreted as a rational p / q. We assume a function with rational arguments and values. A function f (x) with x rational is above the calculation limit if it is defined by a rationally weighted total calculation function \u03c6 (x, k) with x a rational number and k a non-negative integer, so that \u03c6 (x, k + 1) \u2264 \u03c6 (x, k) for each k and limk \u2192 \u221e \u03c6 (x, k) = f (x), which means that f can be calculated from above (see [12], p. 35). A function f is less semi-calculable if \u2212 f is semi-calculable from above. If a function is both above and below half-calculable, then it is calculable."}, {"heading": "C. Kolmogorov Complexity", "text": "Kolmogorov complexity is the information in a single finite object [8]. Informally, the Kolmogorov complexity of a finite binary string is the length of the shortest string from which the original can be reconstructed losslessly by an effective general purpose computer like a certain universal Turing machine. Thus, it is a lower limit on the extent to which a lossless compression program can be compressed. For technical reasons, we choose Turing machines with a separate, readable input tape scanned from left to right, without support, a separate workband on which the compilation takes place, an auxiliary paper tape labeled with the auxiliary information, and a separate output tape. All tapes are divided into squares and are semi-infinite. Originally, the input tape contains a semi-infinite binary tape with one bit per square, starting with one square per square, and changing each of its heads."}, {"heading": "D. Metricity", "text": "A distance function d on X is defined by d: X \u2192 R +, where R + is the set of non-negative real numbers. If X, Y, Z, X, then Z = XY, if Z is the set consisting of the elements of the propositions X and Y, which are ordered lengthwise lexicographically. A distance function d is a metric if1) Positive determination: d (X) = 0, if all elements of X are equal and d (X) > 0 otherwise. (For propositions means equality of all members | X | = 1.) 2) Symmetry: d (X) is invariant under all permutations of X. 3) Triangular inequality: d (XY) \u2264 d (XZ) + d (ZY)."}, {"heading": "E. Proofs", "text": "Proof: of Theorem II.5. We can run all programs dovetailed fashion and at each time instant select a =.1 Definition that with inputs e (x) for all x x X has terminated with the same output e (X).February 23, 2015 DRAFT20The lengths of these shortest programs are getting shorter and shorter, and in the growing time we have EGmax (X) = max (X) | e (y) | e (y)."}], "references": [{"title": "Information distance", "author": ["C.H. Bennett", "P. G\u00e1cs", "M. Li", "P.M.B. Vit\u00e1nyi", "W. Zurek"], "venue": "IEEE Trans. Inform. Theory, 44:4", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Measuring semantic similarity between words using web search engines", "author": ["D. Bollegala", "M. Yutaka", "I. Mitsuru"], "venue": "Proc. WWW., Vol. 766", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic keyword prediction using Google similarity distance", "author": ["P.-I. Chen", "S.-J. Lin"], "venue": "Expert Systems with Applications, 37:3", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic Summarization of Changes in Biological Image Sequences using Algorithmic Information Theory", "author": ["A.R. Cohen", "C. Bjornsson", "S. Temple", "G. Banker", "B. Roysam"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The Google similarity distance", "author": ["R.L. Cilibrasi", "P.M.B. Vit\u00e1nyi"], "venue": "IEEE Trans. Knowledge and Data Engineering, 19:3", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "On the symmetry of algorithmic information", "author": ["P. G\u00e1cs"], "venue": "Soviet Math. Doklady, 15:1477\u20131480, 1974. Correction, Ibid., 15", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1974}, {"title": "W", "author": ["R. Gligorov"], "venue": "ten Kate, Z. Aleksovski and F. van Harmelen, Using Google distance to weight approximate ontology matches, Proc. 16th Intl Conf. World Wide Web, ACM Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Three approaches to the quantitative definition of information", "author": ["A.N. Kolmogorov"], "venue": "Problems Inform. Transmission 1:1", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1965}, {"title": "A device for quantizing", "author": ["L.G. Kraft"], "venue": "grouping, and coding amplitude modulated pulses, MS Thesis, EE Dept., Massachusetts Institute of Technology, Cambridge. Mass., USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1949}, {"title": "Laws of information conservation (nongrowth) and aspects of the foundation of probability theory", "author": ["L.A. Levin"], "venue": "Probl. Inform. Transm., 10", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1974}, {"title": "Information shared by many objects", "author": ["C. Long", "X. Zhu", "M. Li", "B. Ma"], "venue": "Proc. 17th ACM Conf. Information and Knowledge Management", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P.M.B. Vit\u00e1nyi"], "venue": "Springer-Verlag, New York, Third edition", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Two inequalities implied by unique decipherability", "author": ["B. McMillan"], "venue": "IEEE Trans. Information Theory, 2:4", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1956}, {"title": "Quantitative Analysis of Culture", "author": ["J.-B. Michel", "Y.K. Shen", "A.P. Aiden", "A. Veres", "M.K. Gray", "T.G.B. Team"], "venue": "Using Millions of Digitized Books, Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "On Spectral Clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M. Jordan", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "The mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "Bell System Tech. J., 27", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1948}, {"title": "Estimating the number of clusters in a dataset via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "Journal of the Royal Statistical Society 63:(2001)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Information distance in multiples", "author": ["P.M.B. Vit\u00e1nyi"], "venue": "IEEE Trans. Inform. Theory, 57:4", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "and E", "author": ["I.H. Witten"], "venue": "Frank, Data Mining: Practical Machine Learning Tools and Techniques", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Asymmetric information distances for automated taxonomy construction", "author": ["W.L. Woon", "S. Madnick"], "venue": "Knowl. Inf. Systems, 21", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Representation of the online tourism domain in search engines", "author": ["Z. Xian", "K. Weber", "D.R. Fesenmaier"], "venue": "J. Travel Research, 47:2", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Genome-wide association study of Alzheimer\u2019s disease.", "author": ["Kamboh", "M. I"], "venue": "Transl Psychiatry", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Post-GWAS Functional Characterization of Susceptibility Variants for Chronic Lymphocytic Leukemia.", "author": ["Sill", "F.C. M"], "venue": "PLoS One", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Chromosome 6p22 Locus Associated with Clinically Aggressive Neuroblastoma.", "author": ["Maris", "J. M"], "venue": "New England Journal of Medicine", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Two New Loci for Body-Weight Regulation Identified in a Joint Analysis of Genome-Wide Association Studies for Early-Onset Extreme Obesity in French and German Study Groups.", "author": ["A Scherag"], "venue": "PLoS Genet", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "GWAS risk factors in Parkinson\u2019s disease: LRRK2 coding variation and genetic interaction with PARK16.", "author": ["Soto-Ortolaza", "A. I"], "venue": "Am J Neurodegener Dis", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Prefrontal dysfunction in schizophrenia involves mixed-lineage leukemia 1-regulated histone methylation at GABAergic gene promoters.", "author": ["Huang", "H. S"], "venue": "J Neurosci", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Obesity and the risk of Parkinson\u2019s disease.", "author": ["H Chen"], "venue": "Am J Epidemiol", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "The classic notion of Kolmogorov complexity [8] is an objective measure for the information in a single object, and information distance measures the information between a pair of objects [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "The classic notion of Kolmogorov complexity [8] is an objective measure for the information in a single object, and information distance measures the information between a pair of objects [1].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "The similarity or relative semantics between pairs of search terms was defined in [5] and demonstrated in practice by using the World Wide Web as database and Google as search engine.", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 21, "endOffset": 24}, {"referenceID": 20, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "See for example [2], [7], [21], [20], [3] and the many references to [5] in Google scholar.", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "In [11] the notion is introduced of the information required to go from any object in a finite multiset (a set where a member can occur more than once) of objects to any other object in the set.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18] the mathematical theory is developed further and the difficulty of normalization is shown.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "A probability mass function on a known set allows us to define the associated prefix-code word length (information content) equal to unique decodable code word length [9], [13].", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "A probability mass function on a known set allows us to define the associated prefix-code word length (information content) equal to unique decodable code word length [9], [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "By the ubiquitous Kraft inequality [9], if l1, l2, .", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Then NWD(X) \u2208 [0, 1].", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "(Note that to keep NWD(X) \u2208 [0, 1] for all X we have the factor (|X| \u2212 1) in the denominator of NWD(X).", "startOffset": 28, "endOffset": 34}, {"referenceID": 4, "context": "For the questions considered previously [5] we found that these approximate measures were sufficient at that time to generate useful answers, especially in the absence of any a priori domain knowledge.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "For the pairwise formulation, we use the gap spectral clustering unsupervised approach developed in [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 16, "context": "Gap spectral clustering uses the gap statistic as first proposed in [17] to estimate the number of clusters in a data set from an arbitrary clustering algorithm.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "In [4], it was shown that the gap statistic in conjunction with a spectral clustering [15] of the distance matrix obtained from pairwise NWD measurements is an estimate of randomness deficiency for clustering models.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [4], it was shown that the gap statistic in conjunction with a spectral clustering [15] of the distance matrix obtained from pairwise NWD measurements is an estimate of randomness deficiency for clustering models.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Randomness deficiency is a measure of the meaningful information that a model, here a clustering algorithm, captures in a particular set of data [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "In practice, this is achieved by picking the first number of clusters where the gap value achieves a maximum as described in [4].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "Following [4] we set B to 100.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "We used the page counts returned using the built in search from each website for the frequencies, and following the approach in [5] choose N as the frequency for the search term \u2019the\u2019.", "startOffset": 128, "endOffset": 131}, {"referenceID": 18, "context": "include in parenthesis the 95% confidence interval for the result, computed as described in [19] The first three classification questions we considered used the wikipedia search engine.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "The diseases included Alzheimers [22], Parkinsons [27], Amyotrophic lateral sclerosis (ALS) [23], Schizophrenia [28], Leukemia [24], Obesity [26], and Neuroblastoma [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 26, "context": "After these findings we found that there actually have been recent findings of strong relationships between both Schizophrenia and Leukemia [29] as well as between Parkinsons and Obesity [30], relationships that have also been identified by clinical evidence not relating to GWAS approaches.", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "After these findings we found that there actually have been recent findings of strong relationships between both Schizophrenia and Leukemia [29] as well as between Parkinsons and Obesity [30], relationships that have also been identified by clinical evidence not relating to GWAS approaches.", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The self-delimiting code for x of length n is x\u0304 = 1|x|0x of length 2n+ 1, or even shorter x\u2032 = 10x of length n+ 2 log n+ 1 (see [12] for still shorter self-delimiting codes).", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "This means that f can be computed from above (see [12], p.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "The Kolmogorov complexity is the information in a single finite object [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "Theory and applications are given in the textbook [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "The symmetry of information property [6] for strings x, y is", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "It is not computable since for X = {x, y} we have EGmax(X) = max{K(e(x)|e(y)), K(e(y)|e(x))}+O(1), the information distance between e(x) and e(y) which is known to be incomputable [1].", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "since for every x the set {EGmax(X) : X 3 x & EGmax(X) > 0} is the length set of a binary prefix code and therefore the summation above satisfies the Kraft inequality [9] given by (II.", "startOffset": 167, "endOffset": 170}, {"referenceID": 11, "context": "Let m denote the universal distribution [12].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "It is chosen such that the quotient NWD(X) has a value in [0, 1] (Lemma III.", "startOffset": 58, "endOffset": 64}], "year": 2015, "abstractText": "Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or any other large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a similarity on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity according to all (upper semi)computable properties. We develop the theory and give applications. The derivation of the NWD method is based on Kolmogorov complexity.", "creator": "LaTeX with hyperref package"}}}