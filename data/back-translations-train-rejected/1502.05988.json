{"id": "1502.05988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Deep Learning for Multi-label Classification", "abstract": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature", "histories": [["v1", "Wed, 17 Dec 2014 12:06:47 GMT  (339kb,D)", "http://arxiv.org/abs/1502.05988v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jesse read", "fernando perez-cruz"], "accepted": false, "id": "1502.05988"}, "pdf": {"name": "1502.05988.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Multi-label Classification", "authors": ["Jesse Read", "Fernando Perez-Cruz"], "emails": [], "sections": [{"heading": null, "text": "In this context, it should be noted that this project is a project, which is a project, which is primarily a project, which is about putting the needs of people at the centre of society, and which is about putting the needs of people at the centre."}, {"heading": "II. PRIOR WORK", "text": "In fact, it is in such a way that most people who are in a position to put themselves in the world, to put themselves in the world, to put themselves in the world, in the world, in the world, in the world, in the world in which they live, in the world in which they live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in itself, in itself, in the world, in itself, in itself, in the world, in itself, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in itself, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in itself, in the world, in itself, in the world, in itself, in the world, in itself, in the world, in the world, in itself, in the world, in itself, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in the world, in itself, in itself, in the world, in the world, in the world, in the world, in the world, in itself, in itself, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in the world, in the world, in the world, in itself, in the world, in itself, in the world, in the world, in itself, in the world, in the world, in the world, in the world, in the world, in the world, in itself, in itself, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in"}, {"heading": "III. DEEP LEARNING WITH RESTRICTED BOLTZMANN MACHINES", "text": "A well-known approach of deep learning consists in modelling each level of higher characteristics in a restricted Boltzmann machine [9]."}, {"heading": "A. Preliminaries", "text": "For everything that follows: X-Rd is the input domain of all possible attribute values. An instance is represented as a vector of the d attribute values x = [x1,.., xd]. The set is the output domain of the possible L labels. Each instance x is associated with a subset of these labels Y, which is typically represented by a binary vector y = [y1,..., yL], where yj = 1, if and only the jth label is associated with instance x, and 0 otherwise. We assume a series of training data from N-designated examples {(xi, yi)} Ni = 1; yi is the labeling vector mapping of the ith example; y (i) j is the relevance of the jth label for the ith example."}, {"heading": "B. Restricted Boltzmann Machines", "text": "A Boltzmann machine is a type of fully networked neural network that can reveal the underlying regularities of the (observed) training data [1]. If many features are involved, this type of network is traceable only in the restricted Boltzmann machine setting [9], where units are fully connected between layers but not connected within layers. An RBM can capture a layer of u hidden feature variables from the original d feature variables of a training set (usually u < d). These hidden variables can provide a compact representation of the underlying pattern and structure of the input. In fact, an RBM can capture input areas in 2u, whereas a standard cluster formation requires O (2u) parameters and examples to capture this much complexity. Figure 2 shows an RBM can be used as a graphical model with two sets of nodes: visible (X, variable, layered and)."}, {"heading": "C. Deep Belief Networks", "text": "RBMs can be stacked to so-called DBNs [9]. RBMs are greedily formed: the first RBM occupies input range X and produces output Z (1), then the second RBM treats Z (1) as if it were the input room, producing Z (2), and so on. When used for label classification, the last output layer is typically a softmax function (located where only one of the output units should be turned on to specify one of the K classes). In the following section, we outline our approach to create DBNs suitable for multi-label classification."}, {"heading": "IV. DEEP BELIEF NETWORKS (DBNS) FOR MULTI-LABEL CLASSIFICATION", "text": "So we can assume that the hidden layer of data will be more closely related to the labels than the original data, so it makes sense to use it as a feature to classify the instances. Hence, by using the hidden space that RBM has created, we expect the hidden layers of data to be more closely related to the labels than the original data. We do this simply by using the hidden representation of each instance as an instance to classify the instances. Hence, by using the hidden space that RBM has created to achieve better performance. We do this simply by using the hidden representation of each instance as an instance and associating it with the labels to create training sets."}, {"heading": "V. EXPERIMENTS", "text": "We perform empirical evaluations to measure the effectiveness and efficiency of RBMs and DBNs in a number of different multi-label classification scenarios, using different learning algorithms and a large collection of databases. We have implemented these methods in the MEKA Framework1, an open source Java framework with a number of important multi-label benchmarking methods. Within this framework, RBMs can easily be used in a variety of multi-label schemes. The source code of our implementations is provided as part of the MEKA Frameworking.We selected commonly used data sets from a variety of domains listed in Table I. Together with some basic statistics about them. The data sets differ significantly in terms of the type of data and its dimensions (number of labels, characteristics and examples).In music, instances of music are associated with emotions; scenes are documented in associations with Reuters, and can be described in detail in medical categories."}, {"heading": "A. RBM performance", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "B. DBN performance", "text": "After analyzing the performance of the RBM generated characteristics, we will focus on two DBN structures for multi-label classification: \u2022 DBN2ECC: a network of two hidden layers, the last of which merges with the labels in a new dataset and is trained with ECC (see Figure 3a) \u2022 DBN3bp: a network of three hidden layers in which the last layer represents the labels; finely tuned with reverse propagation (see Figure 3b). Both arrangements can be represented in Figure 6, where h \u2261 W 'in the case of DBN3bp.We use u = d / 5 hidden units, 1000 RBM epochs, 100 BP epochs (on DBN3bp), and the best of both arrangements can be represented in Figure 6, with h = 0.1 and \u03b1 = 0.8, \u03bb = 0.1 on an internal traction / test validation, the BBC, and the choice and learning rate and dynamics."}, {"heading": "VI. CONCLUSIONS", "text": "Our empirical analysis of a variety of multi-label data sets shows that a selection of powerful multi-label methods from literature can be improved by using an RBM-processed feature space. It is easier to model labels during training and can be predicted over time. We achieved an improvement in accuracy of up to 15 percentage points compared to the direct use of the original feature space. Our study showed that in multi-label classification, important improvements in both scalability and predictive capability can be achieved when applying deep learning in the field of multi-label classification. As a result, we can recommend multi-labels to focus more on feature modeling and not only on modeling dependencies between output labels. Overall, our multi-label DBN models achieved the best prediction performance compared to seven competing methods from the multi-label literature."}], "references": [{"title": "A learning algorithm for boltzmann machines", "author": ["David H. Ackley", "Geoffrey E. Hinton", "Terrence J. Sejnowski"], "venue": "Cognitive Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["Weiwei Cheng", "Krzysztof Dembczy\u0144ski", "Eyke H\u00fcllermeier"], "venue": "In ICML\u201910: 27th International Conference on Machine", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Combining instance-based learning and logistic regression for multilabel classification", "author": ["Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Multilabel classification via calibrated label ranking", "author": ["Johannes F\u00fcrnkranz", "Eyke H\u00fcllermeier", "Eneldo Loza Menc\u0131\u0301a", "Klaus Brinker"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Discriminative methods for multi-labeled classification", "author": ["Shantanu Godbole", "Sunita Sarawagi"], "venue": "Eighth Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Multi-label classification using conditional dependency networks", "author": ["Yuhong Guo", "Suicheng Gu"], "venue": "In IJCAI \u201911: 24th International Conference on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey Hinton"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey Hinton", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Multilabel prediction via compressed sensing", "author": ["Daniel Hsu", "Sham M. Kakade", "John Langford", "Tong Zhang"], "venue": "In NIPS \u201909: Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Scalable Multi-label Classification", "author": ["Jesse Read"], "venue": "PhD thesis, University of Waikato,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Multi-label classification using ensembles of pruned sets", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoff Holmes"], "venue": "In ICDM \u201908: Eighth IEEE International Conference on Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Classifier chains for multi-label classification", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoff Holmes", "Eibe Frank"], "venue": "In ECML \u201909: 20th European Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Classifier chains for multi-label classification", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoffrey Holmes", "Eibe Frank"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Neural networks and related methods for classification", "author": ["B.D. Ripley"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["Robert E. Schapire", "Yoram Singer"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Predicting gene function using hierarchical multi-label decision tree ensembles", "author": ["Leander Schietgat", "Celine Vens", "Jan Struyf", "Hendrik Blockeel", "Dragi Kocev", "Saso Dzeroski"], "venue": "BMC Bioinformatics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Feedforward nets for interpolation and classification", "author": ["Eduardo D. Sontag"], "venue": "J. Comp. Syst. Sci,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "A comparison of multi-label feature selection methods using the problem transformation approach", "author": ["Newton Spolar", "Everton Alvares Cherman", "Maria Carolina Monard", "Huei Diana Lee"], "venue": "Electronic Notes in Theoretical Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Multi-label classification with principle label space transformation", "author": ["Farbound Tai", "Hsuan-Tien Lin"], "venue": "In Workshop Proceedings of Learning from Multi-Label", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Mining multi-label data", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "Data Mining and Knowledge Discovery Handbook", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Multi label classification: An overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Random k-labelsets: An ensemble method for multilabel classification", "author": ["Grigorios Tsoumakas", "Ioannis P. Vlahavas"], "venue": "In ECML \u201907: 18th European Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Model-shared subspace boosting for multi-label classification", "author": ["Rong Yan", "Jelena Tesic", "John R. Smith"], "venue": "In KDD \u201907: 13th ACM SIGKDD International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Bayesian chain classifiers for multidimensional classification", "author": ["Julio H. Zaragoza", "Luis Enrique Sucar", "Eduardo F. Morales", "Concha Bielza", "Pedro Larra\u00f1aga"], "venue": "In IJCAI\u201911: 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "LIFT: Multi-label learning with label-specific features", "author": ["Min-Ling Zhang"], "venue": "In IJCAI\u201911: 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Multi-label learning by exploiting label dependency", "author": ["Min-Ling Zhang", "Kun Zhang"], "venue": "ACM SIGKDD International conference on Knowledge Discovery and Data mining,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "ML-KNN: A lazy learning approach to multi-label learning", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}], "referenceMentions": [{"referenceID": 11, "context": "The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, audio data, still images and video, and bioinformatics, [12], [22], [23] and the references therein.", "startOffset": 177, "endOffset": 181}, {"referenceID": 21, "context": "The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, audio data, still images and video, and bioinformatics, [12], [22], [23] and the references therein.", "startOffset": 183, "endOffset": 187}, {"referenceID": 22, "context": "The multi-label context is receiving increased attention and is applicable to a wide variety of domains, including text, audio data, still images and video, and bioinformatics, [12], [22], [23] and the references therein.", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": ", [22], [15].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [22], [15].", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 220, "endOffset": 223}, {"referenceID": 4, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 225, "endOffset": 228}, {"referenceID": 13, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 230, "endOffset": 234}, {"referenceID": 23, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 236, "endOffset": 240}, {"referenceID": 3, "context": "To date, many successful multi-label algorithms have been obtained by the so-called problem transformation methods (where the multi-label problem is transformed into several multi-class or binary problems), for example, [2], [5], [14], [24], [4].", "startOffset": 242, "endOffset": 245}, {"referenceID": 13, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 162, "endOffset": 166}, {"referenceID": 27, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 185, "endOffset": 189}, {"referenceID": 24, "context": "Most of the highest performing methods also use ensembles, for example with support vector machines (SVMs) [14], [24], decision trees [18], probabilistic methods [26], [28] or boosting [17], [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "A Restricted Boltzmann Machine (RBM) [9] learns a layer of hidden features in an unsupervised fashion.", "startOffset": 37, "endOffset": 40}, {"referenceID": 21, "context": "An overview of the most well known and influential work in this area is provided in [22], [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "An overview of the most well known and influential work in this area is provided in [22], [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "A well-known alternative is the label powerset (LP) method [23] which transforms the multi-label problem into singlelabel problem with a single class, having the powerset as the set of values (i.", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "The complexity issue has been addressed in works such as [24] and [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "The complexity issue has been addressed in works such as [24] and [13].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "The classifier chain approach (CC) [15] has received recent attention, for example in [3] and [26].", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "The classifier chain approach (CC) [15] has received recent attention, for example in [3] and [26].", "startOffset": 86, "endOffset": 89}, {"referenceID": 25, "context": "The classifier chain approach (CC) [15] has received recent attention, for example in [3] and [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "The predictions result more naturally in a set of pairwise preferences than a multi-label prediction (thus becoming popular in ranking schemes), but PW methods can be adapted to make multi-label predictions, for example [5].", "startOffset": 220, "endOffset": 223}, {"referenceID": 29, "context": "MLkNN [30] is a k-nearest neigh-", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "BPMLL [29] is a back-propagation neural network adapted for multi-label classification by having multiple binary outputs as the label variables.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "[20] presents an overview of the main techniques with respect to problem transformation methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In [27] a clustering-based supervised approach is used to obtain label-specific features for each label.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [25] redundancy is eliminated from the learning space of the BR method by taking random subsets of the training space across an ensemble.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Compressive sensing techniques have also been used in the literature for reducing the complexity multi-label data by taking advantage of label sparsity [21], [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Compressive sensing techniques have also been used in the literature for reducing the complexity multi-label data by taking advantage of label sparsity [21], [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 21, "context": "More examples of featurespace reduction for multi-label classification are reviewed in [22].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "The authors of [7] use a fully-connected network closely related to a Boltzmann machine for multi-label classification, using Gibbs sampling for inference.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "A well-known approach to deep learning is to model each layer of higher level features in a restricted Boltzmann machine [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "A Boltzmann machine is a type of fully-connected neural network that can be used to discover the underlying regularities of the (observed) training data [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "When many features are involved, this type of network is only tractable in the restricted Boltzmann machine setting [9], where units are fully connected between layers, but are unconnected within layers.", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Contrastive divergence [8] is typically used for this task.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "RBMs can be stacked to form so-called DBNs [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "Since the sub-optimality produced by greedy learning is not necessarily harmful to many discriminative supervised methods [10], we can treat the final hidden layer variables Z as the feature input variables, and train any off-the-shelf multi-label model h that can predict", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "to the neural network of BPMLL [29], except that create the layers and initialize the weights using RBMs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "We can employ back propagation to fine-tune the network in a supervised fashion (with respect to label assignments) as in, for example, [9] (for single-label classification).", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "In the multi-label context, this has previously been done to the basic BR method in [6], where a second BR is trained on the outputs of the first (a stacking approach).", "startOffset": 84, "endOffset": 87}, {"referenceID": 18, "context": ", [19], [16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [19], [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "These datasets are described in greater detail in [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "As it is unclear what should be the best ordering, we use an ensemble of 50 CC, in which the labels are randomly ordered in each realization (as in [15]).", "startOffset": 148, "endOffset": 152}, {"referenceID": 22, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 58, "endOffset": 61}, {"referenceID": 14, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "In Table IIa, we report the accuracy, as defined in [23], [6], [15], [13], to report the performance of our multi-label classifiers2:", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "2There are a variety of multi-label evaluation measures used in multi-label experiments in the literature; [22] provides an overview of some of the most popular.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The accuracy provides a good balance to gauge the overall predictive performance of multi-label methods [12], [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The accuracy provides a good balance to gauge the overall predictive performance of multi-label methods [12], [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "We find that overall it obtains better predictive performance than the pairwise methods that create decision boundaries between labels (where yjk \u2208 {01, 10}), as in [5], for example, especially with SVMs.", "startOffset": 165, "endOffset": 168}], "year": 2015, "abstractText": "In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multilabel methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature.", "creator": "LaTeX with hyperref package"}}}