{"id": "1502.06105", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2015", "title": "Regularization and Kernelization of the Maximin Correlation Approach", "abstract": "Robust classification becomes challenging when classes contain multiple subclasses. Examples include multi-font optical character recognition and automated protein function prediction. In correlation-based nearest-neighbor classification, the maximin correlation approach (MCA) provides the worst-case optimal solution by minimizing the maximum misclassification risk through an iterative procedure. Despite the optimality, the original MCA has drawbacks that have limited its wide applicability in practice. That is, the MCA tends to be sensitive to outliers, cannot effectively handle nonlinearities in datasets, and suffers from having high computational complexity. To address these limitations, we propose an improved solution, named regularized maximin correlation approach (R-MCA). We first reformulate MCA as a quadratically constrained linear programming (QCLP) problem, incorporate regularization by introducing slack variables into the primal problem of the QCLP, and derive the corresponding Lagrangian dual. The dual formulation enables us to apply the kernel trick to R-MCA so that it can better handle nonlinearities. Our experimental results demonstrate that the regularization and kernelization make the proposed R-MCA more robust and accurate for various classification tasks than the original MCA. Furthermore, when the data size or dimensionality grows, R-MCA runs substantially faster by solving either the primal or dual (whichever has a smaller variable dimension) of the QCLP.", "histories": [["v1", "Sat, 21 Feb 2015 14:37:44 GMT  (2286kb)", "http://arxiv.org/abs/1502.06105v1", null], ["v2", "Tue, 29 Mar 2016 04:42:12 GMT  (1909kb)", "http://arxiv.org/abs/1502.06105v2", "Submitted to IEEE Access"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["taehoon lee", "taesup moon", "seung jean kim", "sungroh yoon"], "accepted": false, "id": "1502.06105"}, "pdf": {"name": "1502.06105.pdf", "metadata": {"source": "CRF", "title": "Regularization and Kernelization of the Maximin Correlation Approach", "authors": ["Taehoon Lee", "Taesup Moon", "Seung Jean Kim", "Sungroh Yoon"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 2.06 105v 1 [cs.C V] 21 Feb 2015 1Index terms nearest neighbor, correlation, maximin, SOCP, QCLP, QP, regularization, kernel trick."}, {"heading": "1 INTRODUCTION", "text": "The main problems arising from the different tasks are that (1) it becomes computer-intensive to find the neighbors as the number of training samples, and (2) the notion of next neighbors in high-dimensional spaces can break up. Approaches that have been proposed to reduce the computer-conditioned complexity [8] and determine the next neighbors (even in high-dimensional spaces). Template Matching is another widely used technique that is a representative vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vectors-vector-v"}, {"heading": "2 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Maximin Correlation Approach (MCA)", "text": "If u and x are column vectors, the centered correlation is defined as \u03c6 (u, x) = uTx / | | u | | | 2 | | x | | | 2. MCA can construct a template vector u that maximizes the minimum correlation by the following formulation: Maximize the Minx and X values (u, x) subject to | | u | | 2 6 = 0. (1) Optimization (1) is referred to as the MCA problem (MCAP). The original MCA [12] assumes that all X (u, x) points are independent, namely 6 = 0. (1) Optimization (1) is referred to as the MCA problem that maximizes the minimum correlation."}, {"heading": "2.2 Geometric Interpretation and Motivation", "text": "Fig. 1 shows the geometric interpretation and comparison of MCA and the proposed R-MCA, which will be formally defined in the next section. As shown in Fig. 1 (a), the MCA solution corresponds to the search for a template vector whose direction minimizes the worst-case angle between the vector and class members. Without outliers, the maximin template that MCA provides represents the group reasonably reasonably reasonable.The existence of outliers significantly impairs the performance of MCA. Fig. 1, for example, shows the scenario in which outliers are added to the data shown in Fig. 1 (a).The maximin template returned by the original MCA swings abruptly toward the outliers because MCA does not recognize outliers. In contrast, the r-maximin template returned by R-MCA takes the outliers and outliers into account, creating a template that more sensibly represents the group as an example from Abb."}, {"heading": "3 PROPOSED R-MCA METHDOLOGY", "text": "In order to propose more efficient solutions for the MCAP (1), we first formulate it as an example of QCLP [16]. The QCLP formulation (2) allows us to find a solution using the general IPMs [17] instead of the iterative method proposed in [12]. The QCLP formulation also allows us to define slip variables that lead to a regulated version (3) that effectively handles outliers. From the regulated version (3), we further derive its storage dual form (7), which indicates the structure suitable for the application of the kernel trick. Finally, to handle non-linearity, we nuclearize the dual form (7) into the nuclearized R-MCA formulation (10)."}, {"heading": "3.1 QCLP Formulation of MCA", "text": "After normalization of the input vectors, (1) becomes an equivalent tomaximize mini = 1,..., n (u T xi) subject | | u | | 2 \u2264 1. The maximizer of the above maximin problem corresponds to the solution of the following optimization problem: maximize t-R subject uTxi \u2265 t, i = 1,.., nu T u \u2264 1. (2) The equivalent formulation (2) for the MCAP with a finite quantity X is simple; it involves minimizing a linear function over m variables, with n linear equality constraints and a square constraint. It is an example of QCLP, a special type of optimization problem that can be solved globally and efficiently by the IPMs \u2212 np."}, {"heading": "3.2 Regularization of MCA", "text": "To construct a representative vector that is more resistant to outliers (see Figure 1 (b) for an example), we apply regulation to MCA. Regularization is a popular technique to prevent overmatch. Bertsimas and Copenhaver recently described a unifying view of the link between rugged station1 and regularization [20]. Specifically, we present a non-negative \"slack\" variable for each xi object, which can help make the optimization problem insensitive to outlier variables. Using the slack variables, we can find the regulated version of QCLP (2) as1. Shielding a statistical problem from noise in the Data5 maximize t-n-en, which are insensitive to the optimization problem compared to uTxi-en. We can use the regulated version of QCLP (2) as1. Shielding a statistical problem from noise in the Data5 maximize t-n-en, which are insensitive to the optimization problem."}, {"heading": "3.3 Analysis of \u03bb and a Comparison with MCA", "text": "To fulfill the constraint 1Tv = 1 therein, we can consider the following four cases: 1) [\u03bb < 1] If the multipliers vi's are less than 1 / n, the constraint vi = 1 cannot be fulfilled, that is, (7) is not feasible if \u03bb < 1,2) [\u03bb = 1] Since vi's is the upper limit, the only solution corresponding to the constraint vi = 1 must be vi = 1 / n for all i. In this case, u \u2212 straid points in the same direction as the centering xi \u2212 n with the scaling factor (u = c \u00b2 v \u00b2 xi = c \u00b2 xi / n). 3) [1 < < n] the constraint vi's at the z-z level is."}, {"heading": "3.4 Kernelization of (Regularized) MCA", "text": "Nonlinearity in the input space can often be better handled in high (possibly infinite) dimensional space = 1.27 (GEtis). Mapping and computing in such high-dimensional spaces can be costly, if not impossible, but if the input data is retrieved only through internal products, we can use the so-called kernel trick [19] to perform implicit mapping and efficient computation. Checking the dual form of (7) immediately suggests that we can apply the kernel trick to R-MCA. Replacing the inner products in (7) with a kernel matrix K results in implicit mapping and efficient computation."}, {"heading": "4 EXPERIMENTAL RESULTS AND DISCUSSION", "text": "For our experiments, we implemented the proposed QCLP-based MCA and R-MCA solvers using the SeDuMi software, a MATLAB toolbox for optimizing symmetrical cones [28]. For comparison, we also prepared implementations of the original iterative solution for MCA, as described in [12], the Support Vector Machine (SVM), and logistics regression."}, {"heading": "4.1 Effect of Regularization on Subtype Correlation", "text": "To see the effects of regulation, we applied and measured R-MCA with different values of the parameters \u03bb to a dataset of several subclasses, such as the variation in the correlation between subclass objects and the overall template. We used the dataset of the Kennedy Space Center (KSC) [24], which contains 5211 vectors with 176 dimensions. Each vector represents the signal intensities of different wavelengths measured over 13 types of land caps (105-927 vectors per class). Based on the characterization of vegetation, these classes can be grouped into three types or \"superclasses\" (upland with seven subclasses of land coverage, wetlands with five and water types with one). Figure 4 (a) shows the correlation of seven subclasses of the \"upland\" class with the regulated maximums of the aggregate templates (r-maximum in the total template), with the maximum correlation between the non-maximum values being 2.0, 2.8, 3.6, 1.4, 3.6."}, {"heading": "4.2 Effect of Regularization Parameter \u03bb", "text": "In Section 4.1, we discussed that the regularization parameter \u03bb functions as a control button that places the result from the use of the r-maximum somewhere between those from the centric template and the non-regulated maximum template. To visualize the effects of different \u03bb, we used the MNIST database of handwritten digits [25]. From this database, we sampled 1135 and 982 images representing the digits \"1\" and \"4,\" respectively. Each sample is a 28 x 28 image that can be represented by a 784-dimensional vector. We performed the PCA of these samples, taking only the first two main components and transforming each of them into a 2-dimensional point, as shown in Figure 4 (b). In the figure, each of the two intake openings magnifies the centric and the r-maximum region along with the corresponding images for visual inspection."}, {"heading": "4.3 Effect of Regularization on Classification", "text": "In order to see the regulatory effects in the context of the classification, we performed the binary classification of the SONAR data [26], which consists of 111 mine-reflected and 97 rock-reflected sonar signals of 60 dimensions each. For the NN classification using templates, we implemented the closest template classifier, which assigns an unknown vector to the class of its next (r-maximin, maximin or centric) template. For comparison, we also tested logistic regression and linear SVM. According to the experimental results from the use of neural networks in [26], non-linearity exists in the distribution characteristics of the SONAR data. We processed the data using the PCA [29] kernel with the Gaussian kernel (\u03c3 = 1). We then divided the data into two sets (for training and validation) and tested the five different classifiers with 2-fold cross-validation (W9) of the WAU.V (W9) with the 1.1 WAU.V being performed."}, {"heading": "4.4 Effect of Kernelization", "text": "Through kernization, we expect R-MCA to be applicable to classification problems that contain complex shapes in the input space. Fig. 6 shows the results of a proof-of-concept experiment with a synthetic dataset called 3D-NUT, which was created as follows: We took a point x = [x1, x2, x3] from a trivariate normal distribution N (\u00b5, \u03a3), where \u00b5 = [0, 0, 0] and \u03a3 = I. For the sake of visualization, x was discarded if x2 < 0 and x3 < 0. Otherwise, we place x in the \"core\" class if | x | < 1 and the \"shell\" class if | x | > 2. Fig. 6 (a) shows the distribution of 272 points with binary membership (either \"core\" or \"shell\" class) in the input space."}, {"heading": "4.5 Comparison of Execution Time with MCA", "text": "We compare the runtime of the proposed QCLP-based solution and the original iterative solution [12] to the maximum correlation approach. To this end, we have conducted two types of experiments, one varies the number of objects n fixed with the dimensionality m, and the other involves varying m with n fixed. We measure runtime using a Windows 7 PC equipped with an Intel i5-3570K CPU (3.4 GHz, 6MB, 5GT / s) and 16GB RAM. Fig. 7 (a) shows the varying n fixed cases for detecting the digit \"0\" in the MNIST data (fixed m = 784). The time requirement of the iterative solution remained highest and also grew faster than the others. As described in Section 3, there are additional inequalities and variables in the regulated forms [3) and (7)."}, {"heading": "5 CONCLUSION", "text": "The Maximin Correlation Approach (MCA) was originally proposed in the context of the classification problems of several subclasses, ranging from the problem of optical character recognition to the automated prediction of the protein family. MCA's found aggregated templates are well suited for such applications as they can minimize the maximum risk of misclassification in the correlation-based classification structure of the nearest neighbors. However, we first described how to formulate the MCA as an example of QCLP and present an efficient and general solution that can replace the original iterative solution. Based on this QCLP-based formulation, we further explained how to regulate and kernel the MCA in real-world applications to make it more robust for outliers and applicable to non-linearity data."}], "references": [{"title": "An optimal global nearest neighbor metric", "author": ["K. Fukunaga", "T.E. Flick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 6, no. 3, pp. 314\u2013318, May 1984.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1984}, {"title": "When is \u201cnearest neighbor", "author": ["K. Beyer"], "venue": "meaningful?\u201d in Database Theory\u2013ICDT\u201999. Springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Using discriminant eigenfeatures for image retrieval", "author": ["D.L. Swets", "J.J. Weng"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 18, no. 8, pp. 831\u2013836, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient and effective querying by image content", "author": ["C. Faloutsos"], "venue": "Journal of Intelligent Information Systems, vol. 3, no. 3-4, pp. 231\u2013262, 1994.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "P-n learning: Bootstrapping binary classifiers by structural constraints", "author": ["Z. Kalal"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, June 2010, pp. 49\u201356.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Grid-partition index: A hybrid method for nearest-neighbor queries in wireless location-based services", "author": ["B. Zheng"], "venue": "The VLDB Journal, vol. 15, no. 1, pp. 21\u201339, Jan. 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Basic local alignment search tool", "author": ["S.F. Altschul"], "venue": "Journal of molecular biology, vol. 215, no. 3, pp. 403\u2013410, 1990.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "A fast knn algorithm for text categorization", "author": ["Y. Wang", "Z.-O. Wang"], "venue": "Machine Learning and Cybernetics, 2007 International Conference on, vol. 6, Aug 2007, pp. 3436\u20133441.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Diffusion decision making for adaptive k-nearest neighbor classification", "author": ["Y.-K. Noh", "F. Park", "D.D. Lee"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1925\u20131933.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Deformable templates for face recognition", "author": ["A.L. Yuille"], "venue": "Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 59\u201370, 1991.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1991}, {"title": "Recognizing action at a distance", "author": ["A. Efros", "A. Berg", "G. Mori", "J. Malik"], "venue": "Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, Oct 2003, pp. 726\u2013733 vol.2.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple subclass pattern recognition: A maximin correlation approach", "author": ["H.I. Avi-Itzhak", "J.A. Van Mieghem", "L. Rub"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 17, no. 4, pp. 418\u2013431, 1995.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Robust multifont ocr system from gray level images", "author": ["F. Lebourgeois"], "venue": "Document Analysis and Recognition, 1997., Proceedings of the Fourth International Conference on, vol. 1, 1997, pp. 1\u20135 vol.1.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Application of maximin correlation analysis to classifying protein environments for function prediction", "author": ["T. Lee", "H. Min", "S.J. Kim", "S. Yoon"], "venue": "Biochemical and biophysical research communications, vol. 400, no. 2, pp. 219\u2013224, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "K-maximin clustering: a maximin correlation approach to partition-based clustering", "author": ["T. Lee", "S.J. Kim", "E.-Y. Chung", "S. Yoon"], "venue": "IEICE Electronics Express, vol. 6, no. 17, pp. 1205\u20131211, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Applications of second-order cone programming", "author": ["M.S. Lobo", "L. Vandenberghe", "S. Boyd", "H. Lebret"], "venue": "Linear Algebra and its Applications, vol. 284, no. 1-3, pp. 193\u2013228, Nov. 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "An interior-point method for semidefinite programming", "author": ["C. Helmberg", "F. Rendl", "R.J. Vanderbei", "H. Wolkowicz"], "venue": "SIAM Journal on Optimization, vol. 6, pp. 342\u2013361, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Support-Vector Networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, 1995, pp. 273\u2013297.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Characterization of the equivalence of robustification and regularization in linear, median, and matrix regression", "author": ["D. Bertsimas", "M.S. Copenhaver"], "venue": "arXiv preprint arXiv:1411.6160, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Functions of positive and negative type and their connection with the theory of integral equations", "author": ["J. Mercer"], "venue": "Philos. Trans. Royal Soc. (A), vol. 83, no. 559, pp. 69\u201370, Nov. 1909.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1909}, {"title": "Hierarchical Fusion of Multiple Classifiers for Hyperspectral Data Analysis", "author": ["S. Kumar", "J. Ghosh", "M.M. Crawford"], "venue": "Pattern Analysis & Applications, vol. 5, no. 2, pp. 210\u2013220, Jun. 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, November 1998.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Analysis of hidden units in a layered network trained to classify sonar targets", "author": ["R.P. Gorman", "T.J. Sejnowski"], "venue": "Neural Networks, vol. 1, no. 1, pp. 75\u201389, 1988.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1988}, {"title": "Regional variation in gene expression in the healthy colon is dysregulated in ulcerative colitis", "author": ["C.L. Noble"], "venue": "Gut, vol. 57, no. 10, pp. 1398\u2013405, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Using sedumi 1.02, a matlab toolbox for optimization over symmetric cones", "author": ["J.F. Sturm"], "venue": "1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K.R. M\u00fcller"], "venue": "Advances in kernel methods: support vector learning, pp. 327\u2013352, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "NCBI GEO: Mining Millions of Expression Profiles - Database and Tools", "author": ["T. Barrett"], "venue": "Nucleic Acids Research, vol. 33, pp. D562\u2013D566, 2005.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION N EAREST neighbor (NN) classifiers [1], [2] are non-parametric methods that classify an object based on its distance to the nearest trained class.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "1 INTRODUCTION N EAREST neighbor (NN) classifiers [1], [2] are non-parametric methods that classify an object based on its distance to the nearest trained class.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 4, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 238, "endOffset": 241}, {"referenceID": 6, "context": "Owing largely to their simplicity and reasonable performance in practical problems, they have been widely used for various tasks such as image retrieval [3], video indexing [4], object tracking [5], location-dependent information service [6], and sequence alignment [7].", "startOffset": 266, "endOffset": 269}, {"referenceID": 7, "context": "Approaches have been proposed to reduce the computational complexity [8] and to adaptively determine nearest neighbors (even in high-dimensional spaces) [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Approaches have been proposed to reduce the computational complexity [8] and to adaptively determine nearest neighbors (even in high-dimensional spaces) [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 9, "context": "Template matching is another widely used technique that pre-computes a representative vector for each class and uses it to locate the nearest neighbor of an object [10], [11].", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "Template matching is another widely used technique that pre-computes a representative vector for each class and uses it to locate the nearest neighbor of an object [10], [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "In multiple subclass classification problems, where each class contains more than one subclass, a template is constructed for each subclass, and then the aggregate template of a class is created based on the subclass templates [12].", "startOffset": 227, "endOffset": 231}, {"referenceID": 11, "context": "In this paper, we consider constructing the aggregate template based on the idea of the maximin correlation approach (MCA) [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "MCA was originally proposed for multi-font optical character recognition [13], and has been successfully applied to automated protein function prediction [14] and typography clustering [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "MCA was originally proposed for multi-font optical character recognition [13], and has been successfully applied to automated protein function prediction [14] and typography clustering [15].", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "MCA was originally proposed for multi-font optical character recognition [13], and has been successfully applied to automated protein function prediction [14] and typography clustering [15].", "startOffset": 185, "endOffset": 189}, {"referenceID": 15, "context": "As opposed to the iterative method employed by MCA, we reformulate it as an instance of quadratically constrained linear programming (QCLP) [16].", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "In contrast, the proposed QCLP formulation can be solved with linear complexity by the interior-point methods (IPMs) [17] when coefficient matrices are positive semidefinite.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Our formulation has some resemblance to the regularization employed by the soft-margin support vector machine (SVM) [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "We furthermore develop the Lagrangian dual of the regularized QCLP, which enables us to apply the kernel trick [19] to effectively handle nonlinear structures possibly embedded in data.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "Additional details can be found in [12].", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The original MCA [12] assumes that all of the xi\u2019s are linearly independent, ||xi||2 = 1 for all xi \u2208 X , and xi xj \u2265 0 for all xi,xj \u2208 X (note that these assumptions are not required in the proposed R-MCA).", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "An iterative solution to the MCAP was proposed in [12]: the template vector u is initialized to the centroid vector and is updated at each iteration to find the optimal vector u.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "For this reason, in multi-font character recognition, the maximin template, which incorporates outlier information, results in higher accuracy than the centroid template [12], [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "For this reason, in multi-font character recognition, the maximin template, which incorporates outlier information, results in higher accuracy than the centroid template [12], [14].", "startOffset": 176, "endOffset": 180}, {"referenceID": 15, "context": "To propose more efficient solutions to the MCAP (1), we first formulate it as an instance of QCLP [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "The QCLP formulation (2) enables us to find a solution using the general IPMs [17], instead of the iterative method proposed in [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "The QCLP formulation (2) enables us to find a solution using the general IPMs [17], instead of the iterative method proposed in [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "It is an instance of QCLP, a special type of optimization problem that can be solved globally and efficiently by the IPMs [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "Since the number of iterations that is necessary for IPM to find a solution is practically constant (typically from 10 to 50) [16], we can see that the QCLP (2) can be solved in O(nm +m) flops.", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "For comparison, the number of flops required for the iterative method [12] is either 4mnp\u2212mp2 or 4n2p\u2212 2np +mn, depending on the implementation, where p is the number of iterations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "The empirical study in [12] shows that p grows nearly linearly in n.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "Bertsimas and Copenhaver recently described a unifying view of the connection between robustification and regularization [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "This formulation is similar to the optimization problem within the soft-margin support vector machine (SVM) [18], which is a relaxation of the original SVM.", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "Note that minimizing the form vCv occurs frequently in multivariate data analysis, such as the principal component analysis [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "The mapping to and the computation in such high-dimensional spaces can be costly, if not impossible, but when the input data are acessed only through inner products, we can use the so-called kernel trick [19] to perform implicit mapping and efficient computation.", "startOffset": 204, "endOffset": 208}, {"referenceID": 22, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "Name n m Number of classes (description) KSC [24] 5211 176 13 (land cover types) MNIST [25] 10000 784 10 (digits \u20180\u2019\u2013\u20189\u2019) SONAR [26] 208 60 2 (rock or mine) GEO [27] 606 30954 2 (ulcerative colitis patient or not) 3D-NUT 272 3 2 (core or shell)", "startOffset": 161, "endOffset": 165}, {"referenceID": 21, "context": "where Kij = k(xi,xj) for a Mercer kernel k [23].", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "For our experiments, we implemented the proposed QCLP-based MCA and R-MCA solvers using SeDuMi software, a MATLAB toolbox for optimization over symmetric cones [28].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "For comparison, we also prepared implementations of the original iterative solution to MCA as described in [12], the support vector machine (SVM), and the logistic regression.", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "We used the Kennedy Space Center (KSC) dataset [24], which contains 5211 vectors with 176 dimensions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "To visualize the effects of varying \u03bb, we utilized the MNIST database of handwritten digits [25].", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "3 Effect of Regularization on Classification To see the regularization effects in the context of classification, we carried out binary classification of the SONAR data [26], which consists of 111 mine-reflected and 97 rock-reflected sonar signals of 60 dimensions each.", "startOffset": 168, "endOffset": 172}, {"referenceID": 24, "context": "According to the experimental results from using neural networks in [26], nonlinearities exist in the distribution characteristics of the SONAR data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "We thus preprocessed the data using the kernel PCA [29] with the Gaussian kernel (\u03c3 = 1).", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "5 Comparison of Execution Time with MCA We compare the runtime of the proposed QCLP-based solution and the original iterative solution [12] to the maximin correlation approach.", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "We used the NCBI GEO microarray dataset [30] (the accession number: GSE11223), which provides the regional variation of gene expression in ulcerative colitis patients [27].", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "We used the NCBI GEO microarray dataset [30] (the accession number: GSE11223), which provides the regional variation of gene expression in ulcerative colitis patients [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "PSfrag replacements Iterative method [12] R-MCA dual (7) R-MCA primal (3) MCA dual (9) MCA primal (2)", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "As the primal forms and the dual forms have O(m) and O(n) variables, respectively, the same observations can be made from the computational complexity of SeDuMi, which is O(xy + y) [28] (x is the number of variables, and y is the number of independent inequalities).", "startOffset": 181, "endOffset": 185}], "year": 2017, "abstractText": "Robust classification becomes challenging when classes contain multiple subclasses. Examples include multi-font optical character recognition and automated protein function prediction. In correlation-based nearest-neighbor classification, the maximin correlation approach (MCA) provides the worst-case optimal solution by minimizing the maximum misclassification risk through an iterative procedure. Despite the optimality, the original MCA has drawbacks that have limited its wide applicability in practice. That is, the MCA tends to be sensitive to outliers, cannot effectively handle nonlinearities in datasets, and suffers from having high computational complexity. To address these limitations, we propose an improved solution, named regularized maximin correlation approach (R-MCA). We first reformulate MCA as a quadratically constrained linear programming (QCLP) problem, incorporate regularization by introducing slack variables into the primal problem of the QCLP, and derive the corresponding Lagrangian dual. The dual formulation enables us to apply the kernel trick to R-MCA so that it can better handle nonlinearities. Our experimental results demonstrate that the regularization and kernelization make the proposed R-MCA more robust and accurate for various classification tasks than the original MCA. Furthermore, when the data size or dimensionality grows, R-MCA runs substantially faster by solving either the primal or dual (whichever has a smaller variable dimension) of the QCLP. Index Terms nearest neighbor, correlation, maximin, SOCP, QCLP, QP, regularization, kernel trick. \u2726", "creator": "LaTeX with hyperref package"}}}