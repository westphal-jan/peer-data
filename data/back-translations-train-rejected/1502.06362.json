{"id": "1502.06362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2015", "title": "Contextual Dueling Bandits", "abstract": "We consider the problem of learning to choose actions using contextual information when provided with limited feedback in the form of relative pairwise comparisons. We study this problem in the dueling-bandits framework of Yue et al. (2009), which we extend to incorporate context. Roughly, the learner's goal is to find the best policy, or way of behaving, in some space of policies, although \"best\" is not always so clearly defined. Here, we propose a new and natural solution concept, rooted in game theory, called a von Neumann winner, a randomized policy that beats or ties every other policy. We show that this notion overcomes important limitations of existing solutions, particularly the Condorcet winner which has typically been used in the past, but which requires strong and often unrealistic assumptions. We then present three efficient algorithms for online learning in our setting, and for approximating a von Neumann winner from batch-like data. The first of these algorithms achieves particularly low regret, even when data is adversarial, although its time and space requirements are linear in the size of the policy space. The other two algorithms require time and space only logarithmic in the size of the policy space when provided access to an oracle for solving classification problems on the space.", "histories": [["v1", "Mon, 23 Feb 2015 09:47:54 GMT  (140kb,D)", "https://arxiv.org/abs/1502.06362v1", null], ["v2", "Sat, 13 Jun 2015 21:56:25 GMT  (617kb,D)", "http://arxiv.org/abs/1502.06362v2", "25 pages, 4 figures, Published at COLT 2015"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["miroslav dud\\'ik", "katja hofmann", "robert e schapire", "aleksandrs slivkins", "masrour zoghi"], "accepted": false, "id": "1502.06362"}, "pdf": {"name": "1502.06362.pdf", "metadata": {"source": "CRF", "title": "Contextual Dueling Bandits", "authors": ["Miroslav Dud\u0131\u0301k", "Katja Hofmann", "Robert E. Schapire", "Aleksandrs Slivkins", "Masrour Zoghi", "SLIVKINS ZOGHI"], "emails": ["MDUDIK@MICROSOFT.COM", "KATJA.HOFMANN@MICROSOFT.COM", "SCHAPIRE@MICROSOFT.COM", "SLIVKINS@MICROSOFT.COM", "M.ZOGHI@UVA.NL"], "sections": [{"heading": "1. Introduction", "text": "We study how to act on the basis of contextual information when we are provided with only partial, relative feedback. Of course, this problem arises in information retrieval (IR) and recommendation systems, where user feedback is much more reliable when interpreted as relative comparisons rather than absolute labels (Radlinski et al., 2008). For example, in web search, the IR system can have multiple candidate rankings of documents, which are usually dependent on the respective user option. By presenting a mix or linkage of two of the candidate rankings and observing the user's reaction (Chapelle et al., 2012; Hofmann et al., 2013), it is possible to get feedback on user preferences. Some of this research was conducted during an internship with Microsoft Research."}, {"heading": "2. Dueling bandits and the von Neumann winner", "text": "In the dueling bandits, the problem (Yue et al., 2009) is that the learner has access to K's possible actions, 1,., K., and tries to determine the \"best\" action by repeated stochastic pair comparisons of actions called duels. Thus, at each step, the learner selects a pair of actions (a, b) for a duel; the result of the duel is + 1 if a player wins, and \u2212 1 if he wins a game. The (unknown) expected value of this result is called P (a, b) and is assumed to depend only on the selected pair (a, b). In other words, the probability that a beat b is won in a duel is (a, b), and the two actions are exactly matched if P (a, b) = 0."}, {"heading": "3. Incorporating context", "text": "Next, we look at how the previous development can be extended to a much more realistic environment, in which the best way of acting can depend on additional, observable information called context. Thus, before selecting actions, the learner is allowed to observe a certain value x, the context that is selected by nature from an unspecified space X. For example, x could be a feature vector description of a web user. Formally, we assume that in each round of the learning process, a context and preference matrix x x capability is no longer static; rather, which actions are better than those that others now vary and depend on the context, which must therefore be taken into account in order to fully optimize the choice of actions. Formally, we assume that in each round of the learning process, a context and preference matrix capability of Pt is selected by nature. The context text is revealed to the learner, but the preference matrix is determined by the current situation (the preference matrix)."}, {"heading": "4. Learning scenarios", "text": "We consider two possible learning scenarios, except that the learner plays an active role in selecting the actions he or she performs in each game. (We assume that the learner has the opportunity to explore a certain number of rounds (where, as described above, a different strategy is applied in each round). (We assume that the learner may consider the outcome of a duel between a pair of actions of his or her choice.) At the end of these m-rounds, the learner will perform a distribution of actions that he or she can apply to all actions. (W) The goal of the learner is to produce an approximate von Neumann winner, that is, that is, that he or she receives a reward for each action of a different type of player. (W) This setting is almost like learning from a passively selected group of learning examples, except that the learner has an active role in selecting what actions to play. (Of course, we should be \"reasonable\" as a function that we select \"almost as passively as learning from a batch.\""}, {"heading": "6. Explore-then-exploit algorithms with a classification oracle", "text": "We start next with a development that will lead to efficient methods (in terms of time, space and data) to manage even extremely large political spaces, under a certain premise that will be discussed below. We describe a general approach to research, to using the collected data to find a statistically sound solution, and to reduce the problem that needs to be led to a more tractable form. We mainly focus on the exploration-then-exploitation problem, and on each round i, a pair (xi, Pi) is randomly selected, and the learner is allowed to select and observe the outcome of a single duel (ai, bi). Although xi is not observed, we propose a unified exploration strategy in which each dueling pair (ai, bi) is selected to unify the resulting results."}, {"heading": "Acknowledgments", "text": "We would like to thank Alekh Agarwal for his insightful comments and discussions."}, {"heading": "Appendix A. Failure of the Condorcet winner to exist", "text": "Here we examine the reliability of the Condorcet assumption by repeating the experiment of (Zoghi et al., 2014a, Section 6.1) with a small modification. As in their environment, we look at a family of K-armed dueling bandit problems resulting from the Ranker evaluation problem in IR, where comparisons are made using Probabilistic Interleave (Hofmann et al., 2011) and preferences are generated using click models that simulate user behavior (Guo et al., 2009a, b). The Rankers are randomly selected from the group of 136 Rankers equipped with the MSLR dataset. However, in contrast to the experiments of Zoghi et al. (2014a), we use an informative click model instead of a perfect one (Hofmann et al., 2013), the former simulates the behavior of a user seeking general information on a broad topic, while the latter idealized user believes that any problem that is realistically represented in the document is a faulty one."}, {"heading": "Appendix B. Comparison between the Copeland and von Neumann winners", "text": "It is a generalization of the Condorcet Winner in the sense that if the Condorcet Winner exists, he will be a Copeland Winner. However, we maintain that the von Neumann Winner is a more natural generalization than the Copeland Winner for the following two reasons: first, in the absence of a Condorcet Winner, Copeland Winner, Copeland Winner, both individually and collectively, can lose an arm that is not a Copeland Winner, whereas the von Neumann Winner has beats or ties with each arm; second, the number of Copeland Winner can be changed by the introduction of \"clones,\" i.e., arms whose corresponding rows of preference matrix are identical to each other. To demonstrate this lack of stability of the Copeland Winner, we consider each K + 3 armed example with K > 4, where the weapons a1, a2 and a3 beat all other arms in a cycle."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Reducing dueling bandits to cardinal bandits", "author": ["Nir Ailon", "Zohar Karnin", "Thorsten Joachims"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert Schapire"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "A survey of preference-based online learning with bandit algorithms", "author": ["R\u00f3bert Busa-Fekete", "Eyke H\u00fcllermeier"], "venue": "In Algorithmic Learning Theory (ALT),", "citeRegEx": "Busa.Fekete and H\u00fcllermeier.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete and H\u00fcllermeier.", "year": 2014}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Large-scale validation and analysis of interleaved search evaluation", "author": ["Olivier Chapelle", "Thorsten Joachims", "Filip Radlinski", "Yisong Yue"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Chapelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2012}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dud\u0131\u0301k", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "Mathematical games: The paradox of the nontransitive dice and the elusive principle of indifference", "author": ["Martin Gardner"], "venue": "Scientific American,", "citeRegEx": "Gardner.,? \\Q1970\\E", "shortCiteRegEx": "Gardner.", "year": 1970}, {"title": "Tailoring click models to user goals", "author": ["Fan Guo", "Lei Li", "Christos Faloutsos"], "venue": "In Workshop on Web Search Click Data (WSCD),", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "Efficient multiple-click models in web search", "author": ["Fan Guo", "Chao Liu", "Yi-Min Wang"], "venue": "In Proceedings of the International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "A probabilistic method for inferring preferences from clicks", "author": ["Katja Hofmann", "Shimon Whiteson", "Maarten de Rijke"], "venue": "In Proceedings of the International Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Hofmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2011}, {"title": "Fidelity, soundness, and efficiency of interleaved comparison methods", "author": ["Katja Hofmann", "Shimon Whiteson", "Maarten de Rijke"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "Efficient algorithms for the online decision problem", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Kalai and Vempala.,? \\Q2003\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2003}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["John Langford", "Tong Zhang"], "venue": "In Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Langford and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "Iterative ranking from pair-wise comparisons", "author": ["Sahand Negahban", "Sewoong Oh", "Devavrat Shah"], "venue": "In Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["Filip Radlinski", "Madhu Kurup", "Thorsten Joachims"], "venue": "In Proceedings of the International Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "A new monotonic, clone-independent, reversal symmetric, and Condorcetconsistent single-winner election method", "author": ["Markus Schulze"], "venue": "Social Choice and Welfare,", "citeRegEx": "Schulze.,? \\Q2011\\E", "shortCiteRegEx": "Schulze.", "year": 2011}, {"title": "Generic exploration and karmed voting bandits", "author": ["Tanguy Urvoy", "Fabrice Clerot", "Raphael F\u00e9raud", "Sami Naamane"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Urvoy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urvoy et al\\.", "year": 2013}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yisong Yue", "Thorsten Joachims"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Yue and Joachims.,? \\Q2009\\E", "shortCiteRegEx": "Yue and Joachims.", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Yisong Yue", "Thorsten Joachims"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Yue and Joachims.,? \\Q2011\\E", "shortCiteRegEx": "Yue and Joachims.", "year": 2011}, {"title": "The k-armed dueling bandits problem", "author": ["Yisong Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}, {"title": "Relative confidence sampling for efficient on-line ranker evaluation", "author": ["Masrour Zoghi", "Shimon Whiteson", "Maarten de Rijke", "R\u00e9mi Munos"], "venue": "In Proceedings of the International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Relative upper confidence bound for the k-armed dueling bandits problem", "author": ["Masrour Zoghi", "Shimon Whiteson", "R\u00e9mi Munos", "Maarten de Rijke"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "MergeRUCB: A method for large-scale online ranker evaluation", "author": ["Masrour Zoghi", "Shimon Whiteson", "Maarten de Rijke"], "venue": "In Proceedings of the International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}, {"title": "Failure of the Condorcet winner to exist Here, we investigate the reliability of the Condorcet assumption by replicating the experiment of (Zoghi et al., 2014a, Section 6.1) with a small modification. As in their setting, we consider a family of K-armed dueling bandit problems arising from the ranker evaluation problem in IR", "author": [], "venue": null, "citeRegEx": "Appendix,? \\Q2014\\E", "shortCiteRegEx": "Appendix", "year": 2014}, {"title": "Theorem 1.1) prove the following (slightly simplified) result: assume that D, R and A are such that for all d \u2208 D", "author": ["t. Kalai", "Vempala"], "venue": null, "citeRegEx": "Kalai and Vempala,? \\Q2003\\E", "shortCiteRegEx": "Kalai and Vempala", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "We study this problem in the dueling-bandits framework of Yue et al. (2009), which we extend to incorporate context.", "startOffset": 58, "endOffset": 76}, {"referenceID": 17, "context": "This problem naturally arises in information retrieval (IR) and recommender systems, where the user feedback is considerably more reliable when interpreted as relative comparisons rather than absolute labels (Radlinski et al., 2008).", "startOffset": 208, "endOffset": 232}, {"referenceID": 6, "context": "By presenting a mix or interleaving of two of the candidate rankings and observing the user\u2019s response (Chapelle et al., 2012; Hofmann et al., 2013), it is possible for such a system to get feedback about user preferences.", "startOffset": 103, "endOffset": 148}, {"referenceID": 13, "context": "By presenting a mix or interleaving of two of the candidate rankings and observing the user\u2019s response (Chapelle et al., 2012; Hofmann et al., 2013), it is possible for such a system to get feedback about user preferences.", "startOffset": 103, "endOffset": 148}, {"referenceID": 2, "context": "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dud\u0131\u0301k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies \u03a0, with the goal of performing as well as the \u201cbest\u201d in the space.", "startOffset": 58, "endOffset": 146}, {"referenceID": 15, "context": "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dud\u0131\u0301k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies \u03a0, with the goal of performing as well as the \u201cbest\u201d in the space.", "startOffset": 58, "endOffset": 146}, {"referenceID": 7, "context": "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dud\u0131\u0301k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies \u03a0, with the goal of performing as well as the \u201cbest\u201d in the space.", "startOffset": 58, "endOffset": 146}, {"referenceID": 0, "context": "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dud\u0131\u0301k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies \u03a0, with the goal of performing as well as the \u201cbest\u201d in the space.", "startOffset": 58, "endOffset": 146}, {"referenceID": 22, "context": "Most previous work on dueling bandits (Yue et al., 2009; Yue and Joachims, 2011; Urvoy et al., 2013; Zoghi et al., 2014b) has in fact explicitly or implicitly assumed that such a Condorcet winner exists.", "startOffset": 38, "endOffset": 121}, {"referenceID": 21, "context": "Most previous work on dueling bandits (Yue et al., 2009; Yue and Joachims, 2011; Urvoy et al., 2013; Zoghi et al., 2014b) has in fact explicitly or implicitly assumed that such a Condorcet winner exists.", "startOffset": 38, "endOffset": 121}, {"referenceID": 19, "context": "Most previous work on dueling bandits (Yue et al., 2009; Yue and Joachims, 2011; Urvoy et al., 2013; Zoghi et al., 2014b) has in fact explicitly or implicitly assumed that such a Condorcet winner exists.", "startOffset": 38, "endOffset": 121}, {"referenceID": 3, "context": "P multi-armed bandit algorithm (Beygelzimer et al., 2011) are run against one another (using a \u201csparring\u201d approach previously suggested", "startOffset": 31, "endOffset": 57}, {"referenceID": 13, "context": "The dueling-bandits problem of Yue et al. (2009) formalizes this setting.", "startOffset": 31, "endOffset": 49}, {"referenceID": 15, "context": "To address this difficulty, we propose an approach used previously in other works on contextual bandits (Langford and Zhang, 2007; Dud\u0131\u0301k et al., 2011; Agarwal et al., 2014).", "startOffset": 104, "endOffset": 173}, {"referenceID": 7, "context": "To address this difficulty, we propose an approach used previously in other works on contextual bandits (Langford and Zhang, 2007; Dud\u0131\u0301k et al., 2011; Agarwal et al., 2014).", "startOffset": 104, "endOffset": 173}, {"referenceID": 0, "context": "To address this difficulty, we propose an approach used previously in other works on contextual bandits (Langford and Zhang, 2007; Dud\u0131\u0301k et al., 2011; Agarwal et al., 2014).", "startOffset": 104, "endOffset": 173}, {"referenceID": 22, "context": "Numerous algorithms have been proposed for the (non-contextual) dueling bandits problem: Interleaved Filter (Yue et al., 2009); Beat the Mean (BTM) (Yue and Joachims, 2011); Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al.", "startOffset": 108, "endOffset": 126}, {"referenceID": 21, "context": ", 2009); Beat the Mean (BTM) (Yue and Joachims, 2011); Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 19, "context": ", 2009); Beat the Mean (BTM) (Yue and Joachims, 2011); Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., 2013); Relative Confidence Sampling (Zoghi et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": ", 2014b); Doubler, MultiSBM and Sparring (Ailon et al., 2014) and mergeRUCB (Zoghi et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 20, "context": "The method that is the most closely related to our work is Dueling Bandit Gradient Descent (DBGD) (Yue and Joachims, 2009), a policy gradient method that iteratively improves upon the current policy by conducting comparisons with nearby policies, assuming that the policy space comes equipped with a distance metric, and incrementally adapting the policy if a better alternative is encountered.", "startOffset": 98, "endOffset": 122}, {"referenceID": 0, "context": ", 2011; Agarwal et al., 2014). Specifically, we assume that we have access to a classification oracle for our policy class that can find the minimumcost policy in \u03a0 when given the cost of each action on each of a sequence of contexts. In fact, an ordinary cost-sensitive, multiclass classification learning algorithm can be used for this purpose, which suggests that, practically, this may be a reasonable and natural assumption. We then consider techniques for constructing a von Neumann winner from empirical exploration data. (Although we focus on a batch-like setting, the resulting algorithms can be used online as well.) We analyze the statistical efficiency of this approach in Section 6. In Sections 7 and 8, we give two polynomial-time algorithms for computing an approximate von Neumann winner from data: one based on Kalai and Vempala\u2019s Follow-the-Perturbed-Leader algorithm (2003), and the other based on projected gradient ascent as studied by Zinkevich (2003).", "startOffset": 8, "endOffset": 893}, {"referenceID": 0, "context": ", 2011; Agarwal et al., 2014). Specifically, we assume that we have access to a classification oracle for our policy class that can find the minimumcost policy in \u03a0 when given the cost of each action on each of a sequence of contexts. In fact, an ordinary cost-sensitive, multiclass classification learning algorithm can be used for this purpose, which suggests that, practically, this may be a reasonable and natural assumption. We then consider techniques for constructing a von Neumann winner from empirical exploration data. (Although we focus on a batch-like setting, the resulting algorithms can be used online as well.) We analyze the statistical efficiency of this approach in Section 6. In Sections 7 and 8, we give two polynomial-time algorithms for computing an approximate von Neumann winner from data: one based on Kalai and Vempala\u2019s Follow-the-Perturbed-Leader algorithm (2003), and the other based on projected gradient ascent as studied by Zinkevich (2003). These techniques yield learning algorithms that approximate or perform as well as the von Neumann winner, using data, time, and space that only depend logarithmically on the cardinality of the space \u03a0, and therefore, are applicable even with huge policy spaces.", "startOffset": 8, "endOffset": 974}, {"referenceID": 22, "context": "Dueling bandits and the von Neumann winner In the dueling bandits problem (Yue et al., 2009), the learner has access to K possible actions, 1, .", "startOffset": 74, "endOffset": 92}, {"referenceID": 19, "context": "Existing work typically assumes the existence of a Condorcet winner (Urvoy et al., 2013; Zoghi et al., 2014b), that is, an action a\u2217 that beats every other action a 6= a\u2217.", "startOffset": 68, "endOffset": 109}, {"referenceID": 18, "context": "Before continuing, we briefly mention some of the other solution concepts that have been proposed to remedy the potential non-existence of a Condorcet winner (Schulze, 2011).", "startOffset": 158, "endOffset": 173}, {"referenceID": 19, "context": "Two of these are the Borda winner, the action that has the highest probability of winning a duel against a uniformly random action; and the Copeland winner, the action that wins the most pairwise comparisons (Urvoy et al., 2013).", "startOffset": 208, "endOffset": 228}, {"referenceID": 18, "context": "Both of these fail the independence of clones criterion (Schulze, 2011), meaning that adding multiple identical copies of an action can change the Borda or Copeland winner.", "startOffset": 56, "endOffset": 71}, {"referenceID": 22, "context": "In the standard dueling-bandits setting with a static preference matrix, a seemingly different definition of regret was used by Yue et al. (2009) in terms of an assumed Condorcet winner.", "startOffset": 128, "endOffset": 146}, {"referenceID": 8, "context": "Under this interpretation, it becomes especially natural to use ordinary no-regret learning algorithms as players of this game since it is known that such algorithms, when properly configured for this purpose, will converge to maxmin or minmax strategies (Freund and Schapire, 1999).", "startOffset": 255, "endOffset": 282}, {"referenceID": 1, "context": "Such a \u201csparring\u201d approach was previously proposed for dueling bandits by Ailon et al. (2014), though without details, and not in the contextual setting.", "startOffset": 74, "endOffset": 94}, {"referenceID": 3, "context": "P (Beygelzimer et al., 2011) for this purpose in the full-explore-exploit setting.", "startOffset": 2, "endOffset": 28}, {"referenceID": 3, "context": "P (Beygelzimer et al., 2011) for this purpose in the full-explore-exploit setting. Exp4.P is well-suited since it is designed to work with partial information as in our bandit setting, and since it can handle the kind of adversarially generated data that arises unavoidably when playing a game. It also is designed to work with policies in a contextual setting like ours (or, more generally, to accept the advice of \u201cexperts\u201d). The learning setting for Exp4.P is as follows (somewhat, but straightforwardly, modified for our present purposes). There are K possible actions, 1, . . . ,K, and a finite space \u03a0 of policies. On each round t = 1, . . . , T , an adversary chooses and reveals a context xt, and also chooses, but does not reveal rewards rt(1), . . . , rt(K) \u2208 [\u22121,+1] for each of the K actions. The learner then selects an action at, and receives the revealed reward rt(at). The learner\u2019s total reward is thus GA = \u2211T t=1 rt(at), while the reward of each policy \u03c0 is G\u03c0 = \u2211T t=1 rt(\u03c0(xt)). The learner\u2019s goal is to receive reward close to that of the best policy. Beygelzimer et al. (2011) prove that (subject to very benign conditions) with probability at least 1\u2212 \u03b4, Exp4.", "startOffset": 3, "endOffset": 1100}, {"referenceID": 14, "context": "In the sections that follow, we provide two algorithms: Algorithm SparringFPL that builds on the Follow-the-PerturbedLeader algorithm of Kalai and Vempala (2003), and Algorithm ProjectedGD that builds on online projected gradient descent methods of Zinkevich (2003).", "startOffset": 137, "endOffset": 162}, {"referenceID": 14, "context": "In the sections that follow, we provide two algorithms: Algorithm SparringFPL that builds on the Follow-the-PerturbedLeader algorithm of Kalai and Vempala (2003), and Algorithm ProjectedGD that builds on online projected gradient descent methods of Zinkevich (2003). For a given approximation quality, the performance of either algorithm is characterized by several quantities: the sufficient number of exploration rounds, the running time, the storage requirement, and the number of policies in the support of \u0174.", "startOffset": 137, "endOffset": 266}, {"referenceID": 14, "context": "(7), SparringFPL, is based on the Follow-the-Perturbed-Leader (FPL) algorithm of Kalai and Vempala (2003). FPL is designed for a standard online learning problem: Let D and L be subsets of RmK .", "startOffset": 81, "endOffset": 106}, {"referenceID": 23, "context": "(7) using online projected gradient descent methods as studied by Zinkevich (2003). The algorithm maintains a vector wt \u2208 C corresponding to a strategy for the row player.", "startOffset": 66, "endOffset": 83}], "year": 2015, "abstractText": "We consider the problem of learning to choose actions using contextual information when provided with limited feedback in the form of relative pairwise comparisons. We study this problem in the dueling-bandits framework of Yue et al. (2009), which we extend to incorporate context. Roughly, the learner\u2019s goal is to find the best policy, or way of behaving, in some space of policies, although \u201cbest\u201d is not always so clearly defined. Here, we propose a new and natural solution concept, rooted in game theory, called a von Neumann winner, a randomized policy that beats or ties every other policy. We show that this notion overcomes important limitations of existing solutions, particularly the Condorcet winner which has typically been used in the past, but which requires strong and often unrealistic assumptions. We then present three efficient algorithms for online learning in our setting, and for approximating a von Neumann winner from batch-like data. The first of these algorithms achieves particularly low regret, even when data is adversarial, although its time and space requirements are linear in the size of the policy space. The other two algorithms require time and space only logarithmic in the size of the policy space when provided access to an oracle for solving classification problems on the space.", "creator": "LaTeX with hyperref package"}}}