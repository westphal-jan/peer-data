{"id": "1502.06922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2015", "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval", "abstract": "This paper develops a model that addresses sentence embedding using recurrent neural networks (RNN) with Long Short Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model automatically attenuates the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These keyword detection and topic allocation tasks enabled by the LSTM-RNN allow the network to perform web document retrieval, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform all existing state of the art methods.", "histories": [["v1", "Tue, 24 Feb 2015 19:39:27 GMT  (1307kb,D)", "http://arxiv.org/abs/1502.06922v1", null], ["v2", "Sun, 5 Jul 2015 06:11:19 GMT  (1490kb,D)", "http://arxiv.org/abs/1502.06922v2", null], ["v3", "Sat, 16 Jan 2016 06:35:23 GMT  (1759kb,D)", "http://arxiv.org/abs/1502.06922v3", "To appear in IEEE/ACM Transactions on Audio, Speech, and Language Processing"]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG cs.NE", "authors": ["hamid palangi", "li deng", "yelong shen", "jianfeng gao", "xiaodong he", "jianshu chen", "xinying song", "rabab ward"], "accepted": false, "id": "1502.06922"}, "pdf": {"name": "1502.06922.pdf", "metadata": {"source": "META", "title": "Deep Sentence Embedding Using the Long Short Term Memory Network: Analysis and Application to Information Retrieval", "authors": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "emails": ["HAMIDP@ECE.UBC.CA", "RABABW@ECE.UBC.CA", "DENG@MICROSOFT.COM", "JFGAO@MICROSOFT.COM", "XIAOHE@MICROSOFT.COM", "YESHEN@MICROSOFT.COM", "JIANSHUC@MICROSOFT.COM", "XINSON@MICROSOFT.COM"], "sections": [{"heading": null, "text": "Reports from the 31st International Conference on Machine Learning, Lille, France, 2015. JMLR: W & CP Volume 37. Copyright 2015 by the author (s)."}, {"heading": "1. Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2. Related Work", "text": "\"It's that we're in a position to be able to do that,\" he says."}, {"heading": "3. Sentence Embedding Using RNNs with and without LSTM Cells", "text": "In this section we present the model of recurrent neural networks and its long-term short-term memory to learn the sentence that embeds vectors. We start with the basic RNN and then continue with LSTM-RNN."}, {"heading": "3.1. The basic version of RNN", "text": "The RNN is a type of deep neural networks that are \"deep\" in the temporal dimension (Huang that is a fixed W-gram) and it has been widely used in time sequencing (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014). The basic idea of using RNN for sentence embedding is to find a dense and low-dimensional semantic representation by sequential and recurring processing of each word in a sentence and transform it into a low-dimensional vector. In this model, the global contextual features of the entire text are found in the semantic representation of the last word in the text sequence - see Figure 1, where x (t) the t-word is the ekt-word, coded as an operator-1-h."}, {"heading": "3.2. The RNN with LSTM cells", "text": "Although RNN performs the transformation from a set to a vector in principle, it is generally difficult to learn the long-term dependence within the sequence (due to the problem of disappearing sequences), one of the effective solutions to this problem in RNN is to use memory cells instead of neurons originally proposed in (Hochreiter & Schmidhuber, 1997) as Long Short Term Memory (LSTM) and completed in (Gers et al., 1999) and (Gers et al., 2003) by adding Forgiss gate and peephole connections to the architecture.We use the architecture of LSTM shown in Fig. 2 for the proposed embedding method of the set. In this figure, i (t), f (t), o (t), c (t), c (t) Forgateway, exit gate, exit gate and cell state vector are each forgotten. Wp1, Wp2, function Wpy \u00b7, and function are directional Wpy \u2212 1, and Wpy \u2212 3 respectively."}, {"heading": "4. Learning Method", "text": "(D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s) (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s) (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s) (D (D) s) (D) s (D (D) s) (D) s (D) s) (D (D) s) (D) s (D (D) s) s) (D) s (D) s (D) s (D) s) (D) s (D) s (D) s (D) s (D (D) s) s (D) s (D) s (D) s (D) s (D) s) s (D) s (D) s (D) s (D) s (D) s) s (D) s (D) s (D) s (D) s (D) s (D) s) (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s (D) s"}, {"heading": "5. Analysis of the Sentence Embedding Process and Performance Evaluation", "text": "To understand how the LSTM-RNN \"first minibatch\" works, we use visualization tools to analyze the semantic algorithms 1, 2, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "5.1. Analysis", "text": "In this section we will examine how the information in the input record is sequentially extracted by the LSTMRNN model and embedded in the semantic vector over time."}, {"heading": "5.1.1. ATTENUATING UNIMPORTANT INFORMATION", "text": "First, we examine the development of the semantic vector and how unimportant words are attenuated. Specifically, we introduce the following input records from the test data set into the trained LSTM RNN model: \u2022 Query: \"Hotels in shanghai\" \u2022 Document: \"shanghai hotels accommodation hotel in shanghai discount and reservation\" Activations of the input data set, the source gate, the cell state and the embedding vector for each cell for query and document are shown in Figure 4 and Figure 5 respectively. The vertical axis is the cell index from 1 to 32, and the horizontal axis is the word index from 1 to 10 numbered from left to right in a sequence of words and color codes showing activation values. From Figures 4-5, we make the following observations: \u2022 Semantic representation y (t) and cell states c (t) develop over time. Valuable continformations are gradually absorbed into c () (and not) absorbed (over) the cell set."}, {"heading": "5.1.2. KEYWORDS EXTRACTION", "text": "In this section, we show how the trained LSTM-RNN extracts the important information, i.e., keywords, from the input records. To this end, we follow semantic representations, y (t), over time. We focus on the 10 most active cells in the final semantic representation. Whenever there is a sufficiently large change in the value of cell activation (y (t), we assume that an important keyword has been recognized by the model. We illustrate the result using the example above (\"Hotels in Shanghai\"). The development of the 10 most active cell activation, y (t), over time, Figures 6 and 7 each show an important keyword for the query and the judgments of the document. 2 From Figures 6-7, we also find that different words activate different cells. In Tables 1-2, we show that the cells in cells 151-2 are clearly visible, refer to Figure 13 of Annex C.3."}, {"heading": "5.1.3. TOPIC ALLOCATION", "text": "Now we further show that the trained LSTM RNN model not only recognizes the keywords, but also correctly assigns them to different cells according to the topics to which they belong. To do this, we run through the test data set using the trained LSTM RNN model and search for the keywords that 3Note that before submitting the first word, the activation values are initially zero, so that there is always a significant change in the cell states after submitting the first word. Therefore, we did not specify the number of cells that recognize the first word as a keyword. In addition, another example of keyword extraction is in Appendix C.1 2 3246810 00.020.040.060.08Figure 6. Activation values y (t) of 10 most active cells for the query: \"Hotels in Shanghai\" that are recognized by a particular cell."}, {"heading": "5.2. Performance Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1. WEB DOCUMENT RETRIEVAL TASK", "text": "In this section we apply the proposed set of embedding method to an important web document retrieval task for a commercial web search engine. In particular, the RNN models (with and without LSTM cells) embed the sentences from the query and the document pages into their corresponding semantic vectors and then calculate the cosmic similarity between these vectors to measure the semantic similarity between the query and the candidate documents. Experimental results for this task are shown in Table 3 using the standard recorded Cu-mulative gain (NDCG) (Yes, rvelin & Keka, la \ufffd inen)."}, {"heading": "6. Conclusions and Future Work", "text": "We show that the semantic vector evolves over time, extracting only useful information from each new input, made possible by input gates that detect and mitigate useless information. Due to the general limitation of available human-flagged data, we propose to train the model with a weak supervision signal, by embedding the click data of a commercial web search engine technology. By performing a detailed analysis of the model, we showed that: 1) the proposed model is noise-resistant, i.e. it mainly embeds keywords in the final semantic vector representing the whole sentence, and 2) In the proposed model, each cell is usually associated with keywords from a specific topic. These results were supported by extensive examples. As an example application of the proposed sentence embedding method, we extended it to the important task of the web document Gaeval."}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Expressions for the Gradients", "text": "In this appendix, we present the final gradient expressions necessary to train the proposed models; the full derivatives of these gradients are presented in appendix B."}, {"heading": "A.1. RNN", "text": "For the recurring parameters, we have omitted the following r-subscripts for simplicity: \"MP,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"\" GT, \"\" GT, \"\" GT, \"\" GT, \"\" GT, \"\" GT, \"\" GT, \"\" GT, \"GT,\" \"GT,\" \"T,\" \"T,\" \"T,\" \"T,\" \"T,\" \"T,\" \"T,\" \"T,\" \"T,\" \"T,\" T, \"\" T, \"\" T, \"\" T, \"\" T, \"\" T, \"\" T, \"\" T, \"\" T, \"\" \"T,\" \"T,\" \"\" T, \"\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"GT,\" \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" \"T,\" T, \"\" T, \"T,\" \"\" T, \"T,\" \"T,\" \"\" T, \"T,\" \"\" \"GT,\" \"\" \"GT,\" \"\" \"GT,\" \"\" \"GT,\" \"\" \"\" GT, \"\" \"GT,\" \"\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" T, \"T,\" GT, \""}, {"heading": "A.2. LSTM-RNN", "text": "Based on the cost function in (4), we use the Nesterov method described in (6) to update the parameters of the LSTM-RNN model, which is one of the weight matrices or bias vectors {W1, W2, W3, W4, Wrec1, Wrec2, Wrec3, Wrec4, Wp1, Wp2, Wp3, b2, b3, b4} in the LSTM-RNN architecture. The general format of the cost differential is the same as (7). By definition, we have: \"R,\" \"J,\" \"R\" (Qr, D + r), \"R\" (Qr, Dr \"j),\" R \"(14),\" R \"and\" J \"Subscriptions\" (Nr, D), which for the sake of simplicity and the present \"R\" (Q, D) and \"T.\" In the following sections \"(Q,\" Q, \"D)."}, {"heading": "A.2.1. OUTPUT GATE", "text": "For recursive connections we have: \u2202 R (D) \u2202 Wrec1 = \u03b4rec1yQ (Q) Q (Q) Q (Q) Q (Q).yQ (t \u2212 1) T + \u03b4rec1yD (t).yD (t).yD (t \u2212 1) T (16) where\u03b4rec1yQ (t) Q (t).yQ (t) (t) (1 \u2212 oQ (t).yD (t).yD (t).yD (t).lQ (t) Q (t) = oQ (t).yQ (t).yQ (t (t (t). (Wp1, and peephole connections, Wp1, we have: \u0438 R (Q, D).W1 \u2212 ag."}, {"heading": "A.2.3. FORGET GATE", "text": "For the recurrent connections we will: mps R = Q = Q Q = Q Q Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (T) (T)) Q (t \u2212 h (t) Q (t) Q (t) Q (t \u2212 h (t)) Q (t \u2212 h (t)))) Q (t \u2212 h) Wrec2 + bf, Q (t)."}, {"heading": "A.2.5. ERROR SIGNAL BACKPROPAGATION", "text": "Error signals are propagated back over time using the following equations: \u03b4rec1Q (t \u2212 1) = [oQ (t \u2212 1) \u0445 (1 \u2212 oQ (t \u2212 1)) \u0445 h (cQ (t \u2212 1)) \u0445 WTrec1.\u03b4rec1Q (t) (43) \u03b4reciQ (t \u2212 1) = [(1 \u2212 h (cQ (t \u2212 1))) \u0445 (1 + h (cQ (t \u2212 1)) \u0441oQ (t \u2212 1)] \u0445 WTreci.\u03b4 reci Q (t), for i {2, 3, 4} (44)"}, {"heading": "B. Derivation of BPTT for RNN and LSTM-RNN", "text": "In this appendix we present the complete derivation of the gradients for RNN and LSTM-RNN."}, {"heading": "B.1. Derivation of BPTT for RNN", "text": "From (4) and (5) we have: MP = Q = Q = Q = Q = Q = Q Q (Q = Q = Q = Q = Q Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q Q = Q Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "B.2. Derivation of BPTT for LSTM-RNN", "text": "The following values arise from (50) for each parameter in the LSTMNN architecture: \u2202 R (Q, D) \u2202 = \u2202 a (t = TQ)"}, {"heading": "B.2.1. OUTPUT GATE", "text": "Da \u03b1 \u0445 \u03b2 = diag (\u03b1) \u03b2 = diag (\u03b2 = diag (\u03b2 = Q =) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q) Q (Q = 1) Q (Q = 1) Q (Q) Q (Q = 1) Q (Q = 1) Q (Q) Q (Q = 1) Q (Q = 1) Q (Q = 1) Q (Q) Q (Q (Q = 1) Q (Q) Q (Q) Q (Q = 1) Q (Q) Q (Q (Q) Q (Q = 1) Q (Q) Q (Q) Q (Q (Q) Q (Q) Q (Q) Q (Q (Q) Q (Q = 1) Q (Q) Q (1) Q (Q (Q) Q (Q) Q (Q (Q) Q (Q) Q) Q (Q (Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (1) Q (1) Q (1) Q (Q"}, {"heading": "B.2.3. FORGET GATE", "text": "If we forget that Tor, with a similar lead to the input gate, will not have, then we have either Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q (Q) Q) Q (Q) Q (Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q (Q) Q (Q) Q (Q) Q ("}, {"heading": "C. LSTM-RNN Visualization", "text": "In this appendix we present further examples of LSTM-RNN visualization."}, {"heading": "C.1. LSTM-RNN Semantic Vectors: Another Example", "text": "Consider the following example from the test data set: \u2022 Query: \"How to repair bathtubs, do not turn off\" \u2022 Document: \"How to paint a bathtub and what color should you use?\" Three interesting observations from Fig.9 and Fig.10: \u2022 Semantic representation of entrance gate, exit gate, cell state and cell output for each cell for query and document evolve over time. \u2022 In part (a) of Fig.10, we find that input values for most cells corresponding to word 3, word 4, word 7 and word 9 have very low values (light blue color) on the document page of LSTM-RNN. These correspond to the words \"you,\" \"\" paint \"and\" paint. \"Interestingly, the input values of cell gates are such that they are well represented in the context of STM-RNN."}, {"heading": "C.2. Key Word Extraction: Another Example", "text": "This year, it is so far that it has never been so far as this year, \"he said.\" It is very important that we are able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "C.5. Topic Allocation", "text": "rE \"n\" h ealth \"D\" i \"s\" i, s \"i\" s \"i\" s \"i\" s \"i\" s. \"S\" D \"i nI\" e \"n\" i \"h\" h \"i.\" S \"D\" i \"s\" i \"i.\" S \"D\" i \"h\" h \"i\" s. \"S\" D \"i\" i \"s\" i \"i\" i. \"S\" i \"i\" i \"i.\" S \"i\" i \"i\" i. \"S\" i \"i\" i. \"i\" i. \"S\" i \"i\" i. \"i\" i. \"i\" i. \"i\" i. \"i\" i \"i.\" S \"i\" i \"i.\" i \"i\" i. \"i\" i \"i.\" S \"i\" i \"i\" i. \"i\" i. \"i\" i. \"i\" i. \"i\" i. \"i.\" i \"i.\" i \"i.\" i \"i.\" i. \"i\" i \"i.\" i \"i.\" i \"i.\" i \"i.\" i \"i\" i. \"i\" i. \"i\" i \"i.\" i \"i.\" i \"i\" i. \"i\" i. \"i\" i. \"i\" i. \"i\" i \"i\" i \"i.\" i \"i.\" i \"i.\" i \"i.\" i \"i.\" i. \""}], "references": [{"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In Proc. ICASSP,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A primal-dual method for training recurrent neural networks constrained by the echo-state property", "author": ["Chen", "Jianshu", "Deng", "Li"], "venue": "In Proceedings of the International Conf. on Learning Representations (ICLR),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Large vocabulary continuous speech recognition with contextdependent DBN-HMMs", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "In Proc. IEEE ICASSP,", "citeRegEx": "Dahl et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2011}, {"title": "Contextdependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G.E. Dahl", "Yu", "Dong", "Deng", "Li", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Sequence classification using highlevel features extracted from deep neural networks", "author": ["L. Deng", "J. Chen"], "venue": "In Proc. ICASSP,", "citeRegEx": "Deng and Chen,? \\Q2014\\E", "shortCiteRegEx": "Deng and Chen", "year": 2014}, {"title": "Analysis of the correlation structure for a neural predictive model with application to speech recognition", "author": ["L. Deng", "K. Hassanein", "M. Elmasry"], "venue": "Neural Networks,", "citeRegEx": "Deng et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Deng et al\\.", "year": 1994}, {"title": "Scalable stacking and learning for building deep architectures", "author": ["L. Deng", "D. Yu", "J. Platt"], "venue": "In Proc. ICASSP,", "citeRegEx": "Deng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Elman", "Jeffrey L"], "venue": "Cognitive Science,", "citeRegEx": "Elman and L.,? \\Q1990\\E", "shortCiteRegEx": "Elman and L.", "year": 1990}, {"title": "Clickthrough-based latent semantic models for web search", "author": ["Gao", "Jianfeng", "Toutanova", "Kristina", "Yih", "Wen-tau"], "venue": "SIGIR \u201911,", "citeRegEx": "Gao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao", "Jianfeng", "Pantel", "Patrick", "Gamon", "Michael", "He", "Xiaodong", "Deng", "Li", "Shen", "Yelong"], "venue": "In Proc. EMNLP,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix A", "Schmidhuber", "Jrgen", "Cummins", "Fred"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gers et al\\.", "year": 1999}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Sequence transduction with recurrent neural networks", "author": ["A. Graves"], "venue": "In Representation Learning Workshp,", "citeRegEx": "Graves,? \\Q2012\\E", "shortCiteRegEx": "Graves", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In Proc. ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Probabilistic latent semantic analysis", "author": ["Hofmann", "Thomas"], "venue": "Proc. of Uncertainty in Artificial Intelligence,", "citeRegEx": "Hofmann and Thomas.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "Ir evaluation methods for retrieving highly relevant documents", "author": ["J\u00e4rvelin", "Kalervo", "Kek\u00e4l\u00e4inen", "Jaana"], "venue": "In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2000}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Deng", "Li"], "venue": "In Proc. ACL,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Nesterov", "Yurii"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Nesterov and Yurii.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "In ICML 2013,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "An application of recurrent nets to phone probability estimation", "author": ["A.J. Robinson"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Robinson,? \\Q1994\\E", "shortCiteRegEx": "Robinson", "year": 1994}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Sak", "Hasim", "Senior", "Andrew", "Beaufays", "Fran\u00e7oise"], "venue": "In Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH),", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "A latent semantic model with convolutional-pooling structure for information", "author": ["Shen", "Yelong", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Mesnil", "Gregoire"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George E", "Hinton", "Geoffrey E"], "venue": "In ICML (3)\u201913,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc VV"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Deep learning and its applications to signal and information processing [exploratory dsp", "author": ["Yu", "Dong", "Deng", "Li"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 29, "context": "By mapping texts into a unified semantic representation, the embedding vector can be further used for different applications, such as machine translation (Sutskever et al., 2014), sentiment analysis (Le & Mikolov, 2014), and information retrieval (Huang et al.", "startOffset": 154, "endOffset": 178}, {"referenceID": 27, "context": ", 2013) and CLSM (Shen et al., 2014) models, developed for information retrieval, can also be interpreted as sentence embedding methods.", "startOffset": 17, "endOffset": 36}, {"referenceID": 13, "context": "It has been successfully applied to speech recognition, which achieves state-of-art performance (Graves et al., 2013; Sak et al., 2014).", "startOffset": 96, "endOffset": 135}, {"referenceID": 26, "context": "It has been successfully applied to speech recognition, which achieves state-of-art performance (Graves et al., 2013; Sak et al., 2014).", "startOffset": 96, "endOffset": 135}, {"referenceID": 29, "context": "In the machine translation work (Sutskever et al., 2014), an input English sentence is converted into a vector representation using LSTM-RNN, and then another LSTM-RNN is used to generate an output French sentence.", "startOffset": 32, "endOffset": 56}, {"referenceID": 25, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 5, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 20, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 12, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 0, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 19, "context": "The basic version of RNN The RNN is a type of deep neural networks that are \u201cdeep\u201d in temporal dimension and it has been used extensively in time sequence modelling (Elman, 1990; Robinson, 1994; Deng et al., 1994; Mikolov et al., 2010; Graves, 2012; Bengio et al., 2013; Chen & Deng, 2014; Mesnil et al., 2013; Deng & Chen, 2014).", "startOffset": 165, "endOffset": 329}, {"referenceID": 27, "context": "This is also different from (Shen et al., 2014) where the sliding window of a fixed size (akin to an FIR filter) is used to capture local features and a max-pooling layer on the top to capture global features.", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": "One of the effective solutions for this problem in RNNs is using memory cells instead of neurons originally proposed in (Hochreiter & Schmidhuber, 1997) as Long Short Term Memory (LSTM) and completed in (Gers et al., 1999) and (Gers et al.", "startOffset": 203, "endOffset": 222}, {"referenceID": 11, "context": ", 1999) and (Gers et al., 2003) by adding forget gate and peephole connections to the architecture.", "startOffset": 12, "endOffset": 31}, {"referenceID": 28, "context": "1 of (Sutskever et al., 2013) where Nesterov method is derived as a momentum method.", "startOffset": 5, "endOffset": 29}, {"referenceID": 24, "context": "To resolve the gradient explosion problem we use gradient renormalization method described in (Pascanu et al., 2013; Mikolov et al., 2010).", "startOffset": 94, "endOffset": 138}, {"referenceID": 20, "context": "To resolve the gradient explosion problem we use gradient renormalization method described in (Pascanu et al., 2013; Mikolov et al., 2010).", "startOffset": 94, "endOffset": 138}, {"referenceID": 27, "context": ", 2013) and CLSM (Shen et al., 2014), on the same training dataset and evaluated their performance on the same task.", "startOffset": 17, "endOffset": 36}, {"referenceID": 8, "context": "PLSA (Probabilistic Latent Semantic Analysis) is a topic model proposed in (Hofmann, 1999), which is trained using the Maximum A Posterior estimation (Gao et al., 2011) on the documents side from the same training dataset.", "startOffset": 150, "endOffset": 168}, {"referenceID": 2, "context": "This work has been motivated by the earlier successes of deep learning methods in speech (Dahl et al., 2011; Yu & Deng, 2011; Dahl et al., 2012; Hinton et al., 2012; Deng et al., 2012) and in semantic modeling (Huang et al.", "startOffset": 89, "endOffset": 184}, {"referenceID": 3, "context": "This work has been motivated by the earlier successes of deep learning methods in speech (Dahl et al., 2011; Yu & Deng, 2011; Dahl et al., 2012; Hinton et al., 2012; Deng et al., 2012) and in semantic modeling (Huang et al.", "startOffset": 89, "endOffset": 184}, {"referenceID": 6, "context": "This work has been motivated by the earlier successes of deep learning methods in speech (Dahl et al., 2011; Yu & Deng, 2011; Dahl et al., 2012; Hinton et al., 2012; Deng et al., 2012) and in semantic modeling (Huang et al.", "startOffset": 89, "endOffset": 184}, {"referenceID": 27, "context": ", 2012) and in semantic modeling (Huang et al., 2013; Shen et al., 2014; Gao et al., 2014; Jianfeng Gao & Deng, 2014), and it adds further evidence for the effectiveness of these methods.", "startOffset": 33, "endOffset": 117}, {"referenceID": 9, "context": ", 2012) and in semantic modeling (Huang et al., 2013; Shen et al., 2014; Gao et al., 2014; Jianfeng Gao & Deng, 2014), and it adds further evidence for the effectiveness of these methods.", "startOffset": 33, "endOffset": 117}], "year": 2015, "abstractText": "This paper develops a model that addresses sentence embedding using recurrent neural networks (RNN) with Long Short Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model automatically attenuates the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These keyword detection and topic allocation tasks enabled by the LSTM-RNN allow the network to perform web document retrieval, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform all existing state of the art methods. Proceedings of the 31 st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}