{"id": "1502.07058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval", "abstract": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.", "histories": [["v1", "Wed, 25 Feb 2015 05:58:43 GMT  (5256kb,D)", "http://arxiv.org/abs/1502.07058v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.IR cs.LG cs.NE", "authors": ["adam w harley", "alex ufkes", "konstantinos g derpanis"], "accepted": false, "id": "1502.07058"}, "pdf": {"name": "1502.07058.pdf", "metadata": {"source": "CRF", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval", "authors": ["Adam W. Harley", "Alex Ufkes", "Konstantinos G. Derpanis"], "emails": ["kosta}@scs.ryerson.ca"], "sections": [{"heading": null, "text": "Following this observation, documents are often stored as images before they are processed by an optical character recognition (OCR) system, which means that basic image analysis is the only tool available for initial indexing and classification. [26] As a precursor to image analysis, documents can be facilitated and improved by providing information about the visual layout of each document. [10] In addition, document information that is lost in OCR, such as writings, graphics, and layouts, can only be stored and indexed."}, {"heading": "A. Related Work", "text": "In the last twenty years of documentary film analysis, research has shifted from region-based analysis to holistic image analysis, both in terms of craft characteristics and machine skills. The power of region-based analysis of documents that are able to identify themselves and the way in which they are applied. In general, this approach assumes that many documentaries have a clear and consistent configuration of visually identifiable components. For example, formal business letters are usually associated with a particular spatial configuration of letters."}, {"heading": "B. Contributions", "text": "In light of previous work, this paper provides the following contributions: First, the paper thoroughly evaluates the performance of deep CNN features for displaying document images. To this end, the paper presents experiments in CNN design, training, feature processing, and compression. Second, the results show that features extracted from CNN are superior to all handcrafted competitors, and can also be compressed into very short codes without any slight performance loss. Second, this paper shows that CNNs trained in non-document imaging translate well to document-related tasks. Third, this paper examines a strategy for embedding human knowledge of the document structure in CNN architectures by guiding a group of CNNs to learn region-specific features. Interestingly, the results show little to no improvement in classification and retrievability after this augmentation, suggesting that a basic integral set of CNN features may automatically provide CNN with better features (or, eventually, a new set of CDs)."}, {"heading": "II. TECHNICAL APPROACH", "text": "In structured documents, the layout of text and graphic elements often reflects important information about the genre. Therefore, documents in a category often have region-specific characteristics. This paper attempts to learn these informative characteristics by training either a single holistic CNN or an interaction of regional CNNs. In addition, the paper examines two different initialization strategies: the first randomly initializes the weights of the CNNs and relies entirely on the training process to find the characteristics; the second transfers weights from a network trained on another task and relies on training just to match the features to the area of document analysis."}, {"heading": "A. Holistic convolutional neural networks", "text": "In most modern implementations of neural networks for computer vision, the network takes a square matrix of pixels as input, processes this input through a stack of revolutionary layers, and then classifies the output of these revolutionary layers using two or three completely connected layers [24, 19]. A typical network of this type has about 60 million traceable parameters; this enormous representational capacity, along with the hierarchical organization of this representation, is thought to be responsible for the network's power as a feature builder and classifier [24]. Constitutional activations of neural networks are not geometrically invariant. In applications such as object recognition, this is sometimes an unfavorable property. Much work has been done to add spatial invariance to CNNs, e.g. by \"trembling\" the training data to add geometric variants to each image in the dataset, or by modifying the architecture of the CNN document to multiple input."}, {"heading": "B. Region-based guidance", "text": "Taking into account the possibility that a holistic CNN cannot take advantage of region-specific information in document images, CNNs to learn region-specific features can contribute to fine-grained discrimination by isolating subtle region-specific differences between document categories. Consider the example of discriminatory letters and memos as shown in Figure 2. These two categories differ only consistently in the \"Address\" section; memos have a short \"To\" and \"From,\" and letters have full addresses. It is possible that a holistic CNN learns this automatically, but a CNN to classify documents that use only this region increases the likelihood that this feature will be learned. The idea of this approach is to dedicate one CNN to each region and thus force multiple CNNs to learn region-specific representations from which features can be extracted and combined. Any number of region-specific CNNs can be used in this approach."}, {"heading": "C. Transfer learning", "text": "The typical initialization strategy for CNNs is to place all weights on small random numbers and set all distortions to either 1 or 0 [24]. An alternative strategy is to prepare the network for a complementary task that potentially has more training data than the target task. This brings the network close to a good solution to the target problem and prevents it from descending into local minimums early in the training process [29]. A popular choice for pre-training is the ImageNet challenge ILSVRC 2012, as it contains over a million training examples of natural images categorized into 1,000 object categories [30]. Features extracted from ImageNet-trained networks have proven to be effective all-purpose features in a variety of other vision challenges, even without fine-tuning the target problem."}, {"heading": "III. EMPIRICAL EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "The performance of the various proposed approaches was evaluated on two versions of the IIT CDIP Test Collection [25], each containing high-resolution images of scanned documents compiled from public records of lawsuits against American tobacco companies. \u2212 The database contains more than seven million documents that are tagged. Frequently, the first day of a document image is indicative of the document category, but many documents in the dataset have missing or erroneous tags.The first version of the dataset listed in the results as SmallTobacco is a sample of 3,482 images from the collection that were selected and labeled in another paper. [20] This version of the dataset was used in a number of related papers. [20, 22, 17] Each image has one of ten labels. There is an unparalleled number of images per category, with the largest proportion of images in the \"letter\" category. The distribution of categories is representative of the distribution of the available data."}, {"heading": "C. Classification results", "text": "Table I shows the classification accuracy of the various BoW approaches across the various CNNs-based ratings, both on the SmallTobacco dataset and on the BigTobacco dataset. On SmallTobacco, the overall set of regional CNNs performed better than any other approach and achieved a classification accuracy of 79.9%. The previously best reported result on this dataset was 65.4% with a randomly initialized \"small\" CNN, which was roughly replicated here. The holistic network performed only marginally worse than the overall set of CNNs, suggesting that the holistic CNN could learn some level of information that region-based analysis should add. Interestingly, the \"small\" CNN compares CNN similarly to the large-scale holistic CNN when both are initialized with random weights. This seems to suggest that the additional parameters in the large network are not necessarily beneficial."}, {"heading": "D. Retrieval results", "text": "Mean average precision for the first 10 retrievals both datasets are marmarmarmarmarmarmarmarmarmed in the SmallTobacco dataset. Formally, the discrete version of this measurement is AP = \u2211 nk = 1 (P) \u00d7 rel (k)) number of relevant documents, (1) where the value of the retrieved document is simply limited to 1 if the document is relevant and 0 otherwise. This measurement is higher in relation to the ranking of the relevant documents when relevant documents are retrieved before irrelevant documents. Mean average precision is simply the average precision summed up over all queries, divided by the number of queries. The retrieved documents were determined to be \"relevant\" if they had the same class identifier as the retrieval image. Mean average precision for the first 10 retrievals both datasets are marmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmed in the SmallTobacco dataset."}, {"heading": "IV. CONCLUSION", "text": "This work established a new state of the art in classifying and restoring document images using features learned through deep Convolutionary Neural Networks (CNNs). Generic features extracted from a CNN trained on ImageNet outperformed the performance of modern alternatives, and fine-tuning these features to document images even boosted the results. Interestingly, experiments also showed that enforcing region-specific feature learning was unnecessary; a single CNN trained on whole images performed about as well as a group of CNNs trained on specific subsections of document images. Overall, this work showed that CNN's approach to displaying document images exceeds the power of hand-crafted alternatives."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Palomino Systems for the helpful discussions. The authors are grateful for the support of NVIDIA Corporation by donating a Tesla K40 GPU used for this research."}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "ICML, pages 17\u201324.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "From generic to specific deep representations for visual recognition", "author": ["H. Azizpour", "A.S. Razavian", "J. Sullivan", "A. Maki", "S. Carlsson"], "venue": "arXiv, 1406.5774,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural codes for image retrieval", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin", "V.S. Lempitsky"], "venue": "ECCV, 8689:584\u2013599,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "SURF: Speeded Up Robust Features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": "A. Leonardis, H. Bischof, and A. Pinz, editors, Proceedings of ECCV, volume 3951, pages 404\u2013417,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep poselets for human detection", "author": ["L.D. Bourdev", "F. Yang", "R. Fergus"], "venue": "arXiv, 1407.0717,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Bird species categorization using pose normalized deep convolutional nets", "author": ["S. Branson", "G.V. Horn", "S. Belongie", "P. Perona"], "venue": "BMVC,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Form classification using DP matching", "author": ["Y. Byun", "Y. Lee"], "venue": "Proc. of the 2000 ACM Symp. on App. Comp., volume 1, pages 1\u20134,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey of document image classification: Problem statement, classifier architecture and performance evaluation", "author": ["N. Chen", "D. Blostein"], "venue": "IJDAR, 10(1):1\u201316,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "A clustering-based algorithm for automatic document separation", "author": ["K. Collins-Thompson", "R. Nickolov"], "venue": "SIGIR, pages 1\u20138,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Clustering and classification of document structure-a machine learning approach", "author": ["A. Dengel", "F. Dubiel"], "venue": "ICDAR, volume 2, pages 587\u2013591,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Evaluation of GIST descriptors for web-scale image search", "author": ["M. Douze", "H. J\u00e9gou", "H. Sandhawalia", "L. Amsaleg", "C. Schmid"], "venue": "Proceedings of the ACM Int. Conf. on Image and Video Retrieval, CIVR \u201909, pages 19:1\u201319:8, New York, NY, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR, volume 2, pages 524\u2013531,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "ECCV, 8695:392\u2013 407,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison and classification of documents based on layout similarity", "author": ["J. Hu", "R. Kashi", "G. Wilfong"], "venue": "Information Retrieval, 2(2- 3):227\u2013243,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "arXiv, http://caffe.berkeleyvision.org/,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for document image classification", "author": ["L. Kang", "J. Kumar", "P. Ye", "Y. Li", "D. Doerman"], "venue": "ICPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "User-defined template for identifying document type and extracting information from documents", "author": ["T. Kochi", "T. Saitoh"], "venue": "ICDAR, pages 127\u2013 130,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1106\u20131114,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised classification of structurally similar document images", "author": ["J. Kumar", "D. Doermann"], "venue": "ICDAR, pages 1225\u20131229,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning document structure for retrieval and classication", "author": ["J. Kumar", "P. Ye", "D. Doermann"], "venue": "ICPR, pages 1558\u20131561,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Structural similarity for document image classification and retrieval", "author": ["J. Kumar", "P. Ye", "D. Doermann"], "venue": "PRL, 43:119,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR, volume 2, pages 2169\u20132178,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "PIEEE, 86(11):2278\u20132324,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Building a test collection for complex document information processing", "author": ["D. Lewis", "G. Agam", "S. Argamon", "O. Frieder", "D. Grossman", "J. Heard"], "venue": "SIGIR, pages 665\u2013666,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Digital libraries and document image retrieval techniques: A survey", "author": ["S. Marinai", "B. Miotti", "G. Soda"], "venue": "M. Biba and F. Xhafa, editors, Learning Structure and Schemas from Documents, volume 375, pages 181\u2013204. Springer Berlin Heidelberg,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Twenty years of document image analysis in PAMI", "author": ["G. Nagy"], "venue": "PAMI, 22(1):38\u201362,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, 42(3):145\u2013175,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet large scale visual recognition", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Classification of document pages using structure-based features", "author": ["C. Shin", "D. Doermann", "A. Rosenfeld"], "venue": "IJDAR, 3(4):232\u2013247,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and functional decomposition of business documents", "author": ["S. Taylor", "M. Lipshutz", "R.W. Nilson"], "venue": "ICDAR, volume 2, pages 563\u2013566,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1995}, {"title": "PANDA: Pose aligned networks for deep attribute modeling", "author": ["N. Zhang", "M. Paluri", "M. Ranzato", "T. Darrell", "L. Bourdev"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "In digital libraries, documents are often stored as images before they are processed by an optical character recognition (OCR) system, which means basic image analysis is the only available tool for initial indexing and classification [26].", "startOffset": 235, "endOffset": 239}, {"referenceID": 9, "context": "As a pre-processing stage, document image analysis can facilitate and improve OCR by providing information about each document\u2019s visual layout [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "This level of intra-class variability renders spatial layout analysis difficult, and rigid template matching impossible [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 23, "context": "In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29].", "startOffset": 146, "endOffset": 158}, {"referenceID": 18, "context": "In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29].", "startOffset": 146, "endOffset": 158}, {"referenceID": 28, "context": "In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29].", "startOffset": 146, "endOffset": 158}, {"referenceID": 6, "context": "has been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters [7, 18].", "startOffset": 112, "endOffset": 119}, {"referenceID": 17, "context": "has been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters [7, 18].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": ", forms) can be reduced to the problem of template matching [7], and less-rigid document types (e.", "startOffset": 60, "endOffset": 63}, {"referenceID": 14, "context": ", letters) can similarly be classified by fitting the geometric configuration of the document\u2019s components to one of several template configurations, via geometric transformations [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 31, "context": "An alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative \u201clandmark\u201d features that may appear anywhere in the document [32, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 30, "context": "An alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative \u201clandmark\u201d features that may appear anywhere in the document [32, 31].", "startOffset": 192, "endOffset": 200}, {"referenceID": 11, "context": "This strategy is sometimes called a \u201cbag of visual words\u201d approach, since it describes images with a histogram over an orderless vocabulary of features [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "For example, a landmark feature discriminating letters from most other document classes is the salutation: finding a salutation in a document (potentially through OCR) is a good cue that the document is a letter, regardless of that feature\u2019s exact spatial position [32].", "startOffset": 265, "endOffset": 269}, {"referenceID": 22, "context": "By concatenating image features pooled at several stages, beginning with a whole-image pool and proceeding into smaller and smaller regions, it is possible to build a descriptor that contains both global and local layout characteristics [23].", "startOffset": 237, "endOffset": 241}, {"referenceID": 21, "context": "This technique, known as spatial pyramid matching, was initially developed for categorizing scenes, but it has been shown to apply well to documents also, especially if the pooling regions are designed with document categorization in mind [22].", "startOffset": 239, "endOffset": 243}, {"referenceID": 9, "context": "At the same time, many researchers have replaced handcrafted features and representations with machine-learned variants [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 8, "context": "At the same time, many researchers have replaced handcrafted features and representations with machine-learned variants [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 9, "context": "\u201clandmarks\u201d) within each document type, toward the goal of structure-based classification [10, 21].", "startOffset": 90, "endOffset": 98}, {"referenceID": 20, "context": "\u201clandmarks\u201d) within each document type, toward the goal of structure-based classification [10, 21].", "startOffset": 90, "endOffset": 98}, {"referenceID": 16, "context": "Most recently, it was shown that the entire pipeline of supervised document image classification, from feature-building to decision making, can be learned by a convolutional neural network (CNN) [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin [19, 13].", "startOffset": 123, "endOffset": 131}, {"referenceID": 12, "context": "In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin [19, 13].", "startOffset": 123, "endOffset": 131}, {"referenceID": 28, "context": "The CNN approach has even been shown to apply well to domains for which it was traditionally believed ill-suited, such as attribute detection, and fine-grained object recognition [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 29, "context": "First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6].", "startOffset": 167, "endOffset": 178}, {"referenceID": 12, "context": "First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6].", "startOffset": 167, "endOffset": 178}, {"referenceID": 5, "context": "First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6].", "startOffset": 167, "endOffset": 178}, {"referenceID": 5, "context": "Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33].", "startOffset": 226, "endOffset": 236}, {"referenceID": 4, "context": "Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33].", "startOffset": 226, "endOffset": 236}, {"referenceID": 32, "context": "Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33].", "startOffset": 226, "endOffset": 236}, {"referenceID": 23, "context": "After a CNN is trained on classification, the layers of the network can be interpreted as forming a hierarchical chain of abstraction, where the lowest layers contain simple features, and the highest layers contain concise and descriptive representations [24].", "startOffset": 255, "endOffset": 259}, {"referenceID": 28, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 2, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 13, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 1, "context": "Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2].", "startOffset": 132, "endOffset": 146}, {"referenceID": 24, "context": "Finally, this work makes available a new labelled subset of the IIT-CDIP collection of tobacco litigation documents [25], containing 400,000 document images across 16 categories.", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "In most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully-connected layers [24, 19].", "startOffset": 295, "endOffset": 303}, {"referenceID": 18, "context": "In most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully-connected layers [24, 19].", "startOffset": 295, "endOffset": 303}, {"referenceID": 23, "context": "A typical network of this type has approximately 60 million trainable parameters; this vast representational capacity, along with the hierarchical organization of that representation, is assumed to be responsible for the network\u2019s power as a featurebuilder and classifier [24].", "startOffset": 272, "endOffset": 276}, {"referenceID": 23, "context": ", by \u201cjittering\u201d the training data to add geometric variants of each image in the dataset [24], or by altering the architecture of the CNN to process the input at multiple scales and positions [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": ", by \u201cjittering\u201d the training data to add geometric variants of each image in the dataset [24], or by altering the architecture of the CNN to process the input at multiple scales and positions [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 28, "context": "It has been found that the activation patterns near the top of a deep CNN make very descriptive feature vectors [29].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": ", to 128 dimensions) without significantly affecting their discriminative power [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "The goal of transfer learning is to take advantage of shared structure in related problems, to facilitate learning on problems with little training data [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 23, "context": "The typical initialization strategy for CNNs is to set all weights to small random numbers, and set all biases to either 1 or 0 [24].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "This puts the network near a good solution in the target problem, and prevents it from descending into local minima early in the training process [29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 29, "context": "A popular choice for pre-training is the ILSVRC 2012 ImageNet challenge, as it contains over a million training examples of natural images, categorized into 1000 object categories [30].", "startOffset": 180, "endOffset": 184}, {"referenceID": 28, "context": "ImageNet-trained network have been shown to be effective general-purpose features in a variety of other vision challenges, even without fine-tuning on the target problem [29].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "The performance of the various proposed approaches was evaluated on two versions of the IIT CDIP Test Collection [25].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "The first version of the dataset, listed in the results as SmallTobacco, is a sample of 3482 images from the collection, selected and labelled in another work [20].", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "This version of the dataset was used in a number of related papers [20, 22, 17].", "startOffset": 67, "endOffset": 79}, {"referenceID": 21, "context": "This version of the dataset was used in a number of related papers [20, 22, 17].", "startOffset": 67, "endOffset": 79}, {"referenceID": 16, "context": "This version of the dataset was used in a number of related papers [20, 22, 17].", "startOffset": 67, "endOffset": 79}, {"referenceID": 26, "context": "The selection of categories was guided by earlier work on document categorization [27], and also by the range of categories present in the already-existing SmallTobacco sample from the same collection.", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": ", over a million images) [19], so selection was restricted to categories that were well represented in the dataset.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "The SmallTobacco dataset was split as in the related work [20, 22, 17]: 800 images were used for training, 200 for validation, and the remainder for testing.", "startOffset": 58, "endOffset": 70}, {"referenceID": 21, "context": "The SmallTobacco dataset was split as in the related work [20, 22, 17]: 800 images were used for training, 200 for validation, and the remainder for testing.", "startOffset": 58, "endOffset": 70}, {"referenceID": 16, "context": "The SmallTobacco dataset was split as in the related work [20, 22, 17]: 800 images were used for training, 200 for validation, and the remainder for testing.", "startOffset": 58, "endOffset": 70}, {"referenceID": 29, "context": "The BigTobacco dataset was split in proportions similar to those of ImageNet [30]: 320000 images were used for training, 40000 images for validation, and 40000 images for testing.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "The CNNs were implemented in Caffe [16].", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The first network with a different architecture is listed in the results as \u201cSmall holistic CNN\u201d, which uses hyperparameters established in another work on document image analysis [17].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "As in previous work [22], the words were k-means clustered SURF features [4].", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "As in previous work [22], the words were k-means clustered SURF features [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "These features were pooled in a spatial pyramid [23], as well as in various combinations of horizontal and vertical partitions [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "These features were pooled in a spatial pyramid [23], as well as in various combinations of horizontal and vertical partitions [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 27, "context": "Three additional features were added as baselines to the featured approaches: the GIST descriptor [28], average brightness, and ensemble-of-regions average brightness.", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "The GIST descriptor has been shown to perform well on image retrieval tasks [11], but has not yet been applied to document analysis.", "startOffset": 76, "endOffset": 80}, {"referenceID": 28, "context": "This is consistent with the related work [29, 14]; it not only enables fast retrieval, but also to keeps the task within reasonable memory limits.", "startOffset": 41, "endOffset": 49}, {"referenceID": 13, "context": "This is consistent with the related work [29, 14]; it not only enables fast retrieval, but also to keeps the task within reasonable memory limits.", "startOffset": 41, "endOffset": 49}], "year": 2015, "abstractText": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.", "creator": "LaTeX with hyperref package"}}}