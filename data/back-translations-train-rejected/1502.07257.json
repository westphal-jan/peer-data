{"id": "1502.07257", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", "abstract": "Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.", "histories": [["v1", "Wed, 25 Feb 2015 17:15:56 GMT  (74kb)", "http://arxiv.org/abs/1502.07257v1", null], ["v2", "Sun, 15 Nov 2015 10:36:49 GMT  (133kb,D)", "http://arxiv.org/abs/1502.07257v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sergey bartunov", "dmitry kondrashkin", "anton osokin", "dmitry vetrov"], "accepted": false, "id": "1502.07257"}, "pdf": {"name": "1502.07257.pdf", "metadata": {"source": "META", "title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", "authors": ["Sergey Bartunov"], "emails": ["SBOS@SBOS.IN", "KONDRA2LP@GMAIL.COM", "ANTON.OSOKIN@INRIA.FR", "VETROVD@YANDEX.RU"], "sections": [{"heading": null, "text": "ar Xiv: 150 2.07 257v 1 [cs.C L] 2"}, {"heading": "1. Introduction", "text": "They could serve as input features for higher algorithms in word processing and help the word use natural texts sparingly."}, {"heading": "2. Skip-gram model", "text": "The original model of \"Skip-gram\" (Mikolov et al., 2013b) is then formulated as a series of \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = i \"i =\" i \"= i\" i = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i\" = \"i =\" i \"i =\" i = \"i =\" i \"=\" i = \"i =\" i \"=\" i = \"i =\" i = \"i =\" i = \"i =\" i = \"i\" = \"i =\" i = \"i =\" i = \"i =\" i \"=\" i = \"i =\" i = \"i =\" i = \"i =\" i = \"i =\" i = \"i =\" i = \"=\" i = \"i =\" i = \"i\" = \"i =\" i = \"i =\" i = \"=\" i = \"i =\" i \"=\" i = \"i\" = \"i =\" i = \"i =\" i = \"i =\" = \"i =\" i = \"i\" = \"i =\" = \"i =\" i = \"i =\" i = \"i =\" = \"=\" i = \"i =\" i = \"i\" = \"=\" i = \"i =\" = \"i =\" i = \"i\" = \"i =\" i \"=\" i = \"=\" i = \""}, {"heading": "3. Adaptive Skip-gram", "text": "It would be unrealistic to assume that we can grasp the semantics of all possible word meanings. (This means that we will expand the original number of prototypes required to assign additional prototypes to ambiguous words, further describing our Adaptive Skip-gram (AdaGram) models that extend the original Skip-gram and the required number of prototypes for each word with Bayesian Nonparametric Approach.First, that each word has the same meaning as its own prototypes. This means that we need to change the meaning (3) for certain meanings."}, {"heading": "3.1. Learning representations", "text": "One way to train the AdaGram is to maximize the marginal probability = q = q = q = q = q Efficiency of the model (Y = X = q = q = q Efficiency (Y = q = q Efficiency) (Y = q Efficiency) (Y = q Efficiency) (Y = q Efficiency) (Y = q Efficiency) (Y = q Efficiency) (Y = q Efficiency) (Y = q Efficiency) (Y = q Efficiency). It can be seen that the marginal probability is not comprehensible due to the latent variables Z and \u03b2. To make this comprehensible, we consider the variable lower limit on the marginal probability (5) L = Eq [Log p] and other methods for learning multiple word representations, our model could not be ascended by stochastic gradients (Z).r.t. To make this comprehensible, let us consider the probability of the marginal variability on the lower limit of L = p."}, {"heading": "3.2. Disambiguation and prediction", "text": "After the model has been trained on the basis of data D = {(xi, yi)} Ni = 1, it can be used to derive the meaning of an input word x in view of its context y. The predictive probability of a meaning can be calculated as follows: p (z = k | x, D, \u03b8, \u03b1). Since q (\u03b2) has the form of independent beta distributions whose parameters are given in (8), the integral can be taken analytically. The number of learned prototypes for a word w can be calculated as approximation of p (\u03b2 = k | w, \u03b1). Since q (\u03b2) has the form of independent beta distributions whose parameters are given in (8), the probability of any meaning of x given context y can be calculated as follows: p (z = k | x, y, short-term p (z = k | w, D, \u03b8, \u03b1, terminal, \u03b1, \u03b1) >."}, {"heading": "4. Related work", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "5. Experiments", "text": "In this section, we evaluate our model empirically using several tests: first, we demonstrate learned multi-prototype representations using several sample words; second, we examine how different \u03b1 values affect the number of learned prototypes, which we call semantic resolution of a model; and third, we evaluate our approach to the Word Sense Induction (WSI) task. To evaluate our method, we trained several models with different \u03b1 values in the April 2010 snapshot of Wikipedia (Shaoul & Westbury, 2010), which contains nearly 2 million articles and 990 million tokens, not including words that have less than 20 occurrences; the context width was set to C = 10; and the truncation level of stick-breaking approximation (the maximum number of meanings) to T = 30."}, {"heading": "5.1. Nearest neighbors", "text": "In Table 1 we present the meanings discovered by our model with the parameter \u03b1 = 0.1 for words from Neelakantan et al. (2014) and for a few other sample words. To distinguish the meanings, we obtain their closest neighbors by calculating the cosinal similarity between each meaning prototype and the meaning prototypes of all other words. We can see that the AdaGram model learns an appropriate number of significant and interpretable prototypes, and the predictive probability of each meaning reflects how often it was used in the training corpus."}, {"heading": "5.2. Semantic resolution", "text": "As mentioned in section 3, the hyperparameter \u03b1 of the AdaGram model indirectly influences the number of word meanings generated. Figure 1 shows the distribution of the number of word meanings generated among other values of \u03b1. It can be seen that while for most words relatively few meanings are learned, larger values of \u03b1 lead to more meanings in general. This effect can be explained by the property of the Dirichlet process to assign the number of prototypes that logarithmically depend on the number of word occurrences. Since word occurrences are known to be distributed by Zipf's law, the majority of words are rather rare and therefore our model finds few meanings for them. Figure 2 demonstrates this phenomenon quantitatively. In Table 2, we show how greater values of \u03b1 lead to more meanings using the example of the word \"light.\" The original Skip gram discovered only the meaning that refers to a physical phenomenon, AdaGram with \u03b1 = 0.075 based on the second meaning, which is more likely to be simultaneous with an increase in military significance."}, {"heading": "5.3. Word prediction", "text": "Since both Skip-gram and AdaGram are defined as models for predicting the context of a word, it is critical to evaluate how well they explain test data by predictive probability. In this experiment, we use the last 100 megabytes of the December 2014 snapshot from Wikipedia and 100 megabytes from the One Billion Word reference data set (Chelba et al., 2014) as test data. Similar to the traction method, we consider this text as pairs of input words and contexts of size C = 10, i.e. Dtest = {(xi, yi)} Ni = 1, and compare AdaGram with the original Skip-gram with an average log probability calculated by Equation (11). The results are given in Table 4."}, {"heading": "5.4. Word-sense induction", "text": "Non-parametric learning of a multi-prototype representation model is closely related to the Word Sense Induction (WSI) task, which aims at the automatic discovery of different meanings for the words. In fact, learned prototypes identify different word meanings and it is natural to judge how well they are consistent with human judgments. We compare our AdaGram model with the non-parametric Multi-sense Skip-gram (NP-MSSG), which was proposed by Neelakantan et al. (2014) and is currently the only existing approach to learning multi-prototype word representations with Skip-gram. In the comparison, we also include the parametric form of NP-MSSG, which fixed the number of meanings for all words during the training to 3. All models were trained on the same dataset that is the Wikipedia snapshot for word representations by Shaoul & Westbury (2010). For comparison with the MSSG and MSG models we used, we used the source code and MSG models."}, {"heading": "5.4.1. SEMEVAL-2010 DATASET", "text": "The SemEval-2010 dataset was introduced for SemEval2010 Word Sense Induction & Disambiguation competition (Manandhar et al., 2010), which consists of 100 target words and a total of 8915 contexts, and is particularly suitable for evaluating word representation learning methods because it consists only of one-line target words. We do not consider the SemEval-2013 dataset (Navigli & Vannella, 2013) for evaluation because it contains many multi-line target words (such as \"Mortal Kombat\") that cannot be handled properly using single words and altogether consist of only 6400 contexts. Manandhar et al. (2010) proposed two metrics for evaluation: V-Measure (VM) and F-Score (FS). The authors of the dataset pointed to the weakness of both VM and FS."}, {"heading": "5.4.2. SEMEVAL-2007 DATASET", "text": "Here, we report on the results of the SemEval 2007 WSI dataset (Agirre & Soroa, 2007), which contains 27232 contexts summarized from the Wall Street Journal (WSJ) corpus. Because the WSJ corpus is different from the general Wikipedia corpus, we have merged all the models we looked at into the \"One Billion Word Benchmark\" dataset (Chelba et al., 2014), which consists of news articles and is therefore more suitable for evaluating WSJ data. Our results on the Word prediction task also suggest that differences in education and the test corpus affect performance. Results are presented in Table 6. Each model was supplied with contexts of the size that maximizes its ARI performance."}, {"heading": "5.4.3. NEW WWSI DATASET", "text": "To make the evaluation more comprehensive, we are introducing the new Wikipedia Word-sense Induction (WWSI) dataset, which consists of 188 target words and 36,354 contexts. To our knowledge, this is currently the largest WSI dataset available. While SemEval datasets are handcrafted by experts who mapped contexts in gold standard sense inventory, we began collecting WWSI snapshots from Wikipedia using a fully automated approach as of December 2014. Further details on the dataset construction process can be found in the appendix. We use an adjusted margin index averaged using test words to compare AdaGram models that compare with different values of \u03b1 and the models of Neelakantan et al. (2014). Results are provided in Table 7. Our model exceeds both MSSG and NP-MSSG for all values of parameter \u03b1 except \u03b1 = 0.05. This shows that such results can be interpreted not only by a large number of adjectives, but also by a large number of adjectives."}, {"heading": "6. Conclusion", "text": "In the thesis, we proposed AdaGram, the non-parametric extension of the well-known Skip-gram model. AdaGram uses different prototypes to represent a word according to context, and can therefore handle different forms of word ambiguity. Our experiments suggest that representations learned through our model correspond to different word meanings. By using the resolution parameter \u03b1, we can control how many prototypes are extracted from the same text corpus. Excessive values of \u03b1 lead to different prototypes that correspond to the same meaning, which reduces model performance. The values \u03b1 = 0.1 \u2212 0.2 are generally good for practical purposes. For these values, the truncation level T = 30 is sufficient and does not affect the number of detected prototypes. AdaGram also has a variational online learning algorithm that is highly scalable and allows our model to be trained only by a multiple slower rate than extremely efficient Skip-gram models, since the problem of using multiple word flutes is closely related."}, {"heading": "Appendix. WWSI Dataset construction details", "text": "Similarly (Navigli & Vannella, 2013), we considered Wikipedia's Disambiguation Pages to be a list of ambiguous words. From this list, we selected targeted one-dimensional words that appeared at least 5,000 times in the text to ensure that there were enough training contexts in Wikipedia to capture different meanings of a word (note, however, that all models were trained on previous Wikipedia snapshots), we also considered pages belonging to some categories, such as \"Letternumber _ disambiguation _ pages,\" as they did not contain meaningful words, and we prepared the meaningful inventory for each word in the list of names of Wikipedia pages that match the pattern \"WORD _ (*),\" which is used as a convenient naming of specific word meanings, as we applied some automatic filtering to remove names of people and geographical locations."}], "references": [{"title": "SemEval-2007 task 02: Evaluating word sense induction and discrimination systems", "author": ["E. Agirre", "A. Soroa"], "venue": "In International workshop on semantic evaluation (SemEval),", "citeRegEx": "Agirre and Soroa,? \\Q2007\\E", "shortCiteRegEx": "Agirre and Soroa", "year": 2007}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan,? \\Q2005\\E", "shortCiteRegEx": "Blei and Jordan", "year": 2005}, {"title": "Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements", "author": ["J. Boyd-Graber", "D. Mimno", "D. Newman"], "venue": "CRC Handbooks of Modern Statistical Methods. CRC Press, Boca Raton, Florida,", "citeRegEx": "Boyd.Graber et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "A unified model for word sense representation and disambiguation", "author": ["X. Chen", "Z. Liu", "M. Sun"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Fast variational inference in the conjugate exponential family", "author": ["J. Hensman", "M. Rattray", "N.D. Lawrence"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hensman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2012}, {"title": "Stochastic variational inference", "author": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT),", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "SemEval-2010 task 14: Word sense induction & disambiguation", "author": ["S. Manandhar", "I.P. Klapaftis", "D. Dligach", "S.S. Pradhan"], "venue": "In International workshop on semantic evaluation (SemEval),", "citeRegEx": "Manandhar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Manandhar et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Technical Report 1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mnih and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2008}, {"title": "Inducing word senses to improve web search result clustering", "author": ["R. Navigli", "G. Crisafulli"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Navigli and Crisafulli,? \\Q2010\\E", "shortCiteRegEx": "Navigli and Crisafulli", "year": 2010}, {"title": "SemEval-2013 task 11: Word sense induction and disambiguation within an end-user application", "author": ["R. Navigli", "D. Vannella"], "venue": "In International workshop on semantic evaluation (SemEval),", "citeRegEx": "Navigli and Vannella,? \\Q2013\\E", "shortCiteRegEx": "Navigli and Vannella", "year": 2013}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["A. Neelakantan", "J. Shankar", "A. Passos", "A. McCallum"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["L. Qiu", "Y. Cao", "Z. Nie", "Y. Yu", "Y. Rui"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "The infinite Gaussian mixture model", "author": ["C.E. Rasmussen"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rasmussen,? \\Q2000\\E", "shortCiteRegEx": "Rasmussen", "year": 2000}, {"title": "A mixture model with sharing for lexical semantics", "author": ["J. Reisinger", "R. Mooney"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Reisinger and Mooney,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "Multi-prototype vectorspace models of word meaning. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)", "author": ["J. Reisinger", "R.J. Mooney"], "venue": null, "citeRegEx": "Reisinger and Mooney,? \\Q2010\\E", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "Nonlinear models using Dirichlet process mixtures", "author": ["B. Shahbaba", "R. Neal"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Shahbaba and Neal,? \\Q2009\\E", "shortCiteRegEx": "Shahbaba and Neal", "year": 2009}, {"title": "A probabilistic model for learning multiprototype word embeddings", "author": ["F. Tian", "H. Dai", "J. Bian", "B. Gao", "R. Zhang", "E. Chen", "Liu", "T.-Y"], "venue": "In International Conference on Computational Linguistics (COLING),", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Word-sense disambiguation for machine translation", "author": ["D. Vickrey", "L. Biewald", "M. Teyssier", "D. Koller"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Vickrey et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 12, "context": "Among many others the two particular models Continuous Bag of Words (CBOW) and Skip-gram (SG) proposed by Mikolov et al. (2013a) were used to obtain high-dimensional distributed representations that capture many semantic relationships and linguistic regularities (Mikolov et al.", "startOffset": 106, "endOffset": 129}, {"referenceID": 27, "context": "Many natural language processing (NLP) applications benefit from ability to deal with word ambiguity (Navigli & Crisafulli, 2010; Vickrey et al., 2005).", "startOffset": 101, "endOffset": 151}, {"referenceID": 26, "context": "Since word representations have been used as word features in dependency parsing (Chen & Manning, 2014), named-entity recognition (Turian et al., 2010) and sentiment analysis (Maas et al.", "startOffset": 130, "endOffset": 151}, {"referenceID": 10, "context": ", 2010) and sentiment analysis (Maas et al., 2011) among many other tasks, employing multi-prototype representations could increase the performance of such representation-based approaches.", "startOffset": 31, "endOffset": 50}, {"referenceID": 5, "context": "we use the constructive definition of Dirichlet process (Ferguson, 1973) for automatic determination of the required number of prototypes.", "startOffset": 56, "endOffset": 72}, {"referenceID": 20, "context": ") is not known a priori (Shahbaba & Neal, 2009; Rasmussen, 2000) which is exactly our case.", "startOffset": 24, "endOffset": 64}, {"referenceID": 23, "context": "We use the constructive definition of DP via the stickbreaking representation (Sethuraman, 1994) to define a prior over meanings of a word.", "startOffset": 78, "endOffset": 96}, {"referenceID": 12, "context": "Similarly to Mikolov et al. (2013a) we do not consider any regularization (and so the informative prior) for representations and seek for point estimate of \u03b8.", "startOffset": 13, "endOffset": 36}, {"referenceID": 9, "context": "the minimization of Kullback-Leibler divergence between q(Z,\u03b2) and the true posterior (Jordan et al., 1999).", "startOffset": 86, "endOffset": 107}, {"referenceID": 7, "context": "In order to keep the efficiency of Skip-gram training procedure, we employ stochastic variational inference approach (Hoffman et al., 2013) and derive online optimization algorithm for the maximization of L.", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": "Thus L allows for employing stochastic optimization framework and was shown to converge faster than L when using gradient methods (Hensman et al., 2012).", "startOffset": 130, "endOffset": 152}, {"referenceID": 6, "context": "Thus L allows for employing stochastic optimization framework and was shown to converge faster than L when using gradient methods (Hensman et al., 2012). Following Hoffman et al. (2013) we iteratively optimizeL with respect to the global parameters using stochastic gradient estimated at a single object.", "startOffset": 131, "endOffset": 186}, {"referenceID": 7, "context": "The stochastic gradient with respect to natural parameters awk and bwk according to (Hoffman et al., 2013) can be estimated by computing intermediate values of natural parameters (\u00e2wk, b\u0302wk) on the i-th data point as if we estimated q(z) for all occurrences of xi = w equal to q(zi):", "startOffset": 84, "endOffset": 106}, {"referenceID": 25, "context": "While MSSG defines the number of prototypes apriori similarly to (Tian et al., 2014), NP MSSG features automatic discovery of multiple meanings for each word.", "startOffset": 65, "endOffset": 84}, {"referenceID": 7, "context": "Huang et al. (2012); Reisinger & Mooney (2010a;b) propose the neural network-based methods for learning multiprototype representations.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Huang et al. (2012); Reisinger & Mooney (2010a;b) propose the neural network-based methods for learning multiprototype representations. Both methods include clustering contexts for all words as prepossessing or intermediate step. While this allows to learn multiple prototypes per word, clustering large number of contexts brings serious computational overhead and limit these approaches to offline setting. Recently various modifications of Skip-gram were proposed to learn multi-prototype representations. Qiu et al. (2014) developed Proximity-Ambiguity Sensitive Skipgram which maintains individual representations for different parts of speech (POS) of the same word.", "startOffset": 0, "endOffset": 526}, {"referenceID": 7, "context": "Huang et al. (2012); Reisinger & Mooney (2010a;b) propose the neural network-based methods for learning multiprototype representations. Both methods include clustering contexts for all words as prepossessing or intermediate step. While this allows to learn multiple prototypes per word, clustering large number of contexts brings serious computational overhead and limit these approaches to offline setting. Recently various modifications of Skip-gram were proposed to learn multi-prototype representations. Qiu et al. (2014) developed Proximity-Ambiguity Sensitive Skipgram which maintains individual representations for different parts of speech (POS) of the same word. While this may handle word ambiguity to some extent, clearly there could be many meanings even for the same part of speech of some word remaining not discovered by this approach. Work of Tian et al. (2014) can be considered as a parametric form of our model with number of meanings for each word fixed.", "startOffset": 0, "endOffset": 878}, {"referenceID": 4, "context": "Chen et al. (2014) incorporate external knowledge about word meanings into Skip-gram in the form of sense inventory.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Chen et al. (2014) incorporate external knowledge about word meanings into Skip-gram in the form of sense inventory. First, single-prototype representations are pre-trained with original Skip-gram. Afterwards, meanings provided by WordNet lexical database are used learn multi-prototype representations for ambiguous words. The dependency on the external high-quality linguistic resources such as WordNet makes this approach inapplicable to languages lacking such databases. In contrast, our model does not consider any form of supervision and learns the sense inventory automatically from the raw text. Recent work of Neelakantan et al. (2014) proposing Multisense Skip-gram (MSSG) and its nonparameteric (not in the sense of Bayesian nonparametrics) version (NP MSSG) is the closest to AdaGram prior art.", "startOffset": 0, "endOffset": 645}, {"referenceID": 18, "context": "In our experimental evaluation we use models of Neelakantan et al. (2014) as baselines.", "startOffset": 48, "endOffset": 74}, {"referenceID": 18, "context": "1 for words used in Neelakantan et al. (2014) and for a few other sample words.", "startOffset": 20, "endOffset": 46}, {"referenceID": 18, "context": "ric Multi-sense Skip-gram (NP-MSSG) proposed by Neelakantan et al. (2014) which is currently the only existing approach to learning multi-prototype word representations with Skip-gram.", "startOffset": 48, "endOffset": 74}, {"referenceID": 18, "context": "ric Multi-sense Skip-gram (NP-MSSG) proposed by Neelakantan et al. (2014) which is currently the only existing approach to learning multi-prototype word representations with Skip-gram. We also include in comparison the parametric form of NP-MSSG which has the number of meanings fixed to 3 for all words during the training. All models were trained on the same dataset which is the Wikipedia snapshot by Shaoul & Westbury (2010). For the comparison with MSSG and NP-MSSG we used source code and models released by the authors.", "startOffset": 48, "endOffset": 429}, {"referenceID": 18, "context": "ric Multi-sense Skip-gram (NP-MSSG) proposed by Neelakantan et al. (2014) which is currently the only existing approach to learning multi-prototype word representations with Skip-gram. We also include in comparison the parametric form of NP-MSSG which has the number of meanings fixed to 3 for all words during the training. All models were trained on the same dataset which is the Wikipedia snapshot by Shaoul & Westbury (2010). For the comparison with MSSG and NP-MSSG we used source code and models released by the authors. Neelakantan et al. (2014) limited the number of words for which multi-prototype representations were learned (30000 and 6000 most frequent words) for these models.", "startOffset": 48, "endOffset": 553}, {"referenceID": 11, "context": "The SemEval-2010 dataset was introduced for SemEval2010 Word Sense Induction & Disambiguation competition (Manandhar et al., 2010).", "startOffset": 106, "endOffset": 130}, {"referenceID": 2, "context": "(Boyd-Graber et al., 2014).", "startOffset": 0, "endOffset": 26}, {"referenceID": 17, "context": "We use adjusted Rand index averaged over test words to compare AdaGram models trained with different values of \u03b1 and the models of Neelakantan et al. (2014). The results are provided in Table 7.", "startOffset": 131, "endOffset": 157}], "year": 2015, "abstractText": "Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on wordsense induction task.", "creator": "LaTeX with hyperref package"}}}