{"id": "1502.07428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Representative Selection in Non Metric Datasets", "abstract": "This paper considers the problem of representative selection: choosing a subset of data points from a dataset that best represents its overall set of elements. This subset needs to inherently reflect the type of information contained in the entire set, while minimizing redundancy. For such purposes, clustering may seem like a natural approach. However, existing clustering methods are not ideally suited for representative selection, especially when dealing with non-metric data, where only a pairwise similarity measure exists. In this paper we propose $\\delta$-medoids, a novel approach that can be viewed as an extension to the $k$-medoids algorithm and is specifically suited for sample representative selection from non-metric data. We empirically validate $\\delta$-medoids in two domains, namely music analysis and motion analysis. We also show some theoretical bounds on the performance of $\\delta$-medoids and the hardness of representative selection in general.", "histories": [["v1", "Thu, 26 Feb 2015 04:16:31 GMT  (1240kb,D)", "https://arxiv.org/abs/1502.07428v1", null], ["v2", "Fri, 19 Jun 2015 22:44:29 GMT  (1237kb,D)", "http://arxiv.org/abs/1502.07428v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["elad liebman", "benny chor", "peter stone"], "accepted": false, "id": "1502.07428"}, "pdf": {"name": "1502.07428.pdf", "metadata": {"source": "CRF", "title": "Representative Selection in Non Metric Datasets", "authors": ["Elad Liebman", "Benny Chor", "Peter Stone"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Consider the task of a teacher charged with introducing his class to a large corpus of songs (for example, popular Western music since 1950).When drafting the curriculum, this teacher will have to select a relatively small set of songs to discuss with his students, so that 1) each song in the larger corpus is represented by his selection (in the sense that it is relatively similar to one of the selected songs), and 2) the set of selected songs is small enough to be covered in a single semester. This task is an example of the representative selection problem. Similar challenges often arise in tasks related to summarizing and modeling data. For example, finding a characteristic subset of Facebook profiles from a large set or a subset of representative news articles from the entire set of information collected in a single day from many different sources."}, {"heading": "1.1 Representative Selection: Problem Definition", "text": "Let S be a data set, d: S \u00d7 S \u2192 R + be a distance measure (not necessarily a measurement measure), and \u03b4 be a distance threshold below which samples are considered sufficiently similar. We are charged with finding a representative subset that best summarizes the data. We make the following two demands on each algorithm to find a representative subset: - Requirement 1: The algorithm must return a subset C-S so that for each sample x-S, there is a sample c-C that d (x, c) \u2264 \u03b4.- Requirement 2: The algorithm cannot rely on a metric representation of samples in S.To compare the quality of the different subsets, we measure the quality of encapsulation by two criteria: - Criterion 1: | C | - we seek the smallest possible subset C that meets requirement 1:.- Criterion 2: We would best match the representative set of data."}, {"heading": "1.2 Testbed Applications", "text": "Representative selection is useful in many contexts, especially if the complete data set is either redundant (due to many nearly identical samples) or if all the samples are not practicable. Thus, for example, in a large document and a satisfactory degree of similarity between sentences, the summary of texts [25] could be formulated as a representative selection task - you get a subset of sentences that best capture the nature of the document. Likewise, this problem could be attributed to the extraction of \"visual words\" or representative frameworks from visual inputs [42,26]. This paper examines two concrete cases in which representatives are needed: - Music Analysis - the last decade has seen an increase in computer-based analysis of music databases for the recovery of music information [5], recommendation systems [23] and computer-based musicology [8]. A problem of interest in these contexts consists in extracting short representative musical segments that best represent the overall character of the piece of the piece or the analysis (in many respects)."}, {"heading": "2 Background and Related Work", "text": "There are several existing classes of algorithms that solve problems related to representative selection, and this section examines them and discusses to what extent they apply (or not) to our problem."}, {"heading": "2.1 Limitations of Traditional Clustering", "text": "Given the prevalence of cluster algorithms, it is tempting to make a representative selection by clustering the data and using cluster centers (if they are included in the set) as representatives or the closest point to each center. In some cases, it may even seem sufficient once the data is clustered to select randomly selected samples from each cluster as representatives. However, such an approach usually only takes into account the average distance between representatives and samples, and is unlikely to yield good results in terms of other requirements, such as minimizing the worst-case distance or maintaining the smallest possible number of clusters. In addition, the task of determining the desirable number of clusters k for sufficient representation can be a difficult challenge in itself. Consider the example in Figure 1: Given a set of | S | = 100 points and a distance measurement (in this case, the Euclidean distance measurement), we are looking for a group of representatives that is within the distance of 1 point, but with a minimum number of 1 and a minimum number of 1 required."}, {"heading": "2.2 Clustering and Spatial Representation", "text": "The above constraint on clustering applies even if the data can be embedded as coordinates in some n-dimensional vector spaces. [3] This constraint renders unapplicable many common clustering techniques, including canonical k-means [24], or more recent works such as [39]. Moreover, the distance functions we construct (which detect both local and global similarities) are not metric because they violate the triangle of inequality. Due to this property, methods that rely on distance are also not applicable. Among such methods are neighborhood connections that apply to non-metric data or the prototypes of algorithms [1]. [3] Nevertheless, certain cluster algorithms are applicable, such as k-medoid algorithms, and hierarchical clustering."}, {"heading": "3 The \u03b4-Medoids Algorithm", "text": "The algorithm does not assume a metric or spatial representation, but relies solely on the presence of any (not necessarily symmetrical) measure of distance or dissimilarity d: S \u00d7 S \u2192 R +. Similar to the solution with the k centers, the approach attempts to directly find samples that sufficiently cover the entire dataset. The algorithm then refines the selected list of representatives by iteratively scanning the dataset and adding representatives when they differ sufficiently from the current set. During scanning, the algorithm associates with each representative a cluster that includes the samples it represents. Subsequently, the algorithm refines the selected list of representatives to reduce the average range. This procedure repeats until the algorithm achieves convergence. Thus, we address both minimality (criterion 1) and average distance considerations (criterion 2)."}, {"heading": "3.1 Straightforward \u03b4-Representative Selection", "text": "Consider a simpler \"one-shot\" representative selection algorithm that meets the \u03b4 distance criterion. The algorithm sweeps through the algorithm 2 One-shot \u03b4-Representative selection algorithm1: Input: Data x0... xm, required distance \u03b4 2: Initialize representatives = \u2205. 3: Initialize clusters = \u2205 4: Representative assignment of subordinated representatives, RepAssign, lines 5-22: 5: do for i = 0 to m 6: Initialize distancers = \u221e 7: Initialize representatives = null 8: do for representatives 9: if d (xi, rep) \u2264 dist then10: Representative = rep 11: dist = d (xi, rep) 12: End, if 13: End for 14: if distancers then add xi to cluster representative 16: otherwise 17: more representative = xi 18: Initialize cluster representative GOP = 20: add cluster representative element if this cluster is not representative: 20 before."}, {"heading": "3.2 The Full \u03b4-Medoids Algorithm", "text": "This algorithm is based on the simple approach described in Section 3.1. Unlike algorithm 2, however, it repeats the samples repeatedly. In each iteration, the algorithm associates each sample with a representative so that it is never further removed from any representative (the RepAssign subroutine, see algorithm 3), just as in algorithm 2. The main difference is that at the end of each iteration, it finds a more closely fitting representative for each cluster S associated with representative s. Specifically, the representative S = medoidS = argmins S x x (x, s) (lines 8 \u2212 13), with the proviso that no sample S is further than a convergence of medoidS. This step en algorithm 3 The subject medoid representative selection algorithm: 1 x x: Input: data x0. xm, required distance: 2: 0."}, {"heading": "3.3 Merging Close Clusters", "text": "Since fulfillment of the distance restriction with a minimum set of representatives (criterion 1) is NP-hard (see section 4), this is not guaranteed with the \u03b4-medoids algorithm. A simple optimization procedure can reduce the number of representatives in certain cases. Thus, in some cases, there may be over-segmentation. To reduce such occurrence, it is possible to iterate through representative pairs no more than \u03b4 apart, and to see if a grouping of their respective clusters could produce a new representative covering all samples in the associated clusters. If possible, the two representatives are eliminated in favor of the new common representative. The process is repeated until no pair in the potential pairs list can be merged. This procedure can be generalized for larger representative group sizes, depending on computational traceability. These refinement steps can be taken after each iteration of the algorithm. However, if the number of representatives is high, this approach may not be feasible overall in certain cases (see section 5)."}, {"heading": "4 Analysis Summary", "text": "In this section we present the hardness of the representative selection problem and briefly discuss the efficiency of the \u03b4-medoid algorithm. We show that the problem of finding a minimum representative selection set is NP-hard and give certain limits for the performance of \u03b4-medoids in metric spaces in terms of representative target size and average distance. We also show that an approximation of the representative selection problem is NP-hard in non-metric spaces, both in terms of the representative target size and in terms of maximum distance. For reasons of legibility, we present all the details in Appendix D."}, {"heading": "4.1 NP-Hardness of the Representative Selection Problem", "text": "Theorem 2 Fulfillment criterion 1 (minimum representative proposition) is NP-Hard."}, {"heading": "4.2 Bounds on \u03b4-medoids in Metric Spaces", "text": "Theorem 3 In a metric space, the average distance of a representative set | C | = k obtained by the \u03b4-medoid algorithm is bound to 2OPT, where OPT is the maximum distance achieved by an optimal allocation of k representatives (in terms of maximum distance). Theorem 4 The size of the representative set returned by the \u03b4-medoid algorithm is limited by k \u2264 N (\u03b42), where N (x) is the minimum number of representatives required to meet the distance criterion."}, {"heading": "4.3 Hardness of Approximation of Representative Selection in Non-Metric Spaces", "text": "In non-metric spaces, the problem of representative selection becomes much more difficult. We now show that there is no c approximation for the representative selection problem either with respect to the first criterion (representative quantity) or the second criterion (distance - we focus on the maximum distance, but a similar result for the average distance is implied). Theorem 5 For the representative quantity problem, there is no approximation of the constant factor with respect to the representative quantity size. Theorem 6 For representative quantities of optimal quantity k, there is no approximation of the constant factor with respect to the maximum distance between the optimal representative quantity set and the samples."}, {"heading": "4.4 Efficiency of \u03b4-Medoids", "text": "An important observation is that with each iteration, each sample is compared only with the current representative set, and a sample is only introduced into the representative set if it is > \u03b4 removed from all other representatives. After each iteration, the representatives induce a partition into clusters, and only samples within the same cluster are compared with each other. In the worst case scenario, the runtime complexity of algorithm O (| S | 2) may be, but in practice, we can achieve significantly better runtime performance that moves asymptotically closer to | S | 1.5. We note that in each iteration of the algorithm after the partitioning phase (the RepAssign subroutine in algorithm 3), the algorithm maintains a statutory representative set, so that in practice we can stop the algorithm long before convergence, depending on need and resources."}, {"heading": "5 Empirical Results", "text": "In this section, we analyze empirically the performance of the \u03b4-medoid algorithm in two problem areas - music analysis and agent motion analysis. We show that \u03b4-medoids perform well on criterion 1 (minimizing the representative set), while we get a good solution for criterion 2 (maintaining a small average distance). We compare ourselves to three alternative methods - k-medoids, the greedy heuristic k-center and spectral clustering (using cluster medoids as representatives) [33], and show that we exceed all three methods. We note that while these methods are not necessarily designed to address the representative selection problem, they and clustering approaches are generally used for such purposes in practice (see [17], for example). To get a measure of statistical significance, we use a random subset of sample S | 5000 as a representative sample for each set we analyze."}, {"heading": "5.1 Distance Measures", "text": "In our two problem areas, there is no simple or generally accepted measure of the distance between samples. For this reason, we have developed a distance function based on domain knowledge and experiments for each setting. Feature selection and distance optimization are outside the scope of this work. For completeness, the full details of our distance functions appear in Appendix E. We believe that the results are not particularly sensitive to the choice of a particular distance function, but will leave this analysis to future work."}, {"heading": "5.2 Setting 1 - musical segments", "text": "This domain illustrates many of the motivations listed in Section 1. The need for good representative selection is driven by several tasks, including the characterization of style by comparing different musical corpora (see [12]), and the classification of music by composer, genre, or period [2]. For the purpose of this work, we used the Music21 corpus, which is provided in MusicXML format [9]. For simplicity, we focus on the melodic content of the piece, which can be characterized as a variation of pitch (or frequency). We use thirty pieces of music: 10 representative pieces by Mozart, Beethoven, and Haydn. The melodic lines in the pieces are isolated and segmented, using basic grouping principles adapted to [29]. In the segmentation process, short melodic sequences of 5 to 8 beats are produced."}, {"heading": "5.3 Setting 2 - agent movement in robot soccer simulation", "text": "As described in Section 1.2, the analysis of agent behavior may be of interest in several areas. RoboCup is a well-established problem domain for AI in general [22]. In this work, we have decided to focus on the RoboCup 2D Simulation League. We have collected match data from several complete games from the past two RoboCup competitions. An example of gameplay setting and potential motion paths can be found in Figure 5.Our purpose is to extract segments that best represent the movement patterns of agents throughout the game. In the specific context of the Robocup Simulation League, there are several tasks that motivate a representative selection, including agent and team characterization and learning trajectories for optimization. Data using simulation prototype data, we extract the movement of the 22 agents over the course of the game (# timesteps = 6000). Agents can be seen moving in two-dimensional trajectories (three examples)."}, {"heading": "5.4 Stability of the \u03b4-Medoids Algorithm", "text": "In this section, we find that the \u03b4-medoid algorithm is actually robust in terms of the scanning arrangement (satisfaction of desideratum 1 from section 1). To test stability, we performed \u03b4-medoids, k-medoids, k-center heuristics and spectral clustering several times on a large collection of data sets, changed the input sequence for each iteration, and examined how well the representative group was preserved across iterations and methods. Our analysis found that the first three algorithms consistently overlap > 90% on average, and the observed degree of stability is almost identical. Spectral clustering results in drastically less stable representative groups. For a more comprehensive description of these results, see Appendix B."}, {"heading": "6 Summary and Discussion", "text": "In this paper, we present a novel heuristic algorithm to solve the problem of representative selection: We introduce the novel \u03b4-medoids algorithm and show that it surpasses other approaches that only aim to best fit the data into a given number of clusters or to minimize the maximum distance. There is a subtle but significant influence on focusing on a maximum distance criterion rather than selecting the number of clusters k. While both \u03b4-medoids and k-medoids aim to minimize the sum of distances between representatives and the complete set, k-medoids do so without considering any individual distance. Therefore, we need to drastically increase the value of k selection to ensure that our distance criterion is met and that sparse regions of our sample are adequately represented."}, {"heading": "Acknowledgements", "text": "This work took place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory of the University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation (CNS-1330072, CNS1305287), ONR (21C184-01), AFOSR (FA8750-14-1-0070, FA9550-14-1-0087) and Yujin Robot."}, {"heading": "A Proof of Convergence for the \u03b4-medoids Algorithm", "text": "Theorem 1 Algorithm 3 converges after a finite number of steps. Proof sketch: Let's consider the total sum of distances from each point to its associated cluster representative when iterating i Ci (s, Ci (s)). Let's consider the total sum of distances from each point to its associated cluster representative when iterating i Ci (s, Ci (s). Let's assume that after the i-th round we get a partition to k clusters, C1.. Ck. Our next step is to go over each cluster and redetermine a representative sample that minimizes the sum of distances from each point to the representative of that cluster."}, {"heading": "B Stability of \u03b4-Medoids", "text": "In this section, we note that the \u03b4-medoids algorithm is robust with respect to the scan order (satisfaction of desideratum 1 from Section 1).To test this problem, we generated a large collection (N = 1000) of data sets (randomly generated from multimodal distributions), then calculated the algorithm # repetitions = 100 times for each data set in the collection, changing the input order each time. Next, we calculated the average overlap between any two representative sets generated for the same data set by this method, and then calculated a histogram of average overlap results across all data inputs. Finally, we compared these stability results with those of the k-medoids algorithm (randomized start positions), with the heuristic centers (randomizing the start node) and spectral clustering considered identical."}, {"heading": "C Performance of \u03b4-Medoids in Metric Spaces", "text": "In this section, we compare the performance of the algorithm with the benchmark methods used in the Empirical Results section. To create a standard metric setting, we consider a 10-dimensional metric space in which samples are taken from a multivariate Gaussian distribution. We sample 1000 samples per experiment, 20 experiments per experiment, with randomly chosen means and deviations. Results are presented in Figure 7. As can be seen, the performance of the \u03b4 medoids algorithm compared to the other methods is qualitatively similar compared to the non-metric cases, despite the metric property of the data in this setting."}, {"heading": "D Extended Analysis", "text": "In this section we will consider the hardness of the representative selection problem and discuss the efficiency of \u03b4-medoids algorithm.D.1 NP-Hardness of the Representative Selection Probleorem 2 Satisfying Criterion 1 is NP-Hard.Proof Sketch: We show this via a reduction of the vertex coverage problem. Faced with a diagram G = (V, E) we construct a distance matrix M of size. If we are connected to two different wells in the diagram, vi, i = j, we set the value of the entries (i, j) and (j, i) to be in M. Otherwise we set the value of the input to + 1. Formally: M (i, j) = 1 if (i, j).E 0 i = jB = otherwiseThis construction is polynomial in | V."}, {"heading": "E Calculating the Distance Measures", "text": "In this section, we describe in some detail how the distance measurements we used were calculated, as well as some of the considerations involved in their formulas. - Segments of individual segments are divided into two different segments. - The following information is extracted from each segment: - Pitch Sequence - the sequential representation of the sound frequency over time. - Pitch Bag - a \"bag\" that contains all keys in the sequence, with sensitivity to registration. - Pitch Class Bag - a \"bag\" that contains all keys in the sequence, with sensitivity to registration. - Pitch Class Bag - a \"bag\" that contains all rhythmic patterns in the sequence, with simplicity, as pairs of subsequent annotations in the sequence. - Interval Bag - a \"bag\" that contains all keys in the sequence."}], "references": [{"title": "A new approach to data driven clustering", "author": ["Arik Azran", "Zoubin Ghahramani"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Aggregate features and a da b oost for music classification", "author": ["James Bergstra", "Norman Casagrande", "Dumitru Erhan", "Douglas Eck", "Bal\u00e1zs K\u00e9gl"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Opponent modeling in poker", "author": ["Darse Billings", "Denis Papp", "Jonathan Schaeffer", "Duane Szafron"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Opponent modeling in multi-agent systems. In Adaption and Learning in Multi-Agent Systems, pages 40\u201352", "author": ["David Carmel", "Shaul Markovitch"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Content-based music information retrieval: current directions and future challenges", "author": ["Michael A Casey", "Remco Veltkamp", "Masataka Goto", "Marc Leman", "Christophe Rhodes", "Malcolm Slaney"], "venue": "Proceedings of the IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Parallel spectral clustering in distributed systems", "author": ["Wen-Yen Chen", "Yangqiu Song", "Hongjie Bai", "Chih-Jen Lin", "Edward Y Chang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Automatic selection of representative photo and smart thumbnailing using near-duplicate detection", "author": ["Wei-Ta Chu", "Chia-Hung Lin"], "venue": "In Proceedings of the 16th ACM international conference on Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Computational and comparative musicology", "author": ["Nicholas Cook"], "venue": "Empirical musicology: aims,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "music21: A toolkit for computeraided musicology and symbolic music data", "author": ["Michael Scott Cuthbert", "Christopher Ariza"], "venue": "In Int. Society for Music Information Retrieval Conf.(ISMIR", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["Jason V Davis", "Brian Kulis", "Prateek Jain", "Suvrit Sra", "Inderjit S Dhillon"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Distance-based clustering of xml documents", "author": ["Francesco De Francesca", "Gianluca Gordano", "Riccardo Ortale", "Andrea Tagarelli"], "venue": "In ECML/PKDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Using machine-learning methods for musical style modeling", "author": ["Shlomo Dubnov", "Gerard Assayag", "Olivier Lartillot", "Gill Bejerano"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Using representative-based clustering for nearest neighbor dataset editing", "author": ["Christoph F Eick", "Nidal Zeidat", "Ricardo Vilalta"], "venue": "In Data Mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Finding exemplars from pairwise dissimilarities via simultaneous sparse recovery", "author": ["Ehsan Elhamifar", "Guillermo Sapiro", "Rene Vidal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Computers and intractability: A guide to the theory of np-completeness", "author": ["Michael R Gary", "David S Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1979}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["Teofilo F Gonzalez"], "venue": "Theoretical Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "Video summarization by k-medoid clustering", "author": ["Youssef Hadi", "Fedwa Essannouni", "Rachid Oulad Haj Thami"], "venue": "In Proceedings of the 2006 ACM symposium on Applied computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A best possible heuristic for the k-center problem", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Mathematics of operations research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1985}, {"title": "A unified approach to approximation algorithms for bottleneck problems", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}, {"title": "Data clustering: a review", "author": ["Anil K Jain", "M Narasimha Murty", "Patrick J Flynn"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Reducibility Among Combinatorial Problems", "author": ["R.M. Karp"], "venue": "Complexity of Computer Computations,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1972}, {"title": "The robocup synthetic agent challenge 97", "author": ["Hiroaki Kitano", "Milind Tambe", "Peter Stone", "Manuela Veloso", "Silvia Coradeschi", "Eiichi Osawa", "Hitoshi Matsubara", "Itsuki Noda", "Minoru Asada"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Social tagging and music information retrieval", "author": ["Paul Lamere"], "venue": "Journal of New Music Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["James MacQueen"], "venue": "In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1967}, {"title": "Advances in automatic text summarization", "author": ["Inderjeet Mani", "Mark T Maybury"], "venue": "MIT press,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Wearable hand activity recognition for event summarization", "author": ["WW Mayol", "DW Murray"], "venue": "In Wearable Computers,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["Saul B Needleman", "Christian D Wunsch"], "venue": "Journal of molecular biology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1970}, {"title": "Automatic summarization", "author": ["Ani Nenkova", "Kathleen Rose McKeown"], "venue": "Now Publishers Inc,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "A comparison of statistical and rulebased models of melodic segmentation", "author": ["MT Pearce", "D M\u00fcllensiefen", "GA Wiggins"], "venue": "In Proceedings of the Ninth International Conference on Music Information Retrieval,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "A sub-constant error-probability low-degree test, and a sub-constant error-probability pcp characterization of np", "author": ["Ran Raz", "Shmuel Safra"], "venue": "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Finding groups in data: An introduction to cluster analysis", "author": ["Peter J Rousseeuw", "Leonard Kaufman"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1990}, {"title": "The neighbor-joining method: a new method for reconstructing phylogenetic trees", "author": ["Naruya Saitou", "Masatoshi Nei"], "venue": "Molecular biology and evolution,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1987}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "Efficient spectral neighborhood blocking for entity resolution", "author": ["Liangcai Shu", "Aiyou Chen", "Ming Xiong", "Weiyi Meng"], "venue": "In Data Engineering (ICDE),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Slink: an optimally efficient algorithm for the single-link cluster method", "author": ["Robin Sibson"], "venue": "The Computer Journal,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1973}, {"title": "Identification of common molecular subsequences", "author": ["T.F. Smith", "M.S. Waterman"], "venue": "Journal of molecular biology,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1981}, {"title": "Resc: An approach for real-time, dynamic agent tracking", "author": ["Milind Tambe", "Paul S Rosenbloom"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike Von Luxburg"], "venue": "Statistics and computing,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Beyond kmedoids: Sparse model based medoids algorithm for representative selection", "author": ["Yu Wang", "Sheng Tang", "Feidie Liang", "YaLin Zhang", "JinTao Li"], "venue": "In Advances in Multimedia Modeling,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P Xing", "Andrew Y Ng", "Michael I Jordan", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Survey of clustering algorithms", "author": ["Rui Xu", "Donald Wunsch"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Discovery of collocation patterns: from visual words to visual phrases", "author": ["Junsong Yuan", "Ying Wu", "Ming Yang"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}], "referenceMentions": [{"referenceID": 19, "context": "Several surveys of clustering techniques can be found in the literature [20,41].", "startOffset": 72, "endOffset": 79}, {"referenceID": 40, "context": "Several surveys of clustering techniques can be found in the literature [20,41].", "startOffset": 72, "endOffset": 79}, {"referenceID": 10, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 173, "endOffset": 177}, {"referenceID": 16, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 207, "endOffset": 213}, {"referenceID": 6, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 207, "endOffset": 213}, {"referenceID": 27, "context": "The idea of reducing a full set to a smaller set of representatives has been suggested before in specific contexts, such as clustering xml documents [11] or dataset editing [13], and more recently in visual [17,7] and text summarization [28].", "startOffset": 237, "endOffset": 241}, {"referenceID": 38, "context": "It has also been discussed as a general problem in [39].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "For instance, given a large document and a satisfactory measure of similarity between sentences, text summarization [25] could be framed as a representative selection task - obtain a subset of sentences that best captures the nature of the document.", "startOffset": 116, "endOffset": 120}, {"referenceID": 41, "context": "Similarly, one could map this problem to extracting \u201cvisual words\u201d or representative frames from visual input [42,26].", "startOffset": 110, "endOffset": 117}, {"referenceID": 25, "context": "Similarly, one could map this problem to extracting \u201cvisual words\u201d or representative frames from visual input [42,26].", "startOffset": 110, "endOffset": 117}, {"referenceID": 4, "context": "\u2013 Music analysis - the last decade has seen a rise in the computational analysis of music databases for music information retrieval [5], recommender systems [23] and computational musicology [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 22, "context": "\u2013 Music analysis - the last decade has seen a rise in the computational analysis of music databases for music information retrieval [5], recommender systems [23] and computational musicology [8].", "startOffset": 157, "endOffset": 161}, {"referenceID": 7, "context": "\u2013 Music analysis - the last decade has seen a rise in the computational analysis of music databases for music information retrieval [5], recommender systems [23] and computational musicology [8].", "startOffset": 191, "endOffset": 194}, {"referenceID": 2, "context": "\u2013 Team strategy/behavior analysis - opponent modeling has been discussed in several contexts, including game playing [3], real-time agent tracking [37] and general multiagent settings [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 36, "context": "\u2013 Team strategy/behavior analysis - opponent modeling has been discussed in several contexts, including game playing [3], real-time agent tracking [37] and general multiagent settings [4].", "startOffset": 147, "endOffset": 151}, {"referenceID": 3, "context": "\u2013 Team strategy/behavior analysis - opponent modeling has been discussed in several contexts, including game playing [3], real-time agent tracking [37] and general multiagent settings [4].", "startOffset": 184, "endOffset": 187}, {"referenceID": 23, "context": "This constraint renders many common clustering techniques inapplicable, including the canonical k-means [24], or more recent works such as [39].", "startOffset": 104, "endOffset": 108}, {"referenceID": 38, "context": "This constraint renders many common clustering techniques inapplicable, including the canonical k-means [24], or more recent works such as [39].", "startOffset": 139, "endOffset": 143}, {"referenceID": 31, "context": "Among such methods are neighbor-joining [32], which becomes unreliable when applied on non-metric data, or the k-prototypes algorithm [1].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "Among such methods are neighbor-joining [32], which becomes unreliable when applied on non-metric data, or the k-prototypes algorithm [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 34, "context": "Nevertheless, certain clustering algorithms still apply, such as the k-medoids algorithm, hierarchical clustering [35], and spectral clustering [38].", "startOffset": 114, "endOffset": 118}, {"referenceID": 37, "context": "Nevertheless, certain clustering algorithms still apply, such as the k-medoids algorithm, hierarchical clustering [35], and spectral clustering [38].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "The k-medoids algorithm [31] is a variation on the classic k-means algorithm that only selects centers from the original dataset, and is applicable to data organized as a pairwise distance matrix.", "startOffset": 24, "endOffset": 28}, {"referenceID": 39, "context": "3 In certain contexts, metric learning [40,10] can be applied, but current methods are not well suited for data without vector space representation, and in some sense, learning a metric is of lesser interest for representative selection, as we care less about classification or the structural relations latent in the data.", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "3 In certain contexts, metric learning [40,10] can be applied, but current methods are not well suited for data without vector space representation, and in some sense, learning a metric is of lesser interest for representative selection, as we care less about classification or the structural relations latent in the data.", "startOffset": 39, "endOffset": 46}, {"referenceID": 33, "context": "In the case of spectral clustering, an efficient algorithm that does not compute the full distance matrix exists [34], but it relies on a vector space representation of the data, rendering it inapplicable in our case.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The k-centers problem is defined as follows: Given a set S and a number k, find a subset R \u2282 S, |R| = k so that maxs\u2208Sd(s,R) is minimal [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 5, "context": "[6] 6 We note that no better approximation scheme is possible under standard complexity theoretic assumptions [18].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[6] 6 We note that no better approximation scheme is possible under standard complexity theoretic assumptions [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "Another algorithm that is related to this problem is Gonzales\u2019 approximation algorithm for minimizing the maximal cluster diameter [16], which iteratively takes out elements from existing clusters to generate new clusters based on the inter-cluster distance.", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Lastly, in a recent, strongly related paper [14], the authors consider a similar problem of selecting exemplars in data to speed up learning.", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "We compare ourselves to three alternative methods - k-medoids, the greedy k-center heuristic, and spectral clustering (using cluster medoids as representatives) [33], and show we outperform all three.", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "We note that although these methods weren\u2019t necessarily designed to tackle the representative selection problem, they, and clustering approaches in general, are used for such purposes in practice (see [17], for instance).", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "The need for good representative selection is driven by several tasks, including style characterization, comparing different musical corpora (see [12]), and music classification by composer, genre or period [2].", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "The need for good representative selection is driven by several tasks, including style characterization, comparing different musical corpora (see [12]), and music classification by composer, genre or period [2].", "startOffset": 207, "endOffset": 210}, {"referenceID": 8, "context": "For the purpose of this work we used the Music21 corpus, provided in MusicXML format [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 28, "context": "The melodic lines in the pieces are isolated and segmented using basic grouping principles adapted from [29].", "startOffset": 104, "endOffset": 108}, {"referenceID": 26, "context": "\u2013 Global alignment - the global alignment score between the two segments, calculated using the Needleman-Wunsch algorithm [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 35, "context": "\u2013 Local alignment - the local alignment score between the two segments, calculated using the Smith-Waterman algorithm [36].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "The robot world-cup soccer competition (RoboCup) is a wellestablished problem domain for AI in general [22].", "startOffset": 103, "endOffset": 107}], "year": 2015, "abstractText": "This paper considers the problem of representative selection: choosing a subset of data points from a dataset that best represents its overall set of elements. This subset needs to inherently reflect the type of information contained in the entire set, while minimizing redundancy. For such purposes, clustering may seem like a natural approach. However, existing clustering methods are not ideally suited for representative selection, especially when dealing with non-metric data, where only a pairwise similarity measure exists. In this paper we propose \u03b4medoids, a novel approach that can be viewed as an extension to the k-medoids algorithm and is specifically suited for sample representative selection from non-metric data. We empirically validate \u03b4-medoids in two domains, namely music analysis and motion analysis. We also show some theoretical bounds on the performance of \u03b4-medoids and the hardness of representative selection in general.", "creator": "LaTeX with hyperref package"}}}