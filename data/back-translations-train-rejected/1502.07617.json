{"id": "1502.07617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Online Learning with Feedback Graphs: Beyond Bandits", "abstract": "We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multi-armed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced $T$-round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with $\\widetilde\\Theta(\\alpha^{1/2} T^{1/2})$ minimax regret, where $\\alpha$ is the independence number of the underlying graph; the second class induces problems with $\\widetilde\\Theta(\\delta^{1/3}T^{2/3})$ minimax regret, where $\\delta$ is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time.", "histories": [["v1", "Thu, 26 Feb 2015 16:18:53 GMT  (33kb)", "http://arxiv.org/abs/1502.07617v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["noga alon", "nicol\\`o cesa-bianchi", "ofer dekel", "tomer koren"], "accepted": false, "id": "1502.07617"}, "pdf": {"name": "1502.07617.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Feedback Graphs: Beyond Bandits", "authors": ["Noga Alon", "Nicol\u00f2 Cesa-Bianchi", "Ofer Dekel", "Tomer Koren"], "emails": ["nogaa@post.tau.ac.il.", "bianchi@unimi.it.", "oferd@microsoft.com.", "tomerk@technion.ac.il."], "sections": [{"heading": null, "text": "ar Xiv: 150 2.07 617v 1 [cs.L G] 26 Fe \u0445 Tel Aviv University, Tel Aviv, Israel, and Microsoft Research, Herzliya, Israel, nogaa @ post.tau.ac.il. \u2020 Dipartimento di Informatica, Universita degli Studi di di Milano, Milan, Italy, nicolo.cesabianchi @ unimi.it. Parts of this work were done during the author's stay at Microsoft Research, Redmond, Washington; oferd @ microsoft.com. \u00a7 Technion - Israel Institute of Technology, Haifa, Israel, and Microsoft Research, Herzliya, Israel, tomerk @ technion.ac.il. Parts of this work were carried out during the author's stay at Microsoft Research, Redmond."}, {"heading": "1 Introduction", "text": "s performance is measured by the game theory idea of regret, which is the difference between his cumulative loss and the cumulative loss of the action player. It is about the difference between the cumulative losses and the cumulative loss of the best fixed action in Hindsight. We say that the player learns when his regret is after the rounds o (T).After choosing an action, the player observes some feedback that allows him to learn and improve his decisions in the following rounds. A variety of different feedback models are discussed in online learning."}, {"heading": "2 Problem Setting and Main Results", "text": "Let G = (V, E) be a directed feedback curve over the series of actions V = (1,.., K). Let N be in (i, j) = (j, i), (E) the environment of i in G, and let Nout (i) = {j, V: (i, j), (E) the environment of i in G. If I have a self-loop, it is (i, i), then i in (i) and i, Nout (i). Before the game begins, the environment privately selects a sequence of losses, namely 1, 2....., where the sequence t: V 7 \u2192 [0, 1] for each t more than 1. In each round, the player randomly selects an action. It inherits the loss of T and attracts the loss of T as a sequence."}, {"heading": "2.1 Main Results", "text": "The main result of this paper is a complete characterization of Minimax repentance when the feedback diagrams G = all this is fixed and known in relation to the player. Our characterization is based on various properties of G, which we define as: A), B), D (A), D (A), D (A), D (B), D (A), D (B), D (A), D (B), D (B), D (B), D (B), D), D (B), D), D (B), D), D (B), D (B), D (D), D (D), D (D), D (D), D (D), D (D), D (D), D (D), D (D), D (G)."}, {"heading": "3 The Exp3.G Algorithm", "text": "The upper limits for weak and highly observable graphs in Theorem 1 are both achieved by an algorithm we are introducing, called Exp3.G (see Algorithm 1), which is a variant of the Exp3-SET algorithm for undirected feedback graphs (Alon et al., 2013).Similar to Exp3 and Exp3.SET, our algorithm uses importance samples to construct unbiased loss estimates with controlled variance. In fact, Pt (i) = P (i) Nout (It)) is simply the probability of observing the loss of T (i) while playing. The following sampling of T (i) is an unbiased estimate of the true loss of T (i), and for all t and i (V) we have haveEt [t (i)] = observation methods loss of i."}, {"heading": "3.1 A Tight Bound for the Loopless Clique", "text": "One of the simplest examples of a feedback graph that is not self-aware is the loopless clique (fig. 1c). This graph is highly observable with an independence number of 1, so that theorem 2 guarantees that the regret of the algorithm 1 in the induced game is O (\u221a T ln (KT). In this case, however, we can do better than theorem 2 and prove (see appendix C) that the regret of the same algorithm is actually O (\u221a T lnK), which is the regret rate of the entire feedback game (fig. 1a). In other words, if we start with full feedback and then hide the player's own loss, the regret rate remains the same (up to constants). Theorem 3. For each sequence of loss functions, 1,... applies to the upper rate T, where the regret of the algorithm 1 (0, 1], with the cliopless feedback 2 for the feedback and N (feedback free)."}, {"heading": "3.2 Refined Second-order Bound for Hedge", "text": "Our analysis of Exp3.G builds on a new second rule of repentance that applies to the classic hedge fund algorithm. (Recall that Hedge (Freund and Schapire, 1997) operates on the full feedback rule (see Fig. 1a), in which the player has access to losses in due course. (i) for all s < t and i.V. Hedge funds trade on the basis of the pt distribution provided by \u0432 i, qt (i) = exp (\u2212 certaint \u2212 1s (i)))). (\u2212 V exp (\u2212 certaint)). (\u2212 T exp). (\u2212 T). (\u2212 T). (\u2212 T). (\u2212 T). (4). The following novel rule of repentance is the key to proving that our algorithm reaches narrow limits beyond regret (within logarithmic factors)."}, {"heading": "3.3 Proof of Theorem 2", "text": "We now turn to Theorem 2. For proof, we need the following graph-theoretical result, which is a variant of Alon et al. (2014, Lemma 16); for completeness, we assume a proof in Appendix A. Lemma 5. Let G = (V, E) be a directed diagram with (V) K, in which a positive weight is assigned to each node i (V). Let us suppose that (V) wi wi wi wi wi wi wi (V) wi \u2264 1, and that wi (V) for any constant 0 < (V) < 1 2. Then we will i (Vwi +) j j (N) n 4K (G) is the independence number of G.Proof Theorem 2. Without loss of generality, we can assume that K (2) we assume that the proof is guided by the application of Lemma 4 and the upper limit of the conditions of the second order."}, {"heading": "4 Lower Bounds", "text": "In this section, we point out lower limits for the minimax reue for unobservable and dimly observable graphs that we all know. < / p > p > p > p > p > p > p > p > p > p > p > p \"p > p > p > p > p > p > p > p > p > p > p > p > p > p > p\" p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p."}, {"heading": "5 Time-Varying Feedback Graphs", "text": "The setting discussed above can be generalized by changing the feedback diagrams arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Koca \u0301 k et al. (2014). The environment chooses a sequence of feedback diagrams G1,.., GT along with the sequence of loss functions. We consider two different variants of this setting: in this section we can discuss how our algorithm can be modified to deal with time-varying feedback graphs, and whether this generalization increases the minimax regret of the induced online learning problem. If G1,.,., GT are all highly observable, algorithm 1 and its analysis can be adapted to the circumstances of time."}, {"heading": "Acknowledgements", "text": "We would like to thank Se \ufffd bastien Bubeck for helpful discussions during various phases of this work and Ga \ufffd bor Barto \ufffd k for clarifying the connections to the observability of partial surveillance."}, {"heading": "A Additional Proofs", "text": "To prove our new remorse for Hedge, we must first prove the second order for these algorithms (1). (2). (2). (3). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4). (4).). (4). (4). (4). (4). (4). (4).). (4).). (4). (4). (4). (4). (4). (4).). (4). (4). (4).). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4). (4).). (4). (4). (4).). (4). (4.).). (4).). (4). (4).).)."}, {"heading": "B Proofs of Lower Bounds", "text": "\"We have to be prepared for such a loss to occur.\" \"We have to be prepared for such a loss to occur.\" \"We have to be prepared for such a loss to occur.\" \"We have to be prepared for such a loss to occur.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \".\" \"We.\" \"\" \"\" \".\" \"\" \"\" We. \"\" \"\". \"\" \"\". \"\" \"\".. \"\" \"\" \"\" \"..\" \"\" \"\" \"..\" \"\" \"\". \"\" \".\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \".\" \"\" \".\" \"\" \"\". \"\" \".\" \"\". \"\" \".\" \"\". \"\" \".\" \"\".. \"\" \"\". \"...\" \"\" \".\"... \"\" \"\".. \".\" \"\". \".\". \".\" \".\". \".\" \"\". \".\". \".\" \".\". \".\". \".\". \".\". \".\". \".\" \".\". \".\". \".\". \".\". \"\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\".. \".\". \".\". \".\". \".\". \".\" \".\". \".\". \".\". \".\". \".\". \"\". \".\". \".\". \".\". \".\". \"\". \".\". \"...\". \"....\". \"....\". \".\"... \".\". \"..\". \"."}, {"heading": "C Tight Bounds for the Loopless Clique", "text": "We repeat and prove theorem 3.Theorem 3. Theorem 3 (redefined). For each sequence of q-functions (redefined) we can Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-Qi-"}, {"heading": "D Connections to Partial Monitoring", "text": "The matrices L and H are both of size K \u00b7 M, where K is the number of actions of the player and M is the number of actions in the vicinity. (It) is the player who sets a sequence y1, y2,. of actions (i.e., matrix row index). (It, yt) the only feedback the player observes is the symbol H (It) the player who selects the action. (It) the action is chosen. (It, a matrix row index) is given by the matrix entry L (It, yt). (It, yt) the only feedback the player observes is the symbol H (It, yt). (It, yt) the column index yt and the loss value L (It, yt)."}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J.H. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer.,? \\Q2008\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2008}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Alon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2013}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour", "O. Shamir"], "venue": "CoRR, abs/1409.8428,", "citeRegEx": "Alon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2014}, {"title": "Toward a classification of finite partialmonitoring games", "author": ["A. Antos", "G. Bart\u00f3k", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "Antos et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2013}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Partial monitoring\u2014 classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D.P. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D. Helmbold", "R. Schapire", "M. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["T. Koc\u00e1k", "G. Neu", "M. Valko", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mannor and Shamir.,? \\Q2011\\E", "shortCiteRegEx": "Mannor and Shamir.", "year": 2011}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Aggregating strategies", "author": ["V. Vovk"], "venue": "In Proceedings of the 3rd Annual Workshop on Computational Learning Theory,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "Following the proof idea of Alon et al. (2013), let M = \u23082K/\u01eb\u2309 and introduce a discretization of the values", "author": ["G. Proof"], "venue": null, "citeRegEx": "Proof.,? \\Q2013\\E", "shortCiteRegEx": "Proof.", "year": 2013}, {"title": "Definitions 5 and 6) can be expressed as follows. Let L(i, \u00b7) be the column vector denoting the i-th row of L. Let also rowsp be the rowspace of a matrix and \u2295 be the cartesian product between linear spaces", "author": ["Bart\u00f3k"], "venue": null, "citeRegEx": "Bart\u00f3k,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Online learning can be formulated as a repeated game between a randomized player and an arbitrary, possibly adversarial, environment (see, e.g., Cesa-Bianchi and Lugosi, 2006; Shalev-Shwartz, 2011).", "startOffset": 133, "endOffset": 197}, {"referenceID": 7, "context": "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 66, "endOffset": 136}, {"referenceID": 11, "context": "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 66, "endOffset": 136}, {"referenceID": 14, "context": "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 66, "endOffset": 136}, {"referenceID": 4, "context": "Another common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose.", "startOffset": 49, "endOffset": 68}, {"referenceID": 4, "context": "Another common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose. In this model, the player\u2019s choices influence the feedback that he receives, so he has to balance an exploration-exploitation trade-off. On one hand, the player wants to exploit what he has learned from the previous rounds by choosing an action that is expected to have a small loss; on the other hand, he wants to explore by choosing an action that will give him the most informative feedback. The canonical example of online learning with bandit feedback is online advertising. Say that we operate an Internet website and we present one of K ads to each user that views the site. Our goal is to maximize the number of clicked ads and therefore we incur a unit loss whenever a user doesn\u2019t click on an ad. We know whether or not the user clicked on the ad we presented, but we don\u2019t know whether he would have clicked on any of the other ads. Full feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), where the feedback model is specified by a feedback graph.", "startOffset": 50, "endOffset": 1097}, {"referenceID": 5, "context": "Freund and Schapire (1997) proves that the minimax regret of the full feedback game is \u0398( \u221a T lnK) while Auer et al.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "Freund and Schapire (1997) proves that the minimax regret of the full feedback game is \u0398( \u221a T lnK) while Auer et al. (2002) proves that the minimax regret of the bandit feedback game is \u0398\u0303( \u221a KT ).", "startOffset": 105, "endOffset": 124}, {"referenceID": 1, "context": "The minimax regret rates induced by self-aware feedback graphs were extensively studied in Alon et al. (2014). In this paper, we focus on the intriguing situation that occurs when the feedback graph is missing some self-loops, namely, when the player does not always observe his own loss.", "startOffset": 91, "endOffset": 110}, {"referenceID": 1, "context": "The set of strongly observable feedback graphs includes the set of self-aware graphs, so this result extends the characterization given in Alon et al. (2014). The second category is the set of weakly observable feedback graphs, which induce learning problems whose minimax regret is \u0398\u0303(\u03b4T ), where \u03b4 is a new graph-dependent quantity called the weak domination number of the feedback graph.", "startOffset": 139, "endOffset": 158}, {"referenceID": 1, "context": "G (see Algorithm 1), which is a variant of the Exp3-SET algorithm for undirected feedback graphs (Alon et al., 2013).", "startOffset": 97, "endOffset": 116}, {"referenceID": 1, "context": ", strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Koc\u00e1k et al.", "startOffset": 73, "endOffset": 92}, {"referenceID": 1, "context": ", strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Koc\u00e1k et al. (2014). The tightness of our bounds in all cases is discussed in Section 4 below.", "startOffset": 73, "endOffset": 113}, {"referenceID": 9, "context": "Recall that Hedge (Freund and Schapire, 1997) operates in the full feedback setting (see Fig.", "startOffset": 18, "endOffset": 45}, {"referenceID": 1, "context": "While Alon et al. (2014) only consider the special case of graphs that have self-loops at all vertices, their lower bound applies to any strongly observable graph: we can simply add any missing self-loops to the graph, without changing its independence number \u03b1.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al.", "startOffset": 126, "endOffset": 151}, {"referenceID": 1, "context": "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Koc\u00e1k et al.", "startOffset": 152, "endOffset": 171}, {"referenceID": 1, "context": "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Koc\u00e1k et al. (2014)).", "startOffset": 152, "endOffset": 192}, {"referenceID": 10, "context": ", using a doubling trick, or an adaptive learning rate as in Koc\u00e1k et al. (2014)).", "startOffset": 61, "endOffset": 81}], "year": 2015, "abstractText": "We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multiarmed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced T -round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with \u0398\u0303(\u03b11/2T 1/2) minimax regret, where \u03b1 is the independence number of the underlying graph; the second class induces problems with \u0398\u0303(\u03b41/3T 2/3) minimax regret, where \u03b4 is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time. Tel Aviv University, Tel Aviv, Israel, and Microsoft Research, Herzliya, Israel, nogaa@post.tau.ac.il. Dipartimento di Informatica, Universit\u00e0 degli Studi di Milano, Milan, Italy, nicolo.cesabianchi@unimi.it. Parts of this work were done while the author was at Microsoft Research, Redmond. Microsoft Research, Redmond, Washington; oferd@microsoft.com. Technion\u2014Israel Institute of Technology, Haifa, Israel, and Microsoft Research, Herzliya, Israel, tomerk@technion.ac.il. Parts of this work were done while the author was at Microsoft Research, Redmond.", "creator": "LaTeX with hyperref package"}}}