{"id": "1502.07776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Efficient Geometric-based Computation of the String Subsequence Kernel", "abstract": "Kernel methods are powerful tools in machine learning. They have to be computationally efficient. In this paper, we present a novel Geometric-based approach to compute efficiently the string subsequence kernel (SSK). Our main idea is that the SSK computation reduces to range query problem. We started by the construction of a match list $L(s,t)=\\{(i,j):s_{i}=t_{j}\\}$ where $s$ and $t$ are the strings to be compared; such match list contains only the required data that contribute to the result. To compute efficiently the SSK, we extended the layered range tree data structure to a layered range sum tree, a range-aggregation data structure. The whole process takes $ O(p|L|\\log|L|)$ time and $O(|L|\\log|L|)$ space, where $|L|$ is the size of the match list and $p$ is the length of the SSK. We present empiric evaluations of our approach against the dynamic and the sparse programming approaches both on synthetically generated data and on newswire article data. Such experiments show the efficiency of our approach for large alphabet size except for very short strings. Moreover, compared to the sparse dynamic approach, the proposed approach outperforms absolutely for long strings.", "histories": [["v1", "Thu, 26 Feb 2015 22:12:22 GMT  (475kb,D)", "http://arxiv.org/abs/1502.07776v1", "24 pages, 11 figures"]], "COMMENTS": "24 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CG", "authors": ["slimane bellaouar", "hadda cherroun", "djelloul ziadi"], "accepted": false, "id": "1502.07776"}, "pdf": {"name": "1502.07776.pdf", "metadata": {"source": "CRF", "title": "Efficient Geometric-based Computation of the String Subsequence Kernel", "authors": ["Slimane Bellaouar", "Hadda Cherroun", "Djelloul Ziadi"], "emails": ["cherroun}@mail.lagh-univ.dz", "djelloul.ziadi@univ-rouen.fr"], "sections": [{"heading": null, "text": "Keywords: string subsequence core, calculation geometry, layer domain tree, domain query, domain sum"}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them will be able to move to another world in which they are able to find themselves."}, {"heading": "2 Preliminaries", "text": "First we deal with concepts of string, substring, subsequence and kernel. Afterwards we present the structure of the layer area structures."}, {"heading": "2.1 String", "text": "A string s = s1... s | s | is a finite sequence of symbols of length | s |, where si denotes the ith element of s. The symbol denotes the empty string. We use \u0435n to denote the set of all finite strings of length n, and \u03a3 \u0445 the set of all strings. The notation [s = t] is a Boolean function that {1 if s and t are identical; otherwise 0."}, {"heading": "2.2 Substring", "text": "For 1 \u2264 i \u2264 j \u2264 | s |, the string s (i: j) denotes the substring sisi + 1... sj of s. Accordingly, a string t is a substring of a string s, if there are strings u and v, so that s = utv (u and v can be empty). The substrings of the length n are denoted as n-gram (or n-mer)."}, {"heading": "2.3 Subsequence", "text": "The string t is a sub-sequence of s if there is an increasing sequence of indices I = (i1,..., i | t |) in s, (1 \u2264 i1 <... < i | t | \u2264 | s |), so that tj = sij, for j = 1,..., | t |. In literature we use t = s (I) if t is a sub-sequence of s in the positions indicated by I. The empty string is indexed by the empty tuple. The absolute value | t | denotes the length of the sub-sequence t, which is the number of indices | I |, while l (I) = i | t | \u2212 i1 + 1 refers to the number of characters of s covered by the sub-sequence t."}, {"heading": "2.4 Kernel methods", "text": "Traditional machine learning and statistical algorithms focus on linearly separable problems (i.e. the recognition of linear relationships between data), which is the case when data can be represented by a single line of table. However, real data analysis requires nonlinear methods. In this case, the target concept cannot be expressed as simple linear combinations of the given attributes [4]. This was proposed in 1960 by Minsky and Papert.Kernel methods were proposed as a solution by embedding the data in a high-dimensional attribute space where linear learning machines based on algebra, geometry, and statistics can be applied. This embedding is called kernel. It arises as a measure of similarity (inner product) in a high-dimensional space, the so-called feature description. The trick is to be able to calculate this inner product directly from the original data space using the core function. This can be formally clarified as follows: The core function K corresponds to the inner product in a feature space over a map F."}, {"heading": "2.5 Layered Range Tree", "text": "It is reasonable to describe a two-dimensional area tree in order to understand the LRT. Consider a set of S of points in R2. A range tree is primarily a balanced two-dimensional search tree (BBST), which is based on the x-coordinate of the points of S. Data is stored only in the leaves. Each node in the BBST is extended by an associated data structure (Tassoc (v))), which is a one-dimensional area tree, it can be a BBST or a sorted array, a canonical subset P (v) on y coordinates. Subset P (v) is the one rooted in the leaves of the subtree. Figure 1 shows a two-dimensional area for a set of points S = {(2, 2), (3, 3), (4, 3), 2), 4 (4), 4 (4), 4 (4), 4 (4), 5)."}, {"heading": "3 String Subsequence Kernels", "text": "The philosophy of all string approaches can be reduced in different ways to count common substrings or subsequences that occur in the two strings in order to compare. This philosophy manifests itself in two steps: - Project the strings via an alphabet that refers to a high dimensional vector space F, where the main idea is indexed by a subset of the input space. - Calculate the distance (inner product) between strings in F. This distance reflects their similarity. However, a new weighting method is applied that reflects the degree of consistency of the sequence in the string. To measure the distance of non-continuous elements of the sequence, a gap in the subsequence is introduced."}, {"heading": "3.1 Naive Implementation", "text": "The calculation of the similarity of two strings (sa and tb) depends on their final symbols. In the case of a = b, we must conceive a recursion of all the prefixes of s and t: KSp (sa, tb) = [a = b] | s | \u2211 i = 1 | t | \u2211 j = 1 \u03bb2 + | s | \u2212 i + | t | \u2212 jKSp \u2212 1 (s (1: i), t (1: j)). (2) This calculation results in a complexity of O (p (| s | 2 | t | 2))."}, {"heading": "3.2 Efficient Implementations", "text": "We present three methods that calculate the SSK efficiently, namely dynamic programming [6], trie-based [5,7,8] and sparse dynamic programming [7]. To describe such approaches, we use two strings s = gatta and t = cata as a running example DP = q-1. The starting point of the dynamic programming approach is the suffix recursion given by Equation (2). From this equation, we can consider a separate dynamic programming table DPp for storing the double sum: DPp (k, l) = k-based on dynamic programming i = 1 l = 1 l-based recursion."}, {"heading": "4 Geometric based Approach", "text": "With a view to improving the complexity of SSK, our approach is based on two observations. The first concerns the calculation of KSp (s, t), which is only required if we have created a list of index pairs of these entries, rather than the entire suffix table, L (s, t) as a parameter indicating the size of the input data. The complexity of naively implementing the list version is O (p), and it does not seem obvious to calculate KSp (s, t) efficiently on a list of data structures."}, {"heading": "5 Experimentation", "text": "In this section, we describe the experiments that focus on the evaluation of our geometric algorithm against the dynamic and sparse dynamic algorithms, which are described as geometric, dynamic, and economical, respectively. We discarded the drive-based algorithm from this comparison because, on the one hand, it is an approximate algorithm, and, on the other hand, it was significantly slower than Dynamic and economical in the preliminary tests carried out in [7].In order to benefit from the empirical evaluation carried out in [7], we tried to maintain the same conditions of their experiments, and for this reason, we carried out a series of experiments on both synthetically generated and newswire article data for Reuters news.We have extended the tests on Intel Core i7 with 2.40 GHZ processor with 16 GB of RAM under Windows 8.1 64 bit. We have implemented all the tested algorithms in Java. For the LRST implementation, we have extended the LRT implementation page / httthony."}, {"heading": "5.1 Experiments with synthetic data", "text": "These experiments concern the effects of string length and alphabet size on the efficiency of the different approaches and to determine under which conditions our approach performs better. To simplify string generation, we considered string symbols as integers in [1, alphabet size]. For the convenience of data visualization, we used the logarithmic scale on all axes. To perform precise experiments, we generated several pairs for the same string length and alphabet size, and for each pair took several measurements for the runtime with a sequence length of p = 10 and a decay parameter of p = 0.5. As a result, Fig. 7 for our geometric approaches shows an inverse dependence on runtime with alphabet size."}, {"heading": "5.2 Experiments with newswire article data", "text": "We have created a dataset that is presented as sequences of syllables by transferring all XML articles to text documents, after which the text documents are pre-processed by removing stopwords, punctuation marks, special symbols, and finally word syllabifications. We have created 22260 different syllables with narrow lengths. As in the first experiment, an integer is assigned to each syllable case. To randomize the documents, we have mixed these preliminary datasets while ensuring that their lengths are close. On this condition, we have collected 916 pairs of documents as final datasets. To compare the candidate algorithms, we have calculated the SSK for each document pair of the datasets by assigning the sub-sequence lengths from 2 to 20."}, {"heading": "5.3 Discussion of the experiment results", "text": "In line with the results of the two experiments, it is easy to see that the algorithms behave essentially the same way, both on synthetically generated data and on newswire article data. These results show that our approach performs better on the size of the large alphabet with the exception of very small strings. In terms of sparse, Geometric can be competitive on long strings. We can argue this as follows: Firstly, the size of the alphabet and the length of the strings significantly influence the shape of the core matrix. Large alphabets can potentially reduce partial matching sub-sequences, especially on long strings, resulting in a sparse form of the matrix. Consequently, a large amount of data stored in the core matrix does not contribute to the result. In other cases, our approach to the dense matrix may be worse than dynamic on most Log | L | factor.On the other hand, the complexities of the geometric and sparse data differ only by the factors Log | L | Log and Log | Log | Log."}, {"heading": "6 Conclusions and further work", "text": "We introduced a novel algorithm that efficiently calculates the String Subsequence Kernel (SSK). Our approach is refined in two phases. We started by constructing a match list L (s, t) that contains only the information that contributes to the result, then calculated the sum within a range for the match list, and we expanded the initial size of the match tree to be a layer sum tree. O (p | L | log | L |) takes time and the space in which p is the length of the SSK, and | L | is the initial size of the match tree. The result shows an asymptotic improvement in complexity compared to the naive implementation of the list version O."}, {"heading": "1. Bellaouar, S., Cherroun, H., Ziadi, D.: Efficient list-based computation of the string", "text": "In: LATA '14. pp. 138-148 (2014) 2. Berg, M.d., Cheong, O., Kreveld, M.v., Overmars, M.: Computational Geometry: Algorithms and Applications. Springer-Verlag TELOS, Santa Clara, CA, USA, 3rd edn. (2008) 3. Chazelle, B., Guibas, L.J.: Fractional Cascading: I. a data structuring technique. Algorithmica 1 (2), 133-162 (1986) 4. Cristianini, N., Shawe-Taylor, J.: An introduction to support Vector Machines: and other kernel-based learning methods. Cambridge Mar Press, New York, NY, USA (2000) 5. Leslie, C., Eskin, E., Noble, W.: Mismatch String Kernels for SVM Protein Classification."}], "references": [{"title": "Efficient list-based computation of the string subsequence kernel", "author": ["S. Bellaouar", "H. Cherroun", "D. Ziadi"], "venue": "LATA\u201914. pp. 138\u2013148", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational Geometry: Algorithms and Applications", "author": ["Berg", "M.d.", "O. Cheong", "Kreveld", "M.v.", "M. Overmars"], "venue": "Springer-Verlag TELOS, Santa Clara, CA, USA, 3rd ed. edn.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Fractional cascading: I", "author": ["B. Chazelle", "L.J. Guibas"], "venue": "a data structuring technique. Algorithmica 1(2), 133\u2013162", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "An introduction to support Vector Machines: and other kernel-based learning methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Mismatch String Kernels for SVM Protein Classification", "author": ["C. Leslie", "E. Eskin", "W. Noble"], "venue": "Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "J. Mach. Learn. Res", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Efficient computation of gapped substring kernels on large alphabets", "author": ["J. Rousu", "J. Shawe-Taylor"], "venue": "J. Mach. Learn. Res", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "Kernel methods [4] offer an alternative solution to the limitation of traditional machine learning algorithms, applied solely on linear separable problems.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "[6] apply dynamic programming paradigm to the suffix version of the SSK.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Later, Rousu and Shawe-Taylor [7] propose an improvement to the dynamic programming approach based on the observation that most entries of the dynamic programming matrix (DP) do not really contribute to the result.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Beyond the dynamic programming paradigm, the trie-based approach [5,7,8] is based on depth first traversal on an implicit trie data structure.", "startOffset": 65, "endOffset": 72}, {"referenceID": 6, "context": "Beyond the dynamic programming paradigm, the trie-based approach [5,7,8] is based on depth first traversal on an implicit trie data structure.", "startOffset": 65, "endOffset": 72}, {"referenceID": 7, "context": "Beyond the dynamic programming paradigm, the trie-based approach [5,7,8] is based on depth first traversal on an implicit trie data structure.", "startOffset": 65, "endOffset": 72}, {"referenceID": 3, "context": "In this case, the target concept cannot be expressed as simple linear combinations of the given attributes [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "It consists to replace the real number by a composite-number space [2].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "The application of the fractional cascading technique introduced by [3] on a range tree creates a new data structure so called layered range tree.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "For the String Subsequence Kernel (SSK) [6], the main idea is to compare strings depending on common subsequences they contain.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 110, "endOffset": 117}, {"referenceID": 6, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 110, "endOffset": 117}, {"referenceID": 7, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 110, "endOffset": 117}, {"referenceID": 6, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "For instance the (p,m)-mismatch string kernel [5] and restricted SSK [8].", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "For instance the (p,m)-mismatch string kernel [5] and restricted SSK [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "In the present section, we try to describe a trie-based SSK presented in [7] that slightly differ from those cited above [5,8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "In the present section, we try to describe a trie-based SSK presented in [7] that slightly differ from those cited above [5,8].", "startOffset": 121, "endOffset": 126}, {"referenceID": 7, "context": "In the present section, we try to describe a trie-based SSK presented in [7] that slightly differ from those cited above [5,8].", "startOffset": 121, "endOffset": 126}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 K(1 : p)\u2190 0 /* Computation of K1(s, t) */ 4 for i = 1:m do 5 for j = 1:n do 6 if s[i] = t[j] then 7 KPS[i, j]\u2190 \u03bb2 8 K[1]\u2190 K[1] +KPS[i, j]", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 K(1 : p)\u2190 0 /* Computation of K1(s, t) */ 4 for i = 1:m do 5 for j = 1:n do 6 if s[i] = t[j] then 7 KPS[i, j]\u2190 \u03bb2 8 K[1]\u2190 K[1] +KPS[i, j]", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "Rousu and Shawe-Taylor [7] have proposed a solution using the sparse dynamic programming technique to avoid unnecessary computations.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "In this respect, we have used a layered range tree (LRT) in [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 Creation of the initial match list L /* Computation of K1(s, t) */ 4 foreach ((i, j), v) \u2208 L do 5 K[1]\u2190 K[1] + v \u00b7 \u03bbi+j", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 Creation of the initial match list L /* Computation of K1(s, t) */ 4 foreach ((i, j), v) \u2208 L do 5 K[1]\u2190 K[1] + v \u00b7 \u03bbi+j", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "This trick is inspired from [7] to make the range sum results correct.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "We have discarded the trie-based algorithm from this comparison because it is an approximate algorithm on the one hand, on the other hand in the preliminary experiments conducted in [7] it was significantly slower than Dynamic and Sparse.", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "To benefit from the empiric evaluation conducted in [7], we tried to keep the same conditions of their experiments.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "Rousu and Shawe-Taylor [7] state that with long strings based on large alphabets their approach is faster.", "startOffset": 23, "endOffset": 26}], "year": 2015, "abstractText": "Kernel methods are powerful tools in machine learning. They have to be computationally efficient. In this paper, we present a novel Geometric-based approach to compute efficiently the string subsequence kernel (SSK). Our main idea is that the SSK computation reduces to range query problem. We started by the construction of a match list L(s, t) = {(i, j) : si = tj} where s and t are the strings to be compared; such match list contains only the required data that contribute to the result. To compute efficiently the SSK, we extended the layered range tree data structure to a layered range sum tree, a range-aggregation data structure. The whole process takes O(p|L| log |L|) time and O(|L| log |L|) space, where |L| is the size of the match list and p is the length of the SSK. We present empiric evaluations of our approach against the dynamic and the sparse programming approaches both on synthetically generated data and on newswire article data. Such experiments show the efficiency of our approach for large alphabet size except for very short strings. Moreover, compared to the sparse dynamic approach, the proposed approach outperforms absolutely for long strings.", "creator": "LaTeX with hyperref package"}}}