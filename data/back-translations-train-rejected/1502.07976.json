{"id": "1502.07976", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2015", "title": "Error-Correcting Factorization", "abstract": "Error Correcting Output Codes (ECOC) is a successful technique in multi-class classification, which is a core problem in Pattern Recognition and Machine Learning. A major advantage of ECOC over other methods is that the multi- class problem is decoupled into a set of binary problems that are solved independently. However, literature defines a general error-correcting capability for ECOCs without analyzing how it distributes among classes, hindering a deeper analysis of pair-wise error-correction. To address these limitations this paper proposes an Error-Correcting Factorization (ECF) method, our contribution is three fold: (I) We propose a novel representation of the error-correction capability, called the design matrix, that enables us to build an ECOC on the basis of allocating correction to pairs of classes. (II) We derive the optimal code length of an ECOC using rank properties of the design matrix. (III) ECF is formulated as a discrete optimization problem, and a relaxed solution is found using an efficient constrained block coordinate descent approach. (IV) Enabled by the flexibility introduced with the design matrix we propose to allocate the error-correction on classes that are prone to confusion. Experimental results in several databases show that when allocating the error-correction to confusable classes ECF outperforms state-of-the-art approaches.", "histories": [["v1", "Fri, 27 Feb 2015 17:22:53 GMT  (5873kb,D)", "https://arxiv.org/abs/1502.07976v1", "Under review at TPAMI"], ["v2", "Thu, 5 Mar 2015 17:49:16 GMT  (5878kb,D)", "http://arxiv.org/abs/1502.07976v2", "Under review at TPAMI"]], "COMMENTS": "Under review at TPAMI", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["miguel angel bautista", "oriol pujol", "fernando de la torre", "sergio escalera"], "accepted": false, "id": "1502.07976"}, "pdf": {"name": "1502.07976.pdf", "metadata": {"source": "CRF", "title": "Error-Correcting Factorization", "authors": ["Miguel Angel Bautista", "Oriol Pujol", "Fernando De la Torre", "Sergio Escalera"], "emails": ["pujol}@ub.edu", "ftorre@cs.cmu.edu"], "sections": [{"heading": null, "text": "Index terms - error correction of output codes, multi-level learning, matrix factorization"}, {"heading": "1 INTRODUCTION", "text": "In the last decade, we have achieved an exponential growth rate based on data analysis. In the second decade, we are dealing with an explosion in data availability. In the third decade, we are dealing with a doubling of the amount of data. In the third decade, we are dealing with a doubling of the amount of data. In the fourth decade, we are dealing with a doubling of the amount of data."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Single-machine/Single-loss Approaches", "text": "The multi-class problem can be directly addressed by some methods that exhibit multi-class behavior off-the-shelf (i.e. nearest neighbors [22], decision trees [30], random forests [6]). However, some of the most powerful methods for binary classifications such as support vector machines (SVM) or adaptive boosting (AdaBoost) cannot be directly extended to the multi-class case, and further development is needed. In this sense, the literature is fertile for single-class strategies to estimate f. One of the most well-known approaches is the extensions of SVMs [7] to the multi-class case, and the work of Weston and Watkins [55] represents a unique extension of the SVM method to handle the multi-class case in which k-dictator functions associated with k-1 are trained."}, {"heading": "2.2 Divide and Conquer Approaches", "text": "On the other hand, the Dividend and Conquest approach has attracted a lot of attention due to its excellent results and the easy parallelization architecture [48], [52], [18], [46], [40], [28]. However, instead of developing a method to deal with the multi-class case, we decouple and conquer approaches into a number of l binary problems that are treated separately. Once the answers of the binary classifiers are received, a reject strategy is used to find the final issue. In this trend, one can find three main lines of research: flat strategies, hierarchical classification and ECOC. Flat strategies such as one against one [52] and one against all [48] are those that use a predefined problem distribution scheme, followed by a reject strategy to aggregate the binary classifications."}, {"heading": "2.3 Problem-dependent Strategies", "text": "Alternatively, problem-dependent strategies for ECOC have proven successful in the case of multi-level classification problems [57], [23], [58], [60], [59], [46]. A common trend of this work is the use of information from the multi-level data distribution obtained a priori to design a decomposition into easily separable binary problems. In this sense, [57] constitutes a spectral decomposition of the graph laptop computer associated with the multi-level problem. The most likely, most separable partitions correspond to the thresholds of the eigenvectors of the laptop computer. In this sense, however, this approach offers no guarantees for the definition of unique code words (which is a core property of the ECOC coding framework) or the achievement of a suitable code length. In [24] Gao and Koller propose a method that separates the individual categories by making a clear coding framework between the classes (which results in a coding characteristic of the code C or an optimal one)."}, {"heading": "2.4 Our approach", "text": "In this paper, we present the Error-Correcting Factorization (ECF) method for factorizing a design matrix of the desired \"error correction\" between classes into a discrete ECOC matrix. The proposed ECF method is a general framework for ECOC coding, since the design matrix is a flexible tool for error correction analysis. In this sense, the problem of designing the ECOC matrix is reduced to the definition of the design matrix, with the most recent state-of-the-art work allowing a design matrix to be constructed according to a \"hard class\" that falls short of the spirit, showing the limitations of the easily separable classes and disregarding the classes that are not easily separable."}, {"heading": "3 METHODOLOGY", "text": "In this section, we review the existing characteristics of the ECOC framework and propose to consider the ECOC coding matrix optimization as a matrix factorization problem that can be efficiently solved with an approach of limited coordinate parentage."}, {"heading": "3.1 Error-Correcting Output Codes", "text": "It is a multi-class system, inspired by the error correction principles of communication theory [16], which consists of two different steps: encoding [16], [2] and decoding [17], [61]. At the encoding step of an ECOC encoding matrix X (1, + 1) k \u00b7 l is constructed, where k denotes the number of classes in the problem and l the number of bi-partitions (also known as dichotomies) are learned. In the encoding matrix, the rows (also known as codios) are clearly defined, as these are the identifiers of each category in the multi-class problem. The columns of the X (xj's) denote the bi-partitions learned by base classifiers (also known as codichotomizers)."}, {"heading": "3.2 Good practices in ECOC", "text": "Several papers have examined the properties of a good ECOC coding matrix [16], [36], [3], [57], [4] which can be summarized in the following three properties: 1) Correctability: If we have H-Rk-k denote a symmetrical matrix of hammer spacing between all line pairs in X, the correctability is expressed as bmin (H) -12 c 2, with only off-diagonal values of H. In this sense, the induced binary problems should be as uncorrelated as possible so that X can restore binary classification errors. 3) Use of powerful binary classifiers: Since the final class prediction consists of the aggregation of bit predictors, accurate binary classifiers are needed to obtain multi-precise class predictions."}, {"heading": "3.3 From global to pair-wise correction capability", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "3.4 Error-Correcting Factorization", "text": "This section describes the objective function and optimization strategy of the ECF algorithm."}, {"heading": "3.4.1 Objective", "text": "Our goal is to find an ECOC coding matrix that encodes the properties designated by the design matrix D. (In this sense, ECF seeks a factorization of the design matrix D-Rk \u00b7 k into a discrete ECOC matrix X. This factorization is formulated as the square form XX >, which reconstructs D with minimal Frobenius distance under several constraints, such as inEquation (1) \u2212 X \u2212 X > 2F (1) provided that X {\u2212 1, + 1} k \u00b7 l (2) XX > P \u2264 0 (3) X > X \u2212 X \u2212 X \u2212 X \u2212 1 (l) \u2212 X \u2212 X \u2212 X > X \u2212 X \u2212 X \u2212 X (5). The component X \u00b2 k \u00b7 l that solves this optimization problem that solves this optimization problem is the inner product of discrete vectors closest to D under the Frobenius norm."}, {"heading": "3.4.2 Optimization", "text": "In this section, we detail the process of optimizing X. The minimization problem posed in Equation (1) with the relaxation of the Boolean constraint in Equation (2) is nothing more than a global minimum. In this sense, although gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient gradient grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade grade gra"}, {"heading": "3.5 Connections to Singular Value Decomposition, Nearest Correlation Matrix and Discrete Basis problems", "text": "Similar objective functions as defined in the ECF problem in Equation (1) are also found in other contexts, for example in the Singular Value Decomposition Problem (SVD). SVD uses the same objective function as ECF, which is subject to the constraint XX > = I. However, the solution of the SVD results in an orthogonal basis and does not match the goal defined in Equation (1), which ensures different correlations between the xi. Furthermore, we can also find common ground with the Nearest Correlation Matrix (NMC) problem [32], [9], [39]. However, the NMC solution does not provide a discrete factor X, but searches directly for the Gramian XX >, where X is not discrete, as in Equation (12). Minimize X-X-D-2F (12), are subject to X 0 (13) cXc > = b (14). Furthermore, the ECF has similarities to the base DX-BP (X-BP) factor {42) (D)."}, {"heading": "4 DISCUSSION", "text": "In this section, we discuss how to ensure that the design matrix D is valid, how to automatically calculate the code length for each given problem, and how to analyze the convergence of ECF in terms of the order in which the coordinates are updated. Finally, we show that ECF converges under certain conditions to a solution with an almost negligible objective value."}, {"heading": "4.1 Ensuring a representable design matrix", "text": "An alternative interpretation for ECF is that it is a discrete matrix X whose gram D comes closest to the Frobenius standard. However, since D can be set directly by the user, we must guarantee that D is a correlation matrix that can be realized in the Rk \u00b7 k space, i.e. D must be symmetrical and positively semi-defined. In particular, we would like to find the correlation matrix D-Rk \u00b7 k that comes closest to D under the Frobenius standard. This problem has been addressed in several papers [32], [9], [27], leading to various algorithms that often use an alternating projection approach. In this particular case, however, in addition to the Positive Semidefinitive (PSD) cone and symmetrically, we require D to be scaled in the [\u2212 l, + l] range in which D \u2212 l is converted."}, {"heading": "4.2 Defining a code length with representation guarantees", "text": "Defining a problem-dependent ECOC code length l, that is, selecting the number of binary partitions for a given multi-class task, is a problem that has been overlooked in the literature. For example, predefined coding designs such as One vs. All or One vs. One with fixed code length are proposed. On the other hand, coding designs such as Dense or Sparse Random codes (which are very often used in experimental comparisons), [58], [18], are proposed to have a code length of d10log2 (k) e and d15log2 (k) e. These values are arbitrary and unwarranted, in order to build a dense or sparse random ECOC matrix, one must generate a set of 1000 matrices and choose the one that maximizes the minimum (H). Consider the Dense Random Coding class, the length l = d10 log2 (k) e, the ECOMatrix class."}, {"heading": "4.3 Order of Coordinate Updates", "text": "The coordinate deviation has been applied to a wide range of problems in order to achieve satisfactory results, but the problem of coordinate selection, which is to be minimized with each iteration, remains active [47], [21], [53], [33]. In particular, [44] derives a convergence rate that is faster when coordinates are uniformly randomly and not cyclically selected. Therefore, randomization is an appropriate choice if the problem has some of the following characteristics [47]: \u2022 Not all data are available at all times. \u2022 A randomized strategy is capable of avoiding an unfavorable coordinate order, and therefore it may be preferable. \u2022 Recent efforts suggest that randomization can improve the convergence rate [44]. However, the structure of the ECF is different and requires a different analysis. In particular, we indicate the following points. (i) Each coordinate update of the ECF has information about the rest of the coordinate sequence."}, {"heading": "4.4 Approximation Errors and Convergence results when", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D is an inner product of binary data", "text": "The optimization problem that ECF poses in Equation (1) is not convex due to the square term XX > DB (DB), even if the discrete constraint is loosened. This means that we cannot guarantee that the algorithm will approach the global optima. Remember that ECF is looking for the term XX >, which comes closest to D under the Frobenius standard. In this sense, we are introducing DB, which is the matrix of internal products of discrete vectors that D comes closest under the Frobenius standard. \u2212 D-2F-D error is the local optimal point to which ECF-X error is converted. In this sense, we are introducing DB, which is the matrix matrix of internal products of discrete vectors that D comes closest under the Frobenius standard. \u2212 D-D error is the local optimal point to which ECF-X is the following equation: \"optimal X-D-X to local error.\""}, {"heading": "5 EXPERIMENTS", "text": "In this section, we present the experimental results of the proposed Error-Correcting Factorization method, starting with the data, methods and settings."}, {"heading": "5.1 Data", "text": "The proposed method of factorization error correction was applied to a total of 8 sets of data. To allow for in-depth analysis and understanding of the method, we synthetically created a toy problem consisting of k = 14 classes, with each class containing 100 two-dimensional dots sampled from a Gaussian distribution with the same standard deviation but different means. Figure 6 (d) shows the synthetic multi-class data, where each color corresponds to a different category. We selected 5 known UCI data sets: Glass, Segmentation, Ecoli, Yeast and Vowel, which differ in their complexity and number of classes. Finally, we apply the classification methodology to two challenging computer vision problems. First, we test the methods in a real traffic sign categorization problem consisting of 36 traffic sign classes. Second, 50 classes from ARFaces [41] data sets with the present methodology."}, {"heading": "5.2 Methods and settings", "text": "We compared the proposed Error-Correcting Factorization Method with the standard predefined One vs. All (OVA) and One vs. One (OVO) approaches [48], [52]. In addition, we present two random designs for ECOC matrices. In the first case, we generate random ECOC coding matrices that set the general correction capability to a certain value (RAND). In the second case, we generate a dense random coding matrix [3] (DENSE). These comparisons allow us to analyze the effects of the reorganization of the inter-class correction capabilities of an ECOC matrix (RAND). Finally, to compare our proposal with state-of-the-art methods, we also used the Spectral ECOC (SECOC) method [57] and the Relaxed Hierarchy [23] (R-H). Finally, we propose two different flavors of the ECF-Class E, ECF-H and ECF-H"}, {"heading": "5.3 Experimental Results", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6 CONCLUSIONS", "text": "We introduced the Error-Correcting Factorization method for multi-level learning based on the Error-Correcting Output Codes. The proposed method factorizes a design matrix of desired correction properties into a discrete error component. ECF is a general method for building a multi-level classification that can be done either directly or indirectly."}], "references": [{"title": "Generalized non-metric multidimensional scaling", "author": ["Sameer Agarwal", "Josh Wills", "Lawrence Cayton", "Gert Lanckriet", "David J Kriegman", "Serge Belongie"], "venue": "In ICAIS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E. Allwein", "R. Schapire", "Y. Singer"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["Erin L. Allwein", "Robert E. Schapire", "Yoram Singer", "Pack Kaelbling"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "On the design of an ecoc-compliant genetic algorithm", "author": ["Miguel \u00c1ngel Bautista", "Sergio Escalera", "Xavier Bar", "Oriol Pujol"], "venue": "Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Introducing the separability matrix for error correcting output codes coding", "author": ["Miguel Bautista", "Oriol Pujol", "Xavier Bar\u00f3", "Sergio Escalera"], "venue": "MCS, pages 227\u2013236,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Image classification using random forests and ferns", "author": ["Anna Bosch", "Andrew Zisserman", "Xavier Muoz"], "venue": "ICCV", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard E Boser", "Isabelle M Guyon", "Vladimir N Vapnik"], "venue": "In COLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Least-squares covariance matrix adjustment", "author": ["Stephen Boyd", "Lin Xiao"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "On the accuracy and performance of the GeoMobil system", "author": ["J. Casacuberta", "J. Miranda", "M. Pla", "S. Sanchez", "A.Serra", "J.Talaya"], "venue": "In International Society for Photogrammetry and Remote Sensing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Robust euclidean embedding", "author": ["Lawrence Cayton", "Sanjoy Dasgupta"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Improved output coding for classification using continuous relaxation", "author": ["Koby Crammer", "Yoram Singer"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "A least-squares framework for component analysis", "author": ["Fernando De la Torre"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T. Dietterich", "G. Bakiri"], "venue": "In Journal of Artificial Intelligence Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "On the decoding process in ternary error-correcting output codes", "author": ["S. Escalera", "O. Pujol", "P.Radeva"], "venue": "Transactions in Pattern Analysis and Machine Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Subclass problem-dependent design for errorcorrecting output codes", "author": ["Sergio Escalera", "David MJ Tax", "Oriol Pujol", "Petia Radeva", "Robert PW Duin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Describing objects by their attributes", "author": ["Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In COLT,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["Jerome Friedman", "Trevor Hastie", "Rob Tibshirani"], "venue": "Journal of statistical software,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "An optimal global nearest neighbor metric", "author": ["Keinosuke Fukunaga", "Thomas E Flick"], "venue": "Pattern Anaylsis and Machine Intelligence, Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1984}, {"title": "Discriminative learning of relaxed hierarchy for large-scale visual recognition", "author": ["Tianshi Gao", "Daphne Koller"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Multiclass boosting with hinge loss based on output coding", "author": ["Tianshi Gao", "Daphne Koller"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Evolving output codes for multiclass problems", "author": ["N. Garcia-Pedrajas", "C. Fyfe"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Numerical linear algebra and optimization", "author": ["Phil Gill"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Euclidean embedding of co-occurrence data", "author": ["Amir Globerson", "Gal Chechik", "Fernando Pereira", "Naftali Tishby"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Learning and using taxonomies for fast visual categorization", "author": ["Gregory Griffin", "Pietro Perona"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "On the convergence of the block nonlinear gaussseidel method under convex constraints", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Operations Research Letters,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Application of a multilayer decision tree in computer recognition of chinese characters", "author": ["YX Gu", "Qing Ren Wang", "Ching Y Suen"], "venue": "Pattern Anaylsis and Machine Intelligence, Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1983}, {"title": "Nenmf: an optimal gradient method for nonnegative matrix factorization", "author": ["Naiyang Guan", "Dacheng Tao", "Zhigang Luo", "Bo Yuan"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Computing the nearest correlation matrixa problem from finance", "author": ["Nicholas J Higham"], "venue": "IMA journal of Numerical Analysis,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "Fast coordinate descent methods with variable selection for non-negative matrix factorization", "author": ["Cho-Jui Hsieh", "Inderjit S Dhillon"], "venue": "In ACM SIGKDD,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method", "author": ["Hyunsoo Kim", "Haesun Park"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Nonmetric multidimensional scaling: a numerical method", "author": ["Joseph B Kruskal"], "venue": "Psychometrika, 29(2):115\u2013129,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1964}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["Ludmila I Kuncheva", "Christopher J Whitaker"], "venue": "Machine learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2003}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Chih-Jen Lin"], "venue": "Neural computation,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Evolutionary design of multiclass support vector machines", "author": ["Ana C. Lorena", "Andr\u00e9 C.P.L.F. Carvalho"], "venue": "Journal of Intelligent Fuzzy Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "A dual approach to semidefinite least-squares problems", "author": ["J\u00e9r\u00f4me Malick"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Constructing category hierarchies for visual recognition", "author": ["Marcin Marszalek", "Cordelia Schmid"], "venue": "In ECCV,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "The AR face database", "author": ["A. Martinez", "R. Benavente"], "venue": "In Computer Vision Center Technical Report", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1998}, {"title": "The discrete basis problem. Knowledge and Data Engineering", "author": ["Pauli Miettinen", "Taneli Mielikainen", "Aristides Gionis", "Gautam Das", "Heikki Mannila"], "venue": "IEEE Transactions on,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2008}, {"title": "A theory of multiclass boosting", "author": ["Indraneel Mukherjee", "Robert E Schapire"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Efficiency of coordinate descent methods on hugescale optimization problems", "author": ["Yu Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Relative attributes", "author": ["Devi Parikh", "Kristen Grauman"], "venue": "In ICCV, pages 503\u2013510", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Discriminant ECOC: A heuristic method for application dependent design of error correcting output codes", "author": ["O. Pujol", "P. Radeva", "J. Vitri\u00e0"], "venue": "In Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1vc"], "venue": "Mathematical Programming,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2004}, {"title": "Methods for binary multidimensional scaling", "author": ["Douglas LT Rohde"], "venue": "Neural Computation,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2002}, {"title": "Multiclass boosting: Theory and algorithms", "author": ["Mohammad J Saberian", "Nuno Vasconcelos"], "venue": "In NIPS,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "Using output codes to boost multiclass learning problems", "author": ["Robert E Schapire"], "venue": "In ICML,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1997}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["Paul Tseng"], "venue": "Journal of optimization theory and applications,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2001}, {"title": "Multidimensional spectral hashing", "author": ["Yair Weiss", "Rob Fergus", "Antonio Torralba"], "venue": "ECCV", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Support vector machines for multi-class pattern recognition", "author": ["Jason Weston", "Chris Watkins"], "venue": "In ESANN,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1999}, {"title": "Designing category-level attributes for discriminative visual recognition", "author": ["Felix X Yu", "Liangliang Cao", "Rogerio S Feris", "John R Smith", "Shih-Fu Chang"], "venue": "In CVPR,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Spectral error correcting output codes for efficient multiclass recognition", "author": ["Xiao Zhang", "Lin Liang", "Heung-Yeung Shum"], "venue": "In ICCV,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2009}, {"title": "Sparse output coding for large-scale visual recognition", "author": ["Bin Zhao", "Eric P Xing"], "venue": "In CVPR,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2013}, {"title": "Adaptive errorcorrecting output codes. In IJCAI, pages 1932\u20131938", "author": ["Guoqiang Zhong", "Mohamed Cheriet"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2013}, {"title": "Joint learning of error-correcting output codes and dichotomizers from data", "author": ["Guoqiang Zhong", "Kaizhu Huang", "Cheng-Lin Liu"], "venue": "Neural Computing and Applications,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2012}, {"title": "Decoding design based on posterior probabilities in ternary error-correcting output codes", "author": ["Jin Deng Zhou", "Xiao Dan Wang", "Hong Jian Zhou", "Jie Ming Zhang", "Ning Jia"], "venue": "Pattern Recognition,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2012}], "referenceMentions": [{"referenceID": 46, "context": "All [48] or Random [2] approaches ignore the data distribution, thus not taking profit of allocating the error-correcting capabilities of ECOCs in a problemdependent fashion.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "All [48] or Random [2] approaches ignore the data distribution, thus not taking profit of allocating the error-correcting capabilities of ECOCs in a problemdependent fashion.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "e Nearest Neighbours [22], Decision Trees [30], Random Forests [6]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "e Nearest Neighbours [22], Decision Trees [30], Random Forests [6]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "e Nearest Neighbours [22], Decision Trees [30], Random Forests [6]).", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "One of the most well know approaches are the extensions of SVMs [7] to the multi-class case.", "startOffset": 64, "endOffset": 67}, {"referenceID": 52, "context": "For instance, the work of Weston and Watkins [55] presents a single-machine extension of the SVM method to cope with the multi-class case, in which k predictor functions are trained, constrained with k\u22121 slack variables per sample.", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "However, a more recent adaptation of [14] reduces the number of constraints per samples to one, paying only for the second largest classification score among the k predictors.", "startOffset": 37, "endOffset": 41}, {"referenceID": 46, "context": "Despite these efforts, single-machine approaches to estimate f scale poorly with the number of classes and are often outperformed by simple decompositions [48], [52].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "In recent years various works that extended the classical Adaptive Boosting method [20] to the multi-class setting have been presented [51], [43].", "startOffset": 83, "endOffset": 87}, {"referenceID": 49, "context": "In recent years various works that extended the classical Adaptive Boosting method [20] to the multi-class setting have been presented [51], [43].", "startOffset": 135, "endOffset": 139}, {"referenceID": 41, "context": "In recent years various works that extended the classical Adaptive Boosting method [20] to the multi-class setting have been presented [51], [43].", "startOffset": 141, "endOffset": 145}, {"referenceID": 48, "context": "Furthermore, the work of Saberian and Vasconcenlos [50] presents a derivation of a new margin loss function for multi-class classification altogether with the set of real class codewords that maximize the presented multi-class margin, yielding boundaries with max margin.", "startOffset": 51, "endOffset": 55}, {"referenceID": 46, "context": "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].", "startOffset": 160, "endOffset": 163}, {"referenceID": 16, "context": "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].", "startOffset": 165, "endOffset": 169}, {"referenceID": 44, "context": "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].", "startOffset": 171, "endOffset": 175}, {"referenceID": 3, "context": "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].", "startOffset": 177, "endOffset": 180}, {"referenceID": 38, "context": "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].", "startOffset": 182, "endOffset": 186}, {"referenceID": 26, "context": "On the other hand, the divide and conquer approach has drawn a lot of attention due to its excellent results and easily parallelizable architecture [48], [52], [2], [18], [46], [4], [40], [28].", "startOffset": 188, "endOffset": 192}, {"referenceID": 46, "context": "All [48] are those that use a predefined problem partition scheme followed by a committee strategy to aggregate the binary classifier outputs.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "On the other hand, hierarchical classification relies on a similarity metric distance among classes to build a binary tree in which nodes correspond to different problem partitions [23], [40], [28].", "startOffset": 181, "endOffset": 185}, {"referenceID": 38, "context": "On the other hand, hierarchical classification relies on a similarity metric distance among classes to build a binary tree in which nodes correspond to different problem partitions [23], [40], [28].", "startOffset": 187, "endOffset": 191}, {"referenceID": 26, "context": "On the other hand, hierarchical classification relies on a similarity metric distance among classes to build a binary tree in which nodes correspond to different problem partitions [23], [40], [28].", "startOffset": 193, "endOffset": 197}, {"referenceID": 14, "context": "Finally, the ECOC framework consists of two steps: In the coding step, a set of binary partitions of the original problem are encoded in a matrix of discrete codewords [16] (univocally defined, one code per class) (see Figure 2).", "startOffset": 168, "endOffset": 172}, {"referenceID": 15, "context": "At the decoding step a final decision is obtained by comparing the test codeword resulting of the union of the binary classifier responses with every class codeword and choosing the class codeword at minimum distance [17], [61].", "startOffset": 217, "endOffset": 221}, {"referenceID": 58, "context": "At the decoding step a final decision is obtained by comparing the test codeword resulting of the union of the binary classifier responses with every class codeword and choosing the class codeword at minimum distance [17], [61].", "startOffset": 223, "endOffset": 227}, {"referenceID": 46, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 144, "endOffset": 147}, {"referenceID": 16, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 187, "endOffset": 191}, {"referenceID": 44, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 193, "endOffset": 197}, {"referenceID": 3, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 199, "endOffset": 202}, {"referenceID": 54, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 204, "endOffset": 208}, {"referenceID": 22, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 210, "endOffset": 214}, {"referenceID": 55, "context": "The coding step has been widely studied in literature, yielding three different types of codings: predefined codings [48], [52], random codings [2] and problem-dependent codings for ECOC [18], [46], [4], [57], [24], [58].", "startOffset": 216, "endOffset": 220}, {"referenceID": 1, "context": "In [2], the authors propose the Dense and Sparse Random coding designs with a fixed code length of {10, 15} log2(K), respectively.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2] the authors encourage to generate a set of 10 random matrices and select the one that maximizes the minimum distance between rows, thus showing the highest correction capability.", "startOffset": 3, "endOffset": 6}, {"referenceID": 54, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 124, "endOffset": 128}, {"referenceID": 22, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 130, "endOffset": 134}, {"referenceID": 55, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 142, "endOffset": 146}, {"referenceID": 57, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 148, "endOffset": 152}, {"referenceID": 56, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 154, "endOffset": 158}, {"referenceID": 44, "context": "Alternatively, problem-dependent strategies for ECOC have proven to be successful in multi-class classification tasks [57], [23], [24], [58], [18], [60], [59], [46].", "startOffset": 160, "endOffset": 164}, {"referenceID": 54, "context": "In that sense, [57] computes a spectral decomposition of the graph laplacian associated to the multi-class problem.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In [24], Gao and Koller propose a method which adaptively learns an ECOC coding by optimizing a novel multi-class hinge loss function sequentially.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "On an update of their earlier work, Gao and Koller propose in [23] a joint optimization process to learn a hierarchy of classifiers in which each node corresponds to a binary subproblem that is optimized to find easily separable subproblems.", "startOffset": 62, "endOffset": 66}, {"referenceID": 55, "context": "al [58] proposes a dual projected gradient method embedded on a constrained concave-convex procedure to optimize an objective composed of a measure of expected problem separability, codeword correlation and regularization terms.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "2(a) boost the boundaries of classes that are prone to be confused, while other approaches that use equal or higher number of classifiers like Dense Random [2] in Fig.", "startOffset": 156, "endOffset": 159}, {"referenceID": 14, "context": "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].", "startOffset": 163, "endOffset": 167}, {"referenceID": 1, "context": "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].", "startOffset": 169, "endOffset": 172}, {"referenceID": 15, "context": "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].", "startOffset": 186, "endOffset": 190}, {"referenceID": 58, "context": "ECOC is a multi-class framework inspired on the basis of errorcorrecting principles of communication theory [16], which is composed of two different steps: coding [16], [2] and decoding [17], [61].", "startOffset": 192, "endOffset": 196}, {"referenceID": 1, "context": "However, [2] introduced a third value, defining ternary valued coding matrices.", "startOffset": 9, "endOffset": 12}, {"referenceID": 15, "context": "Nevertheless, it is not until the work of [17] that decoding functions took into account the meaning of the 0 value at the decoding step.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:", "startOffset": 76, "endOffset": 80}, {"referenceID": 34, "context": "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:", "startOffset": 88, "endOffset": 91}, {"referenceID": 54, "context": "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Several works have studied the characteristics of a good ECOC coding matrix [16], [36], [3], [57], [4], which are summed up in the following three properties:", "startOffset": 99, "endOffset": 102}, {"referenceID": 14, "context": "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].", "startOffset": 130, "endOffset": 134}, {"referenceID": 1, "context": "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].", "startOffset": 136, "endOffset": 139}, {"referenceID": 34, "context": "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 54, "context": "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].", "startOffset": 153, "endOffset": 157}, {"referenceID": 23, "context": "In this sense, different authors have always agreed on defining correction capability for an ECOC coding matrix as a global value [16], [2], [36], [57], [23], [25].", "startOffset": 159, "endOffset": 163}, {"referenceID": 4, "context": "However, since H expresses the hamming distance between rows of X, one can alternatively express the correction capability in a pair-wise fashion [5], allowing for a deeper understanding of how correction is distributed among codewords.", "startOffset": 146, "endOffset": 149}, {"referenceID": 54, "context": "However, recent works [57], [23], [58] have focused on designing a matrix X where binary problems are easily separable.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "However, recent works [57], [23], [58] have focused on designing a matrix X where binary problems are easily separable.", "startOffset": 28, "endOffset": 32}, {"referenceID": 55, "context": "However, recent works [57], [23], [58] have focused on designing a matrix X where binary problems are easily separable.", "startOffset": 34, "endOffset": 38}, {"referenceID": 51, "context": "Therefore, instead of directly requiring H to match D, we can equivalently require the product XX> to match D [54].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "Similar constraints have been studied thoroughly in literature [16], [36], [25] defining methods that rely on diversity measures for binary problems to obtain a coding matrix X.", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "Similar constraints have been studied thoroughly in literature [16], [36], [25] defining methods that rely on diversity measures for binary problems to obtain a coding matrix X.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Similar constraints have been studied thoroughly in literature [16], [36], [25] defining methods that rely on diversity measures for binary problems to obtain a coding matrix X.", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "To overcome this issue and following [13], [58], [8] we relax the discrete constraint in 2 an replace it by X \u2208 [\u22121,+1]k\u00d7l in Equation 7.", "startOffset": 37, "endOffset": 41}, {"referenceID": 55, "context": "To overcome this issue and following [13], [58], [8] we relax the discrete constraint in 2 an replace it by X \u2208 [\u22121,+1]k\u00d7l in Equation 7.", "startOffset": 43, "endOffset": 47}, {"referenceID": 47, "context": "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 33, "context": "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].", "startOffset": 138, "endOffset": 141}, {"referenceID": 35, "context": "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].", "startOffset": 321, "endOffset": 325}, {"referenceID": 13, "context": "In this sense, although gradient descent techniques have been successfully applied in the literature to obtain local minimums [49], [35], [1] these techniques do not enjoy the efficiency and scalability properties present in other optimization methods applied to Matrix Factorization problems, such as Coordinate Descent [37], [15].", "startOffset": 327, "endOffset": 331}, {"referenceID": 32, "context": "Coordinate Descent techniques have been widely applied in Nonnegative Matrix Factorization obtaining satisfying results in terms of efficiency [34], [31].", "startOffset": 143, "endOffset": 147}, {"referenceID": 29, "context": "Coordinate Descent techniques have been widely applied in Nonnegative Matrix Factorization obtaining satisfying results in terms of efficiency [34], [31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 27, "context": "been proved that if each of the coordinate sub-problems can be solved exactly, Coordinate Descent converges to a stationary point [29], [53].", "startOffset": 130, "endOffset": 134}, {"referenceID": 50, "context": "been proved that if each of the coordinate sub-problems can be solved exactly, Coordinate Descent converges to a stationary point [29], [53].", "startOffset": 136, "endOffset": 140}, {"referenceID": 24, "context": "To solve the minimization problem in Algorithm 1 we use the Active Set method described in [26], which finds an initial feasible solution by first solving a linear programming problem.", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "In addition, we can also find a common ground with the Nearest Correlation Matrix (NMC) Problem [32], [9], [39].", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "In addition, we can also find a common ground with the Nearest Correlation Matrix (NMC) Problem [32], [9], [39].", "startOffset": 102, "endOffset": 105}, {"referenceID": 37, "context": "In addition, we can also find a common ground with the Nearest Correlation Matrix (NMC) Problem [32], [9], [39].", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "In addition, the ECF has similarities with the Discrete Basis Problem (DBP) [42], since the factors are X discrete valued.", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "This problem has been treated in several works [32], [9], [11], [27], resulting in various algorithms that often use an alternating projections approach.", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "In this sense, to find D\u0303 we follow an alternating projections algorithm, similar as [32], which is shown in Algorithm 2.", "startOffset": 85, "endOffset": 89}, {"referenceID": 54, "context": "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.", "startOffset": 125, "endOffset": 129}, {"referenceID": 55, "context": "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.", "startOffset": 131, "endOffset": 135}, {"referenceID": 3, "context": "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.", "startOffset": 137, "endOffset": 140}, {"referenceID": 16, "context": "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "On the other hand, coding designs like Dense or Sparse Random codings (which are very often used in experimental comparisons [57], [58], [4], [18]) are suggested [2] to have a code length of d10log2(k)e and d15log2(k)e respectively.", "startOffset": 162, "endOffset": 165}, {"referenceID": 54, "context": "Other approaches, like Spectral ECOC [57] search for the code length by looking at the best performance on a validation set.", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "Nevertheless, recent works have shown that the code length can be reduced to of l = log2(k) with very small loss in performance if the ECOC coding design is carefully chosen [38] and classifiers are strong.", "startOffset": 174, "endOffset": 178}, {"referenceID": 45, "context": "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].", "startOffset": 105, "endOffset": 109}, {"referenceID": 50, "context": "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].", "startOffset": 111, "endOffset": 115}, {"referenceID": 31, "context": "However, the problem of choosing the coordinate to minimize at each iteration still remains active [47], [21], [53], [33].", "startOffset": 117, "endOffset": 121}, {"referenceID": 42, "context": "In particular, [44] derives a convergence rate which is faster when coordinates are chosen uniformly at random rather than on a cyclic fashion.", "startOffset": 15, "endOffset": 19}, {"referenceID": 45, "context": "Hence, choosing coordinates at random its a suitable choice when the problem shows some of the following characteristics [47]:", "startOffset": 121, "endOffset": 125}, {"referenceID": 42, "context": "\u2022 Recent efforts suggest that randomization can improve the convergence rate [44].", "startOffset": 77, "endOffset": 81}, {"referenceID": 39, "context": "from the ARFaces [41] dataset are classified using the present methodology.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "\u2022Traffic sign categorization: We test ECF on a real traffic sign categorization problem, of 36 classes [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 39, "context": "\u2022ARFaces classification: The ARFace database [41] is composed of 26 face images from 126 different subjects (from which 50 are selected), portraying different expressions and complements.", "startOffset": 45, "endOffset": 49}, {"referenceID": 46, "context": "One (OVO) approaches [48], [52].", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "In the second, we generate a Dense Random coding matrix [3] (DENSE).", "startOffset": 56, "endOffset": 59}, {"referenceID": 54, "context": "Finally, in order to compare our proposal with state-of-the-art methods, we also used the Spectral ECOC (SECOC) method [57] and the Relaxed Hierarchy [23] (R-H) .", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "Finally, in order to compare our proposal with state-of-the-art methods, we also used the Spectral ECOC (SECOC) method [57] and the Relaxed Hierarchy [23] (R-H) .", "startOffset": 150, "endOffset": 154}, {"referenceID": 21, "context": "Although, there exist a number of approaches to define D from data [23], [58], [57], i.", "startOffset": 67, "endOffset": 71}, {"referenceID": 55, "context": "Although, there exist a number of approaches to define D from data [23], [58], [57], i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 54, "context": "Although, there exist a number of approaches to define D from data [23], [58], [57], i.", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "The parameter C was tunned on a grid-search on a log sampling in the range [0, 10], and the \u03b3 parameter was equivalently tuned on a equidistant linear sampling in the range [0, 1], we used the libsvm implementation available at [12].", "startOffset": 75, "endOffset": 82}, {"referenceID": 0, "context": "The parameter C was tunned on a grid-search on a log sampling in the range [0, 10], and the \u03b3 parameter was equivalently tuned on a equidistant linear sampling in the range [0, 1], we used the libsvm implementation available at [12].", "startOffset": 173, "endOffset": 179}, {"referenceID": 10, "context": "The parameter C was tunned on a grid-search on a log sampling in the range [0, 10], and the \u03b3 parameter was equivalently tuned on a equidistant linear sampling in the range [0, 1], we used the libsvm implementation available at [12].", "startOffset": 228, "endOffset": 232}, {"referenceID": 21, "context": "For the Relaxed Hierarchy method [23] we used values for \u03c1 \u2208 {0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "g all tested methods but the one in [23]) we used both the Hamming Decoding (HD) and the Loss-Weighted decoding (LWD) [46].", "startOffset": 36, "endOffset": 40}, {"referenceID": 44, "context": "g all tested methods but the one in [23]) we used both the Hamming Decoding (HD) and the Loss-Weighted decoding (LWD) [46].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "We used non-linear SVM classifiers and we define the relative computational complexity as the number of unique Support Vectors (SVs) yielded for each method, as in [23].", "startOffset": 164, "endOffset": 168}, {"referenceID": 46, "context": "Finally, Figures 10(c) and 10(f) show the comparison for OVA, which is a standard method often defended by its simplicity [48].", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "The R-H method [23] is far less complex than the compared methods, however we compare it to the to the closest operating complexity for each of the rest of the methods.", "startOffset": 15, "endOffset": 19}], "year": 2015, "abstractText": "Error Correcting Output Codes (ECOC) is a successful technique in multi-class classification, which is a core problem in Pattern Recognition and Machine Learning. A major advantage of ECOC over other methods is that the multi-class problem is decoupled into a set of binary problems that are solved independently. However, literature defines a general error-correcting capability for ECOCs without analyzing how it distributes among classes, hindering a deeper analysis of pairwise error-correction. To address these limitations this paper proposes an Error-Correcting Factorization (ECF) method, our contribution is three fold: (I) We propose a novel representation of the error-correction capability, called the design matrix, that enables us to build an ECOC on the basis of allocating correction to pairs of classes. (II) We derive the optimal code length of an ECOC using rank properties of the design matrix. (III) ECF is formulated as a discrete optimization problem, and a relaxed solution is found using an efficient constrained block coordinate descent approach. (IV) Enabled by the flexibility introduced with the design matrix we propose to allocate the error-correction on classes that are prone to confusion. Experimental results in several databases show that when allocating the error-correction to confusable classes ECF outperforms state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}