{"id": "1502.08030", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2015", "title": "Author Name Disambiguation by Using Deep Neural Network", "abstract": "Author name ambiguity decreases the quality and reliability of information retrieved from digital libraries. Existing methods have tried to solve this problem by predefining a feature set based on expert's knowledge for a specific dataset. In this paper, we propose a new approach which uses deep neural network to learn features automatically from data. Additionally, we propose the general system architecture for author name disambiguation on any dataset. In this research, we evaluate the proposed method on a dataset containing Vietnamese author names. The results show that this method significantly outperforms other methods that use predefined feature set. The proposed method achieves 99.31% in terms of accuracy. Prediction error rate decreases from 1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with other methods that use predefined feature set (Table 3).", "histories": [["v1", "Fri, 27 Feb 2015 19:34:42 GMT  (706kb,D)", "https://arxiv.org/abs/1502.08030v1", null], ["v2", "Sat, 29 Jul 2017 01:32:31 GMT  (358kb,D)", "http://arxiv.org/abs/1502.08030v2", null]], "reviews": [], "SUBJECTS": "cs.DL cs.CL cs.LG", "authors": ["hung nghiep tran", "tin huynh", "tien do"], "accepted": false, "id": "1502.08030"}, "pdf": {"name": "1502.08030.pdf", "metadata": {"source": "META", "title": "Author Name Disambiguation by Using Deep Neural Network", "authors": ["Hung Nghiep Tran", "Tin Huynh", "Tien Do", "Linh Trung Ward", "Thu Duc"], "emails": ["tiendv}@uit.edu.vn"], "sections": [{"heading": null, "text": "Keywords: Digital Library, Bibliographic Data, Author Name Disambiguation, Machine Learning, Feature Learning, Deep Neural Network."}, {"heading": "1 Introduction", "text": "The author identifies a problem that occurs when a series of publications contain ambiguous author names, i.e., the same author may appear under different names (synonyms), or different authors may have similar names (polysemy).This problem reduces the quality and reliability of information retrieved from digital libraries, such as the effects of authors, the effects of organizations, etc. Therefore, the author name disambiguation is a critical task in digital libraries. There are two approaches to naming the author disambiguations: (1) grouping of publications that show some similarity between them (author grouping of methods) or (2) assigning them directly to their respective authors."}, {"heading": "2 Related Work", "text": "In their study, they tried to create, select and combine attributes based on the similarity of attributes by using some stringmatching measures or some specific heuristics, such as the number of co-author names in common, etc.Bhattacharya and Getoor [1] suggesting a combined similarity function based on attributes and relational information.The method has a high F1 score of about 0.99 in the arXiv collection and only about 0.81 in the BioBase collection. In another study, Torvik and al used a trait that emerges from the comparison between generic quotation attributes and medical headings, language and affiliation to two references in the database. In another paper, Torvik and Smalheiser will incorporate some attributes into their method to achieve a better result."}, {"heading": "3 Our Approach", "text": "In this section we describe our proposed method and the general system architecture for disambiguating author names on arbitrary datasets. This method uses a combination of many DNNs to learn features from a data representative that could be automatically calculated for each dataset."}, {"heading": "3.1 Deep Neural Network", "text": "There are many types of DNN. All of them are neural networks with many layers, but they differ in parameter initialization schemes, formation of algorithms, activation of functions, etc. In this study, we use DNN with simple feedfoward architecture, a.k.a., multi-layer perceptrons [12]. Figure 1 shows the general architecture of such a network. The network has many layers that are stacked on top of each other. Each neural unit in each layer connects to each layer in the sequential layer. The number of units in the input layer corresponds to the number of basic features we use."}, {"heading": "3.2 Data Representative", "text": "As we have shown in the previous subsection, DNN could learn internal features from data. To do this, data must be inserted correctly into the input layer. Input should be a good representative of data, i.e. it could describe details in data. We call this data the basic attribute set. There are many different ways to calculate a data representative. An obvious way is to measure the similarity between all attributes of two release records, such as car name, affiliations, co-author and paper keyword, etc., using string matching measures. We assume that the similarity between these attributes expresses how much two publications belong to the same authority. According to some surveys that check string matching measures to identify duplication [2, 4] there are three types of attributes: (1) Edit spacing such as Levenshtein, Monger-Elkan, Jaro, and Jaro Winkler."}, {"heading": "3.3 System Architecture", "text": "In this subsection, we describe the general system architecture for disambiguating the author name. The system could run on any dataset without the expert making any changes. Figure 2 shows the proposed system architecture. The system includes two components. The first component takes the data and calculates a representative dataset, i.e. a basic attribute set x, to represent data. There could be many representatives for the same data, so this component could be implemented in many ways. In this research, we use string matching measures to calculate the representative data. The calculations could be performed automatically on each dataset. The second component takes the basic attribute set as input and then learns characteristics in its hidden layers to determine unique author names. In this research, we use string matching measures to calculate the probabilities py | x (y = c | x) = pL (y = c | vL) to determine if two author names are not part of the same architecture (&p) or 0.5 (p)."}, {"heading": "4 Experiment, Evaluation and Discussion", "text": "We examine the impact of automatic feature learning by DNNs on the problem of author name disambiguation on the Vietnamese author dataset (very ambiguous cases [5]) collected from online digital libraries. In this section, we present our experimental settings, evaluation results and discussions."}, {"heading": "4.1 Dataset", "text": "The data was obtained from three online digital libraries, which are ACM 4, IEEE Xplore 5 and MAS 6, by querying their search engine using the names of 10 Vietnamese authors. For these authors, there are many different instance names, e.g. author \"Hoang Kiem\" can have many instance names, such as \"Hoang Kiem,\" \"Kiem Hoang,\" \"Kiem Van Kiem,\" etc. Query results are publications with different instance names of the author. We created the data set by creating publication pairs for each author. Based on our understanding of these authors, we manually labeled each pair with value 1 if ambiguous names in that pair were actually one person and value 0 other records. In this research, we expand the data set so that there are a total of 30537 samples in the data set."}, {"heading": "4.2 Tuning Hyperparameters", "text": "We start with the smallest network size of one hidden layer and 10 hidden units. We use this network as the basis for matching hyperparameters and achieve an average validation accuracy of 95.35%. Then we increase the number of hidden layers or hidden units and conduct experiments on many network sizes. Table 2 shows five network sizes (the number of hidden layers \u00d7 the number of hidden units in each layer) with the highest accuracy. The network with 7 hidden layers and 50 hidden units in each layer achieves the highest average validation accuracy."}, {"heading": "4.3 Evaluation", "text": "In our most recent research [9], we proposed an approach based on a predefined feature set for the Vietnamese author name, and applied several classification models to this feature set. In accordance with this research, k-NN, Random Forest, C4.5, SVM, and Naive Bayes, respectively, are the most appropriate methods for the predefined feature set. In this study, we compare the proposed method with these methods. The proposed method implements the system architecture we described. DNNs use the hyperparameters that have been tuned, while the other methods use the same settings and implementations as in our previous studies [9]. Table 3 shows evaluation results in terms of accuracy and error on a separate test set. Results show that the proposed method significantly exceeds methods that use predefined features. The proposed method achieves 99.31% in terms of accuracy, while the best method that uses predefined features achieves 917% in terms of accuracy."}, {"heading": "4.4 Discussion", "text": "Evaluation results clearly show advantages of learning functions compared to predefined functions in terms of accuracy. In addition, automatic feature learning does not require expert knowledge about specific datasets. DNN was used to successfully learn functions. However, due to its high ability to learn complex features, it is prone to overfits. In this research, we use many techniques to reduce overfits. We extend the Vietnamese author dataset. We use k-fold cross-validation for hyperparameter tuning and early stoppage. In addition, our system architecture uses multi-column technology to have a lower variance. The type of DNN we use has the problem of disappearing progression when trained with the traditional sigmoid activation function and the reverse propagation algorithm. One solution is the choice of a good activation function [6]. The hyperbolic tangent function (x) (= 1 \u2212 e \u2212 2x / \u2212 e is better than its traditional signature function."}, {"heading": "5 Conclusion and Future Work", "text": "We evaluated the proposed method using a Vietnamese author dataset, and the results show that this method significantly outperforms other methods that use predefined characteristics; the proposed method achieves 99.31% accuracy; the error rate in the prediction decreases from 1.83% to 0.69%, i.e. it decreases in relative terms by 1.14% or 62.3% compared to other methods that use predefined characteristics; the proposed method could be expanded to address some outstanding challenges such as the lack of marked training data, incremental and new author data; and in the future, we will compare the proposed method with other datasets."}], "references": [{"title": "Collective entity resolution in relational data", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "ACM Trans. Knowl. Discov. Data 1(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive name matching in information integration", "author": ["M. Bilenko", "R. Mooney", "W. Cohen", "P. Ravikumar", "S. Fienberg"], "venue": "IEEE Intell. Sys. 18(5), 16\u201323", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-column deep neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "CVPR. pp. 3642\u20133649", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "A comparison of string distance metrics for name-matching tasks", "author": ["W.W. Cohen", "P.D. Ravikumar", "S.E. Fienberg"], "venue": "IIWeb. pp. 73\u201378", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A brief survey of automatic methods for author name disambiguation", "author": ["A.A. Ferreira", "M.A. Gon\u00e7alves", "A.H. Laender"], "venue": "SIGMOD Rec. 41(2), 15\u201326", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "JLMR - Proceedings Track 9, 249\u2013256", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS. pp. 315\u2013323", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Comput. 18(7), 1527\u20131554", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Vietnamese author name disambiguation for integrating publications from heterogeneous sources", "author": ["T. Huynh", "K. Hoang", "T. Do", "D. Huynh"], "venue": "The 5th ACIIDS - Vol. Part I. pp. 226\u2013235. ACIIDS\u201913, Springer, Berlin, Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS 25, pp. 1106\u20131114", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y.L. Boureau", "Y. LeCun"], "venue": "NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel distributed processing: explorations in the microstructure of cognition, vol", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "1. chap. Learning internal representations by error propagation, pp. 318\u2013362. MIT Press, Cambridge, MA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1986}, {"title": "Author name disambiguation in medline", "author": ["V.I. Torvik", "N.R. Smalheiser"], "venue": "ACM Trans. Knowl. Discov. Data 3(3), 11:1\u201311:29", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "A probabilistic similarity metric for medline records: A model for author name disambiguation: Research articles", "author": ["V.I. Torvik", "M. Weeber", "D.R. Swanson", "N.R. Smalheiser"], "venue": "J. Am. Soc. Inf. Sci. Technol. 56(2), 140\u2013158", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature learning in deep neural networks - a study on speech recognition tasks", "author": ["D. Yu", "M.L. Seltzer", "J. Li", "J.T. Huang", "F. Seide"], "venue": "CoRR abs/1301.3605", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": ", the same author may appear under distinct names (synonymy), or distinct authors may have similar names (polysemy) [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "There are two approaches to author name disambiguation: (1) grouping publication records of a same author by finding some similarity among them (author grouping methods) or (2) directly assigning them to their respective authors (author assignment methods) [5].", "startOffset": 257, "endOffset": 260}, {"referenceID": 2, "context": "Recent researches [3, 10, 15] have shown their strong ability in feature learning in many tasks.", "startOffset": 18, "endOffset": 29}, {"referenceID": 9, "context": "Recent researches [3, 10, 15] have shown their strong ability in feature learning in many tasks.", "startOffset": 18, "endOffset": 29}, {"referenceID": 14, "context": "Recent researches [3, 10, 15] have shown their strong ability in feature learning in many tasks.", "startOffset": 18, "endOffset": 29}, {"referenceID": 14, "context": "Internal features learned by the DNN are relatively stable for variants in data if the training data are sufficiently representative [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] did a brief survey of author name disambiguation methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Bhattacharya and Getoor [1] proposed a combined similarity function defined on attributes and relational information.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "[14] used a feature set resulting from the comparison between the common citation attributes along with medical subject headings, language, and affiliation of two references in MEDLINE dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In a subsequent work [13], Torvik and Smalheiser incorporated some features into their method to achieve better result.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "In our previous research [9], we predefined a feature set to learn a similarity function specifically for Vietnamese author dataset, one of the most difficult case, and obtained around 0.", "startOffset": 25, "endOffset": 28}, {"referenceID": 11, "context": "DNN could be regarded as feedforward neural network with more than one layer [12].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": ", such as RBM [8], sparse auto-encoders [11], and normalized initialization [6].", "startOffset": 14, "endOffset": 17}, {"referenceID": 10, "context": ", such as RBM [8], sparse auto-encoders [11], and normalized initialization [6].", "startOffset": 40, "endOffset": 44}, {"referenceID": 5, "context": ", such as RBM [8], sparse auto-encoders [11], and normalized initialization [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "[3] was very successful in using a big DNN to learn features in image recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] used a simple deep feedforward neural network to learn features in speech recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "DNN is a popular method for automatic feature learning [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": "Some recent researches have successfully exploited feature learning using DNN to achieve stateof-the-art performance in many tasks [3, 10,15].", "startOffset": 131, "endOffset": 141}, {"referenceID": 9, "context": "Some recent researches have successfully exploited feature learning using DNN to achieve stateof-the-art performance in many tasks [3, 10,15].", "startOffset": 131, "endOffset": 141}, {"referenceID": 14, "context": "Some recent researches have successfully exploited feature learning using DNN to achieve stateof-the-art performance in many tasks [3, 10,15].", "startOffset": 131, "endOffset": 141}, {"referenceID": 11, "context": ", multi-layer perceptron [12].", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Internal features will be learned in each hidden layer [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "To address this issue, we use the activation function softsign(x) = x/(1 + |x|) and the adaptive resilient backpropagation algorithm together with some training techniques [6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "The number of units should be equal among hidden layers so that information could flow effectively [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Those two hyperparameters are usually chosen based on experiments on the validation set [3, 15].", "startOffset": 88, "endOffset": 95}, {"referenceID": 14, "context": "Those two hyperparameters are usually chosen based on experiments on the validation set [3, 15].", "startOffset": 88, "endOffset": 95}, {"referenceID": 1, "context": "According to some surveys reviewing string-matching measures to identify the duplication [2, 4], there are three types of measure: (1) Edit distance such as Levenshtein, Monger-Elkan, Jaro, and Jaro-Winkler; (2) Token-based such as Jaccard and TF/IDF and (3) Hybrid measures such as Mogne-Elkan for comparing two long strings.", "startOffset": 89, "endOffset": 95}, {"referenceID": 3, "context": "According to some surveys reviewing string-matching measures to identify the duplication [2, 4], there are three types of measure: (1) Edit distance such as Levenshtein, Monger-Elkan, Jaro, and Jaro-Winkler; (2) Token-based such as Jaccard and TF/IDF and (3) Hybrid measures such as Mogne-Elkan for comparing two long strings.", "startOffset": 89, "endOffset": 95}, {"referenceID": 2, "context": "In this system architecture, we use multi-column DNN technique, which is illustrated in figure 3, to improve the generalization capabilities of the system [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "We evaluate the effect of using automatic feature learning by DNNs for the author name disambiguation problem on Vietnamese author dataset (very ambiguous cases [5]), which we collected from online digital libraries.", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "In a previous work [9], we built a Vietnamese author dataset for checking author name ambiguity.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "In our recent research [9], we proposed an approach based on a predefined feature set for Vietnamese author name and applied several classification models to that feature set.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "The other methods use the same settings and implementations as in our previous research [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "One solution is choosing a good activation function [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "The rectifier function max(x, 0) is one of the best activation functions [6, 7].", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "The rectifier function max(x, 0) is one of the best activation functions [6, 7].", "startOffset": 73, "endOffset": 79}, {"referenceID": 2, "context": "On the other hand, automatic feature learning using DNN has shown its ability in many complex tasks such as image recognition, where this method could recognize object just by using raw pixels [3].", "startOffset": 193, "endOffset": 196}], "year": 2017, "abstractText": "Author name ambiguity is one of the problems that decrease the quality and reliability of information retrieved from digital libraries. Existing methods have tried to solve this problem by predefining a feature set based on expert\u2019s knowledge for a specific dataset. In this paper, we propose a new approach which uses deep neural network to learn features automatically for solving author name ambiguity. Additionally, we propose the general system architecture for author name disambiguation on any dataset. We evaluate the proposed method on a dataset containing Vietnamese author names. The results show that this method significantly outperforms other methods that use predefined feature set. The proposed method achieves 99.31% in terms of accuracy. Prediction error rate decreases from 1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with other methods that use predefined feature set (Table 3).", "creator": "Hung Nghiep Tran"}}}