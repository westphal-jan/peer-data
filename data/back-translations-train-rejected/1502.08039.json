{"id": "1502.08039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2015", "title": "Probabilistic Zero-shot Classification with Semantic Rankings", "abstract": "In this paper we propose a non-metric ranking-based representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources. We apply the ranking-based representation to zero-shot learning problems, and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining. We demonstrate their the advantages on two large real-world image datasets. In particular, we show that aggregating different sources of semantic information, including crowd-sourcing, leads to more accurate classification.", "histories": [["v1", "Fri, 27 Feb 2015 20:00:53 GMT  (51kb,D)", "http://arxiv.org/abs/1502.08039v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["jihun hamm", "mikhail belkin"], "accepted": false, "id": "1502.08039"}, "pdf": {"name": "1502.08039.pdf", "metadata": {"source": "META", "title": "Probabilistic Zero-shot Classification with Semantic Rankings", "authors": ["Jihun Hamm", "Mikhail Belkin"], "emails": ["HAMMJ@CSE.OHIO-STATE.EDU", "MBELKIN@CSE.OHIO-STATE.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is not the case that it is a real case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is a case, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another case is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another case is another, in which it is another, in which it is another, in which it is another, in which it is another case, in which it is another case is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another, in which it is another case which it is another case which it is another, in which it is another, in which it is another"}, {"heading": "2. Zero-shot learning with rankings", "text": "Let R specify the set of all rankings on C positions / classes, and \u03c0 = [\u03c0 (1),..., \u03c0 (C)] \u0432R stands for a ranking order: \u03c0 (i) is the position of point i and \u03c0-1 (j) is the position of point j. We write i-j (\"i precedes j\") when \u03c0 (i) < \u03c0 (j) (\"point i is higher than point j.) A top K ranking is a simple generalization of a ranking in which only the order of the first K points \u03c0-1 (1),..., \u03c0-1 (K) and the order of the remaining C-K points are ignored. In case of misuse of notation, we use \u03c0 and R as top K rankings and the order of all top K rankings likewise, as a complete ranking order is a special case (K = C.) A partial ranking is a further generalization of a ranking and a top K ranking order, and the order of all top K rankings can be considered as a complete or partial satisfying pair of either i."}, {"heading": "2.1. Deterministic approach", "text": "A simple deterministic approach to zero-point learning based on semantic rankings has already been outlined in the introduction. In a one-on-one setting, C (C \u2212 1) 2 pre-trained classifiers assign a test point according to the distances to C-1, a ranking \u03c0 (x). In view of this ranking (x) of a test sample x and prior knowledge of semantic rankings {\u03c0z | z \u0445 Z} of test domain classes Z, we predict a ranking \u03c0 (x) = argmin z-Z d (x), (3), whereas d (\u00b7 \u00b7) is a distance between two rankings. Let us < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "A = {i \u2208 1, ..., C | \u03c01(i) \u2264 K, \u03c02(i) \u2264 K}", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B = {i \u2208 1, ..., C | \u03c01(i) \u2264 K, \u03c02(i) > K}", "text": "D = {i-1,..., C | \u03c01 (i) > K, \u03c02 (i) > K}.Then the top K distance of Kendall can be calculated according to dK (\u03c01, \u03c02) = | {(i, j) \u0394A \u00b7 A | \u03c01 (i) < \u03c01 (j), \u03c02 (i) > \u03c02 (j)} | + | B | (C + K \u2212 | B | \u2212 1 2) \u2212 \u2211 i-B \u03c01 (i) \u2212 \u0445i-\u0445i \u04322 (i).( 5) Zero-shot classification according to the rule (3) is called a deterministic ranking-based (DR) method."}, {"heading": "2.2. Probabilistic approach", "text": "We can further refine ranking-based algorithms by considering a probabilistic approach. There are several causes of uncertainty in rank-based representation. First, the results of classifiers for a test domain sample x may be low in confidence, since classifiers are trained only on training domain samples. Second, we are discussing three models: the Mallows (Mallows, 1957), the Plackett-Luce (Plackett, 1975; Luce, 1959; Marden, 1995), and the BabingtonSmith (Joe & Verducci, 1993; Smith, 1950), which we will introduce where they are needed (see Critchlow, 1985; Critchlow x al., 1991; Marden, 1995)."}, {"heading": "2.3. Prior for semantic ranking", "text": "We encode the semantic similarity between training and test domain classes by using probabilistic ranking models of training domain classes P (\u03c0 | z) for each test domain class. To learn P (\u03c0 | z), we use multiple rankings for each test domain class. These rankings can come directly from multiple linguistic corporations or through human-rated rankings. Below, we outline three popular ranking models - the Plackett-Luce, the Mallows, and the Babington-Smith models. Plackett-Luce model for the probability of observing a top K ranking isP (v) = K-ranking class."}, {"heading": "2.4. Probabilistic ranker from classifiers", "text": "The probable ranking P (\u03c0) takes a sample x (j) as input and probability forecast (1). The similarity of x to training domain classes Y (1). We suggest to build ranking from default settings of multi-class classifiers. (1) In this setting there will be such results f1 (x),...., fC (x) for each training domain class. We refer to the real rated results {fi} and the non-negative parameters of the Plackett Luce model (8) by using vi = efi (x), to getP (x) = 1 efp-1 efp-1 (i)."}, {"heading": "2.5. Zero-shot prediction", "text": "The probability rankings P (\u03c0 | x) are constructed from pre-formed classifiers and the priors for semantic rankings P (\u03c0 | z) learned from semantic sources in (7) P (z | x) = \u2211 \u03c0 \u2022 P (z | x) P (z | x) p (z) p (z) p (z) p (z) p (z) p (z) p (z) p (z) z (z) z p (z) z) p (z) z) p (z) p (z) z) p (z) z) p (z) z) p (z) p (z) p (z) p (z) p (z) z) p (z) p (z) p (z) z) p (z) p (z) p (z) p (z) p) (z) p (z) p (p) p (z) p (p) p (z) p (p) p) p (z) p (p) p) p (z) p) p (z) p) p (z) p) p (z) p) p (z) p (z) p) p (z) p) p (z) p) p (z) p (z) p) p (z) p) p (z) p) p (z) p (z) p) p (z) p) p (z) p (z) p) p) p (z) p) p (z) p) p (z) p) p (z) p) p) p (z) p) p (z) p) p (z) p) p (z) p) p (p) p) p (p) p) p (p) p) p) p) p (p) p) p) p) p (p) p) p) p (p) p) p) p (p) p) p (p) p) p) p (p) p) p) p) p (p) p) p (p) p) p) p (p) p) p) p (p) p) p) p (p) p) p) p) p) p (p) p) p) p) p) p) p) p) p"}, {"heading": "3. Related work", "text": "There are two important approaches to zero-shot learning that are studied in the literature: attribute-based and similarity-based networks. In contrast to attribute-based knowledge transfer (e.g., (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2010), classes from training and test domains are distinguished by a common list of attributes. Attribute-based approaches often show excellent empirical performance (Palatucci et al., 2009; Rohrbach et al., 2010). However, attributes that are discriminatory are used by several classes and correlate with the original trait simultaneously, may be a non-trivial task that usually requires human supervision. Similar arguments can be found in (Rohrbach et al., 2010) or (Mensink et al al al al al., 2014)."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "We use two datasets 1) Animals with Attributes dataset (Lampert et al., 2009) and 2) CIFAR-100 / 10 (Torralba et al., 2008) collected by (Krizhevsky et al., 2009). Semantic similarity results from WordNet distance, web searches (Rohrbach et al., 2010), word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and Amazon Mechanical Turk. Table 1 summarizes the characteristics of the datasets and the types of available semantic information used in the experiments. Further details on data processing are included in the appendix."}, {"heading": "4.2. Methods", "text": "We conduct comprehensive tests of the probabilistic ranking-based (PR) zero-shot model under 1) three learning settings (one against the rest, one against one, multi-class), 2) two types of semantic sources (linguistic, crowdsourcing), and 3) various previous models of semantic rankings (Plackett-Luce and the Mallows models). We compare probabilistic ranking-based methods (PR, Sec. 2.5) with de-terministic ranking-based methods (DR, Sec. 2.1) and direct similarity-based methods (DS, (Rohrbach et al., 2010), which are closest to our state of the art and use classification results. We also refer to other results in the literature for comparison. Regularization parameters for classifiers are determined from the validation set and partially manually to avoid exhaustive cross-validation. We test with different hyperparameters (in the top K list and one against the SVM results)."}, {"heading": "4.3. Result 1 \u2013 Discriminability of semantic rankings", "text": "Using all five linguistic sources for animals, we calculate pairs of distances of 5 \u00d7 10 = 50 similarity vectors. Two types of distances are calculated - the Euclidean distance of numerical similarity, with or without l1 normalization, and the Hausdorff distance for top K lists using the ranking distance (5). Note that the ranking is achieved by sorting the numerical similarity. For these different representations, the average accuracy of the deviation from 1-1-1-nearest neighbor classification is 0.44 (euclidean), 0.62 (euclidean with normalization), 0.72 (Kendall's, K = 2), 0.70 (Kendall's, K = 5), and 0.64 (Kendall's, K = 5, 'K)."}, {"heading": "4.4. Result 2 - Comparison of PR, DR, and DS", "text": "We are comparing the likelihood of such a scenario with the likelihood of such a scenario occurring."}, {"heading": "5. Conclusion", "text": "In this paper, we propose a rank-based representation of semantic similarities as an alternative to metric representation of similarities. Using rankings, semantic information from multiple sources can be naturally aggregated to produce a better representation than individual sources. Using this representation and ranking probability models, we present new zero-shot classifiers that can be constructed with 4Socher et al. and use the rest of the classes of CIFAR-10 instead of CIFAR-100 for training, as well as various semantic information from pre-trained classifiers without retraining, demonstrating their potential to use semantic structures of real visual objects."}, {"heading": "A. Datasets", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}], "references": [{"title": "Label-embedding for attribute-based classification", "author": ["Akata", "Zeynep", "Perronnin", "Florent", "Harchaoui", "Zaid", "Schmid", "Cordelia"], "venue": "In CVPR,", "citeRegEx": "Akata et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Akata et al\\.", "year": 2013}, {"title": "Voting schemes for which it can be difficult to tell who won", "author": ["J. Bartholdi", "C.A. Tovey", "M. Trick"], "venue": "Social Choice and Welfare,", "citeRegEx": "Bartholdi et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Bartholdi et al\\.", "year": 1989}, {"title": "Label ranking methods based on the Plackett-Luce model", "author": ["Cheng", "Weiwei", "Dembczynski", "Krzysztof", "Hllermeier", "Eyke"], "venue": "In ICML, pp", "citeRegEx": "Cheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2010}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Crammer", "Koby", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2002}, {"title": "Probability models on rankings", "author": ["Critchlow", "Douglas E", "Fligner", "Michael A", "Verducci", "Joseph S"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Critchlow et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Critchlow et al\\.", "year": 1991}, {"title": "Metric methods for analyzing partially ranked data. Number 34 in Lecture notes in statistics", "author": ["Critchlow", "Douglas Edward"], "venue": null, "citeRegEx": "Critchlow and Edward.,? \\Q1985\\E", "shortCiteRegEx": "Critchlow and Edward.", "year": 1985}, {"title": "Visual and semantic similarity in imagenet", "author": ["T. Deselaers", "V. Ferrari"], "venue": "In CVPR, pp", "citeRegEx": "Deselaers and Ferrari,? \\Q2011\\E", "shortCiteRegEx": "Deselaers and Ferrari", "year": 2011}, {"title": "On a model for concordance between judges", "author": ["Feigin", "Paul D", "Cohen", "Ayala"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Feigin et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Feigin et al\\.", "year": 1978}, {"title": "Distance based ranking models", "author": ["Fligner", "Michael A", "Verducci", "Joseph S"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Fligner et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fligner et al\\.", "year": 1986}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang", "Eric H", "Socher", "Richard", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proceedings of ACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "MM algorithms for generalized Bradley-Terry models", "author": ["Hunter", "David R"], "venue": "The Annals of Statistics,", "citeRegEx": "Hunter and R.,? \\Q2004\\E", "shortCiteRegEx": "Hunter and R.", "year": 2004}, {"title": "On the Babington Smith class of models for rankings. In Probability models and statistical analyses for ranking", "author": ["Joe", "Harry", "Verducci", "Joseph S"], "venue": null, "citeRegEx": "Joe et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Joe et al\\.", "year": 1993}, {"title": "Mathematics without numbers", "author": ["Kemeny", "John G"], "venue": "Daedalus, pp", "citeRegEx": "Kemeny and G.,? \\Q1959\\E", "shortCiteRegEx": "Kemeny and G.", "year": 1959}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "CVPR, pp", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Individual Choice Behavior", "author": ["R.D. Luce"], "venue": null, "citeRegEx": "Luce,? \\Q1959\\E", "shortCiteRegEx": "Luce", "year": 1959}, {"title": "Non-null ranking models", "author": ["Mallows", "Colin L"], "venue": "i. Biometrika,", "citeRegEx": "Mallows and L.,? \\Q1957\\E", "shortCiteRegEx": "Mallows and L.", "year": 1957}, {"title": "Analyzing and modeling rank data, volume 64", "author": ["Marden", "John I"], "venue": "CRC Press,", "citeRegEx": "Marden and I.,? \\Q1995\\E", "shortCiteRegEx": "Marden and I.", "year": 1995}, {"title": "Consensus ranking under the exponential model", "author": ["Meila", "Marina", "Phadnis", "Kapil", "Patterson", "Arthur", "Bilmes", "Jeff A"], "venue": null, "citeRegEx": "Meila et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meila et al\\.", "year": 2007}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["Mensink", "Thomas", "Verbeek", "Jakob", "Perronnin", "Florent", "Csurka", "Gabriela"], "venue": "In ECCV,", "citeRegEx": "Mensink et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mensink et al\\.", "year": 2012}, {"title": "Costa: Co-occurrence statistics for zero-shot classification", "author": ["Mensink", "Thomas", "Gavves", "Efstratios", "Snoek", "Cees GM"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Mensink et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mensink et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["Miller", "George A"], "venue": "Commun. ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Zero-shot learning with semantic output codes", "author": ["Palatucci", "Mark", "Pomerleau", "Dean", "Hinton", "Geoffrey", "Mitchell", "Tom"], "venue": "In NIPS, pp", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The analysis of permutations", "author": ["Plackett", "Robin L"], "venue": "Applied Statistics, pp", "citeRegEx": "Plackett and L.,? \\Q1975\\E", "shortCiteRegEx": "Plackett and L.", "year": 1975}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["Platt", "John C"], "venue": "In ADVANCES IN LARGE MARGIN CLASSIFIERS,", "citeRegEx": "Platt and C.,? \\Q1999\\E", "shortCiteRegEx": "Platt and C.", "year": 1999}, {"title": "Towards cross-category knowledge propagation for learning visual concepts", "author": ["Qi", "Guo-Jun", "C. Aggarwal", "Rui", "Yong", "Tian", "Chang", "Shiyu", "T. Huang"], "venue": null, "citeRegEx": "Qi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2011}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "Rohrbach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2011}, {"title": "What helps where and why? semantic relatedness for knowledge transfer", "author": ["Rohrbach", "Marcus", "Stark", "Michael", "Szarvas", "Gyorgy", "Gurevych", "Iryna", "Schiele", "Bernt"], "venue": null, "citeRegEx": "Rohrbach et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2010}, {"title": "Discussion of professor Ross\u2019s paper", "author": ["Smith", "B Babington"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Smith and Babington.,? \\Q1950\\E", "shortCiteRegEx": "Smith and Babington.", "year": 1950}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In NIPS, pp", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Rob", "Freeman", "William T"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}], "referenceMentions": [{"referenceID": 10, "context": "More recently, elaborate methods for learning vectorial representations of words have also been proposed (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 105, "endOffset": 172}, {"referenceID": 22, "context": "More recently, elaborate methods for learning vectorial representations of words have also been proposed (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 105, "endOffset": 172}, {"referenceID": 25, "context": "More recently, elaborate methods for learning vectorial representations of words have also been proposed (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 105, "endOffset": 172}, {"referenceID": 10, "context": "Figure 1 (Right) is an example MDS embedding from the representation from (Huang et al., 2012).", "startOffset": 74, "endOffset": 94}, {"referenceID": 10, "context": "Right: MDS embedding from (Huang et al., 2012).", "startOffset": 26, "endOffset": 46}, {"referenceID": 24, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 15, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 30, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 28, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 20, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 9, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 32, "context": "In this paper, we apply non-metric rankings-based representations of semantic similarity to zero-shot classification problems (Palatucci et al., 2009; Lampert et al., 2009; Rohrbach et al., 2010; 2011; Qi et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013).", "startOffset": 126, "endOffset": 281}, {"referenceID": 15, "context": "In the experiment section we demonstrate the advantages of our approach over a numerically-based approach and a deterministic approach using two well-known image databases Animals-with-attributes (Lampert et al., 2009) and CIFAR-10/100 (Krizhevsky, 2009).", "startOffset": 196, "endOffset": 218}, {"referenceID": 16, "context": "We discuss three models: the Mallows (Mallows, 1957), the Plackett-Luce (Plackett, 1975; Luce, 1959; Marden, 1995), and the BabingtonSmith (Joe & Verducci, 1993; Smith, 1950), which we will introduce where they are needed (see (Critchlow, 1985; Critchlow et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 4, "context": "We discuss three models: the Mallows (Mallows, 1957), the Plackett-Luce (Plackett, 1975; Luce, 1959; Marden, 1995), and the BabingtonSmith (Joe & Verducci, 1993; Smith, 1950), which we will introduce where they are needed (see (Critchlow, 1985; Critchlow et al., 1991; Marden, 1995) for more reviews.", "startOffset": 227, "endOffset": 282}, {"referenceID": 1, "context": "The minimization (12) is also known as the Kemeny optimal consensus or aggregation problem (Kemeny, 1959) and is known to be NP-hard (Bartholdi et al., 1989).", "startOffset": 133, "endOffset": 157}, {"referenceID": 19, "context": "However, there are known heuristic methods such as sequential transposition of adjacent items (Critchlow, 1985) or other admissible heuristics (Meila et al., 2007).", "startOffset": 143, "endOffset": 163}, {"referenceID": 2, "context": "Note that if the original classifier is a multinomial logistic regression, the (14) is in fact a direct generalization of logistic regression for K = 1, which is also observed in (Cheng et al., 2010).", "startOffset": 179, "endOffset": 199}, {"referenceID": 30, "context": "Alternatively, we use P (\u03c0|z) = I[\u03c0 = \u03c0 0 ] and a uniform prior P (z), somewhat similar to (Rohrbach et al., 2010).", "startOffset": 91, "endOffset": 114}, {"referenceID": 24, "context": ", (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2013)), the classes from training and test domains are assumed to be distinguishable by a common list of attributes.", "startOffset": 2, "endOffset": 68}, {"referenceID": 15, "context": ", (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2013)), the classes from training and test domains are assumed to be distinguishable by a common list of attributes.", "startOffset": 2, "endOffset": 68}, {"referenceID": 0, "context": ", (Palatucci et al., 2009; Lampert et al., 2009; Akata et al., 2013)), the classes from training and test domains are assumed to be distinguishable by a common list of attributes.", "startOffset": 2, "endOffset": 68}, {"referenceID": 24, "context": "Attribute-based approaches often show excellent empirical performance (Palatucci et al., 2009; Rohrbach et al., 2010).", "startOffset": 70, "endOffset": 117}, {"referenceID": 30, "context": "Attribute-based approaches often show excellent empirical performance (Palatucci et al., 2009; Rohrbach et al., 2010).", "startOffset": 70, "endOffset": 117}, {"referenceID": 30, "context": "Similar arguments can be found in (Rohrbach et al., 2010) or (Mensink et al.", "startOffset": 34, "endOffset": 57}, {"referenceID": 21, "context": ", 2010) or (Mensink et al., 2014).", "startOffset": 11, "endOffset": 33}, {"referenceID": 30, "context": "Similarity information has been used to build a probabilistic zero-shot classifier called direct similarity-based method (DS) (Rohrbach et al., 2010; 2011), which parallels the attribute-based approach from (Lampert et al.", "startOffset": 126, "endOffset": 155}, {"referenceID": 15, "context": ", 2010; 2011), which parallels the attribute-based approach from (Lampert et al., 2009).", "startOffset": 65, "endOffset": 87}, {"referenceID": 9, "context": "More recently, similarity-based approaches using semantic embedding have been proposed (Frome et al., 2013; Socher et al., 2013).", "startOffset": 87, "endOffset": 128}, {"referenceID": 32, "context": "More recently, similarity-based approaches using semantic embedding have been proposed (Frome et al., 2013; Socher et al., 2013).", "startOffset": 87, "endOffset": 128}, {"referenceID": 21, "context": "use a linear combination of pre-trained classifiers for classifying unseen data (Mensink et al., 2014).", "startOffset": 80, "endOffset": 102}, {"referenceID": 15, "context": "We use two datasets 1) Animals with Attributes dataset (Lampert et al., 2009) and 2) CIFAR-100/10 (Torralba et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 30, "context": "Semantic similarity is obtained from WordNet distance, web searches (Rohrbach et al., 2010), word2vec (Mikolov et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 22, "context": ", 2010), word2vec (Mikolov et al., 2013), GloVe (Pennington et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 25, "context": ", 2013), GloVe (Pennington et al., 2014), and from Amazon Mechanical Turk.", "startOffset": 15, "endOffset": 40}, {"referenceID": 22, "context": "Animals CIFAR Feature dimension 8941 4000 Number of training/validation/test samples 21847 / 2448 / 6180 50000 / 50000 / 10000 Number of training/test classes 40 / 10 100 / 10 Linguistic sources WordNet, Wikipedia, Yahoo, YahooImage, Flicker WordNet, word2vec (Mikolov et al., 2013), GloVe (Pennington et al.", "startOffset": 260, "endOffset": 282}, {"referenceID": 25, "context": ", 2013), GloVe (Pennington et al., 2014) Number of surveys from crowd-sourcing 500 500", "startOffset": 15, "endOffset": 40}, {"referenceID": 30, "context": "1)and direct similarity-based method (DS, (Rohrbach et al., 2010)) which is the closest state-of-the-art to our methods that uses classifier scores.", "startOffset": 42, "endOffset": 65}, {"referenceID": 0, "context": "44 (Tables 3 and 4, (Akata et al., 2013)), compared to 0.", "startOffset": 20, "endOffset": 40}, {"referenceID": 30, "context": "Table 1, (Rohrbach et al., 2010)).", "startOffset": 9, "endOffset": 32}, {"referenceID": 32, "context": "Lastly, Figure 4 shows two-class classification accuracy of PR (PL+linguistic sources), DS, and an embedding-based method on select pairs of classes from CIFAR (Figure 3, (Socher et al., 2013)).", "startOffset": 171, "endOffset": 192}], "year": 2015, "abstractText": "In this paper we propose a non-metric rankingbased representation of semantic similarity that allows natural aggregation of semantic information from multiple heterogeneous sources. We apply the ranking-based representation to zeroshot learning problems, and present deterministic and probabilistic zero-shot classifiers which can be built from pre-trained classifiers without retraining. We demonstrate their the advantages on two large real-world image datasets. In particular, we show that aggregating different sources of semantic information, including crowd-sourcing, leads to more accurate classification.", "creator": "LaTeX with hyperref package"}}}