{"id": "1503.00064", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes", "abstract": "This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.", "histories": [["v1", "Sat, 28 Feb 2015 04:26:21 GMT  (6435kb,D)", "http://arxiv.org/abs/1503.00064v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["dahua lin", "chen kong", "sanja fidler", "raquel urtasun"], "accepted": false, "id": "1503.00064"}, "pdf": {"name": "1503.00064.pdf", "metadata": {"source": "CRF", "title": "Generating Multi-Sentence Lingual Descriptions of Indoor Scenes", "authors": ["Dahua Lin", "Sanja Fidler", "Raquel Urtasun"], "emails": ["dhlin@ie.cuhk.edu.hk", "chenk@cs.cmu.edu", "fidler@cs.toronto.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "2 Related Work", "text": "A large body of existing work deals with images and texts in one form or another. The dominant subfield uses complex networks in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al., 2012; Silberer et al., 2013). These kinds of approaches have also been explored in videos to describe visual action models using videos (Ramanathan et al., 2013), or visual concepts from videos described in short sentences (Yu and Siskind, 2013). Another direction is to edit short sentences associated with images to enhance visual recognition tasks (Fidler et al., 2013; Kong et al., 2014)."}, {"heading": "3 Framework Overview", "text": "Our framework for creating descriptions of indoor scenes is based on a central consideration: images and their accompanying descriptions are two different ways of expressing the common semantics underlying them. As shown in Figure 2, it first restores the underlying semantics through a holistic visual analysis (Lin et al., 2013), which leads to a scene diagram that captures detected objects and the spatial relationships between them (e.g., on and near them, etc.) The semantics embodied by a visual scene usually have several aspects. In describing such a complex scene, people often use a paragraph consisting of several sentences, each of which is geared to a particular aspect. To mimic this behavior, this framework transforms the scene diagram into a sequence of semantic trees and yields several tendencies, each of which consists of a semantic tree. To make the results as natural as possible, we apply two strategies: 1) instead of taking into account the number of people in a series of pre-descriptions, whereby we learn from a D to form the number of grammatics."}, {"heading": "4 From RGB-D Images to Semantics", "text": "Faced with an RGB-D image, we extract semantics through holistic visual parsing. In particular, we first analyze the image to obtain the interesting objects, their properties, and their physical relationships, and then construct a scene graphic that provides a coherent summary of these aspects."}, {"heading": "4.1 Holistic Visual Parsing", "text": "To analyze the visual scene, we use a recently proposed approach for recognizing 3D objects in RGBD data (Lin et al., 2013), which we briefly summarize here. First, a series of \"objectivity regions\" are then generated (Carreira and Sminchisescu, 2012) that are encouraged to respect intensity and occlusion limits in 3D. These regions are projected over the depth in 3D and then cuboids are placed tightly around them, with the proviso that they are parallel to the ground. A holistic CRF model is then constructed to jointly judge the classes of cuboids as well as the class of cuboids (e.g. kitchen, bathroom). Therefore, the CRF has a random variable for each cuboid representing its class, and a variable for the scene. To have the ability to remove a bad, non-objective cuboid, we have an additional background state for each cuboid."}, {"heading": "4.2 Scene Graphs", "text": "Based on the extracted visual information, we construct a scene graph that captures objects, their attributes such as color and size, and the relationships between them. Specifically, a scene graph uses nodes to represent objects and their attributes, and edges to represent relationships between nodes. Here, we look at three types of edges: attribute edges that associate objects with their attributes, position edges that represent the positions of objects relative to the scene (e.g., spatial angles), and paired edges that characterize the relative positions between objects (e.g., on the top and bottom). In the face of an image, a series of objects (with class labels) and the scene class are achieved by visual parsing, as explained in the previous section. However, in order to form a scene graph, we need further analysis to extract attributes and relationships between objects."}, {"heading": "5 Generating Lingual Descriptions", "text": "Based on a scene diagram, our framework generates a descriptive paragraph in two steps. First, it transforms the scene diagram into a sequence of semantic trees, each of which focuses on a specific semantic aspect. Then, it produces sentences, one from each semantic tree, following a generative grammar."}, {"heading": "5.1 Semantic Trees", "text": "A semantic tree collects information about what entities are described and what relationships exist between them. Specifically, a semantic tree contains a series of terminal nodes that correspond to the individual entities or their attributes and relation nodes that express relationships with each other.1 We have tried to obtain basic truths for relationships via MTurk (which would allow us to train classifiers instead), but the results of all lots were extremely noisy. Consider a sentence: \"A red box is on a table.\" The corresponding semantic tree can be expressed as ason-top-of (color (box, red), indet (table). This tree has three terminals: \"box,\" \"table\" and \"red.\" The relation node \"color\" describes the relationship between \"box\" and \"red,\" namely \"red\" specifies the color of the \"box.\""}, {"heading": "5.2 Dependencies among Sentences", "text": "In human descriptions, sentences are compiled in such a way that the resulting paragraphs are considered coherent. In particular, the interdependencies between sentences as outlined below play a crucial role in maintaining the coherence of a descriptive paragraph: Logical Order. However, when describing a scene, people avoid using the same prepositional relationship in several sentences. Even if an object is mentioned in several sentences, it usually plays a different role, e.g. \"There is a table near the wall. On the table is a microwave oven.\" Here, \"table\" serves as the source and target in each of these two sentences. Salienz influences the order of sentences. Statistics in (Kong et al., 2014) show that larger objects are often mentioned in a description."}, {"heading": "5.3 From Scene Graphs to Semantic Trees", "text": "Motivated by these considerations, we conceive below a method that turns a scenograph into a sequence of semantic trees, each selected for a set. First, we initialize wsi = w t i = si \u00b7 ci. Here, wsi and w t i are the weights that each determine how likely the i-th object in the next set is to be selected as the source or target; si is a positive value that measures the emphasis of the i-th object, while ci is given by the classifier to indicate its reliability whether it makes a correct prediction of the object class. These weights are updated over the course of the generation. To generate the leading sentence, we first draw a source i with a probability that is proportional to wsi, and create a semantic tree by choosing a relationship, say \"in,\" that would lead to a sentence like \"There is a table in the dining room.\""}, {"heading": "5.4 Grammar and Derivation", "text": "Considering a semantic tree, our frame creates a sentence that follows a generative grammar, namely a map of each semantic relationship to a set of templates (i.e. derivative rules), as shown below: indet -- > a {1} color -- > {2} {1} on-top-of -- > {1} is on {2} on-top-of {2} is {1}. For each relationship node, a template is selected, with a probability proportional to its weight. In the following, an example showing how a sentence from a semantic tree is derived from the root and downward recursively to the terminals. {on-top-of (color (box, red), indet (table)} = > {indet (color (field, red) is derived a sentence according to the above grammar."}, {"heading": "5.5 Learning the Grammar", "text": "The grammar for generating sentences is often set manually in previous work (Barbu et al., 2012; Das et al., 2013), but in this way it is time-consuming, unreliable, and tends to simplify the language too much. In this work, we explore a new approach, that is, we learn grammar from data. The basic idea is to construct a semantic tree from each sentence by linguistic parsing, and then derive the templates by mapping nodes of the semantic tree to parts of the sentence. First, we use the Stanford parser (Toutanova et al., 2003) to obtain a parse tree for each sentence, which is then simplified by a series of filtering operations. For example, we merge nouns phrases from parts of the sentence (e.g. \"fire distinguishers\") into a single node and compress ordinary prepositional phrases (e.g. \"in the left corner\" in a single bar)."}, {"heading": "6 Experimental Evaluation", "text": "We are testing the proposed framework on the basis of the NYU v2 dataset (Silberman et al., 2012), supplemented by additional text descriptions, one for each image. In particular, we are focusing on assessing both the relevance and quality of the descriptions generated."}, {"heading": "6.1 Data Preparation", "text": "The NYU-v2 dataset includes 1449 RGB-D images of indoor scenes (e.g. dining rooms, kitchens, offices), divided into a test set according to the partition used in (Lin et al., 2013) The training set contains 795 scenes, while the test set contains the remaining 654. We use the descriptions from (Kong et al., 2014) collected by asking MTurkers to describe the image to someone who does not see it in order to give him / her a vivid impression of the scene. The number of sentences per description ranges from 1 to 10 with an average of 3. A description has an average of 40 words. We learn generative grammar using the algorithm described in Section 5.5 of the training set of descriptions. We also train the CRF for visual analysis and use it to identify objects and predict their attributes and relationships, according to the procedure described in Section 4.1. These models are then used to create text descriptions for each scene."}, {"heading": "6.2 Performance Metrics", "text": "To evaluate our method, we are looking for metrics that are typically used in machine translation, including BLEU metrics (Papineni et al., 2002) and ROUGE metrics. The BLEU score measures the accuracy of n-grams and is therefore less suited to our goal of linguistic image description, as mentioned in (Mitchell et al., 2012; Das et al., 2013). ROUGE, on the other hand, is a callable n-gram measure that evaluates the information coverage between summaries produced by human commentators and those automatically produced by systems. ROUGE-1 (Unigram) Recall is the best option to compare descriptions based solely on predicted keywords (Das et al., 2013). ROUGE-2 (Bigram) and ROUGE-SU4 (Skip-4 Bigram) are best suited to evaluate summaries in terms of consistency and fluidity."}, {"heading": "6.3 Comparison of Results", "text": "The proposed method of text generation has five optional buttons that control whether the following characteristics are used during generation: (1) promote diversity of sentences by suppressing the mentioned entities and relationships; (2) highlight prominent objects with higher probability; (3) Scene: Leading sentence mentions the class of the scene; (4) Attributes: use colors and sizes to describe objects when they are available; (5) Correlation: use a pronoun to refer to an object when it is mentioned in the previous sentence. Our experiments test the framework among six attribute levels, level 0 to level 5, where the layer-k configuration uses the first k characteristics when the sentences are generated."}, {"heading": "7 Conclusion", "text": "We presented a new framework for generating natural descriptions of indoor scenes. Our framework integrates a CRF model for visual parsing, generative grammar that is automatically learned from training descriptions, and a transformation algorithm for deriving semantic trees from scene graphics that takes into account the interdependencies between sentences. Our experiments show much better descriptions than those generated by a baseline. Such results suggest that high-quality description generation requires not only a reliable understanding of images, but also a sensitive attention to linguistic issues such as diversity, coherence, and logical sentence order."}], "references": [{"title": "Cpmc: Automatic object segmentation using constrained parametric min-cuts", "author": ["J. Carreira", "C. Sminchisescu."], "venue": "TPAMI.", "citeRegEx": "Carreira and Sminchisescu.,? 2012", "shortCiteRegEx": "Carreira and Sminchisescu.", "year": 2012}, {"title": "Semantic segmentation with secondorder pooling", "author": ["J. Carreira", "R. Caseiroa", "J. Batista", "C. Sminchisescu."], "venue": "ECCV12.", "citeRegEx": "Carreira et al\\.,? 2012", "shortCiteRegEx": "Carreira et al\\.", "year": 2012}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick."], "venue": "arXiv:1411.5654.", "citeRegEx": "Chen and Zitnick.,? 2014", "shortCiteRegEx": "Chen and Zitnick.", "year": 2014}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R.F. Doell", "J. J Corso."], "venue": "CVPR.", "citeRegEx": "Das et al\\.,? 2013", "shortCiteRegEx": "Das et al\\.", "year": 2013}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell."], "venue": "arXiv:1411.4389.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Dollar", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig."], "venue": "arXiv:1411.4952.", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences for images", "author": ["A. Farhadi", "M. Hejrati", "M. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth."], "venue": "ECCV.", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "A sentence is worth a thousand pixels", "author": ["S. Fidler", "A. Sharma", "R. Urtasun."], "venue": "CVPR.", "citeRegEx": "Fidler et al\\.,? 2013", "shortCiteRegEx": "Fidler et al\\.", "year": 2013}, {"title": "Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers", "author": ["A. Gupta", "L. Davis."], "venue": "ECCV.", "citeRegEx": "Gupta and Davis.,? 2008", "shortCiteRegEx": "Gupta and Davis.", "year": 2008}, {"title": "A primal-dual message-passing algorithm for approximated large scale structured prediction", "author": ["T. Hazan", "R. Urtasun."], "venue": "NIPS.", "citeRegEx": "Hazan and Urtasun.,? 2010", "shortCiteRegEx": "Hazan and Urtasun.", "year": 2010}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei."], "venue": "arXiv:1412.2306.", "citeRegEx": "Karpathy and Fei.Fei.,? 2014", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel."], "venue": "arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "What are you talking about? text-to-image coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler."], "venue": "CVPR.", "citeRegEx": "Kong et al\\.,? 2014", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Generating natural-language video descriptions using textmined knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R.J. Mooney", "K. Saenko", "S. Guadarrama."], "venue": "AAAI, July.", "citeRegEx": "Krishnamoorthy et al\\.,? 2013", "shortCiteRegEx": "Krishnamoorthy et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton."], "venue": "NIPS, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A. Berg", "T. Berg."], "venue": "CVPR.", "citeRegEx": "Kulkarni et al\\.,? 2011", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A. Berg", "T. Berg", "Y. Choi."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Kuznetsova et al\\.,? 2012", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "T.L. Berg", "Y. Choi."], "venue": "TACL.", "citeRegEx": "Kuznetsova et al\\.,? 2014", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Towards total scene understanding:classification, annotation and segmentation in an automatic framework", "author": ["L. Li", "R. Socher", "L. Fei-Fei."], "venue": "CVPR.", "citeRegEx": "Li et al\\.,? 2009", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Holistic scene understanding for 3d object detection with rgbd cameras", "author": ["D. Lin", "S. Fidler", "R. Urtasun."], "venue": "ICCV.", "citeRegEx": "Lin et al\\.,? 2013", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Visual semantic search: Retrieving videos via complex textual queries", "author": ["D. Lin", "S. Fidler", "C. Kong", "R. Urtasun."], "venue": "CVPR.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz."], "venue": "NIPS.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille."], "venue": "arXiv:1410.1090.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "A joint model of language and perception for grounded attribute learning", "author": ["C. Matuszek", "N. FitzGerald", "L. Zettlemoyer", "L. Bo", "D. Fox."], "venue": "International Conference on Machine Learning.", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "J. Dodge", "A. Goyal", "Kota Yamaguchi", "K. Sratos", "X. Han", "A. Mensch", "A.C. Berg", "T.L. Berg", "H. Daume III."], "venue": "European Chapter of the Association for compu-", "citeRegEx": "Mitchell et al\\.,? 2012", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T. Berg."], "venue": "NIPS.", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["S.K. Papineni", "T. Ward Roukos", "W.J. Zhu."], "venue": "ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning visual representations using images with captions", "author": ["A. Quattoni", "M. Collins", "T. Darrell."], "venue": "CVPR07.", "citeRegEx": "Quattoni et al\\.,? 2007", "shortCiteRegEx": "Quattoni et al\\.", "year": 2007}, {"title": "Video event understanding using natural language descriptions", "author": ["V. Ramanathan", "P. Liang", "L. Fei-Fei."], "venue": "ICCV.", "citeRegEx": "Ramanathan et al\\.,? 2013", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2013}, {"title": "Rgb-(d) scene labeling: Features and algorithms", "author": ["X. Ren", "L. Bo", "D. Fox."], "venue": "CVPR.", "citeRegEx": "Ren et al\\.,? 2012", "shortCiteRegEx": "Ren et al\\.", "year": 2012}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele."], "venue": "ICCV.", "citeRegEx": "Rohrbach et al\\.,? 2013", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Distributed message passing for large scale graphical models", "author": ["A. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun."], "venue": "CVPR.", "citeRegEx": "Schwing et al\\.,? 2011", "shortCiteRegEx": "Schwing et al\\.", "year": 2011}, {"title": "Models of semantic representation with visual attributes", "author": ["C. Silberer", "V. Ferrari", "M. Lapata."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Silberer et al\\.,? 2013", "shortCiteRegEx": "Silberer et al\\.", "year": 2013}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "P. Kohli", "D. Hoiem", "R. Fergus."], "venue": "ECCV.", "citeRegEx": "Silberman et al\\.,? 2012", "shortCiteRegEx": "Silberman et al\\.", "year": 2012}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei."], "venue": "CVPR.", "citeRegEx": "Socher and Fei.Fei.,? 2010", "shortCiteRegEx": "Socher and Fei.Fei.", "year": 2010}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning."], "venue": "HLT-NAACL.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan."], "venue": "arXiv:1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba."], "venue": "CVPR.", "citeRegEx": "Xiao et al\\.,? 2010", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "III H. Daum\u00e9", "Y. Aloimonos."], "venue": "EMNLP, pages 444\u2013454.", "citeRegEx": "Yang et al\\.,? 2011", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Grounded language learning from video described with sentences", "author": ["H. Yu", "J.M. Siskind."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Yu and Siskind.,? 2013", "shortCiteRegEx": "Yu and Siskind.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "templates (Barbu et al., 2012; Krishnamoorthy et al., 2013), which often result in tedious descriptions.", "startOffset": 10, "endOffset": 59}, {"referenceID": 25, "context": "Another line of work solves the description generation problem via retrieval, where a description for an image is borrowed from semantically most similar image from the training set (Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 182, "endOffset": 226}, {"referenceID": 6, "context": "Another line of work solves the description generation problem via retrieval, where a description for an image is borrowed from semantically most similar image from the training set (Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 182, "endOffset": 226}, {"referenceID": 11, "context": "Recently, the field has witnessed a boom in generating image descriptions via deep neural networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Chen and Zitnick, 2014) which are able to both, learn a weak language model as well as generalize description to unseen images.", "startOffset": 99, "endOffset": 170}, {"referenceID": 2, "context": "Recently, the field has witnessed a boom in generating image descriptions via deep neural networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Chen and Zitnick, 2014) which are able to both, learn a weak language model as well as generalize description to unseen images.", "startOffset": 99, "endOffset": 170}, {"referenceID": 14, "context": "The results have been impressive, perhaps partly due to powerful representation on the image side (Krizhevsky et al., 2012).", "startOffset": 98, "endOffset": 123}, {"referenceID": 33, "context": "To test the effectiveness of our approach, we construct an augmented dataset based on NYURGBD (Silberman et al., 2012), where each scene is associated with up to 5 natural language descriptions from human annotators.", "startOffset": 94, "endOffset": 118}, {"referenceID": 27, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 18, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 34, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 8, "context": "The dominant subfield exploits text in the form of tags or short sentences as weak labels to learn visual models (Quattoni et al., 2007; Li et al., 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al.", "startOffset": 113, "endOffset": 202}, {"referenceID": 23, "context": ", 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al., 2012; Silberer et al., 2013).", "startOffset": 80, "endOffset": 126}, {"referenceID": 32, "context": ", 2009; Socher and Fei-Fei, 2010; Gupta and Davis, 2008), as well as attributes (Matuszek et al., 2012; Silberer et al., 2013).", "startOffset": 80, "endOffset": 126}, {"referenceID": 28, "context": "This type of approaches have also been explored in videos to learn visual action models from textual summaries of videos (Ramanathan et al., 2013), or learning visual concepts from videos described with short sentences (Yu and Siskind, 2013).", "startOffset": 121, "endOffset": 146}, {"referenceID": 39, "context": ", 2013), or learning visual concepts from videos described with short sentences (Yu and Siskind, 2013).", "startOffset": 80, "endOffset": 102}, {"referenceID": 7, "context": "Another direction is to exploit short sentences associated with images in order to improve visual recognition tasks (Fidler et al., 2013; Kong et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 12, "context": "Another direction is to exploit short sentences associated with images in order to improve visual recognition tasks (Fidler et al., 2013; Kong et al., 2014).", "startOffset": 116, "endOffset": 156}, {"referenceID": 21, "context": "Just recently, an interested problem domain was introduced in (Malinowski and Fritz, 2014) with the aim to learn how to answer questions about images from Q&A examples.", "startOffset": 62, "endOffset": 90}, {"referenceID": 20, "context": "In (Lin et al., 2014), the authors address visual search with complex natural lingual queries.", "startOffset": 3, "endOffset": 21}, {"referenceID": 25, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 6, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 16, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 30, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 38, "context": "The most popular approach has been to retrieve a sentence from a large corpus based on similarity of visual content (Ordonez et al., 2011; Farhadi et al., 2010; Kuznetsova et al., 2012; Rohrbach et al., 2013; Yang et al., 2011).", "startOffset": 116, "endOffset": 227}, {"referenceID": 6, "context": "However, typically such approaches adopt a limited image representation such as triplets actionobject-scene (Farhadi et al., 2010).", "startOffset": 108, "endOffset": 130}, {"referenceID": 17, "context": "In (Kuznetsova et al., 2014) the authors go further by only learning phrases from related images.", "startOffset": 3, "endOffset": 28}, {"referenceID": 11, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 36, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 22, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 4, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 5, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 2, "context": "Parallel to our work, there has been a recent boom in image description generation with deep networks (Kiros et al., 2014; Karpathy and FeiFei, 2014; Vinyals et al., 2014; Mao et al., 2014; Donahue et al., 2014; Fang et al., 2014; Chen and Zitnick, 2014).", "startOffset": 102, "endOffset": 254}, {"referenceID": 13, "context": "(Barbu et al., 2012; Krishnamoorthy et al., 2013) output a video description in the form of subject-action-object.", "startOffset": 0, "endOffset": 49}, {"referenceID": 3, "context": "In (Das et al., 2013), \u201cconcept detectors\u201d are formed, which are detectors for combined object and action or scene in a particular chunk of a video.", "startOffset": 3, "endOffset": 21}, {"referenceID": 30, "context": "(Rohrbach et al., 2013) predicts semantic representations from low-level video features and uses machine translation techniques to generate a sentence.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "The closest to our work is (Kulkarni et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014) which, like us, is able to describe objects, their modifiers, and prepositions between objects.", "startOffset": 27, "endOffset": 98}, {"referenceID": 24, "context": "The closest to our work is (Kulkarni et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014) which, like us, is able to describe objects, their modifiers, and prepositions between objects.", "startOffset": 27, "endOffset": 98}, {"referenceID": 17, "context": "The closest to our work is (Kulkarni et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014) which, like us, is able to describe objects, their modifiers, and prepositions between objects.", "startOffset": 27, "endOffset": 98}, {"referenceID": 15, "context": "In such a setting, describing every detectable object and all relations between them as in (Kulkarni et al., 2011) would generate prohibitively long, complex and unnatural descriptions.", "startOffset": 91, "endOffset": 114}, {"referenceID": 19, "context": "As shown in Figure 2, given an image, it first recovers the underlying semantics through holistic visual analysis (Lin et al., 2013), which results in a scene graph that captures detected objects and the spatial relations between them (e.", "startOffset": 114, "endOffset": 132}, {"referenceID": 19, "context": "To parse the visual scene we use a recently proposed approach for 3D object detection in RGBD data (Lin et al., 2013).", "startOffset": 99, "endOffset": 117}, {"referenceID": 0, "context": "First, a set of \u201cobjectness\u201d regions are generated following (Carreira and Sminchisescu, 2012), which are encouraged to respect intensity as well as occlusion boundaries in 3D.", "startOffset": 61, "endOffset": 94}, {"referenceID": 37, "context": "To incorporate global information, a unary potential over the scene label is computed by means of a logistic on top of the scene classification score (Xiao et al., 2010).", "startOffset": 150, "endOffset": 169}, {"referenceID": 1, "context": "Appearance-based classifiers, including CPMC-o2 (Carreira et al., 2012), superpixel scores (Ren et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 29, "context": ", 2012), superpixel scores (Ren et al., 2012) are used to classify cuboids into a pre-defined set of object classes.", "startOffset": 27, "endOffset": 45}, {"referenceID": 14, "context": "In this paper, we additionally use CNN (Krizhevsky et al., 2012) features for classification.", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "The CRF weights to combine the potentials are learned with a primal dual learning framework (Hazan and Urtasun, 2010), and inference of class labels is done with an approximated algorithm (Schwing et al.", "startOffset": 92, "endOffset": 117}, {"referenceID": 31, "context": "The CRF weights to combine the potentials are learned with a primal dual learning framework (Hazan and Urtasun, 2010), and inference of class labels is done with an approximated algorithm (Schwing et al., 2011).", "startOffset": 188, "endOffset": 210}, {"referenceID": 12, "context": "Object saliency: The dataset of (Kong et al., 2014) contains alignment between the nouns in a sentence and the visual objects in the scene.", "startOffset": 32, "endOffset": 51}, {"referenceID": 12, "context": "The statistics in (Kong et al., 2014) shows that bigger objects are often mentioned earlier on in a description and co-referred across sentences, e.", "startOffset": 18, "endOffset": 37}, {"referenceID": 3, "context": "The grammar for generating sentences are often specified manually in previous work (Barbu et al., 2012; Das et al., 2013).", "startOffset": 83, "endOffset": 121}, {"referenceID": 35, "context": "First, we use the Stanford parser (Toutanova et al., 2003) to obtain a parse tree for each sentence, which is then simplified through a series of filtering operations.", "startOffset": 34, "endOffset": 58}, {"referenceID": 33, "context": "We test the proposed framework on the NYU-v2 dataset (Silberman et al., 2012) augmented with an additional set of textual descriptions, one for each image.", "startOffset": 53, "endOffset": 77}, {"referenceID": 19, "context": "These images are divided into a training a testing set, following the partition used in (Lin et al., 2013).", "startOffset": 88, "endOffset": 106}, {"referenceID": 12, "context": "We use the descriptions from (Kong et al., 2014) which were collected by asking MTurkers to describe the image to someone who does not see it in order to provide him/her with a vidid impression of the scene.", "startOffset": 29, "endOffset": 48}, {"referenceID": 26, "context": "These include the BLEU (Papineni et al., 2002) and ROUGE metrics among others.", "startOffset": 23, "endOffset": 46}, {"referenceID": 24, "context": "The BLEU score measures precision on n-grams, and is thus less suitable for our goal of lingual image description, as already noted in (Mitchell et al., 2012; Das et al., 2013).", "startOffset": 135, "endOffset": 176}, {"referenceID": 3, "context": "The BLEU score measures precision on n-grams, and is thus less suitable for our goal of lingual image description, as already noted in (Mitchell et al., 2012; Das et al., 2013).", "startOffset": 135, "endOffset": 176}, {"referenceID": 3, "context": "ROUGE-1 (unigram) recall is the best option to use for comparing descriptions based only on predicted keywords (Das et al., 2013).", "startOffset": 111, "endOffset": 129}, {"referenceID": 3, "context": "We use the ROUGE metrics following (Das et al., 2013) who uses it to evaluate lingual video summarization.", "startOffset": 35, "endOffset": 53}, {"referenceID": 37, "context": "We use (Xiao et al., 2010) to compute the kernels.", "startOffset": 7, "endOffset": 26}], "year": 2015, "abstractText": "This paper proposes a novel framework for generating lingual descriptions of indoor scenes. Whereas substantial efforts have been made to tackle this problem, previous approaches focusing primarily on generating a single sentence for each image, which is not sufficient for describing complex scenes. We attempt to go beyond this, by generating coherent descriptions with multiple sentences. Our approach is distinguished from conventional ones in several aspects: (1) a 3D visual parsing system that jointly infers objects, attributes, and relations; (2) a generative grammar learned automatically from training text; and (3) a text generation algorithm that takes into account the coherence among sentences. Experiments on the augmented NYU-v2 dataset show that our framework can generate natural descriptions with substantially higher ROGUE scores compared to those produced by the baseline.", "creator": "TeX"}}}