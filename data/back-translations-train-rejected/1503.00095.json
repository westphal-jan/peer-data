{"id": "1503.00095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "Task-Oriented Learning of Word Embeddings for Semantic Relation Classification", "abstract": "We present a novel learning method for word embeddings designed for relation classification. Our word embeddings are trained by predicting words between noun pairs using lexical relation-specific features on a large unlabeled corpus. This allows us to explicitly incorporate relation-specific information into the word embeddings. The learned word embeddings are then used to construct feature vectors for a relation classification model. On a well-established semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models without syntactic information or manually constructed external resources. Furthermore, when incorporating external resources, our method outperforms the previous state of the art.", "histories": [["v1", "Sat, 28 Feb 2015 07:59:59 GMT  (193kb)", "https://arxiv.org/abs/1503.00095v1", null], ["v2", "Sun, 17 May 2015 00:04:42 GMT  (193kb)", "http://arxiv.org/abs/1503.00095v2", null], ["v3", "Mon, 22 Jun 2015 13:48:30 GMT  (195kb)", "http://arxiv.org/abs/1503.00095v3", "The Nineteenth Conference on Computational Natural Language Learning (CoNLL 2015)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuma hashimoto", "pontus stenetorp", "makoto miwa", "yoshimasa tsuruoka"], "accepted": false, "id": "1503.00095"}, "pdf": {"name": "1503.00095.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hassy@logos.t.u-tokyo.ac.jp", "tsuruoka@logos.t.u-tokyo.ac.jp", "pontus@stenetorp.se", "makoto-miwa@toyota-ti.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.00 095v 3 [cs.C L] 22 Jun 2015"}, {"heading": "1 Introduction", "text": "Automatic classification of semantic relationships has a variety of applications, such as extracting information and building semantic networks (Girju et al., 2007; Hendrickx et al., 2010). A traditional approach to classifying relationships is to train classifiers based on different types of traits with human-annotated class markers (Rink and Harabagiu, 2010). Carefully designed traits derived from lexical, syntactic and semantic resources play an important role in achieving high accuracy for semantic relationship classification (Rink and Harabagiu, 2010).In recent years, there has been a growing interest in word embedding as an alternative to traditional handcrafted traits, which depict tactical embeddings as real-valued vectors and capture syntactical sim-ilarity between words. For example, word2vec1 et al al al al al al al al al al al al al al al al al al al., 2013b is a well-established tool for word learning."}, {"heading": "2 Related Work", "text": "A traditional approach to relationship classification is to train classifiers in a controlled manner based on a variety of traits, including lexical word-bed traits and traits based on syntactic parse trees. However, in syntactic parse trees, paths between constituency and dependency tree target groups have proven useful (Bunescu and Mooney, 2005; Zhang et al., 2006). However, in the joint task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted traits, which were then used to train a support vector machine (SVM). Recently, word-bedding became popular as an alternative to handcrafted traits (Collobert et al., 2011). However, one of the limitations is that word-bedding is usually learned by predicting an existential target in its context, which leads to the inclusion of local information (Leberg 2014 and Leberg)."}, {"heading": "3 Relation Classification Using Word Embedding-based Features", "text": "We propose a novel method of embedding words that is designed for classifying relationships; word embedding is trained by predicting each word between pairs of nouns, using the corresponding low-level characteristics for classifying relationships. To classify the relationships between pairs of nouns, the most important characteristics are derived from the pairs themselves and the words between and around the pairs (Hendrickx et al., 2010). For example, in the sentence in Figure 1 (b) there is a cause-effect relationship between the two nouns conflicts and players. To classify the relationship, the most common characteristics are the pair of nouns (conflicts, players), the words between the pair of nouns (caused by), the words before the pair (the, external) and the words after the pair (play, tiles, to,...). As Rink and Harabagiu (2010) have shown, the words between the pair of nouns are the most important characteristics we have assigned to the pair of nouns (the first of these nouns)."}, {"heading": "3.1 Learning Word Embeddings", "text": "Suppose there are a number of words, and N is also a number of terms, but they only contain two terms: the terms we use in another context, and the terms we use in another context. (...) The terms we use in another context are: the terms we use in another context, and the terms we use in another context. (...) The terms we use in another context are in another context. (...) The terms we use in another context are in another context. (...) The terms we use in another context. (...) The terms we use in another context. (...) The terms we use in another context are in another context. (...) The terms we use in another context. (...) The terms we use in another context are in another context. (...) The terms we use in another context. (...) The terms we use in another context are in another context. (...)"}, {"heading": "3.2 Constructing Feature Vectors", "text": "In fact, it is such that we are able to see ourselves in a position to feel as we have experienced it in the past. (iW) \"W's\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" W's \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s"}, {"heading": "3.3 Supervised Learning", "text": "For each k-th training sample with a corresponding label lk under L, we calculate a conditional probability that its characteristics vector ek: p (lk | ek) = exp (o (lk), L (o (i), 8) where it is defined as o = sec + s, and S (lk | ek).RL \u00d7 1 are the softmax parameters. o (i) is the i-th element of o. We define the objective function as Jlabeled = K = 1log (p (lk | ek))."}, {"heading": "4 Experimental Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Training Data", "text": "In preparation, we used a snapshot of Wikipedia2 from November 2013. First, we extracted 80 million sentences from the original Wikipedia file, and then used Enju3 (Miyao and Tsujii, 2008) to automatically assign word building blocks. From the POS tags, we used NN, NNS, NNP, or NNPS to localize noun pairs in the corpus. Then, we collected training data by listing noun pairs and the words between, before, and after the noun pairs. A noun pair was omitted if the number of words between the pairs was greater than 10, and then collected 1.4 billion pairs of nouns and their contexts 4. We used the 300,000 most common words and 300,000 most common nouns, and treated words outside the vocabulary as special UNK tokens."}, {"heading": "4.2 Initialization and Optimization", "text": "We initialized the embedding matrices N and W with zero-mean Gaussian noise with a variance of 1 d W \u0442 and b were zero-initialized; the model parameters were optimized by maximizing the objective function in eq. (3) by stochastic gradient ascent; the learning rate was set to \u03b1 and linearly decreased to 0 during training, as described in Mikolov et al. (2013a); the hyperparameters are embedding dimensionality d, context size c, the number of negative samples k, the initial learning rate \u03b1 and mout, the number of words outside the substance pairs; for hyperparameter adjustment, we first set \u03b1 to 0.025 and Mout to 5 and then set d to {50, 100, 300}, c to {1, 2, 3}, and k to {5, 15, 25}."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Evaluation Dataset", "text": "We evaluated our method using the SemEval 2010 Task 8 dataset set5 (Hendrickx et al., 2010), which includes predicting the semantic relationships between pairs of noun in their context. The dataset, which contains 8,000 training samples and 2,717 test samples, defines nine classes (cause-effect, entity-origin, etc.) for ordered relationships and one class (others) for other relationships. Thus, the task can be treated as a 19-class classification task. The following are two examples from the training set. (a) Financial [stress] E1 is one of the main causes of [divorce] E2 (b) The [burst] E1 was classified by Waterhammer [pressure] E2Training example (a) as CauseEffect (E1, E2), meaning that E2 is an effect caused by E1, while Training example (b) is classified as cause-effect (E2, E1), which corresponds to the inversion of E2, E1-effect."}, {"heading": "5.2 Models", "text": "In order to empirically investigate the performance of our proposed method, we compared it with several baselines and previously proposed models."}, {"heading": "5.2.1 Random and word2vec Initialization", "text": "Rand-Init. The first baseline is RelEmb itself, but without applying the learning method to the blank corpus. In other words, we train the Softmax classifier from Section 3.3 to the labeled training data with randomly initialized model parameters. The second baseline is RelEmb using Word embedding, which were learned from word2vec. More specifically, we initialize the embedding matrices N and W with the word2vec embedding. Relating to our method, word2vec has a series of weight vectors that are similar to W when we are trained with negative sampling, and we use these weight vectors as a substitute for W. We trained the word2vec embedding with the CBOW model with subsampling on the total Wikipedia corpus. As with our experimental settings we fix the learning rate to http: / / cdogs.oogle.com 0,00000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,"}, {"heading": "5.2.2 SVM-Based Systems", "text": "A simple approach to classifying relationships is to use SVMs with standard binary word combinations. Word combinations included the noun pairs and words between, before and after the pairs, and we used LIBLINEAR6 as the classifier."}, {"heading": "5.2.3 Neural Network Models", "text": "Socher et al. (2012) used Recursive Neural Network (RNN) models to classify relationships. Afterwards, Ebrahimi et al. (2015) and Hashimoto et al. (2013) presented RNN models to better manage relationships, based on syntactic parse trees. Yu et al. (2014) presented their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the most powerful of which were FCMEMB and FCMFULL. The former uses only Word embedding information and the latter does not rely on dependency paths and NE features, in addition to word embedding. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernymen. Remarkably, in terms of RNN-based methods, the CNN model is not based on parse trees. More recently, Santos and CNN have extended the best model to include CNN in 2015 (CR)."}, {"heading": "5.3 Results and Discussion", "text": "The values of the test set for SemEval 2010 Task 8 are shown in Table 1. RelEmb achieves 82.8% of F1, which is better than almost all models compared and comparable to previous state of the art, with the exception of CR-CNNBest. Note that RelEmb does not use external semantic characteristics and syntactic parse characteristics. 7 In addition, 6http: / / www.csie.ntu.edu.tw / \u02dc cjlin / liblinear /. 7While we use a POS tagger to locate noun pairs, RelEmbFULL does not use explicit POS characteristics in the managed Learn-RelEmbFULL and reaches 83.5% of F1. We calculated a confidence interval (82.0, 84.9) (p < 0.05) using bootstrap resampling (Noreen, 1989)."}, {"heading": "5.3.1 Comparison with the Baselines", "text": "RelEmb significantly exceeds not only the edge-init baseline, but also the W2V-init baseline. These results show that our task-specific word embeddings are more useful than those trained in window-based contexts. One point we want to emphasize is that baselines are unexpectedly strong. As Wang and Manning (2012) noted, we should carefully implement strong baselines and see if complex models can surpass these baselines."}, {"heading": "5.3.2 Comparison with SVM-Based Systems", "text": "RelEmb performs much better than word-based SVM. Not surprisingly, we use a large uncommented corpus and embedding with a large number of parameters. RelEmb also performs better than Rink and Harabagiu's SVM system (2010), which shows the effectiveness of our task-specific word embedding, although our only requirement is a large uncommented corpus and a POS tagger."}, {"heading": "5.3.3 Comparison with Neural Network Models", "text": "RelEmb exceeds the RNN models. In our preliminary experiments, we found some unwanted parse trees in the calculation of vector representations using RNN-based models and searches for step-by-step parsing errors that could affect the performance of the RNN models. FCMFULL, which relies on dependency paths and NE characteristics, achieves a better value than RElEmb. Without such characteristics, RelEmb outperforms FCMEMB by a large margin. By including external resources, RelEmbFULL outperforms FCMFULL.RelEmb in comparison with CR-CNNOther, although our method is less computationally expensive than CR-CNNOther. When classifying an instance, the number of floating number multiplications is 4d (2 + c) L in our method, as our method only requires a matrix vector product for the softmax classification class CN."}, {"heading": "5.4 Analysis on Training Settings", "text": "We analyse the training sequence with the focus on RelEmb."}, {"heading": "5.4.1 Effects of Tuning Hyperparameters", "text": "In Tables 2 and 3 we show how the tuning of the hyperparameters of our method and word2vec influences the classification results by performing 10-fold cross-validations on the training set. Results for d = 50, 100 show that RelEmb benefits from relatively large context sizes. In contrast to these trends, the n-gram embedding in RelEmb captures richer information by setting c to 3, compared to setting c to 1. RelEmbFULL also achieves a relatively large number of negative samples slightly. Contrary to these trends, the score with d = 300 does not improve. We use the best setting (c = 3, d = 100, k = 25) for the remaining analysis. We note that RelEmbFULL achieves an F1 score of 82.5, but the tactical capture of similar experiments for the W2V index baseline, and the results in this comparison are represented only by the comparability of the results in Table 3, not by the similarity of the results in synchronization."}, {"heading": "5.4.2 Ablation Tests", "text": "As described in Section 3.2, we link three types of trait vectors, gin, and gout, for supervised learning. Table 4 shows classification values for ablation tests using a 10-fold cross-validation. We also provide a score using a simplified version of gin, in which the trait vector g \"in is calculated by averaging the word embeddings [W (wini); W (w in i)] of the words between the noun pairs. This trait vector g\" in then serves as a trait with dictions.Table 4 clearly shows that the averaged n-gram embeddings contribute most to the semantic classification performance. The difference between the values of gin and g \"in shows the effectiveness of our averaged n-gram embeddings."}, {"heading": "5.4.3 Effects of Dropout", "text": "For the supervised learning step, we use the dropout to regularize our model. Without dropout, our performance drops from 82.2% to 81.3% of the F1 value in the training set through 10-fold cross-validation."}, {"heading": "5.4.4 Performance on a Word Similarity Task", "text": "As described in Section 3.1, we have the noun-specific embeddings N as well as the standard word embeddings W. We evaluated the learned embeddings using a semantic WordSim353 (Finkelstein et al., 2001) wordlevel evaluation task. This data set consists of 353 pairs of nouns and each pair has an average human evaluation that corresponds to a semantic similarity value. The evaluation is done by measuring the rank correlation between the human evaluation and the cosmic similarity values of the embeddings. Table 5 shows the evaluation results. We used the best embeddings mentioned in Tables 2 and 3, as our method is designed for classifying relationships and it is not clear how the hyperparameters for the word similarity problem can be matched. As shown in the results table, the nounspecific embeddings perform better than the standard embeddings in our method, which indicate that the specific embeddings are more useful than the specific ones approximately."}, {"heading": "5.5 Qualitative Analysis on the Embeddings", "text": "Using the n-gram embeddings hi in Equation (5), we check which n-grams are relevant for each relationship class after the supervised learning step of RelEmb. If the context size is c 3, we can use a maximum of 7 grams. The learned weight matrix S in Section 3.3 is used to determine the most relevant n-grams for each class. Specifically, for each n-gram embedment (n = 1, 3) in the training set, we calculate the point product between the n-gram embeddings and the corresponding components in S. Then we select the pairs of n-grams and class labels with the highest values. In Table 6, we show the five best n-grams for six classes. These results clearly show that the n-gram embeddings capture important syntactical patterns that are useful for the task of classifying relationships."}, {"heading": "6 Conclusions and Future Work", "text": "We have introduced a method for learning word embedding that has been developed specifically for classifying relationships. Word embedding is trained using large, unlabeled corpora to capture lexical characteristics for classifying relationships. On an established semantic classification task, our method significantly exceeds the baseline based on word2vec. Our method also compares favorably with previous state-of-the-art models based on syntactic parsers and external semantic resources, although our method only requires access to an uncommented corpus and a POS tagger. In future work, we will investigate how well our method works on other domains and datasets, and how relational labels can help when learning embedding in a semi-supervised learning environment."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments and suggestions."}], "references": [{"title": "Tailoring Continuous Word Representations for Dependency Parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space", "author": ["Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Event Role Extraction using DomainRelevant Word Representations", "author": ["Boros et al.2014] Emanuela Boros", "Romaric Besan\u00e7on", "Olivier Ferret", "Brigitte Grau"], "venue": null, "citeRegEx": "Boros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boros et al\\.", "year": 2014}, {"title": "A Shortest Path Dependency Kernel for Relation Extraction", "author": ["Bunescu", "Mooney2005] Razvan Bunescu", "Raymond Mooney"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language", "citeRegEx": "Bunescu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2005}, {"title": "Feature Embedding for Dependency Parsing", "author": ["Chen et al.2014] Wenliang Chen", "Yue Zhang", "Min Zhang"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Broad-Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger", "author": ["Ciaramita", "Yasemin Altun"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying Relations by Ranking with Convolutional Neural Networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the Joint Conference of the 53rd Annual Meeting of the Association", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Chain Based RNN for Relation Classification", "author": ["Ebrahimi", "Dou2015] Javid Ebrahimi", "Dejing Dou"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Ebrahimi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ebrahimi et al\\.", "year": 2015}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["Gabrilovich Evgenly", "Matias Yossi", "Rivlin Ehud", "Solan Zach", "Wolfman Gadi", "Ruppin Eytan"], "venue": "In Proceedings of the Tenth International World Wide Web", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals", "author": ["Girju et al.2007] Roxana Girju", "Preslav Nakov", "Vivi Nastase", "Stan Szpakowicz", "Peter Turney", "Deniz Yuret"], "venue": "In Proceedings of the Fourth International Workshop", "citeRegEx": "Girju et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Girju et al\\.", "year": 2007}, {"title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "author": ["Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural", "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Revisiting Embedding Features for Simple Semi-supervised Learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Simple Customization of Recursive Neural Networks for Semantic Relation Classification", "author": ["Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama"], "venue": "In Proceedings of the 2013 Confer-", "citeRegEx": "Hashimoto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2013}, {"title": "Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of the 2014 Conference", "citeRegEx": "Hashimoto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "SemEval-2010 Task 8: Multi-Way Classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep Recursive Neural Networks for Compositionality in Language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Prior Disambiguation of Word Tensors for Constructing Sentence Vectors", "author": ["Kartsaklis", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kartsaklis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2013}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Workshop at the International Conference on Learning Representations", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Feature Forest Models for Probabilistic HPSG Parsing", "author": ["Miyao", "Tsujii2008] Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "Computational Linguistics,", "citeRegEx": "Miyao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Miyao et al\\.", "year": 2008}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction", "author": ["Nguyen", "Grishman2014] Thien Huu Nguyen", "Ralph Grishman"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "ComputerIntensive Methods for Testing Hypotheses: An Introduction", "author": ["Eric W. Noreen"], "venue": null, "citeRegEx": "Noreen.,? \\Q1989\\E", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Global Belief Recursive Neural Networks", "author": ["Paulus et al.2014] Romain Paulus", "Richard Socher", "Christopher D Manning"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Paulus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources", "author": ["Rink", "Harabagiu2010] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Rink et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2010}, {"title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification", "author": ["Tang et al.2014] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Compu-", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Turian et al.2010] Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification", "author": ["Wang", "Manning2012] Sida Wang", "Christopher Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Factor-based Compositional Embedding Models", "author": ["Yu et al.2014] Mo Yu", "Matthew R. Gormley", "Mark Dredze"], "venue": "In Proceedings of Workshop on Learning Semantics at the 2014 Conference on Neural Information Processing Systems", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Relation Classification via Convolutional Deep Neural Network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features", "author": ["Zhang et al.2006] Min Zhang", "Jie Zhang", "Jian Su", "GuoDong Zhou"], "venue": "In Proceedings of the 21st International Conference on Computational Linguis-", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Automatic classification of semantic relations has a variety of applications, such as information extraction and the construction of semantic networks (Girju et al., 2007; Hendrickx et al., 2010).", "startOffset": 151, "endOffset": 195}, {"referenceID": 16, "context": "Automatic classification of semantic relations has a variety of applications, such as information extraction and the construction of semantic networks (Girju et al., 2007; Hendrickx et al., 2010).", "startOffset": 151, "endOffset": 195}, {"referenceID": 31, "context": "While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 0, "context": ", 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification.", "startOffset": 104, "endOffset": 209}, {"referenceID": 2, "context": ", 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification.", "startOffset": 104, "endOffset": 209}, {"referenceID": 4, "context": ", 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification.", "startOffset": 104, "endOffset": 209}, {"referenceID": 13, "context": ", 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification.", "startOffset": 104, "endOffset": 209}, {"referenceID": 35, "context": "For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 138, "endOffset": 184}, {"referenceID": 6, "context": "Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011).", "startOffset": 89, "endOffset": 113}, {"referenceID": 12, "context": "On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of hand-crafted features which were then used to train a Support Vector Machine (SVM).", "startOffset": 33, "endOffset": 57}, {"referenceID": 12, "context": "On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of hand-crafted features which were then used to train a Support Vector Machine (SVM).", "startOffset": 33, "endOffset": 84}, {"referenceID": 3, "context": "Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this limitation. Le and Mikolov (2014) integrated paragraph information into a word2vec-based model, which allowed them to capture paragraphlevel information.", "startOffset": 90, "endOffset": 420}, {"referenceID": 0, "context": "For dependency parsing, Bansal et al. (2014) and Chen et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 0, "context": "For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependency-based context information into their embeddings.", "startOffset": 24, "endOffset": 68}, {"referenceID": 0, "context": "For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependency-based context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts.", "startOffset": 24, "endOffset": 198}, {"referenceID": 0, "context": "For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependency-based context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features.", "startOffset": 24, "endOffset": 304}, {"referenceID": 0, "context": "For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependency-based context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction.", "startOffset": 24, "endOffset": 507}, {"referenceID": 0, "context": "For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependency-based context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction.", "startOffset": 24, "endOffset": 598}, {"referenceID": 0, "context": "For dependency parsing, Bansal et al. (2014) and Chen et al. (2014) found ways to improve performance by integrating dependency-based context information into their embeddings. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction. Another kind of task-specific word embeddings was proposed by Tang et al. (2014), which used sentiment labels on tweets to adapt word embeddings for a sentiment analysis tasks.", "startOffset": 24, "endOffset": 750}, {"referenceID": 16, "context": "In general, to classify relations between pairs of nouns the most important features come from the pairs themselves and the words between and around the pairs (Hendrickx et al., 2010).", "startOffset": 159, "endOffset": 183}, {"referenceID": 14, "context": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its partof-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013).", "startOffset": 170, "endOffset": 313}, {"referenceID": 15, "context": "This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its partof-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013).", "startOffset": 170, "endOffset": 313}, {"referenceID": 21, "context": "When training we employ several procedures introduced by Mikolov et al. (2013b), namely, negative sampling, a modified unigram noise distribution and subsampling.", "startOffset": 57, "endOffset": 80}, {"referenceID": 8, "context": "\u03b8 = (N,W,W\u0303,S, s) is the set of parameters and Jlabeled is maximized using AdaGrad (Duchi et al., 2011).", "startOffset": 83, "endOffset": 103}, {"referenceID": 17, "context": "We have found that dropout (Hinton et al., 2012) is helpful in preventing our model from overfitting.", "startOffset": 27, "endOffset": 48}, {"referenceID": 27, "context": "Recently dropout has been applied to deep neural network models for natural language processing tasks and proven effective (Irsoy and Cardie, 2014; Paulus et al., 2014).", "startOffset": 123, "endOffset": 168}, {"referenceID": 29, "context": "Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014).", "startOffset": 169, "endOffset": 233}, {"referenceID": 33, "context": "Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014).", "startOffset": 169, "endOffset": 233}, {"referenceID": 29, "context": "Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL. Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006).", "startOffset": 196, "endOffset": 363}, {"referenceID": 21, "context": "The learning rate was set to \u03b1 and linearly decreased to 0 during training, as described in Mikolov et al. (2013a). The hyperparameters are the embedding dimensionality d, the context size c, the number of negative samples k, the initial learning rate \u03b1, and Mout, the number of words outside the noun pairs.", "startOffset": 92, "endOffset": 115}, {"referenceID": 16, "context": "We evaluated our method on the SemEval 2010 Task 8 data set5 (Hendrickx et al., 2010), which involves predicting the semantic relations between noun pairs in their contexts.", "startOffset": 61, "endOffset": 85}, {"referenceID": 14, "context": "Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations.", "startOffset": 42, "endOffset": 66}, {"referenceID": 7, "context": "More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to date.", "startOffset": 19, "endOffset": 40}, {"referenceID": 33, "context": "1 / n/a FCMFULL (Yu et al., 2014) embeddings, dependency paths, NE 83.", "startOffset": 16, "endOffset": 33}, {"referenceID": 34, "context": "7 / n/a CNN (Zeng et al., 2014) embeddings, WordNet 82.", "startOffset": 12, "endOffset": 31}, {"referenceID": 29, "context": "7 / n/a MVRNN (Socher et al., 2012) embeddings, parse trees, WordNet, NE, POS 82.", "startOffset": 14, "endOffset": 35}, {"referenceID": 33, "context": "4 / n/a FCMEMB (Yu et al., 2014) embeddings 80.", "startOffset": 15, "endOffset": 32}, {"referenceID": 14, "context": "6 / n/a RNN (Hashimoto et al., 2013) embeddings, parse trees, phrase categories, etc.", "startOffset": 12, "endOffset": 36}, {"referenceID": 26, "context": "05) using bootstrap resampling (Noreen, 1989).", "startOffset": 31, "endOffset": 45}, {"referenceID": 7, "context": "dos Santos et al. (2015) also boosted the score of CR-CNNOther by omitting the noisy class \u201cOther\u201d by a ranking-based classifier, and achieved the best score (CR-CNNBest).", "startOffset": 4, "endOffset": 25}, {"referenceID": 10, "context": "We evaluated the learned embeddings using a wordlevel semantic evaluation task called WordSim353 (Finkelstein et al., 2001).", "startOffset": 97, "endOffset": 123}], "year": 2015, "abstractText": "We present a novel learning method for word embeddings designed for relation classification. Our word embeddings are trained by predicting words between noun pairs using lexical relation-specific features on a large unlabeled corpus. This allows us to explicitly incorporate relationspecific information into the word embeddings. The learned word embeddings are then used to construct feature vectors for a relation classification model. On a wellestablished semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models that use syntactic information or manually constructed external resources.", "creator": "LaTeX with hyperref package"}}}