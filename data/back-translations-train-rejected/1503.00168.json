{"id": "1503.00168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "The NLP Engine: A Universal Turing Machine for NLP", "abstract": "It is commonly accepted that machine translation is a more complex task than part of speech tagging. But how much more complex? In this paper we make an attempt to develop a general framework and methodology for computing the informational and/or processing complexity of NLP applications and tasks. We define a universal framework akin to a Turning Machine that attempts to fit (most) NLP tasks into one paradigm. We calculate the complexities of various NLP tasks using measures of Shannon Entropy, and compare `simple' ones such as part of speech tagging to `complex' ones such as machine translation. This paper provides a first, though far from perfect, attempt to quantify NLP tasks under a uniform paradigm. We point out current deficiencies and suggest some avenues for fruitful research.", "histories": [["v1", "Sat, 28 Feb 2015 19:46:50 GMT  (29kb)", "http://arxiv.org/abs/1503.00168v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "eduard hovy"], "accepted": false, "id": "1503.00168"}, "pdf": {"name": "1503.00168.pdf", "metadata": {"source": "CRF", "title": "The NLP Engine: A Universal Turing Machine for NLP", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu", "ehovy@andrew.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.00 168v 1 [cs.C L] 28 Feb 2015"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 The Dilemma for Shannon Entropy", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Review of Entropy and Cross Entropy", "text": "Entropy, referred to as \u2212 x p (y) log (y), illustrates the amount of information contained in a message and can be characterized as the uncertainty of a random variable in a process. For example, Shannon (1951) reported an upper limit of 1.3 bits / character symbol for the English prediction and 5.9 bits / word symbol for the English word prediction, meaning that it is very likely that the English word prediction is a more difficult task than the English letter prediction. If the output Y n = {y0, y1,... yn} is a sequence generated from the input, a stationary stochastic process. Then, the entropy of Y is transferred to the standard model of P by: H (Y) = lim n \u2192 H (yn \u2212 1, yn \u2212 2...,) (1) After the Shannon-McMillan-Breiman theorem (Algoet and Cover, 1988)."}, {"heading": "2.2 The Dilemma for Entropy", "text": "While entropy describes the intrinsic nature of the problem or task, its actual appreciation must be determined by the specific model you use for predicting it. When Shannon took on the task of character prediction, his wife acted as the predictor. If Shannon had used a child as the predictor, he would have received a much greater estimated entropy. Similarly, when comparing the entropy of two tasks, for example, to determine which language sequence is more difficult to predict, English or French, it would be problematic to compare the entropy calculated by a linguist for English and a child for French. Twins are needed that are mathematically and linguistically identical with respect to English and French, in order to allow a fair comparison (Cover and Thomas, 2012). However, in the real world it is almost impossible to find such twins. Different models are equally suited to different scenarios, tasks, tasks, but parameters, but SVM may be more suitable for assessments or SVM tasks."}, {"heading": "3 Prerequisites for Fair Comparison", "text": "We argue that a framework should include the following elements in order to allow a fair complexity comparison of different NLP tasks and systems: A universal measure: Complexity can be measured by several aspects (e.g. the amount of training data required, the amount of pre-processing required, the size and complexity of auxiliary information, training time, memory usage, or even lines of code), but we need a universal and appropriate measure. In this work, we propose Shannon Entropy as a universal measure that we believe reflects the intrinsic randomness, predictability, and uncertainty of data sets and tasks. All of the above are highly correlated to Shannon Entropy. A universal engine: A POS tagging system makes decisions by selecting tags with the highest probability, while a summarization system selects the best-placed sets. A fair comparison of complexity, however, requires a single general framework of a unified, and at least most unified NLP to predict."}, {"heading": "4 The Nature of NLP", "text": "In order to propose a single universal multi-purpose engine for NLP, one must take a very general perspective. In building an NL system, one typically assembles a multitude of components, some of which are active modules that instantiate algorithms and perform transformations, others are passive resources (such as encyclopedias, probability tables, or rule sets) that support the previous one. Active modules are sometimes built to generate passive ones. It is important to differentiate the role of modules in a framework in order to properly estimate the overall complexity. In this section, we first categorize the primary roles of NLP systems (sub-) and then postulate that modern NLP algorithms fall (largely) into three different types of complexity."}, {"heading": "4.1 Three Classes of NLP System", "text": "NLP systems generally perform one of the following three functions / roles: (i) research into aspects of the nature of language (s), (ii) application tasks and subtasks, and (iii) support for algorithms. The majority of NLP development today falls into the second and third classes. Application tasks include studies such as determining the zip nature and entropy of language, detecting changes in usage patterns over time and across geographical regions, identifying text genres through the creation of text and distribution profiles, etc. Application tasks include machine translation, information retrieval, language recognition, natural language interpretation (both syntactical parses and semantic analysis), question answering, word processing, text summary, text (sentence and multi-sentence distribution profiles), voice analysis / opinion formation, word processing / processing and text processing, text processing, text processing, text processing, subalgorithm, and use of algorithms."}, {"heading": "4.2 Three Levels of NLP Algorithm", "text": "We postulate that (almost any) NLP task / subtask can be defined as [a combination of] one of three basic operations, listed in the order of complexity: Level 1: Prediction: The algorithm reads its input, which basically includes a sequence of units, and predicts the next element in the order. Example: Predicting the next word in a stream, as used by Shannon to calculate the information content of texts.Level 2: The labeling of tasks can be divided into two subcategories: \u2022 Aligned labels: There is a one-to-one correspondence between inputs and outputs. Aligned tasks include most tagging of entity tagging from tasks and generated labels (s)."}, {"heading": "5 The Universal NLP Engine", "text": "The generic NLP engine contains (see Figure 1): The transformation motor E, which takes one or more symbols of S as input, with zero or more labels in response. The input stream X, which contains the text (without loss of generality), talks about text (a sequence of words and punctuation), but S could instead be a sequence of symbols from a different vocabulary, such as a part of speech marks, or a mixture of multiple vocabularies, such as words with their individual part of speech marks. We consider S to be composed of an essentially infinite stream of units, each unit being a symbol (or set of associated symbols) for which a label (or set of labels) of E. Let X denote the set of source symbols. The data resource R, typically a lexicon, a grammar, a probability model, or the output of any task used by E."}, {"heading": "6 The Universal Entropy Model", "text": "In this section we discuss the generic model for calculating entropy within a language engine."}, {"heading": "6.1 Requirements", "text": "First, we identify the requirements for the universal model. The random guess model meets the generality, as it can be used in any predictive model, but of course it is not a good predictive model because it provides estimated distributions far away from the actual distribution. In contrast, n-order Markov models (n = 1,2,3...) seem to serve the purpose well and can easily be applied in sequence marking tasks such as NER and POS marking. However, it is difficult to adapt them to single-label predictions such as text classification. Furthermore, their dependence on specific feature selections makes a fair comparison between different implementations complicated or impossible. This consideration leads to the first requirement for a model: requirement 0.1 The model should be able to automatically use different types of characteristics to avoid infinitely complicated feature developments. We are looking at P (Y n) and P (Y n | Xn) to gain insights on predictions for better entropy."}, {"heading": "6.2 Model", "text": "This way of thinking suggests using recurrent neural networks as a model (Mikolov et al., 2010; Funahashi and Nakamura, 1993) or complex versions such as LSTM (Hochreiter and Schmidhuber, 1997) (details on LSTM models can be found in the appendix). Recurrent networks obtain a fixed-size vector for each step in the processing sequence by entwining current information with results from earlier steps. Such vectors can be considered a combination of evidence obtained to date and are used to predict subsequent tokens, typically using a softmax function. To label tasks, recurrent neural networks first map the input of Xn of any sequence length to a fixed-size vector that can be considered evidence, and then map this vector to output by entangling function representations at each step."}, {"heading": "7 Experiments: Comparing the Uncertainty of NLP Tasks", "text": "Using the above framework, we now calculate the exact entropy for a few NLP tasks. What should never be overlooked, however, is the effect of the training / test datasets used (e.g., the complexity of guessing subsequent words in novels and newspapers may vary) and how exactly the task is defined (e.g., the complexity differences in mood classification between a five-class and a two-class problem are enormous)."}, {"heading": "7.1 Tasks and Datasets", "text": "We use the most common 200,000 words and add an \"unknown\" symbol to represent the rest, so it is a 200,001 class prediction problem. It is a simple prediction task. Sentiment Analysis (Pang et al., 2002) of the data set includes sentences that contain gold standard sentiment labels marked at the beginning of each sentence. We divide the original data sets into Training (8101) / Dev (500) / Testing (2000). This is an unaligned labeling (single) label. Labels, the gold standard sentiment labels (UMD) The data set covers two areas, history and literature, and contains approximately 2,000 questions in which each question is paired with an answer (Iyyer et al., 2014). Since the answers are selected from a pool of approximately 100 response candidates, this is not an open QA problem, but a multi-class classification problem."}, {"heading": "7.2 Implementations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.2.1 Prediction Task", "text": "Implementations for predictive tasks in which P (Y n) is to be estimated are similar to the recurrent language models defined in (Mikolov et al., 2010). Let eYt denote the characteristics of the token predicted at the time t. By adopting a Softmax function, the conditional probability for the occurrence of the current token is given on the basis of previous evidence: p (yt | yt \u2212 1,..., yt \u2212 n) = f (et \u2212 1, eyt) \u2211 Y-Y f (et \u2212 1, ey) (5), where f (et \u2212 1, eyt) denotes the compositional function between vectors et \u2212 1 and eyt. In this paper, we take the form of an exponential point product for f (\u00b7): f (et \u2212 1, eyt = exp (et \u2212 1, eyt)."}, {"heading": "7.2.2 Labeling Task", "text": "\"We refer to a binary classification problem (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014))\" We refer to frameworks \"(Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014)\" We refer to frameworks \"(Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Vinyals., 2014;\" We refer to frameworks. \"\" We refer to frameworks. \"We refer to frameworks.\" We refer to frameworks. \"We refer to frameworks.\" We refer to frameworks. \"We refer to frameworks (Sutskever et al., 2014; Bahdanau et al., 2014)\" We refer to frameworks. \""}, {"heading": "7.3 Details", "text": "For each task, word embeddings are initialized using the same pre-trained vectors to ensure fairness. Pre-trained embeddings were obtained from word2vec on a 6 billion word corpus with dimensionality 512. LSTM models consist of a single hidden layer. Stochastic gradients decently (without impulse) with mini-batch (Cotter et al., 2011) are assumed. For each task, we use an initial learning rate of 0.5 with linear decay. Learning ceases after 4 iterations. We initialized the LSTM parameters using a uniform distribution between [-0.1, 0.1]. Referring to (Sutskever et al., 2014), the gradient is normalized when its value exceeds a threshold to avoid exploding gradients. For unbalanced sequence prediction tasks (i.e. syntactical parses, QA (open domain), the inputs are reversed as suggested in Sutskever 2014."}, {"heading": "7.4 Results", "text": "Estimated entropies for various tasks calculated in the proposed paradigm are presented in Table 1. As can be seen, MT is less complex than word prediction tasks, which is in line with our expectations: For MT, output marks are based on source marks. Input data provide additional information and reduce the degree of uncertainty: H (Y | X) \u2265 H (Y) for arbitrary X and Y values. As already discussed, estimated entropies are subjective for datasets. As they are significantly short in the training data, a high degree of entropy is observed in the summary. This phenomenon shows a crucial disadvantage of the proposed model - the failure to take into account the effects of datasets. In particular, we calculate the upper limit for a specific task in view of the assumed specific dataset. How to take into account the influence of different datasets (e.g. quantities of training data, quality of training data) presents a major challenge for the development of a general Nine Engine-P."}, {"heading": "8 Deficiencies and Directions for Improvement", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, live, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in fact, in fact, in which they, are able to live."}, {"heading": "9 Conclusion: Toward a Theory of NLP", "text": "Almost all NLP researchers today would all agree that there is no such thing as a theory of NLP. Wehope that we lay some foundations for such a theory in this paper. Each theory deals with a complex phenomenon by (i) quantifying some categories (of objects or states or events) within them, (ii) providing some properties and perhaps some definitions for them, (iii) describing some relationships between them if possible, and (iv) quantifying (some) aspects of these relationships if possible. A scientific theory measures aspects of some phenomena and uses rules that predict the relationships to the values of other phenomena under certain conditions. The framework outlined in this paper are categories, the commonly used linguistic phenomena of NLP such as words, part of speech tags, syntactic classes, and any other linguistically motivated category that NLP researchers choose to study."}, {"heading": "10 Appendix", "text": "The long-term memory model LSTM, first proposed in (Hochreiter and Schmidhuber, 1997), maps an input sequence to a fixed-size vector by sequentially entwining the current representation with the output representation of the previous step. LSTM associates each epoch with an input, control, and memory gate and tries to minimize the effects of unrelated information. If it is made to correspond to the gate states at the time t, et \u2212 1 and et denote the output at the time t \u2212 1 and t, ext denotes the embedding associated with the symbol at the time t as defined in (Hochreiter and Schmidhuber, 1997), haveit = \u03c3 (Wi \u00b7 ext + Vi \u00b7 et \u2212 1) ft = \u03c3 (Wf \u00b7 ext + Vf \u00b7 et \u2212 1) ft = ft = ft \u00b7 ft \u00b7 ft \u00b7 t \u00b7 t \u00b7 t = within the range (where \u00b7 ext + Vo \u00b7 et \u2212 1) of the range (Vext \u00b7 t), it is within the range \u2212 1 (Vext \u00b7 tn)."}], "references": [{"title": "A sandwich proof of the shannonmcmillan-breiman theorem", "author": ["Algoet", "Cover1988] Paul H Algoet", "Thomas M Cover"], "venue": "The annals of probability,", "citeRegEx": "Algoet et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Algoet et al\\.", "year": 1988}, {"title": "Semantic parsing as machine translation", "author": ["Andreas Vlachos", "Stephen Clark"], "venue": "In ACL", "citeRegEx": "Andreas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "An estimate of an upper bound for the entropy of english", "author": ["Brown et al.1992] Peter F Brown", "Vincent J Della Pietra", "Robert L Mercer", "Stephen A Della Pietra", "Jennifer C Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Better minibatch algorithms via accelerated gradient methods", "author": ["Cotter et al.2011] Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "A convergent gambling estimate of the entropy of english", "author": ["Cover", "King1978] Thomas M Cover", "R King"], "venue": "Information Theory, IEEE Transactions", "citeRegEx": "Cover et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1978}, {"title": "Elements of information theory", "author": ["Cover", "Thomas2012] Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cover et al\\.", "year": 2012}, {"title": "Approximation of dynamical systems by continuous time recurrent neural networks", "author": ["Funahashi", "Nakamura1993] Ken-ichi Funahashi", "Yuichi Nakamura"], "venue": "Neural networks,", "citeRegEx": "Funahashi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Funahashi et al\\.", "year": 1993}, {"title": "Open problems in communication and computation", "author": ["Gopinath", "Cover1987] B Gopinath", "Thomas M Cover"], "venue": null, "citeRegEx": "Gopinath et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Gopinath et al\\.", "year": 1987}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: the 90% solution", "author": ["Hovy et al.2006] Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel"], "venue": "In Proceedings of the human language technology conference of the NAACL,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference on Empir-", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Computational analysis of presentday", "author": ["Kucera", "Francis1967] Henry Kucera", "Nelson Francis"], "venue": null, "citeRegEx": "Kucera et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Kucera et al\\.", "year": 1967}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["Morency et al.2007] L Morency", "Ariadna Quattoni", "Trevor Darrell"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Morency et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Morency et al\\.", "year": 2007}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Petrov", "McDonald2012] Slav Petrov", "Ryan McDonald"], "venue": "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "A mathematical theory of communication", "author": ["Claude Elwood Shannon"], "venue": "ACM SIGMOBILE Mobile Computing and Communications", "citeRegEx": "Shannon.,? \\Q1948\\E", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "Prediction and entropy of printed english", "author": ["Claude E Shannon"], "venue": "Bell system technical journal,", "citeRegEx": "Shannon.,? \\Q1951\\E", "shortCiteRegEx": "Shannon.", "year": 1951}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the ACL conference. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "In his epoch-making work, Shannon (1951) demonstrated how to compute the amount of information in a message.", "startOffset": 26, "endOffset": 41}, {"referenceID": 19, "context": "For example, Shannon (1951) reported an upper bound of 1.", "startOffset": 13, "endOffset": 28}, {"referenceID": 19, "context": "So we can define its hardness or complexity by computing entropy from the distribution P (Y ) for tasks like Shannon\u2019s word prediction model, or extend it to a noisy channel model (Shannon, 1948): given a sequence of inputs X, the uncertainty of the output transformation is given by H(Y |X), interpreted as the amount of uncertainty remaining about Y when X is already known.", "startOffset": 180, "endOffset": 195}, {"referenceID": 4, "context": ",(Kucera and Francis, 1967; Cover and King, 1978; Gopinath and Cover, 1987; Brown et al., 1992)) have explored methods to lower the upper bound of character prediction entropy in English by using more sophisticated models.", "startOffset": 1, "endOffset": 95}, {"referenceID": 22, "context": "The proposed framework is inspired by recent progress of sequence-to-sequence prediction models in NLP, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014; Graves et al., 2014) and the work of Andreas et al.", "startOffset": 132, "endOffset": 240}, {"referenceID": 2, "context": "The proposed framework is inspired by recent progress of sequence-to-sequence prediction models in NLP, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014; Graves et al., 2014) and the work of Andreas et al.", "startOffset": 132, "endOffset": 240}, {"referenceID": 5, "context": "The proposed framework is inspired by recent progress of sequence-to-sequence prediction models in NLP, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014; Graves et al., 2014) and the work of Andreas et al.", "startOffset": 132, "endOffset": 240}, {"referenceID": 1, "context": ", 2014) and the work of Andreas et al. (2013) that illustrates that semantic parsing can to some extent be viewed as a machine translation problem.", "startOffset": 24, "endOffset": 46}, {"referenceID": 20, "context": "As proved by Shannon (Shannon, 1951), Fn monotonically decreases with respect to n and H(Y ) is strictly bounded by Fn:", "startOffset": 21, "endOffset": 36}, {"referenceID": 20, "context": "Taking as example an n-gram word prediction model, theoretically estimated entropy decreases as predictions are made based on increasingly many preceding tokens, roughly stated in (Shannon, 1951).", "startOffset": 180, "endOffset": 195}, {"referenceID": 15, "context": "This line of thinking suggests using as model recurrent neural networks (Mikolov et al., 2010; Funahashi and Nakamura, 1993) or sophisticated versions like LSTM (Hochreiter and Schmidhuber, 1997) (Please see Appendix for details about LSTM models).", "startOffset": 72, "endOffset": 124}, {"referenceID": 22, "context": "These thoughts are inspired by recent progress of sequence-to-sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014).", "startOffset": 89, "endOffset": 176}, {"referenceID": 2, "context": "These thoughts are inspired by recent progress of sequence-to-sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014).", "startOffset": 89, "endOffset": 176}, {"referenceID": 5, "context": "These thoughts are inspired by recent progress of sequence-to-sequence generation models (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014; Cho et al., 2014).", "startOffset": 89, "endOffset": 176}, {"referenceID": 17, "context": "Sentiment Analysis (Pang et al., 2002)\u2019s dataset comprises sentences containing goldstandard sentiment labels tagged at the start of each sentence.", "startOffset": 19, "endOffset": 38}, {"referenceID": 17, "context": "Sentiment Analysis (Pang et al., 2002)\u2019s dataset comprises sentences containing goldstandard sentiment labels tagged at the start of each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). This is an Unaligned Labeling (single) task.", "startOffset": 20, "endOffset": 222}, {"referenceID": 13, "context": "Question-Answering (UMD) The dataset comprises two domains, History and Literature, and contains roughly 2,000 questions where each question is paired with an answer (Iyyer et al., 2014).", "startOffset": 166, "endOffset": 186}, {"referenceID": 12, "context": "Syntactic Parsing Training data is the OntoNotes corpus (Hovy et al., 2006) and English Web Treebank (Petrov and McDonald, 2012) with an additional 5 million random sentences, all parsed by the Stanford Parser (Socher et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 21, "context": ", 2006) and English Web Treebank (Petrov and McDonald, 2012) with an additional 5 million random sentences, all parsed by the Stanford Parser (Socher et al., 2013).", "startOffset": 142, "endOffset": 163}, {"referenceID": 15, "context": "Implementations for prediction tasks, where P (Y ) is to be estimated, are similar to recurrent language models as defined in (Mikolov et al., 2010).", "startOffset": 126, "endOffset": 148}, {"referenceID": 22, "context": "We refer to frameworks (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014)) by first concatenating input and output {X, Y } = {x1, .", "startOffset": 23, "endOffset": 92}, {"referenceID": 2, "context": "We refer to frameworks (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2014)) by first concatenating input and output {X, Y } = {x1, .", "startOffset": 23, "endOffset": 92}, {"referenceID": 2, "context": "Unaligned Sequence Labeling Following (Bahdanau et al., 2014; Vinyals et al., 2014), the conditional probability for predicting the current token yt in Y n is given by", "startOffset": 38, "endOffset": 83}, {"referenceID": 6, "context": "Stochastic gradient decent (without momentum) with mini-batch (Cotter et al., 2011) is adopted.", "startOffset": 62, "endOffset": 83}, {"referenceID": 22, "context": "Referring to (Sutskever et al., 2014), the gradient is normalized if its value exceeds a threshold to avoid exploding gradients.", "startOffset": 13, "endOffset": 37}, {"referenceID": 22, "context": ", syntactic parsing, QA(open domain)), inputs are reversed, as suggested in (Sutskever et al., 2014).", "startOffset": 76, "endOffset": 100}, {"referenceID": 3, "context": "Recurrent models are by no means perfect: they inevitably forget previous information and are fundamentally incapable of capturing long-term dependencies (Bengio et al., 1994).", "startOffset": 154, "endOffset": 175}, {"referenceID": 16, "context": "We are optimistic that other and more sophisticated variations of neural models or other models, such as LDCRFs (Long-Dependency CRFs) (Morency et al., 2007), memory networks (Weston et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 23, "context": ", 2007), memory networks (Weston et al., 2015) will cope with the aforementioned disadvantages bit by bit.", "startOffset": 25, "endOffset": 46}], "year": 2015, "abstractText": "It is commonly accepted that machine translation is a more complex task than part of speech tagging. But how much more complex? In this paper we make an attempt to develop a general framework and methodology for computing the informational and/or processing complexity of NLP applications and tasks. We define a universal framework akin to a Turning Machine that attempts to fit (most) NLP tasks into one paradigm. We calculate the complexities of various NLP tasks using measures of Shannon Entropy, and compare \u2018simple\u2019 ones such as part of speech tagging to \u2018complex\u2019 ones such as machine translation. This paper provides a first, though far from perfect, attempt to quantify NLP tasks under a uniform paradigm. We point out current deficiencies and suggest some avenues for fruitful research.", "creator": "LaTeX with hyperref package"}}}