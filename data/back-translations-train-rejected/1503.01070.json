{"id": "1503.01070", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research", "abstract": "In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.", "histories": [["v1", "Tue, 3 Mar 2015 19:22:01 GMT  (603kb,D)", "http://arxiv.org/abs/1503.01070v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["atousa torabi", "christopher pal", "hugo larochelle", "aaron courville"], "accepted": false, "id": "1503.01070"}, "pdf": {"name": "1503.01070.pdf", "metadata": {"source": "CRF", "title": "Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research", "authors": ["Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "emails": ["atousa.torabi@umontreal.ca", "christopher.pal@polymtl.ca", "hugo.larochelle@usherbrooke.ca", "aaron.courville@umontreal.ca"], "sections": [{"heading": "1. Introduction", "text": "This year, the time has come for us to be able to find a new home in the city, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home, to be able to find a new home."}, {"heading": "2. Related Work", "text": "Recently, significant advances have been made in generating natural language descriptions from images [13, 10, 12, 21]. Existing, relatively large open domain image datasets such as Filckr8k [15], Filckr30k [23], MSCOCO [14], SBU [16] have played an important role in recent breakthroughs in captioning images with natural sentences. In contrast, video annotation with natural language descriptions has not been studied as extensively and with as much success. Recently, it has been proposed to use a 2D ConvNet and an RNN to generate video descriptions by transferring knowledge from 100,000 images with captions."}, {"heading": "AT A FUNERAL, DINNER FOR SCHMUCKS, DISTRICT 9, EASY", "text": "A, FLIGHT, FRIENDS WITH BENEFITS, GOES TO THE GREEK, BUSINESS SPIRIT OF VENGEANCE, GREEN ZONE, GREAT UPS, HANDLE GRETEL WITH THE DOGS, JULIE AND THE BUGS, JUST GO WITH THE MARCH, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN, THE MAN."}, {"heading": "3. DVS-Derived Dataset Collection", "text": "According to the American Council of the Blind (ACB) [3] and the WGBH Media Access Group [6], the number of DVS-supported DVDs is growing rapidly each year, making DVS narratives an attractive data source for capturing large paired video / typesetting data sets. DVS narrations are mixed with original audio tracks. We found that efficient methods with minimal human intervention are sufficient to build a large DVS-derived data set by processing the large amount of DVS audio, matching it with the movies, and transcribing it into text. In the following sections, we describe in detail the steps we have followed for our data collection."}, {"heading": "3.1. DVS Narrations Segmentation Using Vocal Isolation", "text": "This technique is often used in karaoke machines for stereo signals to remove the voice track by reversing the phase of a channel to pick up any signal that comes out of the center while the signals perceived as coming from the left or right are missing. Therefore, the main reason why we use voice isolation for DVS segmentation is the fact that the DVS narration is mixed within natural pauses in the dialogue when there is no dialogue with DVS narration. Therefore, when the narrator speaks, the film signal is almost a flat line in relation to the DVS signal that allows us to separate the narration from other signals. Figure 1 shows an example from the movie \"Life of Pi,\" in which there is the sound of oceans in the background."}, {"heading": "3.2. Movie/DVS Alignment And Professional Transcription", "text": "DVS audio narration segments are timestamped based on our automatic DVS narration segmentation. To compensate for possible 1-2 second misalignment between the speaking DVS narrator and the corresponding scene in the film, we automatically added two seconds to the end of each video clip, without human intervention. We believe that the alignment of film / DVS narrations in [8] was done manually in part because the DVS narration segments were partially inaccurately detected in DVS signals, as we explained above. To obtain high-quality text descriptions, DVS audio segments were transcribed with over 98 percent transcription accuracy, using professional transcription services [7]. These services use a combination of automatic speech recognition techniques and human transcription to produce high-quality transcription. Our audio narration isolation technology allows us to clearly work the entire segments, and to clearly integrate the small audio into the segments."}, {"heading": "4. DVS-Derived Dataset Statistics and Comparison With Other Datasets", "text": "Our dataset currently contains video clips from 92 DVDs covering a variety of movie genres. As the DVS narrator reads every text shown in the movie (e.g. credits) at the beginning and end of the movie, we discard sentences that are timestamped within three minutes at the beginning and four minutes at the end of the movie. Table 1 shows the overall statistics for our unfiltered and filtered dataset. The filtered data contains 48,986 video clips with an average length of 6.2 seconds (for a total of 84.6 hours). The number of records (based on the number of time periods in the dataset) is 55,904, which means that some clips are paired with more than one record. In Table 2, we compare our dataset with existing paired set / video datasets. To the best of knowledge, our dataset is the largest dataset derived from DVS."}, {"heading": "5. DVS-Derived Dataset Corpus Preparation", "text": "There are over 500 proper names in our DVS dataset. When we train a video description model, we are usually not interested in learning how to create such names. Furthermore, our recent work on training an LSTM-based model on this dataset suggests that it is advantageous to remove proper names from the dataset. In this dataset, we have replaced the names of all people with a single symbolic word (e.g., \"SOMEONE\").We also offer an official training / validation / test split for our dataset, which consists of 38,949, 4888, and 5149 video clips, respectively. This split balances DVD genres within Table 3: Corpus POS statistics. The overall vocabulary # Substantive # Verbs # Adjective # Adverb 17,609 9,512 2,571 3,560 857 Figure 3: Four samples of generated sets of our LSTM model and DVS narrations from our movies is very motivated by our character A: The Cloud: 57B: 571."}, {"heading": "6. Conclusion", "text": "In this thesis, we introduced a new, large DVS-derived video dataset that is available to the research community. DVS \"automatic segmentation and alignment method enabled us to capture this dataset with minimal human intervention, and we also offer a balanced distribution of data to define an official task."}], "references": [{"title": "A dataset for movie description", "author": ["N.T.B.S. A Rohrbach", "M Rohrbach"], "venue": "arXiv", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D.L. Chen", "W.B. Dolan"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "Proceedings of the 14th International Conference on Computer Vision ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "[cs.LG],", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Microsoft COCO: common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "CoRR, abs/1405.0312", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Large scale image annotations on amazon mechanical turk", "author": ["S. Maji"], "venue": "Technical Report UCB/EECS-2011-79, EECS Department,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira, and K. Q. Weinberger, editors, NIPS, pages 1143\u20131151", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Coherent multisentence video description with variable level of detail", "author": ["A. Senina", "M. Rohrbach", "W. Qiu", "A. Friedrich", "S. Amin", "M. Andriluka", "M. Pinkal", "B. Schiele"], "venue": "CoRR, abs/1403.6173", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "NAACL \u201903: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173\u2013180, Morristown, NJ, USA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "CoRR, abs/1412.4729", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR, abs/1411.4555", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Video description generation incorporating spatio-temporal features and a soft-attention mechanism", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "CoRR, abs/1502.08029", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL, 2:67\u201378", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Recent success have been fueled by the availability of large labeled datasets such as Flickr30k [23] and MSCOCO [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "Recent success have been fueled by the availability of large labeled datasets such as Flickr30k [23] and MSCOCO [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Recently and in parallel to our work, it was shown that DVS on DVDs are a better source of visual description [8] in that they are more precisely aligned with the visual content", "startOffset": 110, "endOffset": 113}, {"referenceID": 0, "context": "In [8], DVS segments are detected by exploiting a similarity comparison between the DVS track and the original movie soundtracks, using Fast Fourier Transform (FFT) techniques.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "We have collected a new DVS-derived dataset from 92 DVDs 1 Other researchers may contact us to discuss how they may obtain access to the information necessary to replicate a well defined evaluation that follows the experiments with this dataset presented [22].", "startOffset": 255, "endOffset": 259}, {"referenceID": 14, "context": "We present some qualitative examples of automatically generated descriptions from our work in [22] in Section 5 of this paper.", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "Recently, there have been important advances in deep learning for natural language description generation from images [13, 10, 12, 21].", "startOffset": 118, "endOffset": 134}, {"referenceID": 2, "context": "Recently, there have been important advances in deep learning for natural language description generation from images [13, 10, 12, 21].", "startOffset": 118, "endOffset": 134}, {"referenceID": 4, "context": "Recently, there have been important advances in deep learning for natural language description generation from images [13, 10, 12, 21].", "startOffset": 118, "endOffset": 134}, {"referenceID": 13, "context": "Recently, there have been important advances in deep learning for natural language description generation from images [13, 10, 12, 21].", "startOffset": 118, "endOffset": 134}, {"referenceID": 7, "context": "Existing, relatively big open-domain image datasets such as Filckr8k [15], Filckr30k [23], MSCOCO [14], SBU [16] have played an important role in recent breakthroughs in image annotation with natural sentences.", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "Existing, relatively big open-domain image datasets such as Filckr8k [15], Filckr30k [23], MSCOCO [14], SBU [16] have played an important role in recent breakthroughs in image annotation with natural sentences.", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Existing, relatively big open-domain image datasets such as Filckr8k [15], Filckr30k [23], MSCOCO [14], SBU [16] have played an important role in recent breakthroughs in image annotation with natural sentences.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "Existing, relatively big open-domain image datasets such as Filckr8k [15], Filckr30k [23], MSCOCO [14], SBU [16] have played an important role in recent breakthroughs in image annotation with natural sentences.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "Recently, [20] proposed to use a 2D-ConvNet and an RNN for video description generation by transferring knowledge from 100,000 images with captions.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "The majority of available video datasets are toy domains with with small word vocabularies such as TaCos [17], TaCos Multi-Level [18], YouCook [11], and MSVD [9].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "The majority of available video datasets are toy domains with with small word vocabularies such as TaCos [17], TaCos Multi-Level [18], YouCook [11], and MSVD [9].", "startOffset": 129, "endOffset": 133}, {"referenceID": 3, "context": "The majority of available video datasets are toy domains with with small word vocabularies such as TaCos [17], TaCos Multi-Level [18], YouCook [11], and MSVD [9].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "The majority of available video datasets are toy domains with with small word vocabularies such as TaCos [17], TaCos Multi-Level [18], YouCook [11], and MSVD [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "In their comparative study, they have shown that DVS is a richer and more accurate source of descriptions, while the movie scripts are less reliable in many cases and are not as well temporally aligned with movies [8].", "startOffset": 214, "endOffset": 217}, {"referenceID": 9, "context": "TACoS [17] cooking 123 crowd 7,206 18,227 TACoS multilevel [18] cooking 273 crowd 14,105 52,593 MSVD [9] various crowd 1,970 70,028 movie description [8] various 72 Script + DVS 54,076 54,076 movie description (DVS part) [8] various 46 DVS 30,680 30,680 M-VAD (ours) various 92 DVS 48,986 55,904", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "TACoS [17] cooking 123 crowd 7,206 18,227 TACoS multilevel [18] cooking 273 crowd 14,105 52,593 MSVD [9] various crowd 1,970 70,028 movie description [8] various 72 Script + DVS 54,076 54,076 movie description (DVS part) [8] various 46 DVS 30,680 30,680 M-VAD (ours) various 92 DVS 48,986 55,904", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "TACoS [17] cooking 123 crowd 7,206 18,227 TACoS multilevel [18] cooking 273 crowd 14,105 52,593 MSVD [9] various crowd 1,970 70,028 movie description [8] various 72 Script + DVS 54,076 54,076 movie description (DVS part) [8] various 46 DVS 30,680 30,680 M-VAD (ours) various 92 DVS 48,986 55,904", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "TACoS [17] cooking 123 crowd 7,206 18,227 TACoS multilevel [18] cooking 273 crowd 14,105 52,593 MSVD [9] various crowd 1,970 70,028 movie description [8] various 72 Script + DVS 54,076 54,076 movie description (DVS part) [8] various 46 DVS 30,680 30,680 M-VAD (ours) various 92 DVS 48,986 55,904", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "TACoS [17] cooking 123 crowd 7,206 18,227 TACoS multilevel [18] cooking 273 crowd 14,105 52,593 MSVD [9] various crowd 1,970 70,028 movie description [8] various 72 Script + DVS 54,076 54,076 movie description (DVS part) [8] various 46 DVS 30,680 30,680 M-VAD (ours) various 92 DVS 48,986 55,904", "startOffset": 221, "endOffset": 224}, {"referenceID": 0, "context": "In contrast, in recent work [8], DVS audio segementation was done by comparing the similarity of the two signals (the standard movie soundtrack and the soundtrack+DVS) in the Fourier domain against a threshold.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "We believe that in [8], movie/DVS narrations alignment has been done manually partially because of some imprecise detection of the DVS narrations segments in DVS signal as we explained above.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "The recent movie description dataset of [8] is the most similar, however the source for the text descriptions is a combination of movie scripts and DVS.", "startOffset": 40, "endOffset": 43}, {"referenceID": 11, "context": "We tagged all words in the dataset corpus using the Standford Part-Of-Speech (POS) tagger toolbox [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "Figure 3 shows some of the preliminary qualitative results of our recent LSTM video description model [22] on this dataset, based on our official dataset split.", "startOffset": 102, "endOffset": 106}], "year": 2015, "abstractText": "In this work, we introduce a dataset of video annotated with high quality natural language phrases describing the visual content in a given segment of time. Our dataset is based on the Descriptive Video Service (DVS) that is now encoded on many digital media products such as DVDs. DVS is an audio narration describing the visual elements and actions in a movie for the visually impaired. It is temporally aligned with the movie and mixed with the original movie soundtrack. We describe an automatic DVS segmentation and alignment method for movies, that enables us to scale up the collection of a DVS-derived dataset with minimal human intervention. Using this method, we have collected the largest DVS-derived dataset for video description of which we are aware. Our dataset currently includes over 84.6 hours of paired video/sentences from 92 DVDs and is growing.", "creator": "LaTeX with hyperref package"}}}