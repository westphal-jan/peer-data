{"id": "1503.01239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2015", "title": "Joint Active Learning and Feature Selection via CUR Matrix Decomposition", "abstract": "This paper focuses on the problem of simultaneous sample and feature selection for machine learning in a fully unsupervised setting. Though most existing works tackle these two problems separately that derives two well-studied sub-areas namely active learning and feature selection, a unified approach is inspirational since they are often interleaved with each other. Noisy and high-dimensional features will bring adverse effect on sample selection, while `good' samples will be beneficial to feature selection. We present a unified framework to conduct active learning and feature selection simultaneously. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the selected features are very representative. Additionally our method is one-shot without iteratively selecting samples for progressive labeling. Thus our model is especially suitable when the initial labeled samples are scarce or totally absent, which existing works hardly address particularly for simultaneous feature selection. To alleviate the NP-hardness of the raw problem, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experiments on publicly available datasets validate that our method is promising compared with the state-of-the-arts.", "histories": [["v1", "Wed, 4 Mar 2015 06:47:16 GMT  (582kb,D)", "https://arxiv.org/abs/1503.01239v1", null], ["v2", "Fri, 24 Mar 2017 08:58:34 GMT  (582kb,D)", "http://arxiv.org/abs/1503.01239v2", null], ["v3", "Mon, 3 Apr 2017 02:20:27 GMT  (2161kb,D)", "http://arxiv.org/abs/1503.01239v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "xiangfeng wang", "weishan dong", "junchi yan", "qingshan liu", "hongyuan zha"], "accepted": false, "id": "1503.01239"}, "pdf": {"name": "1503.01239.pdf", "metadata": {"source": "CRF", "title": "Joint Active Learning with Feature Selection via CUR Matrix Decomposition", "authors": ["Changsheng Li", "Xiangfeng Wang", "Weishan Dong", "Junchi Yan", "Qingshan Liu", "Hongyuan Zha"], "emails": ["lcsheng@cn.ibm.com", "dongweis@cn.ibm.com"], "sections": [{"heading": null, "text": "Index Terms - Active Learning, Feature Selection, Matrix Factorization"}, {"heading": "1 INTRODUCTION", "text": "Developing powerful predictive models usually requires a detailed selection of examples selected by experts. Typically, active learning algorithms prefer to query these undescribed patterns, which can best improve predictive performance when labeled and used as training data. In this way, the active learner aims to pick as few examples as possible to minimize the overall cost of annotation, while a precise, supervised learning model can be built on these marked data. Over the past decade, many active learning algorithms have been proposed [2], [4], [5], [6] and successfully applied to a variety of problems in computer vision [8], [9], [10], [11], [11]."}, {"heading": "2 PROPOSED METHOD", "text": "With an unlabeled data set X = [x1,.., xn] \u0445 Rd \u00b7 n, our goal is to select m (m < n) samples for labeling by the user, while selecting r (r < d) characteristics as a new feature representation, so that potential performance is maximized when the model is trained on the selected m-labeled samples under the new representation. This is a more difficult problem than traditional active learning problems based on representativity, since selecting m samples to best approximate X often leads to a NP-hard problem [18], and locating r characteristics as the most representative feature subgroup is also often NP-hard [28]."}, {"heading": "2.1 Active Learning and Feature Selection via Matrix Decomposition", "text": "The suggestions that have been mentioned are capable of uniting in order to find a solution that is capable of finding a solution that meets the needs of the people."}, {"heading": "2.2 A Convex Formulation", "text": "Leave p = (p1,.., pn) T + (q1,.., qd) T + (q1,.., qd) T \u00b7 (0,.) d denote two indicator variables to indicate whether a sample and a feature is selected or not. Minimizing p = 1 (or 0) means that the i-th sample is selected (or not) and qi = 1 (or 0) means that the i-th feature is selected (or not). However, minimizing p \u2212 X \u2212 CUR \u00b2 2F can be rewritten as: min, q, U \u00b2 Rn \u00b2 Rn \u00b2 X \u2212 Xdiag (p) U \u2212 diag (q) X \u2212 2Fs.t. 1Tnp = m \u00b2, (2) 1Td \u00b2 q = r \u00b2, q \u00b2 W \u00b2 (0, 1} r \u00b2, where diag (p) n."}, {"heading": "2.3 Local Linear Reconstruction", "text": "In the new objective function (5) we can see that each data point is reconstructed by a linear combination of all selected points (if the i-th series of the reconstruction coefficient matrix WX in (5) is not a zero vector, xi is selected as one of the most representative samples. Otherwise, it is more reasonable to assume that the data point can be recovered mainly by its neighbours [20], [21]. Intuitively, if the distance between the reconstructed point and the selected point is large, the contribution of the selected point to the reconstruction of the target point should be small, and therefore the reconstruction coefficient should be punished by its neighbours [21]. Intuitively, if the distance between the reconstructed point and the selected point is large, the contribution of the selected point to the reconstruction of the target point should be small, and therefore the reconstruction coefficient should be punished by its neighbours [21]. Intuitively, if the distance between the reconstructed point is large, the contribution of the selected point should be small, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c,"}, {"heading": "2.4 Optimization Algorithm", "text": "Although the problem (6) is convex, it is not easy to solve by sub-gradient methods, as various non-smooth terms are involved. (In this section, we use the alternating direction of multipliers (ADMM) method [33] to solve the problems (6). Theoretical results are then given, including global convergence and iteration complexity. (1) Once we have solved the problem as Tij = 1 | cos = 1 | cos problem, where there is a very small positive constant. (2) The dataset is available at http: / / sting.cycollege.ac.cy / alanitis / fgnetaging / index.htm.4To solve the problems (6), we first introduce three variables W, W and Z to convert them (6)."}, {"heading": "2.5 Algorithm Analysis", "text": "From the framework of ALFS, we can find that algorithm 1 is the direct application of the classic two-block ADMM, although the problem has more than two block variables. All sub-problems in algorithm 1 have closed form solutions. Based on the classic convergence results, we can obtain the global convergence of algorithm 1 to the primary-dual optimal solution of problem (8) (see [38], [39]. Below, we present both the global convergence and the iteration complexity of algorithm 1. Theorem 2.1. For given constant parameters."}, {"heading": "3 RELATED WORK", "text": "As described, the work most related to our proposed approach is the second group of active learning methods aimed at selecting the most representative samples. In this section, we will briefly give an overview of the approaches in this group, among which the most popular is the Transductive Experimental Design (TED) [18]. TED aimed to find a representative sample subset from the unlabeled data set, so that the data set can best be approximated by linear combinations of the selected samples. As this optimization problem is NP-hard, [18] a suboptimal sequential optimization algorithm and a non-greedy optimization algorithm were proposed to solve it. Following TED, more active learning algorithms were developed. Cai and He [20] extended TED to select samples by using a nearest neighborhood graph to reproduce the intrinsic lapometric structure that is adaptable to the graphical structure of the 42, in order to incorporate the intrinsic lapometric structure of the 43."}, {"heading": "4 EXPERIMENT", "text": "In this section, we evaluate the proposed method, ALFS, empirically using six publicly available datasets, including artificial data, microarray data, image data, and video data."}, {"heading": "4.1 Experimental Setting", "text": "It is indeed the case that we will be able to go in search of a solution that will enable us, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position, will enable us to put ourselves in a position where we are."}, {"heading": "4.2 Experimental Result", "text": "To demonstrate the effectiveness of our ALFS in selecting representative samples, we compare ALFS with some status-of-the-art active learning algorithms. For ALFS and R-CUR, we vary the number of features selected from 10 to 100 with an incremental step of 10 across all datasets, showing that collaborative active learning is beneficial in selecting features to improve classification accuracy. ALFS-II achieves the best classification performance of all datasets. On certain datasets, such as the Madelon datasets, our ALFS-I and ALFS-II are significantly better than the other methods."}, {"heading": "4.3 CPU Time and Sensitivity Analysis", "text": "We test the CPU runtime with different convergence tolerances on the Madelon dataset and on the FG-NET dataset. The experiments are performed on a laptop with Intel (R) Core (TM) CPUs of 3.20 GHz and 4 GB RAM, and ALFS-II is implemented with the MATLAB R2014b 64bit output without parallel operation. The result is in Fig. 4. The CPU time grows linearly with increasing number of selected samples on both datasets. We also examine the sensitivity of our algorithm to the parameters \u03b1, \u03b2 and \u03bb on the Madelon dataset. In the experiment, we first set the number of selected characteristics to 10 and set the number of selected samples to 1200. 12Then we fix one parameter and vary the other two parameters. We report the accuracy of our algorithm with SVM as the final classifier. The results are shown in Fig. 5, our method is smaller, we can see that F\u03b2."}, {"heading": "5 CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we present a unified framework for performing Active Sample Learning and Feature Selection (ALFS) at the same time. Faced with an unlabeled data set, our formulation naturally and effectively integrates feature and sample selection by solving a regulated optimization problem based on CUR factorization. The superior performance of our method over modern methods is verified by extensive experimental evaluations with six benchmark data sets. Several interesting directions can be followed that are not covered by our current work: \u2022 Labeled Sample Usage: ALFS selects samples and features from the perspective of data reconstruction in an unattended environment. If label information is available, we can incorporate such prior information into our framework, for example by presenting the objective function of [52] as a problem that presents our specific selection in an effective mode that is actually helpful. \""}], "references": [{"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, vol. 28, no. 2-3, pp. 133\u2013168, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research, vol. 4, pp. 129\u2013145, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Active learning by querying informative and representative examples", "author": ["S.-J. Huang", "R. Jin", "Z.-H. Zhou"], "venue": "Advances in Neural Information Processing Systems, 2010, pp. 892\u2013900.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-domain active learning for text classification", "author": ["L. Li", "X. Jin", "S.J. Pan", "J.-T. Sun"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2012, pp. 1086\u20131094.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A convex optimization framework for active learning", "author": ["E. Elhamifar", "G. Sapiro", "A. Yang", "S.S. Sasrty"], "venue": "IEEE International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 209\u2013216.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Bidirectional active learning: A two-way exploration into unlabeled and labeled data set", "author": ["X.-Y. Zhang", "S. Wang", "X. Yun"], "venue": "IEEE Transactions on Neural Network and Learning Systems (TNNLS), 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Twodimensional active learning for image classification", "author": ["G.-J. Qi", "X.-S. Hua", "Y. Rui", "J. Tang", "H.-J. Zhang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Active learning for large multi-class problems", "author": ["P. Jain", "A. Kapoor"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009, pp. 762\u2013769.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-class active learning for image classification", "author": ["A.J. Joshi", "F. Porikli", "N. Papanikolopoulos"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009, pp. 2372\u20132379.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Far-sighted active learning on a budget for image and video recognition", "author": ["S. Vijayanarasimhan", "P. Jain", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3035\u20133042.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Beyond comparing image pairs: Setwise active learning for relative attributes", "author": ["L. Liang", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 208\u2013215.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale live active learning: Training object detectors with crawled data and crowds", "author": ["S. Vijayanarasimhan", "K. Grauman"], "venue": "International Journal of Computer Vision, vol. 108, no. 1-2, pp. 97\u2013114, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Querying discriminative and representative samples for batch mode active learning", "author": ["Z. Wang", "J. Ye"], "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2013, pp. 158\u2013166.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequential algorithm for training text classifiers", "author": ["D.D. Lewis", "W.A. Gale"], "venue": "Proceedings of the 17th ACM SIGIR International Conference on Research and Development in Information Retrieval. Springer-Verlag New York, Inc., 1994, pp. 3\u201312.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Multiclass active learning by uncertainty sampling with diversity maximization", "author": ["Y. Yang", "Z. Ma", "F. Nie", "X. Chang", "A.G. Hauptmann"], "venue": "International Journal of Computer Vision, vol. 113, no. 2, pp. 113\u2013127, 2014.  13", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "Proceedings of International Conference on Machine Learning, 2001, pp. 441\u2013448.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Early active learning via robust representation and structured sparsity", "author": ["F. Nie", "H. Wang", "H. Huang", "C. Ding"], "venue": "Proceedings of the 23th International Joint Conference on Artificial Intelligence. AAAI Press, 2013, pp. 1572\u20131578.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning via transductive experimental design", "author": ["K. Yu", "J. Bi", "V. Tresp"], "venue": "Proceedings of the 23th International Conference on Machine Learning. ACM, 2006, pp. 1081\u20131088.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Batch mode active sampling based on marginal probability distribution matching", "author": ["R. Chattopadhyay", "Z. Wang", "W. Fan", "I. Davidson", "S. Panchanathan", "J. Ye"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2012, pp. 741\u2013749.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Manifold adaptive experimental design for text categorization", "author": ["D. Cai", "X. He"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 24, no. 4, pp. 707\u2013719, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Active learning via neighborhood reconstruction", "author": ["Y. Hu", "D. Zhang", "Z. Jin", "D. Cai", "X. He"], "venue": "Proceedings of the 23th International Joint Conference on Artificial Intelligence. AAAI Press, 2013, pp. 1415\u20131421.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 1151\u20131157.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature selection for unsupervised and supervised inference: The emergence of sparsity in a weight-based approach", "author": ["L. Wolf", "A. Shashua"], "venue": "The Journal of Machine Learning Research, vol. 6, pp. 1855\u20131887, 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1855}, {"title": "Using active learning with integrated feature selection", "author": ["J. Hemant", "X. Xiaowei"], "venue": "Technical Report UALR06-02, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Active learning with feedback on features and instances", "author": ["H. Raghavan", "O. Madani", "R. Jones"], "venue": "The Journal of Machine Learning Research, vol. 7, pp. 1655\u20131686, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Dual active feature and sample selection for graph classification", "author": ["X. Kong", "W. Fan", "P.S. Yu"], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2011, pp. 654\u2013662.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining active learning and dynamic dimensionality reduction.", "author": ["M. Bilgic"], "venue": "SIAM International Conference on Data Mining,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "A variance minimization criterion to feature selection using laplacian regularization", "author": ["X. He", "M. Ji", "C. Zhang", "H. Bao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 2013\u20132025, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal cur matrix decompositions", "author": ["C. Boutsidis", "D.P. Woodruff"], "venue": "arXiv preprint arXiv:1405.7910, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Cur matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 3, pp. 697\u2013702, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative-error cur matrix decompositions", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 2, pp. 844\u2013881, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving cur matrix decomposition and the nystr\u00f6m approximation via adaptive sampling", "author": ["S. Wang", "Z. Zhang"], "venue": "The Journal of Machine Learning Research, vol. 14, no. 1, pp. 2729\u20132769, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, vol. 2, no. 1, pp. 17\u201340, 1976.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1976}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 612\u2013620.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast algorithm for edgepreserving variational multichannel image restoration", "author": ["J. Yang", "W. Yin", "Y. Zhang", "Y. Wang"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 2, pp. 569\u2013592, 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "Technical report, UIUC Technical Report UILU-ENG-09-2215, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning high-order task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proceedings of the 23-th International Joint Conference on Artificial Intelligence. AAAI Press, 2013, pp. 1917\u2013 1923.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "A new inexact alternating directions method for monotone variational inequalities", "author": ["B. He", "L.-Z. Liao", "D. Han", "Y. Hai"], "venue": "Mathematical Programming, vol. 92, no. 1, pp. 103\u2013118, 2002.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "On the o(1/n) convergence rate of the douglas-rachford alternating direction method", "author": ["B. He", "X. Yuan"], "venue": "SIAM Journal on Numerical Analysis, vol. 50, no. 2, pp. 700\u2013709, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "On non-ergodic convergence rate of douglas\u2013rachford alternating direction method of multipliers", "author": ["\u2014\u2014"], "venue": "Numerische Mathematik, pp. 1\u201311, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Active learning based on locally linear reconstruction", "author": ["L. Zhang", "C. Chen", "J. Bu", "D. Cai", "X. He", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 2026\u20132038, 2011.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2026}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, no. 5500, pp. 2323\u2013 2326, 2000.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2000}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Toxicity from radiation therapy associated with abnormal transcriptional responses to dna damage", "author": ["K.E. Rieger", "W.-J. Hong", "V.G. Tusher", "J. Tang", "R. Tibshirani", "G. Chu"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, no. 17, pp. 6635\u20136640, 2004.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised feature selection with structured graph optimization", "author": ["F. Nie", "W. Zhu", "X. Li"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, 2016, pp. 1302\u20131308.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Demographic estimation from face images: Human vs. machine performance.", "author": ["H. Han", "C. Otto", "X. Liu", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Learning ordinal discriminative features for age estimation", "author": ["C. Li", "Q. Liu", "J. Liu", "H. Lu"], "venue": "Computer Vision and Pattern Recognition, 2012, pp. 2570\u20132577.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Recognizing realistic actions from videos", "author": ["J. Liu", "J. Luo", "M. Shah"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2009, pp. 1996\u20132003.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems, 2005, pp. 507\u2013514.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "Trace ratio criterion for feature selection", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan"], "venue": "AAAI\u201908 Proceedings of the 23rd national conference on Artificial intelligence, vol. 2, 2008, pp. 671\u2013676.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Active learning [1] provides a means to alleviate this problem by carefully selecting samples to be labeled by experts.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 183, "endOffset": 186}, {"referenceID": 8, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 188, "endOffset": 191}, {"referenceID": 9, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 11, "context": "In the past decade, lots of active learning algorithms have been proposed [2], [3], [4], [5], [6], and have been successfully applied to a variety of problems in computer vision [7], [8], [9], [10], [11], [12].", "startOffset": 205, "endOffset": 209}, {"referenceID": 12, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 207, "endOffset": 210}, {"referenceID": 15, "context": "Generally speaking, there are two main group methods for selecting unlabeled samples to label [13]: One is to select the most informative samples, such as uncertainty sampling [14], [15], query by committee [1], and empirical risk minimization [16].", "startOffset": 244, "endOffset": 248}, {"referenceID": 16, "context": "edu training model usually needs a large number of labeled data to avoid the samples bias, the methods above should be used after sufficient labeled samples are collected [17].", "startOffset": 171, "endOffset": 175}, {"referenceID": 16, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "The other group aims at querying the most representative samples from a perspective of data reconstruction [17], [18], [19], [20], [21].", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "One may state that, if we apply some state-of-the-art feature selection techniques, such as SPEC [22], Q \u2212 \u03b1 [23], to learn a low-dimensional representation before active learning, these problems might be solved.", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "One may state that, if we apply some state-of-the-art feature selection techniques, such as SPEC [22], Q \u2212 \u03b1 [23], to learn a low-dimensional representation before active learning, these problems might be solved.", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "Recently, Joshi and Xu [24] presented an active learning method with integrated feature selection based on linear kernel SVMs and GainRatio.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "[25] intended to use human feedback on both features and samples for active learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] proposed a dual feature selection and sample selection method in the context of graph classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Bilgic [27] proposed a dynamic dimensionality reduction algorithm that determined the appropriate number of dimensions for each active learning iteration.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "This is a more challenging problem than traditional representativeness based active learning problems, because selecting m samples to best approximate X often leads to an NP-hard problem [18], and finding r features as the most representative feature subset is also often NP-hard [28].", "startOffset": 187, "endOffset": 191}, {"referenceID": 27, "context": "This is a more challenging problem than traditional representativeness based active learning problems, because selecting m samples to best approximate X often leads to an NP-hard problem [18], and finding r features as the most representative feature subset is also often NP-hard [28].", "startOffset": 280, "endOffset": 284}, {"referenceID": 28, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "Inspired by the CUR matrix decomposition [29], [30], [31], [32], we propose a unified framework to find the most representative samples and features.", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 330, "endOffset": 334}, {"referenceID": 33, "context": "Moreover, unlike most existing CUR solvers being randomized or heuristic algorithms [29], [30], we utilize the structured sparsity-inducing norms to relax the objective from a non-convex optimization problem to a convex one, which allows for devising an efficient variant of the alternating direction method of multipliers (ADMM) [33], [34].", "startOffset": 336, "endOffset": 340}, {"referenceID": 16, "context": "Fortunately, there exists theoretical progress that \u2016W\u20162,1 is the minimum convex hull of \u2016W\u20162,0 [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "However, it is more reasonable to suppose that a data point can be mainly recovered from its neighbors [20], [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "However, it is more reasonable to suppose that a data point can be mainly recovered from its neighbors [20], [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 32, "context": "In this section, we employ the alternating direction method of multipliers (ADMM) [33] to solve (6).", "startOffset": 82, "endOffset": 86}, {"referenceID": 34, "context": "The problem (16) can be solved by the following lemma [35]:", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "The problem (22) can be solved by the following matrix shrinkage operation Lemma [36]: Lemma 2.", "startOffset": 81, "endOffset": 85}, {"referenceID": 36, "context": "We can also extend our method to the kernel version by defining a new data representation to incorporate the kernel information as in [37].", "startOffset": 134, "endOffset": 138}, {"referenceID": 37, "context": "Based on the classical convergence results, we can obtain the global convergence of Algorithm 1 to the primaldual optimal solution of problem (8) (see [38], [39]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 38, "context": "Based on the classical convergence results, we can obtain the global convergence of Algorithm 1 to the primaldual optimal solution of problem (8) (see [38], [39]).", "startOffset": 157, "endOffset": 161}, {"referenceID": 39, "context": "3) (Ergodic Iteration Complexity [40]) Let (W,\u0174,W\u0303,Z,\u039b1,\u039b \u2217 2,\u039b \u2217 3) be an optimal solution pair, we have", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "4) (Non-ergodic Iteration Complexity [41]) The nonergodic iteration complexity can be written as \u2225\u2225\u2225\u03a3k \u2212\u03a3k+1\u2225\u2225\u22252 H \u2264 C2 k + 1 , (26)", "startOffset": 37, "endOffset": 41}, {"referenceID": 38, "context": "For the detailed proof, please refer to [39], [40].", "startOffset": 40, "endOffset": 44}, {"referenceID": 39, "context": "For the detailed proof, please refer to [39], [40].", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Among them, the most popular one is the Transductive Experimental Design (TED) [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Since this optimization problem is NP-hard, [18] proposed a suboptimal sequential optimization algorithm and a non-greedy optimization algorithm to solve it, respectively.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "Cai and He [20] extended TED to choose samples by utilizing a nearest neighbor graph to capture intrinsic local manifold structure, where the graph Laplacian is incorporated into a manifold adaptive kernel space.", "startOffset": 11, "endOffset": 15}, {"referenceID": 41, "context": "[42] adopted the idea from Locally Linear Embedding (LLE) [43] to find the reconstruction coefficients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[42] adopted the idea from Locally Linear Embedding (LLE) [43] to find the reconstruction coefficients.", "startOffset": 58, "endOffset": 62}, {"referenceID": 41, "context": "Similar to [42], Hu et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "[21] incorporated the local geometrical information into the active learning process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] proposed a novel method to relax the objective of TED to an efficient convex formulation, and utilized the robust sparse representation loss function to reduce the effect of outliers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "Dataset We evaluate the performance of our ALFS on six datasets, including 1) an artificial dataset, Madelon [44],", "startOffset": 109, "endOffset": 113}, {"referenceID": 44, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 100, "endOffset": 104}, {"referenceID": 43, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 114, "endOffset": 118}, {"referenceID": 45, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 277, "endOffset": 281}, {"referenceID": 46, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 308, "endOffset": 312}, {"referenceID": 47, "context": "used in the NIPS 2003 feature selection challenge; 2) two preprocessed microarray datasets, TOX-171 [45] and Musk [44], for studying toxicity prediction and musk activity prediction respectively; 3) two image datasets, ORL and FGNET, which are widely used for face recognition [46] and facial age estimation [47], [48], respectively.", "startOffset": 314, "endOffset": 318}, {"referenceID": 48, "context": "For FG-NET, we conduct age range estimation as in [49], and divide the dataset into five age groups in the experiment; 4) a video dataset, UCF11 [50], which is collected from YouTube.", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "\u2022 Transductive Experimental Design (TED) [18]3: an active learning method developing experimental design in a transductive setting.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "\u2022 RRSS [17]4: an active learning method taking advantage of robust representation and structure sparsity.", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "\u2022 Active Learning via Neighbor Reconstruction (ALNR) [21]: an active learning method using neighborhood reconstruction.", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "\u2022 R-CUR [30]: a randomized algorithmic approach for solving CUR matrix factorization.", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "We use three kinds of unsupervised feature selection methods, Laplacian5 [51], SPEC6 [22], and SOGFS7 [46], to com-", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "We use three kinds of unsupervised feature selection methods, Laplacian5 [51], SPEC6 [22], and SOGFS7 [46], to com-", "startOffset": 85, "endOffset": 89}, {"referenceID": 45, "context": "We use three kinds of unsupervised feature selection methods, Laplacian5 [51], SPEC6 [22], and SOGFS7 [46], to com-", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "Experiments protocol Following [17], for each dataset, we first randomly select 50% of the data points as candidate samples for training, from which we apply the compared active learning methods to select a subset of samples to request human labeling.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "The reason is that R-CUR [30] is a general CUR model and adopts a randomized algorithmic approach to seek the matrices C and R for satisfying (1).", "startOffset": 25, "endOffset": 29}, {"referenceID": 29, "context": "When the user inputs the desired number of samples m and the number of features r, the final outputs m\u2032 and r\u2032 of R-CUR [30] may be slightly different m and r, respectively.", "startOffset": 120, "endOffset": 124}, {"referenceID": 0, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 1, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 2, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 3, "context": "The noisy variable is sampled from the standard normal distribution, and the noisy label is drawn from the discrete uniform distribution on [1, 2, 3, 4].", "startOffset": 140, "endOffset": 152}, {"referenceID": 50, "context": ", taking the objective function of [52] as a regularization term.", "startOffset": 35, "endOffset": 39}], "year": 2017, "abstractText": "This paper focuses on the problem of simultaneous sample and feature selection for machine learning in a fully unsupervised setting. Though most existing works tackle these two problems separately that derives two well-studied sub-areas namely active learning and feature selection, a unified approach is inspirational since they are often interleaved with each other. Noisy and high-dimensional features will bring adverse effect on sample selection, while \u2018good\u2019 samples will be beneficial to feature selection. We present a framework to jointly conduct active learning and feature selection based on the CUR matrix decomposition. From the data reconstruction perspective, both the selected samples and features can best approximate the original dataset respectively, such that the selected samples characterized by the selected features are very representative. Additionally our method is one-shot without iteratively selecting samples for progressive labeling. Thus our model is especially suitable when the initial labeled samples are scarce or totally absent, which existing works hardly address particularly for simultaneous feature selection. To alleviate the NP-hardness of the raw problem, the proposed formulation involves a convex but non-smooth optimization problem. We solve it efficiently by an iterative algorithm, and prove its global convergence. Experiments on publicly available datasets validate that our method is promising compared with the state-of-the-arts.", "creator": "LaTeX with hyperref package"}}}