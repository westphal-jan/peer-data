{"id": "1503.01800", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2015", "title": "EmoNets: Multimodal deep learning approaches for emotion recognition in video", "abstract": "The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based \"bag-of-mouths\" model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67% on the 2014 dataset.", "histories": [["v1", "Thu, 5 Mar 2015 22:03:26 GMT  (573kb,D)", "https://arxiv.org/abs/1503.01800v1", null], ["v2", "Mon, 30 Mar 2015 00:55:02 GMT  (574kb,D)", "http://arxiv.org/abs/1503.01800v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["samira ebrahimi kahou", "xavier bouthillier", "pascal lamblin", "caglar gulcehre", "vincent michalski", "kishore konda", "s\\'ebastien jean", "pierre froumenty", "yann dauphin", "nicolas boulanger-lewandowski", "raul chandias ferrari", "mehdi mirza", "david warde-farley", "aaron courville", "pascal vincent", "roland memisevic", "christopher pal", "yoshua bengio"], "accepted": false, "id": "1503.01800"}, "pdf": {"name": "1503.01800.pdf", "metadata": {"source": "CRF", "title": "EmoNets: Multimodal deep learning approaches for emotion recognition in video", "authors": ["Samira Ebrahimi Kahou", "Pascal Lamblin", "Vincent Michalski", "Kishore Konda", "Pierre Froumenty", "Yann Dauphin", "Nicolas Boulanger-Lewandowski", "Raul Chandias Ferrari", "Mehdi Mirza", "David Warde-Farley", "Aaron Courville", "Roland Memisevic", "Yoshua Bengio"], "emails": ["christopher.pal}@polymtl.ca", "konda.kishorereddy}@gmail.com", "bengioy}@iro.umontreal.ca"], "sections": [{"heading": null, "text": "Wild (EmotiW) challenge is to assign one of seven emotions to short video clips extracted from Hollywood-style films. The videos depict played emotions under realistic conditions with a large degree of variation in attributes such as pose and enlightenment, making it worthwhile to explore approaches that consider combinations of features from different modalities for assigning labels. In this essay, we present our approach to learning multiple specialized models using deep learning techniques, each of which is focused on a modality, including an evolutionary neural network that focuses on collecting visual information in recognized faces, a deep web of belief that focuses on the representation of the audio stream, a K-Means-based \"Bag-of-ths\" model that extracts visual features around the oral region, and a relational autoencoder that focuses on spatial C. de C. de C. de C. Moncenty, Montrole, P\u00e9tral, P\u00e9tral."}, {"heading": "1 Introduction", "text": "This is an enhanced version of the paper that describes our winning submission [22] to the Emotion Recognition in the Wild Challenge (EmotiW) in 2013 [11]. Here, we describe our approach in more detail and present the results on the new data sets from the 2014 contest [10]. The task in this contest is to assign each short video clip in the Acted Facial Expression in the Wild (AFEW) data set to one of seven emotion labels (angry, disgusted, fear, happy, neutral, sad, surprise), extracted from feature films. Given the small number of samples per emotion category, it is difficult to deal with the wide variety of subjects, light conditions and poses in these close to the real world videos, the clips are about 1 to 2 seconds long and also feature an audio track that could contain voices and background music. We explore various methods to combine predictions of modality-specific models, including a network that will recognize (1) an evolutionary network (1)."}, {"heading": "2 Related work", "text": "The task of recognizing the emotions associated with a short video clip lends itself well to methods and models that combine characteristics of different modalities. Therefore, many other successful approaches to emotion recognition in the Wild (EmotiW) 2013 and 2014 focus on the merging of modalities, including [32] that used Multiple Kernel Learning (MKL) to merge visual and audiovisual characteristics; the recent success of deep learning methods in the challenge of computer vision [27] [31] [21], speech modeling [23], and speech recognition [18] seem to transfer to emotion recognition, bearing in mind that the winners of the 2014 challenge [30] also used a deep revolutionary neural network that they combined with other visual and audiographic characteristics that used a classifier of partial-least squares (PLS)."}, {"heading": "3 Models for modality-specific representation learning", "text": "3.1 A revolutionary networking approach for Faces ConvNets is artificial neural network architectures that require a topological entrance space, such as a 2d image plane. A set of two-dimensional or three-dimensional (if the inputs are color images) filters is applied to small regions across the entire image by folding, creating a bank of filter response maps (one map per filter) that also share a similar 2d topology. To reduce the dimensionality of feature banks and introduce inventory in terms of easy translations of the input image, revolutionary layers are often followed by a pooling layer that distinguishes the feature maps by breaking down small regions into a single element (e.g. by selecting the maximum or average value in the region). ConvNets have recently demonstrated that the performance of art is achieved in demanding object recognition tasks."}, {"heading": "3.1.1 Additional Face Dataset", "text": "The \"additional data\" we used to train the deep network consists of two large static image sets of facial expressions for the seven emotion classes; the first and larger data set is the Google data set [5], which consists of 35,887 images with the seven facial expressions: angry, disgusted, fear, happy, sad, surprising, and neutral; the second data set was created by harvesting images returned from Google image search with keywords associated with expressions that were then cleaned and labeled by hand. We use the grayscale 48 x 48 pixel versions of these images; and the second is the Toronto Face Dataset (TFD) with 4,178 images labeled with basic emotions, essentially with a full frontal view of positions. To make the data sets compatible, there are major differences between subjects, lights, and poses."}, {"heading": "3.1.2 Extracting frame-wise emotion probabilites", "text": "Our ConvNet uses the C + + and CUDA implementation written by Alex Krizhevsky [26] in Python. The architecture of the network is illustrated here in Figure 3. ConvNet takes stacks of 48 x 48 images as input and performs a random section into smaller 40 x 40 sub-images in each epoch. These images are then flipped horizontally with a 0.5 probability. These two common methods allow us to expand the limited amount of training and overfitting.The ConvNet architecture has 4 levels containing different layers. The first two levels include a layer followed by a pooling layer, then a local response normalization layer [27]. The third level only includes a convolutionary layer followed by a pooling layer in the first stage, which contains different layers."}, {"heading": "3.1.3 Facetube extraction procedure", "text": "For the contest dataset video frames were extracted preserving the original aspect ratio. Then, the Google Picasa face detector [14] was used to crop captured faces in each frame. To get the Bounding Box parameters into the original image, we used hair-like features for customization because direct pixel-to-pixel matching did not achieve the required performance. Picasa did not detect faces in each frame. To fix this, we looked for the spatial neighborhood of the closest-timed Bounding Box for regions with a roughly matching histogram of color intensities. We used heuristics, such as the relative positioning, size and overlap, to associate bounding boxes of consecutive frames and generate facetube for each subject in the contest test sets that Picasa face detector could not detect faces. So we used the combined Landmark Placement and Face Detection method to find faces in these clips."}, {"heading": "3.1.4 Aggregation into video descriptors and classification", "text": "We aggregated the pro-frame probabilities for all frames of a facetube for which a face was detected in a fixed-length video descriptor that can be used as input into an SVM classifier. For this aggregation step, we linked the seven-dimensional probability vectors of ten consecutive frames and yielded 70-dimensional feature vectors. Most videos have more than ten frames and some are too short, and there are frames without captured faces. We solved these problems with the following two aggregation approaches: - Video averaging: For videos that were too long, weaveraged the probability vectors of 10 independent groups of frames that were consistently recorded over time, contract the facetube to fit into the 10-frame video descriptors. This is illustrated in Figure 4. - For videos that contain too few frames with detectable, we have expanded by formally repeating frames to get a total of 10 frames."}, {"heading": "3.2.1 Audio Preprocessing", "text": "Selecting the right characteristics is a crucial aspect of audio classification. On the other hand, emotion detection in the film sound is very different from other audio tasks. In addition to the language in the audio track, background noise and the soundtrack can also be important indicators of emotion. For the EmotiW challenge, we extracted 29 characteristics from each audio track using the Yafee library1 with a sampling rate of 48 kHz. We used all characteristics provided by the Yafee library except \"frames.\" In addition, 3 types of MFCC characteristics were used, the first using 22 cepstral coefficients, the second using a first-order characteristic transformation with the first-order temporal derivative, and the last second-order temporal derivative. Online PCA was applied to the extracted characteristics, and 909 characteristics per timeframe were retained [16]."}, {"heading": "3.2.2 DBN Pretraining", "text": "We have the RBMs with stochastic maximum probability and contrast divergence in a Gibbs step (CD-1).1 Yaafe: Audio Features Toolbox: http: / / yaafe. sourceforge.net / Each RBM layer had 350 hidden units. The first and second layers of RBMs were trained at learning rates of 0.0006, 0.0005 and 0.001, respectively. A penalty of L2 of 2 x 10 \u2212 3 and 2 x 10 \u2212 4 was used for the first and second layers of LBM. Both the first and second layers of RBM were trained at learning rates of 0.0005 and 0.001, respectively."}, {"heading": "3.2.3 Temporal Pooling for Audio Classification", "text": "We used a multi-layered learning model [16] for the MLP, in which we merged the last hidden representation layer of an MLP to aggregate information across frames before a final Softmax layer. We experimented with various pooling methods, including max pooling and mean pooling, but the best results were obtained with a specially designed type of pooling for the characteristics of the MLP discussed below. Suppose we have a matrix A for activating the characteristics of the last layer of the MLP that includes activations of all time scales in the clip, where A-Rdt \u00b7 df and dt is the variable number of characteristics, df is the number of characteristics in each time scale. We sort the columns of A in descending order and get the uppermost N lines weighted using the map f: Rdt \u00b7 df \u2192 RN \u00b7 df. The most active characteristics in each time scale are summarized with N = a weighted average of 1 (N = 7)."}, {"heading": "3.2.4 Supervised Fine-tuning", "text": "The competition training dataset was used for supervised fine tuning, and we applied early stops by measuring the error rate on the competition validation dataset. Characteristics were centered prior to training, and prior to initiating supervised training, we chose the sequence of clips. During the supervised fine tuning phase, we randomly confused the sequence of features in the clip with each iteration on the training dataset, and we randomly omitted 40% of the features in the clip. 0.121% of the hidden units are omitted, and we applied a norm restriction on weights so that the L2 standard of incoming weights is not exceeded to a hidden unit. [20] In addition to drop-out and maximum norm restriction on weights, a coefficient of 10 \u2212 5 was used."}, {"heading": "4 Experimental results", "text": "This year, the time has come for EU member states to be able to pay their debts."}, {"heading": "4.4.1 An SVM combination approach using all data", "text": "A simple method of learning when working with a single training set and a single validation set is to use the training set to train a model multiple times with different hyperparameters, and then select the best model using the validation set. We can then simply use the predictions of the ConvNet1 model and retrain the model with the combined training and validation set, which is known to work well in practice. We first used this method to train an SVM to combine the predictions of the ConvNet1 model and the audio model. It resulted in a 44.71% test accuracy, an impressive 7% improvement over ConvNet1 alone (37.35%), and a 6% improvement over the same combination that was only trained on the 2013 AFEW2 training set (38.26%). An important factor could be that we use predictions on data that are said during the sub-model training rather than the combination model that they are seen to train."}, {"heading": "4.4.2 Weighting models and random search using all data", "text": "A random search method for determining the parameters of a linear per class and per model weighting was calculated, as described in Section 4.3, but for the AFEW4 (EmotiW 2014 challenge data). For our first experiment, we did a random search using the validation set predictions, then we used the resulting weights to calculate the weighted average of predictions of sub-models on all data. To be clear, the only difference from our best model from 2013 submissions was that we applied the weighted average to sub-models trained on the combined training and validation of the data set of 2014. This resulted in a test accuracy of 44.72%, 2% higher than the same method with SVM training, but no gain on the best combination of ConvNet1 with audio models (44.71%). Random search can also be applied to mixed predictions as explained in the previous section."}, {"heading": "5 Conclusions and discussion", "text": "Our experiments with both competition datasets (2013 and 2014) have led to a number of contributions and insights that we believe may be broader in scope. First, we believe that our approach to using the large-scale removal of images from Google Image Search to train our deep neural network has helped us avoid overadjustment to the challenges we face. We performed better when we used the competition data solely to train the classifier and used additional image datasets to form the Convolutionary Network. Validation accuracy was significantly higher than in our experiment, in which we trained the network directly on the challenging faces from the challenge data. It is our intuition that video frames in isolation are not always representative of the emotional tag associated clips, and with a video length label introduced leads to an experiment in which we trained the network directly on the faces taken from the challenge."}], "references": [{"title": "Spatiotemporal energy models for the perception of motion", "author": ["E.H. Adelson", "J.R. Bergen"], "venue": "JOSA A 2(2), 284\u2013299", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5590", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Random search for hyperparameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "JMLR 13, 281\u2013305", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for scientific computing conference (SciPy), vol. 4, p. 3. Austin, TX", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "FER-2013 Face Database", "author": ["P.L. Carrier", "A. Courville", "I.J. Goodfellow", "M. Mirza", "Y. Bengio"], "venue": "Tech. rep., 1365, Universit\u00e9 de Montr\u00e9al", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology 2, 27:1\u201327:27", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Emotion recognition in the wild with feature fusion and multiple kernel learning", "author": ["J. Chen", "Z. Chen", "Z. Chi", "H. Fu"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, pp. 508\u2013513. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "An Analysis of SingleLayer Networks in Unsupervised Feature Learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "AISTATS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Proc. ICASSP", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Emotion recognition in the wild challenge 2014: Baseline, data and protocol", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "K. Sikka", "T. Gedeon"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, pp. 461\u2013 466. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Emotion recognition in the wild challenge 2013", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "M. Wagner", "T. Gedeon"], "venue": "ACM ICMI", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Collecting large, richly annotated facial-expression databases from movies", "author": ["A. Dhall", "R. Goecke", "S. Lucey", "T. Gedeon"], "venue": "IEEE MultiMedia (3), 34\u201341", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Why is facial expression analysis in the wild challenging? In: Proceedings of the 2013 on Emotion recognition in the wild challenge and workshop, pp", "author": ["T. Gehrig", "H.K. Ekenel"], "venue": "9\u201316. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.R. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 6645\u20136649. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "ISMIR, pp. 729\u2013734", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Lighting normalization algorithms for face verification", "author": ["G. Heusch", "F. Cardinaux", "S. Marcel"], "venue": "IDIAP Communication Com05-03", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.R. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T.N"], "venue": "IEEE Sig. Proc. Magazine, 29(6), 82\u201397", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation 18(7), 1527\u20131554", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "arXiv:1207.0580", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Facial expression analysis based on high dimensional binary features", "author": ["S.E. Kahou", "P. Froumenty", "C. Pal"], "venue": "ECCV Workshop on Computer Vision with Local Binary Patterns Variants. Zurich, Switzerland", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining modality specific deep neural networks for emotion recognition in video", "author": ["S.E. Kahou", "C. Pal", "X. Bouthillier", "P. Froumenty", "C. Gulcehre", "R. Memisevic", "P. Vincent", "A. Courville", "Y. Bengio", "R.C. Ferrari", "M. Mirza", "S. Jean", "P.L. Carrier", "Y. Dauphin", "N. Boulanger-Lewandowski", "A. Aggarwal", "J. Zumer", "P. Lamblin", "J.P. Raymond", "G. Desjardins", "R. Pascanu", "D. Warde-Farley", "A. Torabi", "A. Sharma", "E. Bengio", "M. C\u00f4t\u00e9", "K.R. Konda", "Z. Wu"], "venue": "Proceedings of the 15th ACM on International Conference on Multimodal Interaction, ICMI \u201913", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv:1404.2188", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "The role of spatio-temporal synchrony in the encoding of motion", "author": ["K.R. Konda", "R. Memisevic", "V. Michalski"], "venue": "ICLR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Tech. rep.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Cuda-convnet Google code home page", "author": ["A. Krizhevsky"], "venue": "https://code.google.com/p/cuda-convnet/", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS, pp. 1106\u20131114", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q. Le", "W. Zou", "S. Yeung", "A. Ng"], "venue": "CVPR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Partial least squares regression on grassmannian manifold for emotion recognition", "author": ["M. Liu", "R. Wang", "Z. Huang", "S. Shan", "X. Chen"], "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction, pp. 525\u2013530. ACM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild", "author": ["M. Liu", "R. Wang", "S. Li", "S. Shan", "Z. Huang", "X. Chen"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, pp. 494\u2013501. ACM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Moddrop: adaptive multi-modal gesture recognition", "author": ["N. Neverova", "C. Wolf", "G.W. Taylor", "F. Nebout"], "venue": "arXiv:1501.00102", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel learning for emotion recognition in the wild", "author": ["K. Sikka", "K. Dykstra", "S. Sathyanarayana", "G. Littlewort", "M. Bartlett"], "venue": "Proceedings of the 15th ACM on International conference on multimodal interaction, pp. 517\u2013524. ACM", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Gabor-based kernel partial-leastsquares discrimination features for face recognition", "author": ["V. \u0160truc", "N. Pave\u0161i\u0107"], "venue": "Informatica 20(1), 115\u2013138", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Combining multimodal features with hierarchical classifier fusion for emotion recognition in the wild", "author": ["B. Sun", "L. Li", "T. Zuo", "Y. Chen", "G. Zhou", "X. Wu"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, pp. 481\u2013486. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "The toronto face database", "author": ["J. Susskind", "A. Anderson", "G. Hinton"], "venue": "Tech. rep., UTML TR 2010-001, University of Toronto", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "ICML 2013", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional learning of spatio-temporal features", "author": ["G.W. Taylor", "R. Fergus", "Y. LeCun", "C. Bregler"], "venue": "Proceedings of the 11th European conference on Computer vision: Part VI, ECCV\u201910", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Photometric normalization techniques for illumination invariance, pp", "author": ["V. \u0160truc", "N. Pave\u0161i\u0107"], "venue": "279\u2013300. IGIGlobal", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluation of local spatio-temporal features for action recognition", "author": ["H. Wang", "M.M. Ullah", "A. Kl\u00e4ser", "I. Laptev", "C. Schmid"], "venue": "BMVC", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Face Detection, Pose Estimation, and Landmark Localization in the Wild", "author": ["X. Zhu", "D. Ramanan"], "venue": "CVPR", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "This is an extended version of the paper describing our winning submission [22] to the Emotion Recognition in the Wild Challenge (EmotiW) in 2013 [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "This is an extended version of the paper describing our winning submission [22] to the Emotion Recognition in the Wild Challenge (EmotiW) in 2013 [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 9, "context": "Here we describe our approach in more detail and present results on the new data set from the 2014 competition [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "The task in this competition is to assign one of seven emotion labels (angry, disgust, fear, happy, neutral, sad, surprise) to each short video clip in the Acted Facial Expression in the Wild (AFEW) dataset [12].", "startOffset": 207, "endOffset": 211}, {"referenceID": 28, "context": "37% reported by the challenge winners [30].", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "These include [32], who used Multiple Kernel Learning (MKL) for fusion of visual and audio features.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "The recent success of deep learning methods in challenging computer vision [27][31][21], language modeling [23] and speech recognition [18] tasks seems to carry over to emotion recognition, taking into account that the 2014 challenge winners [30] also employed a deep convolutional neural net, which they combined with other visual and audio features using a Partial Least Squares (PLS) classifier.", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "The recent success of deep learning methods in challenging computer vision [27][31][21], language modeling [23] and speech recognition [18] tasks seems to carry over to emotion recognition, taking into account that the 2014 challenge winners [30] also employed a deep convolutional neural net, which they combined with other visual and audio features using a Partial Least Squares (PLS) classifier.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "The recent success of deep learning methods in challenging computer vision [27][31][21], language modeling [23] and speech recognition [18] tasks seems to carry over to emotion recognition, taking into account that the 2014 challenge winners [30] also employed a deep convolutional neural net, which they combined with other visual and audio features using a Partial Least Squares (PLS) classifier.", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "The recent success of deep learning methods in challenging computer vision [27][31][21], language modeling [23] and speech recognition [18] tasks seems to carry over to emotion recognition, taking into account that the 2014 challenge winners [30] also employed a deep convolutional neural net, which they combined with other visual and audio features using a Partial Least Squares (PLS) classifier.", "startOffset": 107, "endOffset": 111}, {"referenceID": 16, "context": "The recent success of deep learning methods in challenging computer vision [27][31][21], language modeling [23] and speech recognition [18] tasks seems to carry over to emotion recognition, taking into account that the 2014 challenge winners [30] also employed a deep convolutional neural net, which they combined with other visual and audio features using a Partial Least Squares (PLS) classifier.", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "The recent success of deep learning methods in challenging computer vision [27][31][21], language modeling [23] and speech recognition [18] tasks seems to carry over to emotion recognition, taking into account that the 2014 challenge winners [30] also employed a deep convolutional neural net, which they combined with other visual and audio features using a Partial Least Squares (PLS) classifier.", "startOffset": 242, "endOffset": 246}, {"referenceID": 27, "context": "The adoption of deep learning for visual features likely played a big role in the considerable improvement compared to their submission in the 2013 competition [29], although the first and second runners up also reached quite good performances without deep learning methods; [34] used a hierarchical classifier for combining audio and video features and [7] introduced an extension of Histogram of Oriented Gradients (HOG) descriptors for spatio-temporal data, which they fuse with other visual and audio features using MKL.", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "The adoption of deep learning for visual features likely played a big role in the considerable improvement compared to their submission in the 2013 competition [29], although the first and second runners up also reached quite good performances without deep learning methods; [34] used a hierarchical classifier for combining audio and video features and [7] introduced an extension of Histogram of Oriented Gradients (HOG) descriptors for spatio-temporal data, which they fuse with other visual and audio features using MKL.", "startOffset": 275, "endOffset": 279}, {"referenceID": 6, "context": "The adoption of deep learning for visual features likely played a big role in the considerable improvement compared to their submission in the 2013 competition [29], although the first and second runners up also reached quite good performances without deep learning methods; [34] used a hierarchical classifier for combining audio and video features and [7] introduced an extension of Histogram of Oriented Gradients (HOG) descriptors for spatio-temporal data, which they fuse with other visual and audio features using MKL.", "startOffset": 354, "endOffset": 357}, {"referenceID": 25, "context": "ConvNets have recently been shown to achieve state of the art performance in challenging object recognition tasks [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "The first and larger one is the Google dataset [5] consisting of 35,887 images with the seven facial expression classes: angry, disgust, fear, happy, sad, surprise and neutral.", "startOffset": 47, "endOffset": 50}, {"referenceID": 33, "context": "The second one is the Toronto Face Dataset (TFD) [35] containing 4,178 images labeled with basic emotions, essentially with only fully frontal facing poses.", "startOffset": 49, "endOffset": 53}, {"referenceID": 38, "context": "Registration To build a common dataset, TFD images and frames from the competition dataset had to be integrated with the Google dataset, for which we used the following procedure: For image registration we used 51 of the 68 facial keypoints extracted by the mixture of trees method from [40].", "startOffset": 287, "endOffset": 291}, {"referenceID": 15, "context": "dataset, we used the diffusion-based approach introduced in [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "We used the isotropic smoothing (IS) function from the INface toolbox [33,38] with the default smoothness parameter and without normalization as post-processing.", "startOffset": 70, "endOffset": 77}, {"referenceID": 36, "context": "We used the isotropic smoothing (IS) function from the INface toolbox [33,38] with the default smoothness parameter and without normalization as post-processing.", "startOffset": 70, "endOffset": 77}, {"referenceID": 24, "context": "Our ConvNet uses the C++ and CUDA implementation written by Alex Krizhevsky [26] interfaced in Python.", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "The first two stages include a convolutional layer followed by a pooling layer, then a local response normalization layer [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "These hyperparameters are the same as the one provided by Krizhevsky [26] in his example layers configuration files.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "For details on the architecture see [22].", "startOffset": 36, "endOffset": 40}, {"referenceID": 38, "context": "So we used the combined landmark placement and face detection method described in [40] to find faces in these clips.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "The video descriptors for the training set were then used to train an SVM (implemented by [6]) with a radial basis function (RBF) kernel.", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "As we have described earlier, deep learning based techniques have led to important successes in speech recognition [18,15].", "startOffset": 115, "endOffset": 122}, {"referenceID": 13, "context": "As we have described earlier, deep learning based techniques have led to important successes in speech recognition [18,15].", "startOffset": 115, "endOffset": 122}, {"referenceID": 17, "context": "In the context of emotion recognition on audio features extracted from movie clips, we used a deep learning approach for performing emotion recognition just by pretraining a deep MLP as a deep belief network (DBN) [19].", "startOffset": 214, "endOffset": 218}, {"referenceID": 14, "context": "Online PCA was applied on the extracted features, and 909 features per timescale were retained [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "The DBN has three layers of RBMs, the first layer is a Gaussian RBM with noisy rectified linear unit (ReLU) nonlinearity [9], the second and third layer are both GaussianBernoulli RBMs.", "startOffset": 121, "endOffset": 124}, {"referenceID": 14, "context": "We used a multi-time-scale learning model [16] for the MLP where we pooled the last hidden representation layer of an MLP so as to aggregate information across frames before a final softmax layer.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "2875 [20].", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "The rmsprop adaptive learning rate algorithm was used to tune the learning rate with a variation of Nesterov\u2019s Momentum [36].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "Using local motion features for activity recognition is a popular approach employed in many previous works [28,24,37,39].", "startOffset": 107, "endOffset": 120}, {"referenceID": 22, "context": "Using local motion features for activity recognition is a popular approach employed in many previous works [28,24,37,39].", "startOffset": 107, "endOffset": 120}, {"referenceID": 35, "context": "Using local motion features for activity recognition is a popular approach employed in many previous works [28,24,37,39].", "startOffset": 107, "endOffset": 120}, {"referenceID": 37, "context": "Using local motion features for activity recognition is a popular approach employed in many previous works [28,24,37,39].", "startOffset": 107, "endOffset": 120}, {"referenceID": 0, "context": "Traditional motion energy models [1] encode spatiotemporal features of successive video frames as sums of squared quadrature Fourier or Gabor coefficients across multiple frequencies and orientations [28].", "startOffset": 33, "endOffset": 36}, {"referenceID": 26, "context": "Traditional motion energy models [1] encode spatiotemporal features of successive video frames as sums of squared quadrature Fourier or Gabor coefficients across multiple frequencies and orientations [28].", "startOffset": 200, "endOffset": 204}, {"referenceID": 22, "context": "In contrast to the motion energy view, in [24] it has been shown that the learning of transformations and introduction of invariance can be viewed as two independent aspects of learning.", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "The classic approach is to use hand-engineered features for spatio-temporal feature extraction [39].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "In contrast to hand-engineered features, deep learning based methods have been shown to yield low-level motion features, which generalize well across datasets [28,24].", "startOffset": 159, "endOffset": 166}, {"referenceID": 22, "context": "In contrast to hand-engineered features, deep learning based methods have been shown to yield low-level motion features, which generalize well across datasets [28,24].", "startOffset": 159, "endOffset": 166}, {"referenceID": 26, "context": "We use a pipeline commonly employed in works on activity recognition [28,24,39] with the SAE model for local motion feature computation.", "startOffset": 69, "endOffset": 79}, {"referenceID": 22, "context": "We use a pipeline commonly employed in works on activity recognition [28,24,39] with the SAE model for local motion feature computation.", "startOffset": 69, "endOffset": 79}, {"referenceID": 37, "context": "We use a pipeline commonly employed in works on activity recognition [28,24,39] with the SAE model for local motion feature computation.", "startOffset": 69, "endOffset": 79}, {"referenceID": 26, "context": "We chose to use the SAE model because, compared to other learning based methods like ISA [28] and convGBM [37] with complex learning rules, it can be trained very efficiently, while performing competitively.", "startOffset": 89, "endOffset": 93}, {"referenceID": 35, "context": "We chose to use the SAE model because, compared to other learning based methods like ISA [28] and convGBM [37] with complex learning rules, it can be trained very efficiently, while performing competitively.", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "In past works It has been shown that spatially combining local features learned from smaller input regions leads to better representations than features learned on larger regions [28,8].", "startOffset": 179, "endOffset": 185}, {"referenceID": 7, "context": "In past works It has been shown that spatially combining local features learned from smaller input regions leads to better representations than features learned on larger regions [28,8].", "startOffset": 179, "endOffset": 185}, {"referenceID": 38, "context": "This region was globally chosen by visualizing many training images, but a more precise method, such as mouth keypoint extraction [40], could also be applied.", "startOffset": 130, "endOffset": 134}, {"referenceID": 7, "context": "[8], which achieved state-of-the-art performance on the CIFAR-10 dataset [25] in 2011, even though that method has since been superseded by convolutional networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[8], which achieved state-of-the-art performance on the CIFAR-10 dataset [25] in 2011, even though that method has since been superseded by convolutional networks.", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "After centering all patches from the same spatial region, we apply whitening, which was shown to be useful for this kind of approach [8], keeping 90% of the variance.", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "Each patch is assigned a 400-dimensional vector by comparing it to the centroids with the triangle activation function [8], where the Euclidean distance zk between the patch and each centroid is computed, as well as the mean \u03bc of these distances.", "startOffset": 119, "endOffset": 122}, {"referenceID": 20, "context": "A more detailed analysis of Convnet #2 and comparisons on AFEW2 can be found in [22], but we provide some highlights here.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "We implemented this via a search over discretized [0, 1, 2, 3] per dimension scaling factors.", "startOffset": 50, "endOffset": 62}, {"referenceID": 1, "context": "We implemented this via a search over discretized [0, 1, 2, 3] per dimension scaling factors.", "startOffset": 50, "endOffset": 62}, {"referenceID": 2, "context": "We implemented this via a search over discretized [0, 1, 2, 3] per dimension scaling factors.", "startOffset": 50, "endOffset": 62}, {"referenceID": 2, "context": "Recent work [3] has shown that random search for hyperparameter optimization can be an effective strategy, even when the dimensionality of hyperparameters is moderate (ex.", "startOffset": 12, "endOffset": 15}, {"referenceID": 30, "context": "MKL [32] 35.", "startOffset": 4, "endOffset": 8}, {"referenceID": 27, "context": "89% PLS [29] 34.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "61% Linear SVM [13] 29.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "Our method [22] 41.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "PLS [30] 50.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "37% HCF [34] 47.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "17% MKL [7] 45.", "startOffset": 8, "endOffset": 11}, {"referenceID": 32, "context": "67%, slightly higher than the first runner up in the EmotiW 2014 competition [34].", "startOffset": 77, "endOffset": 81}, {"referenceID": 1, "context": "Acknowledgements The authors would like to thank the developers of Theano [2,4].", "startOffset": 74, "endOffset": 79}, {"referenceID": 3, "context": "Acknowledgements The authors would like to thank the developers of Theano [2,4].", "startOffset": 74, "endOffset": 79}], "year": 2015, "abstractText": "The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based \u201cbag-of-mouths\u201d model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. S.E. Kahou, P. Froumenty, C. Pal \u00c9cole Polytechique de Montr\u00e9al, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Canada Email: {samira.ebrahimi-kahou, pierre.froumenty, christopher.pal}@polymtl.ca V. Michalski, K. Konda Goethe-Universit\u00e4t Frankfurt, Frankfurt, Germany Email: {michalskivince, konda.kishorereddy}@gmail.com X. Bouthillier, P. Lamblin, C. Gulcehre, S. Jean, Y. Dauphin, N. Boulanger-Lewandowski, R.C. Ferrari, M. Mirza, D. Warde-Farley, A. Courville, P. Vincent, R. Memisevic, Y. Bengio Laboratoire d\u2019Informatique des Syst\u00e8mes Adaptatifs, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Canada Email: {bouthilx, lamblinp, gulcehrc, jeasebas, dauphiya, boulanni, chandiar, mirzamom, wardefar, courvila, vincentp, memisevr, bengioy}@iro.umontreal.ca We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67% on the 2014 dataset.", "creator": "LaTeX with hyperref package"}}}