{"id": "1503.01820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Latent Hierarchical Model for Activity Recognition", "abstract": "We present a novel hierarchical model for human activity recognition. In contrast to approaches that successively recognize actions and activities, our approach jointly models actions and activities in a unified framework, and their labels are simultaneously predicted. The model is embedded with a latent layer that is able to capture a richer class of contextual information in both state-state and observation-state pairs. Although loops are present in the model, the model has an overall linear-chain structure, where the exact inference is tractable. Therefore, the model is very efficient in both inference and learning. The parameters of the graphical model are learned with a Structured Support Vector Machine (Structured-SVM). A data-driven approach is used to initialize the latent variables; therefore, no manual labeling for the latent states is required. The experimental results from using two benchmark datasets show that our model outperforms the state-of-the-art approach, and our model is computationally more efficient.", "histories": [["v1", "Fri, 6 Mar 2015 00:05:12 GMT  (3515kb,D)", "http://arxiv.org/abs/1503.01820v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG", "authors": ["ninghang hu", "gwenn englebienne", "zhongyu lou", "ben kr\\\"ose"], "accepted": false, "id": "1503.01820"}, "pdf": {"name": "1503.01820.pdf", "metadata": {"source": "CRF", "title": "Latent Hierarchical Model for Activity Recognition", "authors": ["Ninghang Hu", "Gwenn Englebienne", "Zhongyu Lou", "Ben Kr\u00f6se"], "emails": ["n.hu@uva.nl;", "g.englebienne@uva.nl;", "z.lou@uva.nl;", "b.j.a.krose@uva.nl)."], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}, {"heading": "II. RELATED WORK", "text": "The work to date can be divided into two methodologies: the first methodology subdivides the approaches based on the hierarchical arrangement of the model3, i.e. whether the model contains a single layer or several layers; the second methodology is based on the nature of the learning method, i.e. whether the method is discriminatory or generative."}, {"heading": "A. Single-layer Approach and Hierarchical Approach", "text": "Depending on the complexity and duration of the activities, activity detection approaches can be divided into two categories [14]: single-layer approaches and hierarchical approaches. Typical activities in this category include walking, waiting, falling, jumping and waving. However, activities in the real world are not always as simple as these basic actions. For example, the activity of preparing breakfast can consist of several actions, such as opening a refrigerator, getting a salad, and making coffee. Typical hierarchical approaches [9], [11], [24] - [26], archaic actions that are linked."}, {"heading": "B. Generative Models and Discriminative Models", "text": "In fact, most of them will be able to move to another world, in which they will be able to move to another world, in which they will be able to move, in which they will be able to move."}, {"heading": "III. MODELING ACTIVITY HIERARCHY", "text": "The graphical model of our proposed system is in Fig. 3. Let x = {x1, x2,.., xK | xk,.., be the sequence of observations, where K is the total number of time segments in the video. Our goal is to predict the most likely underlying action sequence y = {y1, y2,.., yK | yk, Y} and the associated activity designation A \u0394H based on the observations. We define x0 as the global characteristics resulting from x.Each observation xk is a feature vector extracted from the segment. The shape of xk is quite flexible. xk can be a collection of data from various sources, e.g. simple sensor measurements, human positions, human postures and object locations. Some of these observations may be highly correlated, e.g. portable accelerometers and motion sensors. Due to the discriminatory nature of our model, we do not need to define correlation between the observations."}, {"heading": "A. Potential Function", "text": "The first potential measures we take to have such a structure depend on whether we have such a structure, because it cannot be independent of a given example."}, {"heading": "V. LEARNING", "text": "We use the max margin approach to learn the parameters in our graphical model. In view of a number of N training examples, < x (n), y (n), A (n) > (n = 1, \u00b7 \u00b7 \u00b7, N), we would like to learn the model parameters w that can produce the activity mark A and action marks y that result in a new test entry x. Note: Both activities and action marks are observed during the training. The latent variables z are not observed and are automatically derived from the training process. The goal of learning is to find the optimal model parameters w that minimize the objective function. A regulation term is used to over-fitting.min w {12% w 2 + C N = 1 (i), y (i), y (i), y (i) that we find the optimal model parameters w that represent a normalization constant that is used to establish a balance between model complexity and adjustment."}, {"heading": "VI. EXPERIMENTS AND RESULTS", "text": "We implemented the proposed model, which is called the full model, along with its three variants. Specifically, the first model detects only low-level actions, the second model detects high-level activities, and the third model detects activities based on actions. All models were evaluated on two different datasets, and the results of the different models were compared to gain insights into our research questions (Section I)."}, {"heading": "A. Datasets", "text": "The methods were evaluated on two benchmark datasets, i.e., CAD-60 [12] and CAD-120 [9]. Both datasets contain sequences of color and depth images captured by an RGB-D sensor. [The two datasets are therefore completely different from each other, so they can be used to test the generalizability of our methods. CAD-60 datasets consist of 12 human action labels and no activity labels. Actions include brushing your mouth, brushing your teeth, wearing contact lenses, speaking on the phone, drinking water, opening pill containers (cooking), speaking on the couch, writing on the whiteboard, and working on a computer. These actions are performed by 4 different subjects in 5 different environments, i.e., a kitchen, a bedroom, a bathroom, and an office."}, {"heading": "C. Evaluation Criteria", "text": "In order to select the hyperparameters, i.e. the number of latent states and segmentation methods, we used two subjects for training, one subject for validation and one subject for the test. Once the optimal hyperparameters have been selected, the performance of the model is measured during the test by another cross-validation process, i.e. training with videos of 3 persons and tests on a new person. Each cross-validation is performed three times. To observe the generalization of our model across different datasets, the results are averaged across the wrinkles. In this essay, accuracy (classification rate), precision, recall and F-score are reported to allow comparison of results. In the CAD-120 dataset, more than half of the instances are achieved and moved. Therefore, we consider precision and memory as relatively better evaluation criteria than accuracy because they remain important despite class imbalances."}, {"heading": "D. Results and Analysis", "text": "This year it is so far that it only takes a few days to reach an agreement."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we present a hierarchical approach to simultaneously detecting actions and activities based on RGB-D data. Interactions between actions and activities are captured by a hidden-state CRF framework. In this framework, we use the latent variables to exploit the underlying action structures; the prediction is based on the common interaction between activities and actions, which contrasts with the traditional approach, which focuses on only one of them. Our results show a significant improvement in the use of the hierarchical model compared to the single-layer approach; the results also show the effectiveness of adding a latent layer to the model and the importance of the common assessment of actions and activities. Finally, we show that the proposed hierarchical approach exceeds the current methods on two benchmark datasets."}], "references": [{"title": "Towards robotic assistants in nursing homes: Challenges and results", "author": ["J. Pineau", "M. Montemerlo", "M. Pollack", "N. Roy", "S. Thrun"], "venue": "Robotics and Autonomous Systems, vol. 42, 2003, pp. 271\u2013281.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Living with seal robots - Its sociopsychological and physiological influences on the elderly at a care house", "author": ["K. Wada", "T. Shibata"], "venue": "IEEE Transactions on Robotics, vol. 23, 2007, pp. 972\u2013980.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A Survey on Human Activity Recognition using Wearable Sensors", "author": ["O.D. Lara", "M. a. Labrador"], "venue": "IEEE Communications Surveys & Tutorials, pp. 1\u201318, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Human activity prediction: Early recognition of ongoing activities from streaming videos", "author": ["M.S. Ryoo"], "venue": "Proceedings of the IEEE international conference on computer vision (ICCV). IEEE, 2011, pp. 1036\u2013 1043.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Activity recognition using semi-markov models on real world smart home datasets", "author": ["T.L.M. van Kasteren", "G. Englebienne", "B. Kr\u00f6se"], "venue": "Journal of Ambient Intelligence and Smart Environments, vol. 2, no. 3, pp. 311\u2013325, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Posture Recognition with a Top-view Camera", "author": ["N. Hu", "G. Englebienne", "B. Kr\u00f6se"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2013, pp. 2152\u20132157.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "A Two-layered Approach to Recognize High-level Human Activities", "author": ["\u2014\u2014"], "venue": "Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (ROMAN). IEEE, 2014, pp. 243\u2013248.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Human Activities and Object Affordances from RGB-D Videos", "author": ["H.S. Koppula", "R. Gupta", "A. Saxena"], "venue": "International Journal of Robotics Research (IJRR), vol. 32, no. 8, pp. 951\u2013970, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation", "author": ["H. Koppula", "A. Saxena"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning Latent Structure for Activity Recognition", "author": ["N. Hu", "G. Englebienne", "Z. Lou", "B. Kr\u00f6se"], "venue": "Proceedings of the IEEE  International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 1048\u20131053.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Unstructured human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2012, pp. 842\u2013849.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Assistive technology design and development for acceptable robotics companions for ageing years", "author": ["F. Amirabdollahian", "S. Bedaf", "R. Bormann", "H. Draper", "V. Evers", "J.G. P\u00e9rez", "G.J. Gelderblom", "C.G. Ruiz", "D. Hewson", "N. Hu", "Others"], "venue": "Paladyn, Journal of Behavioral Robotics, pp. 1\u201319, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Human Activity Analysis: A Review", "author": ["J.K. Aggarwal", "M.S. Ryoo"], "venue": "ACM Computing Surveys (CSUR), vol. 43, no. 3, p. 16, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning human actions via information maximization", "author": ["J. Liu", "M. Shah"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Analyzing and recognizing walking figures in XYT", "author": ["S. Niyogi", "E. Adelson"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), 1994.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities", "author": ["M. Ryoo", "J. Aggarwal"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint segmentation and classification of human actions in video", "author": ["M. Hoai", "Z. Lan", "F.D. la Torre"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. 3265\u2013 3272.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-User Identification and Efficient User Approaching by Fusing Robot and Ambient Sensors", "author": ["N. Hu", "R. Bormann", "T. Zw\u00f6lfer", "B. Kr\u00f6se"], "venue": "Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 5299\u20135306.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Model recommendation  11 for action recognition", "author": ["P. Matikainen", "R. Sukthankar", "M. Hebert"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2256\u2013 2263.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Human Action Segmentation and Recognition Using Discriminative Semi-Markov Models", "author": ["Q. Shi", "L. Cheng", "L. Wang", "A. Smola"], "venue": "International Journal of Computer Vision (IJCV), vol. 93, pp. 22\u201332, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding human intentions via hidden markov models in autonomous mobile robots", "author": ["R. Kelley", "M. Nicolescu", "A. Tavakkoli", "C. King", "G. Bebis"], "venue": "Proceedings of the International Conference on Human-Robot Interaction (HRI). IEEE, 2008, pp. 367\u2013374.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognition of visual activities and interactions by stochastic parsing", "author": ["Y. Ivanov", "A. Bobick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), vol. 22, 2000.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Spatial- Temporal correlatons for unsupervised action classification", "author": ["S. Savarese", "A. DelPozo", "J. Niebles", "L.F.-F.L. Fei-Fei"], "venue": "2008 IEEE Workshop on Motion and video Computing, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H.S. Koppula", "A. Saxena"], "venue": "Proceedings of the Robotics Science and Systems (RSS). RSS, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Human Daily Activity Recognition in Robotassisted Living using Multi-sensor Fusion", "author": ["C. Zhu", "W. Sheng"], "venue": "Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2009, pp. 2154\u20132159.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Active-learning assisted self-reconfigurable activity recognition in a dynamic environment", "author": ["Y. chen Ho", "C. hu Lu", "I. han Chen", "S. shinh Huang", "C. yao Wang", "L. chen Fu"], "venue": "Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2009, pp. 1567\u20131572.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Conditional Random Fields for Activity Recognition", "author": ["D.L. Vail", "M.M. Veloso", "J.D. Lafferty"], "venue": "Proceedings of the international joint conference on Autonomous agents and multiagent systems (AAMAS). ACM, 2007, p. 235.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Hidden conditional random fields for gesture recognition", "author": ["S.B. Wang", "A. Quattoni"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), vol. 2. IEEE, 2006, pp. 1521\u20131527.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Max-margin hidden conditional random fields for human action recognition", "author": ["Y. Wang", "G. Mori"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2009, pp. 872\u2013879.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Hidden Conditional Random Fields", "author": ["A. Quattoni", "S. Wang", "L.-P. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), vol. 29, no. 10, pp. 1848\u2013 1852, 2007.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1848}, {"title": "Hidden-Unit Conditional Random Fields", "author": ["L. Maaten", "M. Welling", "L.K. Saul"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics, 2011, pp. 479\u2013488.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Order-Preserving sparse coding for sequence classification", "author": ["B. Ni", "P. Moulin", "S. Yan"], "venue": "Proceedings of the IEEE european conference on computer vision (ECCV). Springer, 2012, pp. 173\u2013187.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimizing binary MRFs via extended roof duality", "author": ["C. Rother", "V. Kolmogorov", "V. Lempitsky", "M. Szummer"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2007, pp. 1\u20138.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning Latent Temporal Structure for Complex Event Detection", "author": ["K. Tang", "L. Fei-Fei", "D. Koller"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 1250\u20131257.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic Programming and Lagrange Multipliers", "author": ["R. Bellman"], "venue": "The Bellman Continuum: A Collection of the Works of Richard E. Bellman, p. 49, 1986.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1986}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, pp. 1453\u20131484, 2005.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning structural SVMs with latent variables", "author": ["C.N. Yu", "T. Joachims"], "venue": "Proceedings of the International Conference on Pattern Recognition (ICPR). ACM, 2009, pp. 1169\u20131176.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "The concave-convex procedure (CCCP)", "author": ["A. Yuille", "A. Rangarajan"], "venue": "Advances in neural information processing systems (NIPS), vol. 2, pp. 1033\u20131040, 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "The cutting-plane method for solving convex programs", "author": ["J.E. Kelley", "Jr."], "venue": "Journal of the Society for Industrial and Applied Mathematics, vol. 8, no. 4, pp. 703\u2013712, 1960.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1960}, {"title": "The EM Algorithm and Extensions", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": "New York, vol. 274, p. 359, 1997.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Numerous studies have focused on providing people with physical [1], cognitive [2] or social [3] support.", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "Numerous studies have focused on providing people with physical [1], cognitive [2] or social [3] support.", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Different types of sensors have been applied to the task of activity recognition [4], [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Different types of sensors have been applied to the task of activity recognition [4], [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "[6] adopt a set of simple sensors, i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] use a ceiling-mounted color camera to recognize human postures, and the postures are recognized based on still images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The recognition of actions is usually formulated as a sequential prediction problem [8] (see Fig.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "Most previous work addresses activity and action recognition as separate tasks [8]\u2013[10], i.", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Most previous work addresses activity and action recognition as separate tasks [8]\u2013[10], i.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "The proposed model of this paper is based on our previous work [11], wherein we recognize the sequence of actions using Conditional Random Fields (CRFs).", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "The model was evaluated using the RGB-D data from two different benchmark datasets [9], [12].", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "The model was evaluated using the RGB-D data from two different benchmark datasets [9], [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "The results are compared with a number of the state-of-the-art approaches [8]\u2013[12].", "startOffset": 74, "endOffset": 77}, {"referenceID": 10, "context": "The results are compared with a number of the state-of-the-art approaches [8]\u2013[12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Single-layer Approach and Hierarchical Approach Human activity recognition is a key component for HRI, particularly for the re-ablement of the elderly [13].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Depending on the complexity and duration of activities, activity recognition approaches can be separated into two categories [14]: single-layer approaches and hierarchical approaches.", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "Singlelayer approaches [15]\u2013[23] refer to methods that are able to directly recognize human activities from the data without defining any activity hierarchy.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Singlelayer approaches [15]\u2013[23] refer to methods that are able to directly recognize human activities from the data without defining any activity hierarchy.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "Typical hierarchical approaches [9], [11], [24]\u2013[26] first estimate the sub-level actions, and then, the high-level activity labels are inferred based on the action sequences.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "Typical hierarchical approaches [9], [11], [24]\u2013[26] first estimate the sub-level actions, and then, the high-level activity labels are inferred based on the action sequences.", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "Typical hierarchical approaches [9], [11], [24]\u2013[26] first estimate the sub-level actions, and then, the high-level activity labels are inferred based on the action sequences.", "startOffset": 43, "endOffset": 47}, {"referenceID": 24, "context": "Typical hierarchical approaches [9], [11], [24]\u2013[26] first estimate the sub-level actions, and then, the high-level activity labels are inferred based on the action sequences.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "[12] proposed a hierarchical maximum entropy Markov model that detects activities from RGB-D videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] presented an interesting approach that models both activities and object affordance as random variables.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 139, "endOffset": 142}, {"referenceID": 28, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 160, "endOffset": 164}, {"referenceID": 29, "context": ", Hidden Markov Models (HMMs) [12], [27], Dynamic Bayesian Networks (DBNs) [28], linear-chain CRFs [29], loopy CRFs [9], SemiMarkov Models [6], and Hidden CRFs [30], [31], have been applied to the recognition of human activities.", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "The graphical models can be divided into two categories: generative models [12], [27] and discriminative models [6], [7], [9].", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "The graphical models can be divided into two categories: generative models [12], [27] and discriminative models [6], [7], [9].", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "The graphical models can be divided into two categories: generative models [12], [27] and discriminative models [6], [7], [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "The graphical models can be divided into two categories: generative models [12], [27] and discriminative models [6], [7], [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "The graphical models can be divided into two categories: generative models [12], [27] and discriminative models [6], [7], [9].", "startOffset": 122, "endOffset": 125}, {"referenceID": 30, "context": "However, these models are limited because they cannot capture the intermediate structures within the target states [32].", "startOffset": 115, "endOffset": 119}, {"referenceID": 31, "context": "The names of these models, including Hiddenunit CRF [33], Hidden-state CRF [32] or Hidden CRF [31], are inter-changeable in the literature.", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "The names of these models, including Hiddenunit CRF [33], Hidden-state CRF [32] or Hidden CRF [31], are inter-changeable in the literature.", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "The names of these models, including Hiddenunit CRF [33], Hidden-state CRF [32] or Hidden CRF [31], are inter-changeable in the literature.", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "[9] present a model for the temporal and spatial interactions between humans and objects in loopy CRFs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The results show that by modeling the human-object interaction, their model outperforms the earlier work in [12] and [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 32, "context": "The results show that by modeling the human-object interaction, their model outperforms the earlier work in [12] and [34].", "startOffset": 117, "endOffset": 121}, {"referenceID": 33, "context": "The inference in the loopy graph is solved as a quadratic optimization problem using the graph-cut method [35].", "startOffset": 106, "endOffset": 110}, {"referenceID": 34, "context": "Another study [36] augments an additional layer of latent variables to the linear-chain CRFs.", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "In contrast to [9], Tang et al.", "startOffset": 15, "endOffset": 18}, {"referenceID": 34, "context": "[36] solve the inference problem by reforming the graph into a set of cliques so that the exact inference can be efficiently solved using dynamic programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "First, similar to [36], our model also uses an extra latent layer.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "Third, although our graph also presents many loops, as in [9], we are able to transform the cyclic graph into a linear-chain structure wherein the exact inference is tractable.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "The exact inference in our graph only requires two passes of messages across the linear chain structure, which is substantially more efficient than the method in [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "Our results show that the model outperforms the state-of-the-art approaches on the CAD120 dataset [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 29, "context": "Comparing with the normal transition potentials [31], our model leverages the latent variable zk for modeling richer contextual information over consecutive temporal segments.", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "In the linear-chain CRF, the exact inference can be efficiently performed using dynamic programming [37].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "This object function can be viewed as a generalized form of our previous work [11], where we recognize only the sequence of actions.", "startOffset": 78, "endOffset": 82}, {"referenceID": 36, "context": "Following [38] and [39], we substitute the loss function in (10) by the margin rescaling surrogate, which serves as an upper-bound of the loss function.", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "Following [38] and [39], we substitute the loss function in (10) by the margin rescaling surrogate, which serves as an upper-bound of the loss function.", "startOffset": 19, "endOffset": 23}, {"referenceID": 38, "context": "This can be solved with the Concave-Convex Procedure (CCCP) [40].", "startOffset": 60, "endOffset": 64}, {"referenceID": 39, "context": "This can be solved using the cutting-plane method [41].", "startOffset": 50, "endOffset": 54}, {"referenceID": 40, "context": "Another intuitive method of understanding the CCCP algorithm is to consider the algorithm as one that solves the learning problem with incomplete data using ExpectationMaximization (EM) [42].", "startOffset": 186, "endOffset": 190}, {"referenceID": 10, "context": ", CAD-60 [12] and CAD-120 [9].", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": ", CAD-60 [12] and CAD-120 [9].", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "In contrast, the CAD-120 dataset [9] contains 126 RGBD videos, and each video contains one activity and a sequence of actions.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "The objects in CAD-120 are automatically detected as in [9], and the locations of the objects are also provided by the dataset.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "A number of recent approaches [8]\u2013[12] have been evaluated on these two datasets; therefore, the results can be directly compared.", "startOffset": 30, "endOffset": 33}, {"referenceID": 10, "context": "A number of recent approaches [8]\u2013[12] have been evaluated on these two datasets; therefore, the results can be directly compared.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "To ensure a fair comparison, the same input features are extracted following [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "1) Recognize Only Low-level Actions: The first model is adopted from our previous work [11], which predicts action labels based on the video sequence.", "startOffset": 87, "endOffset": 91}, {"referenceID": 36, "context": "The parameters of this model are learned with the Structured-SVM framework [38].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "The object affordance labels are provided by the CAD-120 dataset, which are used for training in [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "For CAD-60, we apply uniform segmentation as in [9] to enable a fair comparison with other methods.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "[8] [11] in Table I refers to a combination of the first and third baseline approaches, where we used a two-step approach to successively recognize actions and activities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[8] [11] in Table I refers to a combination of the first and third baseline approaches, where we used a two-step approach to successively recognize actions and activities.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "To be comparable with the other approaches, following [9], [12], we conduct similar experiments on the CAD-60 dataset, where we group the actions based on their environment labels and a separate model is learned and tested for each of the groups.", "startOffset": 54, "endOffset": 57}, {"referenceID": 10, "context": "To be comparable with the other approaches, following [9], [12], we conduct similar experiments on the CAD-60 dataset, where we group the actions based on their environment labels and a separate model is learned and tested for each of the groups.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "We note that our model outperforms [12] in all five environments.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "Compared with the state of the art [9], our model outperforms [12] and [9] on most of the environments.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "Compared with the state of the art [9], our model outperforms [12] and [9] on most of the environments.", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Compared with the state of the art [9], our model outperforms [12] and [9] on most of the environments.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "On average, the precision of our model is the same as in [9], and the recall of the model outperforms [9] by over 8 percentage points, achieving 80.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "On average, the precision of our model is the same as in [9], and the recall of the model outperforms [9] by over 8 percentage points, achieving 80.", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "[9] 86.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] 89.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] [11] 87.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[8] [11] 87.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "[9] 68.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] 70.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] [11] 70.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[8] [11] 70.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "[8] [11] 66.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[8] [11] 66.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "The average F-Score is over 4% percentage points better than in [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Similar to [8], Koppula et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "[10] use a two-step approach to infer high-level activity labels only after the actions are estimated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] 72.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] 88.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[2] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] O.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] \u2014\u2014, \u201cA Two-layered Approach to Recognize High-level Human Activities,\u201d in Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (ROMAN).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Q.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[38] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[41] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] G.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We present a novel hierarchical model for human activity recognition. In contrast to approaches that successively recognize actions and activities, our approach jointly models actions and activities in a unified framework, and their labels are simultaneously predicted. The model is embedded with a latent layer that is able to capture a richer class of contextual information in both state-state and observation-state pairs. Although loops are present in the model, the model has an overall linear-chain structure, where the exact inference is tractable. Therefore, the model is very efficient in both inference and learning. The parameters of the graphical model are learned with a Structured Support Vector Machine (Structured-SVM). A data-driven approach is used to initialize the latent variables; therefore, no manual labeling for the latent states is required. The experimental results from using two benchmark datasets show that our model outperforms the state-of-the-art approach, and our model is computationally more efficient.", "creator": "LaTeX with hyperref package"}}}