{"id": "1503.01824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Deep Clustered Convolutional Kernels", "abstract": "Deep neural networks have recently achieved state of the art performance thanks to new training algorithms for rapid parameter estimation and new regularization methods to reduce overfitting. However, in practice the network architecture has to be manually set by domain experts, generally by a costly trial and error procedure, which often accounts for a large portion of the final system performance. We view this as a limitation and propose a novel training algorithm that automatically optimizes network architecture, by progressively increasing model complexity and then eliminating model redundancy by selectively removing parameters at training time. For convolutional neural networks, our method relies on iterative split/merge clustering of convolutional kernels interleaved by stochastic gradient descent. We present a training algorithm and experimental results on three different vision tasks, showing improved performance compared to similarly sized hand-crafted architectures.", "histories": [["v1", "Fri, 6 Mar 2015 00:53:40 GMT  (1058kb)", "http://arxiv.org/abs/1503.01824v1", "draft"]], "COMMENTS": "draft", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["minyoung kim", "luca rigazio"], "accepted": false, "id": "1503.01824"}, "pdf": {"name": "1503.01824.pdf", "metadata": {"source": "META", "title": "Deep Clustered Convolutional Kernels", "authors": ["Minyoung Kim", "Luca Rigazio"], "emails": ["MINYOUNG.KIM@US.PANASONIC.COM", "LUCA.RIGAZIO@US.PANASONIC.COM"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.01 824v 1 [cs.L G] 6M ar2 01"}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they are able to live, in which they, in which they are able to be able to"}, {"heading": "2. Deep Clustered Convolutional Kernels", "text": "The basic idea for our Deep Clustered Convolutional Kernels (DCCKs) is a revolutionary model architecture and associated structural training algorithm. We include a split / merge outer loop into the training process that first increases the model capacity to model new variability factors that can be seen in the data, then estimates new parameters for this larger model through stochastic gradient descent (SGD), and finally reduces the model capacity to minimize redundancy in the model space.Our approach is based on previous work in the field of Gaussian Kernel HMMs (Sankar, 1998; Rigazio et al., 2000; Bocchieri & Mak, 2001; Lee et al., 2001) and is philosophically based on Occam's Razor principle, with a smaller model with similar performance on a given dataset likely having better generalization capabilities to new invisible data."}, {"heading": "2.1. Training algorithm", "text": "From an initial network architecture, we train the model from SGD to validation. Next, we increase the complexity of the model of selected Convolutionary Layers by splitting the Constitutional Cores. Splitting is intended to create new plausible Convolutionary Filters that cover the current set of filters and are done on the cores by applying image pre-processing techniques, as well as adding blur and noise to generate enough variation. After splitting, the model is trained again by SGD and possibly split again until performance is achieved. At this point, the model is merged to reduce parameter space redundancy and trained again by SGD."}, {"heading": "2.1.1. SPLITTING KERNELS", "text": "With splitting, we want to increase the complexity of the model by creating new revolutionary cores from the set of already well-formed cores. Therefore, we create new cores by selectively selecting from a fixed set of transformations. The possible set of transformations we can play with is huge and includes the six isometries of the plane, angular rotation, contrast change (negative \"inversion\") and many others. In our experiments, we focus on two transformations that seemed to offer a consistent improvement: \u2022 Rotation creates new cores by rotating existing cores in random directions. \u2022 Disturbance from noise creates new cores by adding Gaussian noise to the existing cores. An important aspect we have verified in our experiments is that rotating cores cause lower computational costs during training than rotating training images to produce optimal deviation of the cores."}, {"heading": "2.1.2. MERGING KERNELS", "text": "After the splitting step, the model may have too much capacity and thus overparameterise part of the model, possibly resulting in overfitting and lower generalisation performance. Therefore, the purpose of the merger step is to eliminate redundancies in the model space and reduce the model size while maintaining the overall accuracy of the model. In our algorithm, we use k-mean clusters to merge cores, as of course k-mean cluster distortions below the defined distortion scale (we use the L2 standard to calculate cluster distortion). We find empirically that k-mean cluster cards are effective in reducing kernel redundancy (see filters in 2.1.2). Then we train the network and get weight and bias matrices from each convolutional layer, and then select the filters closest to each centrifuge."}, {"heading": "3. Experimental results", "text": "Our experimental results are based on three different datasets: MNIST, German Traffic Sign Recognition Benchmark (GTSRB) and CIFAR-10. In order to make our experiments significant and validate our approach, we started from hand-controlled model architectures that came as close to the state of the art as possible, in an effort to prove that our split / merge method can still improve the model architecture even if it is based on a very highly tuned architecture. baseline performance is presented in Table 1. For all experiments, we used the BVLC Caffe C + + package (Jia et al., 2014). We started our experiments with MNIST because the fast training time allowed us to quickly determine a reasonable range of hyperparameters such as the number of centrioids, the number of nuclei for the split / merge method. Next, we go to a more realistic task as we set out the results of our GTSAR model, each of which were extremely close to the state of the art and the FRB."}, {"heading": "3.1. MNIST results", "text": "The MNIST dataset contains 60,000 training images and 10,000 test images of handwritten 28x28 digits. The base model consists of two revolutionary layers and two fully connected layers, with ReLU and pooling following each revolutionary layer. This base model achieves an error rate of 0.82% with this simple network. The DCCKs training algorithm starts with splitting the first revolutionary layer from 100 to 200 cores; after subsequent fine tuning, the model achieved an error rate of 0.59%, which is almost 30% relative improvement over the original model. Compared to a newly trained 200-core model, which reaches 0.78%, and even a newly trained 300-core model, which reaches 0.75%, this confirms that splitting filters have the potential to help the following SGD-based fine tuning reach an optimal point that generalists are better at."}, {"heading": "3.2. GTSRB results", "text": "The GTSRB dataset contains 39,209 training images and 12,630 test images of various sizes, with 43 different classes consisting of standardized traffic signs from Germany (Houben et al., 2013). First, we reduced all images to 48x48 and then applied pre-processing techniques such as histogram compensation, adaptive histogram compensation and contrast normalization. For this task, we used two groups of output networks: a single model baseline GTSRB1, consisting of three revolutionary and two fully interconnected, which achieve a 2.44% error rate, and a larger MCDNN-inspired ensemble model GTSRB-3DNN (Ciresan et al., 2012), which has a 1.24% error rate."}, {"heading": "1 ORIGINAL 100 50 0.82", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 ORIGINAL 200 50 0.78", "text": "We note that the ensemble models use different input sizes of 48x48 pixels, 38x48 pixels, and 28x48 pixels. Therefore, we expected a high degree of redundancy on the GTSRB-3DNN cores, which can be successfully exploited by the fusion step of the DCCKs. In fact, we could easily identify an abundance of redundancy by visual inspection of the lower convolutionary layers (see 2.1.2). Due to this highly redundant structure in the initial model, we reversed the sequence of our training process to fuse the cores first, rather than splitting them, which maintains accuracy and allows for significantly faster training. Furthermore, the specific structure of the traffic signs provided for some peculiar behaviors on this database shows that, for example, the core rotation helped improve performance. A detailed inspection of the detection errors highlighted that several traffic signs were misdistributed by the base model."}, {"heading": "3.3. CIFAR10 results", "text": "The CIFAR-10 dataset consists of 50,000 training images and 10,000 test images. Each image has 32 x 32 pixels and represents a class of naturally occurring objects. To develop the CIFAR-10 baseline, we used the same techniques discussed in (Goodfellow et al., 2013) and the Network-InNetwork (Lin et al., 2013) model, which achieves a 10.4% error rate, which is a reasonable distance from the state of the art. If we apply DCCK's training to the CIFAR-10 dataset, the increased performance is not as large as for the previous datasets, but it is still significant and consistent. We believe this is due to the highly successful highly (manually) optimized NetworkIn-Network architecture making it difficult for the automatically created DCCKs to achieve a large improvement. Therefore, these results should show that even when applied to more complex parameters, we can still increase the average by 0.5% while some of the parameters can be improved."}, {"heading": "4. Discussion", "text": "In this thesis, we introduced the concept of DCCKs and introduced a training process in which Convolutionary Cores learned from SGD can be effectively divided and merged, and experimental results confirmed this process in a gradual improvement in performance, while the training algorithm collectively optimizes the structure and parameters of the model. Results show that DCCKs parsimonious use of model capacity by approaching the minimum number of parameters that provide the best performance, even when starting with highly manual optimization of the network architecture. Figure 3 shows that the training and validation of data loss goes beyond fine-tuning epochs; the \"merge\" curves point to the formation and generalization of models that exhibit the curves, such as the \"merge\" curve consistently exceeds the \"curve and the validation of data loss beyond fine-tuning epochs."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "The minimum description length principle in coding and modeling", "author": ["Barron", "Andrew", "Rissanen", "Jorma", "Yu", "Bin"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Barron et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Barron et al\\.", "year": 1998}, {"title": "Subspace distribution clustering hidden markov model", "author": ["Bocchieri", "Enrico", "Mak", "BK-W"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Bocchieri et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bocchieri et al\\.", "year": 2001}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "CoRR, abs/1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Multicolumn deep neural networks for image classification", "author": ["Ciresan", "Dan", "Meier", "Ueli", "Schmidhuber", "Jrgen"], "venue": "In IN PROCEEDINGS OF THE 25TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["Dahl", "George E", "Yu", "Dong", "Deng", "Li", "Acero", "Alex"], "venue": "IEEE Transactions on,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["Hinton", "Geoffrey E", "Zemel", "Richard S"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hinton et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1994}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Hornik", "Kurt"], "venue": "Neural networks,", "citeRegEx": "Hornik and Kurt.,? \\Q1991\\E", "shortCiteRegEx": "Hornik and Kurt.", "year": 1991}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark", "author": ["Houben", "Sebastian", "Stallkamp", "Johannes", "Salmen", "Jan", "Schlipsing", "Marc", "Igel", "Christian"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Houben et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Houben et al\\.", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Sergio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sergio et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Kolmogorov\u2019s theorem and multilayer neural networks", "author": ["K\u016frkov\u00e1", "V\u011bra"], "venue": "Neural networks,", "citeRegEx": "K\u016frkov\u00e1 and V\u011bra.,? \\Q1992\\E", "shortCiteRegEx": "K\u016frkov\u00e1 and V\u011bra.", "year": 1992}, {"title": "Data-driven design of hmm topology for online handwriting recognition", "author": ["Lee", "Jay J", "Kim", "Jahwan", "Jin H"], "venue": "International journal of pattern recognition and artificial intelligence,", "citeRegEx": "Lee et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2001}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "Experiments with a gaussian mergingsplitting algorithm for hmm training for speech recognition", "author": ["Sankar", "Ananth"], "venue": "In Proceedings of DARPA Speech Recognition Workshop,", "citeRegEx": "Sankar and Ananth.,? \\Q1998\\E", "shortCiteRegEx": "Sankar and Ananth.", "year": 1998}, {"title": "Mdl-based contextdependent subword modeling for speech recognition", "author": ["Shinoda", "Koichi", "Watanabe", "Takao"], "venue": "The Journal of the Acoustical Society of Japan (E),", "citeRegEx": "Shinoda et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Shinoda et al\\.", "year": 2000}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1958}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lior"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Taigman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Recently, deep neural networks (DNNs) have led to significant improvement in several machine learning domains, from speech recognition (Dahl et al., 2012) to computer vision (Krizhevsky et al.", "startOffset": 135, "endOffset": 154}, {"referenceID": 12, "context": ", 2012) to computer vision (Krizhevsky et al., 2012; Taigman et al., 2013) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 21, "context": ", 2012) to computer vision (Krizhevsky et al., 2012; Taigman et al., 2013) and machine translation (Sutskever et al.", "startOffset": 27, "endOffset": 74}, {"referenceID": 9, "context": "DNNs have reached state of the art performance thanks to their theoretically proven modeling and generalization capabilities (Hornik et al., 1989; Hornik, 1991; K\u016frkov\u00e1, 1992), and practically driven by improvements in training algorithms for rapid parameter estimation (Martens, 2010; Sutskever et al.", "startOffset": 125, "endOffset": 175}, {"referenceID": 20, "context": ", 1989; Hornik, 1991; K\u016frkov\u00e1, 1992), and practically driven by improvements in training algorithms for rapid parameter estimation (Martens, 2010; Sutskever et al., 2013), novel regularization methods to reduce overfitting (Srivastava et al.", "startOffset": 131, "endOffset": 170}, {"referenceID": 6, "context": "creasing data-sets (Deng et al., 2009) and powerful new computing platforms (Chetlur et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 3, "context": ", 2009) and powerful new computing platforms (Chetlur et al., 2014).", "startOffset": 45, "endOffset": 67}, {"referenceID": 1, "context": "Information theoretic methods, such as the minimum description length criterion, were also applied to the problem of structural optimization (Barron et al., 1998), resulting in improved performance in speech recognition (Shinoda & Watanabe, 2000) and as well as training algorithms for autoencoders (Hinton & Zemel, 1994).", "startOffset": 141, "endOffset": 162}, {"referenceID": 18, "context": "Although, recently Bayesian optimization of hyper-parameters have been introduced (Snoek et al., 2012).", "startOffset": 82, "endOffset": 102}, {"referenceID": 14, "context": "Our approach takes inspiration by previous work in the area of Gaussian kernel HMMs (Sankar, 1998; Rigazio et al., 2000; Bocchieri & Mak, 2001; Lee et al., 2001), and is philosophically based on Occam\u2019s razor principle whereby a smaller model with similar performance on a given data-set is likely to have better generalization capabilities to new unseen data.", "startOffset": 84, "endOffset": 161}, {"referenceID": 10, "context": "The GTSRB data-set contains 39,209 training images and 12630 testing images of various size, with 43 different classes consisting of standard traffic signs from Germany (Houben et al., 2013).", "startOffset": 169, "endOffset": 190}, {"referenceID": 4, "context": "44% error rate, and larger state of the art ensemble model GTSRB-3DNN (Table 4), inspired by MCDNN(Ciresan et al., 2012), and reaching 1.", "startOffset": 98, "endOffset": 120}], "year": 2015, "abstractText": "Deep neural networks have recently achieved state of the art performance thanks to new training algorithms for rapid parameter estimation and new regularization methods to reduce overfitting. However, in practice the network architecture has to be manually set by domain experts, generally by a costly trial and error procedure, which often accounts for a large portion of the final system performance. We view this as a limitation and propose a novel training algorithm that automatically optimizes network architecture, by progressively increasing model complexity and then eliminating model redundancy by selectively removing parameters at training time. For convolutional neural networks, our method relies on iterative split/merge clustering of convolutional kernels interleaved by stochastic gradient descent. We present a training algorithm and experimental results on three different vision tasks, showing improved performance compared to similarly sized hand-crafted architectures.", "creator": "LaTeX with hyperref package"}}}