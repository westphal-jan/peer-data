{"id": "1503.01910", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Sequential Relevance Maximization with Binary Feedback", "abstract": "Motivated by online settings where users can provide explicit feedback about the relevance of products that are sequentially presented to them, we look at the recommendation process as a problem of dynamically optimizing this relevance feedback. Such an algorithm optimizes the fine tradeoff between presenting the products that are most likely to be relevant, and learning the preferences of the user so that more relevant recommendations can be made in the future.", "histories": [["v1", "Fri, 6 Mar 2015 11:02:41 GMT  (102kb,D)", "http://arxiv.org/abs/1503.01910v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["vijay kamble", "nadia fawaz", "fernando silveira"], "accepted": false, "id": "1503.01910"}, "pdf": {"name": "1503.01910.pdf", "metadata": {"source": "CRF", "title": "Sequential Relevance Maximization with Binary Feedback", "authors": ["Vijay Kamble", "Nadia Fawaz", "Fernando Silveira"], "emails": ["vjk@eecs.berkeley.edu", "nadia.fawaz@technicolor.com", "fernando.silveira@technicolor.com"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "1.1 Related work", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "1.2 Structure of the paper", "text": "In Section 2 we present our model and define the problem of relevance optimization. Section 3 deals with the analysis of this problem by deriving key structural properties of the optimal policy and finally presenting an algorithm for its calculation. In Section 4 we propose two strategies that are easier to calculate and prove to be approximately optimal. In Section 5 we simulate our two approximately optimal strategies for randomly generated problem cases and compare their performance with the optimal policy. Finally, Section 6 summarizes our work and discusses extensions of our model. The evidence of all our results can be found in the appendix."}, {"heading": "2 Model", "text": "We consider the setting of a user who enters an online system as a sequence in which products from different categories are presented, with the aim of maximizing the number of relevant products available to him before he leaves the system."}, {"heading": "2.1 Relevance maximization", "text": "The primary objective of the system designer is to maximize the expected number of relevant products marked in the session. (...) It is the question of whether it is a problem that has arisen in the past. (...) It is the question of whether it is a problem that has arisen in the past. (...) It is the question of whether it is a problem. (...) It is the question of whether it is a problem. (...) It is the question of whether it is a problem that has arisen in the past. (...) It is the question of whether it is a problem. (...) It is the question of whether it is a problem. (...) It is the question of whether it is a problem. (...) It is the question of whether it is a problem. (... It is the question.) It is the question. (... It is the question. (... It is the question.) It is the question."}, {"heading": "3 Characteristics of the optimal allocation policy", "text": "In this section we present some structural characteristics of the optimal allocation policy."}, {"heading": "3.1 Property 1: If category A is relevant, show it", "text": "First of all, we present the following intuitive property.Lemma 3.1. In the optimal allocation policy, at every possible opportunity, depending on previous observations, if there is a product category j that generates positive feedback with probability 1, i.e. P (Yj = 1 | Ht) = 1, then every product in j that has not been shown is immediately allocated. If there are several such products, they can be allocated in any order. This property implies that if a positive feedback is received for a product of a certain category j, all Lj products of this category should be presented in the immediately following opportunities2. The proof uses a simple probabilistic exchange argument."}, {"heading": "3.2 Property 2: If \u2018likes A\u2019 implies \u2018likes B\u2019, then show B before showing A", "text": "To describe this next property, we first formally define a few ideas. In the dynamic mapping of the products to the possibilities, we will invoke a possibility to be an experimental possibility, if there is no such category, there is not a single category that is so dominated that Yj = 1 is likely to be dominated. So, if there is such a category, the previous problem tells us that all non-trivial decisions are exhausted in the optimal dynamic mapping policy. However, since there is no such category, an experimental opportunity brings us the non-trivial problem of deciding which category is presented to the user next. Thus, all non-trivial decisions in the optimal dynamic mapping policy are made at the experimental possibilities. Let's S (t) = {i) the experimental possibility that gives us the non-trivial problem of the user types that have a non-zero probability on the history. Then, we note that after considering the feedback, the mapping becomes an experimental possibility."}, {"heading": "3.3 Structure of the optimal policy", "text": "The lemmas 3.1, 3.2 and 3.3 reveal the following structure of optimal politics. Starting with a series of non-dominated equivalence classes of categories, these classes are presented in a certain order as long as we continue to receive negative feedback. If any class receives positive feedback in the process, we present all products in that class, \"zoom into\" the next level \"(which eliminates all other types from the relevance matrix), and restart with a new group of non-dominated equivalence classes. Using this structure, the following algorithm 2 calculates the optimal pair effect. Algorithm 2 (Optimal) function V (Q, \u03b2), where Q is a relevance matrix and p is a probability distribution over user types. \u2022 If Q is empty, the return V (Q, p) = 0. \u2022 If Q is not empty, enumerate the non-dominated equivalence classes of Q."}, {"heading": "4 Approximately optimal policies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Policy 1: Farsighted Greedy", "text": "Let's consider the optimization problem (3) assuming that we V (Q\u03c0k) for each k = 1, \u00b7 K, and each p = 1, \u00b7 \u00b7 \u00b7 \u00b7 K, so that k / p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "4.2 Policy 2: Naive Greedy", "text": "Another simple heuristic that we can apply is the following greedy policy.Politics (Naively greedy) Let the number of non-dominated equivalence classes be in each of these classes on an experiment opportunity (U1 (t), \u00b7 \u00b7, UK (t)) and (L1 (t), \u00b7 \u00b7, LK (t). Then select a product from a class k *, where the naively greedy policy achieves a factor of 1 \u2212 \u03b2 Lmin1 + \u03b2 \u2212 \u03b2H of the optimal payoff. Note that in the worst case, if Lmin = 1 and H is large, the greedy algorithm achieves at least 1 \u2212 \u03b2 1 + \u03b2 factor of optimal benefit."}, {"heading": "4.3 A lower bound for \u03b2 close to 1", "text": "Intuitively, this results from the observation that if the user stays long enough so that the number of available advertising options is greater than L, each policy receives all the positive feedback that can be received. Theorem 4.3. Each viable policy achieves an \u03b2L \u2212 1 factor of optimal benefit."}, {"heading": "5 Simulations", "text": "In this section, we compare the performance of the greedy with forward-looking policies and naive greedy policies with optimal policies. We generate 50 random samples, each with 5 x 5 and 7 x 7 relevance matrices, with randomly selected priorities. We calculate the benefits among all three strategies, for \u03b2 in the range of 0 to 1. For each \u03b2, we then plot the average and minimum across the 50 samples of the ratio of disbursement under a non-optimal policy and optimal policy. Our results are shown in Figure 4.Note that even in the worst case, both strategies are very close to optimum in the samples. Also, note that for \u03b2 near 0 and for \u03b2 near 1 the disbursement under both strategies responds to the optimal disbursement, which confirms our limits in theories 4.1, 4.2 and 4.3. The curve corresponding to naive greedy policies is smooth because the policy does not depend on \u03b2 and therefore the resulting disbursement in size (and thus the optimal disbursement) is continuous."}, {"heading": "6 Discussion and Conclusions", "text": "Our main contribution in this paper is the introduction and analysis of sequential relevance maximizing problem with binary feedback. This problem naturally arises in several settings in which a designer must adaptively make a sequence of suggestions to a user while he learns his preferences from his feedback. This basic framework is accessible to extensions that adapt our approach to a more practical environment in which some of our assumptions may not hold. For example, we assume that the number of display options in a session is independent of both the type of user and the relevance of the feedback that cannot hold in practice. For example, a user may leave earlier if he has shown successively irrelevant products. Also, one of our key assumptions is that user feedback is binary, but in practice one can benefit from a fine-grained feedback from the user."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Proof of lemma 3.1", "text": "Suppose the subsequent distribution over the totality of the possible types is {P (X = i | Ht)} = (p1t, \u00b7 \u00b7 \u00b7, pNt) and the totality of the remaining products is A (t). Consider the event W with a fixed realization of user type X = i and a fixed realization of the random variable C = c, which is the time the user goes to. So the string of binary feedback for the various categories is {qij: j = [H]}}}}. Then, for each set policy, the order of the allocation of the products from time to c is dictated and determined by the policy. Let's leave this order of allocations {lt + 1, \u00b7 \u00b7, lc} and the corresponding order of the feedback of Ylt: Ylt and the corresponding order of the feedback of Ylt."}, {"heading": "7.2 Proof of lemma 3.2", "text": "Suppose that on the occasion t = the optimal policy that a product l \u00b2 assigns to category j \u00b2, while there is a category j \u00b2, so there are two cases: (A) If the user finds j \u00b2 relevant, it exhausts all the products in that category and then moves toward the allotment j \u00b2. After the allotment j \u00b2 is done, it behaves as if j \u00b2 was never assigned until the allotment j to which the designer updates the information that j is relevant and moves to the allotment of the next category, and so on. (B) If the user finds j \u00b2 not relevant, then it behaves as if the allotment j \u00b2 and as if it is not relevant."}, {"heading": "7.3 Proof of Theorem 4.1", "text": "The proof: If the relevance matrix Q is such that all categories form a single, non-dominated equivalence class, then the far-sighted, non-dominated policy is the same as the optimal policy, and therefore W (Q, p, \u03b2) = V (Q, p, \u03b2). Let us now consider an experiment opportunity with a related relevance matrix Q that has K non-dominated equivalence classes (U1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, UK). Let us further assume that there is any factor 0 < p < 1 such a possibility as W (QE, p, k, \u03b2) V (QE, p)."}, {"heading": "7.4 Proof of Theorem 4.2", "text": "Proof. Consider the set (U1, \u00b7 \u03b2L \u00b7 \u00b7, UK) of non-dominated classes of ad categories at the first experiment opportunity. From the dynamic programming equation (3) it follows that V \u03c0k = P (\u03c9 (\u03c0, k)) (1 \u2212 \u03b2Lk1 \u2212 \u03b2 + \u03b2Lk V \u03c0k). Here, the number of ads in classes k and V \u03c0k is made dependent on event E as the optimal payment condition, since class k is also used up. We approach this result by defining OPK as P (\u03c0, k) (1 \u2212 \u03b2Lk1 \u2212 \u03b2). Note that in the context of greedy policy, k is chosen to maximize the size of K. The ratio of the two quantities is the optimization of V \u03c0k = 1 \u2212 \u03b2L k1 \u2212 \u03b2Lk1 \u2212 \u03b2Lk1 \u2212 \u03b2LkVK \u2212 \u03b2 + \u03b2 kVK \u2212 kK \u2212 kK \u2212 1 \u2212 kK \u2212 1."}, {"heading": "7.5 Proof of Theorem 4.3", "text": "Evidence. For a type i user, the total number of products with positive feedback is given by ri = \u2211 H j = 1 q i jLj. Therefore, the expected total number of products with positive feedback is R = N \u2211 i = 1 riPX (i). In the case of W that the number of exhibit opportunities is greater than L, each policy will receive the full payout by R. Thus, its expected payout by VG \u2265 P (W) R = P (C \u2265 L) R = \u03b2L \u2212 1R. In addition, the optimal policy cannot achieve a payout greater than R. Thus, the ratio of payout under each policy and that under the optimal policy is at least \u03b2L \u2212 1."}, {"heading": "7.6 Recursive computation of Farsighted Greedy", "text": "Algorithm 4 (farsighted greedy): Function [W (Q, p, \u03b2), A (Q, p, \u03b2)], where Q is a relevance matrix and p is a probability distribution over user types. \u2022 If Q is empty, return W (Q, p, \u03b2) = 0. \u2022 If Q is not empty, leave the non-dominated equivalence classes (U1, \u00b7 \u00b7 \u00b7, UK) and the number of products in each class with Lk = \u2211 j, Uk Lj. Let N be the number of frosts in Q according to user types. \u2022 Let the event (k) be the event {X-S (k)}, where S (k) = {i-N: qij = 1, j-Uk}. Let Qk be the matrix obtained after removing all columns in Q according to user types."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering", "author": ["Gediminas Adomavicius", "Alexander Tuzhilin"], "venue": "IEEE Transactions on,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Fab: content-based, collaborative recommendation", "author": ["Marko Balabanovi\u0107", "Yoav Shoham"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "When policies are better than plans: Decisiontheoretic planning of recommendation sequences", "author": ["Thorsten Bohnenberger", "Anthony Jameson"], "venue": "In Proceedings of the 6th international conference on Intelligent user interfaces,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["John S Breese", "David Heckerman", "Carl Kadie"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Hybrid recommender systems: Survey and experiments", "author": ["Robin Burke"], "venue": "User modeling and user-adapted interaction,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Sequential design of experiments", "author": ["Herman Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1959}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Contributions to the", "author": ["Dorian Feldman"], "venue": "two-armed bandit\u201d problem. The Annals of Mathematical Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1962}, {"title": "Multi-armed bandit allocation indices", "author": ["John Gittins", "Kevin Glazebrook", "Richard Weber"], "venue": "Wiley Online Library,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["Jonathan L Herlocker", "Joseph A Konstan", "Loren G Terveen", "John T Riedl"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Parametrized stochastic multi-armed bandits with binary rewards", "author": ["Chong Jiang", "R Srikant"], "venue": "In American Control Conference (ACC),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Further contributions to the", "author": ["Robert Keener"], "venue": "two-armed bandit\u201d problem. The Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1985}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["Tze Leung Lai"], "venue": "The Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1985}, {"title": "Content-based recommender systems: State of the art and trends", "author": ["Pasquale Lops", "Marco De Gemmis", "Giovanni Semeraro"], "venue": "In Recommender systems handbook,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A structured multiarmed bandit problem and the greedy policy", "author": ["Adam J Mersereau", "Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Active sequential hypothesis testing", "author": ["Mohammad Naghshvar", "Tara Javidi"], "venue": "The Annals of Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Multi-armed bandit problems with dependent arms", "author": ["Sandeep Pandey", "Deepayan Chakrabarti", "Deepak Agarwal"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Content-based recommendation systems. In The adaptive web, pages 325\u2013341", "author": ["Michael J Pazzani", "Daniel Billsus"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Markov decision processes: discrete stochastic dynamic programming, volume 414", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Paul Resnick", "Neophytos Iacovou", "Mitesh Suchak", "Peter Bergstrom", "John Riedl"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N. Tsitsiklis"], "venue": "Math. Oper. Res.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Recommender systems in e-commerce", "author": ["J Ben Schafer", "Joseph Konstan", "John Riedl"], "venue": "In Proceedings of the 1st ACM conference on Electronic commerce,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "An mdp-based recommender system", "author": ["Guy Shani", "Ronen I Brafman", "David Heckerman"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1933}, {"title": "Multi-armed bandits and the gittins index", "author": ["Peter Whittle"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1980}], "referenceMentions": [{"referenceID": 25, "context": "Predicting the preferences of users in order to present them with more relevant engagements is a fundamental component of any recommendation system [27, 25].", "startOffset": 148, "endOffset": 156}, {"referenceID": 0, "context": "Over the years, a wide variety of approaches have been proposed for this problem (see [1] for a survey).", "startOffset": 86, "endOffset": 89}, {"referenceID": 21, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 110, "endOffset": 118}, {"referenceID": 17, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 110, "endOffset": 118}, {"referenceID": 23, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 155, "endOffset": 163}, {"referenceID": 12, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 155, "endOffset": 163}, {"referenceID": 3, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 268, "endOffset": 274}, {"referenceID": 7, "context": "These include content based approaches that rely on generating user and item profiles based on available data [22, 18], collaborative filtering approaches [24, 13] that recommend items based on similarity measures between users and/or items, and a combination of both [4, 8].", "startOffset": 268, "endOffset": 274}, {"referenceID": 26, "context": "In this paper, motivated by several settings of interest in which explicit feedback about the relevance of the recommendations can be received from the user on small timescales, we pursue a less studied approach (see [28]) of modeling the recommendation process as a sequential optimization problem.", "startOffset": 217, "endOffset": 221}, {"referenceID": 5, "context": "We consider a model that is derived from cluster models for collaborative filtering (see [6]) in which the history of user behaviors is compressed into a predictive model, where users are classified into \u2018types\u2019 that capture the preference profile of the user.", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "To the best of our knowledge, its earliest appearance in literature can be traced back to [5], which proposed a decision-theoretic modeling of the problem of generating recommendations (on a palm-top) for a user navigating through an airport.", "startOffset": 90, "endOffset": 93}, {"referenceID": 26, "context": "[28] proposed a framework for modeling the sequential optimization problem in online recommendation systems as a Markov Decision Process (MDP) [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[28] proposed a framework for modeling the sequential optimization problem in online recommendation systems as a Markov Decision Process (MDP) [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "In a multi-armed bandit problem (MAB), first introduced by Thompson in [29], a decision-maker faces a set of arms whose reward characteristics are uncertain and seeks to optimize the sequence in which they are pulled so as to maximize some long-run reward.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "Our model falls in the Bayesian setting [12, 30], in which an initial prior distribution is assumed over the parameters of a probabilistic reward generating model for each arm, and one performs Bayesian updates of these estimates as rewards are observed.", "startOffset": 40, "endOffset": 48}, {"referenceID": 28, "context": "Our model falls in the Bayesian setting [12, 30], in which an initial prior distribution is assumed over the parameters of a probabilistic reward generating model for each arm, and one performs Bayesian updates of these estimates as rewards are observed.", "startOffset": 40, "endOffset": 48}, {"referenceID": 16, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 23, "endOffset": 31}, {"referenceID": 15, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 23, "endOffset": 31}, {"referenceID": 6, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 293, "endOffset": 299}, {"referenceID": 1, "context": "The stochastic setting [17, 16] (also see [7] for a recent survey) does not assume any prior distribution over the parameters and one instead tries to find policies that minimize the worst case rate at which losses relative to the expected reward of best arm (called \u2018regret\u2019) are accumulated [3, 2].", "startOffset": 293, "endOffset": 299}, {"referenceID": 11, "context": "In the Bayesian case, a seminal result by Gittins [12] shows that the optimal policy dynamically computes an index for each arm independently of all other arms, and picks the arm with the highest index at each step.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "[11] and [15] analyze two-armed bandit problems in which reward characteristics of two arms are known, but which arm corresponds to which reward distribution is not known, which leads to a natural dependence between the arms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[11] and [15] analyze two-armed bandit problems in which reward characteristics of two arms are known, but which arm corresponds to which reward distribution is not known, which leads to a natural dependence between the arms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "[21] studies another version of the problem in which the arms can be grouped into clusters of dependent arms, in which case the Gittins decomposition result can be partially extended.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Recently, [19] considered a specific model of a MAB problem with dependent arms, where they analyzed the performance of a greedy policy and derived asymptotic optimality results.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 85, "endOffset": 96}, {"referenceID": 9, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 85, "endOffset": 96}, {"referenceID": 24, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 85, "endOffset": 96}, {"referenceID": 13, "context": "These type of problems have recently also gained attention in the stochastic setting [2, 10, 26] (also see [14] for the case of binary rewards), although the formulations and techniques in that setting are very different.", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "In the special case when each category has a single product, our problem is also related to the active sequential hypothesis testing problem [9, 20].", "startOffset": 141, "endOffset": 148}, {"referenceID": 19, "context": "In the special case when each category has a single product, our problem is also related to the active sequential hypothesis testing problem [9, 20].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "As mentioned earlier, this problem is a type of a Bayesian multi-armed bandit problem with correlated rewards (see [12, 30]) with an additional constraint on the number of times each arm may be pulled.", "startOffset": 115, "endOffset": 123}, {"referenceID": 28, "context": "As mentioned earlier, this problem is a type of a Bayesian multi-armed bandit problem with correlated rewards (see [12, 30]) with an additional constraint on the number of times each arm may be pulled.", "startOffset": 115, "endOffset": 123}], "year": 2015, "abstractText": "Motivated by online settings where users can provide explicit feedback about the relevance of products that are sequentially presented to them, we look at the recommendation process as a problem of dynamically optimizing this relevance feedback. Such an algorithm optimizes the fine tradeoff between presenting the products that are most likely to be relevant, and learning the preferences of the user so that more relevant recommendations can be made in the future. We assume a standard predictive model inspired by collaborative filtering, in which a user is sampled from a distribution over a set of possible types. For every product category, each type has an associated relevance feedback that is assumed to be binary: the category is either relevant or irrelevant. Assuming that the user stays for each additional recommendation opportunity with probability \u03b2 independent of the past, the problem is to find a policy that maximizes the expected number of recommendations that are deemed relevant in a session. We analyze this problem and prove key structural properties of the optimal policy. Based on these properties, we first present an algorithm that strikes a balance between recursion and dynamic programming to compute this policy. We further propose and analyze two heuristic policies: a \u2018farsighted\u2019 greedy policy that attains at least 1 \u2212 \u03b2 factor of the optimal payoff, and a naive greedy policy that attains at least 1\u2212\u03b2 1+\u03b2 factor of the optimal payoff in the worst case. Extensive simulations show that these heuristics are very close to optimal in practice.", "creator": "LaTeX with hyperref package"}}}