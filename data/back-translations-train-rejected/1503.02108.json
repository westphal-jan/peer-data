{"id": "1503.02108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2015", "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "abstract": "We present a Bayesian approach to adapting parameters of a well-trained context-dependent deep-neural-network hid-den Markov models (CD-DNN-HMMs) to improve automatic speech recognition performance. Due to an abundance of DNN parameters but with only a limited amount of adaptation data, the posterior probabilities of unseen CD states (senones) are often pushed towards zero during adaptation, and consequently the ability to model these senones can be degraded. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output senones and compare it to the feature space maximum a posteriori linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.", "histories": [["v1", "Fri, 6 Mar 2015 22:48:29 GMT  (209kb,D)", "https://arxiv.org/abs/1503.02108v1", null], ["v2", "Wed, 12 Aug 2015 04:53:53 GMT  (162kb,D)", "http://arxiv.org/abs/1503.02108v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["zhen huang", "sabato marco siniscalchi", "i-fan chen", "jiadong wu", "chin-hui lee"], "accepted": false, "id": "1503.02108"}, "pdf": {"name": "1503.02108.pdf", "metadata": {"source": "CRF", "title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "authors": ["Zhen Huang", "Sabato Marco Siniscalchi", "I-Fan Chen", "Jinyu Li", "Jiadong Wu", "Chin-Hui Lee"], "emails": ["huangzhenee@gatech.edu,", "marco.siniscalchi@unikore.it,", "ichen8@gatech.edu,", "jinyli@exchange.microsoft.com,", "jwu65@gatech.edu,", "chl@ece.gatech.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, we will be able to go in search of a solution that meets the needs of the people, \"he said in an interview with the German Press Agency."}, {"heading": "2. Training of Deep Models", "text": "The values of the nodes can be expressed as follows: xi = {W1o t + b1, i = 1Wiyi \u2212 1 + bi, i > 1, (1) yi = {sigmoid (xi), i < Lsoftmax (xi), i = L, (2) where W1 and Wi are the weight matrices, b1, and bi are the bias vectors, ot is the input frame in due time t, L is the total number of hidden layers, and both sigmoid and softmax functions are elementary operations. The vector xi corresponds to pre-nonlinear activations, and yi and yL are the vectors of the neuron outputs at the ith hidden layer and output layer."}, {"heading": "3. Transformation Based Adaptation for Deep Models", "text": "For DNN adaptation, some researchers choose an affinity transformation network between the last hidden layer and the starting layer weight matrix, i.e., an LHN, and adjust only the LHN parameters, while all other DNN parameters are fixed. [11] To reduce the amount of parameters to be adapted, the last hidden layer is designed as a bottleneck (fewer neurons). [23, 24, 25, 26] Superior results have been obtained through this type of LHN formulation than other transformation-based adaptation schemes, such as the linear input network (LIN) and the linear output network (LON). The LIN approach performs the adaptation by adding an augmented linear input layer and adapting only this set of LHN parameters. If we follow the common idea that the hidden layers of a DNN are actually a more suitable data representative and a \"better function\" for the starting layer than the one we directly face \"than the\" for the starting layer."}, {"heading": "4. MAP Adaptation for Deep Models", "text": "Although traditional DNN adaptation approaches attempt to alleviate overpass problems by reducing the number of parameters to be adapted, this number may still be very high in some cases. Inspired by the MAP adaptation, which effectively solves the problem in GMM-HMM systems, we explain in this section how to apply the MAP approach to LHN adaptation. Note that we select LHN for demonstration, but the proposed MAP approach is also easily applicable to other DNN adaptation frames such as [8, 14, 16, 17]."}, {"heading": "4.1. Prior Estimation", "text": "In order to establish a MAP adjustment frame as in [30], a prior distribution over the weights of the affine transformation network must be imposed. In order to analyze and estimate the priority, we used the training data of the baseline DNN. We chose an empirical Bayes approach [30, 29] and treated each speaker in the training set as a model speaker and monitored LHN adaptation. Afterwards, we can obtain a specific LHN for each speaker. We observed that the histograms for the weights of the adapted LHN via the speakers are quite like Gaussian, so we assume that the distribution of the weights in Wlhn takes place as a common Gaussian [31] \u00b7 l \u00b7 l \u00b7 l \u00b7 l \u00b7 l \u00b7 l \u00b7 l \u00b7 l \u00b7 \u00b7 l \u00b7 \u00b7 \u00b7 l \u00b7 \u00b7 \u00b7 \u00b7 l \u00b7 \u00b7 \u00b7 l \u00b7 \u00b7 \u00b7 l \u00b7 \u00b7 \u00b7 l \u00b7 31]. By expressing the weights in the LHN transformation matrix Wlhn as a vector w with each input representing a certain weight, we have the previous density in the following form: p (Wlhn) = 1 (2 w = 1 l / 1 | m = 1 l."}, {"heading": "4.2. MAP Formulation", "text": "Equation (10) formulates the MAP learning idea by adding the concept of the previous density p (Wlhn) to the simple cross entropy objective function. L1: TMAP = \u2212 \u03bb log p (Wlhn) + L1: Txent (10) Using the previous form of Equation (7), the objective function of the MAP-LHN fit takes the form of Equation (11).L1: TMAP = \u03bb 2 (w \u2212 \u00b5) T\u03a3 \u2212 1 (w \u2212 \u00b5) + L1: Txent (11), where only the diagonal entries of the covariance matrix \u03a3 are not zero (from the assumption of independence of weights).A close look at Equation (11), if the previous density is a standard Gaussian N (0, I) \u2212 MAP learning degenerates into conventional L2-regulated training."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experimental Setup", "text": "The basic CD-DNN-HMM system was trained with the open vocabulary of the Wall Street Journal [35] using the Kaldi toolkit [36].The basic CD-DNN-HMM system was trained with the WSJ0 material (SI-84).The standard WSJ0 adaptation kit (si et ad, 8 speakers, 40 sets per speaker) was used to customize the affine transformation added to the speaker-independent DNN. The standard open vocabulary 20,000 words (20K) reads the NVP Senneheiser microphone (si et ad, 8 speakers x 40 sets) for evaluation.A standard trigram language model was adopted during decodecoded.The ASR performance was given in terms of word error rate (WHO).The DNN layer has six hidden layers in the configuration.The first five hidden layers have 48 units hidden during the last 216 layers."}, {"heading": "5.2. Experimental Results", "text": "The word error rate (WHO), which has been achieved with various adaptation techniques, is reproduced in Table 1. All available adaptation materials have been used to carry out the adaptation, namely 40 sets per loudspeaker. The term BASELINE refers to the independent CD-DNN-HMM system. LIN, LIN-KLD and MAP LIN refer to the adaptation techniques based on the standard linear input network approach, the KLD regularization technology1 in combination with LIN, and the maximum posterior transformation defined by the LIN parameters [31]. The terms LON and LON-KLD are used to denote a low misuse of terminology, supporting the direct adaptation of the output layer with or without KLD."}, {"heading": "5.3. Hierarchical Priors: Preliminary Experiments", "text": "Hierarchical structures, such as trees, have long been used in the linguistic community to address the excessive adaptation of problems during the estimation of model parameters. For example, efficient adaptation with a limited amount of adaptation data has been achieved by using a tree data structure to cluster model parameters of a CD-GMM-HMM system in [37]. Similar ideas have recently been explored in DNN learning processes to improve classification performance for classes with few examples in [38], where hierarchical priors for the output layer weights matrix (top layer weights in a DNN) are either fixed or learnable during training using a tree data structure. The top DNN weights in a hybrid acoustic model can be regarded as senonic embedding [39], and hierarchical priors can be defined by organising this embedding in a tree data structure. Let W (D + 1) denote the matrix (including L)."}, {"heading": "6. Conclusion", "text": "The main idea is to treat the parameters of extended affinity transformation as random Gaussian variables and incorporate prior information from the training data.The results of the speaker adaptation show that the proposed MAP approaches can result in consistent performance improvement over conventional LHN adaptation. Furthermore, MAP LHN outperforms other regulatory schemes. A first attempt to use hierarchically based priors with a fixed two-tiered tree structure has also been investigated, and small improvements have been observed in a series of preliminary ASR experiments with a limited number of adaptation sets. Better results could still be hampered by the fixed tree hierarchy structure used in this preliminary work. In fact, learning the tree hierarchy during training has been shown to improve classification performance [38]. Finally, from an objective functional perspective, we are still relying on transverse entropy. Other forms of the framework hierarchy and sequencing objectives can also be applied [40]."}, {"heading": "7. References", "text": "[1] T. N. Sainath, B. Ramabhadran, P. Fousek, P. No-vak, and A. Mohamed, \"Making deep belief networks effective for large vocabulary continuous speech recognition,\" in Proc. H. H. Sing. [2] G. E. Dahl, D. Yu, L. Deng, and A. Acero, \"Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\" IEEE Trans. Audio, Speech, and Language Processing, vol. 20, pp. 30-42, 2012. [3] K. Vesely, A. Ghoshal, L. Burget, and D. Povey \"Sequence discriminative training of deep neural networks,\" in Proc. INTERSPEECH, 2013, ppEc.Klu45-2349."}], "references": [{"title": "Making deep belief networks effective for large vocabulary continuous speech recognition", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran", "P. Fousek", "P. Novak", "A. Mohamed"], "venue": "Proc. ASRU, 2011, pp. 30\u201335.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequencediscriminative training of deep neural networks", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proc. INTER- SPEECH, 2013, pp. 2345\u20132349.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Connectionist speech recognition: A hybrid approach", "author": ["H. Bourlard", "N. Morgan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "Proc. Eurospeech, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Regularized adaptation of discriminative classifiers", "author": ["X. Li", "J. Bilmes"], "venue": "Proc. ICASSP, vol. 1, 2006, pp. I\u2013I.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", "author": ["D. Yu", "K. Yao", "H. Su", "G. Li", "F. Seide"], "venue": "Proc. ICASSP, 2013, pp. 7893\u20137897.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Factorized deep neural networks for adaptive speech recognition", "author": ["D. Yu", "X. Chen", "L. Deng"], "venue": "Proc. Int. Workshop on Statistical Machine Learning for Speech Processing, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "The deep tensor neural network with applications to large vocabulary speech recognition", "author": ["D. Yu", "L. Deng", "S. Seide"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 2, pp. 388\u2013396, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear hidden transformations for adaptation of hybrid ANN/HMM models", "author": ["R. Gemello", "F. Mana", "S. Scanzio", "P. Laface", "R.D. Mori"], "venue": "Speech Communication, vol. 49, no. 10, pp. 827\u2013835, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "Proc. ASRU, 2011, pp. 24\u201329.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "Proc. Spoken Language Technology Workshop, 2012, pp. 366\u2013369.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proc. ASRU, 2013, pp. 55\u201359.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Hermitian polynomial for speaker adaptation of connectionist speech recognition systems", "author": ["S.M. Siniscalchi", "J. Li", "C.-H. Lee"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2152\u20132161, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Rapid and effective speaker adaptation of convolutional neural network based models for speech recognition", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. INTERSPEECH, 2013, pp. 1248\u20131252.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "Proc. IEEE STL, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Factorized adaptation for deep neural network", "author": ["J. Li", "J.-T. Huang", "Y. Gong"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. ICASSP, 2013, pp. 7942\u20137946.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Direct adaptation of hybrid DNN/HMM model for fast speaker adaptation in LVCSR based on speaker code", "author": ["S. Xue", "O. Abdel-Hamid", "H. Jiang", "L. Dai"], "venue": "Proc. ICASSP, 2014, pp. 6339\u20136343.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast adaptation of deep neural network based on discriminant codes for speech recognition", "author": ["S. Xue", "O. Abdel-Hamid", "H. Jiang", "L. Dai", "Q. Liu"], "venue": "IEEE/ACM Trans. on Audio, Speech and Lang. Proc., vol. 22, no. 12, pp. 1713\u20131725, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. INTERSPEECH, 2010, pp. 526\u2013 529.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition.", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "in INTER- SPEECH,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Singular value decomposition based low-footprint speaker adaptation and personalization for deep neural network", "author": ["J. Xue", "J. Li", "D. Yu", "M. Seltzer", "Y. Gong"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of hybrid NN/HMM model for speech recognition based on singular value decomposition", "author": ["S. Xue", "H. Jiang", "L. Dai"], "venue": "Proc. ISCSLP, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Shared-distribution hidden markov models for speech recognition", "author": ["M.-Y.M.-Y. Hwang", "X. Huang"], "venue": "IEEE trans. Speech and Audio Processing, vol. 1, no. 4, pp. 414\u2013420, 1993.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1993}, {"title": "Catastrophic forgetting in connectionist networks: causes, consequences and solutions", "author": ["M. Franch"], "venue": "Trends in Cognitive Sciences, vol. 3, no. 4, 1994.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "On adaptive decision rules and decision parameter adaptation for automatic speech recognition", "author": ["C.-H. Lee", "Q. Huo"], "venue": "Proc. IEEE, vol. 88, no. 8, 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of Markov chains", "author": ["J. Gauvain", "C.-H. Lee"], "venue": "IEEE Trans. Speech and audio processing, vol. 2, no. 2, pp. 291\u2013 298, 1994.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1994}, {"title": "Feature space maximum a posteriori linear regression for adaptation of deep neural networks", "author": ["Z. Huang", "J. Li", "S.M. Siniscalchi", "I.-F. Chen", "C. Weng", "C.-H. Lee"], "venue": "Proc. INTERSPEECH, 2014, pp. 2992\u20132996.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal distributed online prediction", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Proc. ICML, 2011, pp. 713\u2013720.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models", "author": ["C.J. Leggetter", "P.C. Woodland"], "venue": "Computer Speech & Language, vol. 9, no. 2, pp. 171\u2013185, 1995.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "The design for the wall street journalbased CSR corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proc. Workshop on Speech and Natural Language, 1992, pp. 899\u2013902.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1992}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsk\u1ef3", "G. Stemmer", "K. Vesel\u1ef3"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A structural Bayes approach to speaker adaptation", "author": ["K. Shinoda", "C.-H. Lee"], "venue": "IEEE trans. Speech and Audio Processing, vol. 9, no. 3, pp. 276\u2013287, 2001.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "Discriminative transfer learning with tree-based priors", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Proc. NIST, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Decision tree based state tying for speech recognition using DNN derived embeddings", "author": ["X. Li", "X. Wu"], "venue": "Proc. ISCSLP, 2014, pp. 123\u2013127.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond cross-entropy: Towards better frame-level objective functions for deep neural network training in automatic speech recognition", "author": ["Z. Huang", "J. Li", "C. Weng", "C.-H. Lee"], "venue": "Proc. IN- TERSPEECH, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 200, "endOffset": 209}, {"referenceID": 1, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 200, "endOffset": 209}, {"referenceID": 2, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 200, "endOffset": 209}, {"referenceID": 3, "context": "Despite the recent outstanding results demonstrated by contextdependent, deep-neural-network based hidden Markov models (CD-DNN-HMMs) in various automatic speech recognition (ASR) tasks and data sets [1, 2, 3], these acoustic models, similarly to conventional context-dependent, Gaussian-mixturemodel based HMMs (CD-GMM-HMMs) [4], still suffer from a performance degradation under potential mismatched conditions between training and testing conditions.", "startOffset": 326, "endOffset": 329}, {"referenceID": 4, "context": "For standard hybrid system using artificial neural networks (ANNs) and HMMs [5] in which CD-DNN-HMM is a special case, there exist many adaptation techniques.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Unfortunately, it leads to over-fitting on the adaptation material when the amount of adaptation patterns is limited [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 71, "endOffset": 78}, {"referenceID": 5, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 10, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 11, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 12, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 101, "endOffset": 116}, {"referenceID": 13, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 133, "endOffset": 137}, {"referenceID": 14, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 167, "endOffset": 179}, {"referenceID": 15, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 167, "endOffset": 179}, {"referenceID": 16, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 167, "endOffset": 179}, {"referenceID": 17, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 201, "endOffset": 205}, {"referenceID": 18, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 15, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 19, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 20, "context": "Recent approaches, such as regularization based [7, 8], subspace based [9, 10], transformation based [6, 11, 12, 13], i-Vector based [14], native neural network based [15, 16, 17], factorization based [18] and fast adaptation schemes based on discriminant speaker codes [19, 16, 20, 21], have been proposed to circumvent the problem.", "startOffset": 270, "endOffset": 286}, {"referenceID": 5, "context": "The key idea is to augment the structure of the ANN component by adding an affine transformation network to the input [6], hidden [11], or output layer [22].", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "The key idea is to augment the structure of the ANN component by adding an affine transformation network to the input [6], hidden [11], or output layer [22].", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "The key idea is to augment the structure of the ANN component by adding an affine transformation network to the input [6], hidden [11], or output layer [22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 23, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 24, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 25, "context": "For linear hidden network (LHN) layer approaches, the last hidden layer is usually designed to be a bottleneck to ensure an affordable parameter size [23, 24, 25, 26].", "startOffset": 150, "endOffset": 166}, {"referenceID": 26, "context": "However, adapting parameters in a CD-DNN-HMM is much more challenging than earlier connectionist adaptation schemes because of its huge parameter set size with a large number of network branches connected to a large set of tied HMM states, often referred to as senones [27].", "startOffset": 269, "endOffset": 273}, {"referenceID": 27, "context": "Such a phenomenon is commonly referred to as catastrophic forgetting [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "For example, a Kullback-Leibler divergence (KLD) based objective criterion to be used during adaptation was devised in [8] in order to alleviate the catastrophic forgetting problem.", "startOffset": 119, "endOffset": 122}, {"referenceID": 10, "context": "A variation to the standard method of assigning the target values was instead discussed in [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Nonetheless, Bayesian solutions adopted in the CD-GMM-HMMs to address the same issue [29] have not been fully exploited.", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "In this study, we attempt to cast DNN adaptation within a Bayesian framework in the spirit of maximum a posteriori (MAP) adaptation [30].", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "It also compares favorably against the feature space maximum a posteriori linear regression approach to speaker adaptation proposed in [31].", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "Mini-batch stochastic gradient descent (SGD) [32], with a reasonable size of mini-batches to make all matrices fit into the GPU memory, was used to update all neural parameters during training.", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": "Pre-training methods was used for the initialisation of the DNN parameters [33].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": ", an LHN, and adapt only the LHN parameters while keeping fixed all of the other DNN parameters [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 22, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 23, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 24, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 25, "context": "In order to reduce the amount of parameters to adapt, usually the last hidden layer is designed to be a bottleneck (less neurons) [23, 24, 25, 26].", "startOffset": 130, "endOffset": 146}, {"referenceID": 33, "context": "This formulation is quite similar to maximum likelihood linear regression (MLLR) [34].", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 13, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 15, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 16, "context": "Note that though we choose LHN for demonstration, the proposed MAP approach can be easily applied to other DNN adaptation frameworks like [8, 14, 16, 17] as well.", "startOffset": 138, "endOffset": 153}, {"referenceID": 29, "context": "In order to establish a MAP adaptation framework like in [30], a prior distribution over the weights of the affine transformation network need to be imposed.", "startOffset": 57, "endOffset": 61}, {"referenceID": 29, "context": "We adopted an empirical Bayes approach [30, 29] and treated each speaker in the training set as a sample speaker and supervised LHN adaptation was performed.", "startOffset": 39, "endOffset": 47}, {"referenceID": 28, "context": "We adopted an empirical Bayes approach [30, 29] and treated each speaker in the training set as a sample speaker and supervised LHN adaptation was performed.", "startOffset": 39, "endOffset": 47}, {"referenceID": 30, "context": "We observed that the histograms for weights of the adapted LHN over speakers are quite like Gaussian, so we assume that the distribution of the weights in Wlhn to be joint Gaussian [31].", "startOffset": 181, "endOffset": 185}, {"referenceID": 30, "context": "Formal MAP adaptation is conducted following [31].", "startOffset": 45, "endOffset": 49}, {"referenceID": 34, "context": "This study is concerned with the problem of speaker adaptation, and experiments are reported on the 20k-word open vocabulary Wall Street Journal task [35] using the Kaldi toolkit [36].", "startOffset": 150, "endOffset": 154}, {"referenceID": 35, "context": "This study is concerned with the problem of speaker adaptation, and experiments are reported on the 20k-word open vocabulary Wall Street Journal task [35] using the Kaldi toolkit [36].", "startOffset": 179, "endOffset": 183}, {"referenceID": 22, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 23, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 24, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 25, "context": "The bottleneck based low rank methods have been widely used to achieve more compact DNN models with equivalent performance [23, 24, 25, 26].", "startOffset": 123, "endOffset": 139}, {"referenceID": 30, "context": "LIN, LIN-KLD, and MAP LIN refer to the adaptation technique based on the standard linear input network approach, the KLD regularisation technique in combination with LIN, and the maximum a posteriori transformation based adaptation when a prior is defined over the LIN parameters [31], respectively.", "startOffset": 280, "endOffset": 284}, {"referenceID": 36, "context": "For instance, efficient adaptation with a limited amount of adaptation data was obtained through the use of a tree data structure to cluster model parameters of a CD-GMM-HMM system in [37].", "startOffset": 184, "endOffset": 188}, {"referenceID": 37, "context": "Similar ideas have been recently explored in DNN learning for enhancing classification performance for classes with few examples in [38], where hierarchical priors where devised for the output layer weights matrix (top-level weights in a DNN) using a tree data structure either fixed or learnable during training.", "startOffset": 132, "endOffset": 136}, {"referenceID": 38, "context": "Top-level DNN weights in a hybrid acoustic model can be regarded as senone embeddings [39], and hierarchical priors can be defined by organising those embedding in a tree data structure.", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "13 over \u03b8s with fixed DNN weights (see [38]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": "Indeed, it was demonstrated that learning the tree hierarchy during training improves the classification performance [38].", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "Other forms of frame-level and sequence-level discriminative objectives [3, 40] can also be applied.", "startOffset": 72, "endOffset": 79}, {"referenceID": 39, "context": "Other forms of frame-level and sequence-level discriminative objectives [3, 40] can also be applied.", "startOffset": 72, "endOffset": 79}], "year": 2015, "abstractText": "We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.", "creator": "LaTeX with hyperref package"}}}