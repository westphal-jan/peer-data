{"id": "1503.02143", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2015", "title": "Model selection of polynomial kernel regression", "abstract": "Polynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the \" ill-condition\" of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability.", "histories": [["v1", "Sat, 7 Mar 2015 08:39:15 GMT  (123kb)", "http://arxiv.org/abs/1503.02143v1", "29 pages, 4 figures"]], "COMMENTS": "29 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shaobo lin", "xingping sun", "zongben xu", "jinshan zeng"], "accepted": false, "id": "1503.02143"}, "pdf": {"name": "1503.02143.pdf", "metadata": {"source": "CRF", "title": "Model selection of polynomial kernel regression", "authors": ["Shaobo Lin", "Xingping Sun", "Zongben Xu", "Jinshan Zeng"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "II. A FAST REVIEW OF STATISTICAL LEARNING THEORY AND KERNEL METHODS", "text": "Suppose that the unknown probability scale on Z (= X \u00b7 Y) is a finite sample of the size m, m \u00b2 n, drawn and identical. Suppose that f (x) is a function used to model the correspondence between X and Y as it is generated by both sides. A natural measurement of the error caused by the use of f (f) of the generalization is the generalization error defined by f (f). (f) \u2212 Y is a function minimized by the regression function [7], defined by (x). We do not know whether this ideal minimizer function exists, but access to random examples from X (x)."}, {"heading": "III. MODEL SELECTION IN POLYNOMIAL KERNEL REGRESSION", "text": "Let us know that the RKHS of Ks, Ks (\u00b7, y) > s = Ks (x, y), and Pds are the set of algebraic polynomials of degree at most s.We study the parameter selection for the following modelfz, s: = arg min f, Hs1mm, i = 1 (f, xi)."}, {"heading": "IV. AN EFFICIENT MODEL SELECTION FOR POLYNOMIAL KERNEL REGRESSION", "text": "In this section we propose a viable model selection method for polynomial regression based on the theoretical analysis proposed in section 3, and design a learning algorithm with low computational complexity. It is analyzed that the regularizing term in the modelarg min f = 1 (f (xi) -yi) 2 + 2 (s) 2 + 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) -yi) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s) 2 (s (s) 2 (s) 2 (s) 2 (s (s) 2 (s) 2 (s) 2 (s (s) 2 (s) 2 (s) 2 (s) 2 (s (s) 2 (s) 2 (s) 2 (s (s) 2 (s) 2 (s) 2 (s (s) 2 (s) 2 (s (s) 2 (s) 2 (s (s) 2 (s) 2 (s) 2 (s (s) 2 (s (s) s (s) s (s) s (s (s) s (s) s (s) s (s (s) s (s) s (s) s (s (s) s (s) s (s (s) s (s) s (s) s (s) s (s (s) s (s (s) s (s (s) s (s) s (s) s (s (s) s (s) s (s (s) s (s) s (s) s (s (s) s (s) s (s (s) s (s (s) s (s (s (s) s (s)."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In this section we give both toy and UCI standard data simulations of the model intersection strategy for polynomial kernel regression and the EPKR algorithm. All numerical simulations are performed in Matlab R2011b environment with Windows 7, Intel (R) Core (TM) i7-3770K CPU @ 3.50 GHz 3.50 GHz."}, {"heading": "A. Toy simulation", "text": "1) Experimental setting: In this part we put the simulation setting of the toy xperiment = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 + 1 + 1 = 1 + 2 = 1 + 2 = 1 + 1 = 1 + 1 = 2 = 1 + 1 = 2 = 2 = 1 + 1 = 2 = 2 = 2 = 1 + 1 + 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 1 + 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 + 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 ="}, {"heading": "B. UCI data", "text": "In this part we present the simulation setting of the UCI data ex-periment. All data are compared by http: / / www.niaad.liacc.up.pt / \u0445 ltorgo / Regression / ds menu.html. In the UCI data experiment we compare four methods that include support for the vector machine (SVM). [35], Gaussian Kernel Regression (GKR) [15, Eqs. (4)], classic polynomial kernel regression (3) (PKR) and EPKR on 9 real benchmark data sets that vary."}, {"heading": "VI. PROOFS", "text": "The second, which focuses on the coverage of numbers, will be from [14, Chapter 9].Lemma 4: Let F be a class of functions, all delimited by M. (For all m and \u03b1, \u03b2 > 0, we haveP.m, F, L1 (2))) exp (\u2212 n 2568M4), where x = (x1,., xm), Xm and N (t, F, L1 (xx) are the coverage of class F by radius t in L1 (xx), where x = (x1,.,., xm), Xm and N (t, F, L1 (xx) are the coverage of class F by L1 (xx), with Lempirical."}, {"heading": "VII. CONCLUSION", "text": "The main contributions of this paper can be summarized as follows: First, we examine the problem of parameter selection in polynomic nuclear regression. After our analysis, we conclude that the essential role of the regularization concept is to overcome the problem of the disease condition of the nuclear matrix. In fact, arbitrarily small regularization parameters can lead to an almost optimal learning rate in terms of model selection. Second, we improve the existing results on polynomic nuclear regression in the following directions: building a non-distributional theoretical analysis, broadening the functional range of the regression function and determining the near-optimal learning rate. Third, we propose a new model for polynomic nuclear regression based on the above-mentioned theoretical analysis and design an efficient learning algorithm. Both theoretical and experimental results show that the new method is of high quality."}, {"heading": "ACKNOWLEDGEMENT", "text": "The research was supported by the National 973 Programming (2013CB329404) and the National Natural Science Foundation of China (grant number 11401462)."}], "references": [{"title": "A survey of the state of the art in learning the kernels", "author": ["M. Abbasnejad", "D. Ramachandram", "R. Mandava"], "venue": "Knowl. Inf. Syst., 31:193-221", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Trans. Amer. Soc., 68: 337-404", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1950}, {"title": "Approximation and learning by greedy algorithms", "author": ["A.R. Barron", "A. Cohen", "W. Dahmen", "R.A. Devore"], "venue": "Ann. Statist., 36: 64-94", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Random sampling of multivariate trigonometric polynomials", "author": ["R. Bass", "K. Gr\u00f6chenig"], "venue": "SIAM. J. Math. Anal., 36: 773- 795", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast rates for regularized least-squares algorithm", "author": ["A. Caponnetto", "E. De Vito"], "venue": "No. AI-MEMO-2005-013. Massachusetts inst of tech Cambridge computer science and artificial intelligence lab", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Optimal rates for the regularized least squares algorithm", "author": ["A. Caponnetto", "E. DeVito"], "venue": "Found. Comput. Math., 7: 331-368", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bull. Amer. Math. Soc., 39: 1-49", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Best choices for regularization parameters in learning theory: on the bias-variance problem", "author": ["F. Cucker", "S. Smale"], "venue": "Found. Comput. Math., 2: 413-428", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning Theory: An Approximation Theory Viewpoint", "author": ["F. Cucker", "D.X. Zhou"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Ten Lectures on Wavelets", "author": ["I. Daubechies"], "venue": "SIAM, Philadelphia", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "Model selection for regularized least-squares algorithm in learning theory", "author": ["E. De Vito", "A. Caponnetto", "L. Rosasco"], "venue": "Found. Comput. Math., 5: 59-85", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Constructive Approximation", "author": ["R.A. DeVore", "G.G. Lorentz"], "venue": "Springer-Verlag, Berlin", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Approximation methods for supervised learning", "author": ["R.A. Devore", "G. Kerkyacharian", "D. Picard", "V. Temlyakov"], "venue": "Found. Comput. Math., 6: 3-58", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "A Distribution-Free Theory of Nonparametric Regression", "author": ["L. Gy\u00f6rfy", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer, Berlin", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal learning rates for least squares SVMs using Gaussian kernels", "author": ["M. Eberts", "I. Steinwart"], "venue": "Advances in Neural Information Processing Systems 24 : 1539-1547", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple Kernel Learning Algorithms", "author": ["M. G\u00f6nen", "E. Alpaydm"], "venue": "J. Mach. Learn. Res., 12: 2211-2268", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer, New York", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning the Kernel Function via Regularization", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "J. Mach. Learn. Res., 6: 1099-1125", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal Kernels", "author": ["C.A. Micchelli", "Y. Xu", "H. Zhang"], "venue": "J. Mach. Learn. Res., 7: 2651-2667", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Reproducing kernel Hilbert spaces in learning theory", "author": ["H.Q. Minh"], "venue": "Ph. D. Thesis in Mathematics, Brown University", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Spherical Marcinkiewicz-Zygmund inequalities and positive quadrature", "author": ["H.N. Mhaskar", "F.J. Narcowich", "J.D. Ward"], "venue": "Math. Comput., 70: 1113-1130", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Localized tight frames on spheres", "author": ["F.J. Narcowich", "P. Petrushev", "J.D. Ward"], "venue": "Siam J. Math. Anal., 38: 574-594", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning using hyperkernels", "author": ["C.S. Ong", "A.J. Smola"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "A", "author": ["C.S. Ong"], "venue": "J. Smola and R. C.Williamson. Learning the kernel with hyperkernels. J. Mach. Learn. Res., 6: 1043-1071", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "More efficiency in multiple kernel learning", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Localized polynomial frames on the ball", "author": ["P.P. Petrushev", "Y. Xu"], "venue": "Constr. Approx., 27: 121-148", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalized inverse of matrices and its application", "author": ["C.R. Rao", "S.K. Mitra"], "venue": "Wiley, New York", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1971}, {"title": "Learning with Kernel: Support Vector Machine", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning). The MIT Press, Cambridge", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["S. Smale", "D.X. Zhou"], "venue": "Constr. Approx., 26: 153-172", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast rates for support vector machines using Gaussian kernels", "author": ["I. Steinwart", "C. Scovel"], "venue": "Ann. Statist., 35: 575-607", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal rates for regularized least squares regression", "author": ["I. Steinwart", "D. Hush", "C. Scovel"], "venue": "Proceedings of the 22nd Conference on Learning Theory", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Composite kernel learning", "author": ["M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Learning rates for regularized classifiers using multivariate polynomial kernels", "author": ["H.Z. Tong", "D.R. Chen", "Z.P. Li"], "venue": "J. Complex., 24: 619-631", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J.S. Taylor", "N. Cristianini"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning rates of least square regularized regression", "author": ["Q. Wu", "Y.M. Ying", "D.X. Zhou"], "venue": "Found. Comput. Math., 6: 171-192", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Orthogonal polynomials and cubature formulae on spheres and on balls", "author": ["Y. Xu"], "venue": "SIAM J. Math. Anal., 29: 779-793", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Minimax nonparametric classification", "author": ["Y. Yang"], "venue": "I. Rates of convergence. II. Model selection for adaptation. IEEE Trans. Inform. Theory, 45: 2271-2292", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1999}, {"title": "Estimation of learning rate of least square algorithm via Jackson operator", "author": ["Y.Q. Zhang", "F.L. Cao", "Z.B. Xu"], "venue": "Neurocomputing, 74: 516-521", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximation with polynomial kernels and SVM classifiers", "author": ["D.X. Zhou", "K. Jetter"], "venue": "Adv. Comput. Math., 25: 323-344", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "If the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter.", "startOffset": 22, "endOffset": 25}, {"referenceID": 34, "context": "If the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection.", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection.", "startOffset": 87, "endOffset": 90}, {"referenceID": 24, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 157, "endOffset": 160}, {"referenceID": 31, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 127, "endOffset": 131}, {"referenceID": 39, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 157, "endOffset": 161}, {"referenceID": 32, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 163, "endOffset": 167}, {"referenceID": 38, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 205, "endOffset": 208}, {"referenceID": 10, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 210, "endOffset": 214}, {"referenceID": 30, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 216, "endOffset": 220}, {"referenceID": 35, "context": "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].", "startOffset": 222, "endOffset": 226}, {"referenceID": 8, "context": "Different from other widely used kernels [9], the reproducing kernel Hilbert space Hs of the polynomial kernel Ks = (1 + x \u00b7 y) is a finite-dimensional vector space, and its dimension depends only on s.", "startOffset": 41, "endOffset": 44}, {"referenceID": 39, "context": "Using this fact, [41] found that the regularization parameter in polynomial kernel regression should decrease exponentially fast with the sample size for appropriately selected s.", "startOffset": 17, "endOffset": 21}, {"referenceID": 39, "context": "The first purpose of this paper is to continue the study of [41].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression? To answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points.", "startOffset": 283, "endOffset": 286}, {"referenceID": 8, "context": "automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression? To answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points.", "startOffset": 288, "endOffset": 291}, {"referenceID": 14, "context": "For example, since the Gaussian-RKHS is an infinite dimensional vector space, the introducing of regularization term in Gaussian kernel regression is to control both the condition number of the kernel matrix and capacity of the hypothesis space [15].", "startOffset": 245, "endOffset": 249}, {"referenceID": 6, "context": "By the well known representation theorem [7] in learning theory, the essential hypothesis space of polynomial kernel regression is the linear space H := span{(1+x1\u00b7x), \u00b7 \u00b7 \u00b7 , (1+ xm \u00b7x)s}.", "startOffset": 41, "endOffset": 44}, {"referenceID": 26, "context": "Then the pseudo-inverse technique [27] can conduct the estimator easily.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "Z (f(x)\u2212 y)d\u03c1, which is minimized by the regression function [7], defined by f\u03c1(x) := \u222b", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "(2) It is well known [7], [9], [14] that a small H will derive a large bias \u2016f\u03c1\u2212fH\u2016\u03c1, while a large H will deduce a large variance E(fz)\u2212E(fH).", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "(2) It is well known [7], [9], [14] that a small H will derive a large bias \u2016f\u03c1\u2212fH\u2016\u03c1, while a large H will deduce a large variance E(fz)\u2212E(fH).", "startOffset": 26, "endOffset": 29}, {"referenceID": 13, "context": "(2) It is well known [7], [9], [14] that a small H will derive a large bias \u2016f\u03c1\u2212fH\u2016\u03c1, while a large H will deduce a large variance E(fz)\u2212E(fH).", "startOffset": 31, "endOffset": 35}, {"referenceID": 1, "context": "Then HK (see [2]) is the closure of the linear span of the set of functions {Kx = K(x, \u00b7) : x \u2208 X} with the inner product \u3008\u00b7, \u00b7\u3009K satisfying \u3008Kx, Ky\u3009K = K(x, y) and \u3008Kx, f\u3009K = f(x), \u2200x \u2208 X, f \u2208 HK .", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "The following Aronszajn Theorem (see [2]) describes an essential relationship between the RKHS and reproducing kernel.", "startOffset": 37, "endOffset": 40}, {"referenceID": 39, "context": "Then it is obvious that [41] for all t \u2208 R and y \u2208 [\u2212M,M ] there holds E(\u03a0Mfz,\u03bb,s)\u2212 E(f\u03c1) \u2264 E(fz,\u03bb,s)\u2212 E(f\u03c1).", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "The first result, to the best of our knowledge, concerning selection of the optimal regularization parameter in the framework of learning theory belongs to [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "After checking the proof of [8] carefully, we find that there is nothing to surprise.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "On one hand, the optimal parameter mentioned in [8] aims to the generalization error, containing learning rate and the constant C2, while our result only concerns the learning rate.", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "On the other hand, [8]\u2019s result is more suitable to describe the performance of K(\u00b7, \u00b7) satisfying \u2016f\u2016\u221e \u2264 C\u2016f\u2016K , where C is a constant independent of m.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "After [8], we have witnessed the multiple emergence of the selection strategies of regularization parameter.", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 47, "endOffset": 51}, {"referenceID": 29, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 38, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 122, "endOffset": 125}, {"referenceID": 30, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 193, "endOffset": 197}, {"referenceID": 28, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 250, "endOffset": 254}, {"referenceID": 35, "context": "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].", "startOffset": 256, "endOffset": 260}, {"referenceID": 32, "context": "For polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter.", "startOffset": 53, "endOffset": 57}, {"referenceID": 39, "context": "For polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter.", "startOffset": 59, "endOffset": 63}, {"referenceID": 32, "context": "It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m 2r 2r+d+1 ), which is improved by Theorem 1 in the following three directions.", "startOffset": 30, "endOffset": 34}, {"referenceID": 39, "context": "It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m 2r 2r+d+1 ), which is improved by Theorem 1 in the following three directions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "Eberts and Steinwart [15] have already built a similar learning rate analysis for Gaussian kernel regression.", "startOffset": 21, "endOffset": 25}, {"referenceID": 37, "context": "\uf8f4 \uf8f3 1, if fz,\u03bb,s \u2265 12 , 0, if fz,\u03bb,s < 1 2 , (8) Theorem 1 and [39] imply that the classier defined as in (8) is also almost optimal if the well known Bayes decision function satisfies a certain smoothness assumption.", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "To this end, we should introduce the conceptions of Haar space and fundamental system [34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "Since the uniform distribution is continuous with respect to Lebesgue measure [4], we can draw {\u03b7j}j=1 independently and identically according to the uniform distribution.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Let f(t) = (1 \u2212 2t)+(32t 2 + 10t + 1), where t \u2208 [0, 1] and a+ = max{a, 0}.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Then it is easy to see that f \u2208 W 4 \u221e([0, 1]) and f / \u2208 W 5 \u221e([0, 1]).", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": "Then it is easy to see that f \u2208 W 4 \u221e([0, 1]) and f / \u2208 W 5 \u221e([0, 1]).", "startOffset": 62, "endOffset": 68}, {"referenceID": 9, "context": "2, we study the relation between TestRMSE and s for model (3), where \u03bb is the optimal value of 50 candidates drawn equally spaced in [10, 1].", "startOffset": 133, "endOffset": 140}, {"referenceID": 0, "context": "2, we study the relation between TestRMSE and s for model (3), where \u03bb is the optimal value of 50 candidates drawn equally spaced in [10, 1].", "startOffset": 133, "endOffset": 140}, {"referenceID": 0, "context": "Since f \u2208 W 2 \u221e([0, 1]), it follows from Theorem 1 that the optimal s may close to the value \u2308m1/(2r+d)\u2309 = 4.", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "EPKRF denotes that {\u03b7i}i=1 are chosen as the n equally spaced points in [0, 1].", "startOffset": 72, "endOffset": 78}, {"referenceID": 34, "context": "Method choices: In the UCI data experiment, we compare four methods containing support vector machine (SVM) [35], Gaussian kernel regression (GKR) [15, Eqs.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "d according to the uniform distribution on [0, 1].", "startOffset": 43, "endOffset": 49}, {"referenceID": 25, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 81, "endOffset": 87}, {"referenceID": 0, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 129, "endOffset": 135}, {"referenceID": 1, "context": "A function \u03b7 is said to be admissible [26] if \u03b7 \u2208 C\u221e[0,\u221e), \u03b7(t) \u2265 0, and supp\u03b7 \u2282 [0, 2], \u03b7(t) = 1 on [0, 1], and 0 \u2264 \u03b7(t) \u2264 1 on [1, 2].", "startOffset": 129, "endOffset": 135}, {"referenceID": 9, "context": "Such a function can be easily constructed out of an orthogonal wavelet mask [10].", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": ", (11) where G\u03bck is the well known Gegenbauer polynomial with order \u03bc [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "It can easily deduced from [26] that |Ls(x, y)| \u2264 Cs, x, y \u2208 B.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "Since f\u03c1 \u2208 W r \u221e, the well known Jackson inequality [12] shows that (E[s/2](f\u03c1)\u221e) 2 \u2264 Cs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "The second one is the well known cubature formula on the sphere, which can be found in [21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 33, "context": ", tN be the quasi-uniform points [34] on the sphere.", "startOffset": 33, "endOffset": 37}], "year": 2015, "abstractText": "Polynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the \u201c ill-condition\u201d of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability. Index Terms Model selection, regression, polynomial kernel, learning rate. S. Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China, Z. Xu and J. Zeng are with the Institute for Information and System Sciences, School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an 710049, P R China, X. Sun is with the Department of Mathematics, Missouri State University, Springfield, MO 65897, USA March 10, 2015 DRAFT", "creator": "LaTeX with hyperref package"}}}