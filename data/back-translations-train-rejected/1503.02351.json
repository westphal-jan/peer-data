{"id": "1503.02351", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Fully Connected Deep Structured Networks", "abstract": "Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.", "histories": [["v1", "Mon, 9 Mar 2015 01:08:00 GMT  (1545kb,D)", "http://arxiv.org/abs/1503.02351v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["alexander g schwing", "raquel urtasun"], "accepted": false, "id": "1503.02351"}, "pdf": {"name": "1503.02351.pdf", "metadata": {"source": "CRF", "title": "Fully Connected Deep Structured Networks", "authors": ["Alexander G. Schwing", "Raquel Urtasun"], "emails": ["aschwing@cs.toronto.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, Convolutionary Neural Networks (CNNs) have revolutionized computer vision, and have been shown to provide cutting-edge performance for a variety of visual problems, including image classification [19, 31], object recognition [11], estimation of human poses [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10]. This is mainly due to their high representational power, which is achieved by learning complex, nonlinear dependencies. Recently, Constitutional Networks have proven to be very effective for semantic segmentation [12, 30, 21, 41, 3] perhaps due to the fact that pooling operations are performed to achieve inventory, which often reduce the dimensionality of prediction, and a Markov Random Field (MRF) is then used as a refinement step for recording segment boundaries."}, {"heading": "2 Background", "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position to put ourselves in a position to put ourselves in a position where we are in a position to manoeuvre ourselves into a situation where we are in a position to manoeuvre ourselves into a situation in which we are in, in which we are in."}, {"heading": "3 Approach", "text": "Closely connected models were previously considered by [17, 33, 34, 18] and showed impressive results for the task of image segmentation. Learning the parameters of closely connected models was considered by Kra \ufffd henbu \ufffd hl and Koltun [18] in the context of log-linear setting. In the following [4], we aim to extend these fully connected log-linear models to the more general setting of any function F (x, y; w), e.g., a deep Constitutional Neural Network. Note that a similar approach was recently discussed by [41] in independent work. In this section, let's consider how to efficiently combine deep-structured predictions [4] with closely connected probability models [17, 33, 34, 18]. Before going into detail, we note that the presented approach combines the computational complexity of the general method of [4] with a limitation of pairwise functions j (i.e. {j = concretely, the local functions are not considered as {j = B}, but rather as links below the local functions)."}, {"heading": "3.1 Inference", "text": "We begin our discussion by considering the conclusion: In order to obtain a mathematically efficient prediction algorithm, we use an average field approximation of the distribution p (x, y) for each sample (x, y). Formally, we assume that our approximation to the factor q (x, y) (y) = 1 q (x, y), i (y), i (c), i (c), i (c), i (c), i (c), c (c), c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c (c), c (c), c (c), c (c), c (c), c (c), c (c (c), c), c (c (c), c), c (c (c), c (c), c (c (c), c (c), c (c), c (c), c (c (c), c (c), c (c), c (c), c (c), c (c), c (c (c), c (c), c (c), c (c), c (c (c), c (c), c (c), c), c (c (c), c (c (c),"}, {"heading": "3.2 Learning", "text": "Since the exact probability is mathematically very high, however, we use the log probability, which is based on the center margin marginal. Therefore, our surrogate loss function L (x, y) is given for a sample (x, y) with corresponding annotated truth designation y (x, y). (q, y) The log probability is based on the center margin marginal. (6) To perform a parameter update step, we need the gradients of the surrogate loss function w.r.t. The parameters, i.e., L (x, y)."}, {"heading": "4 Experiments", "text": "The task is semantic image segmentation of 21 object classes (including background).The original dataset contains 1464 training images, 1449 validation images and 1456 test images. In addition to these data, we use the annotations from Hariharan et al. [13], resulting in a total of 10582 training instances. Reported performance is measured using the metric of overlaps. Note that we perform our tests on the 1449 validation set images that were not used during training or for fine-tuning."}, {"heading": "4.1 Model", "text": "This is useful because we are not interested in a single variable output prediction, but rather in capturing learning probabilities. \"In order to get a larger probability mask, we take note of the fact that the data is stored in an interleaved way, i.e. to take into account skipped downsampling data during subsequent convolutions, we deal with the\" hole-shaped \"algorithm.\" It takes care of the fact that the data is stored in an interleaved way, i.e., in our case studies, the input data will be increased by a factor of two or four or four respectively. To replace the top level of the DeepNet model, we assume that we are expanding classes for each pixel. Similar to [3] we are expanding the input size of our network by a factor of two or four respectively two object classes."}, {"heading": "4.2 Results", "text": "As already mentioned, all our results were calculated based on the validation set of the Pascal VOC dataset. This part of the data was not used for training or fine tuning.Uniform performance: We first examine the performance of the first training phase of the proposed approach, i.e. fine tuning the 16-layer DeepNet parameters to the Pascal VOC datas.The specified accuracy of the validation is plotted over the number of iterations in Fig. 4 (a).We observe the performance up to a peak of about 4000 iterations with an average intersection above the union measure of 61.476%. The result reported by [3] for this experiment is 59.80%, i.e. we exceed its uniform model by 1.5%. Common training: Next, we illustrate the performance of the second step, i.e. joint training of both convolutionary network parameters and CRF compatibility parameters and form parameters, and we show the best possible validation in the best possible validation."}, {"heading": "5 Discussion", "text": "We presented a first method that trains together revolutionary neural networks and fully connected conditional random fields for semantic image segmentation. To this end, we generalize [3] joint training. Note that a method in this direction has also recently been made publicly available in independent work [41]. While the latter combines dense conditional random fields [17] with the fully conditional networks presented by Long et al. [21], we are using and modifying the 16-layer DeepNet architecture that Simonyan and Zisserman have presented in work [31]. Ideas along the lines of joint training have already been discussed in the 1990s in the work of Bridle [2] and Bottou [1]. More recently [5, 27, 22, 6, 28, 25] integrate nonlinearity into irregular potentials, but generally assume that exact conclusions are traceable."}, {"heading": "6 Conclusion", "text": "We discussed a method of semantic image segmentation that trains Convolutionary Neural Networks and Conditional Random Fields together. Our approach combines techniques from deep Convolutionary Neural Networks with varying mid-range approaches from the graphical model literature. We get good results on the challenging Pascal VOC 2012 datasets. In the future, we plan to train our method on larger datasets. Additionally, we will study training with weakly marked data."}], "references": [{"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. LeCun"], "venue": "Proc. CVPR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters", "author": ["J.S. Bridle"], "venue": "Proc. NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Semantic ImageSegmentation with Deep Convolutional Nets and Fully Connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "http://arxiv.org/abs/1412.7062", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Deep Structured Models", "author": ["L.-C. Chen", "A.G. Schwing", "A.L. Yuille", "R. Urtasun"], "venue": "http://arxiv.org/abs/1407.2538", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural conditional random fields", "author": ["T.-M.-T. Do", "T. Artieres"], "venue": "Proc. AISTATS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Structured Learning via Logistic Regression", "author": ["J. Domke"], "venue": "Proc. NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendrikcs", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "http://arxiv.org/abs/1411.4389", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "and A", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn"], "venue": "Zisserman. The PASCAL Visual Object Classes Challenge 2012 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "http://arxiv.org/abs/1411.4952", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proc. CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast image scanning with deep max-pooling convolutional neural networks", "author": ["A. Guisti", "D. Ciresan", "J. Masci", "L. Gambardella", "J. Schmidhuber"], "venue": "Proc. ICIP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic Contours from Inverse Detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "Proc. ICCV", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "http://arxiv.org/abs/1412.2306", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi- modal neural language models", "author": ["R. Kiros", "R. Salahutdinov", "R. Zemel"], "venue": "Proc. ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient inference in fully connected CRFs with Gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "Proc. NIPS", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter Learning and Convergent Inference for Dense Random Fields", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "Proc. ICML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "High Order Regularization for Semi-Supervised Learning of Structured Output Problems", "author": ["Y. Li", "R. Zemel"], "venue": "Proc. ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "http://arxiv.org/abs/1411.4038", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A conditional neural fields model for protein threading", "author": ["J. Ma", "J. Peng", "S. Wang", "J. Xu"], "venue": "Bioinformatics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A Wavelet Tour of Signal Processing", "author": ["S. Mallat"], "venue": "Academic Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "http://arxiv.org/abs/1412.6632", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional random fields for integrating local discriminative classifiers", "author": ["J. Morris", "E. Fosler-Lussier"], "venue": "IEEE Trans. Audio, Speech, and Language Processing", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1988}, {"title": "Conditional Neural Fields", "author": ["J. Peng", "L. Bo", "J. Xu"], "venue": "Proc. NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Backpropagation training for multilayer conditional random field based phone recognition", "author": ["R. Prabhavalkar", "E. Fosler-Lussier"], "venue": "Proc. ICASSP", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "and L", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg"], "venue": "Fei-Fei. ImageNet Large Scale Visual Recognition Challenge", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "OverFeat: Integrated Recognition", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "Localization and Detection using Convolutional Networks. In Proc. ICLR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "http://arxiv.org/abs/1409.1556", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation", "author": ["J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "Proc. NIPS", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved initialization and Gaussian mixture pairwise terms for dense random fields with mean-field inference", "author": ["V. Vineet", "J. Warrell", "P. Sturgess", "P.H.S. Torr"], "venue": "Proc. BMVC", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Filter-based mean-field inference for random fields with higherorder terms and product label-spaces", "author": ["V. Vineet", "J. Warrell", "P.H.S. Torr"], "venue": "Proc. ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "http://arxiv.org/abs/1411.4555", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing the Stereo Matching Cost with a Convolutional Neural Network", "author": ["J. \u017dbontar", "Y. LeCun"], "venue": "http://arxiv.org/abs/1409.4326", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree-based reparameterization framework for analysis of sum-product and related algorithms", "author": ["M.J. Wainwright", "T. Jaakkola", "A.S. Willsky"], "venue": "Trans. Information Theory", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Graphical models", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "exponential families and variational inference. Foundations and Trends in Machine Learning", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "MAP Estimation", "author": ["Y. Weiss", "C. Yanover", "T. Meltzer"], "venue": "Linear Programming and Belief Propagation with Convex Free Energies. In Proc. UAI", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "The Concave-Convex Procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Conditional Random Fields as Recurrent Neural Networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H.S. Torr"], "venue": "http://arxiv.org/abs/1502.03240", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 125, "endOffset": 133}, {"referenceID": 30, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 125, "endOffset": 133}, {"referenceID": 10, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 31, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 35, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 193, "endOffset": 197}, {"referenceID": 14, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 222, "endOffset": 245}, {"referenceID": 23, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 222, "endOffset": 245}, {"referenceID": 34, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 222, "endOffset": 245}, {"referenceID": 7, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 222, "endOffset": 245}, {"referenceID": 13, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 222, "endOffset": 245}, {"referenceID": 9, "context": "They have been shown to achieve state-of-the-art performance in a variety of vision problems, including image classification [19, 31], object detection [11], human pose estimation [32], stereo [36], and caption generation [15, 24, 35, 8, 14, 10].", "startOffset": 222, "endOffset": 245}, {"referenceID": 11, "context": "It is only very recently that convolutional nets have proven also very effective for semantic segmentation [12, 30, 21, 41, 3].", "startOffset": 107, "endOffset": 126}, {"referenceID": 29, "context": "It is only very recently that convolutional nets have proven also very effective for semantic segmentation [12, 30, 21, 41, 3].", "startOffset": 107, "endOffset": 126}, {"referenceID": 20, "context": "It is only very recently that convolutional nets have proven also very effective for semantic segmentation [12, 30, 21, 41, 3].", "startOffset": 107, "endOffset": 126}, {"referenceID": 40, "context": "It is only very recently that convolutional nets have proven also very effective for semantic segmentation [12, 30, 21, 41, 3].", "startOffset": 107, "endOffset": 126}, {"referenceID": 2, "context": "It is only very recently that convolutional nets have proven also very effective for semantic segmentation [12, 30, 21, 41, 3].", "startOffset": 107, "endOffset": 126}, {"referenceID": 16, "context": "The seminal work of [17] showed that inference in fully connected MRFs is possible if the smoothness potentials are Gaussian.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "Later, [3] extended the unary potentials to incorporate convolutional network features.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "We demonstrate the effectiveness of our approach using the dataset of the PASCAL VOC 2012 challenge [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "[4], who discussed extending log-linear models, i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "In short, [4] assumed the global scoring function F (x, \u0177;w) to decompose into a sum of local scoring functions fr, each depending on a small subset r \u2286 {1, .", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "To this end, message passing type algorithms were employed by [4].", "startOffset": 62, "endOffset": 65}, {"referenceID": 25, "context": "Otherwise computational complexity is prohibitively large and approximations like loopy belief propagation [26], convex belief propagation [39] or treereweighted message passing [37] are alternatives that were successfully applied.", "startOffset": 107, "endOffset": 111}, {"referenceID": 38, "context": "Otherwise computational complexity is prohibitively large and approximations like loopy belief propagation [26], convex belief propagation [39] or treereweighted message passing [37] are alternatives that were successfully applied.", "startOffset": 139, "endOffset": 143}, {"referenceID": 36, "context": "Otherwise computational complexity is prohibitively large and approximations like loopy belief propagation [26], convex belief propagation [39] or treereweighted message passing [37] are alternatives that were successfully applied.", "startOffset": 178, "endOffset": 182}, {"referenceID": 3, "context": "The resulting iterative method of [4] is summarized in Fig.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "Not only does the approach presented by [4] fail if the decomposition assumed in Eq.", "startOffset": 40, "endOffset": 43}, {"referenceID": 16, "context": "Densely connected models were previously considered by [17, 33, 34, 18] and shown to yield impressive results for the image segmentation task.", "startOffset": 55, "endOffset": 71}, {"referenceID": 32, "context": "Densely connected models were previously considered by [17, 33, 34, 18] and shown to yield impressive results for the image segmentation task.", "startOffset": 55, "endOffset": 71}, {"referenceID": 33, "context": "Densely connected models were previously considered by [17, 33, 34, 18] and shown to yield impressive results for the image segmentation task.", "startOffset": 55, "endOffset": 71}, {"referenceID": 17, "context": "Densely connected models were previously considered by [17, 33, 34, 18] and shown to yield impressive results for the image segmentation task.", "startOffset": 55, "endOffset": 71}, {"referenceID": 17, "context": "Learning the parameters of densely connected models was considered by Kr\u00e4henb\u00fchl and Koltun [18] in the context of the log-linear setting.", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "Following [4] we aim at extending those fully connected log-linear models to the more general setting of an arbitrary function F (x, \u0177;w), e.", "startOffset": 10, "endOffset": 13}, {"referenceID": 40, "context": "Note that a similar approach has been recently discussed by [41] in independent work.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "Let us consider within this section how to efficiently combine deep structured prediction [4] with densely connected probabilistic models [17, 33, 34, 18].", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "Let us consider within this section how to efficiently combine deep structured prediction [4] with densely connected probabilistic models [17, 33, 34, 18].", "startOffset": 138, "endOffset": 154}, {"referenceID": 32, "context": "Let us consider within this section how to efficiently combine deep structured prediction [4] with densely connected probabilistic models [17, 33, 34, 18].", "startOffset": 138, "endOffset": 154}, {"referenceID": 33, "context": "Let us consider within this section how to efficiently combine deep structured prediction [4] with densely connected probabilistic models [17, 33, 34, 18].", "startOffset": 138, "endOffset": 154}, {"referenceID": 17, "context": "Let us consider within this section how to efficiently combine deep structured prediction [4] with densely connected probabilistic models [17, 33, 34, 18].", "startOffset": 138, "endOffset": 154}, {"referenceID": 3, "context": "Before getting into the details we note that the presented approach trades computational complexity of the general method of [4] with a restriction on the pairwise functions fij (i.", "startOffset": 125, "endOffset": 128}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Due to non-convexity, only convergence to a stationary point of the KL divergence cost function is guaranteed for sequential block-coordinate updates [38, 16].", "startOffset": 150, "endOffset": 158}, {"referenceID": 15, "context": "Due to non-convexity, only convergence to a stationary point of the KL divergence cost function is guaranteed for sequential block-coordinate updates [38, 16].", "startOffset": 150, "endOffset": 158}, {"referenceID": 17, "context": "Hence the complexity of an update for a single marginal is of O(N), and updating all N marginals therefore requires O(N) operations as also discussed by Kr\u00e4henb\u00fchl and Koltun [18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "Importantly, Kr\u00e4henb\u00fchl and Koltun [17] observed that a high dimensional Gaussian filter can be applied to concurrently update all marginals in O(N).", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "Formally, if the label compatibility functions \u03bc are negative semi-definite \u2200m, and the kernels k are positive definite \u2200m, the KL divergence is readily given as the difference between a concave and a convex term [18].", "startOffset": 213, "endOffset": 217}, {"referenceID": 39, "context": "Hence the concave-convex procedure (CCCP) [40] is directly applicable.", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "As detailed by Kr\u00e4henb\u00fchl and Koltun [18], and as discussed above, finding the linearization is equivalently solved via filtering in time linear in N .", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "We refer the interested reader to [18] for additional details regarding the computation of the gradient \u2202q(x,y),i(\u0177i) \u2202w .", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "But contrasting [18], we no longer assume the unaries to be given by a logistic regression model.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": "Contrasting [3], we don\u2019t assume the unaries to be fixed during CRF parameter updates.", "startOffset": 12, "endOffset": 15}, {"referenceID": 17, "context": "to label compatibility and kernel shape parameters are readily given in [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "3 on the dataset of the Pascal VOC 2012 challenge [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 12, "context": "[13], resulting in a total of 10582 training instances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Our model setup follows [3], i.", "startOffset": 24, "endOffset": 27}, {"referenceID": 30, "context": ", we employ the 16 layer DeepNet model [31].", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "Just like [3] we first convert the fully connected layers into convolutions as first discussed in [12, 30].", "startOffset": 10, "endOffset": 13}, {"referenceID": 11, "context": "Just like [3] we first convert the fully connected layers into convolutions as first discussed in [12, 30].", "startOffset": 98, "endOffset": 106}, {"referenceID": 29, "context": "Just like [3] we first convert the fully connected layers into convolutions as first discussed in [12, 30].", "startOffset": 98, "endOffset": 106}, {"referenceID": 22, "context": "To take into account the skipped downsampling during subsequent convolutions we employ the \u2018\u00e0 trous (with hole) algorithm\u2019 [23].", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "Similar to [3] we assume the input size of our network to be of dimension 306\u00d7 306 which results in a 40\u00d740 sized spatial output of the DeepNet which is in our case an intermediate result however.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Contrasting [3], we jointly optimize for both unary and CRF parameters using the algorithm presented in Fig.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "Data table dog horse mbike person plant sheep sofa train tv Our mean [3] Valid.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "It was shown independently by many authors [31, 4], that successively increasing the number of parameters during training typically yields better performance due to better initialization of larger models.", "startOffset": 43, "endOffset": 50}, {"referenceID": 3, "context": "It was shown independently by many authors [31, 4], that successively increasing the number of parameters during training typically yields better performance due to better initialization of larger models.", "startOffset": 43, "endOffset": 50}, {"referenceID": 30, "context": ", we fine-tune the weights obtained from the DeepNet ImageNet model [31, 29] to the Pascal dataset [9].", "startOffset": 68, "endOffset": 76}, {"referenceID": 28, "context": ", we fine-tune the weights obtained from the DeepNet ImageNet model [31, 29] to the Pascal dataset [9].", "startOffset": 68, "endOffset": 76}, {"referenceID": 8, "context": ", we fine-tune the weights obtained from the DeepNet ImageNet model [31, 29] to the Pascal dataset [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "The result reported by [3] for this experiment is 59.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "To this end we generalize [3] to joint training.", "startOffset": 26, "endOffset": 29}, {"referenceID": 40, "context": "Note that a method along those lines has also been recently made publicly available in independent work [41].", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "Whereas the latter combines dense conditional random fields [17] with the fully convolutional networks presented by Long et al.", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "[21], we employ and modify the 16 layer DeepNet architecture presented in work by Simonyan and Zisserman [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[21], we employ and modify the 16 layer DeepNet architecture presented in work by Simonyan and Zisserman [31].", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "Ideas along the lines of joint training were discussed within machine learning and computer vision as early as the 90\u2019s in work done by Bridle [2] and Bottou [1].", "startOffset": 143, "endOffset": 146}, {"referenceID": 0, "context": "Ideas along the lines of joint training were discussed within machine learning and computer vision as early as the 90\u2019s in work done by Bridle [2] and Bottou [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "More recently [5, 27, 22, 6, 28, 25] incorporate non-linearities into unary potentials but generally assume exact inference to be tractable.", "startOffset": 14, "endOffset": 36}, {"referenceID": 26, "context": "More recently [5, 27, 22, 6, 28, 25] incorporate non-linearities into unary potentials but generally assume exact inference to be tractable.", "startOffset": 14, "endOffset": 36}, {"referenceID": 21, "context": "More recently [5, 27, 22, 6, 28, 25] incorporate non-linearities into unary potentials but generally assume exact inference to be tractable.", "startOffset": 14, "endOffset": 36}, {"referenceID": 5, "context": "More recently [5, 27, 22, 6, 28, 25] incorporate non-linearities into unary potentials but generally assume exact inference to be tractable.", "startOffset": 14, "endOffset": 36}, {"referenceID": 27, "context": "More recently [5, 27, 22, 6, 28, 25] incorporate non-linearities into unary potentials but generally assume exact inference to be tractable.", "startOffset": 14, "endOffset": 36}, {"referenceID": 24, "context": "More recently [5, 27, 22, 6, 28, 25] incorporate non-linearities into unary potentials but generally assume exact inference to be tractable.", "startOffset": 14, "endOffset": 36}, {"referenceID": 19, "context": "Even more recently, Li and Zemel [20] investigate training with hinge-loss objectives using nonlinear unaries, but the pairwise potentials remain fixed, i.", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "Domke [7] decomposes the learning objective into logistic regressors which will be computationally expensive in our setting.", "startOffset": 6, "endOffset": 9}, {"referenceID": 31, "context": "[32] propose joint training for pose estimation based on a heuristic approximation which ignores the normalization constant of the model distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Joint training of conditional random fields and deep networks was also discussed recently by [4] for graphical models in general.", "startOffset": 93, "endOffset": 96}], "year": 2015, "abstractText": "Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.", "creator": "LaTeX with hyperref package"}}}