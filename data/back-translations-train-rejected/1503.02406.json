{"id": "1503.02406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Deep Learning and the Information Bottleneck Principle", "abstract": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.", "histories": [["v1", "Mon, 9 Mar 2015 09:39:41 GMT  (192kb,D)", "http://arxiv.org/abs/1503.02406v1", "5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information Theory Workshop (ITW) (IEEE ITW 2015)"]], "COMMENTS": "5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information Theory Workshop (ITW) (IEEE ITW 2015)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["naftali tishby", "noga zaslavsky"], "accepted": false, "id": "1503.02406"}, "pdf": {"name": "1503.02406.pdf", "metadata": {"source": "CRF", "title": "Deep Learning and the Information Bottleneck Principle", "authors": ["Naftali Tishby", "Noga Zaslavsky"], "emails": ["tishby@cs.huji.ac.il"], "sections": [{"heading": null, "text": "In fact, it is the case that we are able to find a solution that enables us to find a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Deep Neural Networks", "text": "The remarkable success of DNNs in learning is mainly attributed to the sequential processing of the data, namely that each hidden layer acts as input for the next level, enabling a higher level of representation.The remarkable performance of DNNs in learning often allows for a limited hierarchical representation of the data for different machine learning tasks. While there are many different variants of DNNs [9], here we are looking at the more general, supervised learning situation of feedback networks, where multiple hidden layers separate the input and output layers of the network (see Figure 1).Typically, this means that most of the entropy of X is not very informative about Y, and that the relevant features in X are highly distributed and difficult to extract.The remarkable success of DNNs in learning to extract such features is mainly attributed to the sequential processing of the data, namely that each hidden layer acts as input for the next level, which is a higher level of representation."}, {"heading": "B. The Information Bottleneck Principle", "text": "The question is how it has come about that there has been distribution competition between the two countries. (...) The question is to what extent there can be distribution competition between the two countries. (...) The question is to what extent there can be distribution competition between the two countries. (...) The question is to what extent there can be distribution competition between the two countries. (...) The question is to what extent there can be distribution competition between the two countries. (...) The question is to what extent there can be distribution competition between the two countries. (...) The question is to what extent there can be distribution competition between the two countries. (...) The question is to what extent there can be distribution competition between the two countries. (...)"}, {"heading": "III. A NEW INFORMATION THEORETIC LEARNING PRINCIPLE FOR DNNS", "text": "In fact, most of them are able to play by the rules, and they do not have to play by the rules."}, {"heading": "B. Finite Samples and Generalization Bounds", "text": "It is important to note that the IB curve is a property of the common distribution p (X, Y), but this distribution is obviously not able to grasp the actual knowledge of the IB level. (D) The IB level itself is not able to grasp the IB level. (D) The IB level is able to grasp the IB level in its entirety. (D) The IB level is not able to grasp the IB level. (D) The IB level is not able to grasp the IB level in its entirety. (D) The IB level is not able to grasp the IB level in its entirety. (D) The IB level is unable to grasp the IB level. (D) The IB level is unable to grasp the IB level. (D) The IB level is unable to grasp the IB level. (D)"}, {"heading": "V. DISCUSSION", "text": "We propose a novel information theory analysis of deep neural networks based on the principle of the information bottleneck. DNNs probably learn to extract efficient representations of the relevant features of the input layer X in order to predict the output label Y, since a finite sample of the common distribution p (X, Y) is available. This representation can be compromised by the theoretically optimal relevant compression of variable X with respect to Y by the information bottleneck (or information distortion) by introducing a new information theory view of DNN training as a successive (Markovian) relevant compression of the input variable X in view of empirical training data. DNN splitting activates the trained compression layer hierarchy to produce a predicted label Y."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u2013828, Aug. 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013 507, July 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1106\u20131114.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "An exact mapping between the variational renormalization group and deep learning", "author": ["P. Mehta", "D.J. Schwab"], "venue": "CoRR, vol. abs/1410.3831, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Elements of information theory", "author": ["T. Cover", "J. Thomas"], "venue": "Wiley New York,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "The information bottleneck method", "author": ["N. Tishby", "F.C. Pereira", "W. Bialek"], "venue": "Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing, 1999, pp. 368\u2013377.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Successive refinement of information", "author": ["W.H.R. Equitz", "T.M. Cover"], "venue": "IEEE Transactions on Information Theory, vol. 37, no. 2, pp. 269\u2013275, 1991.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning and generalization with the information bottleneck", "author": ["O. Shamir", "S. Sabato", "N. Tishby"], "venue": "Theor. Comput. Sci., vol. 411, no. 29-30, pp. 2696\u20132711, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, p. 310, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "An information theoretic tradeoff between complexity and accuracy", "author": ["R. Gilad-Bachrach", "A. Navot", "N. Tishby"], "venue": "Proceedings of the COLT, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems", "author": ["K. Rose"], "venue": "Proceedings of the IEEE, 1998, pp. 2210\u20132239.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Information bottleneck for gaussian variables", "author": ["G. Chechik", "A. Globerson", "N. Tishby", "Y. Weiss"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 165\u2013188, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Statistical mechanics and phase transitions in clustering", "author": ["K. Rose", "E. Gurewitz", "G.C. Fox"], "venue": "Phys. Rev. Lett., vol. 65, pp. 945\u2013948, 1990.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges [1], [2], [3].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges [1], [2], [3].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges [1], [2], [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 3, "context": "One step in that direction was recently made in a remarkable paper by Metha and Schwab [4] that showed an exact", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "The information theoretic interpretation of minimal sufficient statistics [5] suggests a principled way of doing that: find a maximally compressed mapping of the input variable that preserves as much as possible the information on the output variable.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "This is precisely the goal of the Information Bottleneck (IB) method [6].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "This is closely related to successive refinement of information in Rate Distortion Theory [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "In addition, this representation of DNNs gives a new theoretical sample complexity bound, using the known finite sample bounds on the IB [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "Another outcome of this representation is a possible explanation of the layered architecture of the network, different from the one suggested in [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "While there are many different variants of DNNs [9], here we consider the rather general supervised learning settings of feedforward networks in which multiple hidden layers separate the input and output layers of the network (see figure 1).", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "Interestingly, other DNN architectures implement stochastic mapping between the layers, such as the RBM based DNNs [2], but it is not clear so far why or when such stochasticity can improve performance.", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "Symmetries of the data are often taken into account through weight sharing, as in convolutional neural networks [10], [3].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "Symmetries of the data are often taken into account through weight sharing, as in convolutional neural networks [10], [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "As suggested in [4], approximate conditional independence is effectively achieved for RBM based DNNs through successive RG transformations that decouple the units without loss of relevant information.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "We thus assume the Markov chain Y \u2192 X \u2192 X\u0302 and minimize the mutual information I(X; X\u0302) to obtain the simplest statistics (due to the data processing inequality (DPI) [5]), under a constraint on I(X\u0302;Y ).", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "As was shown in [6], the optimal solutions for the IB variational problem satisfy the following self-consistent equations for some value of \u03b2,", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "The optimal tradeoff for this variational problem is defined by a rate-distortion like curve [11], as depicted by the black curve in figure 2.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "These bifurcations correspond to phase transitions between different topological representations of X\u0302 , such as different cardinality in clustering by deterministic annealing [12], or dimensionality change for continues variables [13].", "startOffset": 176, "endOffset": 180}, {"referenceID": 12, "context": "These bifurcations correspond to phase transitions between different topological representations of X\u0302 , such as different cardinality in clustering by deterministic annealing [12], or dimensionality change for continues variables [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 7, "context": "It has been shown in [8] that the mutual information I(X\u0302;Y ), which corresponds to I (Y ;hi) in our context, can bound the prediction error in classification tasks with multiple", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Nonetheless, it has been shown in [8] that it is possible to generalize using the IB principle as a learning objective from finite samples, as long as the representational complexity (i.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "The generalization bounds proven in [8] guarantee that", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Following the bifurcation analysis of the cluster splits in [14], [12] for the IB phase transitions, one can show that the critical \u03b2 is determined by the largest eigenvalue of the second order correlations of p(X,Y |X\u0302(\u03b2)), at that critical \u03b2.", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "Following the bifurcation analysis of the cluster splits in [14], [12] for the IB phase transitions, one can show that the critical \u03b2 is determined by the largest eigenvalue of the second order correlations of p(X,Y |X\u0302(\u03b2)), at that critical \u03b2.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "in the Gaussian IB problem [13]).", "startOffset": 27, "endOffset": 31}], "year": 2015, "abstractText": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network\u2019s simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.", "creator": "LaTeX with hyperref package"}}}