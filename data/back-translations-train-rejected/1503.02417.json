{"id": "1503.02417", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Structured Prediction of Sequences and Trees using Infinite Contexts", "abstract": "Linguistic structures exhibit a rich array of global phenomena, however commonly used Markov models are unable to adequately describe these phenomena due to their strong locality assumptions. We propose a novel hierarchical model for structured prediction over sequences and trees which exploits global context by conditioning each generation decision on an unbounded context of prior decisions. This builds on the success of Markov models but without imposing a fixed bound in order to better represent global phenomena. To facilitate learning of this large and unbounded model, we use a hierarchical Pitman-Yor process prior which provides a recursive form of smoothing. We propose prediction algorithms based on A* and Markov Chain Monte Carlo sampling. Empirical results demonstrate the potential of our model compared to baseline finite-context Markov models on part-of-speech tagging and syntactic parsing.", "histories": [["v1", "Mon, 9 Mar 2015 10:35:10 GMT  (794kb,D)", "http://arxiv.org/abs/1503.02417v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ehsan shareghi", "gholamreza haffari", "trevor cohn", "ann nicholson"], "accepted": false, "id": "1503.02417"}, "pdf": {"name": "1503.02417.pdf", "metadata": {"source": "CRF", "title": "Structured Prediction of Sequences and Trees using Infinite Contexts", "authors": ["Ehsan Shareghi", "Gholamreza Haffari", "Trevor Cohn", "Ann Nicholson"], "emails": ["1ehsan.shareghi@monash.edu", "gholamreza.haffari@monash.edu", "ann.nicholson@monash.edu", "2t.cohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to feel able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Background and related work", "text": "In fact, it is so that most of them are able to obey the rules that they have imposed on themselves. (...) In fact, it is so that they are able to determine for themselves what they want to do. (...) It is not so that they do it. (...) It is so. (...) It is as if they do it. (...) It is so. (...) \"\" It is so. \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (... \"(...)\" (... \")\" (... \")\" (... \")\" ((...) \"(\") ((...) \"(...\" (\") (\") ((\") ((\") (\"(\") (\"(\") (...) (\"(\" () (\"() (\" () (\"() (\" () () (\"() (\" () (\"() () (\" () (\"() () (\" () () () () () ()) () ()) () () () ()) () () ()) () () () () ()) () () () () ()) () () () ()) () () () () ()) () () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () (() (() () () (() (() () () (() () () (() () () () () (() () () (() (() () () (() () () (() () (() (() () (() () (() () ((("}, {"heading": "3 The Model", "text": "Our model loosens strong local Markov assumptions in PCFG to allow the capture of phenomena outside the local Markov context. Thus, the model requires the generation of a rule in a tree on its infinite vertical history, i.e., its ancestors on their way to the root of the tree (see Figure 1). Thus, the probability of a tree T depends on the context (T) = [u, r). [u,] where r denotes the rule and u denotes its history, and G [u] (.) is the probability of the next key line decision (i.e., the grammar rule) depends on the context and in other words, a tree T can be represented as a sequence of context rule events {u, r) its depth of distribution."}, {"heading": "4 Learning", "text": "Considering a training tree bank, i.e. a collection of statements and their trees, we are interested in the following distribution (u). (u) We use the approach developed in Wood et al. (2011) to learn suffix-based graphical models where the predictive probabilities P (r | u).Under the CRP representation, each context corresponds to a restaurant. As a new (u, r) in the training data, a customer is entered into the restaurant, i.e., the Trie node corresponds to and whenever a customer enters a restaurant, it should be decided whether to take a seat on an existing table or a new table."}, {"heading": "5 Prediction", "text": "In this section, we propose algorithms for the difficult problem of predicting the highest possible scoring tree. Key ideas are to compact the space of all possible trees for a given utterance and then search from top to bottom for the best tree in that space. By traversing the hyper graph from top to bottom, the search algorithms have access to the entire history of grammar rules. In the test period, we have to predict the tree structure of a given utterance w by maximizing tree evaluation: argmax T P (T | D, w) = argmax T (u, r). T P (r | u, D) The unlimited context allowed by our model makes it impossible to apply dynamic programming, e.g. CYK and Schwartz, 1970), to find the highest scoring tree. CYK is a bottom-up algorithm that requires storing in a dynamic programming table."}, {"heading": "5.1 A* Search", "text": "This algorithm gradually expands the boundary nodes of the best sub-tree until a complete tree is constructed. In the expansion step, all possible rules for the extension of all boundary tree types are taken into account and the resulting sub-trees are inserted into a priority list (see Figure 4), sorted by the following score: Score (T +) = logP (T) + logGu (A \u2192 B C) + h (T +, A \u2192 B, i, k, j | G), where T + is a sub-tree after the extension of a cross-border non-terminal tree, P (T) is the probability of the current sub-tree, Gu (A \u2192 B C) is the probability of the extension of a non-terminal tree over a rule A \u2192 B | G \u2032), and h is the heuristic function (i.e. the estimate of the score for the best tree completing T +), Gu (A \u2192 B C) is the probability of the extension of a non-terminal tree over a rule \u2192 B | G \u2032), and h is the best completion of the rule \u2192 C, and P is the completion in the context of the complete tree."}, {"heading": "5.2 MCMC Sampling", "text": "We use the Metropolis-Hastings (MH) algorithm, which is a Markov chain Monte Carlo (MCMC) method, to obtain a sequence of random trees. We then combine these trees to construct the predicted tree. In the MH algorithm, we use a PCFG as our proposal DistributionQ and take samples from it. Each sampled tree is then accepted / rejected using the following acceptance rate: \u03b1 (T, T) = min {1, P (T) Q (T)}}, where T \u2032 s is the sampled tree, P (T) is the probability of the proposed tree under our model, and Q (T) is its probability under the PCFG proposal. Under some conditions, i.e., detailed balance and ergodicity, it is guaranteed that the stationary distribution of the underlying Markov chain (defined by the MH sampling) is."}, {"heading": "6 Experiments", "text": "In order to evaluate the proposed model and the prediction algorithms, we conducted two series of experiments on tasks of different structural complexity. Statistics of the tasks and data sets are listed in Table 1."}, {"heading": "6.1 Syntactic Parsing", "text": "For syntactic parsing, we use the Pens. treebank (PTB) data set (Marcus et al., 1993). We used the standard data splitters for training and testing (turn sec 2-21; validation sec 22; test sec 23). We followed Petrov et al. (2006) pre-processing steps by right-binarizing the trees and replacing words by number \u2264 1 in the training sample with generic unknown word markers representing the lexical characteristics and positions of the tokens. The results in Table 2 are produced by EVALB. The results in Table 2 show the superiority of our model compared to the baseline PCFG. We notice that the A * parser becomes less effective (even with a large beam size) for this task, which we attribute to the large search for the large grammar and long sentences. Our best results are achieved by MCMC by demonstrating the effectiveness of the MCMC in large search spaces."}, {"heading": "6.2 Part-of-Speech Tagging", "text": "The portion of the speech corpora (POS) was taken from the PTB (Sections 0-18 for training and 22-24 for the test) for English and NAACL-HLT 2012 Shared task on Grammar Induction2 for Danish and Swedish (Gelling et al., 2012). We convert the sequence of part-of-speech tags for each sentence into a tree structure analogous to a Hidden Markov Model (HMM). For each POS tag, we perform http: / / wiki.cs.ox.ac.uk / InducingLinguisticStructure / SharedTaska twin (e.g. ADJ 'for ADJ) to encode HMM-like transition and emission probabilities in grammar. As shown in Figure 5, this representation guarantees that all rules in the structures are executed either in the form of ti \u2192 tj t \u2032 j (transition) or t's \u2192 word (emission) PCs. The tagging results are shown in Table 3, including the results for the analogy with the three PCs."}, {"heading": "7 Conclusion and Future Work", "text": "We have proposed a novel hierarchical model of linguistic trees that exploits the global context by conditioning the generation of a rule in a tree based on an unlimited tree context, consisting of the vertical chain of its ancestors. To facilitate the learning of such a large and unlimited model, the predictive distributions associated with tree contexts are smoothed recursively using a hierarchical Pitman-Yor process. We have shown how to perform predictions based on our model for predicting the parse tree of a given utterance using various search algorithms, e.g. A * and Markov Chain Monte Carlo. This has consistently improved on basic methods in two tasks and has provided the most modern results for Danish part-of-speech tagging. In the future, we would like to consider a sample of the seating arrangement and modeling of hyperparameters and try to include several contexts in addition to the precursors."}], "references": [{"title": "The infinite hidden markov model", "author": ["M.J. Beal", "Z. Ghahramani", "C.E. Rasmussen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Beal et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Beal et al\\.", "year": 2001}, {"title": "Tnt \u2013 A statistical part-of-speech tagger", "author": ["T. Brants"], "venue": "In Proceedings of the sixth conference on Applied natural language processing,", "citeRegEx": "Brants.,? \\Q2000\\E", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "Programming languages and their compilers : preliminary notes", "author": ["J. Cocke", "J.T. Schwartz"], "venue": "Technical report,", "citeRegEx": "Cocke and Schwartz.,? \\Q1970\\E", "shortCiteRegEx": "Cocke and Schwartz.", "year": 1970}, {"title": "Inducing treesubstitution grammars", "author": ["T. Cohn", "P. Blunsom", "S. Goldwater"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Cohn et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2010}, {"title": "The infinite tree", "author": ["J. Finkel", "T. Grenager", "C. Manning"], "venue": "In Proceedings of the 45th annual meeting of Association for Computational Linguistics,", "citeRegEx": "Finkel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2007}, {"title": "Improvements to the sequence memoizer", "author": ["J. Gasthaus", "Y.W. Teh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gasthaus and Teh.,? \\Q2010\\E", "shortCiteRegEx": "Gasthaus and Teh.", "year": 2010}, {"title": "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, chapter The PASCAL Challenge on Grammar Induction, pages 64\u201380", "author": ["D. Gelling", "T. Cohn", "P. Blunsom", "J. Graca"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Gelling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gelling et al\\.", "year": 2012}, {"title": "Pcfg models of linguistic tree representations", "author": ["M. Johnson"], "venue": "Computational Linguistics,", "citeRegEx": "Johnson.,? \\Q1998\\E", "shortCiteRegEx": "Johnson.", "year": 1998}, {"title": "Bayesian inference for pcfgs via markov chain monte carlo", "author": ["M. Johnson", "T.L. Griffiths", "S. Goldwater"], "venue": "In HLT-NAACL,", "citeRegEx": "Johnson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Parsing and hypergraphs", "author": ["D. Klein", "C.D. Manning"], "venue": "In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT-2001),", "citeRegEx": "Klein and Manning.,? \\Q2001\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2001}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "A bayesian model for learning scfgs with discontiguous rules. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pages 223\u2013232", "author": ["A. Levenberg", "C. Dyer", "P. Blunsom"], "venue": null, "citeRegEx": "Levenberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Levenberg et al\\.", "year": 2012}, {"title": "The infinite PCFG using hierarchical Dirichlet processes", "author": ["P. Liang", "S. Petrov", "M. Jordan", "D. Klein"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-", "citeRegEx": "Liang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2007}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Probabilistic cfg with latent annotations", "author": ["T. Matsuzaki", "Y. Miyao", "J. Tsujii"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Matsuzaki et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Matsuzaki et al\\.", "year": 2005}, {"title": "Learning and inference for hierarchically split PCFGs", "author": ["S. Petrov", "D. Klein"], "venue": "In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence,", "citeRegEx": "Petrov and Klein.,? \\Q2007\\E", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Enriching the knowledge sources used in a maximum entropy part-ofspeech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora,", "citeRegEx": "Toutanova and Manning.,? \\Q2000\\E", "shortCiteRegEx": "Toutanova and Manning.", "year": 2000}, {"title": "A stochastic memoizer for sequence data", "author": ["F. Wood", "C. Archambeau", "J. Gasthaus", "L. James", "Y.W. Teh"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Wood et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2009}, {"title": "A stochastic memoizer for sequence data", "author": ["F. Wood", "C. Archambeau", "J. Gasthaus", "L. James", "Y.W. Teh"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Wood et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2009}, {"title": "The sequence memoizer", "author": ["F. Wood", "J. Gasthaus", "C. Archambeau", "L. James", "Y.W. Teh"], "venue": "Communications of the ACM,", "citeRegEx": "Wood et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2011}, {"title": "Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale boundconstrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 5, "context": "While infinite order Markov models have been extensively explored for language modelling (Gasthaus and Teh, 2010; Wood et al., 2011), this has not yet been done for structure prediction.", "startOffset": 89, "endOffset": 132}, {"referenceID": 19, "context": "While infinite order Markov models have been extensively explored for language modelling (Gasthaus and Teh, 2010; Wood et al., 2011), this has not yet been done for structure prediction.", "startOffset": 89, "endOffset": 132}, {"referenceID": 16, "context": "obtains similar performance to the state-of-theart Stanford part-of-speech-tagger (Toutanova and Manning, 2000) for English and Swedish.", "startOffset": 82, "endOffset": 111}, {"referenceID": 19, "context": "A prime work is Sequence Memoizer (Wood et al., 2011) which conditions the generation of the next word on an unbounded history of previously generated words.", "startOffset": 34, "endOffset": 53}, {"referenceID": 12, "context": "For syntactic parsing, several infinite extensions of probabilistic context free grammars (PCFGs) have been proposed (Liang et al., 2007; Finkel et al., 2007).", "startOffset": 117, "endOffset": 158}, {"referenceID": 4, "context": "For syntactic parsing, several infinite extensions of probabilistic context free grammars (PCFGs) have been proposed (Liang et al., 2007; Finkel et al., 2007).", "startOffset": 117, "endOffset": 158}, {"referenceID": 3, "context": "An alternative method allows for infinite grammars by considering segmentation of trees into arbitrarily large tree fragments, although only a limited history is used to conjoin fragments (Cohn et al., 2010; Johnson et al., 2006).", "startOffset": 188, "endOffset": 229}, {"referenceID": 7, "context": "(Johnson, 1998) has increased the history for the parsing task by parent-annotation, i.", "startOffset": 0, "endOffset": 15}, {"referenceID": 10, "context": "(Klein and Manning, 2003) have considered vertical and horizontal markovization while using the head words\u2019 part-of-speech tag, and showed that increasing the size of the vertical contexts consistently improves the parsing performance.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": ", 2006), (Petrov and Klein, 2007) and (Matsuzaki et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 14, "context": ", 2006), (Petrov and Klein, 2007) and (Matsuzaki et al., 2005) have treated non-terminal annotations as latent variables and estimated them from the data.", "startOffset": 38, "endOffset": 62}, {"referenceID": 1, "context": "Previous works on applying Markov models to part-of-speech tagging either considered finiteorder Markov models (Brants, 2000), or finite-order HMM (Thede and Harper, 1999).", "startOffset": 111, "endOffset": 125}, {"referenceID": 19, "context": "More specifically, we assume that a distribution with the full history G[u] is related to a distribution with the most recent history G[\u03c0(u)] through the Pitman-Yor process PY P (Wood et al., 2011):", "startOffset": 178, "endOffset": 197}, {"referenceID": 17, "context": "We make use of the approach developed in Wood et al. (2011) for learning such suffix-based graphical models when learning infinite-depth language models.", "startOffset": 41, "endOffset": 60}, {"referenceID": 20, "context": "1 We maximize the posterior with the constraints cm \u2265 0 and dm \u2208 [0, 1) using the L-BFGSB optimisation method (Zhu et al., 1997), which results in the optimised discount and concentration values for each context size.", "startOffset": 110, "endOffset": 128}, {"referenceID": 2, "context": "CYK (Cocke and Schwartz, 1970), for finding the highest scoring tree.", "startOffset": 4, "endOffset": 30}, {"referenceID": 9, "context": "The space of all possible trees for a given utterance can be compactly represented as a hyper-graph (Klein and Manning, 2001).", "startOffset": 100, "endOffset": 125}, {"referenceID": 8, "context": "For each utterence, we sample a fresh tree for the whole utterance from a PCFG using the approach of (Johnson et al., 2007), which works by first computing the inside lattice under the proposal model (which can be computed once and reused), followed by top-down sampling to recover a tree.", "startOffset": 101, "endOffset": 123}, {"referenceID": 11, "context": "We leave local sampling for future work, noting that the obvious local operation of resampling complete sub-trees or local tree fragments would compromise detailed balance, and thus not constitute a valid MCMC sampler (Levenberg et al., 2012).", "startOffset": 218, "endOffset": 242}, {"referenceID": 13, "context": "treebank (PTB) dataset (Marcus et al., 1993).", "startOffset": 23, "endOffset": 44}, {"referenceID": 13, "context": "treebank (PTB) dataset (Marcus et al., 1993). We used the standard data splits for training and testing (train sec 2-21; validation sec 22; test sec 23). We followed Petrov et al. (2006) preprocessing steps by right-binarizing the trees and replacing words with count \u2264 1 in the training sample with generic unknown word markers representing the tokens\u2019 lexical features and position.", "startOffset": 24, "endOffset": 187}, {"referenceID": 10, "context": "An interesting observation is how our results compare with those achieved by bounded vertical and horizontal Markovization reported in (Klein and Manning, 2003).", "startOffset": 135, "endOffset": 160}, {"referenceID": 6, "context": "The part of speech (POS) corpora have been extracted from PTB (sections 0-18 for training and 22-24 for test) for English, and NAACL-HLT 2012 Shared task on Grammar Induction2 for Danish and Swedish (Gelling et al., 2012).", "startOffset": 199, "endOffset": 221}, {"referenceID": 16, "context": "The tagging results are reported in Table 3, including comparison with the baseline PCFG (\u2261 HMM) and the state-of-the-art Stanford POS Tagger (Toutanova and Manning, 2000), which we trained and tested on these datasets.", "startOffset": 142, "endOffset": 171}], "year": 2015, "abstractText": "Linguistic structures exhibit a rich array of global phenomena, however commonly used Markov models are unable to adequately describe these phenomena due to their strong locality assumptions. We propose a novel hierarchical model for structured prediction over sequences and trees which exploits global context by conditioning each generation decision on an unbounded context of prior decisions. This builds on the success of Markov models but without imposing a fixed bound in order to better represent global phenomena. To facilitate learning of this large and unbounded model, we use a hierarchical PitmanYor process prior which provides a recursive form of smoothing. We propose prediction algorithms based on A* and Markov Chain Monte Carlo sampling. Empirical results demonstrate the potential of our model compared to baseline finite-context Markov models on part-of-speech tagging and syntactic parsing.", "creator": "LaTeX with hyperref package"}}}