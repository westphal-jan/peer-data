{"id": "1503.02510", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Compositional Distributional Semantics with Long Short Term Memory", "abstract": "We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the `memory cell') and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank.", "histories": [["v1", "Mon, 9 Mar 2015 15:13:38 GMT  (173kb,D)", "https://arxiv.org/abs/1503.02510v1", "10 pages, 7 figures"], ["v2", "Fri, 17 Apr 2015 23:54:37 GMT  (173kb,D)", "http://arxiv.org/abs/1503.02510v2", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["phong le", "willem zuidema"], "accepted": false, "id": "1503.02510"}, "pdf": {"name": "1503.02510.pdf", "metadata": {"source": "CRF", "title": "Compositional Distributional Semantics with Long Short Term Memory", "authors": ["Phong Le", "Willem Zuidema"], "emails": ["p.le@uva.nl", "zuidema@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Multi-layer Neural Network", "text": "A neuron in layer i receives the signal from neurons in layer i-1 and transmits its output to neurons in layer i + 1. 1 The calculation is made according to Byyi = g (Wi \u2212 1, iyi \u2212 1 + bi) 1This is a simplified definition. In practice, each layer j < ican connect to layer i.e2x + 1, Softsign (x) = x 1 + |.where real vector yi contains the activation of neurons in layer i; Wi \u2212 1, i R | yi \u2212 1 | is the matrix of connections from layer i \u2212 1 to layer i; bi R | yi | is the vector of neuron distortion in layer i; g is an activation function, e.g. sigmoid, tanh, or soft sign (see Figure 2)."}, {"heading": "2.2 Recursive Neural Network", "text": "A recursive neural network (RNN) (Goller and Kuchler, 1996) is an MLN where, given a tree structure, we apply the same weight matrices recursively to each inner node in a bottom-up manner. To see how an RNN works, consider the following example. Suppose there is a component with a parse tree (p2 (p1 x y) z) (Figure 1-right), and that x, y, z Rd are the vector representations of the three words x, y, and z, respectively. We use a neural network consisting of a weight matrix W1 x Rd \u00d7 d for left children and a weight matrix W2 \u00d7 d for right children to calculate the vector for a parent node in bottom-up manners."}, {"heading": "2.3 Recurrent Networks and Long Short-Term Memory", "text": "A neural network is recursive if it has at least one directed ring in its structure. In the field of natural language processing, therefore, Elman's Simple Recursive Neural Network (SRN) (1990) is proposed in 2001 (see Figure 3-left) and its extensions are used to address sequence-related problems, such as machine translation (Sutskever et al., 2014) and voice modeling (Mikolov et al., 2010). In an SRN, an input text is fed into the network at any time. The hidden layer h, which has activation ht \u2212 1 right before the text file arrives, plays a role as memory that captures the entire story (x0, xt \u2212 1). When the hidden layer occurs, it updates its activation byht = g (Wxxt + Whht \u2212 1), where activation is \u2212 1 right before occurrence."}, {"heading": "3 Long Short-Term Memory in RNNs", "text": "In this section, we propose an extension of the LSTM for the RNN model (see Figure 4). A key feature of the RNN is to combine hierarchically information from two children to calculate the parent vector; the idea in this section is to extend the LSTM so that not only the output from each of the children is used, but also the contents of their memory cells. In this way, the network has the ability to store information when the processing of the constituents in the parent tree is low, and to make it available later when the processing constituents are high in the parsetree.For simplicity 2, we assume that the parent node p has two children a and b. Thus, the LSTM at p has two input gates i1, i2 and two forgotten gates f1, f2 for the two children. Computations that occur in this LSTM are: 2Extending our LSTM for trivial trees is alvi.i4x."}, {"heading": "4 LSTM-RNN model for Sentiment", "text": "Analysis 3In this section, we present a model that uses the proposed LSTM for sensitivity analysis. Our model, called LSTM-RNN, is an extension of the traditional RNN model (see Section 2.2), in which the traditional composition function g's in Equations 2-3 is replaced by our proposed LSTM (see Figure 5). Above the node, which includes a phrase / word when its sensory class (e.g. positive, negative or neutral) is available, we insert a Softmax layer (see Equation 1) to calculate the probability of assignment of a class to it. Vector representations of words (i.e. word embedding) can be randomly initialized or pre-trained. The memory of any leaf node w, i.e. cw, is 0.Similar to Irsoy and Cardie (2014), we \"untie\" leaf nodes and inner nodes: We use a single weight matrix for an inner node and another set of leaves."}, {"heading": "4.1 Complexity", "text": "We analyze the complexity of the RNN- and LSTMRNN-models in the run-up phase, i.e. the calculation of vector representations for inner nodes and classification probabilities. The complexity in the return phase, i.e. the calculation of the gradients \u2202 J / \u2202 \u03b8, can be analyzed in a similar manner. We consider only the matrix-vector multiplications that serve to calculate vector representations to the inner node \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "We used the Stanford Sentiment Treebank4 (Socher et al., 2013b), which consists of 5x fine-grained sentiment labels (very negative, negative, neutral, positive, very positive) for 215,154 phrases of 11,855 sentences. There is also the standard division: 8544 sentences for training, 1101 for development and 2210 for testing. The average sentence length is 19.1. In addition, the Treebank also supports binary sentiment classification (positive, negative) by removing neutral labels, resulting in: 6920 sentences for training, 872 for development and 1821 for testing. The rating size is the accuracy given by 100 x # correct # total."}, {"heading": "5.2 LSTM-RNN vs. RNN", "text": "Setting We initialized the word vectors using the 100-D GloVe5 word embedding (Pennington et al., 2014), which was trained on a 6B word corpus. Initial values for a weight matrix were consistently queried from the symmetrical interval [\u2212 1 \u221a n, 1 \u221a n] where n is the number of total input units. 4http: / / nlp.stanford.edu / sentiment / treebank.html 5http: / nlp.stanford.edu / Projects / GloVe / For each model (RNN and LSTM-RNN) we tested three activation functions: softmax, tanh, and softsign, leading to six submodels. Tuning these submodels on the development set, we selected the dimensions of vector representations at the inner node d = 50, learning rate 0.05, regulation parameters f = 10 \u2212 3, and mini-batch size."}, {"heading": "5.3 Compared against other Models", "text": "We compare LSTM-RNN (using tanh) in the previous experiment with existing models: Naive Bayes with bags full of Bigram features (BiNB), Recursive Neural Tensor Network (RNTN) (Socher et al., 2013b), Convolutionary Neural Network (CNN) (Kim, 2014), Dynamic Convolutionary Neural Network (DCNN) (Kalchburner et al., 2014), Sales Vectors (PV) (Le and Mikolov, 2014) and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural network model. RNTN and DRNN are two extensions of RNN. While RNTN, which maintains the structure of the RNN, the two matrix vector multiplication and the sor product no longer make sense for the composition purpose of the network as a deeper DRNN."}, {"heading": "5.4 Toward State-of-the-art with Better Word Embeddings", "text": "We focus on DRNN, which is most similar to LSTM-RNN among these four models CNN, DCNN, PV and DRNN. In fact, from the results reported in Irsoy and Cardie (2014, Table 1a), LSTM-RNN performed on par6 with their 1-layer DRNN (d = 340) using dropout, which is to randomly remove some neurons during training. Dropout is a powerful technique to train neural networks, not only because it plays a role as a strong regulation method to prohibit neurons co-adaptation, but it is also considered as a technique to efficiently form an ensemble of a large number of common neural networks (Srivastava et al., 2014). Thanks to Dropout, Irsoy and Cardie (2014) increased the accuracy of a 3-layer DRNN with d = 200 from 46.06 to 49.5 in the fine-grained model we did not attempt to increase the DRN in the second word accuracy."}, {"heading": "6 Discussion and Conclusion", "text": "We have proposed a new composition method for the recursive neural network (RNN) by extending short-term memory (LSTM), which is widely used in recursive neural network research. 6Irsoy and Cardie (2014) have used the 300-D word2vec word embeddings while we have used the 100-D GloVe word embeddings on a 6B word corpus. From the fact that they have achieved accuracy with an RNN (d = 50) in the fine-grained task and 85.3 in the binary task, and our implementation of RNN (d = 50) we have achieved worse results than the 100-D word embeddings."}, {"heading": "Acknowledgments", "text": "We thank three anonymous reviewers for helpful comments."}], "references": [{"title": "Frege in space: A program for compositional distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "A. Zaenen, B. Webber, and M. Palmer, editors, Linguistic Issues in Language Technologies. CSLI Publications, Stanford, CA.", "citeRegEx": "Baroni et al\\.,? 2013", "shortCiteRegEx": "Baroni et al\\.", "year": 2013}, {"title": "Advances in optimizing recurrent networks", "author": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8624\u20138628. IEEE.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko."], "venue": "Mathematics of control, signals and systems, 2(4):303\u2013314.", "citeRegEx": "Cybenko.,? 1989", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, pages 2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Long short-term memory in recurrent neural networks", "author": ["Felix Gers."], "venue": "Unpublished PhD dissertation, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland.", "citeRegEx": "Gers.,? 2001", "shortCiteRegEx": "Gers.", "year": 2001}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas K\u00fcchler."], "venue": "International Conference on Neural Networks, pages 347\u2013352. IEEE.", "citeRegEx": "Goller and K\u00fcchler.,? 1996", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Alex Graves."], "venue": "Springer.", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber."], "venue": "Kremer and Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press.", "citeRegEx": "Hochreiter et al\\.,? 2001", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems, pages 2096\u20132104.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655\u2013665, Balti-", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.", "citeRegEx": "Le and Zuidema.,? 2014a", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Inside-outside semantics: A framework for neural models of semantic composition", "author": ["Phong Le", "Willem Zuidema."], "venue": "NIPS 2014 Workshop on Deep Learning and Representation Learning.", "citeRegEx": "Le and Zuidema.,? 2014b", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Language models based on semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430\u2013439.", "citeRegEx": "Mitchell and Lapata.,? 2009", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2009}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems, pages 2888\u20132896.", "citeRegEx": "Paulus et al\\.,? 2014", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning representations by backpropagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams."], "venue": "Cognitive modeling, 5.", "citeRegEx": "Rumelhart et al\\.,? 1988", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Advances in Neural Information Processing Systems, 24:801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the 26th International Conference on Machine Learning, volume 2.", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings EMNLP.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos."], "venue": "Proceedings of the IEEE, 78(10):1550\u20131560.", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "arXiv preprint arXiv:1503.04881.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al.", "startOffset": 478, "endOffset": 505}, {"referenceID": 22, "context": "Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al.", "startOffset": 578, "endOffset": 599}, {"referenceID": 0, "context": ", 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013).", "startOffset": 68, "endOffset": 89}, {"referenceID": 2, "context": "The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate any continuous function (Cybenko, 1989) already make them an attractive choice.", "startOffset": 268, "endOffset": 283}, {"referenceID": 26, "context": "In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the con-", "startOffset": 158, "endOffset": 180}, {"referenceID": 12, "context": "volutional neural network model (Kalchbrenner et al., 2014), are even clearer.", "startOffset": 32, "endOffset": 59}, {"referenceID": 24, "context": "Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of", "startOffset": 41, "endOffset": 63}, {"referenceID": 26, "context": "One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014).", "startOffset": 69, "endOffset": 91}, {"referenceID": 11, "context": ", 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014).", "startOffset": 65, "endOffset": 89}, {"referenceID": 11, "context": ", 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al.", "startOffset": 66, "endOffset": 199}, {"referenceID": 11, "context": ", 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis.", "startOffset": 66, "endOffset": 247}, {"referenceID": 10, "context": "Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.", "startOffset": 140, "endOffset": 165}, {"referenceID": 21, "context": "Thanks to the back-propagation algorithm (Rumelhart et al., 1988), the gradient \u2202J/\u2202\u03b8 is efficiently computed; the gradient descent method thus is used to minimize J .", "startOffset": 41, "endOffset": 65}, {"referenceID": 7, "context": "A recursive neural network (RNN) (Goller and K\u00fcchler, 1996) is an MLN where, given a tree structure, we recursively apply the same weight matrices at each inner node in a bottom-up manner.", "startOffset": 33, "endOffset": 59}, {"referenceID": 7, "context": "structure algorithm (Goller and K\u00fcchler, 1996).", "startOffset": 20, "endOffset": 46}, {"referenceID": 25, "context": "The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema,", "startOffset": 134, "endOffset": 156}, {"referenceID": 26, "context": "sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al.", "startOffset": 19, "endOffset": 65}, {"referenceID": 11, "context": "sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al.", "startOffset": 19, "endOffset": 65}, {"referenceID": 23, "context": ", 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)).", "startOffset": 55, "endOffset": 77}, {"referenceID": 16, "context": ", 2011a), semantic role labelling (Le and Zuidema, 2014b)).", "startOffset": 34, "endOffset": 57}, {"referenceID": 4, "context": "In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see", "startOffset": 96, "endOffset": 109}, {"referenceID": 28, "context": "Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al.", "startOffset": 108, "endOffset": 132}, {"referenceID": 17, "context": ", 2014) and language modelling (Mikolov et al., 2010).", "startOffset": 31, "endOffset": 53}, {"referenceID": 10, "context": "In practice, however, training recurrent neural networks with the gradient descent method is challenging because gradients \u2202Jt/\u2202hj (j \u2264 t, Jt is the objective function at time t) vanish quickly after a few back-propagation steps (Hochreiter et al., 2001).", "startOffset": 229, "endOffset": 254}, {"referenceID": 8, "context": "One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM).", "startOffset": 35, "endOffset": 69}, {"referenceID": 5, "context": "One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM).", "startOffset": 85, "endOffset": 97}, {"referenceID": 11, "context": "Similarly to Irsoy and Cardie (2014), we \u2018untie\u2019 leaf nodes and inner nodes: we use one weight matrix set for leaf nodes and another set for inner nodes.", "startOffset": 13, "endOffset": 37}, {"referenceID": 29, "context": "Independently from and concurrently with our work, Tai et al. (2015) and Zhu et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 29, "context": "Independently from and concurrently with our work, Tai et al. (2015) and Zhu et al. (2015) have developed very similar models applying LTSM to RNNs.", "startOffset": 51, "endOffset": 91}, {"referenceID": 7, "context": "Like training an RNN, we use the mini-batch gradient descent method to minimize J , where the gradient \u2202J/\u2202\u03b8 is computed efficiently thanks to the backpropagation through structure (Goller and K\u00fcchler, 1996).", "startOffset": 181, "endOffset": 207}, {"referenceID": 3, "context": "We use the AdaGrad method (Duchi et al., 2011) to automatically update the learning rate for", "startOffset": 26, "endOffset": 46}, {"referenceID": 26, "context": "We used the Stanford Sentiment Treebank4 (Socher et al., 2013b) which consists of 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive) for 215,154 phrases of", "startOffset": 41, "endOffset": 63}, {"referenceID": 20, "context": "Setting We initialized the word vectors by the 100-D GloVe5 word embeddings (Pennington et al., 2014), which were trained on a 6B-word corpus.", "startOffset": 76, "endOffset": 101}, {"referenceID": 6, "context": "The softsign function, which was shown to work better than tanh for deep networks (Glorot and Bengio, 2010), however, did not yield improvements", "startOffset": 82, "endOffset": 107}, {"referenceID": 5, "context": "This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014).", "startOffset": 127, "endOffset": 163}, {"referenceID": 28, "context": "This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014).", "startOffset": 127, "endOffset": 163}, {"referenceID": 26, "context": "We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network Model Fine-grained Binary", "startOffset": 172, "endOffset": 194}, {"referenceID": 13, "context": ", 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network Model Fine-grained Binary", "startOffset": 45, "endOffset": 56}, {"referenceID": 12, "context": "(DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014).", "startOffset": 7, "endOffset": 34}, {"referenceID": 14, "context": ", 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014).", "startOffset": 32, "endOffset": 54}, {"referenceID": 11, "context": ", 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014).", "startOffset": 76, "endOffset": 100}, {"referenceID": 27, "context": "Dropout is a powerful technique to train neural networks, not only because it plays a role as a strong regularization method to prohibit neurons co-adapting, but it is also considered a technique to efficiently make an ensemble of a large number of shared weight neural networks (Srivastava et al., 2014).", "startOffset": 279, "endOffset": 304}, {"referenceID": 11, "context": "Thanks to dropout, Irsoy and Cardie (2014) boosted the accuracy of a 3-layer-DRNN with d = 200 from 46.", "startOffset": 19, "endOffset": 43}, {"referenceID": 23, "context": "auto-encoder (RAE) (Socher et al., 2011a).", "startOffset": 19, "endOffset": 41}, {"referenceID": 23, "context": "Because pre-training an RNN as an RAE can boost the overall performance (Socher et al., 2011a; Socher et al., 2011c), seeing LSTM as a compressor might explain why the LSTM-RNN worked better than RNN with-", "startOffset": 72, "endOffset": 116}, {"referenceID": 11, "context": "Comparing LSTM-RNN against DRNN (Irsoy and Cardie, 2014) gives us a hint about how to improve our model.", "startOffset": 32, "endOffset": 56}], "year": 2015, "abstractText": "We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the \u2018memory cell\u2019) and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank.", "creator": "TeX"}}}