{"id": "1503.02521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "A Single-Pass Classifier for Categorical Data", "abstract": "This paper describes a new method for classifying a dataset that partitions elements into different categories. It has relations with neural networks but works in a different way, requiring only a single pass through the classifier to generate the weight sets. A grid structure is required and a novel idea of converting a row of real values into a 2-D or grid-like structure of value bands. Each cell in the band can then store a cell weight value and also a set of weights that represent its own importance to each of the output categories. For any input that needs to be categorised, all of the output weight lists for each relevant input cell can be retrieved and summed to produce a probability for what the correct output category is. So the relative importance of each input point to the output is distributed to each cell. The construction process itself can simply be the reinforcement of the weight values, without requiring an iterative adjustment process, making it potentially much faster.", "histories": [["v1", "Mon, 9 Mar 2015 15:28:32 GMT  (416kb)", "http://arxiv.org/abs/1503.02521v1", null], ["v2", "Tue, 14 Apr 2015 16:32:44 GMT  (416kb)", "http://arxiv.org/abs/1503.02521v2", "Small correction in the test results. Small amount of new information"], ["v3", "Wed, 14 Oct 2015 18:06:44 GMT  (471kb)", "http://arxiv.org/abs/1503.02521v3", "New test results and additional information"], ["v4", "Wed, 29 Jun 2016 10:40:50 GMT  (499kb)", "http://arxiv.org/abs/1503.02521v4", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kieran greer"], "accepted": false, "id": "1503.02521"}, "pdf": {"name": "1503.02521.pdf", "metadata": {"source": "CRF", "title": "A Single-Pass Classifier for Categorical Data", "authors": ["Kieran Greer"], "emails": [], "sections": [{"heading": null, "text": "It requires a grid structure and a novel idea to convert a set of real values into a 2D or raster-like structure of value bands. Each cell in the cell can then store a cell weight value and also a set of weights that represent their own meaning for each of the output categories. For each input that needs to be categorized, all output weight lists for each relevant input cell can be retrieved and summarized to create a probability of what the right output category is. Thus, the relative meaning of each input point for output is distributed across each cell. The construction process itself can simply be the amplification of the weight values without requiring an iterative adjustment process, potentially making it much quicker. Keywords: classifier, network architecture, deconstructed data."}, {"heading": "1 Introduction", "text": "This paper describes a new method of classifying a dataset that divides elements into different categories. It has relationships to neural networks, but operates in a different way, requiring only a single pass through the classifier to generate the weights. It requires a grid structure and a novel idea for converting a set of real values into a 2-D or grid-like structure of value bands. Each cell in this band can then store a cell weight value and also a set of weights that represent their own meaning for each of the output categories. For each input that needs to be categorized, all output weight lists for each relevant input cell can be retrieved and sumed to generate a probability of what the right output category is. So, the relative meaning of each input point is distributed to each cell. The construction process itself can simply be the amplification of the weights, without requiring an iterative adjustment process, which potentially makes it much faster."}, {"heading": "2 Related Work", "text": "The classifier is not really a neural network, because it does not have a neural architecture, but a fixed lattice-like system. However, it has been somewhat inspired by the topology feature of the Self-Organising Map [6] [7]. Not many neural networks have this feature and it is more visual than what is normal [10]. Topology can represent the input data as a 2-D array, allowing the SOM, for example, to learn handwritten characters. Another example could be [4] (or the associated neognitron). There is also little influence of biological / genetic classifiers or perhaps a recent and controversial paper [1] [8] suggesting that neurons within the brain store a memory of which synapse they should form. Previously, this information was thought to be in the synapses themselves."}, {"heading": "3 New Classifier Architecture", "text": "The decision to use a topology, or in reality simply a grid structure, was an attempt to separate the input values, which would allow them to be more different in their relationship to other value sets. Adding another dimension here could help simplify the learning process. It also became clear that a typical set of real inputs cannot simply be mapped to a topology that is not also a single line of nodes. Therefore, to map a single row of data values to a 2-D grid, it requires the introduction of bands so that each value range can be represented by a separate band. It is a bit like mapping an analog value to a set of binary ones. If each value range is replaced by a series of graded bands, then the values can be placed roughly in the right band and placed in a unique position by other values for the same data point. This deconstruction process would give it unique properties when linked to other data points."}, {"heading": "3.1 Classifier and Data Row Structure", "text": "The classifier and the data row share the same basic structure as shown in Figure 1, where all data rows are combined in the classifier. Each data row is now represented by a grid-like structure, or is now 2-D, not 1-D. So an additional dimension has been added, but the input is then reduced to a more binary form. In the classifier, each data row is added one after the other, simply increasing each cell or output category to which it belongs. As these value adjustments all overlap, these combined value sets can, for example, provide the generating properties of a neural network. A big problem with existing classifiers is whether the data is linearly separable - that is, you can split the categories by a straight line. If the data cannot be linearly separable, more complex transformation functions are often required. This classifier might not have this problem because the linear construct itself is located and deconstructed, or perhaps the separation level itself is broken down by the type of the values calculated."}, {"heading": "4 Example Scenario", "text": "Figure 1 is a scheme that shows what a data row and the classifier itself might look like, and a calculation from it. The upper left side of the figure shows two input lines for training data sets, in which there are only two categories to classify. Data row 1 belongs to category 2 and data row 2 to category 1. Data can initially be normalized, making it easier to divide them into bands. For example, if the data is normalized to be in the range 0 to 1, it is easy to create 10 bands of size 0.1. The bottom half of the scheme shows the classifier itself, using the same color to represent the two rows of data in each of the associated cells. Moreover, each cell stores its own relationship to the output categories created here, but shows the expected weight distortion. Looking at these sums, a new or test data row is presented, which is displayed in cells 1-4, 2-2, 3-4, 4-1. This is displayed in the upper right corner with the weighted values."}, {"heading": "4.1 Construction Process", "text": "For each training data row that is presented, the corresponding cell is calculated for each data point, the weight for that point is then increased by a certain amount. This could be, for example, \"1.0 divided by the number of data rows.\" The output field for the cell is retrieved and the element that corresponds to the correct category is also incremented. If the record is not well balanced in terms of the number of rows in each category, this skew affects the resulting classification. Therefore, it is best if the training record has the same number of training records for each category type. If this is the case, there should be no distortion in any subsequent category selection. If the number of rows for each category is unbalanced, each output category can be increased by a different amount instead, for example: \"1.0 divided by the number of rows in the category.\" This then results in balanced output weights after the training."}, {"heading": "5 Test Results", "text": "A test program written in the C #.Net language can read a data file, normalize it, generate the classifier from it, and measure how many line categories it correctly grades one after the other. The classifier was tested using 3 arbitrarily selected data sets from the UCI Machine Learning Repository [9]. These were the Zoo database [11], the Iris Plants database [2], and the Wine Recognition database [3]. All data sets were assigned output categories, as described at the end of Section 4.1, but it really was only the Zoo data set that was too unbalanced. These data sets were selected earlier to test self-explanatory classifiers, but they contain output categories and are therefore suitable for testing the new classifier. For each data set, the input data was first normalized, and then each line was re-classified to the classifier with the corresponding output category, and the corresponding cell values were re-classified after that."}, {"heading": "6 Conclusions", "text": "This paper describes a new type of classifier that can be trained very quickly and is also very precise. The function used in this version is linear, but it is obviously not one of the known types of neural networks - for example associative or SOM. Its strength could lie in the direct but also deconstructed mapping between each cell and the initial categories, without complex transformations. This new classifier could also have an advantage when it comes to linearly separable or inseparable data sets. The classifier could also be interesting as part of a biological model."}, {"heading": "7 References", "text": "[1] Chen, S., Cai, D., Pearce, K., Sun, P.Y-W, Roberts, A.C. and Glanzman, D.L. (2014). Reinstatementof long-term memory after erasure of its behavioral and synaptic expression in Aplysia, eLife 2014; 3: e03896, pp. 1 - 21. DOI: 10.7554 / eLife.03896. [2] Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems, Annual Eugenics, 7, Part II, 179-188, also in 'Contributions to Mathematical Statistics' (John Wiley, NY, 1950). [3] Forina, M. et al. (1991). PARVUS - An Extendible Package for Data Exploration, 4ysse."}], "references": [{"title": "Reinstatement of long-term memory following erasure of its behavioral and synaptic expression in Aplysia, eLife 2014;3:e03896, pp. 1 - 21. DOI: 10.7554/eLife.03896", "author": ["S. Chen", "D. Cai", "K. Pearce", "Sun", "P.Y-W", "A.C. Roberts", "D.L. Glanzman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The use of multiple measurements in taxonomic problems, Annual Eugenics, 7, Part II, 179-188, also in 'Contributions to Mathematical Statistics", "author": ["Fisher", "R.A"], "venue": "(John Wiley,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1950}, {"title": "PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies", "author": ["M Forina"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "A Neural Network for Visual Pattern Recognition", "author": ["K. Fukishima"], "venue": "IEEE Computer, 21(3),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "A Stochastic Hyper-Heuristic for Matching Partial Information", "author": ["K. Greer"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The self-organizing map", "author": ["T. Kohonen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Neural Networks: A Systematic Introduction. Springer-Verlag, Berlin and online at books.google.com", "author": ["R. Rojas"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "It was however inspired in a small way by the topology feature of the Self-Organising Map [6][7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "It was however inspired in a small way by the topology feature of the Self-Organising Map [6][7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "Another example might be [4] (or the related Neocognitron).", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "There is also a small amount of influence from the biological/genetic classifiers, or maybe a recent and controversial paper [1][8] which suggests that neurons inside the brain store a memory of what synapses they should form.", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "The new classifier also uses a direct association approach, rather like an associative network [7], but it is probably not the same.", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "A grid-like structure was also used in [5] to try to represent bits of a problem or solution, but again using a different type of system.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "These were the Zoo database [11], Iris Plants database [2] and the Wine Recognition database [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "These were the Zoo database [11], Iris Plants database [2] and the Wine Recognition database [3].", "startOffset": 93, "endOffset": 96}], "year": 2015, "abstractText": "This paper describes a new method for classifying a dataset that partitions elements into different categories. It has relations with neural networks but works in a different way, requiring only a single pass through the classifier to generate the weight sets. A grid structure is required and a novel idea of converting a row of real values into a 2-D or grid-like structure of value bands. Each cell in the band can then store a cell weight value and also a set of weights that represent its own importance to each of the output categories. For any input that needs to be categorised, all of the output weight lists for each relevant input cell can be retrieved and summed to produce a probability for what the correct output category is. So the relative importance of each input point to the output is distributed to each cell. The construction process itself can simply be the reinforcement of the weight values, without requiring an iterative adjustment process, making it potentially much faster.", "creator": "Microsoft\u00ae Word 2010"}}}