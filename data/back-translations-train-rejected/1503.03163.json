{"id": "1503.03163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder", "abstract": "We propose a method for using synthetic data to help learning classifiers. Synthetic data, even is generated based on real data, normally results in a shift from the distribution of real data in feature space. To bridge the gap between the real and synthetic data, and jointly learn from synthetic and real data, this paper proposes a Multichannel Autoencoder(MCAE). We show that by suing MCAE, it is possible to learn a better feature representation for classification. To evaluate the proposed approach, we conduct experiments on two types of datasets. Experimental results on two datasets validate the efficiency of our MCAE model and our methodology of generating synthetic data.", "histories": [["v1", "Wed, 11 Mar 2015 03:31:53 GMT  (1746kb,D)", "http://arxiv.org/abs/1503.03163v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["xi zhang", "yanwei fu", "i zang", "leonid sigal", "gady agam"], "accepted": false, "id": "1503.03163"}, "pdf": {"name": "1503.03163.pdf", "metadata": {"source": "CRF", "title": "Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder", "authors": ["Xi Zhang", "Yanwei Fu", "Andi Zang", "Leonid Sigal", "Gady Agam"], "emails": ["xzhang22@hawk.iit.edu,", "zang@hawk.iit.edu,", "agam@iit.edu.", "lsigal}@disneyresearch.com"], "sections": [{"heading": null, "text": ""}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2 RELATED WORK", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "3 MULTICHANNEL AUTOENCODER (MCAE)", "text": "In this section, we present the MCAE model as shown in Figure 3. It can (1) bridge synthetic gaps by minimizing the discrepancy between real and synthetic data; and (2) preserve and emphasize the potentially useful patterns that exist in both real and synthetic data to generate better feature representations that are used for learning classifiers. Synthetic and real data should have similar patterns, a natural idea for bridging synthetic gaps is to learn a mapping from synthetic data to real data using an auto encoder and vice versa. MCAE therefore provides a more flexible way to learn this mapping based on the specific structure of the MCAE. There are two channels in MCAE, one left and one right. Each channel is basically an SAE, but two channels share the same hidden layer. With this structure, MCE learns to produce this mapping based on the specific structure of the MCAE data. There are two input channels in MCAE, one left and one right. Each channel is basically an SAE, but two channels share the same hidden layer."}, {"heading": "3.1 Problem setup", "text": "Our MCAE is based on the sparse autoencoder (SAE). < W = basic autoencoder is a fully connected neural network with a hidden layer and can be split into two parts: an encoding and a decoding process. < W = real data set with n instances X = {xi} ni = 1, where xi \u00b2 Rm and m is the dimension of each instance. Encoding typically transforms input data into hidden layer representation using an affine mapping squeezed out by a sigmoid function: he (xi) = f (Wexi + be) (1), where f (\u00b7) is a sigmoid function and an analog representation of the hidden layer. We can use Rk \u00b7 m, a set of unknown parameters in the coding with k nodes in hidden layers."}, {"heading": "3.2 MCAE model", "text": "We propose a multi-channel autoencoder that uses balance adjustment to use learning between two tasks, i.e. < i: Xs, t: Xr > L and < i: Xr, t: Xr > R. The structure of this new autoencoder is shown in Fig. 3. In this new structure we divide the autoencoder into two separate channels that two tasks will have their own parameters. Splitting the autoencoder into two channels at the decoding layer allows for more flexible control between the two tasks. However, we divide the autoencoder into two separate channels that two tasks will have their own parameters. Splitting the autoencoder into two channels at the decoding layer allows for more flexible control between the two tasks, allowing the autoencoder to better exploit the shared knowledge from the two tasks."}, {"heading": "3.3 The advantages of MCAE over the alternative Configurations", "text": "Our MCAE forces the autoencoder to learn useful class patterns from the two tasks simultaneously, thus helping to capture a common structure of synthetic and real images. Another option is to concatenate the input and target of the two tasks < i: XsXr, t: XrXr > for the autoencoder. We comment on the use of this autoencoder as a concatenate-input autoencoder (CIAE), as this autoencoder learns concatenated tasks at the same time. However, such configurations can lead to an unbalanced optimization of these two tasks: the optimization process of one task takes over the process of the other, resulting in a distorted reconstructed hidden layer of the autoencoder and thus limited classification performance. Our experiments also confirm this point in paragraph 5."}, {"heading": "4 GENERATING SYNTHETIC DATA", "text": "This section discusses the methodology of synthetic data generation used in our experiments. Such synthetic data have some similarities and differences with the augmented data used in deep learning, for example [21]. Both synthetic data and augmented data aim to improve the generalization capacity of classifiers. Nevertheless, the methodology of synthetic data generation produces more deformed patterns than the simple labeling transformations used in data augmentation. Synthetic data is created to highlight the potentially useful pattern in real images. We have two stages of synthetic data generation that are used in the first stage to train MCAE. A synthetic version that best matches the appearance of the real data with the real data is generated, so pairs of more responsive data can be used to train the reactive ME."}, {"heading": "4.1 Explicitly Design of the Synthetic Prototype", "text": "The generation of the synthetic prototype and the control points in this scenario is inspired by the approach proposed by Zhang et al. [49]. With sufficient prior knowledge of the 3D objects, a synthetic prototype of 3D objects is explicitly designed and built. By adapting the control points of the prototype, various types of 3D objects are created. In this work, our data is essentially very similar to theirs in that roof images have many features such as crest lines, talcum lines and intersections between these lines, which make it possible to manually design the synthetic prototypes that characterize these patterns. On the basis of this observation, a synthetic roof prototype could be created by placing the control points at the intersections of the crest or talcum lines and drawing segments that connect these control points. 2.In this scenario, the OptimizeControlPoints (U, V, S) function of Alg. 1 is used as a process to search for optimal control points that lead to a synthetic image reproduction."}, {"heading": "4.2 Learning Synthetic Prototype from Data", "text": "In hand written digit dataset used in this work, we learn a synthetic target from given data. A digit prototype is generated for all images with the same digit. Congealing algorithm proposed in [27] is used in this step to produce the synthetic prototypes for digits. In congealing, the project transformations are applied to images to minimize a joint entropy. Thus, the Prototype2. In our experiments, the classification of roof images is essentially similar to that of [49]. Our approach recognizes the style of roofs based on edges extracted from the roof pictures. For further visualization results, we refer to our supplementary material. Algorithm 2 OptimizeControlPoints (U, V, S) Case 1 Input: \u2022 A real image U: \u2022 A prototype of the synthetic image S = {P, E}."}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "We validate the proposed MCAE dataset for multiple applications in this section. This dataset is organized as follows: First, in Sec 5.1 we introduce a new benchmark dataset - Satellite Roof Classification (SRC) dataset to the Vision Community. This dataset is of high Satellite6 algorithm 3 OptimizeControlPoints (U, V, S) case 2 input: \u2022 A real picture U. \u2022 A prototype of the synthetic image S = {P, E}. \u2022 A synthetic image V.1: steps = 10. 2: Distance calculation transforms the image of U, V as U \u2032, V \u2032. 3: for i = 1 to do steps 4: I = (1 \u2212 isteps) U \u2032 + i steps V \u2032.5: I = Binary (I). 6: Update S by clicking on the tightest limit pixel at I. 7: End for 8: Set the status of S to be merged."}, {"heading": "5.1 Experiment Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Satellite Roof Classification (SRC) Dataset", "text": "A particularly interesting problem is the analysis of satellite images of the Earth. Such problems require very high quality (at expert level). However, there are no previous data sets for such research purposes. To facilitate the study, a new benchmark satellite image (SRC) will be created and used in our experiments. In view of a satellite image, we will use a method that is able to register the labels of the roofs with the satellite image. Later, all roofs will be aligned with their main directions."}, {"heading": "5.1.2 Handwritten Digits Dataset", "text": "The handwritten digits from 0 to 9 in this data set are from 43 persons: 30 contributed to the training set and the other 13 to the test set. In the experiments, the Syn-I data are generated using algorithm 3. The Syn-II data of this data set are generated by interpolation and extrapolation as described in paragraph 4."}, {"heading": "5.2 Experimental Settings", "text": "In fact, it is so that most of us are able to abide by the rules that they have imposed on themselves. (...) It is not so that they are able to understand the rules. (...) It is not so that they abide by the rules. (...) It is not so that they abide by the rules. (...) It is as if they abide by the rules. (...) It is not as if they abide by the rules. (...) It is not as if they abide by the rules. (...) It is as if they abide by the rules. (...) It is as if they abide by the rules. (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.) \"(.\" (.). \"(.\" (.). \"(.\" (.) \"(.\" (.). \"(.\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (. \"(.).\" (. \").\" (.). \"(.).\" (.). \"(.).\" (. \"(.).).\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (. \"(.\" (.).). \"(.).\" (.). \"(.\" (.). \"(.).\" (.). (.). \"(.\" (. \"(.).).\" (.).). \"(.).\" (. \"(.).\" (.). \"(.).\" (. \"(.).\" (.).)."}, {"heading": "6 CONCLUSION", "text": "To better learn classifiers from synthetic data, we have proposed a novel Multichannel Autoencoder (MCAE) model. MCAE has multiple channels in its structure and is an extension of the standard Autoencoder. We show that MCAE not only bridges the synthetic gap between real and synthetic data, but also collectively learns from real and synthetic data, providing a more robust representation for both data. To facilitate the investigation of satellite image analysis, we are introducing a new benchmark dataset - SRC as a dataset used in our experiments. The proposed method has been validated on SRC and handwritten datasets."}, {"heading": "6.1 Optimization of MCAE", "text": "With two branches in the MCAE, our goal is to minimize the reconstruction error of two tasks together, taking into account the balance between two branchs.10The new objective function of the YMAE is given as follows: E = JL (jzz \u2212 jzz)) 2 (10) is a regulation added to balance the learning rate between two branchs.This regulation will have two effects on the YMAE. Firstly, it accelerates the speed of optimization of Eq. \u2212 jR (jzz \u2212 jzz) 2 (10) is a regulation added to balance the learning rate between two branches."}, {"heading": "7 GENERATING SYNTHETIC DATA", "text": "An example in Fig. 9 shows how control points are moved from the source image (the one with a blue border) to the target image (the one with a red border). It could be observed that most control points are moved from the source image to the corresponding locations in the target image. In this step, it is not necessary for all control points to move exactly to the exact corresponding location in the target image. Our goal is simply to use these migrated control points to generate synthetic data that roughly mimic the real data. Our MCAE will later correct the difference between synthetic data and real data."}, {"heading": "7.1 Further Validation for MCAE", "text": "Note that applying a sparse auto encoder directly to our problem does not work well. For example, we can train an auto encoder purely by putting synthetic data into the input layer and real data into the output layer, but these cannot bridge the synthetic gap in our problem. Such a reconstruction is only to supplement the missing information in synthetic data from real data. On the contrary, reconstructed real data using such SAE will add unnecessary information and noisy patterns to reconstructed data. To confirm this point, we expand the experiments of two sets of data and show that SAE cannot bridge the gap in the synthetic gap. Results are shown in Fig. 10. The reconstructed data from SAE show lower average divergences than the other methods, which means that SAE performs worse than MCAE at bridging the synthetic gap."}], "references": [{"title": "Interactive degraded document enhancement and ground truth generation", "author": ["G. Bal", "G. Agam", "O. Frieder", "G. Frieder"], "venue": "Electronic Imaging 2008. International Society for Optics and Photonics,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["P. Baldi"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, page 43,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Mach. Learn., 79(1-2):151\u2013175, May", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Y. Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, page 19,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance transforms in digital images", "author": ["G. Borgefors"], "venue": "Computer Vision, Graphics, and Image Processing, volume 34, pages 344\u2013371. Elsevier,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1986}, {"title": "Smote: Synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of Artificial Intelligence Research, 16:321\u2013357,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "Fei"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hierarchical ranking of facial attributes", "author": ["A. Datta", "R. Feris", "D. Vaquero"], "venue": "IEEE International Conference on Automatic Face & Gesture Recognition,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse autoencoderbased feature transfer learning for speech emotion recognition", "author": ["J. Deng", "Z. Zhang", "E. Marchi", "B. Schuller"], "venue": "Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on, pages 511\u2013516. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "High level describable attributes for predicting aesthetics and interestingness", "author": ["S. Dhar", "V. Ordonez", "T.L. Berg"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Attribute learning for understanding unstructured social activity", "author": ["Y. Fu", "T. Hospedales", "T. Xiang", "S. Gong"], "venue": "European Conference on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Transductive multi-view embedding for zero-shot recognition and annotation", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "Z. Fu", "S. Gong"], "venue": "European Conference on Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multimodal latent attributes", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive multi-label zero-shot learning", "author": ["Y. Fu", "Y. Yang", "T. Hospedales", "T. Xiang", "S. Gong"], "venue": "British Machine Vision Conference,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Borderline-smote: a new over-sampling method in imbalanced data sets learning", "author": ["H. Han", "W.-Y. Wang", "B.-H. Mao"], "venue": "Advances in intelligent computing, pages 878\u2013887. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Adasyn: Adaptive synthetic sampling approach for imbalanced learning", "author": ["H. He", "Y. Bai", "E.A. Garcia", "S. Li"], "venue": "Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, pages 1322\u2013 1328. IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Recognizing human actions by attributes", "author": ["J. Liu", "B. Kuipers", "S. Savarese"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble of exemplarsvms for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A.A. Efros"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning from one example through shared densities on transforms", "author": ["E.G. Miller", "N.E. Matsakis", "P.A. Viola"], "venue": "Computer Vision and Pattern Recognition, 2000. Proceedings. IEEE Conference on, volume 1, pages 464\u2013471. IEEE,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "A threshold selection method from gray level histograms", "author": ["N. OTSU"], "venue": "IEEE Trans. Syst. Man Cybern., 9(1):62\u201366,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1979}, {"title": "Zeroshot learning with semantic output codes", "author": ["M. Palatucci", "G. Hinton", "D. Pomerleau", "T.M. Mitchell"], "venue": "Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Data and Knowledge Engineering, 22(10):1345\u20131359,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Remote Sensing Digital Image Analysis \u2013 An introduction", "author": ["J.A. Richards"], "venue": "Springer Berlin Heidelberg,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "What helps where \u2013 and why? semantic relatedness for knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "I. Gurevych", "B. Schiele"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 910\u2013917,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to share visual appearance for multiclass object detection", "author": ["R. Salakhutdinov", "A. Torralba", "J. Tenenbaum"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining subclassifiers in text categorization: A dst-based solution and a case study", "author": ["K. Sarinnapakorn", "M. Kubat"], "venue": "IEEE Trans. on Knowl. and Data Eng., 19(12):1638\u20131651, Dec.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "H. Sridhar", "O. Bastani", "C.D. Manning", "A.Y. Ng"], "venue": "Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning To Learn: Introduction", "author": ["S. Thrun"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1996}, {"title": "Visualizing high-dimensional data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Effects of training set expansion in handwriting recognition using synthetic data", "author": ["T. Varga", "H. Bunke"], "venue": "In 11th Conf. of the International Graphonomics Society. Citeseer,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Comparing natural and synthetic training data for off-line cursive handwriting recognition", "author": ["T. Varga", "H. Bunke"], "venue": "Frontiers in Handwriting Recognition, 2004. IWFHR-9 2004. Ninth International Workshop on, pages 221\u2013225. IEEE,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, 11:3371\u20133408,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1113\u20131120, New York, NY, USA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L.J. Guibas", "L. Fei- Fei"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Designing category-level attributes for discriminative visual recognition", "author": ["F.X. Yu", "L. Cao", "R.S. Feris", "J.R. Smith", "S.-F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "A learning-based approach for automated quality assessment of computer-rendered images", "author": ["X. Zhang", "G. Agam"], "venue": "IS&T/SPIE Electronic Imaging. International Society for Optics and Photonics,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Alignment of 3d building models with satellite images using extended chamfer matching", "author": ["X. Zhang", "G. Agam", "X. Chen"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from synthetic models for roof style classification in point clouds", "author": ["X. Zhang", "A. Zang", "G. Agam", "X. Chen"], "venue": "Proceedings of the 22nd ACM SIGSPATIAL international conference on Advances in geographic information systems. ACM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and extensible building modeling from airborne lidar data", "author": ["Q.-Y. Zhou", "U. Neumann"], "venue": "Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems, page 7. ACM,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 33, "context": "In terms of quantity, it has been shown that the amount of available training data, per object class, roughly follows a Zipf distribution [35].", "startOffset": 138, "endOffset": 142}, {"referenceID": 30, "context": "the comet images from Rosetta), require extensive and detailed expert user annotation [32], [48].", "startOffset": 86, "endOffset": 90}, {"referenceID": 46, "context": "the comet images from Rosetta), require extensive and detailed expert user annotation [32], [48].", "startOffset": 92, "endOffset": 96}, {"referenceID": 47, "context": "Large volume of LiDAR point cloud data have to be labeled before they can be used to train some classifiers [49].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "To solve the problem of lacking enough training samples, attributes [22], [30], [13] have been introduced to transfer the knowledge held by majority classes to instances in minority classes.", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "To solve the problem of lacking enough training samples, attributes [22], [30], [13] have been introduced to transfer the knowledge held by majority classes to instances in minority classes.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "To solve the problem of lacking enough training samples, attributes [22], [30], [13] have been introduced to transfer the knowledge held by majority classes to instances in minority classes.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 74, "endOffset": 78}, {"referenceID": 44, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "In contrast, rather than using such a \u2018learning to learn\u2019 [38] framework, humans can generalize and associate the similar patterns from images.", "startOffset": 58, "endOffset": 62}, {"referenceID": 24, "context": "In the computing domain, exemplar SVM [26] tries to associate images with training exemplars.", "startOffset": 38, "endOffset": 42}, {"referenceID": 45, "context": "A large number of synthetic 3D meshes in [47] were created by a series of mesh editing steps including subdivision, simplification, smooth, adding noise and Poisson reconstruction, in order to automatically evaluate the subjective visual quality of a 3D object.", "startOffset": 41, "endOffset": 45}, {"referenceID": 47, "context": "Recently, to circumvent the point labeling difficulty in a building roof classification problem using LiDAR point cloud, [49] explicitly indicated semantic roof points on synthetically created roof point clouds and compute point features from the synthetic point clouds.", "startOffset": 121, "endOffset": 125}, {"referenceID": 38, "context": "Previous method generate synthetic data in data space using tools including geometrical transformation and degradation models: In [40][41], to help off-line recognition of handwritten text, a perturbation model combined with morphological operation is applied to real data.", "startOffset": 130, "endOffset": 134}, {"referenceID": 39, "context": "Previous method generate synthetic data in data space using tools including geometrical transformation and degradation models: In [40][41], to help off-line recognition of handwritten text, a perturbation model combined with morphological operation is applied to real data.", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "To enhance the quality of degraded document, in [2] degradation models such as brightness degradation, blurring degradation, noise degradation and textureblending degradation were used to create a training dataset for a handwritten text recognition problem.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "The synthetic minority oversampling technique (SMOTE) [8] and its variants [19][20] are also powerful methods that have shown many success in various applications.", "startOffset": 54, "endOffset": 57}, {"referenceID": 17, "context": "The synthetic minority oversampling technique (SMOTE) [8] and its variants [19][20] are also powerful methods that have shown many success in various applications.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "The synthetic minority oversampling technique (SMOTE) [8] and its variants [19][20] are also powerful methods that have shown many success in various applications.", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 106, "endOffset": 109}, {"referenceID": 34, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 192, "endOffset": 196}, {"referenceID": 20, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 204, "endOffset": 208}, {"referenceID": 43, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 210, "endOffset": 214}, {"referenceID": 13, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 216, "endOffset": 220}, {"referenceID": 15, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 222, "endOffset": 226}, {"referenceID": 31, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 228, "endOffset": 232}, {"referenceID": 32, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 234, "endOffset": 238}, {"referenceID": 35, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 240, "endOffset": 244}, {"referenceID": 29, "context": "Transfer learning is categorized to three classes [31]: inductive transfer learning, transductive transfer learning and unsupervised transfer learning.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "The work in this paper falls into a framework of domain adaptation [4], [44] in the transductive transfer learning.", "startOffset": 67, "endOffset": 70}, {"referenceID": 42, "context": "The work in this paper falls into a framework of domain adaptation [4], [44] in the transductive transfer learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": "input vectors [42].", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 49, "endOffset": 52}, {"referenceID": 16, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 177, "endOffset": 180}, {"referenceID": 9, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 182, "endOffset": 186}, {"referenceID": 41, "context": "By setting different types of input and out data such as the one in denoising autoencoder [43], MCAE is capable for many applications.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Congealing algorithm proposed in [27] is employed in this step to produce the synthetic prototypes for digits.", "startOffset": 33, "endOffset": 37}, {"referenceID": 47, "context": "In our experiments, classification of the roof images is essentially similar to that of [49].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "To generate the intermediate images, we binarize all the images and the distance transformed images[7] of the synthetic prototype and the real image are generated.", "startOffset": 99, "endOffset": 102}, {"referenceID": 46, "context": "Given a satellite image, we employ a method described in [48] to crop roof images by registering artificial building footprints with the satellite image.", "startOffset": 57, "endOffset": 61}, {"referenceID": 48, "context": "Later, all roof images are aligned using their footprint principal directions using a method proposed in [50] and then are scaled to images with resolution of 128\u00d7256.", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "We employed the adaptive Otsu edge detection method [29] to extract edges from the roof images.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "In our experiments we build a LeNet-5 [24] which is originally created for digit recognition.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "To better evaluate the performance of the proposed MCAE, we compare MCAE with Concatenate-Input Autoencoder (CIAE) [28] and Sparse Autoencoder (SAE) [43].", "startOffset": 115, "endOffset": 119}, {"referenceID": 41, "context": "To better evaluate the performance of the proposed MCAE, we compare MCAE with Concatenate-Input Autoencoder (CIAE) [28] and Sparse Autoencoder (SAE) [43].", "startOffset": 149, "endOffset": 153}, {"referenceID": 37, "context": "t-SNE [39]visualization of synthetic gap bridged by MCAE.", "startOffset": 6, "endOffset": 10}], "year": 2015, "abstractText": "We propose a method for using synthetic data to help learning classifiers. Synthetic data, even is generated based on real data, normally results in a shift from the distribution of real data in feature space. To bridge the gap between the real and synthetic data, and jointly learn from synthetic and real data, this paper proposes a Multichannel Autoencoder(MCAE). We show that by suing MCAE, it is possible to learn a better feature representation for classification. To evaluate the proposed approach, we conduct experiments on two types of datasets. Experimental results on two datasets validate the efficiency of our MCAE model and our methodology of generating synthetic data.", "creator": "LaTeX with hyperref package"}}}