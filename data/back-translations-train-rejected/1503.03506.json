{"id": "1503.03506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning", "abstract": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystr\\\"om method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques.", "histories": [["v1", "Wed, 11 Mar 2015 21:09:28 GMT  (3498kb,D)", "http://arxiv.org/abs/1503.03506v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["christian wachinger", "polina golland"], "accepted": false, "id": "1503.03506"}, "pdf": {"name": "1503.03506.pdf", "metadata": {"source": "CRF", "title": "Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning", "authors": ["Christian Wachinger", "Polina Golland"], "emails": [], "sections": [{"heading": null, "text": "A common strategy for overcoming this problem is to reduce the dimensionality of selected landmarks and gradually embed the entire dataset in the Nystro-m Method. The two main challenges arising from this are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error; (ii) the graph constructed from sparsely sampled landmarks must approach the multiplicity well. We propose sampling landmarks from certain distributions in non-Euclidean spaces. As the current determinant sampling algorithms have the same complexity as those for diverse learning, we present an efficient approximation that takes place in linear time. Furthermore, we recover the local geometry after spinning by assigning each landmark a local covariance matrix estimated from the original point."}, {"heading": "1 Introduction", "text": "In fact, it is not the case that this is a way in which people on one side of the Atlantic and people on the other side of the Atlantic move in a way that they do on the other side of the Atlantic, on the other side of the Atlantic, on the other side of the Atlantic, on the other side of the Atlantic, on the other side of the Atlantic, on the other side of the Atlantic, and on one side of the Atlantic, on the other side of the Atlantic, on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic, and on the other side of the Atlantic."}, {"heading": "2 Background", "text": "We assume n points in high-dimensional space x1,.., xn, Rd and let X, Rd x n be the matrix whose i-th column is the dot xi. Techniques for reducing nonlinear dimensionality are based on a positive semidefinitive kernel K with a typical choice of Gaussian or heat core Ki, j = exp. The resulting kernel matrix is of size O (n2). Necessary for spectral analysis is the proper decomposition of the kernel matrix, which has complexity O (n3). For most techniques, it is only necessary to calculate the leading k eigenvectors. Therefore, the problem can also be seen as a search for the best rank-k approximation of the matrix K, with the optimal solution Kk = 3, where Ciuiu > i is the i-th greatest eigenvector and ui is the corresponding eigenvector."}, {"heading": "2.1 Nystro\u0308m Method", "text": "Suppose J {1,.., n} is a subset of the original set of K-K points, and J-K is its complement. We can arrange the core matrix K such that K = [KJ \u00b7 J KJ \u00b7 J-J-K-K > J \u00b7 J-KJ-KJ-KJ-J-J-J-J-K-K = [KJ \u00b7 J KJ \u00b7 J-J-K-K-J-J-J-J-J-J-J-J-K-K-J-K-J-K-K-J-K-K-K-K-J-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-"}, {"heading": "2.2 Annealed Determinantal Sampling", "text": "For general matrix approximation, this step is called row / column selection of matrix K, which is equivalent to selecting a subset of points X. This property is important because it avoids explicit calculation of the O (n2) entries in the kernel matrix K. We focus on the volume collection for the subset selection K \u2212 i \u2212 J) of the points selected by the origin and the selected points KJ \u00b7 J = Y > J YJ, which exists because KJ is positively semi-defined. Based on this factorization, the volume Vol (Yi) i \u2212 J of the single matrix is spanned and the selected points YJ \u00b7 J is calculated, which corresponds to the volume of the parallel epiped of YJ. Subset J is then scanned proportionally to the squared volume."}, {"heading": "3 Method", "text": "In the following, we first analyze the sample of deterministic distributions on non-Euclidean geometries. Then, we present an efficient algorithm for approximate DPP sample on manifolds. Finally, we present our approach for a robust graph construction on sparsely sampled manifolds."}, {"heading": "3.1 DPP Sampling on Manifolds", "text": "As described in Section 2.2, scanning of determinant distributions is used to select rows / columns. Independently, determinant point processes (DPPs) have been introduced to model the probable mutual exclusion [22], providing an attractive scheme to ensure diversity in the selected subset. An interesting construction of DPPs is based on L-ensembles [9]. Given a positive semidefinitive matrix L-Rn, the probability of selecting the subset J {1,., n} isPL (J) = det (LJ \u00d7 J) det (L + I), (5) where I is the identity matrix and LJ \u00d7 J is the submatrix of L containing the rows and columns indexed by J. Identifying L-ensemble matrix L with the core matrix, we cannot assume the DPPPs on the sample of climatic subsets of points we can apply to the Eumatics applications with the help of X."}, {"heading": "3.2 Efficient Approximation of DPP Sampling on Manifolds", "text": "We have seen in the last sections that it is possible to create a decomposition that carries it. [11] We have not been able to find a decomposition in the last sections. [14] We have seen it in the last sections where it is possible to find a decomposition. [14] We have seen it in the last sections. [15] We have seen it in the last sections. [16] We have seen it in the last sections. [17] We have seen it in the last sections. [17] We have seen it in the last sections. [17] We have seen it in the last sections. [17] We have seen it in the last sections. [17] We have not been able to do it in the last sections. [16] We have done it. [17] We have done it. [17] We have done it. [18] We have done it. [18] We have done it."}, {"heading": "3.3 Robust Landmark-based Graph Construction", "text": "After selecting the landmarks, the next step in spectral analysis is to create a graph that approaches the manifold. Frequent techniques for graph construction include selecting the closest neighbors or \u03b5 balls around each node. Both approaches require setting a parameter, either the number of neighbors or the size of the ball, which is critical for performance. Setting the parameter too low leads to a large number of interconnected components, while for many applications one is interested in connecting all points to each other to obtain a consistent embedding of all points. Selecting too high values of the parameters results in shortcuts that result in poor approximation of the manifold. Appropriate selection of parameters is more difficult with sparsely selected manifolds. This is problematic for selecting subsets with consecutive Nystro-m reconstruction, because we drastically reduce the sampling rate to limit the compositional complexity we propose for this technique to address the new distribution problem."}, {"heading": "4 Experiments", "text": "In our first experiment, we show that the proposed efficient DPP sampling algorithm is well suited for a partial selection of non-Euclidean spaces. The benefit of this scenario is that we can limit the updating of the sampling probability D to a local neighborhood Ni around the current point xi. This is in line with the motivation of many varied learning algorithms, which assume that the space behaves locally like a Euclidean space. In our experiment, we set the local neighborhoods Ni to the 20 closest neighbors around the selected point xi. The sampling result is shown in Fig. 1 (d). We obtain a point of high diversity covering the entire diversity, which illustrates that the proposed algorithm preserves the characteristics of the DPP on complex geometries and is therefore appropriate for subset selection in the context of non-linear dimensionality. In our second experiment, we quantify the reconstruction error as formulated in Equ."}, {"heading": "4.1 Image Data", "text": "After evaluating each of the steps of the proposed approach individually, we now present results for scalable, diverse learning based on image data. We work with two sets of data, one of which consists of handwritten digits and a second of patches extracted from 3D medical images. Each set is too large to apply manifold learning methods directly. Consequently, we select boundary stones using the method discussed, perform manifold learning at the boundary stones with the Bhattacharyya distance, and use the Nystro-m method to embed the entire dot set. We only look at the diagonal inputs of the covariance matrices due to space constraints. Given the low-dimensional embedding of the boundary stones with the Bhattacharyya distance and the diagonal matrix of eigenvalues."}, {"heading": "4.1.1 MNIST", "text": "We work with the MNIST dataset [21], consisting of 60,000 binary images of handwritten digits for training and 10,000 for tests with a resolution of 28 x 28 pixels. We set the neighborhood size to m = 5000 and \u03c3 = 5. We embed the images in a 100-dimensional space with laplac eigenmaps. Fig. 7 (a) shows statistical analysis over 20 repetitions for multiple selection schemes for boundary stones of different number of boundary stones k, as well as the graph construction based on Bhattacharyya. The results show that the K mean + + seeding exceeds the uniform initialization, with K mean + + not being able to further improve the initialization. Furthermore, we note a significant improvement in the classification performance for approximate DPP samples compared to K mean + + seeding. Finally, the graph construction based on Bhattacharyya further improves the results."}, {"heading": "4.1.2 Head and Neck", "text": "In a second classification experiment, we used 3D CT scans from the head and neck region with a resolution of 512 x 512 x 145 voxels. These images were taken for radiotherapy of patients with head and neck tumors. Fig. 8 shows a cross-section with segmentations of three risk structures: left parotid gland (green), right parotid gland (blue) and brain stem (green). Segmentation of these structures during treatment planning is of high clinical importance to ensure that they receive a low dose of radiation. We work with image fields of size 7 x 7 x 3 to reflect the physical resolution of the data, which is 0.98 x 0.98 x 2.5 mm3. This results in roughly 150,000 fields being extracted from three scans. 80,000 fields of size 7 x 3 mean that the fields of size 7 x 3 represent the fields of the matrix selection, reflecting the 98 reflecting the physical data of 150,000 to 0.2.5."}, {"heading": "5 Conclusion", "text": "We presented contributions to two critical questions of scalable multifaceted learning: (i) efficient sampling of diverse subsets of multiplicities and (ii) robust graph construction on sparsely sampled multiplicities. We analyzed precisely the sampling from determinant distributions on non-Euclidean spaces and proposed an efficient approximation of the DPP sampling. The algorithm is well suited for selecting milestones on multiplicities as probability updates are locally limited. We also proposed the local covariance estimation around boundaries to capture the local characteristics of the space. This enabled more robust graph construction with the Bhattacharyya distance and yielded lower-dimensional embedding of higher quality. We compared the selection processes of modern subsets and achieved significantly better results with the proposed algorithms. Acknowledgements: This work was partially supported by the Humboldt Foundation's 14005dt-EB5402 image (National Neurobolsis Alliance)."}], "references": [{"title": "Geometric approximation via coresets", "author": ["P. Agarwal", "S. Har-Peled", "K. Varadarajan"], "venue": "Combinatorial and computational geometry 52,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In: SODA. pp", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "The Isomap algorithm and topological stability", "author": ["M. Balasubramanian", "E.L. Schwartz"], "venue": "Science 295(5552),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "On landmark selection and sampling in high-dimensional data analysis", "author": ["M. Belabbas", "P. Wolfe"], "venue": "Philosophical Transactions of the Royal Society", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M. Belabbas", "P. Wolfe"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering", "author": ["Y. Bengio", "J. Paiement", "P. Vincent", "O. Delalleau", "N. Le Roux", "M. Ouimet"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "On adding a list of numbers (and other one-dependent determinantal processes)", "author": ["A. Borodin", "P. Diaconis", "J. Fulman"], "venue": "Bull. Amer. Math. Soc 47,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Determinantal point processes", "author": ["A. Borodin"], "venue": "The Oxford Handbook of Random Matrix Theory", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W. Chen", "Y. Song", "H. Bai", "C. Lin", "E. Chang"], "venue": "TPAMI pp", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Adaptive sampling and fast low-rank matrix approximation. Approximation, Randomization, and Combinatorial Optimization", "author": ["A. Deshpande", "S. Vempala"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "On the nystr\u00f6m method for approximating a gram matrix for improved kernelbased learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "A novel greedy algorithm for nystr\u00f6m approximation", "author": ["A. Farahat", "A. Ghodsi", "M. Kamel"], "venue": "IASTATS", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Scalable training of mixture models via coresets", "author": ["D. Feldman", "M. Faulkner", "A. Krause"], "venue": "In: NIPS. pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Spectral grouping using the nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "TPAMI pp", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Determinantal processes and independence", "author": ["J. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Structured determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "k-dpps: Fixed-size determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML). pp", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Sampling techniques for the nystrom method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "Journal of Machine Learning Research 13,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability pp", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1975}, {"title": "Greedy spectral embedding", "author": ["M. Ouimet", "Y. Bengio"], "venue": "Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics. pp", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Fastmap, metricmap, and landmark mds are all nystrom algorithms", "author": ["J.C. Platt"], "venue": "Proceedings of 10th International Workshop on Artificial Intelligence and Statistics. pp", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Laplacebeltrami spectra as \u201dshape-dna\u201d of surfaces and solids", "author": ["M. Reuter", "F.E. Wolter", "N. Peinecke"], "venue": "Computer-Aided Design 38(4),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Think globally, fit locally: unsupervised learning of low dimensional manifolds", "author": ["L. Saul", "S. Roweis"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K. M\u00fcller"], "venue": "Neural computation", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "TPAMI 22(8),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Large-scale manifold learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. Silva", "J. Langford"], "venue": "Science 290(5500),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Spectral label fusion", "author": ["C. Wachinger", "P. Golland"], "venue": "In: MICCAI. pp", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Sampling from determinantal point processes for scalable manifold learning. In: Information Processing in Medical Imaging", "author": ["C. Wachinger", "P. Golland"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Initiative, A.D.N., et al.: Brainprint: A discriminative characterization of brain morphology", "author": ["C. Wachinger", "P. Golland", "W. Kremen", "B. Fischl", "M. Reuter"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Manifold learning for image-based breathing gating in ultrasound and mri. Medical image analysis", "author": ["C. Wachinger", "M. Yigitsoy", "E.J. Rijkhorst", "N. Navab"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "On a connection between kernel pca and metric multidimensional scaling", "author": ["C. Williams"], "venue": "In: NIPS. pp", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In: NIPS. pp", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Improved nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I. Tsang", "J. Kwok"], "venue": "In: ICML. pp. 1232\u20131239", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}], "referenceMentions": [{"referenceID": 29, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 148, "endOffset": 156}, {"referenceID": 33, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 148, "endOffset": 156}, {"referenceID": 24, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 173, "endOffset": 181}, {"referenceID": 32, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 173, "endOffset": 181}, {"referenceID": 27, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 200, "endOffset": 208}, {"referenceID": 30, "context": "Spectral methods are central for a multitude of applications in machine learning, statistics, and computer vision, such as dimensionality reduction [30, 34], classification [25, 33], and segmentation [28, 31].", "startOffset": 200, "endOffset": 208}, {"referenceID": 35, "context": "To overcome this limitation, the Nystr\u00f6m method [36] is commonly applied to approximate the spectral decomposition of the Gramian matrix.", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "In early work [36], uniform sampling without replacement was proposed.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 13, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 3, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 127, "endOffset": 134}, {"referenceID": 10, "context": "This was followed by numerous alternatives including K-means clustering [37], greedy approaches [23, 14] , and volume sampling [4, 11].", "startOffset": 127, "endOffset": 134}, {"referenceID": 19, "context": "A recent comparison of several approaches is presented in [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "Of particular interest for subset selection is volume sampling [11], equivalent to determinantal sampling [4], because reconstruction error bounds exist.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "Of particular interest for subset selection is volume sampling [11], equivalent to determinantal sampling [4], because reconstruction error bounds exist.", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "This method is, however, not used in practice because of the high computational complexity of sampling from the underlying distributions [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "Independently, determinantal point processes (DPPs) have been proposed recently for tracking and pose estimation [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "A sampling algorithm for DPPs was presented in [17, 18], which has complexity O(n) for n points.", "startOffset": 47, "endOffset": 55}, {"referenceID": 17, "context": "A sampling algorithm for DPPs was presented in [17, 18], which has complexity O(n) for n points.", "startOffset": 47, "endOffset": 55}, {"referenceID": 26, "context": "Popular manifold learning techniques include kernel PCA [27], Isomap [30], and Laplacian eigenmaps [6].", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "Popular manifold learning techniques include kernel PCA [27], Isomap [30], and Laplacian eigenmaps [6].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Popular manifold learning techniques include kernel PCA [27], Isomap [30], and Laplacian eigenmaps [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "It was noted in [3], as a critical response to [30], that the approximation of manifolds with graphs is topologically unstable.", "startOffset": 16, "endOffset": 19}, {"referenceID": 29, "context": "It was noted in [3], as a critical response to [30], that the approximation of manifolds with graphs is topologically unstable.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "A shorter version of this work was published in [32].", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "with K\u0303 being the matrix estimated via the Nystr\u00f6m method [36].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 15, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 23, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 28, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 35, "context": "The Nystr\u00f6m method leads to the minimal kernel completion [4] conditioned on the selected landmarks and has been reported to perform well in numerous applications [10, 16, 24, 29, 36].", "startOffset": 163, "endOffset": 183}, {"referenceID": 10, "context": "We focus on volume sampling for subset selection because of its theoretical advantages [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "These ideas were further generalized in [4] based on annealed determinantal distributions", "startOffset": 40, "endOffset": 43}, {"referenceID": 35, "context": "For s = 0 this is equivalent to uniform sampling [36].", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "It was shown in [11] that for J \u223c p(J), |J | = k", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "Related bounds were presented in [5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "This result establishes a connection between matrix approximation and projective clustering, with the selection of a subsets columns being similar to the construction of a coreset [1, 15].", "startOffset": 180, "endOffset": 187}, {"referenceID": 14, "context": "This result establishes a connection between matrix approximation and projective clustering, with the selection of a subsets columns being similar to the construction of a coreset [1, 15].", "startOffset": 180, "endOffset": 187}, {"referenceID": 21, "context": "Independently, determinantal point processes (DPPs) were introduced for modeling probabilistic mutual exclusion [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "An interesting construction of DPPs is based on L-ensembles [9].", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "To date, applications using determinantal point processes have assumed Euclidean geometry [8, 18].", "startOffset": 90, "endOffset": 97}, {"referenceID": 17, "context": "To date, applications using determinantal point processes have assumed Euclidean geometry [8, 18].", "startOffset": 90, "endOffset": 97}, {"referenceID": 29, "context": "A common solution is to use geodesic distances [30], which can be approximated by the graph shortest path algorithm.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "In [11], an approximative sampling based on the Markov chain Monte Carlo method is proposed to circumvent the combinatorial problem with ( n k ) possible subsets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Further approximations include sampling proportional to the diagonal elements Kii or its squared version K ii, leading to additive error bounds [5, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 12, "context": "Further approximations include sampling proportional to the diagonal elements Kii or its squared version K ii, leading to additive error bounds [5, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 11, "context": "In [12], an algorithm is proposed that yields a k! approximation to volume sampling, worsening the approximation from (k + 1) to (k + 1)!.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Algorithm 1 DPP sampling equivalent to [18]", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "An exact sampling algorithm for DPPs was presented in [17, 18], which requires the eigen decomposition of K = \u2211n i=1 \u03bbiviv > i .", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "An exact sampling algorithm for DPPs was presented in [17, 18], which requires the eigen decomposition of K = \u2211n i=1 \u03bbiviv > i .", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "The update formulation differs from [18], where an orthonormal basis of the eigenvectors in V to the i-th basis vector ei \u2208 R is constructed.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "An approach for efficient sampling proposed in [18] works with the dual representation of K = Y >Y to obtain Q = Y Y >, with Q being hopefully smaller than the matrix K.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "It is noted in the literature that the Gaussian kernel corresponds to an infinite dimensional feature space [27].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "Note that the cardinality of the subset cannot be set in the original DPP sampling algorithm, which led to the introduction of k-DPPs [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "We initialize the vector D = 1n, since it was noted in [18] that the squared norm of the vectors \u2016Bi\u2016 gives rise initially to a fairly uniform distribution because no points have yet been selected.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "We can therefore draw the analogy to multidimensional scaling (MDS) [35] with a Gaussian kernel, where MDS selects the top eigenvectors.", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "Remark: The proposed algorithm bears similarities to K-means++ [2], which replaces the initialization through uniform sampling of K-means by a new seeding algorithm.", "startOffset": 63, "endOffset": 66}, {"referenceID": 35, "context": "We compare the efficient DPP sampling result with uniform sampling [36] and K-means clustering with uniform seeding [37], which performed best in several studies including a recent one [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 36, "context": "We compare the efficient DPP sampling result with uniform sampling [36] and K-means clustering with uniform seeding [37], which performed best in several studies including a recent one [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "We compare the efficient DPP sampling result with uniform sampling [36] and K-means clustering with uniform seeding [37], which performed best in several studies including a recent one [20].", "startOffset": 185, "endOffset": 189}, {"referenceID": 25, "context": "The fish bowl dataset is a punctured sphere proposed in [26], which is sparsely sampled at the bottom and densely at the top, as shown in the supplementary material.", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "where K\u0303 is the normalized kernel for Laplacian eigenmaps and the expectation is calculated over landmark points [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 20, "context": "We work with the MNIST dataset [21], consisting of 60,000 binary images of handwritten digits for training and 10,000 for testing, with a resolution of 28\u00d7 28 pixels.", "startOffset": 31, "endOffset": 35}], "year": 2015, "abstractText": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystr\u00f6m method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques.", "creator": "LaTeX with hyperref package"}}}