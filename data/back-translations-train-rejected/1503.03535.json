{"id": "1503.03535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "On Using Monolingual Corpora in Neural Machine Translation", "abstract": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for English-French and English-German translation. Unlike these language pairs, however, in the majority of scenarios, there is a lack of high quality parallel corpora. In this work, we focus on applying neural machine translation to challenging/low-resource languages such as Chinese and Turkish. In particular, we investigate how to leverage abundant monolingual data for these low-resource translation tasks. Without the use of external alignment tools, we obtained up to a $1.96$ BLEU score improvement with our proposed method compared to the previous best result in Turkish-to-English translation on the IWLST 2014 dataset. On Chinese-to-English translation by using the OpenMT 2015 dataset, we were able to obtain up to a $1.59$ BLEU score improvement over phrase-based and hierarchical phrase-based baselines.", "histories": [["v1", "Wed, 11 Mar 2015 23:50:04 GMT  (146kb,D)", "http://arxiv.org/abs/1503.03535v1", "9 pages, 2 figures"], ["v2", "Fri, 12 Jun 2015 14:05:31 GMT  (159kb,D)", "http://arxiv.org/abs/1503.03535v2", "9 pages, 2 figures"]], "COMMENTS": "9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["caglar gulcehre", "orhan firat", "kelvin xu", "kyunghyun cho", "loic barrault", "huei-chi lin", "fethi bougares", "holger schwenk", "yoshua bengio"], "accepted": false, "id": "1503.03535"}, "pdf": {"name": "1503.03535.pdf", "metadata": {"source": "CRF", "title": "On Using Monolingual Corpora in Neural Machine Translation", "authors": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people are able to move, to unfold, and that they are able to unfold in order to unfold."}, {"heading": "2 Background: Neural Machine Translation", "text": "SMT systems maximize the conditional probability p (y | x) of a correct target translation y, taking into account a source sentence x. This is done by separately maximizing a language model p (y) and the (reverse) translation component p (x | y) using the Bayes rule p (y | x) p (y) p (y).This decomposition into a language model and translation model aims to make full use of the available corpora: a monolingual corpora to adapt the language model and parallel corpora to the translation model. In reality, however, SMT systems tend to model p (y | x) directly by a linear combination of several features, using a so-called log-linear model: log p (y | x) = j fj (x, y) + C, (1), where fj-th is the feature that is based on both or one of the source and target phrases, and is often a standardization C."}, {"heading": "3 Model Description", "text": "We used the model recently proposed by Bahdanau et al. (2014), which has learned to jointly (softly) align and translate as a basic neural machine translation system in this work. Here, we describe in detail this model, which we refer to as \"NMT.\" The NMT encoder is a bidirectional RNN consisting of forward and backward RNNs (Schuster and Paliwal, 1997).The forward RNN reads the input sequence / sentence x = (x1,., xT) in a forward direction, resulting in a sequence of hidden states (\u2212 \u2192 h 1,.,., \u2212 h T) x in an opposite direction and outputs (h 1,.,.,.) We link a pair of hidden states at any time to build a sequence of annotational vectors."}, {"heading": "4 Integrating Language Model into the Decoder", "text": "In this paper, we propose two alternatives to integrate a language model into a neural machine translation system, which we call flat fusion (Sec. 4.1) and deep fusion (Sec. 4.2). Without loss of generality, we use a language model based on recurrent neural networks (RNLM, Mikolov et al., 2011) that corresponds to the decoder described in the previous section, except that it is not distorted by a context vector (i.e. ct = 0 in equations. (2) - (3)). In the following sections, we assume that both an NMT model (on parallel corpora) and an RNNLM (on larger monolingual corpora) have been pre-trained separately before being integrated."}, {"heading": "4.1 Shallow Fusion", "text": "At each step, the translation model proposes a series of candidate words. Candidates are then evaluated according to the weighted sum of the points given by the translation model and the language model. Specifically, the translation model (in this case the NMT) calculates at each step t the score of each possible next word for each hypothesis in a series of hypotheses {y (i) \u2264 t \u2212 1}. Each score is the sum of the score of the hypothesis and the score of the NMT on the word. All these new hypotheses (a hypothesis from the previous time step with a next word attached at the end) are then sorted according to their respectful score, and the TopK values are selected as candidates {y (i) \u2264 t} i = 1,..., K. We then renew these hypotheses with the next word appended to the end of the word."}, {"heading": "4.2 Deep Fusion", "text": "In deep fusion, we integrate the RNNLM and the decoder of the NMT by concatenating their hidden states side by side (see Figure 1 (b)). Unlike vanilla NMT (without the language model component), the hidden layer of deep output takes the hidden state of the RNNLM as input in addition to that of the NMT, the previous word, and the context such as thatp (yt | y < t, x) and exp (y > t (Wofo (s LM t, s TM t, yt \u2212 1, ct) + bo)), (5) where we again used the superscripts LM and TM to distinguish the hidden states of the RNNLM and NMTT as parameters. During fine tuning of the model, we only adjust adjust the parameters used in the output to ensure that all parameters (which are defined only by fine tuning the corporative)."}, {"heading": "4.2.1 Balance between LM and TM", "text": "Intuitively, some words in the translated sentence are decided mainly on the basis of the source sentence (i.e. by the TM), but some other words are decided largely on the basis of the language model (LM). For example, there is no Chinese word that corresponds to articles in English, and in the case of translation from Chinese to English, the decoder must insert a correct / suitable article based mainly on the signal from the LM. On the other hand, if a noun is to be translated, it may be better to ignore any signal from the LM as it might prevent the decoder from selecting the correct translation. To allow the decoder to flexibly balance between the signal from the LM and TM, we extend the decoder with a so-called controller. The controller is implemented as a function by taking the hidden state of the LM as input and disabling the hidden state of the LM (v > g s LM + bg), where it is a logical signature."}, {"heading": "5 Datasets", "text": "We evaluate the proposed approaches to integrate monolingual data into neural machine translation in two tasks: Chinese to English (Zh-En) and Turkish to English (Tr-En)."}, {"heading": "5.1 Parallel Corpora", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Zh-En: OpenMT\u201915", "text": "We used the parallel corpora provided as part of the NIST OpenMT '15 Challenge, but the Chinese characters in Devlin characters were removed. Sentence-aligned pairs from three domains are combined to a training set: (1) SMS / CHAT and (2) conversations national telephone speech (CTS) from DARPA BOLT Project, and (3) newsgroups / weblogs from DARPA GALE Project. Overall, the training set consisted of 430K set pairs (see Table 1 for detailed statistics). We trained models with this training set and the development set (the concatenation of the provided development and tuning sets from the challenge), and eval-In all of our experiments, we used bg = \u2212 1 to ensure that the initial average is 0.2."}, {"heading": "5.1.2 Tr-En: IWSLT\u201914", "text": "We used the WIT parallel corpus (Cettolo et al., 2012) and the SETimes parallel corpus provided as part of the IWSLT '14 (machine translation track), which consists of the sentence-oriented subtitles of the TED and TEDx conversations, and we linked dev2010 and tst2010 to a development set and tst2011, tst2012, tst2013 and tst2014 to a test set. See Table 1 for the detailed statistics of the parallel company. Pre-processing As done in the case of ZhEn, we first removed all special symbols from the corpora and symbolized the Turkish side with the tokenizer provided by Moses. To overcome the exploding vocabulary due to the rich influences and derivatives in Turkish, we segmented each Turkish sentence into a sequence of subordinate units using Zemberek, followed by morphological analysis in 2007 for the morphammeta / a part of the morphology section."}, {"heading": "5.2 Monolingual Corpora", "text": "For language modeling, the Linguistic Data Consortium's English Gigaword Corpus, which consists of Newswire documents, was approved for both OpenMT-15 and IWSLT-15 challenges, and the tokenized Gigaword Corpus was used directly without any additional pre-processing steps to train an external neural language model for the English side of both Zh-En and Tr-En neural machine translation systems."}, {"heading": "6 Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Training Procedure", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 Neural Machine Translation", "text": "We constructed the vocabulary with the most common words in the parallel corpora. Vocabulary sizes for Chinese, Turkish, and English were 10K, 30K, and 40K, respectively. Each word was projected into the continuous space of 620-dimensional Euclidean space, initially reducing the dimensionality of both the encoder and the decoder. We selected the size of the recurring units for Zh-En and Tr-En to 1200 and 1000. Each model was optimized with the help of Adadelta (Zeiler, 2012), with a minibatch of 80 samples. For each update, we used gradient clipping, so that if the L2 standard of progression exceeds 5, we reset it to 5 (Pascanu et al., 2013)."}, {"heading": "6.1.2 Language Model", "text": "We trained two recurrent neural network language models (RNNLM) using 2400 LSTM units in English Gigaword Corpus, using the vocabulary of Zh-En and Tr-En Corpora. Each sentence containing more than 10% of unknown words was discarded."}, {"heading": "6.2 Shallow and Deep Fusion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Shallow Fusion", "text": "The hyperparameter \u03b2 in Eq.4 was set to maximize translation performance on the development sentence in the range between 0.001 and 0.1. In our experiment, we found it important to renormalize the softmax of the LM without the end of the sequence sentence and without the vocabulary symbol (i.e. \u03b2 = 0 in (4)). We suspect that this is due to a different domain and also to a tendency to longer sentences in the training area of the LM."}, {"heading": "6.2.2 Deep Fusion", "text": "We refined the parameters of the deep output layer (Equation (5)) and the controller (see Equation (6) using the Adam Optimizer (Kingma and Ba, 2014) for Zh-En and RMSProp with Pulse to TrEn. During fine tuning, the failure probability and standard deviation of weight noise were set to 0.56 and 0.005 based on preliminary experiments, which will be reduced after the initial 10K updates."}, {"heading": "7 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Zh-En: OpenMT\u201915", "text": "In addition to NMT-based systems, we also trained a phrase-based and hierarchical phrase-based SMT system (Koehn et al., 2003; Chiang, 2005) with / without reassessment by an external neural language model (CSLM, Pivot, 2007b). We present the results in Table 2. We observed that integrating an additional LM through deep fusion (see paragraph 4.2) helped the models perform better in general, except in the case of the CTS task. Interestingly, we found that the NMT-based models, regardless of whether the LM was integrated or not, outperformed the more traditional phrase-based SMT systems. Gigaword corpus is prepared by the John Hopkins University Human Language Technology Center and provided by the LDC."}, {"heading": "7.2 Tr-En: IWSLT\u201914", "text": "In Table 3, we present the results of Tr-En. Compared to Zh-En, we observed a greater improvement in performance (up to + 1.19 BLEU points) from the basic NMT to NMT integrated with the LM via the proposed method of deep fusion. Furthermore, by including the LM by means of deep fusion, the NMT systems were able to significantly exceed the previously reported best result (Y\u0131lmaz et al., 2013) (up to + 1.96 BLEU points) for all individual test sets without ensemble learning."}, {"heading": "7.3 Analysis: Effect of Language Model Performance", "text": "The results we reported on in this paper clearly reflect this dependence. In the case of Zh-En, we can intuitively see that the writing style in both SMS / CHAT and the language of conversation will be significantly different from that in news articles (which make up the bulk of the English gigaword corpus), which is clearly supported by the high plexicity of the development set with our LM (see column Zh-En in Table 4), which explains the marginal improvement we observed in the case of Tr-En. In the case of Tr-En, on the other hand, the similarity between the domain of the monolingual corpus and the translation is higher (see column Tr-En in Table 4) if the translation capacity significantly improves the translation capacity, if the translation capacity of the monolingual corpus and that of the translation capacity significantly increases the translation capacity (see column Tr-En in Table 4)."}, {"heading": "8 Conclusion and Future Work", "text": "In summary, despite recent successes, Neural Machine Translation (NMT) suffers from the fact that it is unable to utilize the vast amount of monolingual data available, and the ability to integrate monolingual corpus is particularly important in the frequent case of resource-poor language pairs, which may be why the recent success of NMT is largely due to pairs of European languages (En-Fr and En-De). In this paper, we have proposed two alternative approaches to this problem that harness monolingual data in NMT models: flat fusion and deep fusion. We observed a significant improvement in performance when the NMT model was integrated with the external language model (LM) using the proposed deep fusion. Furthermore, the NMT models formed with deep fusion could achieve better results than those based on deep fusion between existing systems (BLsimilation)."}, {"heading": "Acknowledgments", "text": "The authors thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We thank the following research funding and computer support organizations: NSERC, Samsung, Calcul Que'bec, Compute Canada, the Canada Research Chairs and CIFAR. O.F. is funded by the TUBITAK 2214-A Program. The authors of this LIUM paper are funded by DARPA BOLT."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "Submited to the Deep Learning and Unsuper-", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["D. Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy).", "citeRegEx": "Farley and Bengio,? 2010", "shortCiteRegEx": "Farley and Bengio", "year": 2010}, {"title": "Wit3: Web inventory of transcribed and translated talks", "author": ["M. Cettolo", "C. Girardi", "M. Federico"], "venue": "Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 261\u2013268.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["D. Chiang"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263\u2013270. Association for Computational Lin-", "citeRegEx": "Chiang,? 2005", "shortCiteRegEx": "Chiang", "year": 2005}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the Empiri-", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "Association for Computational Linguistics.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Maxout networks", "author": ["I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "Proceedings of The 30th International Conference on Machine Learning, pages 1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "Advances in Neural Information Processing Systems, pages 2348\u2013 2356.", "citeRegEx": "Graves,? 2011", "shortCiteRegEx": "Graves", "year": 2011}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.2007.", "citeRegEx": "Jean et al\\.,? 2014", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "Proceedings of the ACL Conference on Empirical Meth-", "citeRegEx": "Kalchbrenner and Blunsom,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv:1412.6980 [cs.LG].", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Statistical Machine Translation", "author": ["P. Koehn"], "venue": "Cambridge University Press, New York, NY, USA.", "citeRegEx": "Koehn,? 2010", "shortCiteRegEx": "Koehn", "year": 2010}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["T. Mikolov", "S. Kombrink", "A. Deoras", "L. Burget", "J. Cernocky"], "venue": "Proc. of the 2011 ASRU Workshop, pages 196\u2013201.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "An outline of turkish morphology", "author": ["K. Oflazer", "E. G\u00f6\u00e7men", "E. Gocmen", "C. Bozsahin"], "venue": null, "citeRegEx": "Oflazer et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Oflazer et al\\.", "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML 2013).", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "Proceedings of the Second International Conference on Learning Representations (ICLR 2014).", "citeRegEx": "Pascanu et al\\.,? 2014", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Morphological disambiguation of turkish text with perceptron algorithm", "author": ["H. Sak", "T. G\u00fcng\u00f6r", "M. Sara\u00e7lar"], "venue": "Computational Linguistics and Intelligent Text Processing, pages 107\u2013118. Springer.", "citeRegEx": "Sak et al\\.,? 2007", "shortCiteRegEx": "Sak et al\\.", "year": 2007}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on, 45(11), 2673\u2013 2681.", "citeRegEx": "Schuster and Paliwal,? 1997", "shortCiteRegEx": "Schuster and Paliwal", "year": 1997}, {"title": "Continuous space language models", "author": ["H. Schwenk"], "venue": "Comput. Speech Lang., 21(3), 492\u2013 518.", "citeRegEx": "Schwenk,? 2007a", "shortCiteRegEx": "Schwenk", "year": 2007}, {"title": "Continuous space language models", "author": ["H. Schwenk"], "venue": "Computer Speech & Language, 21(3), 492\u2013518.", "citeRegEx": "Schwenk,? 2007b", "shortCiteRegEx": "Schwenk", "year": 2007}, {"title": "Continuous space translation models for phrase-based statistical machine translation", "author": ["H. Schwenk"], "venue": "M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on Computational Linguistics (COLIN),", "citeRegEx": "Schwenk,? 2012", "shortCiteRegEx": "Schwenk", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "Advances in Neural Information Processing Systems (NIPS 2014).", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Tubitak turkishenglish submissions for iwslt 2013", "author": ["E. Y\u0131lmaz", "I.D. El-Kahlout", "B. Ayd\u0131n", "Z.S. Ozil", "C. Mermer"], "venue": "Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT), pages 152\u2013", "citeRegEx": "Y\u0131lmaz et al\\.,? 2013", "shortCiteRegEx": "Y\u0131lmaz et al\\.", "year": 2013}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv:1212.5701", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).", "startOffset": 103, "endOffset": 200}, {"referenceID": 23, "context": "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).", "startOffset": 103, "endOffset": 200}, {"referenceID": 5, "context": "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).", "startOffset": 103, "endOffset": 200}, {"referenceID": 0, "context": "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).", "startOffset": 103, "endOffset": 200}, {"referenceID": 23, "context": "The neural machine translation approach, however, showed that it is possible to build a competitive end-to-end neural network-based translation system for English-French and EnglishGerman (Sutskever et al., 2014; Jean et al., 2014) (also see Section 2).", "startOffset": 188, "endOffset": 231}, {"referenceID": 9, "context": "The neural machine translation approach, however, showed that it is possible to build a competitive end-to-end neural network-based translation system for English-French and EnglishGerman (Sutskever et al., 2014; Jean et al., 2014) (also see Section 2).", "startOffset": 188, "endOffset": 231}, {"referenceID": 10, "context": "Available parallel corpora for these two pairs are significantly smaller than those available for language pairs used in previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) such as English-French.", "startOffset": 135, "endOffset": 214}, {"referenceID": 23, "context": "Available parallel corpora for these two pairs are significantly smaller than those available for language pairs used in previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) such as English-French.", "startOffset": 135, "endOffset": 214}, {"referenceID": 0, "context": "Available parallel corpora for these two pairs are significantly smaller than those available for language pairs used in previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) such as English-French.", "startOffset": 135, "endOffset": 214}, {"referenceID": 15, "context": "This leads to the exploding vocabulary problem without proper segmentation (Oflazer et al., 1994).", "startOffset": 75, "endOffset": 97}, {"referenceID": 12, "context": "Chinese sentences, on the other hand, do not utilize blank spaces to separate words, making it difficult to build a vocabulary of word tokens which are often the unit of translation in the traditional statistical machine translation (SMT) system (Koehn, 2010).", "startOffset": 246, "endOffset": 259}, {"referenceID": 10, "context": "This is typically done under the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) consisting of neural networks.", "startOffset": 59, "endOffset": 133}, {"referenceID": 5, "context": "This is typically done under the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) consisting of neural networks.", "startOffset": 59, "endOffset": 133}, {"referenceID": 23, "context": "This is typically done under the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) consisting of neural networks.", "startOffset": 59, "endOffset": 133}, {"referenceID": 5, "context": "By using RNN architectures equipped to learn long term dependencies such as Gated Recurrent Units (GRU)/Long Short-Term Memory (LSTM), the whole system can be trained a end-to-end fashion (Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 188, "endOffset": 230}, {"referenceID": 23, "context": "By using RNN architectures equipped to learn long term dependencies such as Gated Recurrent Units (GRU)/Long Short-Term Memory (LSTM), the whole system can be trained a end-to-end fashion (Cho et al., 2014; Sutskever et al., 2014).", "startOffset": 188, "endOffset": 230}, {"referenceID": 19, "context": "The encoder of the NMT is a bidirectional RNN which consists of forward and backward RNNs (Schuster and Paliwal, 1997).", "startOffset": 90, "endOffset": 118}, {"referenceID": 0, "context": "We used the model proposed recently by Bahdanau et al. (2014) that learns to jointly (soft)align and translate as a baseline neural machine translation system in this paper.", "startOffset": 39, "endOffset": 62}, {"referenceID": 5, "context": "where fr is the gated recurrent unit (Cho et al., 2014).", "startOffset": 37, "endOffset": 55}, {"referenceID": 17, "context": "We use a deep output layer (Pascanu et al., 2014) to compute the conditional distribution over words:", "startOffset": 27, "endOffset": 49}, {"referenceID": 7, "context": "fo is a single-layer feedforward neural network with a 2-way maxout non-linearity (Goodfellow et al., 2013).", "startOffset": 82, "endOffset": 107}, {"referenceID": 6, "context": "Preprocessing Importantly, we did \u201cnot segment\u201d the Chinese sentences and considered each character as a symbol, unlike a more traditional approach of using a separate segmentation tool to segment the Chinese characters into words (Devlin et al., 2014).", "startOffset": 231, "endOffset": 252}, {"referenceID": 3, "context": "We used the WIT parallel corpus (Cettolo et al., 2012) and SETimes parallel corpus made available as a part of the IWSLT\u201914 (machine translation track).", "startOffset": 32, "endOffset": 54}, {"referenceID": 18, "context": "analysis (Sak et al., 2007).", "startOffset": 9, "endOffset": 27}, {"referenceID": 25, "context": "Each model was optimized using Adadelta (Zeiler, 2012) with a minibatch of 80 samples.", "startOffset": 40, "endOffset": 54}, {"referenceID": 16, "context": "At each update, we used gradient clipping such that if the L2 norm of the gradient exceeds 5, we renormalized it back to 5 (Pascanu et al., 2013).", "startOffset": 123, "endOffset": 145}, {"referenceID": 8, "context": "001) to each parameter to prevent overfitting (Graves, 2011).", "startOffset": 46, "endOffset": 60}, {"referenceID": 11, "context": "(6) using the Adam optimizer (Kingma and Ba, 2014) for Zh-En and RMSProp with momentum on TrEn.", "startOffset": 29, "endOffset": 50}, {"referenceID": 13, "context": "In addition to NMT-based systems, we also trained a phrase-based as well as hierarchical phrasebased SMT systems (Koehn et al., 2003; Chiang, 2005) with/without re-scoring by an external neural language model (CSLM, Schwenk, 2007b).", "startOffset": 113, "endOffset": 147}, {"referenceID": 4, "context": "In addition to NMT-based systems, we also trained a phrase-based as well as hierarchical phrasebased SMT systems (Koehn et al., 2003; Chiang, 2005) with/without re-scoring by an external neural language model (CSLM, Schwenk, 2007b).", "startOffset": 113, "endOffset": 147}, {"referenceID": 24, "context": "Furthermore, by incorporating the LM via deep fusion, the NMT systems were able to significantly outperform the previously reported best result (Y\u0131lmaz et al., 2013) (up to +1.", "startOffset": 144, "endOffset": 165}, {"referenceID": 1, "context": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 57, "endOffset": 102}], "year": 2017, "abstractText": "Recent works on end-to-end neural network-based architectures for machine translation have shown promising results for English-French and English-German translation. Unlike these language pairs, however, in the majority of scenarios, there is a lack of high quality parallel corpora. In this work, we focus on applying neural machine translation to challenging/low-resource languages Turkish and low-resource domains such as parallel corpora of Chinese chat messages. In particular, we investigated how to leverage abundant monolingual data for these low-resource translation tasks. Without the use of external alignment tools, we obtained up to a 1.96 BLEU score improvement with our proposed method compared to the previous best result in Turkish-to-English translation on the IWLST 2014 dataset. On Chinese-toEnglish translation by using the OpenMT 2015 dataset, we were able to obtain up to a 1.59 BLEU score improvement over phrase-based and hierarchical phrase-based baselines.", "creator": "LaTeX with hyperref package"}}}