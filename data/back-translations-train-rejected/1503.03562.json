{"id": "1503.03562", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation", "abstract": "Compared to Multilayer Neural Networks with real weights, Binary Multilayer Neural Networks (BMNNs) can be implemented more efficiently on dedicated hardware. BMNNs have been demonstrated to be effective on binary classification tasks with Expectation BackPropagation (EBP) algorithm on high dimensional text datasets. In this paper, we investigate the capability of BMNNs using the EBP algorithm on multiclass image classification tasks. The performances of binary neural networks with multiple hidden layers and different numbers of hidden units are examined on MNIST. We also explore the effectiveness of image spatial filters and the dropout technique in BMNNs. Experimental results on MNIST dataset show that EBP can obtain 2.12% test error with binary weights and 1.66% test error with real weights, which is comparable to the results of standard BackPropagation algorithm on fully connected MNNs.", "histories": [["v1", "Thu, 12 Mar 2015 02:24:31 GMT  (65kb)", "http://arxiv.org/abs/1503.03562v1", "8 pages with 1 figures and 4 tables"], ["v2", "Fri, 13 Mar 2015 01:32:15 GMT  (65kb)", "http://arxiv.org/abs/1503.03562v2", "8 pages with 1 figures and 4 tables"], ["v3", "Sun, 22 Mar 2015 21:47:56 GMT  (65kb)", "http://arxiv.org/abs/1503.03562v3", "8 pages with 1 figures and 4 tables"]], "COMMENTS": "8 pages with 1 figures and 4 tables", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["zhiyong cheng", "daniel soudry", "zexi mao", "zhenzhong lan"], "accepted": false, "id": "1503.03562"}, "pdf": {"name": "1503.03562.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhiyong Cheng"], "emails": ["zy.cheng.2011@smu.edu.sg", "daniel.soudry@gmail.com", "lanzhzh}@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.03 562v 1 [cs"}, {"heading": "1 INTRODUCTION", "text": "In recent years, deep neural networks (DNNs) have attracted enormous attention from a wide range of research areas related to signal and information processing. Advanced performance has been achieved using DNN techniques on various challenging tasks and applications, such as speech recognition (Hinton et al., 2012), object recognition (Krizhevsky et al., 2012; Szegedy et al., 2014), multimedia event detection (Lan et al., 2013), etc. Almost all current DNNs are real valued neural networks (RMNNs). However, effective RMNNs are often massive and require large computerized and energy resources. For example, GoogLeNet has 22 layers with tens of thousands of hidden units (Szegedy et al., 2014). MNNNNNs with binary weights (BMNNNNs) have the advantage that they can be efficiently implemented on specialized hardware."}, {"heading": "2 EXPECTATION BACKPROPAGATION", "text": "In this section we will examine the expectation backwards (EBP) and the question of how the EBP algorithms for binary weight differences (EBP) are to be solved (EBP). (EBP) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (EB) n (n) n (EB) n (n) n (EB) n (EB) n (n) n (EB) n (n) n (n) n (EB) n (n) n (n) n (n) n (EB n) n (n (n) n (n) n (n) n) n (EB n (n) n (n) n (n) n (n (n) n) n (n) n (EB n (n) n (n) n (n) n (EB (n) n (n) n (n (n) n (EB) n (n) n (n) n (n (EB) n (n) n (n) n (EB n (n) n) n (n) n (n (EB) n (n) n (n) n (EB"}, {"heading": "2.1 THE EXPECTATION BACKPROPAGATION ALGORITHM", "text": "In view of the input x and the desired output, a forward gear is carried out to calculate the mean output < vl > for each shift; then a reverse gear is carried out to update P (Wij, l | Dn) for all weight classes. First, we initialize the MNN input < vk, 0 > = xk for all and then recursively calculate the following values for output m = 1,..., L and all km\u00b5k, m = 1 \u221a KmVm \u2212 1 < Wkr, m > < vm \u2212 1 > < vk, m > (\u00b5k, m) \u2212 1 (9) VmVm = 1 \u221a KmVm \u2212 1 < Wkr, m > < vr, m > < m > (\u00b5k, m) \u2212 1 (9) Vm = 1KmVm \u2212 1 Vm < Vm = 1 < Vr > < vr > < vr > < vr > < m > < n > (m)"}, {"heading": "2.2 IMPLEMENTATION FOR BINARY WEIGHTS", "text": "In Soudry et al. (2014), the distribution of Wij, l is parameterized in such a way that P (Wij, l | Dn) = eh (n) ij, l Wij, leh (n) ij, l + e \u2212 h (n) ij, l (15) according to the forward speed (Wij, l) = sech2 (hij, l) parameterization can be used for the calculation < Wij, l > = tanh (hij, l), < W 2ij, l > = 1 and V ar (Wij, l) = sech2 (hij, l). In backward processing, the parameter h (n) ij, l = 1 BMj, l > l < l < l < l < l < l < l < l < l < l < l < l < l < l, l < l < l, l (n) ij, l (n) ij, l = h (n \u2212 1) ij, l < l < l < l;"}, {"heading": "3 IMPLEMENTATION OF EBP ON IMAGE CLASSIFICATION", "text": "However, these experiments are limited to high-dimensional text datasets (the input vector dimensions are from 11,463 to 238,739), and all tasks are binary classification tasks. In this study, we will investigate the performance of the EBP algorithm on image records for multi-class classification, as well as the effectiveness of failure techniques (Srivastava et al., 2014) in EBP algorithms. Two methods are used to integrate the image into the MNs. The first method is to directly transform the 2D image into a 1D vector by linking the pixels in a specific order."}, {"heading": "4 EXPERIMENTS", "text": "In this section we report on the experiments of the EBP algorithm with MNNs with different architectural configurations in the standard MNIST database with handwritten digits (LeCun et al., 1998)."}, {"heading": "4.1 EXPERIMENT SETUP", "text": "The MNIST database contains 60,000 images (28 \u00b7 28 pixels) and the test set has another 10,000 images. During the training process, all images of the training set were presented sequentially in each epoch in a randomized sequence. The task was to identify the label (0, 1, 9) trained by EBP algorithms. The label is designed to be yk = 2, label + 1 + 1. We process the training data by centralizing (mean = 0) and normalizing (std = 1) the pixels recommended for backPropaganda (LeCun et al., 2012)."}, {"heading": "4.2 EXPERIMENTAL RESULTS", "text": "This year it is so far that it will only take one year to reach an agreement."}, {"heading": "5 CONCLUSIONS", "text": "Experimental results show that BMNs using the EBP algorithm can achieve good performance in MNIST classification tasks, and the results also show that the failure techniques can significantly improve BMNNs using the EBP algorithm. Image space configuration improves the performance of networks with real weights, but not BMNs. In this study, we are only conducting experiments with the MNIST dataset. The performance of BMNNNs using the EBP algorithm in image classification tasks needs to be further validated on other image datasets (e.g. CIFAR10). In the future, we would like to investigate the performance of Standard Convolutionary Networks using the EBP algorithm in the use of negative threads to explore the use of equalithms for network evaluation."}, {"heading": "ACKNOWLEDGMENTS", "text": "This is a course project for \"Lab Course on Deep Learning (11-875)\" during Zhiyong Cheng's visit to Carnegie Mellon University."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "tmacs/mw fine-grained stochastic resonant charge-recycling array processor", "author": ["Karakiewicz", "Rafal", "Genov", "Roman", "Cauwenberghs", "Gert"], "venue": "Sensors Journal, IEEE,", "citeRegEx": "Karakiewicz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Karakiewicz et al\\.", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Cmu-informedia at TRECVID 2013 multimedia event detection", "author": ["Lan", "Zhen-Zhong", "Jiang", "Lu", "Yu", "Shoou-I", "Rawat", "Shourabh", "Cai", "Yang", "Gao", "Chenqiang", "Xu", "Shicheng", "Shen", "Haoquan", "Li", "Xuanchong", "Wang", "Yipei"], "venue": "In TRECVID 2013 Workshop,", "citeRegEx": "Lan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient backprop", "author": ["LeCun", "Yann A", "Bottou", "L\u00e9on", "Orr", "Genevieve B", "M\u00fcller", "Klaus-Robert"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "LeCun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Expectation propagation with factorizing distributions: A gaussian approximation and performance results for simple models", "author": ["Ribeiro", "Fabiano", "Opper", "Manfred"], "venue": "Neural computation,", "citeRegEx": "Ribeiro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2011}, {"title": "Expectation backpropagation: parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Soudry", "Daniel", "Hubara", "Itay", "Meir", "Ron"], "venue": "In Proc. of NIPS,", "citeRegEx": "Soudry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "State-of-the-art performances have been achieved with DNN techniques on various challenging tasks and applications, such as speech recognition (Hinton et al., 2012), object recognition (Krizhevsky et al.", "startOffset": 143, "endOffset": 164}, {"referenceID": 2, "context": ", 2012), object recognition (Krizhevsky et al., 2012; Szegedy et al., 2014), multimedia event detection (Lan et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 9, "context": ", 2012), object recognition (Krizhevsky et al., 2012; Szegedy et al., 2014), multimedia event detection (Lan et al.", "startOffset": 28, "endOffset": 75}, {"referenceID": 3, "context": ", 2014), multimedia event detection (Lan et al., 2013), etc.", "startOffset": 36, "endOffset": 54}, {"referenceID": 9, "context": "For example, GoogLeNet has 22 layers with tens of thousands of hidden units (Szegedy et al., 2014).", "startOffset": 76, "endOffset": 98}, {"referenceID": 0, "context": "State-of-the-art performances have been achieved with DNN techniques on various challenging tasks and applications, such as speech recognition (Hinton et al., 2012), object recognition (Krizhevsky et al., 2012; Szegedy et al., 2014), multimedia event detection (Lan et al., 2013), etc. Almost all the current DNNs are real-valuedweight Mutlilayer Neural Networks (RMNNs). However, an effective RMNNs are often massive and require large computational and energetic resources. For example, GoogLeNet has 22 layers with tens of thousands of hidden units (Szegedy et al., 2014). MNNs with binary weights (BMNNs) have the advantage that they can be implemented efficiently on dedicated hardware. For example, Karakiewicz et al. (2012) have presented a chip which enable 10 multiply accumulates per second per mW power efficiency with binary weights.", "startOffset": 144, "endOffset": 730}, {"referenceID": 0, "context": "State-of-the-art performances have been achieved with DNN techniques on various challenging tasks and applications, such as speech recognition (Hinton et al., 2012), object recognition (Krizhevsky et al., 2012; Szegedy et al., 2014), multimedia event detection (Lan et al., 2013), etc. Almost all the current DNNs are real-valuedweight Mutlilayer Neural Networks (RMNNs). However, an effective RMNNs are often massive and require large computational and energetic resources. For example, GoogLeNet has 22 layers with tens of thousands of hidden units (Szegedy et al., 2014). MNNs with binary weights (BMNNs) have the advantage that they can be implemented efficiently on dedicated hardware. For example, Karakiewicz et al. (2012) have presented a chip which enable 10 multiply accumulates per second per mW power efficiency with binary weights. Thus, it is attractive to develop effective algorithms for BMNNs to achieve comparable performances with RMNNs. Traditional MNNs are trained with BackPropagation (BP) or similar gradient descent methods. However, BP or gradient descent methods cannot be directly used for training binary neural networks. A straightforward method for this problem is to binarize the real-valued weights, while this approach will decrease the performance significantly. Recently, Soudry et al. (2014) presented an", "startOffset": 144, "endOffset": 1328}, {"referenceID": 7, "context": "Experiments on several large text datasets show promising performances on binary classification tasks with binary-weighted MNNs (Soudry et al., 2014).", "startOffset": 128, "endOffset": 149}, {"referenceID": 7, "context": "Experiments on several large text datasets show promising performances on binary classification tasks with binary-weighted MNNs (Soudry et al., 2014). As an extension of the previous work by Soudry et al. (2014), in this work, we study the performance of EBP algorithm on image classification tasks with binary and real weights MNNs.", "startOffset": 129, "endOffset": 212}, {"referenceID": 7, "context": "Based on the equation, performing a marginal of the posterior (see appendix A in Soudry et al. (2014) for details) of the Bayes update and re-arrange terms, we can obtain a Bayes-like update to the marginal", "startOffset": 81, "endOffset": 102}, {"referenceID": 7, "context": "More detailed information about the implementation for real weights is described in Soudry et al. (2014).", "startOffset": 84, "endOffset": 105}, {"referenceID": 7, "context": "9 in (Soudry et al., 2014)) as:", "startOffset": 5, "endOffset": 26}, {"referenceID": 7, "context": "1, which is defined as the Deterministic EBP output (EBP-D) (Soudry et al., 2014).", "startOffset": 60, "endOffset": 81}, {"referenceID": 7, "context": "In Soudry et al. (2014), the distribution of Wij,l is parameterized in the way so that", "startOffset": 3, "endOffset": 24}, {"referenceID": 4, "context": "The spatial configuration is considered in a similar way as Convolutional Neural Networks (CNN) (LeCun et al., 1998).", "startOffset": 96, "endOffset": 116}, {"referenceID": 5, "context": "The performance of EBP algorithm has been evaluated in Soudry et al. (2014). However, those experiments are limited to high dimensional text datasets (the dimensions of the input feature vectors are from 11,463 to 238,739), and all the tasks are binary classification tasks.", "startOffset": 55, "endOffset": 76}, {"referenceID": 7, "context": "Algorithm 1 Expectation BackPropagation (EBP) algorithm for fully connected binary MNNs with binary synaptic weights and real bias (Soudry et al., 2014) .", "startOffset": 131, "endOffset": 152}, {"referenceID": 4, "context": "In this section, we report the experiments of the EBP algorithm with MNNs with different architecture configurations in the standard MNIST handwritten digits database (LeCun et al., 1998).", "startOffset": 167, "endOffset": 187}, {"referenceID": 5, "context": "We pre-process the training data by centralizing (mean = 0) and normalizing (std = 1) the pixels as recommended for BackPropagation (LeCun et al., 2012).", "startOffset": 132, "endOffset": 152}, {"referenceID": 7, "context": "For weight initialization, we used the same method as Soudry et al. (2014). Effects of Hidden Unite Number and Hidden Layer Number Table 2 shows the results of MNNs on MNIST dataset using EBP algorithms on different network structures without dropout.", "startOffset": 54, "endOffset": 75}, {"referenceID": 7, "context": "For weight initialization, we used the same method as Soudry et al. (2014). Effects of Hidden Unite Number and Hidden Layer Number Table 2 shows the results of MNNs on MNIST dataset using EBP algorithms on different network structures without dropout. From the results, we can observe that for networks with one hidden layer, the increase of hidden units clearly improves the performance and the best performance is obtained with 800 units. Two-hidden-layer structure with EBP-P outperforms the one-hidden-layer structure significantly, even with only 200 hidden units in each layer. The results demonstrate the EBP works well on MNNs. Another observation is that EBP-P outperforms EBP-D, which is consistent with the results shown in Soudry et al. (2014). Particularly, the performance of B-EBP-D in the two-hidden-layer structure is worse than that of in the one-hidden-layer structure.", "startOffset": 54, "endOffset": 756}], "year": 2017, "abstractText": "Compared to Multilayer Neural Networks with real weights, Binary Multilayer Neural Networks (BMNNs) can be implemented more efficiently on dedicated hardware. BMNNs have been demonstrated to be effective on binary classification tasks with Expectation BackPropagation (EBP) algorithm on high dimensional text datasets. In this paper, we investigate the capability of BMNNs using the EBP algorithm on multiclass image classification tasks. The performances of binary neural networks with multiple hidden layers and different numbers of hidden units are examined on MNIST. We also explore the effectiveness of image spatial filters and the dropout technique in BMNNs. Experimental results on MNIST dataset show that EBP can obtain 2.12% test error with binary weights and 1.66% test error with real weights, which is comparable to the results of standard BackPropagation algorithm on fully connected MNNs.", "creator": "LaTeX with hyperref package"}}}