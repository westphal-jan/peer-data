{"id": "1503.03594", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "Efficient Learning of Linear Separators under Bounded Noise", "abstract": "We study the learnability of linear separators in $\\Re^d$ in the presence of bounded (a.k.a Massart) noise. This is a realistic generalization of the random classification noise model, where the adversary can flip each example $x$ with probability $\\eta(x) \\leq \\eta$. We provide the first polynomial time algorithm that can learn linear separators to arbitrarily small excess error in this noise model under the uniform distribution over the unit ball in $\\Re^d$, for some constant value of $\\eta$. While widely studied in the statistical learning theory community in the context of getting faster convergence rates, computationally efficient algorithms in this model had remained elusive. Our work provides the first evidence that one can indeed design algorithms achieving arbitrarily small excess error in polynomial time under this realistic noise model and thus opens up a new and exciting line of research.", "histories": [["v1", "Thu, 12 Mar 2015 05:38:19 GMT  (1039kb,D)", "http://arxiv.org/abs/1503.03594v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CC", "authors": ["pranjal awasthi", "maria-florina balcan", "nika haghtalab", "ruth urner"], "accepted": false, "id": "1503.03594"}, "pdf": {"name": "1503.03594.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning of Linear Separators under Bounded Noise", "authors": ["Pranjal Awasthi", "Maria-Florina Balcan", "Nika Haghtalab", "Ruth Urner"], "emails": ["pawashti@cs.princeton.edu", "ninamf@cs.cmu.edu", "nhaghtal@cs.cmu.edu", "rurner@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "Furthermore, we show that common algorithms such as minimizing hinge losses and averaging cannot lead to arbitrarily small excess errors under measurement noise, even under uniform distribution. Instead, we use a margin-based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that only logarithmically represents the desired excess error."}, {"heading": "1 Introduction", "text": "In fact, it is so that most people who are able to surpass themselves are able to surpass themselves. In fact, it is so that they are able to surpass themselves. In the other world, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves. In the third world, in the third world, third world, third world, third world, third world and third world, third world, third world, third world, third world, third world, third world, third world, third world, third world, third world, third world, third world, third world, third world, third world."}, {"heading": "2 Preliminaries", "text": "We are looking at the binary classification problem; that is, we are working on the problem of predicting a binary label y for a given instance x. We are assuming that the data points (x, y) from an unknown underlying distribution D * over X \u00b7 Y, where X = < d is the instance space and Y = {\u2212 1, 1} is the label space w. For the purpose of this work, we are looking at distributions in which the marginal distribution D * over X is a uniform distribution on a d-dimensional unit sphere. We are working with the class of all homogeneous half spaces designated by H = {sign (w \u00b7 x)."}, {"heading": "3 Computationally Efficient Algorithm for Massart Noise", "text": "Our main results are as follows: - Theorem 1 - 2 - 3 - 4 - 4 - 4 - 4 - 4 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5"}, {"heading": "5 Hinge Loss Minimization Does Not Work", "text": "In this section, we show that we are able to realize to ourselves that it can lead to a large 0 / 1 loss in our attitude. [6] However, the lower level in this paper is associated with significant distribution losses. [7] Here, we show that the lower level is associated with significant distribution losses. [7] Here, we show that the lower level is associated with significant distribution losses. [8] We show that the lower level is associated with significant distributions at discrete points with the label tilted (which is not possible) at a very large distance from the optimal classification. [9] The result makes heavy use of the hinge losses at a long distance."}, {"heading": "6 Conclusions", "text": "Our work is the first to provide a computationally efficient algorithm within the framework of the Massart Noise Model, a distributional assumption identified in statistical learning to achieve rapid (statistical) convergence rates. Although both computational and statistical efficiency are critical in machine learning applications, computational and statistical complexity has been studied under different assumptions and models. We view our findings on the computational complexity of learning under Massart Noise as a step towards bringing these two areas of research closer together. We hope that this will stimulate further work to identify situations that lead to both computational and statistical efficiency in order to ultimately shed light on the underlying relationships and dependencies of these two important aspects of automated learning.Recognition This work has been partially supported by NSF grants CCF-0953192, CCF-1451177, CCF1422910, Slowship Fellowship Fellowship Fellowship at Google Research, and a Microsoft Research Fellowship Award."}, {"heading": "A Probability Lemmas For The Uniform Distribution", "text": "The following probability lemmas are used during this thesis. Variation of these lemmas is presented in previous work with respect to their asymptotic behavior (2, 4, 22). Here, we focus on finding boundaries that are narrow even when the constants are affected. Indeed, the improved constants within these boundaries are for tolerating mass noise with \u03b2 > 1 \u2212 3,6 \u00b7 10 \u2212 6. During this section, D is used as uniform distribution over a d-dimensional sphere. Let f () be the p.d.f. of D. Let Vd be the volume of a d-dimensional unit sphere. Ratios between the volumes of the unit sphere in different dimensions is commonly used to find the probability mass of different regions under the uniform distribution."}, {"heading": "B Proofs of Margin-based Lemmas", "text": "The evidence of Lemma 1 Let's (w) \u2212 k \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212"}, {"heading": "D Hinge Loss Minimization", "text": "In this section, we show that the reduction of distribution losses in our lineup is not consistent, which means that they do not result in an arbitrary excess of distribution losses. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p."}, {"heading": "L\u03c4 (hw\u2217) = 2 \u00b7 (\u03b7(dA + dB) + (1\u2212 \u03b7)(cA + cB) + cD) .", "text": "For hw, note that the range B \u2212 dD = dC and cA + cB \u2212 dA = dA = dA = dA = dA = dA = dA = dA = dA = dA = dA = dA (and vice versa), so the roles of B and D are exchanged for hw (dA + dD) + (1 \u2212 dB) (dA + cD) + dB \u2212 dB), resulting in the range B \u2212 dW (hw) \u2212 dW (hw) = 2 \u00d7 (hw) (hw) (dA + dD) \u2212 dB \u2212 dD) + cB. We now define the range C as the angle between the two ranges \u2212 dA / 2 and cB + cB \u2212 dA / 2 of w (see Figure 3), leaving cC and dC analogous to the ones above. Note: dA \u2212 dB \u2212 dD \u2212 dD = cD and cB = dA + dB = dB = \u2212 dB and vice versa \u2212 \u2212 dB (dB = dB)."}], "references": [{"title": "The hardness of approximate optima in lattices, codes, and systems of linear equations", "author": ["Sanjeev Arora", "L\u00e1szl\u00f3 Babai", "Jacques Stern", "Z. Sweedyk"], "venue": "In Proceedings of the 34th IEEE Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "The power of localization for efficiently learning linear separators with noise", "author": ["Pranjal Awasthi", "Maria Florina Balcan", "Philip M. Long"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Agnostic active learning", "author": ["Maria-Florina Balcan", "Alina Beygelzimer", "John Langford"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Margin based active learning", "author": ["Maria-Florina Balcan", "Andrei Z. Broder", "Tong Zhang"], "venue": "In Proceedings of the 20th Annual Conference on Learning Theory (COLT),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Statistical active learning algorithms", "author": ["Maria-Florina Balcan", "Vitaly Feldman"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Minimizing the misclassification error rate using a surrogate convex loss", "author": ["Shai Ben-David", "David Loker", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A polynomial-time algorithm for learning noisy linear threshold functions", "author": ["Avrim Blum", "Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "The simplex method, volume 1 of Algorithms and Combinatorics: Study and Research Texts", "author": ["Karl-Heinz Borgwardt"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Theory of classification: a survey of recent advances", "author": ["Olivier Bousquet", "St\u00e9phane Boucheron", "Gabor Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Minimax bounds for active learning", "author": ["Rui M. Castro", "Robert D. Nowak"], "venue": "In Proceedings of the 20th Annual Conference on Learning Theory, (COLT),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["Nello Cristianini", "John Shawe-Taylor"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "From average case complexity to improper learning complexity", "author": ["Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Coarse sample complexity bounds for active learning", "author": ["Sanjoy Dasgupta"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Active learning", "author": ["Sanjoy Dasgupta"], "venue": "Encyclopedia of Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A general agnostic active learning algorithm", "author": ["Sanjoy Dasgupta", "Daniel Hsu", "Claire Monteleoni"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Selective sampling and active learning from single and multiple teachers", "author": ["Ofer Dekel", "Claudio Gentile", "Karthik Sridharan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Yoav Freund", "H. Sebastian Seung", "Eli Shamir", "Naftali Tishby"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Hardness of learning halfspaces with noise", "author": ["Venkatesan Guruswami", "Prasad Raghavendra"], "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Steve Hanneke"], "venue": "In Proceedings of the 24rd International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Theory of disagreement-based active learning", "author": ["Steve Hanneke"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Surrogate losses in passive and active learning", "author": ["Steve Hanneke", "Liu Yang"], "venue": "CoRR, abs/1207.3772,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Agnostically learning halfspaces", "author": ["Adam Tauman Kalai", "Adam R. Klivans", "Yishay Mansour", "Rocco A. Servedio"], "venue": "SIAM Journal on Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "On agnostic boosting and parity learning", "author": ["Adam Tauman Kalai", "Yishay Mansour", "Elad Verbin"], "venue": "In Proceedings of the 40th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Learning in the presence of malicious errors (extended abstract)", "author": ["Michael J. Kearns", "Ming Li"], "venue": "In Proceedings of the 20th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Toward efficient agnostic learning", "author": ["Michael J. Kearns", "Robert E. Schapire", "Linda Sellie"], "venue": "In Proceedings of the 5th Annual Conference on Computational Learning Theory (COLT),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1992}, {"title": "Embedding hard learning problems into gaussian space. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques", "author": ["Adam R. Klivans", "Pravesh Kothari"], "venue": "(AP- PROX/RANDOM),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Learning halfspaces with malicious noise", "author": ["Adam R. Klivans", "Philip M. Long", "Rocco A. Servedio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Risk bounds for statistical learning", "author": ["Pascal Massart", "lodie Ndlec"], "venue": "The Annals of Statistics, 34(5):2326\u20132366,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "A formal model of hierarchical concept learning", "author": ["Ronald L. Rivest", "Robert H. Sloan"], "venue": "Information and Computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Efficient algorithms in computational learning theory", "author": ["Rocco A. Servedio"], "venue": "Harvard University,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}], "referenceMentions": [{"referenceID": 10, "context": "In the absence of noise (when the data is realizable) such algorithms exist via linear programming [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "Such strong guarantees are only known for the well studied random classification noise model [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 27, "context": "In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9].", "startOffset": 165, "endOffset": 169}, {"referenceID": 8, "context": "In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9].", "startOffset": 255, "endOffset": 258}, {"referenceID": 6, "context": "example x is flipped independently with equal probability \u03b7, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] \u2014 note that all these results crucially exploit the high amount of symmetry present in the RCN noise.", "startOffset": 193, "endOffset": 203}, {"referenceID": 29, "context": "example x is flipped independently with equal probability \u03b7, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] \u2014 note that all these results crucially exploit the high amount of symmetry present in the RCN noise.", "startOffset": 193, "endOffset": 203}, {"referenceID": 4, "context": "example x is flipped independently with equal probability \u03b7, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] \u2014 note that all these results crucially exploit the high amount of symmetry present in the RCN noise.", "startOffset": 193, "endOffset": 203}, {"referenceID": 24, "context": "At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24].", "startOffset": 168, "endOffset": 172}, {"referenceID": 22, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 204, "endOffset": 215}, {"referenceID": 26, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 204, "endOffset": 215}, {"referenceID": 1, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 204, "endOffset": 215}, {"referenceID": 1, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 349, "endOffset": 352}, {"referenceID": 11, "context": "In fact, recent evidence shows that this is unavoidable for polynomial time algorithms for such adversarial noise models [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "Our Results In this work we identify a realistic and widely studied noise model in the statistical learning theory, the so called Massart noise [9], for which we can prove much stronger guarantees.", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "From a statistical point of view, it is well known that under this model, we can get faster rates compared to worst case joint distributions [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 28, "context": "In computational learning theory, this noise model was also studied, but under the name of malicious misclassification noise [29, 31].", "startOffset": 125, "endOffset": 133}, {"referenceID": 29, "context": "As a result, as we show in our work (see Section 4), standard algorithms such as the averaging algorithm [30] which work for random noise can only achieve a much poorer excess error (as a function of \u03b7) under Massart noise.", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "Specifically, we analyze a recent margin based algorithm of [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "By using new structural insights, we show that there exists a constant \u03b7 (independent of the dimension), so that if we use Massart noise where the flipping probability is upper bounded by \u03b7, we can use a modification of the algorithm in [2] and achieve arbitrarily small excess error.", "startOffset": 237, "endOffset": 240}, {"referenceID": 5, "context": "While there exists earlier work showing that hinge loss minimization can lead to classifiers of large 0/1-loss [6], the lower bounds in that paper employ distributions with significant mass on discrete points with flipped label (which is not possible under Massart noise) at a very large distance from the optimal classifier.", "startOffset": 111, "endOffset": 114}, {"referenceID": 18, "context": "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.", "startOffset": 186, "endOffset": 198}, {"referenceID": 12, "context": "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.", "startOffset": 186, "endOffset": 198}, {"referenceID": 19, "context": "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.", "startOffset": 186, "endOffset": 198}, {"referenceID": 3, "context": "We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].", "startOffset": 122, "endOffset": 129}, {"referenceID": 19, "context": "We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].", "startOffset": 208, "endOffset": 219}, {"referenceID": 17, "context": "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].", "startOffset": 208, "endOffset": 219}, {"referenceID": 11, "context": "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].", "startOffset": 208, "endOffset": 219}, {"referenceID": 22, "context": "For this model, under our distributional assumptions, [23] provides an algorithm that learns linear separators in <d to excess error at most , but whose running time poly(dexp(1/ )).", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "Recent work show evidence that the exponential dependence on 1/ is unavoidable in this case [26] for the agnostic case.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 12, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 2, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 3, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 18, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 14, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 9, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 13, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 19, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 4, "context": "However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.", "startOffset": 96, "endOffset": 99}, {"referenceID": 15, "context": "However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "We note that work of [21] provides computationally efficient algorithms for both passive and active learning under the assumption that the hinge loss (or other surrogate loss) minimizer aligns with the minimizer of the 0/1-loss.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "The algorithm described above is similar to that of [2] and uses an iterative margin-based approach.", "startOffset": 52, "endOffset": 55}, {"referenceID": 26, "context": "We satisfy the base case by using an algorithm of [27].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "Using these insights, we show that the algorithm by [2] can indeed achieve a much stronger guarantee, namely arbitrarily small excess error in presence of Massart noise.", "startOffset": 52, "endOffset": 55}, {"referenceID": 26, "context": "Take poly(d, 1\u03b4 ) samples and run poly(d, 1 \u03b4 )-time algorithm by [27] to find a half-spacew0 with excess error 0.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "Overview of our analysis: Similar to [2], we divide errD(wk) to two categories; error in the band, i.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "1 of [2] for uniform)", "startOffset": 5, "endOffset": 8}, {"referenceID": 26, "context": "For k = 0, we use the algorithm for adversarial noise model by [27], which can achieve excess error of if errD\u0303(w \u2217) < 2 256 log(1/ ) (Refer to Appendix C for more details).", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "where the last transition holds by the fact that Vd\u22121 Vd \u2264 \u221a d+1 2\u03c0 [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 29, "context": "The Average algorithm introduced by [30] is another computationally efficient algorithm that has provable noise tolerance guarantees under certain noise models and distributions.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "Furthermore, even in the presence of a small amount of malicious noise and less symmetric distributions, Average has been used to obtain a weak learner, which can then be boosted to achieve a non-trivial noise tolerance [27].", "startOffset": 220, "endOffset": 224}, {"referenceID": 5, "context": "It has been shown earlier that hinge loss minimization can lead to classifiers of large 0/1-loss [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "[1] Sanjeev Arora, L\u00e1szl\u00f3 Babai, Jacques Stern, and Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Pranjal Awasthi, Maria Florina Balcan, and Philip M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Maria-Florina Balcan, Alina Beygelzimer, and John Langford.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Maria-Florina Balcan, Andrei Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Maria-Florina Balcan and Vitaly Feldman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Avrim Blum, Alan Frieze, Ravi Kannan, and Santosh Vempala.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Karl-Heinz Borgwardt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Olivier Bousquet, St\u00e9phane Boucheron, and Gabor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Rui M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Nello Cristianini and John Shawe-Taylor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Amit Daniely, Nati Linial, and Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Sanjoy Dasgupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Sanjoy Dasgupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Sanjoy Dasgupta, Daniel Hsu, and Claire Monteleoni.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Ofer Dekel, Claudio Gentile, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Yoav Freund, H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Venkatesan Guruswami and Prasad Raghavendra.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Steve Hanneke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Steve Hanneke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Steve Hanneke and Liu Yang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Adam Tauman Kalai, Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Adam Tauman Kalai, Yishay Mansour, and Elad Verbin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Michael J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Michael J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Pascal Massart and lodie Ndlec.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Ronald L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 3, "context": "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 21, "context": "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 7, "context": "The following bound due to [8] proves useful in our analysis.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "3 of [2], `(w, x, y) = O( \u221a d) for all (x, y) \u2208 Swk\u22121,bk\u22121 and \u03b8(w,wk\u22121) \u2264 rk.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "2 of [2].", "startOffset": 5, "endOffset": 8}, {"referenceID": 26, "context": "We initialize our margin based procedure with the algorithm from [27].", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "The guarantees mentioned in [27] hold as long as the noise rate is \u03b7 \u2264 c 2 log 1/ .", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "[27] do not explicitly compute the constant but it is easy to check that c \u2264 1 256 .", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "This can be computed from inequality 17 in the proof of Lemma 16 in [27].", "startOffset": 68, "endOffset": 72}], "year": 2015, "abstractText": "We study the learnability of linear separators in < in the presence of bounded (a.k.a Massart) noise. This is a realistic generalization of the random classification noise model, where the adversary can flip each example xwith probability \u03b7(x) \u2264 \u03b7. We provide the first polynomial time algorithm that can learn linear separators to arbitrarily small excess error in this noise model under the uniform distribution over the unit ball in <, for some constant value of \u03b7. While widely studied in the statistical learning theory community in the context of getting faster convergence rates, computationally efficient algorithms in this model had remained elusive. Our work provides the first evidence that one can indeed design algorithms achieving arbitrarily small excess error in polynomial time under this realistic noise model and thus opens up a new and exciting line of research. We additionally provide lower bounds showing that popular algorithms such as hinge loss minimization and averaging cannot lead to arbitrarily small excess error under Massart noise, even under the uniform distribution. Our work instead, makes use of a margin based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that is only a logarithmic the desired excess error .", "creator": "LaTeX with hyperref package"}}}