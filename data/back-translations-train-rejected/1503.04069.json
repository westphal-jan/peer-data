{"id": "1503.04069", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Mar-2015", "title": "LSTM: A Search Space Odyssey", "abstract": "Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.", "histories": [["v1", "Fri, 13 Mar 2015 14:01:38 GMT  (1306kb,D)", "http://arxiv.org/abs/1503.04069v1", "10 pages, 5 figures plus 8 pages, 6 figures supplementary"], ["v2", "Wed, 4 Oct 2017 11:40:31 GMT  (5794kb,D)", "http://arxiv.org/abs/1503.04069v2", "12 pages, 6 figures"]], "COMMENTS": "10 pages, 5 figures plus 8 pages, 6 figures supplementary", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["klaus greff", "rupesh kumar srivastava", "jan koutn\\'ik", "bas r steunebrink", "j\\\"urgen schmidhuber"], "accepted": false, "id": "1503.04069"}, "pdf": {"name": "1503.04069.pdf", "metadata": {"source": "META", "title": "LSTM: A Search Space Odyssey", "authors": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "emails": ["KLAUS@IDSIA.CH", "RUPESH@IDSIA.CH", "HKOU@IDSIA.CH", "BAS@IDSIA.CH", "JUERGEN@IDSIA.CH"], "sections": [{"heading": "1. Introduction", "text": "Recurring neural networks with long short-term memory (which we will tersely refer to as LSTM) have emerged as an effective and scalable model for multiple learning problems associated with sequential data. Previous methods of addressing these problems have typically been hand-designed workarounds to deal with the sequential nature of data such as speech and audio signals. Since LSTMs are effective at capturing long-term time dependencies without suffering from the optimization barriers afflicting simple recurring networks (SRNs) (Hochreiter, 1991; Bengio et al., 1994), they have been used to capture the state of the art for many difficult problems, including handwritten recognition methods (Graves et al., 2009; Pham et al., 2014) and generation (Graves et al.) that models language (Zaremba et al., 2014) and translation."}, {"heading": "2. Vanilla LSTM", "text": "The most widely used LSTM architecture in the literature was originally described by Graves & Schmidhuber (2005), we refer to it as vanilla LSTM and use it as a reference for comparing all variants. Vanilla LSTM takes into account changes made by Gers et al. (1999) and Gers & Schmidhuber (2000) to the original LSTM (Hochreiter & Schmidhuber, 1997) and uses complete gradient training. \u2212 Section 3 contains descriptions of these important LSTM changes. A schematic of the vanilla LSTM block is shown in Figure 1. It features three gates (input, forgetting and output), block input, a single cell (the constant error carousel), an output activation function and peephole connections."}, {"heading": "3. History of LSTM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Original Formulation", "text": "This original version of the LSTM block (Hochreiter & Schmidhuber, 1995; 1997) contained (possibly multiple) cells, input and output gates, but no forget-me-not and peephole connections. In certain experiments, output gate, unit bias, or input activation functions were omitted; the training was performed using a mixture of Real Time Recurrent Learning (RTRL) and Backpropogation Through Time (BPTT); only the cell's gradient was propagated back over time, and the gradient for the other recurring connotations was cut off; therefore, the exact gradient for the training was not used in this study; another feature of this version was the use of full gate repetition, meaning that all gates received recurring inputs in the previous time step alongside the recurring inputs from the block exits; this feature did not appear in any of the later work."}, {"heading": "3.2. Forget Gate", "text": "The first paper proposing a modification of the LSTM architecture introduced the Forge Gate (Gers et al., 1999), which enabled the LSTM to reset its own state, enabling the learning of continuous tasks such as embedded vine grammar."}, {"heading": "3.3. Peephole Connections", "text": "Gers & Schmidhuber (2000) argued that in order to obtain precise timings, the cell must control the gates, which was previously only possible through an open exit gate. Peephole connections (connections from the cell to the gates, blue in Figure 1) were added to the architecture to make precise timings easier to learn, and the function of output activation was omitted as there was no evidence that it was indispensable for solving the problems on which LSTM had previously been tested."}, {"heading": "3.4. Full Gradient", "text": "The last modification to vanilla LSTM was carried out by Graves & Schmidhuber (2005), which presented full back propagation through time (BPTT) training for LSTM networks with the architecture described in Section 2, and presented results on the TIMIT benchmark. Full BPTT had the added benefit of allowing LSTM gradients to be checked for finite differences, making practical implementations more reliable."}, {"heading": "3.5. Other Variants", "text": "Prior to the introduction of full BPTT training, Gers et al. (2002) used a training method based on Extended Kalman Filtering that enabled the LSTM to be trained on some pathological cases at the expense of high computational complexity. Schmidhuber et al. (2007) proposed to use a hybrid evolution-based method for training instead of BPTT, but retained the vanilla LSTM architecture. Bayer et al. (2009) developed various LSTM block architectures that maximize fitness on context-sensitive grammars. Sak et al. (2014) introduced a linear projection layer that projects the output of the LSTM layer downwards, before recurring and subsequent connections were established to reduce the number of parameters for LSTM networks with many blocks. By introducing a portable scaling parameter for increasing the activation function of the gate (ATet gate), an M (2014) led to an improvement in gate-gate function."}, {"heading": "4. Evaluation Setup", "text": "The focus of our study is on comparing different LSTM variants, not on obtaining current results. Therefore, our experiments are designed to be simple and the comparisons fair; the vanilla LSTM is used as a baseline and evaluated along with eight of its variants; each variant adds, removes or modifies the baseline in exactly one aspect, isolating its effect; three different sets of data from different domains are used to account for cross-domain variations.Since the hyperparameter space is large and cannot be fully traversed, random searches have been used to obtain the most powerful hyperparameters (Bergstra & Bengio, 2012) for each combination of variant and dataset; then, all analyses focused on the 10% best-performing studies for each variant and dataset (Section 5.1), making the results representative of the appropriate hyperparameter settings."}, {"heading": "4.1. Datasets", "text": "Each data set is divided into three parts: a training set, a validation set used to stop early and optimize hyperparameters, and a test set for final evaluation. Details of the pre-processing of each data set are included in the supplementary material."}, {"heading": "4.1.1. TIMIT", "text": "The TIMIT speech corpus (Garofolo et al., 1993) is large enough to be a reasonable acoustic yardstick for speech recognition, but small enough to keep a large study like ours manageable. Our experiments focus on the frame-by-frame classification task for this data set, with the goal of classifying each audio frame as one of 61 phones. Performance is measured as a percentage of classification errors. According to Halberstadt (1998), the training, testing and validation sets are divided into 3696, 400 and 192 sequences, averaging 304 frames."}, {"heading": "4.1.2. IAM ONLINE", "text": "The IAM Online Handwriting Database (Liwicki & Bunke, 2005) consists of English sets as time series of spring movements that must be mapped to letters. It uses four input functions: changing the pen's X and Y positions, the time since the current stroke, and a binary value that indicates whether the pen is lifted. Performance is measured by the Character Error Rate (CER) after decryption. Subsampling halved the size of the data set, allowing the experiments to run 2 x faster without affecting performance. Training, testing, and validation sets included 5355, 2956, 3859 sequences with an average length of 334 images."}, {"heading": "4.1.3. JSB CHORALES", "text": "JSB Chorales (Allan & Williams, 2005) is a multi-voice music modeling dataset consisting of binary vector sequences whose task is to predict the next step. The performance metric used is the negative log probability on the validation / test record. The complete dataset consists of 229, 76 and 77 sequences, respectively, with an average length of 61."}, {"heading": "4.2. Network Architectures & Training", "text": "For the loss function we used CrossEntropy Error for TIMIT and JSB Chorales, while for the IAM online task the Connectionist Temporal Classification (CTC) Error by Graves et al. (2006) was used. Initial weights for all networks were drawn from a normal distribution with a standard deviation of 0.1. Training was performed using stochastic gradient descent with Nesterov-style dynamics (Sutskever et al., 2013) with updates after each sequence. The learning rate was compressed by a factor (1 dynamics), using Nesterov-style gradients descent (Sutskever et al., 2013)."}, {"heading": "4.3. LSTM Variants", "text": "The vanilla LSTM from section 2 is referred to as vanilla (V.) The derived eight variants of the V architecture are the following: 1. No Input Gate (NIG) 2. No Forget Gate (NFG) 3. No Output Gate (NOG) 4. No Input Activation Function (NIAF) 5. No Output Activation Function (NOAF) 6. No Peepholes (NP) 7. Coupled Input and Forget Gate (CIFG) 8. Full Gate Recurrence (FGR) The first six variants are self-explanatory. The CIFG variant uses only one gate to evaluate both the input and the recurring self-connection of the cell (an LSTM modification proposed in GRU (Cho et al., 2014). This corresponds to the ft = 1 setting - rather than learning the weights of the Forget Gate independently."}, {"heading": "4.4. Hyperparameter Search", "text": "While there are other methods to efficiently search for good hyperparameters (cf. Snoek et al. 2012; Hutter et al. 2011), the random search has a few advantages for our attitude: It is easy to implement, trivial to parallelise, and covers the search space more uniformly, thereby improving the follow-up analysis of the hyperparameter meaning. Each hyperparameter search consists of 200 attempts (for a total of 5400 attempts) of randomly selected hyperparameters: \u2022 Number of LSTM blocks per hidden layer: Log uniform samples from [20, 200]; \u2022 Learning rate: Log uniform samples from [10 \u2212 6, 10 \u2212 2]; \u2022 Dynamics: 1 \u2212 Log uniform samples from [0.01, 1.0]; \u2022 Standard deviation of Gaussian input noise: Uniform samples from [0, 1]. In the case of the TIMIT dataset, two additional (Bosche) data sets for the hyper parameters were not taken into account."}, {"heading": "5. Results & Discussion", "text": "Each of the 5,400 experiments was performed on one of 128 AMD Opteron 2.5 GHz CPUs and took an average of 24.3 hours, resulting in a total computation time of just under 15 years. For TIMIT and JSB Chorales, the test set performance of the best setup was 29.6% classification error (CIFG) or a log probability of -8.38 (NIG). For the IAM online dataset, our best result was a character error rate of 9.26% (NP) on the test set. The best previously published result is 11.5% CER from Graves et al. (2008) using a different and much more extensive pre-processing. 3"}, {"heading": "5.1. Comparison of the Variants", "text": "Welch's t test at a significance level of p = 0.05 was used to determine whether the average test set performance of each variant differs significantly from that of the baseline. A summary of the results is shown in Figure 2. The field for each variant for which the mean is significantly different from the baseline is highlighted in blue. The mean number of parameters used by the twenty most powerful networks is also displayed as a gray bar graph in the background."}, {"heading": "5.1.1. GENERAL OBSERVATIONS", "text": "The first important observation, based on Figure 2, is that the removal of the Output Activation Function (NOAF) or the Forge Gate (NFG) significantly impairs performance on all three datasets. Apart from the CEC, the ability to forget old information and the crushing of the cell state seem to be critical to the LSTM architecture. This is the case because we only specify the best functioning setup determined on the validation set. In Figure 2, on the other hand, we show the test set performance of the 20 best setups for each variant. 4We applied the Bonferroni customization to correct for performing eight different tests (one for each variant)."}, {"heading": "5.1.2. TASK-SPECIFIC OBSERVATIONS", "text": "The removal of the Input Gate (NIG), Output Gate (NOG) and Input Activation Function (NIAF) resulted in a significant reduction in the performance of speech and handwriting recognition. However, there was no significant effect on the performance of music modeling. A small (but statistically insignificant) average improvement in performance was observed for the NIG and NIAF architecture in music modeling, and we expect these behaviors to extend to problems similar to speech modeling. To monitor the learning of continuously real data (such as speech and handwriting recognition), the Input Gate, Output Gate and Input Activation Function are critical to achieving good performance."}, {"heading": "5.2. Impact of Hyperparameters", "text": "The fANOVA framework for assessing the importance of hyperparameters by Hutter et al. (2014) is based on the observation that dimensional marginalization can be efficiently performed in regression trees, which enables the prediction of the boundary error for one hyperparameter space while averaging over all others. Traditionally, this would require a complete search for hyperparameters, whereas here the hyperparameter space can be randomly sampled. Average performance for each section of hyperparameter space is achieved by first training a regression tree and then summing up its predictions along the corresponding subset of dimensions. To be precise, a random regression forest of 100 trees is trained and their prediction power averaged, which improves the generalization and enables an assessment of the uncertainty of these predictions. The obtained marginals can then be used to determine the variance in additive components by means of the functional analogy of their significance (ANANfVA), providing an insight into their significance in the ANFANOVA method (2007)."}, {"heading": "5.2.1. ANALYSIS OF VARIANCE", "text": "Figure 3 shows which fraction of the variance in test set performance is due to different hyperparameters. It is obvious that the learning rate is by far the most important hyperparameter, always accounting for more than two-thirds of the variance. The next most important hyperparameter is the hidden layer size, followed by the input noise, leaving the impulse with less than one percent of the variance. Higher order interactions play an important role in the case of TIMIT, but are much less important in the other two datasets."}, {"heading": "5.2.2. LEARNING RATE", "text": "The learning rate is the most important hyperparameter, so it is very important to understand how to set it correctly to achieve good performance. Figure 4 shows (in blue) how setting the learning rate value affects the predicted average performance on the test set. It is important to note that this is an average over all other hyperparameters and over all trees in the regression forest. Diagrams in Figure 3 show that the optimal value for the learning rate depends on the data set (not other hyperparameters), which quantifies the reliability of the average. The same is shown in green with the predicted average training time. Diagrams in Figure 3 show that the optimal value for the learning rate depends on the data set. For each data set there is a large pool (up to two orders of magnitude) of good learning rates within which the performance does not vary greatly. A related but unsurprising observation is that there is a sweet spot at the upper end of the learning spot."}, {"heading": "5.2.3. HIDDEN LAYER SIZE", "text": "Unsurprisingly, the hidden layer size is an important hyperparameter that affects LSTM network performance; larger networks are expected to perform better, and the required training time increases with network size."}, {"heading": "5.2.4. INPUT NOISE", "text": "Additive Gaussian noise at the inputs, a traditional regulator of neural networks, was also used for LSTM. However, we note that it not only almost always affects performance, but also slightly increases training times, with the only exception being TIMIT, where a small drop in the range of [0.2, 0.5] is observed."}, {"heading": "5.2.5. MOMENTUM", "text": "An unexpected finding of this study is that dynamics does not significantly affect performance or training time, which follows from the observation that for none of the datasets dynamics accounts for more than 1% of the variance in test set performance. It should be noted that for TIMIT, the interaction between learning rate and dynamics accounts for 2.5% of the total variance, but as for the learning rate x hidden size (see Section 5.2.6) has no interpretable structure, which could be the result of our decision to scale learning rates depending on dynamics (Section 4.2). These observations suggest that momentary5Note that it does not provide significant benefits for IAM Online and JSB Chorales outside the stated range when training LSTMs with stochastic online gradient descent."}, {"heading": "5.2.6. INTERACTION OF HYPERPARAMETERS", "text": "Here, we focus on higher order interactions for the TIMIT dataset where they were strongest, but our analysis showed very similar behavior for the other datasets: learning rate \u00b7 hidden size = 6.7% learning rate \u00b7 input noise = 4.4% hidden size \u00b7 input noise = 2.0% learning rate \u00b7 impulse = 1.5% impulse \u00b7 hidden size = 0.6% impulse \u00b7 input noise = 0.4% The interaction between learning rate and hidden size is the strongest, but Figure 5 shows no systematic dependence between the two. In fact, further samples may be needed to properly analyze the fine interaction between them, but given our observations to date, this may not be worthwhile. In any case, it is clear that the variation of the hidden size does not change the region of the optimal learning rate."}, {"heading": "6. Conclusion", "text": "This paper reports on the results of a large-scale study of variations in the LSTM architecture. We conclude that: \u2022 The most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on different datasets and the use of one of eight possible modifications does not significantly improve LSTM performance. \u2022 Certain modifications, such as coupling the input and forgetting gates or removing peephole connections, simplify LSTM without significantly affecting performance. \u2022 The forgetting gate and output activation function are the critical components of the LSTM block. While the first is critical for LSTM performance, the second is necessary when the cell state is unlimited. \u2022 Learning rate and network size are the most important adjustable LSTM hyperparameters. Surprisingly, the use of dynamics turned out to be unimportant (in our Setting of Online Gracent)."}, {"heading": "Acknowledgments", "text": "This research was supported by the scholarships of the Swiss National Science Foundation \"Theory and Practice of Reinforcement Learning 2\" (# 138219) and \"Advanced Reinforcement Learning\" (# 156682) as well as by the EU projects \"NASCENCE\" (FP7-ICT-317662) and \"NeuralDynamics\" (FP7-ICT-270247)."}, {"heading": "A. LSTM formulas", "text": "Here we repeat the vectorized formulas for a vanilla LSTM layer forward (= = 1 = 1 = =) and then introduce the formulas for the reverse gear (?). We also present formulas for all variants studied. A.1. We reproduce the formulas of the forward gear from the paper, but we divide all gates and block input into the activity before (?) and after non-linearity (?).Let N be the number of LSTM blocks and M the number of inputs. Then we get the following weights: \u2022 Input weights: Wz, Ws, Wf, Where RN \u00b7 M Recurrent weights: Rz, Ro, Ro, Ro, RN \u00b7 Peephole weights: ps, po \u2022 RN \u00b7 Bias weights: bz, bs, bf, bo, RNAs in the newspaper we have xt as input vector at time, g, and h are pointed."}, {"heading": "B. Datasets", "text": "This year, the number of women in work who have chosen to study at university has multiplied, and the number of women in work who have chosen to study at university has doubled."}, {"heading": "C. Additional plots", "text": "In fact, most of them will be able to play by the rules they have established in the past."}], "references": [{"title": "Harmonising chorales by probabilistic inference", "author": ["References Allan", "Moray", "Williams", "Christopher KI"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Allan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Allan et al\\.", "year": 2005}, {"title": "Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription", "author": ["Boulanger-Lewandowski", "Nicolas", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": null, "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "DARPA TIMIT AcousticPhonetic Continuous Speech Corpus CD-ROM", "author": ["JS Garofolo", "LF Lamel", "WM Fisher", "JG Fiscus", "DS Pallett", "Dahlgren", "NL"], "venue": "National Institute of Standards and Technology, NTIS Order No PB91-505065,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Heterogeneous acoustic measurements and multiple classifiers for speech recognition", "author": ["Halberstadt", "Andrew K"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Halberstadt and K.,? \\Q1998\\E", "shortCiteRegEx": "Halberstadt and K.", "year": 1998}, {"title": "IAM-OnDB-an on-line English sentence database acquired from handwritten text on a whiteboard", "author": ["Liwicki", "Marcus", "Bunke", "Horst"], "venue": "In Document Analysis and Recognition,", "citeRegEx": "Liwicki et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Liwicki et al\\.", "year": 2005}, {"title": "Distance measures for speech recognition: Psychological and instrumental", "author": ["P. Mermelstein"], "venue": "Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Mermelstein,? \\Q1976\\E", "shortCiteRegEx": "Mermelstein", "year": 1976}], "referenceMentions": [{"referenceID": 2, "context": "TIMIT The TIMIT Speech corpus (Garofolo et al., 1993) is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable.", "startOffset": 30, "endOffset": 53}], "year": 2015, "abstractText": "Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\u2248 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.", "creator": "LaTeX with hyperref package"}}}