{"id": "1503.04250", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Mar-2015", "title": "The YLI-MED Corpus: Characteristics, Procedures, and Plans", "abstract": "The YLI Multimedia Event Detection corpus is a public-domain index of videos with annotations and computed features, specialized for research in multimedia event detection (MED), i.e., automatically identifying what's happening in a video by analyzing the audio and visual content. The videos indexed in the YLI-MED corpus are a subset of the larger YLI feature corpus, which is being developed by the International Computer Science Institute and Lawrence Livermore National Laboratory based on the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting one of ten target events, or no target event, and are annotated for additional attributes like language spoken and whether the video has a musical score. The annotations also include degree of annotator agreement and average annotator confidence scores for the event categorization of each video. Version 1.0 of YLI-MED includes 1823 \"positive\" videos that depict the target events and 48,138 \"negative\" videos, as well as 177 supplementary videos that are similar to event videos but are not positive examples. Our goal in producing YLI-MED is to be as open about our data and procedures as possible. This report describes the procedures used to collect the corpus; gives detailed descriptive statistics about the corpus makeup (and how video attributes affected annotators' judgments); discusses possible biases in the corpus introduced by our procedural choices and compares it with the most similar existing dataset, TRECVID MED's HAVIC corpus; and gives an overview of our future plans for expanding the annotation effort.", "histories": [["v1", "Fri, 13 Mar 2015 23:36:42 GMT  (445kb,D)", "http://arxiv.org/abs/1503.04250v1", "47 pages; 3 figures; 25 tables. Also published as ICSI Technical Report TR-15-001"]], "COMMENTS": "47 pages; 3 figures; 25 tables. Also published as ICSI Technical Report TR-15-001", "reviews": [], "SUBJECTS": "cs.MM cs.CL", "authors": ["julia bernd", "damian borth", "benjamin elizalde", "gerald friedland", "heather gallagher", "luke gottlieb", "adam janin", "sara karabashlieva", "jocelyn takahashi", "jennifer won"], "accepted": false, "id": "1503.04250"}, "pdf": {"name": "1503.04250.pdf", "metadata": {"source": "CRF", "title": "The YLI-MED Corpus: Characteristics, Procedures, and Plans", "authors": ["Julia Bernd Damian Borth", "Benjamin Elizalde", "Gerald Friedland", "Heather Gallagher", "Luke Gottlieb", "Adam Janin", "Sara Karabashlieva", "Jocelyn Takahashi", "Jennifer Won"], "emails": [], "sections": [{"heading": null, "text": "The YLI-MED annotation project described herein was funded by a grant from Cisco Systems, Inc., for Event Detection for Improved Speaker Diarization and Meeting Analysis. In addition to Cisco, ICSI's work on the YLI corpus (in general) is funded by the Lawrence Livermore National Laboratory as part of a collaborative Laboratory Directed Research and Development project under the auspices of the U.S. Department of Energy (Contract DE-AC52-07NA27344) and the National Science Foundation as part of SMASH: Scalable Multimedia Content AnalysiS in a High Language (Scholarship IIS-1251276). Any opinion, findings, conclusions or recommendations are provided by the authors and do not necessarily reflect the views of Cisco, LLNL, or NSF.ar Xiv: 150."}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Corpus-Collection Procedures 3", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Characteristics of the YLI-MED Corpus 17", "text": "The vast majority of respondents consider that the vast majority of respondents are able to change their minds. An overwhelming majority of respondents consider that the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority of the vast majority agrees with the vast majority of the vast majority of the vast majority."}, {"heading": "4 Limitations, Corpus Comparability, and Generalizability 30", "text": "4.1. Potential unintended distortions............................... 304.1.1 Biases arising from the YFCC100M dataset................ 304.1.2. Possible effects of the procedure on representativeness: YLI-MED vs. YFCC100M...... 31The YLI-MED Corpus / / / ICSI TR-15-001 14.1.3 Possible effects of the procedure on comparability: YLI-MED vs. TRECVID MED..... 324.1.4 Are there no biases?........."}, {"heading": "5 The Bigger Picture for YLI-MED 38", "text": "........................................................................................................................"}, {"heading": "6 Acknowledgments 39", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Event Definitions for the YLI-MED Corpus 42", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Corpus-Collection Procedures", "text": "The capture and annotation of the YLI-MED corpus index took place in a series of phases in the summer of 2014; each of these phases is described in this section. \u00a7 2.1 describes how we selected the target events and researched and developed \"definitions\" for these events. \u00a7 2.2 describes how the corpus builders identified and verified videos within the YFCC100M that represented the target events, and how we commented on them for features of interest to multimedia researchers. \u00a7 2.3 describes how we calculated the match and confidence of the commentators and finalized the index of the videos to be published (training, testing and supplementary videos) (including the \"Alternative\" training). \u00a7 2.4 describes how we (automatically) selected and (manually) refined a series of random \"negative\" videos that did not represent any of the target events.6"}, {"heading": "2.1 Choosing and Defining the Events", "text": "This year is more than ever before in the history of the European Union."}, {"heading": "2.2 Event Corpus Collection, Annotation, and Verification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Collection Procedures", "text": "An index of candidates was compiled by scouring the videos for targets and objectives."}, {"heading": "2.2.2 Annotation for Non-Event Characteristics", "text": "Given that most of them are able to identify themselves (whether they are titles or descriptions that are also included in the search results), the question is how effective collectors in other languages can be (the way it actually is). While linguists in the team are supported with brainstorming, there are of course some pertinent gaps. Flickr does well to associate search terms with other languages (changes become changes, changes, changes, etc.), the terms in other languages do not seem to be nearly comparable."}, {"heading": "2.2.3 Verification", "text": "After collecting (most of) the videos of the candidates (positives, near misses and the like) for the YLI-MED corpus, we began a process of review in which each video was reviewed by two additional people. To encourage the examiners to actually record their disagreement (where they disagreed), they were asked to record their judgments on the relationship of each video to the event and their confidence in their judgment using positive examples (rather than simply recording their confirmation / non-confirmation of the collector's comments - although they could see the judgments of the collector and each other); they were also asked to review the judgments of the collectors with respect to the additional comments (post-editing and language), but in the latter cases they were only asked to record their judgments if they differed from or were added to those of the collector; the instructions for the examiners largely reflected the judgments of the collectors (except that they could label a video as \"negative event,\" i.e. that some were not related to the example of the YLI-MED corpus)."}, {"heading": "2.3 Event Set Cleanup and Division into Training/Test", "text": "During the verification phase, the index of candidate videos was checked for duplicates and compared with the full YFCC100M index, so that we had an index of 2274 videos that could potentially be included in the event sets. Notes were also cleaned up and regulated as needed at this point. Upon completion of verification, we determined the consensus relationship to the event, the agreement among commentators, and the average annotator confidence for each video, as described in Section 2.3.1, the index to reduce bias against users with high uploads as described in Section 2.3.2 and Section 2.3.4, and divided the final list for each event into training, testing, and supplement sets as described in Section 2.3.3 and Section 2.3.5."}, {"heading": "2.3.1 Consensus Relations/Removing Unagreeable Videos", "text": "In fact, most of them will be able to play by the rules that they have shown in the past, and they will be able to play by the rules that they have shown in the past."}, {"heading": "2.3.2 Reducing User Bias", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...). (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...). It is. (...) It is. (...) It is. (...). It is. (...) It is. (...). It is. (...) It is. It is. (...). It is. (...) It is. (...). It is. (... It is. (...). It is. (...). It is. It is. (... It is. (...). It is. It is. (...). It is. (...). It is. (...). It is. It is. It is. (...). (... It is. It is. (...). It is. It is. It is. It is. It is. (...). It is. It is. It is. (...). It is. It is. It is. It is. It is. (...). (...). It is. It is. It is. It is. It is. It is. (). It is. (). It is. It is. (). It is. It is. It is. It is. It is. (). It is. (). It is. It is. (). It is. It is. (). It is. (). (). It is. (). It is. (). It is."}, {"heading": "2.3.3 Splitting into Training and Test Sets", "text": "After the candidate's video index had been verified, cleaned up and cropped, we divided each set of positive videos into standard training and test sets so that researchers could compare the results. We did not offer a separate \"development\" set, nor did we withhold any data for future ratings. We grouped the videos by user for each event category, and then selected random users to compile training sets of 100 videos each; the rest of the videos for each event were added to the test sets (from 39 to 137 events). 38 The selection of users to go in each set was random within the following constraints: \u2022 In cases where there were fewer than 200 videos for one event (i.e. in which the test set would be less than 100 videos), we narrowed the selection process so that the test set would have no more than 5% of its videos that came from a single user. This meant that in such cases the more productive users were assigned the standard videos that we tried to assign the supplement to the person for the snow (approximately 6 percent)."}, {"heading": "2.3.4 An Opportunity for Experimentation with User Bias", "text": "Although there is a general view in the multimedia retrieval community that too much of the data set is provided by a single user, and some evidence from specific use cases such as those mentioned in \u00a7 2.3.2, the degree of potential problems and the general parameters that may determine how they affect classification and retrieval outcomes have not yet been well researched. For researchers interested in exploring this area and how best to build robust systems that can handle such skews, we are publishing a separate alternative training pool for Ev105 person who is trying to perform a board trick (Training Alt) that includes both the 100 training videos from the main data set (default) and the 111 videos excluded from the index. 39 (The videos in the main test set are not included in the alternative Ev105 training index.) These 211 videos were provided by 48 users, with an uneven distribution of videos per user."}, {"heading": "2.3.5 A Note on Near Misses and Related Videos", "text": "Near misses and related videos were collected in the course of collecting the positive videos; although collectors were instructed to index all near misses and related videos, in practice they found a very spotty indexing. The majority of near misses and related videos in the final corpus index are 39 The index file is YLI-MED v.X Ev105 Alternate Training.txt. 40 This distribution is an effect of the corpus capture process and should not be taken as an indication of a \"natural\" distribution in the larger datasets. YLI-MED Corpus / / ICSI TR-15-001 14actual videos that collectors had classified as positive cases, but not the auditors. A few were also collected in the process of making the \"negative\" video set (Ev100 none of the above); see \u00a7 2.4.2.Overall, the collection process for near misses and related videos was not particularly systematic and reporting differed greatly between the events."}, {"heading": "2.4 Collection and Verification of Negative Videos", "text": "The \"negative\" videos (Ev100 None of the videos described above), i.e. those that do not depict any of the target events, largely constitute an \"untrustworthy,\" unconfirmed negative sentence, since 99% of them were not viewed individually to confirm that they did not represent cases of any of the target events.42 However, other measures have been taken to reduce the likelihood that they could be positive examples; for example, the potential negative videos compiled by random selection from all videos in the YFCC100M dataset were examined for text metadata that might indicate that they represented one of the target events.About 1% of the videos that do not represent any of the above videos were reviewed by a human commentator to confirm that they did not represent actual examples of any of the target events.These processes are detailed in the rest of this paragraph.It is worth noting that most of the videos were not reviewed individually, and the videos were not checked from 1 seconds to 2.3 seconds, so the sections were not complete."}, {"heading": "2.4.1 Selection and Metadata-Based Exclusion", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2.4.2 Creating the \u201cConfirmed\u201d Negative Set", "text": "Finally, we selected 500 random videos from the Ev100 (negative) index - about 1% - that were reviewed by at least one of the corpus builders. Within the 498 videos that were visible at the time of review, the examiners found 7 videos that they considered positive examples of our target events (1.4% of the negatives reviewed), as well as 4 near misses (0.8%) and 4 related videos (0.8%). These videos were excluded (or added to the event sets), and the rest were designated as confirmed \"high trust\" negatives (recorded in the Corpus file under the \"Neg Confirmation\" section). Since each video was viewed by only one examiner to confirm the results, we included some known positive videos (never seen before by the corpus builders) in the sets of negative videos that were to be reviewed."}, {"heading": "2.4.3 Division into Training and Test Sets", "text": "According to the procedures described in the previous subsections, we had a total of 48,638 negative videos in the Ev100 \"event\" set, including 48,155 unconfirmed and 483 confirmed negatives.47 This results in a negative: positive ratio of 27: 1 for the YLI-MED corpus - higher than most similar corpus, but allows us room to add additional events without having to add additional negative videos.47 We divided these into a training set of 5,000 videos and a test set of 43,138 videos. As with videos for each event set, we grouped the videos by user and then randomly assigned users to either the training set or the test set, the only limitation being to ensure that the relatively smaller training set had no more than 2% of its videos from one user. 48 Because no effort was made to ensure that the positive event sets matched users to the training sets or the test set with the other (within the event only) sets."}, {"heading": "2.4.4 Other Negative Confirmation Mechanisms", "text": "When we begin to use this data set in research, we will undoubtedly conduct experiments in which our classification and detection systems yield obvious false positives. Such experiments provide an opportunity to further refine the negative videos if one of the \"false\" positives are actually positive cases of the event. Finally, we hope to receive feedback from the community of other researchers using this corpus. If, in the course of your research, you discover any positive cases of the target events among the videos labeled Negative (in the Ev100 event set), please send an email to jbernd @ yli-corpus.org so that we can remove them from the set."}, {"heading": "3 Characteristics of the YLI-MED Corpus", "text": "In this section you will find detailed descriptive statistics for YLI-MED version 1. (Unless otherwise noted, the statistics relate specifically to version 1.0.) First, in section 3.1 we give a general overview of the corpus, including videos per event, and the consistency and confidence of commentators about these videos. In section 3.2 we provide information on the distribution of videos per user. In section 3.3 we provide an overview of the additional corpus features that we have described (in addition to events), including post-processing features and language. In section 3.4 we compare training and testing kits to identify statistically significant differences that could affect classification and detection performance, and in section 3.5 we describe preliminary results on how post-processing features affect the commentators \"event assessments.47 The numbers in this section relate to version 1.0 of the corpus. 48 In order to achieve an appropriate distribution of videos per user for the negative index as a whole, no limitations were necessary."}, {"heading": "3.1 Overview of Characteristics for YLI-MED v.1.0", "text": "YLI-MED version 1.0 contains 50,638 videos, including 1823 videos classified as positive cases of event categories, 115 evidence of near misses and 62 videos related to the target events, and 48,638 negative videos, i.e. videos that do not represent or are closely related to any of the target happenings.49"}, {"heading": "3.1.1 Event Breakdown", "text": "Table 9 shows the final number of videos for each event category in the published corpus (including positive, near misses and related videos).The number of videos for each event varies considerably, from 139 positive examples of Ev106 person grooming at Animal to 237 positive examples of Ev101 birthday party. Each event set contains a standard set of 100 positive training videos; the rest of the videos for the event are in a standard test set. (The near misses and related videos are in a separate supplement set.) The negative videos are divided into 5,000 training videos and 48,138 test videos."}, {"heading": "3.1.2 Agreement and Annotator Confidence", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2 Distribution of Videos per User", "text": "As in any other country, the number of people with a migrant background is many times higher than the number of people with a migrant background in the last ten years."}, {"heading": "3.3 Non-Event Characteristics", "text": "This section describes the distribution of positive sample videos in YLI-MED in terms of different post-production characteristics (\u00a7 3.3.1) and linguistic (\u00a7 3.3.2). Information on the geographical and temporal distribution of YLI-MED positive videos can be found in comparison to the total distributions of YFCC100M in \u00a7 4.1.2."}, {"heading": "3.3.1 Post-Production Characteristics", "text": "In fact, most people who are able to survive themselves are also able to survive themselves; most of them are able to survive themselves, without being able to survive themselves; most of them are able to survive themselves, and most of them are able to survive themselves; most of them are able to survive themselves, and most of them are able to survive themselves; most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "3.3.2 Language Characteristics", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to live than in another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "3.4 Comparing the Training and Test Sets", "text": "This section compares the standard training and testing sets of positive events videos in the YLI-MED corpus to determine whether there are any statistically significant differences between them in terms of corpus builders, which are generally minor even where they are statistically significant. (Determining whether such differences affect measurements of classification performance can be an interesting topic for research.) Table 19 shows the degree of agreement among commentators on whether a video was a positive example (2 / 3 vs 3 / 3) and the average confidence of commentators who rate it as positive for training and the test sets as a whole (all events). It also shows that the differences in match are not statistically significant, are not yet significant, nor are the differences in average language."}, {"heading": "3.5 A Mini-Analysis: Correlations Between Non-Event Characteristics and Event Decisions", "text": "In multimedia research, it is known that certain production and post-processing characteristics are involved in the automated analysis of video content of various kinds. It is therefore interesting to investigate whether these particular factors also influence how easy it is for people to classify videos in our case according to the events that occur in them. Although our video collection is not a controlled psychological experiment, it does offer some fascinating data on how people interpret videos that may be of particular interest to multimedia researchers.67 These results are particularly interesting for researchers who are studying how to use audiovisual information in the detection of multimedia events and how to combine information from each domain."}, {"heading": "3.5.1 Effects of Post-Production", "text": "Table 22 shows the impact of selected post-editing characteristics on the agreement between YLI-MED commentators regarding the categorization of videos and their confidence in these judgments. 68Animation had neither a statistically significant effect on the agreement on whether a video is a positive example (full 3 / 3 vs. partial 2 / 3) nor on the average confidence of commentators (among those who rate it positively). Overall, text added in post-editing had no significant effect on the agreement or confidence of commentators. However, a closer examination shows that the effects may depend on what the language is; this is discussed in detail in \u00a7 3.5.2. On the other hand, music reviews had a significant effect on the agreement, with 83% of rated videos achieving full agreement, as opposed to 93% for unrated videos. Musical reviews also had a marginal effect 67 All statistics presented in this section refer to videos where the consensus among commentators is that it is a positive instance."}, {"heading": "3.5.2 Effects of Language", "text": "In fact, most of them are able to put themselves at the centre, not only because they are able to put themselves at the centre, but also because they are able to put themselves at the centre, but also because they are able to put themselves at the centre."}, {"heading": "4 Limitations, Corpus Comparability, and Generalizability", "text": "Our goal in this report is to document these decisions as well as possible so that the users of the corpus are aware of possible distortions. In \u00a7 4.1, we discuss some possible distortions in detail, along with possible consequences for the generalizability of classifiers trained on YLI-MED compared to other sets of data. In particular, we note some potential differences that have been introduced between YLI-MED and the Heterogeneous Audiovisual Internet Collection (HAVIC) (Strassel et al. 2012) and that YLI-MED intentionally exhibits parallels in many respects, as a result of the differences between our procedures and those of the LDC that we have noted in \u00a7 2. In \u00a7 4.2, we point out some differences between YLI-MED and HAVIC that are intended, not mere side effects."}, {"heading": "4.1 Potential Unintentional Biases", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Biases Arising from the YFCC100M Dataset", "text": "Some of the potential differences between YLI-MED and similar corporations arise from our use of the Yahoo Flickr Creative Commons 100 million dataset as the basis for selecting videos. While the YFCC100M's nearly 800,000 videos represent the largest existing open source research dataset of consumed videos, and thus contain quite a wide variety of video types and topics, it is still a product of a specific community of users with a specific geographical, cultural and linguistic distribution, and represents the interests and styles of those users. In fact, even on the order of magnitude of YFCC100M, distortions may occur toward certain productive users; as we noted in Section 3.2, 5.8% of the videos in YFCC100M were contributed by only 10 users and represent the interests and styles of those users. A comparison with YouTube videos from a similar period shows that the distribution of tags is relatively comparable, although YouTube videos have on average twice as many tags per video (Thomee et al. 2015)."}, {"heading": "4.1.2 Possible Effects of Procedure on Representativity: YLI-MED vs. YFCC100M", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "4.1.3 Possible Effects of Procedure on Comparability: YLI-MED vs. TRECVID MED", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "4.1.4 Are There Biases No Search Method Can Avoid?", "text": "There are some ways in which an event corpus is constructed by searching for the metadata supplied by the user for user-generated content that is unlikely to be distorted, that is, the actual distribution of videos reflecting these events along all possible dimensions in the entire source database, or in existential terms compared to all the online videos. These factors will not necessarily distinguish YLI-MED from others, as we describe in Appendix A. However, the definition of Ev107 person feeding to Animal implies that throwing food at a fish or bird would never count (contra HAVIC), and some of the corpus builders have interpreted it in this way. However, the event detection data discussed in the discussion show that the substantially different definitions of this event led to a series of videos that an i-vector-based detection system is comparable well with most modelable events."}, {"heading": "4.2 Intentional Compositional Differences from HAVIC", "text": "Some of the differences between the features of YLI-MED and those of HAVIC / TRECVID MED were intentional desiderata, not just random consequences of the process or data selection. For example, HAVIC does not include animated videos / CGI (nor screencasts) that are always constructed (never natural) and that present a different set of challenges for visual analysis. But HAVIC does include videos that have no audio tracks at all, which of course cannot be analyzed in the audio stream. YLI-MED needed positive examples (and nearby videos) that are contained in both the audio and visual streams."}, {"heading": "4.3 In the End, Is YLI-MED Comparable to Similar Corpora?", "text": "In fact, most people who are in favour of gender equality are in favour of gender equality. (...) It is not that they are in favour of gender equality. (...) It is not that they are in favour of gender equality. (...) It is not that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality. (...) It is that they are in favour of gender equality."}, {"heading": "5 The Bigger Picture for YLI-MED", "text": "Here we discuss how the YLI-MED notes fit into other current and planned efforts to make the YLI corpus a maximum useful resource for a broad sector of multimedia researchers, including ongoing extraction of audiovisual features (paragraph 5.1) and future annotation efforts (paragraph 5.2)."}, {"heading": "5.1 Feature Computation", "text": "The goal of YLI Corpus is to provide researchers with a variety of ready-made data to work with for each of the media activities."}, {"heading": "5.2 Next Steps for YLI-MED", "text": "We are currently moving towards a full-fledged \"Multimedia Genome Project\" (MMGP), which will involve a combination of crowdsourcing and expert annotation. MMGP will include a strong emphasis on multimedia event annotation, including loose annotations for the entire corpus, as well as defined sets of well-defined events as described here. Specifically, we hope to incorporate twenty to fifty new events into YLI-MED, including new events that will never come to the fore."}, {"heading": "6 Acknowledgments", "text": "These annotation efforts are closely related to the work of researchers at ICSI and Lawrence Livermore National Laboratory, who, in addition to several authors, calculate the audio, visual, and motion functions for the videos in the YLIMED corpus, including Jaeyoung Choi (ICSI) and Carmen Carrano, Karl Ni, Roger Pearce, and Doug Poland (LLNL). YLI-MED would not have been possible without the efforts of the researchers at Yahoo Labs who created the YFCC100M dataset in the first place, including Bart Thomee, Li-Jia, Nikhil Rasiwasia, and David Ayman Shamma, along with Yahoo Webscope and Legal Teams.The YLI-MED annotation Project described here was funded by a grant from Cisco Systems, Inc., for Event Detection for Improved Speaker Diarization and Meeting Analysis."}, {"heading": "A Event Definitions for the YLI-MED Corpus", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}], "references": [{"title": "Audio-based multimedia event detection with DNNs and sparse sampling", "author": ["Khalid Ashraf", "Benjamin Elizalde", "Forrest Iandola", "Matthew Moskewicz", "Julia Bernd", "Gerald Friedland", "Kurt Keutzer"], "venue": "Under submission: ICMR", "citeRegEx": "Ashraf et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ashraf et al\\.", "year": 2015}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["Damian Borth", "Rongrong Ji", "Tao Chen", "Thomas Breuel", "Shih-Fu Chang"], "venue": "In Proceedings of the 21st ACM International Conference on Multimedia,", "citeRegEx": "Borth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Borth et al\\.", "year": 2013}, {"title": "Discriminatively trained probabilistic Linear Discriminant Analysis for speaker verification", "author": ["Luk\u00e1\u0161 Burget", "Old\u0159ich Plchot", "Sandro Cumani", "Ond\u0159ej Glembek", "Pavel Mat\u011bjka", "Niko Br\u00fcmmer"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Burget et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Burget et al\\.", "year": 2011}, {"title": "SRI-Sarnoff AURORA system at TRECVID 2012: Multimedia event detection and recounting", "author": ["Hui Cheng", "Jingen Liu", "Saad Ali", "Omar Javed", "Qian Yu", "Amir Tamrakar", "Ajay Divakaran", "Harpreet S. Sawhney", "R. Manmatha", "James Allan", "Alex Hauptmann", "Mubarak Shah", "Subhabrata Bhattacharya", "Afshin Dehghan", "Gerald Friedland", "Benjam\u0131\u0301n Martinez Elizalde", "Trevor Darrell", "Michael Witbrock", "Jon Curtis"], "venue": "In TREC Video Retrieval Evaluation: Notebook Papers and Slides,", "citeRegEx": "Cheng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2012}, {"title": "The Placing Task: A large-scale geoestimation challenge for social-media videos and images", "author": ["Jaeyoung Choi", "Bart Thomee", "Gerald Friedland", "Liangliang Cao", "Karl Ni", "Damian Borth", "Benjamin Elizalde", "Luke Gottlieb", "Carmen Carrano", "Roger Pearce", "Doug Poland"], "venue": "In Proceedings of the ACM Multimedia 2014 Workshop on Geotagging and Its Applications in Multimedia (GeoMM \u201914),", "citeRegEx": "Choi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2014}, {"title": "Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification", "author": ["Najim Dehak", "R\u00e9da Dehak", "Patrick Kenny", "Niko Br\u00fcmmer", "Pierre Ouellet", "Pierre Dumouchel"], "venue": "In Proceedings of INTERSPEECH, UK,", "citeRegEx": "Dehak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dehak et al\\.", "year": 2009}, {"title": "An i-vector representation of acoustic environments for audio-based video event detection on user generated content", "author": ["Benjamin Elizalde", "Howard Lei", "Gerald Friedland"], "venue": "In Proceedings of the IEEE International Symposium on Multimedia (ISM", "citeRegEx": "Elizalde et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elizalde et al\\.", "year": 2013}, {"title": "Frames and the semantics of understanding", "author": ["Charles J. Fillmore"], "venue": "Quaderni di Semantica,", "citeRegEx": "Fillmore.,? \\Q1985\\E", "shortCiteRegEx": "Fillmore.", "year": 1985}, {"title": "Folksonomies: Tidying up tags", "author": ["Marieke Guy", "Emma Tonkin"], "venue": "D-Lib Magazine,", "citeRegEx": "Guy and Tonkin.,? \\Q2006\\E", "shortCiteRegEx": "Guy and Tonkin.", "year": 2006}, {"title": "Probabilistic linear discriminant analysis", "author": ["Sergey Ioffe"], "venue": "Proceedings of the 9th European conference on Computer Vision,", "citeRegEx": "Ioffe.,? \\Q2006\\E", "shortCiteRegEx": "Ioffe.", "year": 2006}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross B. Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "CoRR (arXiv.org),", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Women, Fire, and Dangerous Things: What Categories Reveal about the Mind", "author": ["George Lakoff"], "venue": null, "citeRegEx": "Lakoff.,? \\Q1987\\E", "shortCiteRegEx": "Lakoff.", "year": 1987}, {"title": "SRI-Sarnoff AURORA system at TRECVID 2013: Multimedia event detection and recounting", "author": ["Jingen Liu", "Hui Cheng", "Omar Javed", "Qian Yu", "Ishani Chakraborty", "Weiyu Zhang", "Ajay Divakaran", "Harpreet S. Sawhney", "James Allan", "R. Manmatha", "John Foley", "Mubarak Shah", "Afshin Dehghan", "Michael Witbrock", "Jon Curtis", "Gerald Friedland"], "venue": "In TREC Video Retrieval Evaluation: Notebook Papers and Slides,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Categorization of natural objects", "author": ["Carolyn Mervis", "Eleanor Rosch"], "venue": "Annual Review of Psychology,", "citeRegEx": "Mervis and Rosch.,? \\Q1981\\E", "shortCiteRegEx": "Mervis and Rosch.", "year": 1981}, {"title": "The Yahoo-Livermore-ICSI (YLI) multimedia feature set", "author": ["Karl S. Ni", "Carmen C. Carrano", "Doug N. Poland", "Benjamin M. Elizalde", "Gerald Friedland", "Luke R. Gottlieb", "Damian S. Borth"], "venue": "Technical Report LLNLMI-659231,", "citeRegEx": "Ni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2014}, {"title": "TRECVID 2011 - an overview of the goals, tasks, data, evaluation mechanisms, and metrics", "author": ["Paul Over", "George Awad", "Jonathan Fiscus", "Brian Antonishek", "Martial Michel", "Alan Smeaton", "Wessel Kraaij", "Georges Qu\u00e9not"], "venue": "Technical report, National Institute of Standards and Technology,", "citeRegEx": "Over et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Over et al\\.", "year": 2012}, {"title": "Social Event Detection at MediaEval: a three-year retrospect of tasks and results", "author": ["Georgios Petkos", "Symeon Papadopoulos", "Vasileios Mezaris", "Raphael Troncy", "Philipp Cimiano", "Timo Reuter", "Yiannis Kompatsiaris"], "venue": "In Proceedings of the ACM ICMR 2014 Workshop on Social Events in Web Multimedia (SEWM),", "citeRegEx": "Petkos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Petkos et al\\.", "year": 2014}, {"title": "A comparative study of Flickr tags and index terms in a general image collection", "author": ["Abebe Rorissa"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Rorissa.,? \\Q2010\\E", "shortCiteRegEx": "Rorissa.", "year": 2010}, {"title": "Human categorization", "author": ["Eleanor Rosch"], "venue": null, "citeRegEx": "Rosch.,? \\Q1977\\E", "shortCiteRegEx": "Rosch.", "year": 1977}, {"title": "Evaluation campaigns and TRECVid", "author": ["Alan F. Smeaton", "Paul Over", "Wessel Kraaij"], "venue": "In MIR \u201906: Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval,", "citeRegEx": "Smeaton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smeaton et al\\.", "year": 2006}, {"title": "The new data and new challenges in multimedia research", "author": ["Bart Thomee", "David A. Shamma", "Benjamin Elizalde", "Gerald Friedland", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "Communications of the ACM,", "citeRegEx": "Thomee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomee et al\\.", "year": 2015}, {"title": "Visual concept learning from weakly labeled web videos", "author": ["Adrian Ulges", "Damian Borth", "Thomas M Breuel"], "venue": "Video Search and Mining,", "citeRegEx": "Ulges et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ulges et al\\.", "year": 2010}, {"title": "Linking visual concept detection with viewer demographics", "author": ["Adrian Ulges", "Markus Koch", "Damian Borth"], "venue": "In Proceedings of the 2nd ACM International Conference on Multimedia Retrieval,", "citeRegEx": "Ulges et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ulges et al\\.", "year": 2012}, {"title": "Visual Concept Learning from User-Tagged Web Video", "author": ["Adrian Ulges"], "venue": "PhD thesis, Universita\u0308t Kaiserslautern,", "citeRegEx": "Ulges.,? \\Q2009\\E", "shortCiteRegEx": "Ulges.", "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "YLI is a set of computed audio and visual features and additional annotations (including MED) for the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset (Yahoo Labs 2014; Thomee et al. 2015).", "startOffset": 163, "endOffset": 200}, {"referenceID": 15, "context": "For Version 1 of the YLI-MED corpus, we chose to use events that were covered in the TRECVID MED 2011 evaluation (Over et al. 2012; Smeaton et al. 2006), organized by the National Institute of Standards and Technology (NIST).", "startOffset": 113, "endOffset": 152}, {"referenceID": 19, "context": "For Version 1 of the YLI-MED corpus, we chose to use events that were covered in the TRECVID MED 2011 evaluation (Over et al. 2012; Smeaton et al. 2006), organized by the National Institute of Standards and Technology (NIST).", "startOffset": 113, "endOffset": 152}, {"referenceID": 15, "context": "Table 1: The fifteen events for the TRECVID MED 2011 Evaluation (Over et al. 2012).", "startOffset": 64, "endOffset": 82}, {"referenceID": 1, "context": "To manage the risk of user bias, creators of datasets from online platforms like Flickr and YouTube often set a maximum number of images or videos per user (e.g., Ulges et al. 2012; Borth et al. 2013); we elected to follow suit.", "startOffset": 156, "endOffset": 200}, {"referenceID": 15, "context": "42 This differentiates YLI from the HAVIC data used for TRECVID MED, where each video designated \u201cnegative\u201d had been individually screened (Strassel et al. 2012; Over et al. 2012).", "startOffset": 139, "endOffset": 179}, {"referenceID": 21, "context": "To some degree, the review of random videos served as a test of how well the method of automatically excluding videos based on textual metadata worked, given that the metadata for videos does not necessarily describe what happens in them, and many videos do not have textual metadata at all (Ulges et al. 2010).", "startOffset": 291, "endOffset": 310}, {"referenceID": 15, "context": "\u2022 The event as formulated by the YLI-MED research team (based on the TRECVID MED event names (Over et al. 2012)) may be a subcategory or supercategory of a more basic-level event category (Rosch 1977b) (for example, catching a fish may be a more basic-level event category than the subevent landing a fish).", "startOffset": 93, "endOffset": 111}, {"referenceID": 20, "context": "In addition, it shows the average and maximum numbers and percentages for all of the positives together (where some users may have contributed videos to multiple events) and for the YLI-MED corpus as a whole (including the positives and negatives and the Supplemental set of near misses and related videos), as well as the same statistics for the whole Yahoo Flickr Creative Commons 100 Million dataset (Yahoo Labs 2014; Thomee et al. 2015) from which the YLI-MED corpus is drawn.", "startOffset": 403, "endOffset": 440}, {"referenceID": 20, "context": "A comparison with YouTube videos from a similar time period shows that the distribution of tags is fairly comparable, though YouTube videos had an average of twice as many tags per video (Thomee et al. 2015).", "startOffset": 187, "endOffset": 207}, {"referenceID": 21, "context": "In addition, it will include (mostly) videos where at least some of the textual metadata is descriptive of the content of the video\u2014rather than, for example, descriptions of the camera, reasons for posting (\u201cThought this was interesting\u201d), or random unrelated rants (Ulges et al. 2010).", "startOffset": 266, "endOffset": 285}, {"referenceID": 3, "context": "2013, we used the i-vector approach to provide audio analysis for SRI-Sarnoff\u2019s AURORA multimedia event detection and retrieval system when it competed in TRECVID in 2012 and 2013 (Cheng et al. 2012; Liu et al. 2013).", "startOffset": 180, "endOffset": 216}, {"referenceID": 12, "context": "2013, we used the i-vector approach to provide audio analysis for SRI-Sarnoff\u2019s AURORA multimedia event detection and retrieval system when it competed in TRECVID in 2012 and 2013 (Cheng et al. 2012; Liu et al. 2013).", "startOffset": 180, "endOffset": 216}, {"referenceID": 3, "context": "The system we chose for the comparison is based on an i-vector approach initially developed by Dehak et al. (2009), with an improvement by Burget et al.", "startOffset": 95, "endOffset": 115}, {"referenceID": 2, "context": "(2009), with an improvement by Burget et al. (2011). It involves training a matrix T to model the total variability of a set of statistics for each video\u2019s audio track, primarily the first-order Baum-Welch statistics of the low-level acoustic feature frames (namely, Mel frequency cepstral coefficients).", "startOffset": 31, "endOffset": 52}, {"referenceID": 10, "context": "), a set of audio content-analysis tools based on the deep neural net (DNN) image-analysis framework Caffe (Jia et al. 2014).", "startOffset": 107, "endOffset": 124}, {"referenceID": 14, "context": ", Kaldi) descriptors; and the motion features will begin with dense trajectories (Ni et al. 2014).", "startOffset": 81, "endOffset": 97}, {"referenceID": 4, "context": "A subset of the YLI corpus data, including the computed features, was already used in the MediaEval 2014 Placing Task, and additional YLI data will be added in 2015 and 2016 (Choi et al. 2014).", "startOffset": 174, "endOffset": 192}], "year": 2015, "abstractText": "The YLI Multimedia Event Detection corpus is a public-domain index of videos with annotations and computed features, specialized for research in multimedia event detection (MED), i.e., automatically identifying what's happening in a video by analyzing the audio and visual content. The videos indexed in the YLI-MED corpus are a subset of the larger YLI feature corpus, which is being developed by the International Computer Science Institute and Lawrence Livermore National Laboratory based on the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting one of ten target events, or no target event, and are annotated for additional attributes like language spoken and whether the video has a musical score. The annotations also include degree of annotator agreement and average annotator confidence scores for the event categorization of each video. Version 1.0 of YLI-MED includes 1823 \"positive\" videos that depict the target events and 48,138 \"negative\" videos, as well as 177 supplementary videos that are similar to event videos but are not positive examples. Our goal in producing YLI-MED is to be as open about our data and procedures as possible. This report describes the procedures used to collect the corpus; gives detailed descriptive statistics about the corpus makeup (and how video attributes affected annotators' judgments); discusses possible biases in the corpus introduced by our procedural choices and compares it with the most similar existing dataset, TRECVID MED's HAVIC corpus; and gives an overview of our future plans for expanding the annotation effort. ar X iv :1 50 3. 04 25 0v 1 [ cs .M M ] 1 3 M ar 2 01 5 The YLI-MED Corpus: Characteristics, Procedures, and Plans Julia Bernd Damian Borth Benjamin Elizalde Gerald Friedland Heather Gallagher Luke Gottlieb Adam Janin Sara Karabashlieva Jocelyn Takahashi Jennifer Won", "creator": "LaTeX with hyperref package"}}}