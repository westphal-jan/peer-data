{"id": "1503.04269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2015", "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning", "abstract": "In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD($\\lambda$)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD($\\lambda$), and GQ($\\lambda$). Compared to these methods, our _emphatic TD($\\lambda$)_ is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. On the other hand, the range of problems for which it is stable but does not converge with probability one is larger than for gradient-TD methods. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.", "histories": [["v1", "Sat, 14 Mar 2015 04:44:20 GMT  (3980kb,D)", "http://arxiv.org/abs/1503.04269v1", "29 pages"], ["v2", "Tue, 21 Apr 2015 02:21:57 GMT  (10208kb,D)", "http://arxiv.org/abs/1503.04269v2", "29 pages This is a significant revision based on the first set of reviews. The most important change was to signal early that the main result is about stability, not convergence"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard s sutton", "a rupam mahmood", "martha white"], "accepted": false, "id": "1503.04269"}, "pdf": {"name": "1503.04269.pdf", "metadata": {"source": "CRF", "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning", "authors": ["Richard S. Sutton", "Rupam Mahmood", "Martha White"], "emails": ["sutton@cs.ualberta.ca", "ashique@cs.ualberta.ca", "whitem@cs.ualberta.ca"], "sections": [{"heading": null, "text": "Keywords: time difference learning, extra-political education, functional alignment, convergence, stability"}, {"heading": "1. Parametric Temporal-Difference Learning", "text": "The problem it solves is that one learns efficiently to make a sequence of long-term predictions about how a dynamic system will evolve over time. The key idea is to use the change (time difference) from one prediction to the next as an error in the previous prediction. For example, if one predicts on each day what the stock market index will be at the end of the year, and events one day lead to a much lower prediction, then a TD method would conclude that the predictions made before the fall are likely to be too high; it would adjust the parameters of its prediction function to make lower predictions for similar situations in the future. This approach can be produced using conventional approaches to prediction that would wait until the end of the year, when the final stock market index would be known, or otherwise only make short-term predictions (e.g. one day)."}, {"heading": "2. On-policy Convergence of TD(0)", "text": "To begin with, let us examine the conditions for the convergence of conventional TD (\u03bb) in the context of a training with data from a continuing finite Markov decision process. Let us consider the simplest functional approximation, which is formed by linear TD (\u03bb) with 0 and constant discounting parameters (0, 1). Conventional linear TD (0) is defined by the following update of the parameter, which is formed at each point of a sequence of time steps t = 0, 1, 2 etc., at the transition from state St-S to state St-1-S, which takes action and receives the reward Rt + 1. R: wt + 1 (Rt + 1) is the sequence of time steps t x (St + 1) \u2212 w > t x (St) x (St), (1) where \u03b1 > 0 is a step size parameter, and x (s) is the function vector corresponding to the state."}, {"heading": "3. Instability of Off-policy TD(0)", "text": "Before developing the off-policy attitude in detail, it is useful to informally understand why TD (0) is prone to instability. TD Learning involves learning an estimate that can be problematic if there is a generalization between the two estimates. Suppose there is actually a transition between two states with the same representation of characteristics, except that the second is twice as large: 2w 0 2wwhere here w and 2w are the estimated values of the two states - that is, their characteristics are a single characteristic that is 1 for the first state and 2 for the second. Now features are not redundant), and therefore Xy = 0 is only when y = 0. If this is not true, then the convergence (when it occurs) may not be to a unique w-country, but rather to a subspace of parameter vectors that all produce the same approximate value.Assuming that w is 10 and the reward for the transition is 0. The transition from then to a state is rated with a 20 percent."}, {"heading": "4. Off-policy Stability of Emphatic TD(0)", "text": "The deeper reason for the difficulty of off-policy learning is that behavioral politics can lead the process to a distribution of states that are different from what would occur under the target policy, but the states may appear to be equal or similar due to functional approximation. Previous work by Precup, Sutton, and Dasgupta (2001) tried to completely correct the different distributions by looking at the different distributions in terms of relevance. It works in theory because then the key matrix is again considered dollars, which we know to be positively defined. Most subsequent work has abandoned the idea of a complete correction of the state distribution; for example, work on the gradient TD methods (e.g. Sutton et al. 2009, Maei) attempts to minimize averages."}, {"heading": "5. The General Case", "text": "We now turn to a more general case of extraordinary learning with linear functional approximation. The goal is to evaluate a policy \u03c0 from a single path under a different policy \u00b5, but now the value of a state is defined not in terms of a constant discount rate \u03b3 [0, 1], but in terms of a discount rate that varies from state to state according to a discount function \u03b3 [0, 1] so that most states have such a discount rate. That is, our approximation is still defined by (2), but now (3) is being replaced by Gt. = Rt + 1) is the right policy, the (St + 1) \u03b3 is the time of accumulation. (St + 2). \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (19) The state-dependent discount specifies a time wrapping within which rewards are accumulated. (St) Then the period of accumulation is completely ended."}, {"heading": "6. Off-policy Stability of Emphatic TD(\u03bb)", "text": "As usual, to analyze the stability of the new algorithm, we examine its A matrix (= >).The stochastic update can be written as follows: wt + 1. = wt + 1. \u2212 p = 1. \u2212 p = 1. \u2212 p = 1. \u2212 p = 1. \u2212 p = 1. \u2212 p = 1."}, {"heading": "7. Derivation of the Emphasis Algorithm", "text": "The emphasis algorithm is based on the idea that if we update a state through a TD method =, then we should also update any state from which it is calculated, in direct proportion. - Suppose we choose to update at a later date, perhaps because i (St) = 1, then at a later date t + 1, then we have a value (St + 1) = 1 and a value (St + 1) = 0. Instead, we would gain a half unit of emphasis, and the remaining half would still be available to t + 2, or at later times, depending on their results. And of course, there may be some priorities that can be directly mapped to t (St + 1) if i (St + 1) > 0."}, {"heading": "8. Empirical Examples", "text": "In this section we present empirical results with example problems that verify and clarify the problem already presented. However, a thorough empirical comparison of the empirical results of emphatic TD (\u03bb) with other methods is outside the scope of the present article. As in many earlier theories of TD algorithms with functional approximation, the main focus in this paper is, on the other hand, on the stability of the expected update. If an algorithm is unstable, as Q-learning and off-policy TD (\u03bb) are, then there is no chance that it will behave satisfactorily. On the other hand, the stability of the expected update does not imply implicit convergence, and indeed TD (\u03bb) does not always imply convergence. It is stable and will never come to divergence, but there are cases where there is no convergence in the conventional sense."}, {"heading": "9. Conclusions and Future Work", "text": "We have introduced a way to vary the emphasis or strength of updating TD learning algorithms from step to step, based on the collection of meaning values, which should result in much less variance than previous methods (Precup et al. 2001). In particular, we have introduced the emphatic TD algorithm and demonstrated that it solves the instability problem that plagues conventional TD methods when applied in non-political training situations in conjunction with linear functional approximation. Compared with gradient TD methods, emphatic TD methods are easier because it has a single weight vector and one step size, rather than two from each other. Both methods can be subject to large fluctuations, even if their expected updates are stable, but so far it seems easier to prevent this with gradient TD methods."}, {"heading": "Acknowledgements", "text": "The authors thank Hado van Hasselt, Doina Precup, and Huizhen Yu for their insights and discussions that contributed to the results presented in this paper, and the entire Reinforcement Learning and Artificial Intelligence Research Group for providing the environment to promote and support this research. We thank Alberta Innovates - Technology Futures and the Natural Sciences and Engineering Research Council of Canada for their support."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "In Proceedings of the 12th International Conference on Machine Learning,", "citeRegEx": "Baird,? \\Q1995\\E", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "In Proceedings of the 16th International Conference on Machine Learning,", "citeRegEx": "Boyan,? \\Q1999\\E", "shortCiteRegEx": "Boyan", "year": 1999}, {"title": "Linear least-squares algorithms for temporal difference learning. Machine Learning 22:33\u201357", "author": ["S. Bradtke", "A.G. Barto"], "venue": null, "citeRegEx": "Bradtke and Barto,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "The convergence of TD(\u03bb) for general \u03bb. Machine Learning 8:341\u2013362", "author": ["P. Dayan"], "venue": null, "citeRegEx": "Dayan,? \\Q1992\\E", "shortCiteRegEx": "Dayan", "year": 1992}, {"title": "Reinforcement learning: The good, the bad and the ugly", "author": ["P. Dayan", "Y. Niv"], "venue": "Current Opinion in Neurobiology", "citeRegEx": "Dayan and Niv,? \\Q2008\\E", "shortCiteRegEx": "Dayan and Niv", "year": 2008}, {"title": "Policy evaluation with temporal differences: A survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Dann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dann et al\\.", "year": 2014}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Geist and Scherrer,? \\Q2014\\E", "shortCiteRegEx": "Geist and Scherrer", "year": 2014}, {"title": "Stable function approximation in dynamic programming", "author": ["G.J. Gordon"], "venue": "Proceedings of the 12th International Conference on Machine Learning,", "citeRegEx": "Gordon,? \\Q1995\\E", "shortCiteRegEx": "Gordon", "year": 1995}, {"title": "Stable fitted reinforcement learning", "author": ["G.J. Gordon"], "venue": "Advances in Neural Information Processing Systems: Proceedings of the 1995 Conference,", "citeRegEx": "Gordon,? \\Q1996\\E", "shortCiteRegEx": "Gordon", "year": 1996}, {"title": "Faster Gradient-TD Algorithms. MSc thesis, University of Alberta", "author": ["L. Hackman"], "venue": null, "citeRegEx": "Hackman,? \\Q2012\\E", "shortCiteRegEx": "Hackman", "year": 2012}, {"title": "A neuronal model of classical conditioning", "author": ["A.H. Klopf"], "venue": "Psychobiology", "citeRegEx": "Klopf,? \\Q1988\\E", "shortCiteRegEx": "Klopf", "year": 1988}, {"title": "Least squares policy iteration", "author": ["M. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Evaluating the TD model of classical conditioning", "author": ["E.A. Ludvig", "R.S. Sutton", "E.J. Kehoe"], "venue": "Learning & behavior", "citeRegEx": "Ludvig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ludvig et al\\.", "year": 2012}, {"title": "Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta", "author": ["H.R. Maei"], "venue": null, "citeRegEx": "Maei,? \\Q2011\\E", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "GQ(\u03bb): A general gradient algorithm for temporaldifference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "In Proceedings of the Third Conference on Artificial General Intelligence,", "citeRegEx": "Maei and Sutton,? \\Q2010\\E", "shortCiteRegEx": "Maei and Sutton", "year": 2010}, {"title": "Toward off-policy learning control with function approximation", "author": ["H.R. Maei", "Szepesv\u00e1ri", "Cs", "S. Bhatnagar", "R.S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Maei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2010}, {"title": "Multi-timescale nexting in a reinforcement learning robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Adaptive Behavior", "citeRegEx": "Modayil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Modayil et al\\.", "year": 2014}, {"title": "Dialogues on prediction errors. Trends in cognitive sciences", "author": ["Y. Niv", "G. Schoenbaum"], "venue": null, "citeRegEx": "Niv and Schoenbaum,? \\Q2008\\E", "shortCiteRegEx": "Niv and Schoenbaum", "year": 2008}, {"title": "Beyond simple reinforcement learning: The computational neurobiology of reward learning and valuation", "author": ["J.P. O\u2019Doherty"], "venue": "European Journal of Neuroscience", "citeRegEx": "O.Doherty,? \\Q2012\\E", "shortCiteRegEx": "O.Doherty", "year": 2012}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Problem Solving with Reinforcement Learning", "author": ["G.A. Rummery"], "venue": "PhD thesis,", "citeRegEx": "Rummery,? \\Q1995\\E", "shortCiteRegEx": "Rummery", "year": 1995}, {"title": "Some studies in machine learning using the game of checkers", "author": ["A.L. Samuel"], "venue": "IBM Journal on Research and Development 3:210\u2013229", "citeRegEx": "Samuel,? \\Q1959\\E", "shortCiteRegEx": "Samuel", "year": 1959}, {"title": "A neural substrate of prediction and reward", "author": ["W. Schultz", "P. Dayan", "P.R. Montague"], "venue": "Science", "citeRegEx": "Schultz et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schultz et al\\.", "year": 1997}, {"title": "TD models of reward predictive responses in dopamine neurons", "author": ["R.E. Suri"], "venue": "Neural Networks", "citeRegEx": "Suri,? \\Q2002\\E", "shortCiteRegEx": "Suri", "year": 2002}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In Proceedings of the 12th International Conference on Machine Learning,", "citeRegEx": "Sutton,? \\Q1995\\E", "shortCiteRegEx": "Sutton", "year": 1995}, {"title": "The grand challenge of predictive empirical abstract knowledge. Working Notes of the IJCAI-09 Workshop on Grand Challenges for Reasoning from Experiences", "author": ["R.S. Sutton"], "venue": null, "citeRegEx": "Sutton,? \\Q2009\\E", "shortCiteRegEx": "Sutton", "year": 2009}, {"title": "Beyond reward: The problem of knowledge and data", "author": ["R.S. Sutton"], "venue": "In Proceedings of the 21st International Conference on Inductive Logic Programming,", "citeRegEx": "Sutton,? \\Q2012\\E", "shortCiteRegEx": "Sutton", "year": 2012}, {"title": "Toward a modern theory of adaptive networks: Expectation and prediction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Psychological Review", "citeRegEx": "Sutton and Barto,? \\Q1981\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1981}, {"title": "Time-derivative models of Pavlovian reinforcement", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1990\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1990}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "A new Q(\u03bb) with interim forward view and Monte Carlo equivalence", "author": ["R.S. Sutton", "A.R. Mahmood", "D. Precup", "H. van Hasselt"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesv\u00e1ri", "Cs", "E. Wiewiora"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "Precup D", "S. Singh"], "venue": "Artificial Intelligence", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Temporal abstraction in temporal-difference networks", "author": ["R.S. Sutton", "E.J. Rafols", "A. Koop"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2006}, {"title": "Practical issues in temporal difference learning", "author": ["G. Tesauro"], "venue": "Machine Learning", "citeRegEx": "Tesauro,? \\Q1992\\E", "shortCiteRegEx": "Tesauro", "year": 1992}, {"title": "Temporal difference learning and TD-Gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM", "citeRegEx": "Tesauro,? \\Q1995\\E", "shortCiteRegEx": "Tesauro", "year": 1995}, {"title": "Bias in natural actor\u2013critic algorithms", "author": ["P. Thomas"], "venue": "In Proceedings of the 31st International Conference on Machine Learning. JMLR", "citeRegEx": "Thomas,? \\Q2014\\E", "shortCiteRegEx": "Thomas", "year": 2014}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control", "citeRegEx": "Tsitsiklis and Roy,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1997}, {"title": "True online TD(\u03bb)", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Seijen and Sutton,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton", "year": 2014}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "C.J.C. H"], "venue": null, "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}, {"title": "Q-learning. Machine Learning 8:279\u2013292", "author": ["Watkins", "C.J.C. H", "P. Dayan"], "venue": null, "citeRegEx": "Watkins et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watkins et al\\.", "year": 1992}, {"title": "Convergence of least squares temporal difference methods under general conditions", "author": ["H. Yu"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Yu,? \\Q2010\\E", "shortCiteRegEx": "Yu", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "A less obvious advantage of the TD approach is that it often produces statistically more accurate answers than conventional approaches (Sutton 1988).", "startOffset": 135, "endOffset": 148}, {"referenceID": 15, "context": "Parametric temporal-difference learning was first studied as the key \u201clearning by generalization\u201d algorithm in Samuel\u2019s (1959) checker player.", "startOffset": 111, "endOffset": 127}, {"referenceID": 15, "context": "Parametric temporal-difference learning was first studied as the key \u201clearning by generalization\u201d algorithm in Samuel\u2019s (1959) checker player. Sutton (1988) introduced the TD(\u03bb) algorithm and proved convergence in the mean of episodic linear TD(0), the simplest parametric TD method.", "startOffset": 111, "endOffset": 157}, {"referenceID": 0, "context": "Dayan (1992) proved convergence in expected value of episodic linear TD(\u03bb) for all \u03bb \u2208 [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(\u03bb).", "startOffset": 0, "endOffset": 13}, {"referenceID": 0, "context": "Dayan (1992) proved convergence in expected value of episodic linear TD(\u03bb) for all \u03bb \u2208 [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(\u03bb).", "startOffset": 0, "endOffset": 129}, {"referenceID": 0, "context": "Dayan (1992) proved convergence in expected value of episodic linear TD(\u03bb) for all \u03bb \u2208 [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(\u03bb). Watkins (1989) extended TD learning to control in the form of Q-learning and proved its convergence in the tabular case (without function approximation, Watkins & Dayan 1992), while Rummery (1995) extended TD learning to control in an on-policy form as the Sarsa(\u03bb) algorithm.", "startOffset": 0, "endOffset": 223}, {"referenceID": 0, "context": "Dayan (1992) proved convergence in expected value of episodic linear TD(\u03bb) for all \u03bb \u2208 [0, 1], and Tsitsiklis and Van Roy (1997) proved convergence with probability one of discounted continuing linear TD(\u03bb). Watkins (1989) extended TD learning to control in the form of Q-learning and proved its convergence in the tabular case (without function approximation, Watkins & Dayan 1992), while Rummery (1995) extended TD learning to control in an on-policy form as the Sarsa(\u03bb) algorithm.", "startOffset": 0, "endOffset": 405}, {"referenceID": 0, "context": "Bradtke and Barto (1996) and Boyan (1999) extended linear TD learning to a least-squares form called LSTD(\u03bb).", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "Bradtke and Barto (1996) and Boyan (1999) extended linear TD learning to a least-squares form called LSTD(\u03bb).", "startOffset": 29, "endOffset": 42}, {"referenceID": 0, "context": "Baird (1995) showed definitively that parametric TD learning was much less robust in the off-policy case by exhibiting counterexamples for which both linear TD(0) and linear Q-learning had unstable expected updates and, as a result, the parameters of their linear function approximation diverged to infinity.", "startOffset": 0, "endOffset": 13}, {"referenceID": 0, "context": "Baird (1995) showed definitively that parametric TD learning was much less robust in the off-policy case by exhibiting counterexamples for which both linear TD(0) and linear Q-learning had unstable expected updates and, as a result, the parameters of their linear function approximation diverged to infinity. This is a serious limitation, as the off-policy aspect is key to Q-learning (perhaps the single most popular reinforcement learning algorithm), to learning from historical data and from demonstrations, and to the idea of using TD learning for perception and world knowledge. Over the years, several different approaches have been taken to solving the problem of off-policy learning. Baird (1995) proposed an approach based on gradient descent in the Bellman error for general parametric function approximation that has the desired computa-", "startOffset": 0, "endOffset": 705}, {"referenceID": 9, "context": "2010), including hybrid methods such as HTD (Hackman 2012).", "startOffset": 44, "endOffset": 58}, {"referenceID": 6, "context": "The studies by White (in preparation), Geist and Scherrer (2014), and Dann, Neumann, and Peters (2014) represent our best experience with gradient-TD and related methods.", "startOffset": 39, "endOffset": 65}, {"referenceID": 6, "context": "The studies by White (in preparation), Geist and Scherrer (2014), and Dann, Neumann, and Peters (2014) represent our best experience with gradient-TD and related methods.", "startOffset": 39, "endOffset": 103}, {"referenceID": 25, "context": "From this state weighting, stability of the expected update is then proved using theory similar to that originally developed for TD(\u03bb) (Sutton 1988).", "startOffset": 135, "endOffset": 148}, {"referenceID": 25, "context": "The approach has novel elements but is similar to that developed by Precup, Sutton, and Dasgupta in 2001. They proposed to use importance sampling to reweight the updates of linear TD(\u03bb), emphasizing or de-emphasizing states as they were encountered, and thereby create a weighting equivalent to the stationary distribution under the target policy, from which the results of Tsitsiklis and Van Roy (1997) would apply and guarantee convergence.", "startOffset": 76, "endOffset": 405}, {"referenceID": 0, "context": "Baird\u2019s (1995) counterexample, for example, is not technically about TD(\u03bb), but about the expected update; in effect, he showed that TD(\u03bb)\u2019s A is not positive semi-definite.", "startOffset": 0, "endOffset": 15}, {"referenceID": 22, "context": "Earlier work by Precup, Sutton and Dasgupta (2001) attempted to completely correct for the different state distribution using importance sampling ratios to reweight the states encountered.", "startOffset": 24, "endOffset": 51}, {"referenceID": 13, "context": "2009, Maei 2011) seeks to minimize the mean-squared projected Bellman error weighted by d\u03bc. We call this an excursion setting because we can think of the contemplated switch to the target policy as an excursion from the steady-state distribution of the behavior policy, d\u03bc. The excursions would start from d\u03bc and then follow \u03c0 until termination, followed by a resumption of \u03bc and thus a gradual return to d\u03bc. Of course these excursions never actually occur during off-policy learning, they are just contemplated, and thus the state distribution in fact never leaves d\u03bc. It is the excursion view that we take in this paper, but still we use techniques similar to those introduced by Precup et al. (2001) to determine an emphasis weighting that corrects for the state distribution, only toward a different goal.", "startOffset": 6, "endOffset": 703}, {"referenceID": 25, "context": "With the steps we have already done, this is immediate and after the pattern of Sutton (1988), as in the emphatic TD(0) section.", "startOffset": 80, "endOffset": 94}, {"referenceID": 25, "context": "Equation (6) of the paper by Sutton, Mahmood, Precup, and van Hasselt (2014) specifies this in their \u201cforward view\u201d of off-policy TD(\u03bb) with general state-dependent discounting and bootstrapping.", "startOffset": 29, "endOffset": 77}, {"referenceID": 0, "context": "If an algorithm is unstable, as Q-learning and off-policy TD(\u03bb) are on Baird\u2019s (1995) counterexample, then there is no chance of its behaving in a satisfactory manner.", "startOffset": 71, "endOffset": 86}, {"referenceID": 19, "context": "Another idea for reducing variance is to use weighted importance sampling, which Precup et al. (2001) proved converges robustly even with infinite variance samples, together with the ideas of Mahmood et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 19, "context": "We have introduced a way of varying the emphasis or strength of the update of TD learning algorithms from step to step, based on importance sampling, that should result in much lower variance than previous methods (Precup et al. 2001).", "startOffset": 214, "endOffset": 234}], "year": 2015, "abstractText": "In this paper we introduce the idea of improving the performance of parametric temporaldifference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD(\u03bb)\u2019s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradientTD family of methods including TDC, GTD(\u03bb), and GQ(\u03bb). Compared to these methods, our emphatic TD(\u03bb) is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. On the other hand, the range of problems for which it is stable but does not converge with probability one is larger than for gradient-TD methods. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.", "creator": "LaTeX with hyperref package"}}}