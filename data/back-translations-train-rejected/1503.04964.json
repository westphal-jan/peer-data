{"id": "1503.04964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "Energy Sharing for Multiple Sensor Nodes with Finite Buffers", "abstract": "We consider the problem of finding optimal energy sharing policies that maximize the network performance of a system comprising of multiple sensor nodes and a single energy harvesting (EH) source. Sensor nodes periodically sense the random field and generate data, which is stored in the corresponding data queues. The EH source harnesses energy from ambient energy sources and the generated energy is stored in an energy buffer. Sensor nodes receive energy for data transmission from the EH source. The EH source has to efficiently share the stored energy among the nodes in order to minimize the long-run average delay in data transmission. We formulate the problem of energy sharing between the nodes in the framework of average cost infinite-horizon Markov decision processes (MDPs). We develop efficient energy sharing algorithms, namely Q-learning algorithm with exploration mechanisms based on the $\\epsilon$-greedy method as well as upper confidence bound (UCB). We extend these algorithms by incorporating state and action space aggregation to tackle state-action space explosion in the MDP. We also develop a cross entropy based method that incorporates policy parameterization in order to find near optimal energy sharing policies. Through simulations, we show that our algorithms yield energy sharing policies that outperform the heuristic greedy method.", "histories": [["v1", "Tue, 17 Mar 2015 09:32:29 GMT  (2063kb,D)", "http://arxiv.org/abs/1503.04964v1", "38 pages, 10 figures"]], "COMMENTS": "38 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["sindhu padakandla", "prabuchandran k j", "shalabh bhatnagar"], "accepted": false, "id": "1503.04964"}, "pdf": {"name": "1503.04964.pdf", "metadata": {"source": "CRF", "title": "Energy Sharing for Multiple Sensor Nodes with Finite Buffers", "authors": ["Sindhu Padakandla"], "emails": ["shalabh}@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "Keywords: Energy Harvesting Sensor Nodes, Energy Sharing, Markov Decision Process, Q-Learning, State Aggregation."}, {"heading": "1 Introduction", "text": "In fact, most of us are in a position to put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we find ourselves in another world, in which we find ourselves in another world, in which we find ourselves in another world, in which we find ourselves in another world, in which we find ourselves in which we live ourselves, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live, in which we live in which we live in which we live in which we live in which we live in which we live, we live in which we live in which we live in which we live in which we live, we live in which we live in which we live in which we live in which we live in which we live, we live in which we live in which we live in which we live, we live in which we live in which we live in which we live in which we live in which we live in which we"}, {"heading": "1.1 Related Work", "text": "This year it is more than ever before."}, {"heading": "1.2 Our Contributions", "text": "\u2022 We look at the problem of efficient energy allocation in a system with multiple sensor nodes, each of which has its own data buffer, and a common EH source. \u2022 We model the above problem as an infinite horizon decision-making process on the average costs of Markov [4], [32] with an appropriate one-step cost function. Our goal in the MDP setting is to minimize the long-term average delay in data transmission. \u2022 We develop reinforced learning algorithms that provide optimal energy distribution strategies for the above problem. The applied learning method does not require system knowledge such as data and energy rates or cost structure, and learns online from the data collected. \u2022 To deal with the dimensionality of the state space of the MDP, we present approximation algorithms. These algorithms find approximately optimal energy distribution profiles when the action space of the MDP uncontrollable becomes available to a simulation algorithm (see the algorithms that are better demonstrated from our Q)."}, {"heading": "1.3 Organization of the Paper", "text": "The rest of the paper is broken down as follows: The next section describes the model, associated notation, and assumptions; Section 3 formulates the power distribution problem as MDP; Section 4 presents the RL algorithms used to solve the MDP; Section 5 highlights the need for approximate guidelines and gives a detailed explanation of the approximation algorithms we are developing for the problem; Section 6 presents the simulation results of our algorithms; Section 7 provides the conclusions and possible future directions; and finally, an appendix at the end of the paper provides evidence for two results."}, {"heading": "2 Model and Notation", "text": "We look at the problem of energy sharing at the level of energy generation between multiple sensor nodes. We present a polished, discrete time model (Fig. 2) for this problem. A sensor node in the network captures a random field and stores the captured data in a finite data buffer of size DMAX. In order to transfer the captured data to a core node, the sensor node must require energy, which it draws from an energy harvest source. The energy harvest source has an energy buffer of finite capacity EMAX. The common EH source is a derived entity in the model. It is generally a calculable battery, which is replaced by random energy harvests. We assume fragmentation of the data packets (fluid model) as in [36] and hence these are treated as bit strings."}, {"heading": "3 Energy Sharing Problem as an MDP", "text": "A Markov Decision Process (MDP) is a tuple of states, actions, transition probabilities and single-stage costs. Given that the MDP is in a certain state and an action is chosen by the controller, the MDP moves to a \"next\" state according to the prescribed transition probabilities. However, the objective of the controller is to choose a sequence of actions as a function of states to minimize a certain long-term goal (cost). We formulate the energy-sharing problem in the MDP setting using the long-term average cost criterion. The MDP formulation requires that we identify the states, actions and cost structure for the problem that nex.The state sk is a tuple that includes the data buffer level of all sensor nodes, the level of the energy buffer in the source, the data and energy arrivals in the past. Note that for1."}, {"heading": "4 Energy Sharing Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Background", "text": "Suppose that ir is a reference state in the MDP. Quantities p (i), i), i), i), and h (i) are the relative (or differential) costs that are the minimum of the difference between the expected costs to reach the state ir of i and the expected costs that arise if the costs per stage are the same. Quantities p (i), i), i), and S (i) are the probability that the system will move from state i to state i (i). We designate the model of Q (i, a), i), the optimal differential costs of possible learning (i, a), i), i), i), i), i), i), i), i), i), and i (i), i), i), i), i), and i), i), i), and i."}, {"heading": "4.2 Relative Value Iteration based Q-Learning", "text": "It is a model-free learning algorithm that assumes that the transition probabilities p (i, a, j) are unknown. First, the Q values are set to zero for all state action pairs, i.e., Q0 (i, a) = 0, (i, a) = 0, (i). Then, the transition probabilities p (i, a, j) are set for a state action pair visited during the simulation, as follows: Qk + 1 (i, a) = 0, (k) Qk (i). Then, the Q learning update [1] is performed for a state action pair: Qk + 1 (i, a)."}, {"heading": "5 Approximation Algorithms", "text": "The learning algorithm described in Section 4 is an iterative stochastic algorithm that learns the optimal energy distribution, and this method requires that the Q (s, a) values are stored for all (s, a) tuples. Q (s, a) values for each (s, a) tupel are updated in (21) over a number of iterations, using appropriate exploration, and these upgrades play a key role in determining the optimal control for a given state. However, these calculations are expensive for large action spaces, as each lookup operation and updation requires memory access. For example, if there are two nodes sharing energy and buffer sizes, EMAX = D MAX = 30, then the number of (s, a) tuples would be on the order of 106, which requires enormous computing time and storage space. This condition is exacerbated when the number of nodes increasing energy increases."}, {"heading": "5.1 Threshold based Features", "text": "The following sentence occupies the monotonicity property of the differential value Functions for the scenario in which there is a single node and an EH source. This simple scenario is considered q q q (q q q value) (q value) for clarity's sake. Let H value (q, E) be considered the differential value of the state (q, E). (25) Leave J (s) as the total cost incurred when we assume state s. Define the qman operator L: Rn \u2192 Rn as (LJ) = min T (s) (s, T) Proof."}, {"heading": "5.1.1 Clustering", "text": "The partition (or quantization level) di, (i,.., s) corresponds to a given range (xiL, x i U) and is specified, where xiL and xi U represent the prescribed lower and upper data buffer boundaries. Similarly, the quantization level ej, (j,.,.., r) (or energy buffer partition) corresponds to a specified interval (yiL, yi U), where yi L and yi U represent the prescribed lower and upper data buffer boundaries. Similarly, the quantization level ej, (j,.,.,.) & x \u2212 L (or energy buffer partition) corresponds to a specified interval (yiL, yi U), where yi L and yi U represent the prescribed lower and upper energy buffer boundaries."}, {"heading": "5.1.2 Aggregate States and Actions", "text": "We define an aggregate state as s \u2032 = {l1,.., ln + 1}, where li for 1 \u2264 i \u2264 n is the data buffer level for the ith node and ln + 1 is the energy buffer level. Thus, li \u0432 {1,..., s}, 1 \u2264 i \u2264 n and l n + 1 \u0432 {1,..., r} is a total action corresponding to the state s \u00b2 an n-tuple t \u2032 of the form t \u2032 = (t1,.., tn), where ti \u0445 {1,.., ln + 1}, 1 \u2264 i n. Each component in t \u2032 indicates an energy level. Taking into account the data level in all nodes, the controller decides on an energy level for each node. Thus, the energy level indicates the energy partition that can be supplied to the node. For example, if DMAX = E MAX = 15, s = 3 and there are two effects in the gate, the energy state v \u2032 1 (gate = 1, gate = 2, Gate = 1, \u2264 2, and Gries = 1)"}, {"heading": "5.1.3 Cardinality Reduction", "text": "Note that s D MAX, r E MAX. Let the aggregated state and action spaces be designated by S \u2032 or A. The aggregated state and action space has cardinality. Therefore, the cardinality of the state and action space is greatly reduced by aggregation. In the case of four nodes sharing energy from an EH source and E MAX = D MAX = 30, the cardinality of the state and action space without state aggregation is, for example, | S \u00d7 A | \u2248 309. However, with four partitions each for the data and energy buffers, the cardinality of the state and action space with aggregation is | S \u00b2 A \u00b2 | \u2248 49."}, {"heading": "5.2 Approximate Learning Algorithm", "text": "We now explain our approximate learning algorithm for the energy distribution problem. It is based on Q-learning and state aggregation. Although the simple Q-learning algorithm described in Section 4 requires complete state information and is not mathematically efficient in terms of large state action spaces, its state aggregation-based counterpart requires significantly less computing and storage space. Our experiments also show that we do not make major compromises in terms of the guidelines achieved (see Fig. 9b)."}, {"heading": "5.2.1 Method", "text": "The Q value Q (s, \"t\") indicates how good an aggregated state stakeholder is. The algorithm proceeds with the following update rule: Qk + 1 (s, \"t\") = (1 \u2212 \u03b1 (k)) Qk (s, \"t\") (c, \"t\") + minb \"A\" (j \") Qk\" (j, b \") \u2212 min\" A \"(r\") Qk \"(r,\" u \")) Qk\" (k \") (k\") (c, t \") + minb\" \"(c\") Qk \"(c\") \"(c, b\") \u2212 min. \"(c\") Qk \"(r,\" u \"))\" (30), where j \"is the aggregate state achieved by the simulation of measures."}, {"heading": "5.2.2 Energy distribution", "text": "Note that once a total action is selected for a state, the energy dividend is random compliance with the selected action levels. For example, suppose there are two sensor nodes in the system. Data and energy buffers each have three partitions, and thus s = 3, r = 3. Here, y1L = 0 and y3 U = E MAX belong. Suppose the number of energy bits in the energy buffer is z and these bits belong to partition 3. Let the number of data bits be on nodes 1 and 2 x or y, respectively. Here, x and y belong to partition 2. Thus, the total state is (2, 2, 3). The controller decides on the total action (1, 2)."}, {"heading": "5.3 Cross Entropy using State Aggregation and Policy Parame-", "text": "The method of cross-entropy is an iterative approach ([35]) that we use to find near optimal stationary randomized policies for the energy distribution problem. The algorithm searches for a policy in the space of all stationary randomized policies in a systematic manner. We define a class of randomized stationary policies that are parameterized by a vector. In order to follow the cross-entropy approach and obtain the optimal distribution of RM, we treat each component previ, i, i, a) describes the probability of taking action when the state s is encountered within the framework of the policies corresponding to the vector. To pursue the cross-entropy approach and obtain the optimal distribution of RM, we treat each component previ, i, i, 2."}, {"heading": "5.3.1 Policy Parameterization", "text": "Let us consider the average costs of the system in case of parameterization by \u03b8 = (\u03b81,..., \u03b8M) >. An optimal policy \u03b8 \u043a minimizes the average costs across all parameterizations. That is, \u03b8 \u043c = arg min \u03b8. An example of parameterized randomized strategies that we use for the experiments (with state aggregation) in this paper are the parameterized Boltzmann strategies, which have the following form: \u03c0\u03b8 (s, a) = Requirements for the aggregated state. The parameterized Boltzmann strategies are often used in approximation techniques ([8, 7, 1, 37, 38] which are an M-dimensional characteristic vector for the aggregated state actions tuples (s, a) and \u0441sa \u0441RM (s). The parameterized Boltzmann strategies are often used in approximation techniques that deal with 38, 38, [\u00b5s] randomized state actions."}, {"heading": "5.3.2 Method", "text": "The algorithms run as follows: in the first phase, the iteration index is not set to phase 1: 1. Sample parameters are simulated and the average cost of each measure is calculated; the second phase is drawn independently of the normal distributions. (The second phase is drawn independently of the normal distributions. (The second phase is drawn independently of the normal distributions.) The third phase is carried out independently of the third phase. (The third phase is carried out independently of the fourth phase.) The third phase is carried out independently of the fourth phase. (The third phase is carried out independently of the fourth phase.) The third phase is carried out independently of the fourth phase."}, {"heading": "6 Simulation Results", "text": "In this section, we show simulation results for the energy-sharing algorithms we have described in sections 4 and 5. To make a comparison, we implement the greedy heuristic method in the case where the function g has a nonlinear form. In addition, we implement Q-Learning to find optimal strategies for the case in which we consider the sum of the data at all nodes and the available energy as a state. These methods are as follows: 1. Greedy: This method takes as input the level of the data qik at all nodes and returns the energy based on the request. Since g (x) is the number of data bits that x bits of the allocated energy can be sent, g \u2212 1 (y) indicates the amount of energy needed to send y bits of the data. Enter the energy available in the source is ek at step k. The greedy algorithm then returns tk units of energy, where tk = assigned energy state."}, {"heading": "6.1 Experimental Setup", "text": "The algorithms described in section 4 are fixed with two nodes and an energy source. We consider the following settings: 1. In the case of common Markov data arrival and Markovian energy arrival processes we consider energy buffer size of 20 and data buffer size of 10. The data inputs develop as: Xk = AXk \u2212 1 + \u03c9, where A is a fixed matrix of coefficients and a random noise (or disturbance) > is a 2 \u00d7 1 random noise (or disturbance) vector. Here A = (0.2 0.3 0.3 0.2) The energy arrival develops as Yk = bYk \u2212 1 + probability, where random noise (or disturbance) is variable and b = 0.5 is a fixed coefficient. The components in vector arrangement and vector are distributed Poisson. In the simulations we vary the mean value of random noise variable."}, {"heading": "6.2 Results", "text": "The fact is that we will be able to move to another world, in which we are able to change the world we are in, and in which we are able to change the world we are in, \"he said."}, {"heading": "7 Conclusions and Future Work", "text": "We have presented an MDP model for this problem and an algorithm that determines the optimal amount of energy to be provided to each node at a time of decision. It minimizes the sum of (data) queues in the data buffers by finding the optimal energy distribution profile. To deal with the curse of dimensionality, we have also proposed approximation algorithms that effectively use state aggregation to reduce computational complexity. Numerical experiments have shown that our algorithms exceed the algorithms described in Section 6.Our future work would include the use of threshold tuning for state aggregation, gradient-based approaches and based adaptation methods for policy approximation. The partitions formed for clustering the state space (Section 5.1) can be improved by approximating fixed limits."}, {"heading": "Acknowledgements", "text": "The authors thank all three reviewers of [29] for their detailed comments, which have significantly improved the quality of this report and the manuscript [29], which has been partially supported by projects of the Defense Research and Development Organization (DRDO) and the Department of Science and Technology (DST) of the Indian government."}], "references": [{"title": "Learning algorithms for markov decision processes with average cost", "author": ["Jinane Abounadi", "D Bertsekas", "Vivek S Borkar"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Transmit power control policies for energy harvesting sensors with retransmissions", "author": ["Anup Aprem", "Chandra R Murthy", "Neelesh B Mehta"], "venue": "Selected Topics in Signal Processing, IEEE Journal of,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Dynamic programming and optimal control, Vol. I", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Dynamic Programming and Optimal Control, Vol. II", "author": ["Dimitri P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Stochastic Recursive Algorithms for Optimization, volume 434 of Lecture Notes in Control and Information Sciences", "author": ["S Bhatnagar", "H L Prasad", "L A Prashanth"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Feature search in the grassmanian in online reinforcement learning", "author": ["Shalabh Bhatnagar", "Vivek S Borkar", "K J Prabuchandran"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Introduction to rare event", "author": ["James Bucklew"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Power allocation strategies in energy harvesting wireless cooperative networks", "author": ["Zhiguo Ding", "S.M. Perlaza", "I Esnaola", "H.V. Poor"], "venue": "Wireless Communications, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Optimal throughput-delay scaling in wireless networks - part i: the fluid model", "author": ["A El Gamal", "J. Mammen", "B. Prabhakar", "D. Shah"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Learning rates for Q-learning", "author": ["Eyal Even-Dar", "Yishay Mansour"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Ultra-low-power rfid-based sensor mote", "author": ["Nicolas Gay", "W Fischer"], "venue": "In Sensors, 2010 IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Power constrained and delay optimal policies for scheduling transmission over a fading channel", "author": ["Munish Goyal", "Anurag Kumar", "Vinod Sharma"], "venue": "Twenty- Second Annual Joint Conference of the IEEE Computer and Communications. IEEE Societies,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Energy cooperation in energy harvesting communications", "author": ["B. Gurakan", "O. Ozel", "Jing Yang", "S. Ulukus"], "venue": "Communications, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Optimal energy allocation for wireless communications powered by energy harvesters", "author": ["Chin Keong Ho", "Rui Zhang"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Power management in energy harvesting sensor networks", "author": ["Aman Kansal", "Jason Hsu", "Sadaf Zahedi", "Mani B Srivastava"], "venue": "ACM Transactions on Embedded Computing Systems (TECS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "An environmental energy harvesting framework for sensor networks", "author": ["Aman Kansal", "Mani B Srivastava"], "venue": "In Low Power Electronics and Design,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "On actor-critic algorithms", "author": ["Vijay R Konda", "John N Tsitsiklis"], "venue": "SIAM journal on Control and Optimization,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "The cross-entropy method for continuous multi-extremal optimization", "author": ["Dirk P Kroese", "Sergey Porotsky", "Reuven Y Rubinstein"], "venue": "Methodology and Computing in Applied Probability,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Power-optimal scheduling for a green base station with delay constraints", "author": ["Anusha Lalitha", "Santanu Mondal", "Vinod Sharma"], "venue": "In Communications (NCC),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "The cross entropy method for fast policy search", "author": ["Shie Mannor", "Reuven Y Rubinstein", "Yohai Gat"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Basis function adaptation in temporal difference reinforcement learning", "author": ["Ishai Menache", "Shie Mannor", "Nahum Shimkin"], "venue": "Annals of Operations Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Wireless sensor networks with energy harvesting technologies: a game-theoretic approach to optimal energy management", "author": ["Dusit Niyato", "Ekram Hossain", "Mohammad M Rashid", "Vijay K Bhargava"], "venue": "Wireless Communications,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Transmission with energy harvesting nodes in fading wireless channels: Optimal policies", "author": ["O. Ozel", "K. Tutuncuoglu", "Jing Yang", "Sennur Ulukus", "A Yener"], "venue": "Selected Areas in Communications, IEEE Journal on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Adaptive transmission policies for energy harvesting wireless nodes in fading channels", "author": ["Omur Ozel", "Kaya Tutuncuoglu", "Jing Yang", "Sennur Ulukus", "Aylin Yener"], "venue": "In Information Sciences and Systems (CISS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Optimal broadcast scheduling for an energy harvesting rechargeable transmitter with a finite capacity battery", "author": ["Omur Ozel", "Jing Yang", "Sennur Ulukus"], "venue": "Wireless Communications, IEEE Transactions on,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Energy sharing for multiple sensor nodes with finite buffers", "author": ["Sindhu Padakandla", "K J Prabuchandran", "Shalabh Bhatnagar"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Q-learning based energy management policies for a single sensor node with finite buffer", "author": ["K J Prabuchandran", "Sunil Kumar Meena", "Shalabh Bhatnagar"], "venue": "Wireless Communications Letters, IEEE,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Threshold tuning using stochastic optimization for graded signal control", "author": ["L.A. Prashanth", "S. Bhatnagar"], "venue": "Vehicular Technology, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}, {"title": "Optimal periodic sensor schedule for steady-state estimation under average transmission energy constraint", "author": ["Zhu Ren", "Peng Cheng", "Jiming Chen", "Ling Shi", "Youxian Sun"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Dynamic sensor transmission power scheduling for remote state estimation", "author": ["Zhu Ren", "Peng Cheng", "Jiming Chen", "Ling Shi", "Huanshui Zhang"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "The cross-entropy method for combinatorial and continuous optimization", "author": ["Reuven Rubinstein"], "venue": "Methodology and computing in applied probability,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Optimal energy management policies for energy harvesting sensor nodes", "author": ["Vinod Sharma", "Utpal Mukherji", "Vinay Joseph", "Shrey Gupta"], "venue": "IEEE Transactions on Wireless Communications,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "Asynchronous stochastic approximation and q-learning", "author": ["John N Tsitsiklis"], "venue": "Machine Learning,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1994}, {"title": "Short-term throughput maximization for battery limited energy harvesting nodes", "author": ["Kaya Tutuncuoglu", "Aylin Yener"], "venue": "In Communications (ICC),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Sum-rate optimal power policies for energy harvesting transmitters in an interference channel", "author": ["Kaya Tutuncuoglu", "Aylin Yener"], "venue": "Communications and Networks, Journal of,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Cooperative energy harvesting communications with relaying and energy sharing", "author": ["Kaya Tutuncuoglu", "Aylin Yener"], "venue": "In Information Theory Workshop (ITW),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Transmission completion time minimization in an energy harvesting system", "author": ["Jing Yang", "Sennur Ulukus"], "venue": "In Information Sciences and Systems (CISS),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "Optimal packet scheduling in a multiple access channel with energy harvesting transmitters", "author": ["Jing Yang", "Sennur Ulukus"], "venue": "Communications and Networks, Journal of,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Optimal packet scheduling in an energy harvesting communication system", "author": ["Jing Yang", "Sennur Ulukus"], "venue": "Communications, IEEE Transactions on,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "1) is a single unit on which sensors with different functionalities are arranged (see [13]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "Another scenario (that however we do not consider here) where our techniques are applicable is the case of downlink transmissions [22], where a base station (BS) maintains a separate data queue for each individual sensor node.", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "Optimizing energy usage in battery-powered sensors is addressed in [33, 34].", "startOffset": 67, "endOffset": 75}, {"referenceID": 31, "context": "Optimizing energy usage in battery-powered sensors is addressed in [33, 34].", "startOffset": 67, "endOffset": 75}, {"referenceID": 30, "context": "The problem of designing appropriate sensor schedules of sensor data transmission is discussed in [33].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "The authors in [33] consider battery-powered sensor nodes, each of which needs to minimize the energy utilized for data transmission.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "The objective in [33] is to find optimal periodic sensor schedules which minimize the estimation error at the fusion node and optimize energy usage.", "startOffset": 17, "endOffset": 21}, {"referenceID": 31, "context": "In [34], the authors consider battery-powered sensors with two transmission power levels.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "The objective in [34] is to minimize the average expected error in state estimation under energy constraint.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "The works [33, 34] consider the problem of efficient energy usage in battery powered sensors.", "startOffset": 10, "endOffset": 18}, {"referenceID": 31, "context": "The works [33, 34] consider the problem of efficient energy usage in battery powered sensors.", "startOffset": 10, "endOffset": 18}, {"referenceID": 15, "context": "An early work in rechargeable sensors is [18].", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "The authors of [18] present a framework for the sensor network to adaptively learn the spatio-temporal characteristics of energy availability and provide algorithms to use this information for task sharing among nodes.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "In [17], the irregular and spatio-temporal characteristics of harvested energy are considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Compared to [18, 17], we focus on minimizing the delay in data transmission from the nodes and also ensuring energy neutral operation.", "startOffset": 12, "endOffset": 20}, {"referenceID": 14, "context": "Compared to [18, 17], we focus on minimizing the delay in data transmission from the nodes and also ensuring energy neutral operation.", "startOffset": 12, "endOffset": 20}, {"referenceID": 37, "context": "The scenario of a single EH transmitter with limited battery capacity is considered in [41, 26].", "startOffset": 87, "endOffset": 95}, {"referenceID": 23, "context": "The scenario of a single EH transmitter with limited battery capacity is considered in [41, 26].", "startOffset": 87, "endOffset": 95}, {"referenceID": 23, "context": "In [26], the transmitter communicates in a fading channel, whereas in [41], no specific constraints on the channel are considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "In [26], the transmitter communicates in a fading channel, whereas in [41], no specific constraints on the channel are considered.", "startOffset": 70, "endOffset": 74}, {"referenceID": 37, "context": "The problem of finding the optimal transmission policy to maximize the short-term throughput of an EH transmitter is considered in [41].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "In [26], the transmitter gets channel state information and the node has to adaptively control the transmission rate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "The authors in [26] develop an online algorithm which determines the transmit power at every instant by taking into account the amount of energy available and channel state.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 33, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 27, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 42, "context": "The efficient usage of energy in a single EH node has been dealt with in some recent works [25, 36, 30, 46].", "startOffset": 91, "endOffset": 107}, {"referenceID": 22, "context": "A channel and data queue aware sleep/active/listen mechanism in this direction is proposed in [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 33, "context": "The model proposed in [36, 30] considers a single EH sensor node with finite energy and data buffers.", "startOffset": 22, "endOffset": 30}, {"referenceID": 27, "context": "The model proposed in [36, 30] considers a single EH sensor node with finite energy and data buffers.", "startOffset": 22, "endOffset": 30}, {"referenceID": 33, "context": "In [36], a linear conversion function is used and optimal energy management policies are provided for the same.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "However, in the case of non-linear conversion function, [36] provides certain heuristic policies.", "startOffset": 56, "endOffset": 60}, {"referenceID": 27, "context": "In [30], a nonlinear conversion function is used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 23, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 33, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 22, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 27, "context": "Unlike [41, 26, 36, 25, 30], our work deals with multiple sensors sharing a common EH power source.", "startOffset": 7, "endOffset": 27}, {"referenceID": 42, "context": "Data packet scheduling problems in EH sensor networks are considered in [46] and [45].", "startOffset": 72, "endOffset": 76}, {"referenceID": 41, "context": "Data packet scheduling problems in EH sensor networks are considered in [46] and [45].", "startOffset": 81, "endOffset": 85}, {"referenceID": 42, "context": "It is assumed in [46] that a single EH node has separate data and energy queues, while the data sensed and energy harvested are random.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "The same assumption is made for each sensor in a two-sensor communication system considered in [45].", "startOffset": 95, "endOffset": 99}, {"referenceID": 42, "context": "In [46]([45]) the objective is to minimize the time by which all data packets from the node(s) are transmitted (to the fusion node).", "startOffset": 3, "endOffset": 7}, {"referenceID": 41, "context": "In [46]([45]) the objective is to minimize the time by which all data packets from the node(s) are transmitted (to the fusion node).", "startOffset": 8, "endOffset": 12}, {"referenceID": 38, "context": "A two-user Gaussian interference channel with two EH sensor nodes and receivers is considered in [42].", "startOffset": 97, "endOffset": 101}, {"referenceID": 42, "context": "In contrast to the models developed in [46, 45, 42], our model assumes multiple sensors sharing a common energy source.", "startOffset": 39, "endOffset": 51}, {"referenceID": 41, "context": "In contrast to the models developed in [46, 45, 42], our model assumes multiple sensors sharing a common energy source.", "startOffset": 39, "endOffset": 51}, {"referenceID": 38, "context": "In contrast to the models developed in [46, 45, 42], our model assumes multiple sensors sharing a common energy source.", "startOffset": 39, "endOffset": 51}, {"referenceID": 7, "context": "Cooperative wireless network settings are considered in [10, 15, 43].", "startOffset": 56, "endOffset": 68}, {"referenceID": 12, "context": "Cooperative wireless network settings are considered in [10, 15, 43].", "startOffset": 56, "endOffset": 68}, {"referenceID": 39, "context": "Cooperative wireless network settings are considered in [10, 15, 43].", "startOffset": 56, "endOffset": 68}, {"referenceID": 12, "context": "Three different network settings with energy transfer between nodes are considered in [15].", "startOffset": 86, "endOffset": 90}, {"referenceID": 39, "context": "In [43], there exists an EH relay node and multiple other EH source nodes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [10], multiple pairs of sources and destinations communicate via an EH relay node.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "A multi-user additive white Gaussian noise (AWGN) broadcast channel comprising of a single EH transmitter and M receivers is considered in [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 25, "context": "The objective in [28] is to find a transmission policy that minimizes the time by which all the bits are transmitted to the receivers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "\u2022 We model the above problem as an infinite-horizon average cost Markov decision process (MDP) [4],[32] with an appropriate single-stage cost function.", "startOffset": 95, "endOffset": 98}, {"referenceID": 29, "context": "\u2022 We model the above problem as an infinite-horizon average cost Markov decision process (MDP) [4],[32] with an appropriate single-stage cost function.", "startOffset": 99, "endOffset": 103}, {"referenceID": 33, "context": "We assume fragmentation of data packets (fluid model) as in [36] and hence these will be treated as bit strings.", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 40, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 13, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 24, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 11, "context": "We have assumed the function g to be monotonically non-decreasing and concave as with other references ([36, 44, 16, 27, 14]).", "startOffset": 104, "endOffset": 124}, {"referenceID": 2, "context": "This is the standard description for the state evolution for an MDP (see Chapter 1 in [3]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "This is the fluid model as described in [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 33, "context": "The fluid model assumption (data discretization) has been made in [36, 14, 46].", "startOffset": 66, "endOffset": 78}, {"referenceID": 11, "context": "The fluid model assumption (data discretization) has been made in [36, 14, 46].", "startOffset": 66, "endOffset": 78}, {"referenceID": 42, "context": "The fluid model assumption (data discretization) has been made in [36, 14, 46].", "startOffset": 66, "endOffset": 78}, {"referenceID": 1, "context": "Energy discretization has been considered in some previous works [2, 36].", "startOffset": 65, "endOffset": 72}, {"referenceID": 33, "context": "Energy discretization has been considered in some previous works [2, 36].", "startOffset": 65, "endOffset": 72}, {"referenceID": 0, "context": "Then \u2200k \u2265 0, the Q-learning update [1] for a state-action pair visited during simulation is carried out as follows:", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "These conditions on step sizes guarantee the convergence of Q-learning to the optimal state-action value function, see [1] for a proof of convergence of the algorithm.", "startOffset": 119, "endOffset": 122}, {"referenceID": 36, "context": "The convergence rates for the discounted Q-learning have been studied in [39, 19, 12].", "startOffset": 73, "endOffset": 85}, {"referenceID": 16, "context": "The convergence rates for the discounted Q-learning have been studied in [39, 19, 12].", "startOffset": 73, "endOffset": 85}, {"referenceID": 9, "context": "The convergence rates for the discounted Q-learning have been studied in [39, 19, 12].", "startOffset": 73, "endOffset": 85}, {"referenceID": 36, "context": "The finite-time bounds to reach an -optimal policy by following the Q-learning rule are given in [39, 19, 12].", "startOffset": 97, "endOffset": 109}, {"referenceID": 16, "context": "The finite-time bounds to reach an -optimal policy by following the Q-learning rule are given in [39, 19, 12].", "startOffset": 97, "endOffset": 109}, {"referenceID": 9, "context": "The finite-time bounds to reach an -optimal policy by following the Q-learning rule are given in [39, 19, 12].", "startOffset": 97, "endOffset": 109}, {"referenceID": 29, "context": "(29) As a consequence of the relative value iteration scheme ([32]), when k \u2192 \u221e, LH \u2192 H\u2217 with H(qr, Er) = \u03bb \u2217.", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "In the case of MDP with large state-action space, one goes for function approximation based methods (see Chapter 8 in [37]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 34, "context": "However, if one combines Q-learning with function approximation, we do not have convergence guarantees to the optimal policy unlike Q-learning without function approximation (Q-learning with tabular representation [37]).", "startOffset": 214, "endOffset": 218}, {"referenceID": 32, "context": "The cross-entropy method is an iterative approach ([35]) that we apply to find near-optimal stationary randomized policies for the energy sharing problem.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "The Cross Entropy method is an adaptive importance sampling [9] technique.", "startOffset": 60, "endOffset": 63}, {"referenceID": 18, "context": "The Gaussian distribution used as the importance sampling distribution yields analytical updation formulas (32) for the mean and variance parameters (see [21]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 5, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 0, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 34, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 35, "context": "The parameterized Boltzmann policies are often used in approximation techniques ([8, 7, 1, 37, 38]) which deal with randomized policies.", "startOffset": 81, "endOffset": 98}, {"referenceID": 34, "context": "The Boltzmann distribution for action selection fits these requirements and is a frequently used distribution in the literature (see [37, 38]) on policy learning and approximation algorithms.", "startOffset": 133, "endOffset": 141}, {"referenceID": 35, "context": "The Boltzmann distribution for action selection fits these requirements and is a frequently used distribution in the literature (see [37, 38]) on policy learning and approximation algorithms.", "startOffset": 133, "endOffset": 141}, {"referenceID": 21, "context": "The meta-parameters {(\u03bci, \u03c3 i), 1 \u2264 i \u2264 M} are updated (refer [24]) in this phase.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "However, obtaining gradient estimates in actorcritic architecture is hard as it leads to large variance [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "In [23], the authors sampled from the entire transition probability matrix to calculate the score function and tested on problems with only small state-action space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "This has been derived in [36] for the case of single sensor.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "The performance of the algorithm proposed in [30] with non-linear g is compared with the performance of the greedy method.", "startOffset": 45, "endOffset": 49}, {"referenceID": 28, "context": "1) can be improved by tuning the partition thresholds (see [31]).", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "Gradient based methods [6], [20], [8] approximate the policy using parameter \u03b8 and a set of given (fixed) basis functions {fk : 1 \u2264 k \u2264 n}.", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "Gradient based methods [6], [20], [8] approximate the policy using parameter \u03b8 and a set of given (fixed) basis functions {fk : 1 \u2264 k \u2264 n}.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "Basis adaptation methods [24], [7] start with a given set of basis functions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 5, "context": "Basis adaptation methods [24], [7] start with a given set of basis functions.", "startOffset": 31, "endOffset": 34}, {"referenceID": 26, "context": "The authors would like to thank all the three reviewers of [29] for their detailed comments that significantly helped in improving the quality of this report and the manuscript [29].", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "The authors would like to thank all the three reviewers of [29] for their detailed comments that significantly helped in improving the quality of this report and the manuscript [29].", "startOffset": 177, "endOffset": 181}], "year": 2015, "abstractText": "We consider the problem of finding optimal energy sharing policies that maximize the network performance of a system comprising of multiple sensor nodes and a single energy harvesting (EH) source. Sensor nodes periodically sense the random field and generate data, which is stored in the corresponding data queues. The EH source harnesses energy from ambient energy sources and the generated energy is stored in an energy buffer. Sensor nodes receive energy for data transmission from the EH source. The EH source has to efficiently share the stored energy among the nodes in order to minimize the long-run average delay in data transmission. We formulate the problem of energy sharing between the nodes in the framework of average cost infinite-horizon Markov decision processes (MDPs). We develop efficient energy sharing algorithms, namely Q-learning algorithm with exploration mechanisms based on the -greedy method as well as upper confidence bound (UCB). We extend these algorithms by incorporating state and action space aggregation to tackle state-action space explosion in the MDP. We also develop a cross entropy based method that incorporates policy parameterization in order to find near optimal energy sharing policies. Through simulations, we show that our algorithms yield energy sharing policies that outperform the heuristic greedy method.", "creator": "LaTeX with hyperref package"}}}