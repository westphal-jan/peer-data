{"id": "1503.04996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "On Extreme Pruning of Random Forest Ensembles for Real-time Predictive Applications", "abstract": "Random Forest (RF) is an ensemble supervised machine learning technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empiricallthat ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. First, it investigates how data clustering (a well known diversity technique) can be applied to identify groups of similar decision trees in an RF in order to eliminate redundant trees by selecting a representative from each group (cluster). Second, these likely diverse representatives are then used to produce an extension of RF termed CLUB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, and mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 15 real datasets from the UCI repository prove the superiority of our proposed extension over the traditional RF. Most of our experiments achieved at least 95% or above pruning level while retaining or outperforming the RF accuracy.", "histories": [["v1", "Tue, 17 Mar 2015 11:01:37 GMT  (244kb,D)", "http://arxiv.org/abs/1503.04996v1", "10 pages, 4 Figures"]], "COMMENTS": "10 pages, 4 Figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["khaled fawagreh", "mohamad medhat gaber", "eyad elyan"], "accepted": false, "id": "1503.04996"}, "pdf": {"name": "1503.04996.pdf", "metadata": {"source": "CRF", "title": "On Extreme Pruning of Random Forest Ensembles for Real-time Predictive Applications", "authors": ["Khaled Fawagreh", "Mohamed Medhat Gaber", "Eyad Elyan"], "emails": ["(k.fawagreh@rgu.ac.uk,", "m.gaber1@rgu.ac.uk,", "e.elyan@rgu.ac.uk)"], "sections": [{"heading": null, "text": "This is an approach in which multiple models are used to solve the same problem [1] [2]. Since individual classification systems have limited predictive power [4] [5], the classification of the ensemble was developed to achieve better predictive performance [5]. In such an ensemble, multiple classification systems are used. In its basic mechanism, the majority vote is then used to determine the class label for unlabeled instances in which each classification into the ensemble is predicted. Predicted, the class label of the instance is taken into account. Once all classification classes have been challenged, the largest number of votes is considered as the final decision of the ensemble. Three widely used ensemble approaches could be identified, namely, enhanced, classified and classified."}, {"heading": "II. RELATED WORK", "text": "This is because most people are able to decide for themselves what they want and what they want to do."}, {"heading": "A. Diversity Creation Methods", "text": "G. Brown et al. [21] have summarized the work done so far in this area from two main perspectives: the first is a review of the various attempts that have been made to create a formal basis for diversity; the second, more relevant to this paper, is an overview of the different techniques used to create different ensembles; and the second, two types of diversity methods have been identified: implicit and explicit. While implicit methods tend to use randomness to create different pathways in the hypotheses space, explicit methods choose different paths in space deterministically; in the light of these definitions, dredging and boosting are classified as implicit and explicit methods in the previous section. G. Brown et al. [21] also categorize ensemble diversity techniques into three categories: starting point in the hypotheses space, set of accessible hypotheses, and manipulation of training data are implicit and explicit respectable methods."}, {"heading": "B. Diversity Measures", "text": "Regardless of the diversity generation technique used, measures of diversity were developed to measure the diversity of a particular technique, or perhaps to misclassify the diversity of two techniques. Tang et al. [22] presented a theoretical analysis of six existing measures of diversity: measures of diversity [23], double error measures [24], KW variance [25], interrater agreement [26], generalized diversity [27] and measures of difficulty [26]. The aim was not only to show the underlying relationships between them, but also to relate them to the concept of margin, which is one of the factors for the success of ensemble learning algorithms. Suffice it to describe the first two measures as being outside the scope of this paper. Measures of diversity are used to measure the diversity between two base classifiers, which is one of the factors for the success of ensemble learning algorithms. Suffice it to describe the first two measures as lying outside the scope of this paper. NF measures are used to measure the diversity between two base classifiers, which are classified as NF + 10, where two instances were classified as 001 and 00j = 1j were incorrect:"}, {"heading": "III. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Motivation", "text": "As already mentioned, RF algorithms tend to build between 100 and 500 trees [12]. Some empirical and theoretical studies have also clearly shown that adding more trees to an RF beyond a certain number (i.e. 500) does not necessarily improve RF accuracy [28]. Our research aims to prune RF ensembles by creating a subset of the original trees that are significantly smaller and yet have an accuracy performance at least as good as that of the original RF from which they were derived. In other words, we aim to find the optimal or near-optimal number of trees used to generate a precise RF."}, {"heading": "B. Random Forest", "text": "In fact, the method combines sampling approach [7], and the random selection of attributes independently of Ho [30] and Amit and Common [31] to construct a collection of decision trees with controlled deviations. Instances in the sample are referred to as in-bag instances, and the remaining instances (about 36%) are referred to as out-of-bag instances. Each tree in the group acts as a base classifier to determine the class."}, {"heading": "IV. CLUSTERING", "text": "Contrary to classification, clustering is an unattended learning technique that attempts to organize objects into groups whose members are in some ways similar. Each group is called a cluster, so a cluster is a collection of objects that are similar between them and dissimilar to the objects of other clusters. Clustering is considered a data exploration method because it helps reveal the natural grouping in a dataset without prior knowledge of the groups to be produced. One of the earliest and most popular cluster algorithms is called K-Means. Clustering was developed by MacQueen [39] in the late 1960s, and despite its sensitivity, it is still considered one of the 5 most widely used algorithms, mainly because of its simplicity, efficiency, and empirical success."}, {"heading": "V. PROPOSED EXTENSION", "text": "In this section, we propose an extension of RF called CLUBDRF, which produces a child RF that is 1) much smaller than the parent RF and 2) has an accuracy at least as good as that of the parent RF. In the course of this work, we will refer to the parent / original traditional random forest as RF and the resulting child RF based on our method as CLUB-DRF. Training SetRandom Forest Algorithm."}, {"heading": "A. CLUstering-Based Diverse Random Forest (CLUB-DRF)", "text": "The CLUB DRF extension uses a cluster-based technique to produce various groups of trees. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < / < / < / < / < / < / < / < / < / < / < < < < < < < < < < < < < \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";\"; \";"}, {"heading": "B. Diversity Measure", "text": "Here we propose a simple measure of diversity to measure the diversity of two classifiers working with a binary and multi-class classification. If we have two classifiers hj and hk and a training set T of size n. Let C (tl, si) designate the class name that we obtain after classifying sample si in the training set T. The higher the number of discrepancies between the two classifiers, the higher the diversity. Let's say, for example, that we have a training set that consists of 10 training samples T = {s1, s2, s3, s4, s5, s6, s7, s8, s9, s10}. Suppose that there are three training samples T = {s6, s10}, which are for the values b > b, s4, (b > T), (ltc, & ltc, & ltc), and two classifiers t1 and t2."}, {"heading": "VI. EXPERIMENTAL STUDY", "text": "For our experiments, we used 15 real data sets with different properties from the UCI repository [46]. To apply the holdout test method, each data set was divided into 2 sets: Training and Test. Two thirds (66%) were reserved for training and the rest (34%) for the test. Each data set consists of input variables (features) and an output variable. The latter refers to the class name whose value is predicted in each experiment. First, the more trees we have, the more diverse we can become. Second, the more trees we have, the more trees we have, the less likely we are to run into empty clusters [47]. The CLUB DRF algorithm described above was implemented using the Java CLUP programming language, whereby the average F rate for WaPI rate 44 was determined."}, {"heading": "A. Results", "text": "Table V compares the performance of CLUB-DRF and RF on the 15 datasets. To demonstrate the superiority of CLUB-DRF, we have highlighted the average accuracy of CLUB-DRF if it is higher than that of RF, and the accuracy if it is equal to that of RF (only one occurrence found in the unsaved dataset). If we look more closely at this table, we find that CLUB-DRF was at least as good as RF on 13 datasets. Interestingly, CLUB-DRF has completely lost the RF on 6 datasets, regardless of its size, namely breast cancer, pasture, eucalyptus, glass, sonar and vehicle, while CLUBDRF lost the difference by a tiny fraction of less than 1% on only 2 datasets (testing and tuning)!"}, {"heading": "B. Analysis", "text": "Figure 2 compares the accuracy rate of HF and CLUB-DRF with the number of data sets superior to each other based on different sizes of CLUB-DRF. With the exception of size 35, the figure clearly shows that CLUB-DRF actually performed at least as well as RF. Remarkably, the smaller the ensemble size, the better our method performs, due to the greater diversity of the ensemble. 5 or 10 representatives guarantee 7 more distant trees selected to form the ensemble. If we switch to a larger number of clusters, and thus larger ensemble sizes, CLUB-DRF moves toward the original RF. This may be especially when tree behavior is no longer distinguishable and the creation of more clusters does not cause ensemble diversity."}, {"heading": "C. Pruning Level", "text": "By applying the cluster technique proposed above, we succeeded in achieving two objectives: First, CLUB DRF ensembles were produced with different trees. Second, and most importantly, we succeeded in significantly reducing the size of the HF. The resulting CLUB DRF ensembles performed at least as well as the original HF, but largely exceeded the original HF, as discussed above. In ensemble editing, an intersection level refers to the reduction ratio between the original ensemble and the trimmed DRF. If, for example, the size of the original ensemble is 500 trees and the trimmed tree is of size 50, then 100% \u2212 50500 x 100% = 90% is the intersection level that was reached in the trimmed ensemble. This means that the trimmed ensembles are 90% smaller than the original ensemble and the trimmed DRF. Table I shows the intersection levels that the intersection levels of the first column in this table are the maximum possible intersection level for a CLUBDRF, the RF, the RF, the RF, the RF, the RF, the RF, the second, the RF, the RF, the old, the RF, the RF, the RF, the RF, the RF, the cluster technology, the cluster, the cluster technology, the cluster, the cluster, the cluster, the cluster, the cluster, the cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster and cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster and cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster and cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, cluster, clu"}, {"heading": "D. Performance Comparison with Pruned Neural Network Ensemble", "text": "In a study by Lazarevic and Obradovic [16], in which clustering was also used for the circumcision of neural network ensembles, the researchers used diabetes and glass data sets, which we also used in our experiments. Table II shows the accuracy of their entire and cropped ensembles with RF and our CLUBDRF. For both data sets, our CLUB-DRF was superior to their cropped ensembles. In particular, with the glass data set, CLUB-DRF achieved a clear progression in predictive accuracy with a healthy increase of 7.06% over the cropped neural network."}, {"heading": "E. Bias/Variance Analysis", "text": "The bias measures the difference between the predicted class value of the classifier and the true value of the predicted class label. However, the variance measures the variability of the classifier's prediction as a result of fluctuations in education. For a classifier, it should maintain low bias and variability. There is a trade-off between a classifier's ability to minimize bias and variance. Understanding these two types of measures can help us diagnose the results of the classification and avoid the error of match and variability."}, {"heading": "VII. CONCLUSION AND FUTURE DIRECTIONS", "text": "We have used clustering to produce groups of similar trees and select a representative tree from each group. These likely different trees are then used to create a pruned ensemble from the original group, which we have called CLUB-67 67 67 67. As shown in the experiments, a different method can be considered when selecting representative trees from the clusters. Instead of selecting the best representatives, a randomly selected representative from each cluster can be selected instead."}], "references": [{"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "Circuits and Systems Magazine, IEEE, vol. 6, no. 3, pp. 21\u201345, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review, vol. 33, no. 1-2, pp. 1\u201339, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine learning, vol. 51, no. 2, pp. 181\u2013207, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Designing classifier ensembles with constrained performance requirements", "author": ["W. Yan", "K.F. Goebel"], "venue": "Defense and Security. International Society for Optics and Photonics, 2004, pp. 59\u201368.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Popular ensemble methods: An empirical study", "author": ["R. Maclin", "D. Opitz"], "venue": "Journal Of Artificial Intelligence Research, vol. 11, no. 1-2, pp. 169\u2013198, 1999.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences, vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning, vol. 24, no. 2, pp. 123\u2013140, 1996.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["\u2014\u2014"], "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks, vol. 5, no. 2, pp. 241\u2013259, 1992.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Stacked regressions", "author": ["L. Breiman"], "venue": "Machine learning, vol. 24, no. 1, pp. 49\u201364, 1996.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Linearly combining density estimators via stacking", "author": ["P. Smyth", "D. Wolpert"], "venue": "Machine Learning, vol. 36, no. 1-2, pp. 59\u201383, 1999.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Use R: Data Mining with Rattle and R: the Art of Excavating Data for Knowledge Discovery", "author": ["G. Williams"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "An ensemble pruning primer", "author": ["G. Tsoumakas", "I. Partalas", "I. Vlahavas"], "venue": "Applications of supervised and unsupervised ensemble methods. Springer, 2009, pp. 1\u201313.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering ensembles of neural network models", "author": ["B. Bakker", "T. Heskes"], "venue": "Neural networks, vol. 16, no. 2, pp. 261\u2013269, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Design of effective multiple classifier systems by clustering of classifiers", "author": ["G. Giacinto", "F. Roli", "G. Fumera"], "venue": "Pattern Recognition, 2000. Proceedings. 15th International Conference on, vol. 2. IEEE, 2000, pp. 160\u2013163.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Effective pruning of neural network classifier ensembles", "author": ["A. Lazarevic", "Z. Obradovic"], "venue": "Neural Networks, 2001. Proceedings. IJCNN\u201901. International Joint Conference on, vol. 2. IEEE, 2001, pp. 796\u2013801.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Clustering-based selective neural network ensemble", "author": ["F. Qiang", "H. Shang-Xu", "Z. Sheng-Ying"], "venue": "Journal of Zhejiang University Science A, vol. 6, no. 5, pp. 387\u2013392, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Pruning of random forest classifiers: A survey and future directions", "author": ["V. Kulkarni", "P. Sinha"], "venue": "Data Science Engineering (ICDSE), 2012 International Conference on, July 2012, pp. 64\u201368.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A double pruning scheme for boosting ensembles", "author": ["V. Soto", "S. Garcia-Moratilla", "G. Martinez-Munoz", "D. Hern\u00e1ndez-Lobato", "A. Suarez"], "venue": "Cybernetics, IEEE Transactions on, vol. 44, no. 12, pp. 2682\u20132695, Dec 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection inspired classifier ensemble reduction", "author": ["R. Diao", "F. Chao", "T. Peng", "N. Snooke", "Q. Shen"], "venue": "Cybernetics, IEEE Transactions on, vol. 44, no. 8, pp. 1259\u20131268, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["G. Brown", "J. Wyatt", "R. Harris", "X. Yao"], "venue": "Information Fusion, vol. 6, no. 1, pp. 5\u201320, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of diversity measures", "author": ["E.K. Tang", "P.N. Suganthan", "X. Yao"], "venue": "Machine Learning, vol. 65, no. 1, pp. 247\u2013271, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "The sources of increased accuracy for two proposed boosting algorithms", "author": ["D.B. Skalak"], "venue": "Proc. American Association for Artificial Intelligence, AAAI-96, Integrating Multiple Learned Models Workshop, vol. 1129. Citeseer, 1996, p. 1133.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Design of effective neural network ensembles for image classification purposes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image and Vision Computing, vol. 19, no. 9, pp. 699\u2013707, 2001.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Bias plus variance decomposition for zero-one loss functions", "author": ["R. Kohavi", "D.H. Wolpert"], "venue": "ICML, 1996, pp. 275\u2013283.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical methods for rates and proportions", "author": ["J.L. Fleiss", "B. Levin", "M.C. Paik"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Software diversity: practical statistics for its measurement and exploitation", "author": ["D. Partridge", "W. Krzanowski"], "venue": "Information and software technology, vol. 39, no. 10, pp. 707\u2013717, 1997.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1997}, {"title": "On the selection of decision trees in random forests", "author": ["S. Bernard", "L. Heutte", "S. Adam"], "venue": "Neural Networks, 2009. IJCNN 2009. International Joint Conference on, June 2009, pp. 302\u2013307.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Random decision forests", "author": ["T.K. Ho"], "venue": "Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on, vol. 1. IEEE, 1995, pp. 278\u2013282.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "The random subspace method for constructing decision forests", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 20, no. 8, pp. 832\u2013844, 1998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Shape quantization and recognition with randomized trees", "author": ["Y. Amit", "D. Geman"], "venue": "Neural computation, vol. 9, no. 7, pp. 1545\u20131588, 1997.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "Accuracy and diversity in ensembles of text categorisers", "author": ["J.J.G. Adeva", "U. Beresi", "R. Calvo"], "venue": "CLEI Electronic Journal, vol. 9, no. 1, 2005.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Clustering with diversity", "author": ["J. Li", "K. Yi", "Q. Zhang"], "venue": "Automata, Languages and Programming. Springer, 2010, pp. 188\u2013200.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Using diversity in cluster ensembles", "author": ["L.I. Kuncheva", "S.T. Hadjitodorov"], "venue": "Systems, man and cybernetics, 2004 IEEE international conference on, vol. 2. IEEE, 2004, pp. 1214\u20131219.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "An evaluation of structural descriptors and clustering methods for use in diversity selection", "author": ["R. Brown", "Y. Martin"], "venue": "SAR and QSAR in Environmental Research, vol. 8, no. 1-2, pp. 23\u201339, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Enhancing the diversity of a corporate database using chemical database clustering and analysis", "author": ["N.E. Shemetulskis", "J.B. Dunbar Jr", "B.W. Dunbar", "D.W. Moreland", "C. Humblet"], "venue": "Journal of Computer-Aided Molecular Design, vol. 9, no. 5, pp. 407\u2013416, 1995.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1995}, {"title": "Cluster-based transmit diversity scheme for mimo ofdm systems", "author": ["J. Lee", "Y. Sun", "R. Nabar", "H.-L. Lou"], "venue": "Vehicular Technology Conference, 2008. VTC 2008-Fall. IEEE 68th. IEEE, 2008, pp. 1\u2013 5.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Sifting through genomes with iterative-sequence clustering produces a large, phylogenetically diverse protein-family resource", "author": ["T. Sharpton", "G. Jospin", "D. Wu", "M. Langille", "K. Pollard", "J. Eisen"], "venue": "BMC bioinformatics, vol. 13, no. 1, p. 264, 2012.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MacQueen"], "venue": "Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, vol. 1, no. 281-297. California, USA, 1967, p. 14.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1967}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions to the k-means algorithm for clustering large data sets with categorical values", "author": ["Z. Huang"], "venue": "Data Mining and Knowledge Discovery, vol. 2, no. 3, pp. 283\u2013304, 1998.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "A fuzzy k-modes algorithm for clustering categorical data", "author": ["Z. Huang", "M.K. Ng"], "venue": "Fuzzy Systems, IEEE Transactions on, vol. 7, no. 4, pp. 446\u2013452, 1999.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1999}, {"title": "An alternative extension of the k-means algorithm for clustering categorical data", "author": ["O.M. San", "V.-N. Huynh", "Y. Nakamori"], "venue": "International Journal of Applied Mathematics and Computer Science, vol. 14, no. 2, pp. 241\u2013248, 2004.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "vol. 11,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Multivariate analysis", "author": ["K.V. Mardia", "J.T. Kent", "J.M. Bibby"], "venue": "1980.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1980}, {"title": "Uci machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "2013.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "A modified k-means algorithm to avoid empty clusters", "author": ["M.K. Pakhira"], "venue": "International Journal of Recent Trends in Engineering, vol. 1, no. 1, p. 1, 2009.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R. Kohavi"], "venue": "IJCAI, vol. 14, no. 2, 1995, pp. 1137\u20131145.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1995}, {"title": "Classification and regression trees", "author": ["B. Leo", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Wadsworth International Group, 1984.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1984}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise.", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "in KDD, vol", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1996}, {"title": "Clarans: A method for clustering objects for spatial data mining", "author": ["R.T. Ng", "J. Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 14, no. 5, pp. 1003\u20131016, 2002.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2002}, {"title": "Birch: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "ACM SIGMOD Record, vol. 25, no. 2. ACM, 1996, pp. 103\u2013114.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1996}, {"title": "Cure: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "ACM SIGMOD Record, vol. 27, no. 2. ACM, 1998, pp. 73\u201384.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 5, "context": "AdaBoost [6] is the representative of this class of techniques.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "The other class of ensemble approaches is the Bootstrap Aggregating (Bagging) [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "Random Forest (RF) is the main representative of bagging [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "Instead of choosing among the models, stacking combines them, thereby typically getting performance better than any single one of the trained models [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Stacking has been successfully used on both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "Stacking has been successfully used on both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "Since RF algorithms typically build between 100 and 500 trees [12], in real-time applications, it is of paramount importance to reduce the number of trees participating in majority voting and yet achieve performance that is at least as good as the original ensemble.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "[13] recently amalgamated a survey of ensemble pruning techniques where they classified such techniques into four categories: ranking based, clustering based, optimization based, and others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "One approach by [14] was to train a new model for each cluster, using the cluster centroids as values of the target variable.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Another interesting approach was proposed by [15] that involved selecting from each cluster the classifier that is most distant to the rest of the clusters.", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "A yet different approach by [16] that does not guarantee the selection of a single model from each cluster was by iteratively removing models from the least to the most accurate, until the accuracy of the entire ensemble starts to decrease.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "Selecting the most accurate model from each cluster was proposed by [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "It is worth pointing out that there are several techniques available in the literature that use clustering-based approaches to reduce the number of trees in the ensemble (see [18] for a good review).", "startOffset": 175, "endOffset": 179}, {"referenceID": 13, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "For example, all the approaches proposed by the respective anthers in [14] [15] [16] [17] were developed for neural network ensembles.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 163, "endOffset": 167}, {"referenceID": 16, "context": "Furthermore, at the experimental level, we have used 15 datasets from the UCI repository, however, very few datasets were used by them: 2 in [14], 1 in [15], 4 in [16], and 4 in [17].", "startOffset": 178, "endOffset": 182}, {"referenceID": 18, "context": "Without a significant loss of prediction accuracy, a combination of static and dynamic pruning techniques were applied on Adaboost ensembles in order to yield less memory consumption and improved classification speed [19].", "startOffset": 217, "endOffset": 221}, {"referenceID": 19, "context": "A pruning scheme for high dimensional and large sized benchmark datasets was developed by [20], In such a scheme, an extended feature selection technique was used to transform ensemble predictions into training samples, where classifiers were treated as features.", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "[21] summarized the work done to date in this domain from two main perspectives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] also categorized ensemble diversity techniques into three categories: starting point in hypothesis space, set of accessible hypotheses, and manipulation of training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Because of their poor performance of achieving diversity, such methods are used by many authors as a default benchmark for their own methods [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 21, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 140, "endOffset": 144}, {"referenceID": 25, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 26, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 196, "endOffset": 200}, {"referenceID": 25, "context": "[22] presented a theoretical analysis on six existing diversity measures: disagreement measure [23], double fault measure [24], KW variance [25], inter-rater agreement [26], generalized diversity [27], and measure of difficulty [26].", "startOffset": 228, "endOffset": 232}, {"referenceID": 11, "context": "As mentioned before, RF algorithms tend to build between 100 and 500 trees [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "500) won\u2019t necessarily improve the RF accuracy [28].", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 82, "endOffset": 85}, {"referenceID": 28, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 156, "endOffset": 160}, {"referenceID": 29, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 161, "endOffset": 165}, {"referenceID": 30, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [29] [30] and Amit and Geman [31], in order to construct a collection of decision trees", "startOffset": 185, "endOffset": 189}, {"referenceID": 7, "context": "Algorithm 1 below depicts the RF algorithm [8] where N is the number of training samples and S is the number of features in dataset.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 139, "endOffset": 142}, {"referenceID": 20, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "In fact, it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [21] [32] [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 32, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 88, "endOffset": 92}, {"referenceID": 34, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 93, "endOffset": 97}, {"referenceID": 35, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 98, "endOffset": 102}, {"referenceID": 36, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 103, "endOffset": 107}, {"referenceID": 37, "context": "Clustering has been used extensively as a diversity technique in many applications [33] [34] [35] [36] [37] [38].", "startOffset": 108, "endOffset": 112}, {"referenceID": 38, "context": "It was developed by MacQueen [39] in the late sixties and despite its seniority, it is still considered as one of", "startOffset": 29, "endOffset": 33}, {"referenceID": 39, "context": "efficiency, and empirical success [40].", "startOffset": 34, "endOffset": 38}, {"referenceID": 40, "context": "there have been some extensions of this algorithm to work with categorical data [41] [42] [43].", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "there have been some extensions of this algorithm to work with categorical data [41] [42] [43].", "startOffset": 85, "endOffset": 89}, {"referenceID": 42, "context": "there have been some extensions of this algorithm to work with categorical data [41] [42] [43].", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "Huang [41] developed an extension of K-means called K-modes that uses modes instead of means, and can handle categorical data using the following simple matching dissimilarity measure:", "startOffset": 6, "endOffset": 10}, {"referenceID": 43, "context": "In the experimental stage we will be using a popular machine learning software suite called Waikato Environment for Knowledge Analysis (WEKA) [44].", "startOffset": 142, "endOffset": 146}, {"referenceID": 44, "context": "Mardi [45] proposed a simple rule of thumb: number of clusters \u2248 \u221a n 2 , where n refers to the size of data points to be clustered.", "startOffset": 6, "endOffset": 10}, {"referenceID": 45, "context": "varying characteristics from the UCI repository [46].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "For the RF in Figure 1, the initial RF to produce the CLUB-DRF had a size of 500 trees, a typical upper limit setting for RF [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 46, "context": "Secondly, when we have many clusters, the more trees we have, the more unlikely that we wind up with empty clusters [47].", "startOffset": 116, "endOffset": 120}, {"referenceID": 43, "context": "The CLUB-DRF algorithm described above was implemented using the Java programming language utilizing the API of Waikato Environment for Knowledge Analysis (WEKA) [44].", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "In a research by Lazarevic and Obradovic [16] where clustering was also used to prune neural network ensembles, TABLE I MAXIMUM PRUNING LEVEL WITH BEST POSSIBLE PERFORMANCE", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "TABLE II PERFORMANCE COMPARISON BETWEEN ENTIRE AND PRUNED ENSEMBLE [16] WITH OUR RF AND CLUB-DRFS", "startOffset": 67, "endOffset": 71}, {"referenceID": 47, "context": "Bias and variance are measures used to estimate the accuracy of a classifier [48].", "startOffset": 77, "endOffset": 81}, {"referenceID": 48, "context": "[49] provided an analysis of complexity and induction in terms of a trade-off between bias and variance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 103, "endOffset": 106}, {"referenceID": 20, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 107, "endOffset": 111}, {"referenceID": 31, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [21] [32] [22].", "startOffset": 117, "endOffset": 121}, {"referenceID": 49, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 117, "endOffset": 121}, {"referenceID": 50, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 131, "endOffset": 135}, {"referenceID": 51, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 143, "endOffset": 147}, {"referenceID": 52, "context": "Another interesting research direction would be to use other clustering algorithms other than WEKA\u2019s own like DBSCAN [50], CLARANS [51], BIRCH [52], and/or CURE [53].", "startOffset": 161, "endOffset": 165}], "year": 2015, "abstractText": "Random Forest (RF) is an ensemble supervised machine learning technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. First, it investigates how data clustering (a well known diversity technique) can be applied to identify groups of similar decision trees in an RF in order to eliminate redundant trees by selecting a representative from each group (cluster). Second, these likely diverse representatives are then used to produce an extension of RF termed CLUB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, and mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 15 real datasets from the UCI repository prove the superiority of our proposed extension over the traditional RF. Most of our experiments achieved at least 95% or above pruning level while retaining or outperforming the RF accuracy.", "creator": "LaTeX with hyperref package"}}}