{"id": "1503.05187", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "An Outlier Detection-based Tree Selection Approach to Extreme Pruning of Random Forests", "abstract": "Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofolds. First, it investigates how an unsupervised learning technique, namely, Local Outlier Factor (LOF) can be used to identify diverse trees in the RF. Second, trees with the highest LOF scores are then used to produce an extension of RF termed LOFB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, but mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 10 real datasets prove the superiority of our proposed extension over the traditional RF. Unprecedented pruning levels reaching 99% have been achieved at the time of boosting the predictive accuracy of the ensemble. The notably high pruning level makes the technique a good candidate for real-time applications.", "histories": [["v1", "Tue, 17 Mar 2015 11:05:31 GMT  (60kb,D)", "http://arxiv.org/abs/1503.05187v1", "21 pages, 4 Figures. arXiv admin note: substantial text overlap witharXiv:1503.04996"]], "COMMENTS": "21 pages, 4 Figures. arXiv admin note: substantial text overlap witharXiv:1503.04996", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["khaled fawagreh", "mohamad medhat gaber", "eyad elyan"], "accepted": false, "id": "1503.05187"}, "pdf": {"name": "1503.05187.pdf", "metadata": {"source": "CRF", "title": "An Outlier Detection-based Tree Selection Approach to Extreme Pruning of Random Forests", "authors": ["Khaled Fawagreh", "Mohamed Medhat Gaber", "Eyad Elyan"], "emails": ["@rgu.ac.uk"], "sections": [{"heading": null, "text": "Compared to other ensemble techniques, it has proven its accuracy and superiority, but many researchers believe that there is still room for improving and improving its performance in terms of predictive accuracy. This explains why there have been many RF extensions over the last decade, with each expansion employing a variety of techniques and strategies to improve certain aspects of RF. Since empirical evidence shows that ensembles tend to achieve better results when there is significant diversity between the constituent models, the aim of this paper is twofold. First, it examines how an uncontrolled learning method, the Local Outlier Factor (LOF), can be used to identify different trees in the RF. Second, trees with the highest LOF values are then used to produce an expansion of the RF, known as the XiFB-DRF."}, {"heading": "1. Introduction", "text": "In fact, the situation between the two is a very complex one, where both sides are on an equal footing and where it is a question of finding a solution that both sides are trying to find."}, {"heading": "2. Related Work", "text": "In recent years, several attempts have been made to create a subset of an ensemble that performs both as or better than the original ensemble. The purpose of the ensemble section is to search for such a good subset. This is particularly useful for large ensembles that require additional memory usage, computational costs, and occasional decreases in effectiveness. Grigorios et al. [16] have recently compiled an overview of ensemble editing techniques in which they divided these techniques into four categories: ranking-based, clustering-based, optimization-based, and others. Ranking-based methods that are relevant to us in this paper are the simplest conceptually. Since the use of predictive power for ranking models is too simplistic and does not yield satisfactory results [17], rank-based methods use a rating metric to rank models. Kappa statistical measures used in [19] for editing AdaBoost ensembles do not rank the models."}, {"heading": "2.1. Diversity Creation Methods", "text": "Due to the vital role that diversity plays in the execution of ensembles, it has received much attention in the research community. G. Brown et al. [13] summarized the work done so far in this area from two main perspectives; the first is a review of the various attempts that have been made to create a formal basis for diversity; the second, which is more relevant for this paper, is an overview of the various techniques used to produce diverse ensembles; for the latter, two types of diversity methods have been identified: implicit and explicit. While implicit methods tend to generate multiple trajectories in the hypotheses space, explicit methods choose different paths in space deterministically; in the light of these definitions, bagging and boosting are classified as implicit and explicit methods in the previous section. G. Brown et al. [13] categorized ensemble diversity techniques into three categories: starting point in the hypotheses space, set of accessible hypotheses, and implicit manipulation of training data."}, {"heading": "2.2. Diversity Measures", "text": "Regardless of the diversification techniques used, points-points-points-points-points-points are incorrect-points-points-points-points-points are calculated correctly-points-points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points-points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _ points _"}, {"heading": "3. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Motivation", "text": "As already mentioned, RF algorithms tend to produce between 100 and 500 trees [12]. Our research aims to generate infant RFs that are significantly smaller and yet have an accuracy that is at least as good as that of the parent RF from which they were derived. Guaranteed, the classification speed of each child is much faster than that of the parent RF because 1) it has many fewer trees and 2) every tree used by the child is also with the parent tree (i.e. no new trees were introduced for the child)."}, {"heading": "3.2. Random Forest", "text": "The method developed by Breiman combines Breiman's bagging method [7] and the random selection of characteristics introduced independently of Ho [27] [28] and Amit and Geman [29] to construct a collection of decision trees with controlled variation. Using bagging, each decision tree within the ensemble is constructed based on a sample with substitutes from the training data. Statistically, the sample is likely to have about 64% of the cases that occur at least once in the sample. Cases in the sample are referred to as in-bag instances, and the remaining cases (about 36%) are referred to as out-of-bag instances. Each tree in the ensemble acts as a base classifier to determine the class designation of an unlabeled instance. This is done by majority voting, where each classifier casts a vote for its predicted class designation, then the class designation with the most votes is used to classify the number of attributes below the algorithm S."}, {"heading": "4. Local Outlier Factor", "text": "The Local Outlier Factor (LOF) algorithm was developed by Breunig et al. [30] to measure the superiority of an object. The higher the LOF value, the higher the accessibility algorithm 1 Random Forest Algorithm {User Settings} Input N, S {Process} Create an empty vector (FS Find Best Split Feature B (\u2212 \u2192 FS) Create a new node using B (\u2212 \u2192 FS) Create an empty tree Ti repeatSample S from all F features using bootstrap sampling Create a vector of S features \u2212 \u2192 FS Find Best Split Feature B (\u2212 \u2192 FS) Create a new node using B (\u2212 \u2192 FS) The ability to capture a tree by using Ti to \u2212 \u2192 RFend for performance} A vector of trees \u2212 \u2192 RFasto an object, the more isolated is the object in relation to its neighbors."}, {"heading": "5. LOF-Based Diverse Random Forest (LOFB-DRF)", "text": "In this section, we propose an extension of the RF called LOFB-DRF that produces a childlike RF that is 1) much smaller than the parent RF and 2) has an accuracy at least as good as that of the parent RF. In this extension, we use the one in Section 4. As shown in Figure 1, each tree that matches these predictions on the training data set (indicated by the vector C (ti, T) is assigned a LOF value that indicates the degree of its propagation. The uppermost k (k = 5.10,..., 40) trees that match these predictions with the highest weighted LOF values (which will be discussed next) are then selected to become members of the resulting LOFB-DRF. For the rest of this work, we will refer to the parent / original traditional forest as RF and refer to the resulting child RF based on our method as LOFB-DRF. Based on Figure 1, we will formulate the LOF-DRF values as the algorithm in this section."}, {"heading": "5.1. Selection of Trees", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "5.2. Diversity Measure", "text": "Here we propose a simple measure of diversity to measure the diversity of classifiers that work with binary and multiclassical classification. In view of two classifiers hj and hk and a training set T of size n. Let C (tl, si) specify the class name that was obtained after classifying the sample si in the training set T. The diversity between the two classifiers can be measured by: Diversityj, k = n \u2211 i = 1 \u03b4 (C (tj, ci), C (tk, ci))) n (5) where\u03b4 (xj, yj) = {0, if xj = yj1, otherwise (6) The higher the number of discrepancies between the two classifiers, the higher the diversity. For example, let us assume that we have a training set consisting of 10 training samples T = {s1, s2, s3, s4, s5, s6, s7, sc, < < &tc, &t9} and T =;"}, {"heading": "6. Experiments", "text": "For our experiments, we used 10 real data sets with different properties from the UCI repository [37]. To apply the holdout test method, each data set was divided into 2 sets: Training and Test. Two-thirds (66%) were reserved for training and the rest (34%) for testing. Each data set consisted of input variables (features) and an output variable, the latter referring to the class label whose value is predicted in each experiment. For the RF in Figure 1, the initial RF used to generate the LOFB DRF had a size of 500 trees, a typical upper limit for RF [12]. The above described LOFB DRF algorithm was implemented using the Java programming language using the API of Waikato Environment for Knowledge Analysis (WEKA) [38]. We executed this algorithm ten times on each data set where a new RF was generated for each RF 5. Then we averaged the RF for each RF including the RF for each RF, including the exact LB for each RF."}, {"heading": "6.1. Results", "text": "Table 5 compares the performance of LOFB-DRF and RF in relation to the 10 datasets used in the experiment. To demonstrate the superiority of LOFB-DRF, we have highlighted the average accuracy of LOFB-DRF in bold if it is higher than that of RF. Except for the test and matching datasets (the last 2 datasets), we note that LOFB-DRF was at least as good as RF. Interestingly, LOFB-DRF, regardless of its size, completely outperformed the RF in 3 datasets, namely Squash Stored, Eucalyptus and Sonar. While LOFB-DRF lost on only 2 datasets (check and vote) compared to RF, the difference was a very small, negligible fraction of less than 1% (in case of the test) and less than 1.2% (in case of the vote)!"}, {"heading": "6.2. Pruning Level", "text": "In ensemble pruning, one pruning step refers to the reduction ratio between the original ensemble and the pruned tree. For example, if the size of the original ensemble is 500 trees and the pruned tree is size 50, then 100% \u2212 50500 \u00d7 100% = 90% is the pruning level reached in the pruned ensemble. This means that the pruned ensemble is 90% smaller than the original. Table 1 shows the pruning steps where the first column has the highest possible pruning level for a LOFB DRF that has exceeded the RF, and the second column shows the pruned intersection of the best performer LOFB-DRF. We can see that for extremely healthy pruning steps ranging from 95% to 99%, our technique outperforms the RF. This makes LOFB-DRF a natural choice for real-time applications where fast pruning is an important determinant. In most cases, 100 times faster pruning can be achieved with 99% pruning."}, {"heading": "6.3. Analysis", "text": "For sizes 10, 15, 20, and 25, the figure clearly shows that LOFB-DRF does indeed perform at least as well as RF. As shown in Table 2, the difference was very small in cases (sizes 5, 30, 35, and 40) where RF exceeded LOFB-DRF, taking into account the intersection level.Table 2: Outperformance Range of RF Over LOFB-DRFLOFB-DRF Size 5 30 35 40 Range 0.31% - 4.12% 0.08% - 2.78% 0.05% - 1.45% 0.31% - 3.33% Pruning Level 99% 94% 93% 92% 024610 20 30 40 Size (Number of Trees) Only by D attsMethodLOF \u2212 DRF RF2 DRF Accuracy Comparison:"}, {"heading": "6.4. Bias/Variance Analysis", "text": "The distortion measures the difference between the predicted class value of the classifier and the true value of the predicted class name. The variance, on the other hand, measures the variability of the classifier's prediction as a result of sensitivity due to fluctuations in the training set. If the prediction is always the same regardless of the training set, it is zero. However, as the prediction becomes more sensitive to the training set, the variance tends to increase. For a classifier to be accurate, it should maintain a low distortion and variance. There is a compromise between the ability of a classifier to minimize bias and variance. Understanding these two types of measures can help us diagnose the results of the classifier and avoid the error of over- or underfit. Breiman et al. [40] noted that the variance of the LOF value and the induction with respect to a trade between bias and variance is equal. In this section, we will show that DRF and DRF levels are even better."}, {"heading": "7. Conclusion and Future Directions", "text": "The research conducted in this paper was based on how diversity in ensembles tends to produce better results [3] [14] 3.76 [15].We used the Local Outlier Factor method to select different trees in an RF, and then used these trees to form a hybrid ensemble of the original ensemble, based on both the LOF value and the predictive accuracy of each tree. Experimental results have shown that the potential of this method, with extreme pruning of random forests that can exceed the original population of trees with values reaching 99% level, is a suitable candidate for real-time applications. We have selected trees that correspond to cases with weighted LOF values. Another interesting variation would be combining the LOF with clusters to increase diversity."}], "references": [{"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "Circuits and Systems Magazine, IEEE 6 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review 33 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine learning 51 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Popular ensemble methods: An empirical study", "author": ["R. Maclin", "D. Opitz"], "venue": "Journal Of Artificial Intelligence Research 11 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences 55 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning 24 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning 45 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks 5 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Stacked regressions", "author": ["L. Breiman"], "venue": "Machine learning 24 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Linearly combining density estimators via stacking", "author": ["P. Smyth", "D. Wolpert"], "venue": "Machine Learning 36 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Use R: Data Mining with Rattle and R: the Art of Excavating Data for Knowledge Discovery", "author": ["G. Williams"], "venue": "Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["G. Brown", "J. Wyatt", "R. Harris", "X. Yao"], "venue": "Information Fusion 6 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Accuracy and diversity in ensembles of text categorisers", "author": ["J.J.G. Adeva", "U. Beresi", "R. Calvo"], "venue": "CLEI Electronic Journal 9 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of diversity measures", "author": ["E.K. Tang", "P.N. Suganthan", "X. Yao"], "venue": "Machine Learning 65 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "An ensemble pruning primer", "author": ["G. Tsoumakas", "I. Partalas", "I. Vlahavas"], "venue": "in: Applications of supervised and unsupervised ensemble methods, Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Engineering multiversion neural-net systems", "author": ["D. Partridge", "W.B. Yates"], "venue": "Neural Computation 8 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Ensemble selection for superparent-one-dependence estimators", "author": ["Y. Yang", "K. Korb", "K.M. Ting", "G.I. Webb"], "venue": "in: AI 2005: Advances in Artificial Intelligence, Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of ensemble pruning techniques based on ordered aggregation", "author": ["G. Martinez-Muoz", "D. Hern\u00e1ndez-Lobato", "A. Su\u00e1rez"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 31 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Design of effective neural network ensembles for image classification purposes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image and Vision Computing 19 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Statistical methods for rates and proportions", "author": ["J.L. Fleiss", "B. Levin", "M.C. Paik"], "venue": "John Wiley & Sons", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Software diversity: practical statistics for its measurement and exploitation", "author": ["D. Partridge", "W. Krzanowski"], "venue": "Information and software technology 39 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Random decision forests", "author": ["T.K. Ho"], "venue": "in: Document Analysis and Recognition", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Shape quantization and recognition with randomized trees", "author": ["Y. Amit", "D. Geman"], "venue": "Neural computation 9 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Computing depth contours of bivariate point clouds", "author": ["I. Ruts", "P.J. Rousseeuw"], "venue": "Computational Statistics & Data Analysis 23 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "On evaluation of outlier rankings and outlier scores", "author": ["E. Schubert", "R. Wojdanowski", "A. Zimek", "H.-P. Kriegel"], "venue": "in: Proceedings of the 12th SIAM International Conference on Data Mining (SDM), Anaheim, CA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["K. Bache"], "venue": "Lichman, Uci machine learning repository", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Eibe Frank", "author": ["G.H.B.P.P.R.I.H.W. Mark Hall"], "venue": "The WEKA Data Mining Software: An Update, volume 11, SIGKDD Explorations", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification and regression trees", "author": ["B. Leo", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Wadsworth International Group ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 3, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 4, "context": "AdaBoost [6] is the representative of this class of techniques.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "The other class of ensemble approaches is the Bootstrap Aggregating (Bagging) [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Random Forest (RF) is the main representative of bagging [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Instead of choosing among the models, stacking combines them, thereby typically getting performance better than any single one of the trained models [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 8, "context": "Stacking has been successfully used in both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Stacking has been successfully used in both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "Since RF algorithms typically build between 100 and 500 trees [12], it would be useful to reduce the number of trees participating in majority voting", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 150, "endOffset": 153}, {"referenceID": 11, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "[16] recently amalgamated a survey of ensemble pruning techniques where they classified such techniques into four categories: ranking based, clustering based, optimization based, and others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Since using the predictive performance to rank models is too simplistic and does not yield satisfying results [17] [18], ranking based methods employ an evaluation measure to rank models.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Since using the predictive performance to rank models is too simplistic and does not yield satisfying results [17] [18], ranking based methods employ an evaluation measure to rank models.", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "For bagging ensembles, however, kappa has proven to be non-competitive [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "[13] summarized the work done to date in this domain from two main perspectives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] also categorized ensemble diversity techniques into three categories: starting point in hypothesis space, set of accessible hypotheses, and manipulation of training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Because of their poor performance of achieving diversity, such methods are used by many authors as a default benchmark for their own methods [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 13, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 122, "endOffset": 126}, {"referenceID": 19, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 228, "endOffset": 232}, {"referenceID": 10, "context": "As mentioned before, RF algorithms tend to build between 100 and 500 trees [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 82, "endOffset": 85}, {"referenceID": 21, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 185, "endOffset": 189}, {"referenceID": 6, "context": "Algorithm 1 below depicts the RF algorithm [8] where N is the number of training samples and S is the number of features in data set.", "startOffset": 43, "endOffset": 46}, {"referenceID": 24, "context": "Earlier work on outlier detection was investigated in [31] [32] [33] [34], however, the work was limited by treating an outlier as a binary property to classify an object as an outlier or not, without assigning it a value to measure its outlierness as was done in [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "[36] proposed methods for measuring similarity and diversity of methods for building advanced outlier detection ensembles using LOF variants and other algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "For our experiments, we have used 10 real datasets with varying characteristics from the UCI repository [37].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "For the RF in Figure 1, the initial RF to produce the LOFB-DRF had a size of 500 trees, a typical upper limit setting for RF [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "The LOFB-DRF algorithm described above was implemented using the Java programming language utilizing the API of Waikato Environment for Knowledge Analysis (WEKA) [38].", "startOffset": 162, "endOffset": 166}, {"referenceID": 28, "context": "[40] provided an analysis of complexity and induction in terms of a tradeoff between bias and variance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 117, "endOffset": 121}], "year": 2015, "abstractText": "Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofolds. First, it investigates how an unsupervised learning technique, namely, Local Outlier Factor (LOF) can be used to identify diverse trees in the RF. Second, trees with the highest LOF scores are then used to produce an extension of RF termed LOFB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, but mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 10 real datasets prove the superiority of our proposed extension over the traditional RF. Unprecedented pruning levels reaching as high as 99% have been achieved at the time of boosting the predictive accuracy of the ensemble. The notably high pruning level makes the technique a good candidate for real-time applications.", "creator": "LaTeX with hyperref package"}}}