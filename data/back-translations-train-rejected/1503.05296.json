{"id": "1503.05296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Efficient Machine Learning for Big Data: A Review", "abstract": "With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years, in fact, as much as 90% of current data were created in the last couple of years,a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven, the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.", "histories": [["v1", "Wed, 18 Mar 2015 07:56:12 GMT  (435kb)", "http://arxiv.org/abs/1503.05296v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["o y al-jarrah", "p d yoo", "s muhaidat", "g k karagiannidis", "k taha"], "accepted": false, "id": "1503.05296"}, "pdf": {"name": "1503.05296.pdf", "metadata": {"source": "CRF", "title": "Efficient Machine Learning for Big Data: A Review", "authors": ["O. Y. Al-Jarrah", "P. D. Yoo", "G. K. Karagiannidis", "K. Taha"], "emails": [], "sections": [{"heading": null, "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Big data challenge", "text": "E-science areas are typically data-intensive, as the quality of their results improves with both the quantity and quality of available data. However, current intelligent machine learning systems are not inherently efficient enough, which in many cases leads to an increasing share of this unexplored and under-used quantity data. It is no small problem if existing methods fail to capture such a vast amount of data. If old concepts fail to keep pace with change, traditions and past experience become insufficient guidelines for what to do next. Effective understanding and using this new wealth of raw information is a major challenge for today's green engineers / researchers. It should be noted that the scope of the review is limited to the analytical aspects of scientific areas that use immense data sets, and methods to reduce computing complexity in distributed or grid computing environments are excluded."}, {"heading": "2.1. Geo, climate and environment", "text": "There are many current examples that illustrate the enormous growth in scientific data generation in the literature. It is estimated that there are currently thousands of wireless sensors that generate about one gigabyte of data per sensor per day [8]. Such sensors measure and capture sensory information about the natural environment in a common spatial and temporal dimension that has not previously been possible. This environmental information is collected by sensors through their sensor devices, which are connected to small, low-power computer systems using digital radio communications. The sensor organizes itself into a network to deliver the collected data, and may process it to a base station where it can be made available to users over the Internet. These sensors generate several petabytes of data per year, and decisions need to be made in real time to analyze how much data is being transferred for further analysis."}, {"heading": "2.2. Bio, medicine, and health", "text": "It is estimated that the human genome comprises about 3.2 billion pairs (3.2 gigabytes) of DNA distributed among twenty-three chromosomes, and the relevant volume of data can easily expand to an order of magnitude of about 200 gigabytes. [11] Now, if we add gene sequence data, the number of proteins and the number of amino acids will increase significantly."}, {"heading": "2.3. Stars, galaxies, and the universe", "text": "The digital data volume of stars, galaxies and the universe has multiplied in the last decade due to the rapid development of new technologies such as new satellites, telescopes and other observatory instruments. Recently, the Visible and Infrared Survey Telescope for Astronomy (VISTA) [19] and the Dark Energy Survey (DES) [20] - the largest universe environment projects initiated by two different consortia of universities from the UK and the US - were expected to produce databases of 20-30 terabytes in size over the next ten years. According to DES, the observatory field is so large that a single image will record data from an area of the sky 20 times the size of the moon seen from Earth [20]. The survey will map 5000 degrees of the US southern sky and take about five years to complete this database."}, {"heading": "3. Sustainable data modeling and efficient learning", "text": "Sustainable data modeling can be defined as a form of data modeling technology that aims to make meaningful use of the large amount of data allocated in its own area by discovering patterns and correlations in an effective and efficient manner. Sustainable data modeling focuses in particular on 1) maximum learning accuracy with minimal computing costs and 2) fast and efficient processing of large amounts of data. Sustainable data modeling seems ideal because large amounts of data are handled efficiently and the associated cost reductions can be observed in many cases. In a broader sense, it brings with it a revolution in data modeling in the e-sciences. In fact, these newly developed sustainable data models will effectively address the data problems mentioned above and bring benefits to the various areas of e-sciences as a result. Some of the outstanding examples are discussed extensively in Patnaik, Sunnaal, a green article and an article by Maraval 27."}, {"heading": "3.1. Ensemble models", "text": "One of the most important elements of the success of sustainable data modeling is maintaining or improving its performance while significantly reducing its computing costs. Recent data modeling research has shown that ensemble methods have gained popularity because they often perform better than individual models [28]. Ensemble methodology uses multiple models to achieve better performance than those that could be derived from one of the constituent models [29]. However, it can lead to a significant increase in computing costs. If the model deals with large-scale data, model complexity and computing requirements will grow exponentially. An example of such an ensemble model is the Bayes classifier [31]. In the Bayes classifier, each hypothesis is chosen in proportion to the probability that the educational data set would be sampled from a system if this hypothesis were true. To simplify training data of limited size, the coordination of the individual Bayes hypothesis is multiplied."}, {"heading": "3.2. Model complexity problem", "text": "s non-parametric learning models is its high algorithmic complexity and extensive memory requirements, especially for the necessary quadratic programming in large tasks. [33] As a non-parametric Bayes classifier, it extracts the worst-case x and uses statistical analysis to create a classification model. Any learning algorithm that examines all the attribute values of each training example must have at least the same or lower complexity [33]. Many machine learning applications deal with problems where both the number of features i and the number of examples xi is large. Linear support vectors are among the most prominent machine learning techniques for such high-dimensional learning models, and we use them as two more sparse examples in this article]."}, {"heading": "3.3. Local learning strategy", "text": "They have been proven successful by other similar studies [7]. With a non-parametric model, a unique model must be constructed for each set of exams that significantly increases its computational complexity and costs. A number of recent papers have shown that such a local learning strategy is much better than that of the global learning strategy, especially if it is not evenly distributed."}, {"heading": "3.4. Semiparametric approximation", "text": "This is because the relative widths of the spherical functions at each center are directly proportional to the relative number of training vectors associated with each center. Local learning strategy provides a reasonable approximation, as they are sufficiently close to the input vector space. In this case, they can be adequately represented by a single central vector in that local space. In the case of support vector regression (SVR), the vectors can be derived from either k-means or codebook theory. In SVR, where the two classes are inseparable, they map the input space into a high-dimensional space (where the classes are separable)."}, {"heading": "3.5. Deep learning", "text": "In literature, they are often used to solve simple or well-defined problems, but their limited modeling and representational power support their use in solving complex problems, such as natural language problems. In 2006, so-called deep learning (a.k.a. representation learning) emerged as a new area of ML research [43-45] that uses multiple layers of information processing in a hierarchical architecture for pattern classification and representation learning (e.g. feed-forward neural networks). The main advantage of deep learning is that the dramatically increased processing capabilities of chips are evaluated, and recent advances in ML.Deep neural networks (DNN) are multi-layered networks with many hidden layers."}, {"heading": "3.6. Big data computing", "text": "Big data computing systems fall into two main categories, based on the way data is analyzed with respect to time constraints [53]. First, batch processing large amounts of data on hard drives without time constraints (e.g. MapReduce and GraphLab). Second, streaming real-time or short-time in-memory data processing (e.g. Storm, SAMOA) [54]. In [54], Huang and Li argued that next-generation computing systems for big data analytics require innovative designs in both hardware and software that would provide a good match between big data algorithms and the underlying computing and storage resources. There are several computing frameworks, e.g. Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60] and IBM parallel machine learning toolboxes that have such machine learning capabilities."}, {"heading": "4. Concluding Remarks", "text": "In this review, we gave an overview of the current state of research in the field of sustainable data modeling. In particular, we discussed its theoretical and experimental aspects in large data-intensive areas, in terms of: (1) energy efficiency of the model, including computational requirements in learning, and possible approaches, and (2) structure and design of data-intensive areas, including the relationship between data models and properties. With the surge in e-science data, sustainable data modeling has proven to be a way forward due to its ease of handling large volumes of data. It is also envisaged that such a data modeling revolution can easily be extended to various areas of e-science. These newly designed sustainable data models will not only be able to cope with the emerging large data paradigm, but will also provide a means of maximizing its usefulness in the various e-science areas."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Dr. Jason W.P Ng from EBTIC for his valuable discussions and feedback and in particular British Telecom (BT) in London for their constructive criticism of this work."}], "references": [{"title": "Gartner estimates ict industry accounts for 2 percent of global co2 emissions.", "author": ["C. Pettey"], "venue": "http://www.gartner.com/newsroom/id/503867, Accessed on Aug", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "I.B.M. effort to focus on saving energy.", "author": ["S. Lohr"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Estimating total power consumption by servers in the u.s and the world", "author": ["J.G. Koomey"], "venue": "Stanford university, technical report, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Model-based energy management reduces energy costs.", "author": ["M. Strathman"], "venue": "E&P,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Managing data ood is industry challenge", "author": ["J. Nil"], "venue": "2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "An energy-efficient kernel framework for large-scale data modeling and classiffication", "author": ["P. Yoo", "J. Ng", "A. Zomaya"], "venue": "Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on, pp. 404\u2013408, May 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient algorithm for localized support vector machine", "author": ["H. Cheng", "P.-N. Tan", "R. Jin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 4, pp. 537\u2013549, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Energy, \u201cInsights learned from the human DNA sequence, what has been learned from analysis of the working draft sequence of the human genome? what is still unknown?.", "author": ["D. U"], "venue": "Online. http://www.ornl. gov/hgmis, Accessed on", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The data deluge: An e-science perspective", "author": ["A.J. Hey", "A.E. Trefethen"], "venue": "2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Electronic patient records will soon end doctors scrawl on paper, the globe and mail.", "author": ["T. Baluja"], "venue": "Online. http://www.theglobeandmail.com/news/national/toronto/electronic-patient-records-will-soon-end-doctors-scrawl-on-paper/article1982647, Accessed on", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "News, \u201cDell launches new cloud-based services for hospitals and physician practices.", "author": ["EMR E"], "venue": "Online. http://www.emrandhipaa.com/news/2011/02/21/ dell-launches-new-cloud-based-services-for-hospitals-and-physician-practiceAccessed on", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Vista: project status", "author": ["M A.M."], "venue": "vol. 6267, SPIE 6267, Ground-based and Airborne Telescopes, 626707 (23 June 2006); doi: 10.1117/12.671352, 2006. http://spie.org/Publications/ Proceedings/Paper/10.1117/12.671352.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "The pan-starrs survey telescope project", "author": ["N. Kaiser"], "venue": "Bulletin of the American Astronomical Society, vol. 37, p. 1409, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Lsst receives $30 million fom charles simonyi and bill gates.", "author": ["S. Ref"], "venue": "Online. http://www.spaceref.com/news/viewpr.html?pid=24409, Accessed on", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Using data mining to help design sustainable products", "author": ["M. Marwah", "A. Shah", "C. Bash", "C. Patel", "N. Ramakrishnan"], "venue": "IEEE Computer, vol. 44, no. 8, pp. 103\u2013106, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering life cycle assessment trees from impact factor databases", "author": ["N. Sundaravaradan", "D. Patnaik", "N. Ramakrishnan", "M. Marwah", "A. Shah"], "venue": "AAAI, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Data mining approaches for life cycle assessment", "author": ["N. Sundaravaradan", "M. Marwah", "A. Shah", "N. Ramakrishnan"], "venue": "IEEE ISSST, vol. 11, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Data mining for modeling chiller systems in data centers", "author": ["D. Patnaik", "M. Marwah", "R.K. Sharma", "N. Ramakrishnan"], "venue": "Advances in Intelligent Data Analysis IX, pp. 125\u2013136, Springer, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A review of ensemble methods in bioinformatics", "author": ["P. Yang", "Y. Hwa Yang", "B. B Zhou", "A. Y Zomaya"], "venue": "Current Bioinformatics, vol. 5, no. 4, pp. 296\u2013308, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Popular ensemble methods: An empirical study,\u201dJournal", "author": ["R. Maclin", "D. Opitz"], "venue": "Artifcial Intelligence Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "Circuits and sytems magazine, IEEE, vol. 6, no. 3, pp. 21\u201345, 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards big data Bayesian network learning - an ensemble learning based approach", "author": ["Y. Tang", "Y. Wang", "K. Cooper", "L. Li"], "venue": "Big Data (BigData Congress), 2014 IEEE International Congress on, pp. 355\u2013357, June 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimation and Spectral Analysis", "author": ["J.A.R. Blais"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1988}, {"title": "Naive bayesian learning", "author": ["C. Elkan"], "venue": "Department of Computer Science and Engineering, University of California, San Diego, 1997. 8", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12 ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 217\u2013226, ACM, 2006.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Siteseek: post-transltional modification analysis using adaptive locality-eff ective kernel methods and new profiles", "author": ["P.D. Yoo", "Y.S. Ho", "B.B. Zhou", "A.Y. Zomaya"], "venue": "BMC bioinformatics, vol. 9, no. 1, p. 272, 2008.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Local learning algorithms", "author": ["L. Bottou", "V. Vapnik"], "venue": "Neural computation, vol. 4, no. 6, pp. 888\u2013900, 1992.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1992}, {"title": "Local machine learning models for spatial data analysis", "author": ["N. Gilardi", "S. Bengio"], "venue": "Journal of Geographic Information and Decision Analysis, vol. 4, no. EPFL-ARTICLE-82651, pp. 11\u201328, 2000.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2000}, {"title": "Svm-knn: Discriminative nearest neighbor classiffication for visual category recognition", "author": ["H. Zhang", "A.C. Berg", "M. Maire", "J. Malik"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, vol. 2, pp. 2126\u20132136, IEEE, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Local prediction of non-linear time series using support vector regression", "author": ["K. Lau", "Q. Wu"], "venue": "Pattern Recognition, vol. 41, no. 5, pp. 1539\u20131547, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Zomaya,\u201dDomnet: protein domain boundary prediction using enhanced general regression network and new profiles,", "author": ["P.D. Yoo", "A.R. Sikder", "J. Taheri", "B.B. Zhou", "A. Y"], "venue": "NanoBioscience, IEEE Transactions on,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Vector quantization and signal compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Springer Science & Business Media,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2006}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1828}, {"title": "Big data deep learning: Challenges and perspectives", "author": ["X.-W. Chen", "X. Lin"], "venue": "Access, IEEE, vol. 2, pp. 514\u2013525, 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "author": ["L. Deng"], "venue": "APSIPA Transactions on Signal and Information Processing, vol. 3, p. e2, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "Traffic flow prediction with big data: A deep learning approach", "author": ["Y. Lv", "Y. Duan", "W. Kang", "Z. Li", "F.-Y. Wang"], "venue": "Intelligent Transportation Systems, IEEE Transactions on, vol. PP, no. 99, pp. 1\u20139, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "Proceedings of the 28 International Conference on Machine Learning (ICML-11), pp. 265\u2013272, 2011.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep convex net: A scalable architecture for speech pattern classiffication", "author": ["L. Deng", "D. Yu"], "venue": "Proceedings of the Interspeech, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks, vol. 5, no. 2, pp. 241\u2013259, 1992.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1992}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "A scalable machine learning online service for big data real-time analysis", "author": ["A. Baldominos", "E. Albacete", "Y. Saez", "P. Isasi"], "venue": "Computational Intelligence in Big Data (CIBD), 2014 IEEE Symposium on, pp. 1\u20138, Dec 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data machine learning and graph analytics: Current state and future challenges", "author": ["H. Huang", "H. Liu"], "venue": "Big Data (Big Data), 2014 IEEE International Conference on, pp. 16\u201317, Oct 2014.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data stream learning with samoa,\u201din", "author": ["A. Bifet", "G.D.F. Morales"], "venue": "Data Mining Workshop (ICDMW),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "Shadoop: Improving mapreduce performance by optimizing job execution mechanism in hadoop clusters", "author": ["R. Gu", "X. Yang", "J. Yan", "Y. Sun", "B. Wang", "C. Yuan", "Y. Huang"], "venue": "Journal of Parallel and Distributed Computing, vol. 74, no. 3, pp. 2166 \u2013 2179, 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Commapreduce: An improvement of mapreduce with lightweight communication mechanisms", "author": ["L. Ding", "J. Xin", "G. Wang", "S. Huang"], "venue": "Database Systems for Advanced Applications, pp. 150\u2013168, Springer, 2012.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}, {"title": "Dryad: distributed dataparallel programs from sequential building blocks", "author": ["M. Isard", "M. Budiu", "Y. Yu", "A. Birrell", "D. Fetterly"], "venue": "ACM SIGOPS Operating Systems Review, vol. 41, pp. 59\u201372, ACM, 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Piccolo: Building fast, distributed programs with partitioned tables", "author": ["R. Power", "J. Li"], "venue": "OSDI, vol. 10, pp. 1\u201314, 2010.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Data-intensive applications, challenges, techniques and technologies: A survey on big data", "author": ["C.P. Chen", "C.Y. Zhang"], "venue": "Information Sciences, vol. 275, pp. 314\u2013347, 2014.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8595\u20138598, IEEE, 2013.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale deep belief nets with mapreduce", "author": ["K. Zhang", "X. Chen"], "venue": "2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The global information and communications technology (ICT) industry that pumps out around 830 Mt carbon dioxide (CO2) emission accounts for approximately 2 percent of the global CO2 emissions [1].", "startOffset": 192, "endOffset": 195}, {"referenceID": 1, "context": "The number of server computers in data centers has increased sixfold to 30 million in the last decade, and each server draws far more electricity than its earlier models [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "of new servers [3].", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "Furthermore, most of these businesses, especially in an uncertain economic climate are placed under the pressure to reduce their energy expenditure in order to remain competitive in the market [4].", "startOffset": 193, "endOffset": 196}, {"referenceID": 4, "context": "With the emerging of new technologies and all associated devices, it is predicted that there will be as much data created as was created in the entire history of planet Earth [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "However, despite the fact that there is a demand for such efficient and sustainable data modeling methods for large and complex data-intensive fields, to our best knowledge, only a few of these literatures have been proposed in the field [6][7].", "startOffset": 238, "endOffset": 241}, {"referenceID": 6, "context": "However, despite the fact that there is a demand for such efficient and sustainable data modeling methods for large and complex data-intensive fields, to our best knowledge, only a few of these literatures have been proposed in the field [6][7].", "startOffset": 241, "endOffset": 244}, {"referenceID": 7, "context": "2 gigabase) pairs distributed among twenty-three chromosomes, which is translated to about a gigabyte of information [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "However, when we add the gene sequence data (data on the 100,000 or so translated proteins and the 32,000,000 amino acids), the relevant data volume can easily expand to an order of about 200 gigabyte [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 9, "context": "In fact, this has already been implemented by more than 200 American hospitals, and the days of squinting to decipher a doctor\u2019s untidy scrawl on a handwritten prescription will soon be a thing of the past in Canada and many other countries too [15].", "startOffset": 245, "endOffset": 249}, {"referenceID": 10, "context": "6 million terabytes by 2014 [18].", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "Recently, the Visible and Infrared Survey Telescope for Astronomy (VISTA) [19] and the Dark Energy Survey (DES) [20] \u2013 the largest universe survey projects initiated by two different consortiums of universities, from the U.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "4 terabytes of data per night [19].", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Many other astro-scientific databases, such as the Sloan Digital Sky Survey (SDSS) are already terabytes in size [21] and the Panoramic Survey Telescope-Rapid Response System (Pan-STARRS) is expected to produce a science database of more than 100 terabytes in size for the next five years [22].", "startOffset": 289, "endOffset": 293}, {"referenceID": 13, "context": "Likewise, the Large Synoptic Survey Telescope (LSST) is producing 30 terabytes of data per night, yielding a total database of about 150 petabytes [23].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 15, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 16, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 17, "context": ", and Marwah\u2019s article [24\u201327].", "startOffset": 23, "endOffset": 30}, {"referenceID": 18, "context": "Recent data-modeling research has shown that ensemble methods have gained much popularity as they often perform better than individual models [28][29].", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Recent data-modeling research has shown that ensemble methods have gained much popularity as they often perform better than individual models [28][29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "Ensemble method uses multiple models to obtain better performance than those that could be obtained from any of the constituent models [29][30].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "Ensemble method uses multiple models to obtain better performance than those that could be obtained from any of the constituent models [29][30].", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "An example of such ensemble model is the Bayes classifier [31].", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Bayes estimation techniques have been well-adopted in general intelligent data modeling because they provide a fundamental formalism for combining all the information available, with regards to the parameters to be estimated, with optimized time complexity [33].", "startOffset": 257, "endOffset": 261}, {"referenceID": 22, "context": "As a nonparametric Bayes classifier extracts worst-case example x and uses statistical analysis to build a classifying model, any learning algorithm that examines every attribute values of every training example must have at least the same or worse complexity [33].", "startOffset": 260, "endOffset": 264}, {"referenceID": 23, "context": "The time complexity of the Bayes and SVMs are well discussed in Elkan\u2019s and Joachims\u2019 article respectively [34][35].", "startOffset": 107, "endOffset": 111}, {"referenceID": 24, "context": "The time complexity of the Bayes and SVMs are well discussed in Elkan\u2019s and Joachims\u2019 article respectively [34][35].", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "have proposed two different support-vectorbased efficient ensemble models that have shown to reduce its computational cost while maintaining its performance [36].", "startOffset": 157, "endOffset": 161}, {"referenceID": 6, "context": "Their novel learning technique has proven to be successful by other similar studies [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 27, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 28, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 29, "context": "A number of recent works have demonstrated that such a local learning strategy is far superior to that of the global learning strategy, especially on data sets that are not evenly distributed [37\u201340].", "startOffset": 192, "endOffset": 199}, {"referenceID": 30, "context": "Semiparametric models that are based on local learning help not only in reducing the model complexity but also in finding the optimal tradeoff between the parametric and nonparametric models \u2013 so as to achieve both low model bias and variance [41].", "startOffset": 243, "endOffset": 247}, {"referenceID": 31, "context": "As mentioned, the local model can also be constructed from the principle of codebook [42].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "As Elkan\u2019s discussed [34], the local learning techniques \u2013 use of cn vectors for building a local model \u2013 prove that any intelligent learning model that examines all the attribute values of every training example must have the same or worse complexity.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 27, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 28, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 29, "context": "In other words, such a local learning strategy is far more efficient than that of the global learning strategy, especially on a large volume of data problems [37\u201340].", "startOffset": 158, "endOffset": 165}, {"referenceID": 32, "context": "Representation learning) has emerged as new area of ML research [43\u201345] that exploits multiple layers of information-processing in a hierarchical architecture for pattern classification and or representation learning (e.", "startOffset": 64, "endOffset": 71}, {"referenceID": 33, "context": "Representation learning) has emerged as new area of ML research [43\u201345] that exploits multiple layers of information-processing in a hierarchical architecture for pattern classification and or representation learning (e.", "startOffset": 64, "endOffset": 71}, {"referenceID": 34, "context": "Representation learning) has emerged as new area of ML research [43\u201345] that exploits multiple layers of information-processing in a hierarchical architecture for pattern classification and or representation learning (e.", "startOffset": 64, "endOffset": 71}, {"referenceID": 35, "context": ", Feed-forward neural networks) [46].", "startOffset": 32, "endOffset": 36}, {"referenceID": 35, "context": "with many hidden layers, whose weights are fully connected and often initialized or pretrained using stacked Restricted Boltzmann Machine (RBM) or Deep Belief Networks (DBMs) [46].", "startOffset": 175, "endOffset": 179}, {"referenceID": 36, "context": "DBM is a pretraining unsupervised step that utilizes large amount of unlabeled training data for extracting structures and regularities in input features [47].", "startOffset": 154, "endOffset": 158}, {"referenceID": 37, "context": "DNN has shown great performance in recognition and classification tasks, including natural language processing, image classification, and traffic flow detection [48].", "startOffset": 161, "endOffset": 165}, {"referenceID": 38, "context": "However, DNN has high computational cost and difficult to scale [49].", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "of DNN, simple classifiers are stacked on top of each other in order to construct more complex classifier [50][51].", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "of DNN, simple classifiers are stacked on top of each other in order to construct more complex classifier [50][51].", "startOffset": 110, "endOffset": 114}, {"referenceID": 41, "context": "where dj represents the target probability for output unit j, and Pj is the probability output for j after applying the activation function [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 42, "context": "Big data computing systems fall into two major categories, based on how data is analyzed with regards to time constraint [53].", "startOffset": 121, "endOffset": 125}, {"referenceID": 43, "context": ", Storm, SAMOA) [54][55].", "startOffset": 16, "endOffset": 20}, {"referenceID": 44, "context": ", Storm, SAMOA) [54][55].", "startOffset": 20, "endOffset": 24}, {"referenceID": 43, "context": "In [54], Huang and Li argued that next-generation computing systems for big data analytics need innovative designs in both", "startOffset": 3, "endOffset": 7}, {"referenceID": 45, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 54, "endOffset": 58}, {"referenceID": 48, "context": ", Hadoop [56], SHadoop [57], ComMapReduce [58], Dryad [59], Piccolo [60], and IBM parallel machine learning toolbox, such systems have the capabilities to scale up machine learning.", "startOffset": 68, "endOffset": 72}, {"referenceID": 49, "context": "The combination of deep learning and parallel training implementation techniques provides potential ways to process Big Data [61].", "startOffset": 125, "endOffset": 129}, {"referenceID": 50, "context": "[62] consider the problem of building high-level, class-specific feature detectors from only unlabeled data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Chen [63] presented a distributed learning paradigm for the RBMs and the backpropagation algorithm using MapReduce.", "startOffset": 5, "endOffset": 9}], "year": 2015, "abstractText": "With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years \u2013 in fact, as much as 90% of current data were created in the last couple of years \u2013 a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven \u2013 the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas\u2019 structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.", "creator": "Microsoft\u00ae Word 2013"}}}