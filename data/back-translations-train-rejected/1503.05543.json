{"id": "1503.05543", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Text Segmentation based on Semantic Word Embeddings", "abstract": "We explore the use of semantic word embeddings in text segmentation algorithms, including the C99 segmentation algorithm and new algorithms inspired by the distributed word vector representation. By developing a general framework for discussing a class of segmentation objectives, we study the effectiveness of greedy versus exact optimization approaches and suggest a new iterative refinement technique for improving the performance of greedy strategies. We compare our results to known benchmarks, using known metrics. We demonstrate state-of-the-art performance for an untrained method with our Content Vector Segmentation (CVS) on the Choi test set. Finally, we apply the segmentation procedure to an in-the-wild dataset consisting of text extracted from scholarly articles in the arXiv.org database.", "histories": [["v1", "Wed, 18 Mar 2015 19:44:06 GMT  (913kb,D)", "http://arxiv.org/abs/1503.05543v1", "10 pages, 4 figures. KDD2015 submission"]], "COMMENTS": "10 pages, 4 figures. KDD2015 submission", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["alexander a alemi", "paul ginsparg"], "accepted": false, "id": "1503.05543"}, "pdf": {"name": "1503.05543.pdf", "metadata": {"source": "CRF", "title": "Text Segmentation based on Semantic Word Embeddings", "authors": ["Alexander A Alemi", "Paul Ginsparg"], "emails": ["aaa244@cornell.edu", "ginsparg@cornell.edu"], "sections": [{"heading": null, "text": "Categories and Theme Descriptions I.2.7 [Natural Language Processing]: Text AnalysisGeneral Terms Information Gathering, Clustering, TextKeywords Text Segmentation, Text Mining, Text Vectors"}, {"heading": "1. INTRODUCTION", "text": "Segmentation of text into naturally contiguous sections has many useful applications in information retrieval and automated text summarization and has received much attention in the past. An early text segmentation algorithm was the TextTiling method introduced by Hearst [11] in 1997. Text was scanned linearly, with coherence calculated for each adjacent block, and a heuristic method was used to determine the locations of sections. In addition to linear approaches, there are text segmentation algorithms that optimize some scoring lenses. Permission to make digital or hard copies of all or parts of this work for the personal or classroom is granted at no charge, provided that copies are made for profit or commercial advantage, and that copies carry this message and full quotation on the first page. To copy otherwise to post or redistribute on servers requires prior specific permission and a fee or fee."}, {"heading": "2. TEXT SEGMENTATION", "text": "The segmentation task is to divide a text into contiguous contiguous sections. First, we create a representation of the text by dividing it into N base elements, ~ Vi (i = 1,..., N), each with a D-dimensional characteristic vector Vi\u03b1 (\u03b1 = 1,..., D) representing the element, then we assign each candidate segment a score \u03c3 (i, j) consisting of the ith-continuous (j \u2212 1) th elements, and finally we determine how the text is divided into the appropriate number of segments. Let's designate a segmentation of the text into K segments as a list of K indices s = (s1, s1, \u00b7 s1, \u00b7, sK), with the k-th segment containing the elements ~ Vi with sk \u2212 1 \u2264 i < sk, with s0 \u2261 0. For example, the string considered at character level would be \"aaac,\" ccabbaa \"(8,\" ccb), \"add\" (10, \"add\")."}, {"heading": "2.1 Representation", "text": "Thus, the text representation boils down to transforming a simple text document T into a (N \u00b7 D) -dimensional matrix V, where N is the number of initial elements to be grouped into coherent segments, and D is the dimensionality of the element representation. For example, if one segments at the word level, N is the number of words in the text, and each word could be represented by a D-dimensional vector as obtained from GloVe [16]. Instead, if one segments at the sentence level, N is the number of sentences in the text, and we have to decide how to represent each sentence. There are other preprocessing decisions, such as the use of a stemming algorithm or the removal of stop words before the representation. Special pre-processing decisions can have a big impact on the performance of segmentation algorithms, but for discussing scoring functions and distribution methods, these decisions can be abstracted into the specification of the N \u00d7 D-V matrix."}, {"heading": "2.2 Scoring", "text": "After creating an initial representation of the text, we next specify the coherence of a text segment with a scoring function \u03c3 (i, j), which acts on representation V and returns a score number for the segment ranging from i (inclusive) to j (not inclusive). The score can be a simple scalar or more general object. In addition to the scoring function, we must specify how to return a score total score for the entire segmentation. This score aggregation function can be so simple that the scores for each segment are added, or even a more general function. The score value S (s) for a total segmentation is given by aggregating all segments in the segmentation: S (s) = \u03c3 (s1, s2)."}, {"heading": "2.3 Splitting", "text": "In this paper we look at three methods: (1) greedy splitting, which inserts the best available segmentation boundary in each step and then adjusts the boundaries to improve performance; (2) dynamic segmentation scheme, which uses dynamic segmentation approach to find the optimal segmentation; (3) an iterative segmentation scheme, which starts with greedy segmentation and then adjusts the boundaries to improve performance; (2) Greedy segmentation approach builds an optimal segmentation method to find optimal segmentation in K segments. (4) st + 1 = arg mini [1, N) C (st] s (st {i}) until the desired number of segments is achieved."}, {"heading": "3. SCORING FUNCTIONS", "text": "In the following experiments we will test different possibilities for the presentation, scoring function and splitting method in the above general framework. The segmentation algorithms to be considered are divided into three groups:"}, {"heading": "3.1 C99 Segmentation", "text": "Choi's C99 algorithm [3] was an early text segmentation algorithm with promising results. The feature vector for a text element is chosen as a pair of cosinal distances with other text elements, where these elements in turn are represented by a bag of native word vectors (after pre-processing to remove stop words): The paired cosinal distance matrix is noisy for these properties, and since only the relative values are meaningful, C99 uses a ranking transformation, whereby each value of the matrix is replaced by the fraction of its neighbors with a smaller value: Vij = 1 r2 \u2212 1: The paired cosinal distance matrix is noisy for these properties, and since only the relative values are meaningful, C99 uses a ranking transformation, whereby each value of the matrix is replaced by the fraction of its neighbors with a smaller value: Vij = 1 r2 = \u2264 2 \u2212 j = \u2264 1: \u2264 i / l = \u2264 2: \u2264 1 / l \u2212 i = \u2264 1 / l."}, {"heading": "3.2 Average word vector", "text": "To assess the usefulness of word vectors in segmentation, we first examine how they can be used to improve the C99 algorithm, and then look at more general scoring functions based on our word vector representation. \u2212 As representation of an element, we take Vik = \u2211 w fiwvwk, (16) with fiw representing the frequency of word w in element i, and vwk representing the kth component of the word vector for word w, as learned by a word vector training algorithm, such as word2vec [14] or GloVe [16]. The length of word vectors varies widely within the vocabulary and generally correlates with word frequency. \u2212 To mitigate the effect of common words, we sometimes weight the sum by the inverse frequency of the word (idf) of the word in the corpus: Vik = fiww w fiww w (fivw w) where the number of the word in the fivw w is fictitious."}, {"heading": "3.3 Content Vector Segmentation (CVS)", "text": "Experienced word vectors have a remarkable amount of structure. Analogy tasks such as man: woman: king:? can be solved by finding the vector closest to the linear query: vwoman \u2212 vman + vking. (21) Arora et al. [1] constructed a generative text model that explains how this linear structure arises and can be maintained even in relatively low dimensional vector models. The generative model consists of a content vector that traverses a random path from a stationary distribution to be the product distribution on each of its components ck, uniform on the interval [\u2212 1 \u221a D] (with D the dimensionality of the word vectors). At any point in time, a word vector is generated by the content vector according to a loglinear model: P (w | c) = 1 Zc exp (w \u00b7 c), Zc = v \u00b7 c exp (v \u00b7 c), c \u00b7 c, c \u00b7 c (c)."}, {"heading": "4. EXPERIMENTS", "text": "To investigate the effectiveness of different segmentation strategies and algorithms, we conducted segmentation experiments on two datasets: The first is the Choi dataset [3], a common benchmark in earlier segmentation work, and the second is a similarly constructed dataset based on articles uploaded to arXiv as described in Section 4.3."}, {"heading": "4.1 Evaluation", "text": "To evaluate the performance of our algorithms, we use two standard metrics: the Pk metric and the WindowDiff (WD) metric. In text segmentation, near misses should get more credit than far misses. Pk metrics [2] measures the probability that a sample consisting of a pair of adjacent elements (at constant distance positions (i, i + k)) will be placed in the same segment by reference and hypothetical segmentations. In particular, Pk metrics counts the number of discrepancies in the sample elements: Pk = 1N \u2212 k N \u2212 k = 1 [\u03b4hyp (i, i + k) 6 = \u03b4ref (i, i + k) \u2212 (30) k = nearest integration12 # elements # # segments # 1, the difference (i, j) being 1 or 0, depending on whether the boundary between the elements j is in the same segment or not."}, {"heading": "4.2 Choi Dataset", "text": "The first three sets that we have selected from each document are in a range that can be reached by the subset id (i.e., how it can be reached by an archived version of the C99 segmentation code), there are four areas where you can think: (3-5, 6-8, 3-11, 3-11), the first three of which are 100 sample documents, and the last 400 documents. Datasets can be realized by an archived version of the C99 segmentation code. An extract from one of the documents in the test set is shown in Fig. 1.4.2.1 C99 benchmark We will explore the effects of changing the representation and splitting strategy of the C99 algorithms."}, {"heading": "4.3 ArXiv Dataset", "text": "The people mentioned are capable of hiding without being able to play by the rules."}, {"heading": "5. CONCLUSION", "text": "We presented a general framework for describing and developing segmentation algorithms and compared some existing and new strategies for representation, scoring and splitting. We demonstrated the usefulness of semantic word embedding for segmentation, both in existing algorithms and in new segmentation algorithms. On a real segmentation task at the word level, we demonstrated the ability to generate useful segmentation of scientific articles. In the future, we plan to use this segmentation method to facilitate finding documents with segmented content and identify documents with localized sections of similar content."}, {"heading": "6. ACKNOWLEDGEMENTS", "text": "This work was supported by NSF IIS-1247696. We thank James P. Sethna for useful discussions and feedback on the manuscript."}, {"heading": "7. REFERENCES", "text": "[1] S. Arora, Y. Li, T. M. Yingyu Liang, and A. Risteski.Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings. 2015, arXiv: 1502.03520. [2] D. Beeferman, A. Berger, and J. Lafferty. Statistical models for text segmentation. [3] Machine learning, 34 (1-3): 177-210, 1999. [3] F. Y. Choi. Advances in domain independent linear text segmentation. In Proc. of the 1st North American chapter of the Association for Computational Linguistics conference, 34. Association for Computational Linguistics, 2000, arXiv: cs / 0003083. [4] F. Y. Choi, P. Wiemer-Hastings, and J. Moore. Latent semantic analysis for text segmentation."}, {"heading": "A. DETAILS OF C99 REPRESENTATION", "text": "We implemented our own version of the C99 algorithm (oC99) and tested it on the Choi dataset. We investigated the effects of various changes to the representation part of the algorithm, namely the effects of removing stopwords, truncating small sentence sizes, truncating words, and performing rank conversion on the cosinal similarity matrix. In the parentage process, the implementation of nltk's Porter parentage algorithm was used. For stopwords, we used the list distributed with the C99 code, supplemented by a list of punctuation marks. The results are shown in Table 6.While we reproduce the results reported in [4] without rank conversion (C99 in Table 6), our results for rank conversions (the last two lines for oC99) show better performance without discrepancies. This is probably due to the peculiarities in terms of text transformations, such as the 99 precise parentage algorithm, and the result of the last word being transformed without the discrepancies in the rank algorithm, where the two are likely to be transformations in the last word."}, {"heading": "B. OVERFITTING THE CHOI DATASET", "text": "Note from paragraph 4.2 that each sample document in the Choi data set consists of 10 segments, and each of these segments is the first n sentences of a subset of 124 Brown documents (the sentences ca * *.pos and cj * *.pos), which means that each of the four Choi test sentences (n = 3-5, 6-8, 9-11, 3-11) necessarily contains several repetitions of each sentence. In group 3-5 Choi, for example, there are 3986 sentences, but only 608 unique sentences, so that each sentence appears an average of 6.6 times. In group 3-11, with 400 example documents, there are 28,145 sentences, but only 1353 unique sentences, with an average of 20.8 appearances per sentence. Moreover, in all cases there are only 124 unique sentences that can start a new segment. This redundancy means that a trained method like LDA will see most or all test data during the training, and will easily pass over the number of each segment if you can precisely fit the segments."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["S. Arora", "Y. Li", "T.M. Yingyu Liang", "A. Risteski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Statistical models for text segmentation", "author": ["D. Beeferman", "A. Berger", "J. Lafferty"], "venue": "Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Advances in domain independent linear text segmentation", "author": ["F.Y. Choi"], "venue": "In Proc. of the 1st North American chapter of the Association for Computational Linguistics conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Latent semantic analysis for text segmentation", "author": ["F.Y. Choi", "P. Wiemer-Hastings", "J. Moore"], "venue": "Proceedings of EMNLP. Citeseer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Learning feature representations with k-means", "author": ["A. Coates", "A.Y. Ng"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "On automatic text segmentation", "author": ["B. Dadachev", "A. Balinsky", "H. Balinsky"], "venue": "In Proceedings of the 2014 ACM symposium on Document engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Topic segmentation with a structured topic model", "author": ["L. Du", "W.L. Buntine", "M. Johnson"], "venue": "In HLT-NAACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Latent semantic analysis", "author": ["S.T. Dumais"], "venue": "Ann. Rev. of Information Sci. and Tech.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Bayesian unsupervised topic segmentation", "author": ["J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A dynamic programming algorithm for linear text segmentation", "author": ["P. Fragkou", "V. Petridis", "A. Kehagias"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Texttiling: Segmenting text into multi-paragraph subtopic passages", "author": ["M.A. Hearst"], "venue": "Computational linguistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1309.4168,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Text segmentation via topic modeling: an analytical study", "author": ["H. Misra", "F. Yvon", "J.M. Jose", "O. Cappe"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A critique and improvement of an evaluation metric for text segmentation", "author": ["L. Pevzner", "M.A. Hearst"], "venue": "Comp. Ling.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Text segmentation with topic models", "author": ["M. Riedl", "C. Biemann"], "venue": "Journal for Language Technology and Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Domain-independent unsupervised text segmentation for data management", "author": ["M. Sakahara", "S. Okada", "K. Nitta"], "venue": "In Data Mining Workshop (ICDMW),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Problems and algorithms for sequence segmentations", "author": ["E. Terzi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "A statistical model for domain-independent text segmentation", "author": ["M. Utiyama", "H. Isahara"], "venue": "In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 15, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 11, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 2, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 134, "endOffset": 140}, {"referenceID": 3, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 134, "endOffset": 140}, {"referenceID": 17, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 14, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 2, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 3, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 1, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 79, "endOffset": 86}, {"referenceID": 16, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 79, "endOffset": 86}, {"referenceID": 10, "context": "An early text segmentation algorithm was the TextTiling method introduced by Hearst [11] in 1997.", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "early algorithm in this class was Choi\u2019s C99 algorithm [3] in 2000, which also introduced a benchmark segmentation dataset used by subsequent work.", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Later work by Choi and collaborators [4] used distributed representations of words rather than a bag of words approach, with the representations generated by LSA [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Later work by Choi and collaborators [4] used distributed representations of words rather than a bag of words approach, with the representations generated by LSA [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "[10] attempted to find the optimal splitting for their own objective using dynamic programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] and Riedl and Biemann [18], used LDA based topic models to inform the segmentation task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[15] and Riedl and Biemann [18], used LDA based topic models to inform the segmentation task.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "consider structured topic models for segmentation [7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Eisenstein and Barzilay [9] and Dadachev et al.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "[6] both consider a Bayesian approach to text segmentation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] consider a segmentation algorithm which does affinity propagation clustering on text representations built from word vectors learned from word2vec [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[19] consider a segmentation algorithm which does affinity propagation clustering on text representations built from word vectors learned from word2vec [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "For the most part, aside from [19], the non-topic model based segmentation approaches have been based on relatively simple representations of the underlying text.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "\u2019s word2vec [14], Pennington et al.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "[1] have shown how the remarkable performance of these techniques can be understood in terms of relatively mild assumptions about corpora statistics, which", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "For example, if segmenting at the word level then N would be the number of words in the text, and each word might be represented by a D-dimensional vector, such as those obtained from GloVe [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 2, "context": "For most of the segmentation schemes to be considered, the score function itself returns a scalar, so the score aggregation function \u2295 will be taken as simple addition with the key function the identity, but the generality here allows us to incorporate the C99 segmentation algorithm [3] into the same framework.", "startOffset": 284, "endOffset": 287}, {"referenceID": 2, "context": "Many published text segmentation algorithms are greedy in nature, including the original C99 algorithm [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 19, "context": "For a detailed account of dynamic programming and segmentation in general, see the thesis by Terzi [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "[10], with much success, but we will also consider here an optimizaton of the the C99 segmentation algorithm using a dynamic programming approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Choi\u2019s C99 algorithm [3] was an early text segmentation algorithm with promising results.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "[4] explored the effect of using combinations of LSA word vectors in eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "with fiw representing the frequency of word w in element i, and vwk representing the k th component of the word vector for word w as learned by a word vector training algorithm, such as word2vec [14] or GloVe [16].", "startOffset": 195, "endOffset": 199}, {"referenceID": 15, "context": "with fiw representing the frequency of word w in element i, and vwk representing the k th component of the word vector for word w as learned by a word vector training algorithm, such as word2vec [14] or GloVe [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 0, "context": "[1] constructed a generative model of text that explains how this linear structure arises and can be maintained even in relatively low dimensional vector models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "(This is similar in spirit to the probabilistic segmentation technique proposed by Utiyama and Isahara [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "The first is the Choi dataset [3], a common benchmark used in earlier segmentation work, and the second is a similarly constructed dataset based on articles uploaded to the arXiv, as will be described in Section 4.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "The Pk metric [2], captures the probability for a probe composed of a pair of nearby elements (at constant distance positions (i, i+k)) to be placed in the same segment by both reference and hypothesized segmentations.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "Trivial strategies such as choosing only a single segmentation, or giving each element its own segment, or giving constant boundaries or random boundaries, tend to produce values of around 50% [2].", "startOffset": 193, "endOffset": 196}, {"referenceID": 16, "context": "Hearst [17] introduced the WindowDiff (WD) metric:", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "pos sets) [3].", "startOffset": 10, "endOffset": 13}, {"referenceID": 15, "context": "The word vectors were learned by GloVe [16] on a 42 billion word set of the Common Crawl corpus in 300 dimensions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "The upper section cites results from [4], exploring the utility of using LSA word vectors, and showed an improvement of a few percent over their baseline C99 implementation.", "startOffset": 37, "endOffset": 40}, {"referenceID": 17, "context": "The middle section shows results from [18], which augmented the C99 method by representing each element with a histogram of topics learned from LDA.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "[18], we perform spherical k-means clustering on the word vectors [5] and represent each sentence as a histogram of its word clusters (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[18], we perform spherical k-means clustering on the word vectors [5] and represent each sentence as a histogram of its word clusters (i.", "startOffset": 66, "endOffset": 69}, {"referenceID": 17, "context": "In this case, the word topic representations (oC99k50 and oC99k200 in Table 1) do not perform as well as the C99 variants of [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "But as was noted in [18], those topic models were trained on cross-validated subsets of the Choi dataset, and benefited from seeing virtually all of the sentences in the test sets already in each training set, so have an unfair advantage that would not necessarily convey to real world applications.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "Pk WD Algorithm 3\u20135 6\u20138 9\u201311 3\u201311 3\u20135 6\u20138 9\u201311 3\u201311 C99 [4] 12 11 9 9 C99LSA 9 10 7 5 C99 [18] 11.", "startOffset": 56, "endOffset": 59}, {"referenceID": 17, "context": "Pk WD Algorithm 3\u20135 6\u20138 9\u201311 3\u201311 3\u20135 6\u20138 9\u201311 3\u201311 C99 [4] 12 11 9 9 C99LSA 9 10 7 5 C99 [18] 11.", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "C99LSA) shows the few percent improvement over the C99 baseline reported in [4] of using LSA to encode the words.", "startOffset": 76, "endOffset": 79}, {"referenceID": 17, "context": "C99LDA) shows the effect of modifying the C99 algorithm to work on histograms of LDA topics in each sentence, from [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "The bottom section shows the effect of using word vectors trained from GloVe [16] in our oC99 implementation of the C99 segmentation algorithm.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "29 M09 [15] 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "3 R12 [18] 1.", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "95 D13 [7] 1.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "2), we have included the results of the topic modeling based approaches M09 [15], R12 [18], and D13 [6] for reference.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "2), we have included the results of the topic modeling based approaches M09 [15], R12 [18], and D13 [6] for reference.", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "2), we have included the results of the topic modeling based approaches M09 [15], R12 [18], and D13 [6] for reference.", "startOffset": 100, "endOffset": 103}, {"referenceID": 17, "context": "In [18], it is observed that \u201cThis makes the Choi data set artificially easy for supervised approaches.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "3\u20135 6\u20138 9\u201311 3\u201311 R-CVS vs DP-CVS [3] 0.", "startOffset": 34, "endOffset": 37}], "year": 2015, "abstractText": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation. By developing a general framework for discussing a class of segmentation objectives, we study the effectiveness of greedy versus exact optimization approaches and suggest a new iterative refinement technique for improving the performance of greedy strategies. We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17]. We demonstrate state-of-the-art performance for an untrained method with our Content Vector Segmentation (CVS) on the Choi test set. Finally, we apply the segmentation procedure to an inthe-wild dataset consisting of text extracted from scholarly articles in the arXiv.org database.", "creator": "LaTeX with hyperref package"}}}