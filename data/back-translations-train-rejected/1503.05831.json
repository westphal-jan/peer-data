{"id": "1503.05831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "Neural Network-Based Active Learning in Multivariate Calibration", "abstract": "In chemometrics, data from infrared or near-infrared (NIR) spectroscopy are often used to identify a compound or to analyze the composition of amaterial. This involves the calibration of models that predict the concentration ofmaterial constituents from the measured NIR spectrum. An interesting aspect of multivariate calibration is to achieve a particular accuracy level with a minimum number of training samples, as this reduces the number of laboratory tests and thus the cost of model building. In these chemometric models, the input refers to a proper representation of the spectra and the output to the concentrations of the sample constituents. The search for a most informative new calibration sample thus has to be performed in the output space of the model, rather than in the input space as in conventionalmodeling problems. In this paper, we propose to solve the corresponding inversion problem by utilizing the disagreements of an ensemble of neural networks to represent the prediction error in the unexplored component space. The next calibration sample is then chosen at a composition where the individual models of the ensemble disagree most. The results obtained for a realistic chemometric calibration example show that the proposed active learning can achieve a given calibration accuracy with less training samples than random sampling.", "histories": [["v1", "Thu, 19 Mar 2015 16:30:21 GMT  (1394kb)", "http://arxiv.org/abs/1503.05831v1", "9 pages in final printed version"]], "COMMENTS": "9 pages in final printed version", "reviews": [], "SUBJECTS": "cs.NE cs.CE cs.LG", "authors": ["a ukil", "j bernasconi"], "accepted": false, "id": "1503.05831"}, "pdf": {"name": "1503.05831.pdf", "metadata": {"source": "CRF", "title": "Neural Network-Based Active Learning in Multivariate Calibration", "authors": ["Abhisek Ukil", "Jakob Bernasconi"], "emails": ["abhisek.ukil@ch.abb.com;", "jakob.bernasconi@alumni."], "sections": [{"heading": null, "text": "Index Terms - Active learning, chemometrics, design of experiment, ensemble of models, error prediction, inverse model, model building, multivariate calibration, near infrared (NIR), spectra, spectrometer, spectroscopy.I. INTRODUCTIONINFRARED (IR) or near infrared (NIR) spectroscopy is amethod used to identify a compound or to analyse the composition of a material by studying the interaction of IR light with matter. Chemometrics is the application of mathematical and statistical methods to analyze chemical data, e.g., multivariate calibration, signal / conditioning, pattern recognition, experimental design, etc. [1].The process of such appropriate model parameters that lead from the spectrum to the desired information on the material is Nambration, b."}, {"heading": "A. Motivation", "text": "In chemometry, chemical laboratory tests typically cost about 500-1000 USD per sample, depending on the application [3]. Therefore, it is of particular interest to achieve a given calibration accuracy with a minimum number of training samples in order to minimize the cost of model making. To this end, fixed experimental designs [14] are often used, in which the entire set of training samples is determined in one step. Non-linear calibration techniques, especially NN [3], [7] - [13], attempt to achieve faster convergence to a certain level of accuracy and essentially replace linear methods."}, {"heading": "B. Problem to be Solved", "text": "In this paper, however, we will be primarily interested in a sequential determination of optimal training samples, i.e. active learning techniques. However, our main objective will be to evaluate the use of an ensemble of NN models for active learning in the calibration of chemometric models. However, due to the specific nature of the calibration in chemometry, modern active learning methods cannot be directly applied, but must solve an inverse problem for active learning, which will be explained later. It is also interesting to check whether such an inverse learning method based on active learning provides a tangible advantage over traditional direct methods. The rest of this paper is organized as follows. Section II provides an overview of various active learning techniques based on the latest state of the art. The inverse problem that needs to be solved when active learning is applied in particular to chemometric calibration problems is described in Section III. Experimental results are discussed in detail in Section IV and discussed in Section V, followed by Conclusions."}, {"heading": "II. ACTIVE LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. What is Active Learning", "text": "In industrial research, one is often confronted with the task of investigating the relationship between a number of input variables x = (x1, x2,., xn) and a response y. Inchemometric calibration problems, x can relate, for example, to the main components (PCs) of a measured IR spectrum and y to the concentration of a component in the corresponding sample. In most cases, these relationships are so complex that it is impossible to model them reliably from physical principles, i.e., they can only be explored by a combination of experiments and empirical modeling. The main objectives of such a process are the following: 1) to achieve an acceptable match of the experimental data; 2) to obtain reliable information about the relationship between y and x in a predefined x region of interest."}, {"heading": "B. Maximal Information Gain", "text": "We want to model the relationship between a vector x of input variables and an answer y (x) by a function y = f (x, w), where w denotes a vector of parameters; the result of an experiment is presented as an approximate response pair {xi, ti}, where ti is the measured response to the input xi, and \u03c1N (w) denotes the probability distribution of the model parameters w after N measurements {xi, ti}, i = 1, 2,., N, however, are analyzed. The information gain associated with the creation of a new experiment on x is then equal to the expected entropy degression. (x) = D measurements N (t | x) - SN + 1 (w | x, t})]] (1), whence SN (w) = \u2212 D measurement on x (w) logically N (w)."}, {"heading": "D. Space-Filling Strategies", "text": "A completely different approach to active learning is the space filling strategy. In space filling, the next experiment is simply performed on the x-value that has the largest \"distance\" d (x) to the existing training data. As a measure of distance, the Euclidean distance in x-distance (x) = min k x-xk (7) can usually be chosen, taking the minimum over the current set of training samples, which will be discussed in detail later on with regard to calibration problems."}, {"heading": "E. Discussion and Comparison of Active Learning Strategies", "text": "The performance of active learning strategies was tested on a number of (mostly academic) problems [15] - [21]. Hu et al. [17] proposed unsupervised active learning using a graphical approach. Xingquan et al. [18] used ensembles of different classifiers for active learning from streaming data. Basak and Gupta [19] used forward active learning NN. Krogh and Vedelsby [20] compared NN ensembles for active learning. Poland and Zell [21] used five simple test functions to compare the performance of different active learning strategies with the selection of random data. In their tests, variance-based methods (maximum modeling uncertainty) always led to a much faster reduction in modeling error (with an increasing number of training samples) than random or space-filling strategies. Only for low-dimensional entry spaces, space-based strategies led to a much easier measurement-based response to problem-related problems (as opposed to a significant improvement)."}, {"heading": "III. ACTIVE LEARNING IN CALIBRATION PROBLEMS", "text": "A. Inverse problemAll active learning strategies discussed in the previous section relate to a learning problem where the training samples are constructed by conducting an experiment (i.e. measuring output y) for a selected value of the inputx. However, this paper deals with a different type of learning problem. These relate to the calibration of chemometric models [1], [3] for example, which predict the concentration of a component (output y) from a condensed representation (input x) of the IR spectrum (PCs, partial quadrates [1] or wave components) of a heterogeneous material. Therefore, the search for a highly informative next experiment must be conducted in the output space, rather than in the input space as with conventional modeling problems. Asone cannot measure the \"next spectrum,\" one must select the sample constitutions to obtain the next composition, whose spectrum is then measured."}, {"heading": "B. Algorithm", "text": "In the following, we describe our proposed algorithm for solving this inversion problem. The models used in the algorithm all refer to feedforward NNs [22], [23], which are trained by error feedback [22], [23]. For simplicity, we formulate our active learning algorithm for models that predict the concentration of a single component. The variance-based active learning algorithm is described as follows. 1. Step 0: Select an initial set of N0 calibration samples. Each sample consists of the measured concentration and the corresponding IR spectrum (characterized, for example, by a number of PCs). 2. Step: Train a group of \"prediction models\" (models that predict the concentration of a sample from the measured IR spectrum) with the current amount of calibration samples. 3. Step: Train a group of \"prediction models\" (models that predict the concentration of a sample from the measured IR spectrum with the actual calibration spectra)."}, {"heading": "IV. NUMERICAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset", "text": "In order to validate the proposed active learning algorithm of section III-B, we use the \"Tecator\" dataset [24]. The dataset consists of 240 NIR spectra of several pieces of meat, along with parameters such as fat, protein and water content of each meat sample. We used all 240 spectra and chose fat concentration as parameters of interest. We noticed that there is some repeated data with identical spectra and concentration values. Nevertheless, we included all data in the models."}, {"heading": "B. Models", "text": "The NNs used in the various stages of the active learning algorithm are described below. However, the MATLAB NN toolbox [25] was used for network design and experiments. [23] The \"prediction networks\" used in step 1 (models for predicting the fat value of training samples) have a 10-7-3-1 architecture, which means it has an input layer of ten nodes (ten PCs of the spectra are used as input), two hidden layers, consisting of seven or three neurons, and one output (fat content). The non-linear transfer function between the input and the first hidden layer is a tangsigmode function [22], and a linear transfer function is used between the first and second hidden layer and between the second layer and output. These network design parameters have been optimized in a number of experiments."}, {"heading": "D. Results", "text": "Figures 1-3 show a comparison of the RMSE results for data acquisition by active learning and sampling, for an addition of five samples per iteration, starting with 20, 40 and 80 initial samples, respectively. After several experiments, the three samples are identified as such."}, {"heading": "V. DISCUSSION", "text": "This year, it is time for us to set out to find a solution that paves the way for the future, to pave the way for the future."}, {"heading": "VI. CONCLUSION", "text": "In a multivariate calibration process, one finds the optimal model parameters that lead from the spectrum to the desired information about the composition of the material. An interesting aspect of data addition is the achievement of a certain level of accuracy with a minimum number of calibration samples, since this reduces the number of laboratory tests and thus the cost of model creation. In chemometric calibration models, the input refers to a correct representation of the spectrum and the output to the concentrations of the sample components. Therefore, the search for the most informative possible next experiment must be carried out in the output room, and not in the entrance room as with conventional modeling problems. \"In this essay, we have proposed an inverse modeling technique that uses the inconsistencies of an ensemble of prediction models to determine the next samples in the unexplored component space. We propose to select the next sample (s) where the models are most variable from each other calibration algorithms."}], "references": [{"title": "Chemometrics: Data Analysis for the Laboratory and Chemical Plant", "author": ["R.G. Brereton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "NIR spectroscopy: A rapid-response analytical tool", "author": ["M. Blanco", "I. Villarroya"], "venue": "Trends Anal. Chem., vol. 21, no. 4, pp. 240\u2013250, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved calibration of near-infrared spectra by using ensembles of neural network models", "author": ["A. Ukil", "J. Bernasconi", "H. Braendle", "H. Buijs", "S. Bonenfant"], "venue": "IEEE Sens. J., vol. 10, no. 3, pp. 578\u2013584, Mar. 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "GRAMS R  \u00a9Spectroscopy Software Suite", "author": ["Thermo Scientific. (2004"], "venue": "[Online]. Available: http://www.thermo.com/com/cda/product/detail/ 0,1055,22290,00.html", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2229}, {"title": "The Unscrambler R  \u00a99.7", "author": ["Camo. (2007"], "venue": "[Online]. Available: http:// unscrambler.camo.com", "citeRegEx": "5", "shortCiteRegEx": null, "year": 0}, {"title": "HORIZON MBTM FT-IR Software", "author": ["ABB. (2011"], "venue": "[Online]. Available: http://www.abb.com/product/us/9AAC100055.aspx", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1000}, {"title": "Artificial neural networks in multivariate calibration", "author": ["T. Naes", "K. Kvaal", "T. Isaksson", "C. Miller"], "venue": "J. Near Infrared Spectrosc., vol. 1, pp. 1\u201311, 1993.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Neural networks in multivariate calibration", "author": ["F. Despagne", "D.L. Massart"], "venue": "Analyst, vol. 123, pp. 157R\u2013178R, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Chemometric calibration of infrared spectrometers: Selection and validation of variables by nonlinear models", "author": ["N. Benoudjit", "E. Cools", "M. Meurens", "M. Verleysen"], "venue": "Chemomet. Intell. Lab. Syst., vol. 70, pp. 47\u201353, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Application of an artificial neural network to the identification of amino acids from near infrared spectral data", "author": ["T. Sato"], "venue": "J. Near Infrared Spectrosc, vol. 1, no. 4, pp. 199\u2013208, 1993.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Comparison of linear and nonlinear calibration models based on near infrared (NIR) spectroscopy data for gasoline properties prediction", "author": ["R.M. Balabin", "R.Z. Safieva", "E.I. Lomakina"], "venue": "Chemometr. Intell. Lab. Syst., vol. 88, pp. 183\u2013188, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal technique for optimization of neural network training parameters: Gasoline near infrared data example", "author": ["R.M. Balabin", "R.Z. Safieva", "E.I. Lomakina"], "venue": "Neural Comput. Appl., vol. 18, no. 6, pp. 557\u2013565, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Prediction of polyethylene density by near-infrared spectroscopy combined with neural network analysis", "author": ["K. Saeki", "K. Tanabe", "T. Matsumoto", "H. Uesaka", "T. Amano", "K. Funatsu"], "venue": "J. Comput. Chem. Jpn., vol. 2, no. 1, pp. 33\u201340, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive design of experiments", "author": ["J. Bernasconi", "F. Greuter"], "venue": "Informatik-Informatique, vol. 1, pp. 18\u201320, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Information-based objective functions for active data selection", "author": ["D.J.C. MacKay"], "venue": "Neural Comput., vol. 4, pp. 590\u2013604, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised active learning based on hierarchical graph-theoretic clustering", "author": ["W. Hu", "W. Hu", "N. Xie", "S. Maybank"], "venue": "IEEE Trans. Syst. Man. Cybern. B, Cybern., vol. 39, no. 5, pp. 1147\u20131161, Oct. 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Active learning from stream data using optimal weight classifier ensemble", "author": ["Z. Xingquan", "Z. Peng", "L. Xiaodong", "S. Yong"], "venue": "IEEE Trans. Syst. Man. Cybern. B, Cybern., vol. 40, no. 6, pp. 1607\u20131621, Dec. 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Active evaluation and ranking of multipleattribute items using feedforward neural networks", "author": ["J. Basak", "M. Gupta"], "venue": "IEEE Trans. Syst. Man. Cybern. A, Syst. Humans, vol. 36, no. 6, pp. 1135\u20131145, Nov. 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["A. Krogh", "J. Vedelsby"], "venue": "Advances in Neural Information Processing Systems, G. Tesauro et al., Eds. Cambridge, MA: MIT Press, 1995, pp. 231\u2013 238.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Different criteria for active learning in neural networks: A comparative study", "author": ["J. Poland", "A. Zell"], "venue": "Proc. Eur. Symp. Artif. Neural Netw., 2002, pp. 119\u2013124.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "New York: MacMillan,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Intelligent Systems and Signal Processing in Power Engineering", "author": ["A. Ukil"], "venue": "Berlin, Germany: Springer-Verlag,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Tecator dataset", "author": ["Carnegie Mellon University"], "venue": "[Online]. Available: http://lib.stat.cmu.edu/datasets/tecator", "citeRegEx": "24", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The sequential steps in multivariate calibration are described in [2] and [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "The sequential steps in multivariate calibration are described in [2] and [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "ear regression from standard software like GRAMS/AI [4], Unscrambler [5]), HORIZON MB [6], etc.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "ear regression from standard software like GRAMS/AI [4], Unscrambler [5]), HORIZON MB [6], etc.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "ear regression from standard software like GRAMS/AI [4], Unscrambler [5]), HORIZON MB [6], etc.", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "been widely used, as reported in the works [7]\u2013[9], also for particular spectroscopy applications, e.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "been widely used, as reported in the works [7]\u2013[9], also for particular spectroscopy applications, e.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": ", prediction of cleaning solution component concentrations [3], identification of amino", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "acid [10], gasoline applications [11], [12], polyethylene density prediction [13], etc.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "acid [10], gasoline applications [11], [12], polyethylene density prediction [13], etc.", "startOffset": 33, "endOffset": 37}, {"referenceID": 11, "context": "acid [10], gasoline applications [11], [12], polyethylene density prediction [13], etc.", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "acid [10], gasoline applications [11], [12], polyethylene density prediction [13], etc.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "In chemometrics, depending on the application, the chemical lab tests typically cost around 500\u20131000 USD per sample [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "Nonlinear calibration techniques, particularly using NN [3], [7]\u2013[13], try to achieve a faster convergence to a particular accuracy level, basically replacing the linear methods.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Nonlinear calibration techniques, particularly using NN [3], [7]\u2013[13], try to achieve a faster convergence to a particular accuracy level, basically replacing the linear methods.", "startOffset": 61, "endOffset": 64}, {"referenceID": 12, "context": "Nonlinear calibration techniques, particularly using NN [3], [7]\u2013[13], try to achieve a faster convergence to a particular accuracy level, basically replacing the linear methods.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "This problem of \u201cactive data selection\u201d or \u201cactive learning\u201d has a long history, and it has gained renewed interest in connection with NN learning [15]\u2013[21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 19, "context": "This problem of \u201cactive data selection\u201d or \u201cactive learning\u201d has a long history, and it has gained renewed interest in connection with NN learning [15]\u2013[21].", "startOffset": 152, "endOffset": 156}, {"referenceID": 14, "context": "If we assume that the empirical model y = f(x,w) is capable of representing the experimental data {xi , ti} up to a zeromean Gaussian noise with standard deviation \u03c3, an approximate evaluation of S(x) leads to the following expression [16]:", "startOffset": 235, "endOffset": 239}, {"referenceID": 2, "context": ", start the training process from different, randomly chosen, initial weights w, or use bootstrapping [3] to generate different sets of training samples, etc.", "startOffset": 102, "endOffset": 105}, {"referenceID": 13, "context": "on a number of (mostly academic) problems [15]\u2013[21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "on a number of (mostly academic) problems [15]\u2013[21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "[17] proposed unsupervised active learning using graphtheoretic approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] used ensemble of different classifiers for active learning from streaming data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Basak and Gupta [19] used feedforward NN for active learning.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "Krogh and Vedelsby [20] compared NN ensembles for active learning.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "Poland and Zell [21] have used five simple test functions to compare the performance of different active learning strategies with that of random data selection.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "These refer to the calibration of chemometric models [1], [3] that predict, e.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "These refer to the calibration of chemometric models [1], [3] that predict, e.", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": ", the concentration of some constituent (output y) from a condensed representation (input x) of the IR spectrum (PCs, partial least squares [1], or wavelet components) of a heterogeneous material.", "startOffset": 140, "endOffset": 143}, {"referenceID": 20, "context": "The models used in the algorithm all refer to feedforward NNs [22], [23] that are trained by error backpropagation [22], [23].", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "The models used in the algorithm all refer to feedforward NNs [22], [23] that are trained by error backpropagation [22], [23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "The models used in the algorithm all refer to feedforward NNs [22], [23] that are trained by error backpropagation [22], [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "The models used in the algorithm all refer to feedforward NNs [22], [23] that are trained by error backpropagation [22], [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 2, "context": "Bootstrapping [3] is used to create the individual models of the ensemble.", "startOffset": 14, "endOffset": 17}, {"referenceID": 22, "context": "To validate the proposed active learning algorithm of Section III-B, we use the \u201cTecator\u201d dataset [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "The nonlinear transfer function between the input and the first hidden layer is a tangentsigmoid function [22], [23], [25], and a linear transfer function is used between the first and the second hidden layer and between the second hidden layer and the output.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "The nonlinear transfer function between the input and the first hidden layer is a tangentsigmoid function [22], [23], [25], and a linear transfer function is used between the first and the second hidden layer and between the second hidden layer and the output.", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "In order to avoid overfitting [22], [23], an early stopping criterion is used, each network being trained for 100 epochs.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "In order to avoid overfitting [22], [23], an early stopping criterion is used, each network being trained for 100 epochs.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "Bootstrapping [3] is used for the construction of the ensembles, and the different networks in the ensemble have different initializations.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "this reduces the number of laboratory tests and thus the cost of model building [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "7) Due to the particular nature of the calibration in chemometrics, one cannot directly apply the state-of-the-art active learning methods [15]\u2013[21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 19, "context": "7) Due to the particular nature of the calibration in chemometrics, one cannot directly apply the state-of-the-art active learning methods [15]\u2013[21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "Depending on the application, the typical costs of chemical lab tests could be in the range of 500\u20131000 USD/sample [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 22, "context": "This algorithm was tested using the \u201cTecator\u201d dataset [24].", "startOffset": 54, "endOffset": 58}], "year": 2012, "abstractText": "In chemometrics, data from infrared or near-infrared (NIR) spectroscopy are often used to identify a compound or to analyze the composition of a material. This involves the calibration of models that predict the concentration of material constituents from the measured NIR spectrum. An interesting aspect of multivariate calibration is to achieve a particular accuracy level with a minimum number of training samples, as this reduces the number of laboratory tests and thus the cost of model building. In these chemometric models, the input refers to a proper representation of the spectra and the output to the concentrations of the sample constituents. The search for a most informative new calibration sample thus has to be performed in the output space of the model, rather than in the input space as in conventional modeling problems. In this paper, we propose to solve the corresponding inversion problem by utilizing the disagreements of an ensemble of neural networks to represent the prediction error in the unexplored component space. The next calibration sample is then chosen at a composition where the individual models of the ensemble disagree most. The results obtained for a realistic chemometric calibration example show that the proposed active learning can achieve a given calibration accuracy with less training samples than random sampling.", "creator": null}}}