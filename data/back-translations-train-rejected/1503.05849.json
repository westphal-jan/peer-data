{"id": "1503.05849", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "Deep Transform: Time-Domain Audio Error Correction via Probabilistic Re-Synthesis", "abstract": "In the process of recording, storage and transmission of time-domain audio signals, errors may be introduced that are difficult to correct in an unsupervised way. Here, we train a convolutional deep neural network to re-synthesize input time-domain speech signals at its output layer. We then use this abstract transformation, which we call a deep transform (DT), to perform probabilistic re-synthesis on further speech (of the same speaker) which has been degraded. Using the convolutive DT, we demonstrate the recovery of speech audio that has been subject to extreme degradation. This approach may be useful for correction of errors in communications devices.", "histories": [["v1", "Thu, 19 Mar 2015 17:24:16 GMT  (3269kb)", "http://arxiv.org/abs/1503.05849v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["andrew j r simpson"], "accepted": false, "id": "1503.05849"}, "pdf": {"name": "1503.05849.pdf", "metadata": {"source": "CRF", "title": "Deep Transform: Time-Domain Audio Error Correction via Probabilistic Re-Synthesis", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "In fact, most of them will be able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["GE Hinton", "S Osindero", "Y Teh"], "venue": "Neural Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning deep architectures for AI\u201d, Foundations and Trends in Machine Learning 2:1\u2013127", "author": ["Y Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition", "author": ["CD Ciresan", "U Meier", "LM Gambardella", "J Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis\u201d, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Performance measurement in blind audio source separation", "author": ["E Vincent", "R Gribonval", "C F\u00e9votte"], "venue": "IEEE Trans. on Audio, Speech and Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6], [7].", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6], [7].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6], [7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6], [7].", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6], [7].", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6], [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Deep neural networks [1], [2], [3], [4], [5] (DNN) learn abstract feature representations from data [6], [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "A DNN that is trained to synthesize its inputs at its output layer is known as an autoencoder [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "We may think of the autoencoder as an abstract transformation device \u2013 a deep transform (DT) \u2013 that embodies abstract knowledge of the features of the data [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "The DT may therefore be used to transform new data through the learned abstract feature space, constituting an abstract filter [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "In a previous paper [7], it was shown that an autoencoder DNN could be used for probabilistic re-synthesis of degraded images.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "Here, we extend the same approach to the correction of errors in timedomain audio signals and we use an objective source separation metric [8] to quantify recovery from error.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "We consider the problem of speech audio that has been arbitrarily degraded by non-additive (nonlinear) errors [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "Finally, to capture the degree of recovery from error, we computed the signal-to-distortion ratio (SDR) for the recovered audio signal using the BSS-EVAL toolkit [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "This allowed us to use a signmoidal output layer that is mapped to the range [0,1].", "startOffset": 77, "endOffset": 82}, {"referenceID": 5, "context": "The network used the biased sigmoid activation function [6] throughout and bias was set to zero for the output layer.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "The degraded test audio and the error-corrected test audio were then compared with the original test audio by computing the SDR using the BSS-EVAL toolkit [8] for the whole 10second audio signal.", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "More generally, as in the previous examples [7], the outcome of the DT probabilistic re-synthesis demonstrated here resembles the perceptual error correction facilities of the brain.", "startOffset": 44, "endOffset": 47}], "year": 2015, "abstractText": "In the process of recording, storage and transmission of time-domain audio signals, errors may be introduced that are difficult to correct in an unsupervised way. Here, we train a convolutional deep neural network to resynthesize input time-domain speech signals at its output layer. We then use this abstract transformation, which we call a deep transform (DT), to perform probabilistic re-synthesis on further speech (of the same speaker) which has been degraded. Using the convolutive DT, we demonstrate the recovery of speech audio that has been subject to extreme degradation. This approach may be useful for correction of errors in communications devices.", "creator": "PDFCreator Version 1.7.1"}}}