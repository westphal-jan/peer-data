{"id": "1503.05938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "On Invariance and Selectivity in Representation Learning", "abstract": "We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory -- a recent theory of feedforward processing in sensory cortex.", "histories": [["v1", "Thu, 19 Mar 2015 20:30:46 GMT  (35kb,D)", "http://arxiv.org/abs/1503.05938v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fabio anselmi", "lorenzo rosasco", "tomaso poggio"], "accepted": false, "id": "1503.05938"}, "pdf": {"name": "1503.05938.pdf", "metadata": {"source": "CRF", "title": "On Invariance and Selectivity in Representation Learning", "authors": ["Fabio Anselmi", "Lorenzo"], "emails": [], "sections": [{"heading": null, "text": "Keywords: invariance, machine learning The paper is presented in the Information and Inference Journal."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to develop in a way that is not possible in other countries of the world. In other countries of the world, it is the case that they are able to survive themselves. In other countries of the world, it is the case that they are able to survive themselves. In other countries of the world, it is the case that they are able to survive themselves. In other countries of the world, it is the case that they are able to survive themselves. In other countries of the world, it is the case that they are able to survive themselves. In other countries of the world, it is the case that they are able to survive themselves. In other countries of the world, it is the case that they are able to survive themselves."}, {"heading": "2 Invariant and Selective Data Representations", "text": "It is about the question to what extent it concerns a way in which people are able to determine themselves. (...) It is about the question to what extent people are able to determine themselves. (...) It is about the question to what extent people are able to determine themselves. (...) It is about the question to what extent people are able to determine themselves. (...) It is about the question to what extent they are able to determine themselves. (...) It is about the question to what extent people are able to determine themselves. (...) It is about the question to what extent they are able to determine themselves. (...) It is about the question to what extent they are able to determine themselves. (...) It is about the question. (...) It is about the question. (...) It is about the question. (...) It is about the question. (...) It is about the question. (...) It is about the question. (...) It is about the question. (...) It is about how far. (...) It is about the question. (... It is about how far."}, {"heading": "I \u223c I \u2032 \u21d4 \u2203 g \u2208 G such that gI = I \u2032,", "text": "We say that one representation \u00b5 is invariant in relation to G if\u00b5 \u0445 I \u0445 (I) = \u00b5 (I \"), for all I.\" In words, the above definition states that if two data points are a transformation of the other, they will have the same representation. In fact, if one representation \u00b5 is invariant (I) = \u00b5 (gI) for all I. \"I.\" G. \"Clearly, trivial invariant representations can be defined, e.g. the constant function. This motivates a second requirement, namely selective representation. We say that a representation \u00b5 is selective in relation to G if\u00b5 (I) = invariant representations selective in relation to constant function."}, {"heading": "3 From Invariance to Low Sample Complexity", "text": "In this section, we will first recall how the concepts of data representation and hypotheses space are closely related and how the sample complexity of a monitored problem can be characterized by the coverage of the hypotheses space. Then, we will discuss how invariant representations can reduce the sample complexity of a monitored learning problems.Supervised learning amounts to finding an input-output relationship based on a training set of input-output pairs. Outputs can be evaluated scalarly or vector-wise, as in regression, or categorically, as in multi-category or multi-label classification, with binary classification being a fundamental example.The majority of statistical learning theory is devoted to study conditions under which learning problems can be solved approximately and to some degree of certainty, provided that there is a suitable hypotheses space.A hypotheses is a set of possible subspaces of Y: hypotheses."}, {"heading": "3.1 Data Representation and Hypothesis Space", "text": "In fact, practically useful hypotheses rooms are typically equipped with a Hilbert spatial structure, since most computational solutions can be developed in this environment. A further natural prerequisite is that the evaluation functions are well defined and continuous. This latter property allows to give a well-defined meaning to the evaluation of a function at any point, a property that is probably self-evident, since we are interested in predictions. The requirements for 1) to be a Hilbert functional space and 2) to have continuous evaluation functions define so-called Hilbert core spaces [24]. These functional spaces are characterized, among other things, by the existence of a functional map: I \u2192 F, which is a map from the data space into a characteristic space, which is itself a Hilbert space. Roughly speaking, functions in an RKHS H with an associated characteristic map can be regarded as hyperplanes in the attribute space, in the sense that there is such an F marking."}, {"heading": "3.2 Sample Complexity in Supervised Learning", "text": "The tested statistical learning theory characterizes the difficulty of a learning problem in relation to the \"magnitude\" of the hypotheses space under consideration, measured by suitable capacitance measurements. Specifically, for a measured loss function V: Y \u00b7 Y \u2192 [0, \u221e) for each measurable function f: I \u2192 Y the expected error is defined as E (f) = V (I), y) d\u03c1 (I, y), where \u03c1 is a probability quantity on I \u00b7 Y. For a training quantity Sn = {(I1, y1), the expected error is defined as E (n) of the input-output pairs, which are identical and independent of a hypotheses quantity H \u2212 the goal of learning is to find an approximate solution fn = fSn, H up to the definition of the problem f = H E (f) The difficulty of a learning problem is covered by the following definition."}, {"heading": "3.3 Sample Complexity of the Invariance Oracle", "text": "Consider the simple example of a series of images of p \u00b7 p pixels, each containing an object within a (square) window of k \u00b7 k pixels and surrounded by a uniform background. Let's imagine that the object positions may be located somewhere in the image. Then, it's easy to see that once objects are translated so that they do not overlap, we get an orthogonal subspace. Then, let's imagine that there are r2 = (p / k) 2 possible subspaces of dimension k2, that is, the set of translated images can be seen as a distribution of vectors supported within a sphere in d = p2 dimensions. Following the discussion in the previous section, the best algorithm will emerge based on a linear hypothesis of space that is proportional to d. Let's now assume that we have access to an oracle that can \"register\" each image so that each object occupies the centered position. In this case, the distribution will be less effective because of the complexity we need for a sample that is high and proportional to the space."}, {"heading": "4 Compact Group Invariant Representations", "text": "Consider a series of transformations G that represent a locally compact group. Remember, each locally compact group has a finite measure that is naturally associated with it, the so-called hair measurement. The main feature of the hair measurement is its invariance against the group action, and in particular for all measurable functions f: G \u2192 R and g. \"G, it is: dgf (g) = dgf (g\" g).The above equation is reminiscent of the invariance of the translation of Lebesgue integrals, and in fact the Lebesgue measurement can be called the hair measurement parameter associated with the translation group. The invariance display property of the hair measurement associated with a locally compact group is the key to our development of the invariant representation as we describe it next."}, {"heading": "4.1 Invariance via Group Averaging", "text": "In fact, it is the case that the question of whether it is a problem is a problem that it is not a problem but a problem that it is. (...) It is the case that it is a problem. (...) It is the case that it is a problem. \"(...) S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "4.2 A Probabilistic Approach to Selectivity", "text": "Recall that for each compact group the hair measurement is finite, so that if it is normalized accordingly, it corresponds to a probability measurement. Assumption 1. In the following, we assume that G is of course considered as Abel and compact, and the corresponding hair measurement as normalizable. The first step in our argumentation is the following definition. Definition 4 (representation via orbit probability). For all I-I-I, we define the random variable ZI: (G, dg) \u2192 I, ZI (g) = gI-G, with legal distribution Z \u2212 1I (A) dg, for all measurable propositions A-I. LetP: I \u2192 P (I), P (I) = I-Z I, I-Z, I-Z, I-Z. The map P associates a corresponding probability distribution to each point."}, {"heading": "4.2.1 Tomographic Probabilistic Representations", "text": "We need to introduce some notations and definitions. Let T = S, the unity sphere in I, and let P (R) denote the set of probability scales on the real line. Let T = S, the unity sphere in I, and let P (R) denote the set of probability scales on the real line. Let P (R) T = {h | h: T \u2192 P (R)} and DefinitionR: P (I) \u2192 P (R) T, R (B) d, for all measurable sets B-R. Definition 5 (Radon Embedding). Let P (R) T = {h | h: T \u2192 P (R)} and DefinitionR: P (R) \u2192 P (R), R (R) d), for all measurable sets B-R. Definition 5 (Radon Embedding). Let P (R) T = {h | h: T \u2192 P (R)} and (P) (R)}: P (P)."}, {"heading": "4.2.2 CDF Representation", "text": "A natural way to describe a one-dimensional probability distribution is to consider the associated cumulative distribution function (CDF). Remember that if: (B), (B), (R) is a random variable with the law q, (R), then the associated CDF function is given by fq (B) = q (B), (B), (B), (B), (9), where H is the heaviside step function. Also, remember that the CDF selection criteria clearly define a probability distribution, because by the fundamental theory of Calculus, we haveddb, fq (B) = ddb, (B), another selection criteria for the probability that those are a different selection criteria for the CDF selection criteria are fixed. Let us consider the following definition 7 (CDF, Vector Map). LF (R) {h:"}, {"heading": "4.3 Templates Sampling and Metric Embedings", "text": "Next, we will discuss what happens when only a finite number of (possibly random) templates are available. In this case, while invariance can be ensured, selectivity cannot generally be expected to be maintained. However, it is possible to show that representation is almost selective (see below) if a sufficiently large number of templates are available. In this sense, we are introducing a metric structure to the space of representation. Let us keep in mind that if the results of the representation \"P (R) are two probability distributions on the real line and their cumulative distribution functions, then the universal Kolmogorov-Smirnov method (KS) will be induced by the uniform norm of cumulative distributions that are on the real line and not taken into account for their cumulative distribution functions."}, {"heading": "5 Locally Invariant and Covariant Representa-", "text": "group averages, and refer to this situation as the case of the partially observable group (POG). Broadly speaking, the idea is that this type of measurement can be invariant to sufficiently small transformations, i.e. locally invariant. Furthermore, representations given by collecting POG averages can be presented as covariant (see Section 5.2 for a definition)."}, {"heading": "5.1 Partially Observable Group Averages", "text": "For a subset G0-G, we consider a POG measurement of formation (I) = > difference (I) to be sufficient (< I, gt >). (14) The quantity mentioned above can be interpreted as the \"response\" of a cell that can perceive visual stimuli within a \"window\" (receptive field) of size G0. (14) A POG measurement corresponds to a local group average limited to a subset of transformations G0. (14) Consider a POG measurement at a transformed point of size G0-g- (G) measurement at a transformed point of difference (< g) - (G) measurement at a transformed point of the visual condition G0 dg- (G) = G0 dg- 1gt Note (< I, gt >).If we compare the POG measurements at a transformed point where we compare the difference with and without transformation, we have the Gdg- (G) measurement."}, {"heading": "5.2 POG Representation", "text": "For all g & # 246; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & # 160; G & 160; G & 160; G & 160; G & 160; G & 160; G & 160; G; G & 160; G & 160; G; G & 160; G & 160; G & 160; G; G & 160; G & 160; G & 160; G; G & 160; G; G & 160; G; G; G & 160; G; G; G & 160; G; G; G & 160; G; G & 160; G; G; G & 160; G; G; G & 160; G; G; G & # 160; G; G; G & # 160; G; G; G; G & # 160; G; G; G & # 160; G; G; G & # 160; G; G & # 160; G; G & # 160; G; G & # 160; G; G & # 160; G; G & & # 160; G; G; G & # 160; G; G; G & # 160; G & & # 160; G; G; G & # 160; G; G & # 160; G; G & # 160; G & & # 160; G; G & # 160; G; G & # 160; G; G & # 160; G & # 160; G; G; G; G & # 160; G; G & # 160; G; G & # 160; G & & # 160; G; G; G & & # 160; G; G; G & # 160; G; G & # 160; G; G; G; G & & # 160; G; G & # 160; G; G; G; G; G & & # 160; G; G; G & & # 160; G; G; G & & # 160; G; G; G"}, {"heading": "6 Further Developments: Hierarchical Repre-", "text": "In this section, we will discuss some other developments in the framework presented in the previous section. In particular, we will outline how to obtain multi-layered (deep) representations in the brain (11, 26). (D) The basic idea for building an invariant / selective representation is to consider local (or global) measurements of the form G0. (< I, gt >) dg, (21) with G0. (G) The main difficulty in iterating this idea is that the representation (11) - (20) generated by capturing (local) groups."}, {"heading": "7 Discussion", "text": "This year, the time has come for an agreement to be reached in just a few days."}, {"heading": "Acknowledgment", "text": "We thank the McGovern Institute for Brain Research for their support. This research was supported by grants from the National Science Foundation, AFSOR-THRL (FA8650-05-C-7262). Additional support came from the Eugene McDermott Foundation."}, {"heading": "A Representation Via Moments", "text": "In Section 4.2.2 we discussed the derivation of invariant selective representation taking into account CDFs of suitable one-dimensional probability distributions. As we noted in Remark 4, alternative representations are possible, for example by taking into account moments. Here, we discuss this point of view in somewhat more detail. Remember that if we have the following definitions and results, the associated moment vector is given, then the associated moment vector of mrq = E | B = D = D = R, N (24) In this case, we have the following definitions and results. Definition 12 (Moments Vector Map). Leave M (R) = {h | h: N \u2192 R) T = {h | h: T \u2192 M (R)}. Definition M: M (R) T (R) T (R) T (R)."}, {"heading": "B Kernels on probability distributions", "text": "To consider multiple layers within the framework proposed in the work, we must embed probability spaces in Hilbert spaces = > HS interiors. A natural way to do this is to consider suitable positive defined (PD) nuclei, i.e. symmetrical functions K: X \u00b7 X \u2192 R those related to i, j = 1 K (\u03c1i, \u03c1j) \u03b1i\u03b1j \u2265 0for all,.... n \u00b2 X, \u03b11,.. in the sense that HK is any quantity, e.g. X = R or X = P (R). In fact, it is known that PD nuclei define a unique reproducing Hilbert space (RKHS) for which they correspond to the reproduction of nuclei, in the sense that HK is the property defined by K, then Kx = K (x, \u00b7 HK for all x and < f, Kx > K = f (x), x)."}], "references": [{"title": "The classical moment problem: and some related questions in analysis, University mathematical monographs", "author": ["N. Akhiezer"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1965}, {"title": "Efficient Sketches for Earth-Mover Distance, with Applications", "author": ["A. Andoni", "K.D. Ba", "P. Indyk", "D.P. Woodruff"], "venue": "in FOCS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Unsupervised Learning of Invariant Representations in Hierarchical Architectures. arXiv preprint 1311.4158", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Representation Learning in Sensory Cortex: a theory. CBMM memo n 26", "author": ["F. Anselmi", "T. Poggio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Magic Materials: a theory of deep hierarchical architectures for learning sensory representations", "author": ["T.T.A. P"], "venue": "CBCL paper", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Some theorems on distribution functions", "author": ["H. Cramer", "H. Wold"], "venue": "J. London Math. Soc.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1936}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Computational Topology, An Introduction", "author": ["H. Edelsbrunner", "J.L. Harer"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1980}, {"title": "Invariant Kernel Functions for Pattern Analysis and Machine Learning", "author": ["B. Haasdonk", "H. Burkhardt"], "venue": "Mach. Learn.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Hilbertian Metrics and Positive Definite Kernels on Probability Measures", "author": ["M. Hein", "O. Bousquet"], "venue": "AISTATS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D. Hubel", "T. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1962}, {"title": "Receptive fields and functional architecture in two nonstriate visual areas (18 and 19) of the cat", "author": ["D. Hubel", "T. Wiesel"], "venue": "Journal of Neurophysiology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1965}, {"title": "Receptive fields and functional architecture of monkey striate cortex", "author": ["D. Hubel", "T. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1968}, {"title": "Support Theorems for the Radon Transform and Cramr-Wold Theorems", "author": ["F.L. Jan Boman"], "venue": "Journal of Theoretical Probability,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["Johnson", "J.W.B. Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1984}, {"title": "Rotation Invariant Spherical Harmonic Representation of 3D Shape Descriptors", "author": ["M. Kazhdan", "T. Funkhouser", "S. Rusinkiewicz"], "venue": "Proceedings of the 2003 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "The invariance hypothesis implies domain-specific regions in visual cortex. http://dx.doi.org/10.1101/004473", "author": ["J.Z. Leibo", "Q. Liao", "F. Anselmi", "T. Poggio"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Group Invariant Scattering", "author": ["S. Mallat"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Does invariant recognition predict tuning of neurons in sensory cortex?. MIT-CSAIL-TR-2013-019, CBCL-313", "author": ["T. Poggio", "J. Mutch", "F. Anselmi", "A. Tacchetti", "L. Rosasco", "J.Z. Leibo"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "On the theory of reproducing kernel hilbert spaces", "author": ["A.G. Ramm"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Methods of modern mathematical physics. II. , Fourier Analysis, Self-Adjointness", "author": ["M. Reed", "B. Simon"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1978}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nature Neuroscience,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "in International Conference on Learning Representations (ICLR2014). CBLS", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Actionable information in vision", "author": ["S. Soatto"], "venue": "Computer Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Dictionary learning: What is the right representation for my signal", "author": ["I. Tosic", "P. Frossard"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Estimation of dependencies based on empirical data", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1982}, {"title": "Efficient Additive Kernels via Explicit Feature Maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Pattern Analysis and Machine Intellingence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "[3, 4, 5].", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[3, 4, 5].", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[3, 4, 5].", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "Our work is motivated by a theory of cortex and in particular visual cortex [5].", "startOffset": 76, "endOffset": 79}, {"referenceID": 28, "context": "Examples in this class of methods include so called dictionary learning [30], autoencoders [6] and metric learning techniques (see e.", "startOffset": 72, "endOffset": 76}, {"referenceID": 5, "context": "Examples in this class of methods include so called dictionary learning [30], autoencoders [6] and metric learning techniques (see e.", "startOffset": 91, "endOffset": 94}, {"referenceID": 18, "context": "For example in the analysis of shapes [19] and more generally in computational topology [10], or in the design of positive definite functions associated to reproducing kernel Hilbert spaces [12].", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "For example in the analysis of shapes [19] and more generally in computational topology [10], or in the design of positive definite functions associated to reproducing kernel Hilbert spaces [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "For example in the analysis of shapes [19] and more generally in computational topology [10], or in the design of positive definite functions associated to reproducing kernel Hilbert spaces [12].", "startOffset": 190, "endOffset": 194}, {"referenceID": 21, "context": "The ideas in [22, 28] are close in spirit to the study in this paper.", "startOffset": 13, "endOffset": 21}, {"referenceID": 27, "context": "The ideas in [22, 28] are close in spirit to the study in this paper.", "startOffset": 13, "endOffset": 21}, {"referenceID": 21, "context": "In particular, the results in [22] develop a different invariant and stable representation within a signal processing framework.", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "In [28] an information theoretic perspective is considered to formalize the problem of learning invariant/selective representations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In this work we develop a machine learning perspective closely following computational neuroscience models of the information processing in the visual cortex [15, 16, 26].", "startOffset": 158, "endOffset": 170}, {"referenceID": 15, "context": "In this work we develop a machine learning perspective closely following computational neuroscience models of the information processing in the visual cortex [15, 16, 26].", "startOffset": 158, "endOffset": 170}, {"referenceID": 25, "context": "In this work we develop a machine learning perspective closely following computational neuroscience models of the information processing in the visual cortex [15, 16, 26].", "startOffset": 158, "endOffset": 170}, {"referenceID": 24, "context": "Throughout the article we assume the group representation to be unitary [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "noise (see [22]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "The requirements of 1) being a Hilbert space of of functions and 2) have continuous evaluation functionals, define so called reproducing kernel Hilbert spaces [24].", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "A hypothesis space H is said to be learnable if, for all \u2208 [0,\u221e), \u03b4 \u2208 [0, 1], there exists n( , \u03b4,H) \u2208 N such that inf fn sup \u03c1 P ( E(fn)\u2212 inf f\u2208H E(f) \u2265 ) \u2264 \u03b4.", "startOffset": 70, "endOffset": 76}, {"referenceID": 29, "context": "The sample complexity can be shown [31, 9] to be proportional to the logarithm of the covering number, i.", "startOffset": 35, "endOffset": 42}, {"referenceID": 8, "context": "The sample complexity can be shown [31, 9] to be proportional to the logarithm of the covering number, i.", "startOffset": 35, "endOffset": 42}, {"referenceID": 3, "context": "As discussed in [4], the main motivation for considering measurements of the above form is their interpretation in terms of biological or artificial neural networks, see the following remarks.", "startOffset": 16, "endOffset": 19}, {"referenceID": 13, "context": "Remark 2 (Hubel and Wiesel Simple and Complex Cells [14]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Remark 3 (Convolutional Neural Networks [20]).", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "Interestingly, R can be shown to be a generalization of the Radon Transform to probability distributions [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "Theorem 4 (Cramer-Wold [8]).", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "Let F(R) = {h | h : R\u2192 [0, 1]}, and F(R)T = {h | h : T \u2192 F(R)}.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "and takes values in [0, 1].", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "Define the real random variable Z : S \u2192 [0, 1], Z(ti) = d\u221e(\u03bc i(I), \u03bci(I \u2032)), i = 1, .", "startOffset": 40, "endOffset": 46}, {"referenceID": 17, "context": "Finally, we note that, when compared to classical results on distance preserving embedding, such as Johnson Linderstrauss Lemma [18], Theorem 12 only ensures distance preservation up to a given accuracy which increases with a larger number of projections.", "startOffset": 128, "endOffset": 132}, {"referenceID": 1, "context": "This is hardly surprising, since the problem of finding suitable embedding for probability spaces is known to be considerably harder than the analogue problem for vector spaces [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 2, "context": "Regarding the localization condition discussed above, as we comment elsewhere [3], the fact that a template needs to be localized could have implications from a biological modeling standpoint.", "startOffset": 78, "endOffset": 81}, {"referenceID": 22, "context": "More precisely, it could provides a theoretical foundation of the Gabor like shape of the responses observed in V1 cells in the visual cortex [23, 3, 5].", "startOffset": 142, "endOffset": 152}, {"referenceID": 2, "context": "More precisely, it could provides a theoretical foundation of the Gabor like shape of the responses observed in V1 cells in the visual cortex [23, 3, 5].", "startOffset": 142, "endOffset": 152}, {"referenceID": 4, "context": "More precisely, it could provides a theoretical foundation of the Gabor like shape of the responses observed in V1 cells in the visual cortex [23, 3, 5].", "startOffset": 142, "endOffset": 152}, {"referenceID": 10, "context": "Hierarchical representations, based on multiple layers of computations, have naturally arisen from models of information processing in the brain [11, 26].", "startOffset": 145, "endOffset": 153}, {"referenceID": 25, "context": "Hierarchical representations, based on multiple layers of computations, have naturally arisen from models of information processing in the brain [11, 26].", "startOffset": 145, "endOffset": 153}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 3, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 20, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 22, "context": "Preliminary answers to these questions are described in [3, 4, 21, 23].", "startOffset": 56, "endOffset": 70}, {"referenceID": 2, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 3, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 20, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 22, "context": "Several extensions of the theory are natural and have been sketched with preliminary results in [3, 4, 21, 23].", "startOffset": 96, "endOffset": 110}, {"referenceID": 25, "context": "Feedforward architecture with n layers, consisting of dot products and nonlinear pooling functions, are quite general computing devices, basically equivalent to Turing machines running for n time points (for example the layers of the HMAX architecture in [26] can be described as AND operations (dot products) followed by OR operations (pooling), i.", "startOffset": 255, "endOffset": 259}, {"referenceID": 2, "context": "Exact invariance for each module is equivalent to a localization condition which could be interpreted as a form of sparsity [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 20, "context": "This property, which is similar to compressive sensing \u201cincoherence\u201d (but in a group context), requires that I and t have a representation with rather sharply peaked autocorrelation (and correlation) and guarantees approximate invariance for transformations which do not have group structure, see [21].", "startOffset": 297, "endOffset": 301}, {"referenceID": 2, "context": "A sufficient number of different nonlinearities, each corresponding to a complex cell, can provide selectivity [3].", "startOffset": 111, "endOffset": 114}], "year": 2015, "abstractText": "We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory \u2013 a recent theory of feedforward processing in sensory cortex. [3, 4, 5].", "creator": "LaTeX with hyperref package"}}}