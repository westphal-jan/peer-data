{"id": "1503.05951", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2015", "title": "Rank Subspace Learning for Compact Hash Codes", "abstract": "The era of Big Data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search. Most existing work learn hash functions that are numeric quantizations of feature values in projected feature space. In this work, we propose a novel hash learning framework that encodes feature's rank orders instead of numeric values in a number of optimal low-dimensional ranking subspaces. We formulate the ranking subspace learning problem as the optimization of a piece-wise linear convex-concave function and present two versions of our algorithm: one with independent optimization of each hash bit and the other exploiting a sequential learning framework. Our work is a generalization of the Winner-Take-All (WTA) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length. We compare with several state-of-the-art hashing algorithms in both supervised and unsupervised domain, showing superior performance in a number of data sets.", "histories": [["v1", "Thu, 19 Mar 2015 21:34:33 GMT  (685kb,D)", "http://arxiv.org/abs/1503.05951v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["kai li", "guojun qi", "jun ye", "kien a hua"], "accepted": false, "id": "1503.05951"}, "pdf": {"name": "1503.05951.pdf", "metadata": {"source": "CRF", "title": "Rank Subspace Learning for Compact Hash Codes", "authors": ["Kai Li", "Guojun Qi", "Jun Ye", "Kien A. Hua"], "emails": ["KAILI@EECS.UCF.EDU", "GUOJUN.QI@UCF.EDU", "JYE@EECS.UCF.EDU", "KIENHUA@EECS.UCF.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, we have reached the point where we feel we are able to live in a country where most people are able to move and where they are able to flourish."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to move into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "3. Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Winner-Take-All Hashing", "text": "WTA hashing is a subfamily of hash functions introduced by (Yagnik et al., 2011). WTA is specified by two parameters: the number of random permutations L and the window size K. Each permutation \u03c0 arranges the entries of an input vector x-Rd to x\u03c0 in the order specified by \u03c0. Then, the index of the maximum dimension of the attribute under the first K elements of x\u03c0 is used as a hash code. This process repeats itself L times, resulting in an aK-nary hash code of length L that is compact by L \u00d7 dlog2Ke-Bits.WTA is considered a rank-based hash algorithm that uses the order between permutated entries of a vector and not their values of characteristics. This property has given WTA a certain degree of stability to represent errors in numerical values compactly."}, {"heading": "3.2. Rank Subspace Hashing", "text": "Instead of randomly encoding the input data vector x, we project it onto a series of K one-dimensional subspaces. Then, the input vector is encoded by the index of the subspace that generates the largest projected value. In other words, we have (x; W) = arg max 1 \u2264 KwTk x, (1) where wk \u00b2 Rd, 1 \u2264 k \u2264 K are vectors that indicate the subspace projections, and W = [w1, w2, wK] T. We use a linear projection to map an input vector into subspaces to form its hash code. At first glance, this idea resembles the family of learning-based hash algorithms based on linear projection (Datar et al, 2004; Norouzi & Fleet, 2011). However, the proposed method differs from these existing algorithms by encoding each input column with the index hasar dimension."}, {"heading": "3.3. Reformulation", "text": "The above objective function is simple to formulate but difficult to optimize, since it includes the indicator function and the Arg-Max function, which are typically nonconvex and highly discontinuous. Motivated by (Norouzi & Fleet, 2011), we reformulate the objective function and seek a piecemeal linear upper limit of E (W). First, the hash function can be reformulated in (1) equivalent terms: ash (x; W) = arg max ggTWx, subject to g {0, 1} K, 1Tg = 1, (4) which have a 1-of-K binary code h for an input vector hx. The limitation there must be and there must be only an unequal input of 1 in the resulting hash code. We force this limitation in the following optimization problems, without explicitly meaning it, to avoid notation errors."}, {"heading": "3.4. Optimization", "text": "Consider W. The first step is a discrete optimization problem that is guaranteed to have a global optimal solution. Specifically, the values of Wxi (j) reflect the RSH codes hi (j) in the second and third terms of (5) in O (K). For the corrected error e (gi, gj, sij) + gTi Wxi + g T j Wxj of the first term in square brackets, it is not difficult to derive its maximum value by scanning the elements in the matrix [mkl] K \u00d7 K, defined asmkl = {y (k) i + y (l) j (p) j + p) j (1 \u2212 sij), if k = ly (k) i + j (i) sij otherwise (5), where y (k) i is the k th element of Wxi."}, {"heading": "4. The Sequential Learning", "text": "To maximize the information contained in an L-bit hash code, we suggest learning the hash functions sequentially so that each hash function provides complementary information to previous ones. To motivate our sequential learning algorithms, we can consider each hash bit as a week classifier that assigns similarity markers to an input pair, and the ensemble classifier obtained is related to the hamming distance between the hash codes. Formally, each week classifier is measured according to the lth bit issiml (xi, xj) = 1 \u2212 Hm (xi; Wl), h (xj), h (8) Where Hm (x, y) = I) is the bit-by-bit distance between the two hamming codes."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Dataset and Compared Methods", "text": "To evaluate the proposed hashing approaches, Rank Subspace Hashing (RSH) and Sequential Rank Space Hashing (SRSH), we use three well-known datasets: LabelMe and Peekaboom, two collections of images designed as 512D ghost vectors for object recognition tasks; and MNIST, a corpus of handwritten digits in 24 x 24 grayscale. The above datasets were compiled by (Kulis & Darrell, 2009) and used in (Norouzi & Fleet, 2011). Following the settings of (Norouzi & Fleet, 2011), we randomly selected 1000 points for training and a separate set of 3000 points as test queries. Basic truth neighbors for test queries are defined by thresholds for Euclidean distance so that each query point has an average of 50 points."}, {"heading": "5.2. Methodology", "text": "When evaluating Approximate Proximity (ANN) searches, two methods are commonly used in the literature, i.e., hash table based search and hamming distance based kNN search. We use both methods in our evaluation. In hash table loss, the hash code is used to index all points in a database, and the data points with the same hash key fall into the same bucket. Typically, hash buckets that fall within a hamming ball of radius R (i.e. the hash code differs only by 2 or 3 bits) of the target query are considered relevant query results. A big advantage of hash table searches is that they can be performed in constant time. In contrast, hamming distance based kNN search performs a standard kNN search method based on hamming distance, which includes a linear scan of the entire database. However, since hamming distance can be efficiently calculated, the Nkask search is also evaluated very quickly in the hamming space = our neighbor."}, {"heading": "5.3. Results", "text": "In this context, it should be noted that this is not a purely formal matter, but a purely formal matter, which is a purely formal matter."}, {"heading": "6. Conclusion", "text": "Based on this formulation, we propose a novel hash learning goal, which aims to optimize a number of low-dimensional linear subspaces for high-quality rank-based hash coding. Subsequently, a simple but effective learning algorithm is provided to optimize objective function, resulting in a number of optimal rank-based subspaces. The effectiveness of the proposed learning method in overcoming the limitations of WTA is confirmed in a series of experiments, and we embed our learning method in a sequential learning framework that further enhances the performance of the basic learning algorithm. Extensive experiments on several known data sets demonstrated our superior performance over the state of the art."}], "references": [{"title": "Learning to hash: forgiving hash functions and applications", "author": ["Baluja", "Shumeet", "Covell", "Michele"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Baluja et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Baluja et al\\.", "year": 2008}, {"title": "Survey of hashing techniques for compact bit representations of images", "author": ["Bondugula", "Sravanthi"], "venue": "Ph.D. dissertation,,", "citeRegEx": "Bondugula and Sravanthi.,? \\Q2013\\E", "shortCiteRegEx": "Bondugula and Sravanthi.", "year": 2013}, {"title": "Min-wise independent permutations", "author": ["Broder", "Andrei Z", "Charikar", "Moses", "Frieze", "Alan M", "Mitzenmacher", "Michael"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Broder et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Broder et al\\.", "year": 2000}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Datar", "Mayur", "Immorlica", "Nicole", "Indyk", "Piotr", "Mirrokni", "Vahab S"], "venue": "In Proceedings of the twentieth annual symposium on Computational geometry,", "citeRegEx": "Datar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Datar et al\\.", "year": 2004}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["Dean", "Thomas", "Ruzon", "Mark A", "Segal", "Mark", "Shlens", "Jonathon", "Vijayanarasimhan", "Sudheendra", "Yagnik", "Jay"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dean et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2013}, {"title": "Supervised binary hash code learning with jensen shannon divergence", "author": ["Fan", "Lixin"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Fan and Lixin.,? \\Q2013\\E", "shortCiteRegEx": "Fan and Lixin.", "year": 2013}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Gong", "Yunchao", "Lazebnik", "Svetlana"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Gong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2011}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Kulis", "Brian", "Darrell", "Trevor"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kulis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2009}, {"title": "What\u2019s making that sound", "author": ["Li", "Kai", "Ye", "Jun", "Hua", "Kien A"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Learning hash functions using column generation", "author": ["Li", "Xi", "Lin", "Guosheng", "Shen", "Chunhua", "Hengel", "Anton van den", "Dick", "Anthony"], "venue": "arXiv preprint arXiv:1303.0339,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Hashing with graphs", "author": ["Liu", "Wei", "Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["Liu", "Wei", "Wang", "Jun", "Ji", "Rongrong", "Jiang", "Yu-Gang", "Chang", "Shih-Fu"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Direct loss minimization for structured prediction", "author": ["McAllester", "David", "Hazan", "Tamir", "Keshet", "Joseph"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "McAllester et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2010}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Norouzi", "Mohammad", "Fleet", "David J"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Norouzi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["Norouzi", "Mohammad", "Fleet", "David J", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Fast keypoint recognition in ten lines of code", "author": ["Ozuysal", "Mustafa", "Fua", "Pascal", "Lepetit", "Vincent"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ozuysal et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ozuysal et al\\.", "year": 2007}, {"title": "Robust real-time pattern matching using bayesian sequential hypothesis testing", "author": ["Pele", "Ofir", "Werman", "Michael"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Pele et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pele et al\\.", "year": 2008}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["Shakhnarovich", "Gregory", "Viola", "Paul", "Darrell", "Trevor"], "venue": "In Computer Vision,", "citeRegEx": "Shakhnarovich et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2003}, {"title": "Small codes and large image databases for recognition", "author": ["Torralba", "Antonio", "Fergus", "Robert", "Weiss", "Yair"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Approximate nearest neighbor search through comparisons", "author": ["Tschopp", "Dominique", "Diggavi", "Suhas"], "venue": "arXiv preprint arXiv:0909.2194,", "citeRegEx": "Tschopp et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tschopp et al\\.", "year": 2009}, {"title": "Hashing for similarity search: A survey", "author": ["Wang", "Jingdong", "Shen", "Heng Tao", "Song", "Jingkuan", "Ji", "Jianqiu"], "venue": "CoRR, abs/1408.2927,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Semisupervised hashing for large-scale search", "author": ["Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Spectral hashing", "author": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Rob"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}, {"title": "The power of comparative reasoning", "author": ["Yagnik", "Jay", "Strelow", "Dennis", "Ross", "David A", "Lin", "Ruei-sung"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Yagnik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yagnik et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Hashing is recognized by many researchers as a promising solution to the above Big Data problem, thus attracting significant amount of research in the past few years (Datar et al., 2004; Kulis & Darrell, 2009; Tschopp & Diggavi, 2009; Weiss et al., 2009).", "startOffset": 166, "endOffset": 254}, {"referenceID": 22, "context": "Hashing is recognized by many researchers as a promising solution to the above Big Data problem, thus attracting significant amount of research in the past few years (Datar et al., 2004; Kulis & Darrell, 2009; Tschopp & Diggavi, 2009; Weiss et al., 2009).", "startOffset": 166, "endOffset": 254}, {"referenceID": 10, "context": "Most hashing algorithms encode high-dimensional data into binary codes by quantizing numeric projections (Norouzi & Fleet, 2011; Liu et al., 2011; 2014; 2012).", "startOffset": 105, "endOffset": 158}, {"referenceID": 23, "context": "Ranking-based hashing, such as Winner-Take-All (WTA) (Yagnik et al., 2011) and Min-wise Hashing (MinHash) (Broder et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": ", 2011) and Min-wise Hashing (MinHash) (Broder et al., 2000), ranks the random permutation of input features and uses the index of maximal/minimal feature dimensions to encode a compact representation of the input features.", "startOffset": 39, "endOffset": 60}, {"referenceID": 18, "context": "The benefit of ranking-based hashing lies in the fact that these algorithms are insensitive to the magnitude of features, and thus are more robust against many types of random noises universal in real applications ranging from information retrieval (Salakhutdinov & Hinton, 2007), image classification (Fan, 2013) to object recognition (Torralba et al., 2008).", "startOffset": 336, "endOffset": 359}, {"referenceID": 8, "context": ", revealing the multi-modal correlations (Li et al., 2014).", "startOffset": 41, "endOffset": 58}, {"referenceID": 4, "context": "Although WTA has generated leading performances in many tasks (Dean et al., 2013; Yagnik et al., 2011; Li et al., 2014), it is constrained in the sense that it only ranks the existing features of input data, while incapable of combining multiple features to generate new feature subspaces to rank.", "startOffset": 62, "endOffset": 119}, {"referenceID": 23, "context": "Although WTA has generated leading performances in many tasks (Dean et al., 2013; Yagnik et al., 2011; Li et al., 2014), it is constrained in the sense that it only ranks the existing features of input data, while incapable of combining multiple features to generate new feature subspaces to rank.", "startOffset": 62, "endOffset": 119}, {"referenceID": 8, "context": "Although WTA has generated leading performances in many tasks (Dean et al., 2013; Yagnik et al., 2011; Li et al., 2014), it is constrained in the sense that it only ranks the existing features of input data, while incapable of combining multiple features to generate new feature subspaces to rank.", "startOffset": 62, "endOffset": 119}, {"referenceID": 23, "context": "A direct consequence of such limitation is that this sort of ranking-based hashing usually needs a very large number of permutations and rankings to generate useful codes, especially with a high dimensional input feature space (Yagnik et al., 2011).", "startOffset": 227, "endOffset": 248}, {"referenceID": 20, "context": "Interested reader can refer to (Bondugula, 2013) and (Wang et al., 2014) for a comprehensive review of this research area.", "startOffset": 53, "endOffset": 72}, {"referenceID": 22, "context": "The most representative works on unsupervised hashing includes Spectral Hashing (Weiss et al., 2009) and kernelized variant Locality Sensitive Hashing (Kulis & Grauman, 2009).", "startOffset": 80, "endOffset": 100}, {"referenceID": 21, "context": "(Wang et al., 2012) presents a sequential projection learning method that fits the eigenvector solution into a boosting framework and uses pseudo labels in learning each hash bit.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "More recently, Anchor Graph Hashing (AGH) (Liu et al., 2011) and Discrete Graph Hashing (DGH) (Liu et al.", "startOffset": 42, "endOffset": 60}, {"referenceID": 18, "context": "Similar deep learning hash methods have also been applied to the task of image retrieval in very large databases (Torralba et al., 2008).", "startOffset": 113, "endOffset": 136}, {"referenceID": 14, "context": "This method is further extend to minimize loss functions defined with triplet similarity comparisons (Norouzi et al., 2012).", "startOffset": 101, "endOffset": 123}, {"referenceID": 9, "context": "Similarly, (Li et al., 2013) also learns hash functions based on triplet similarity.", "startOffset": 11, "endOffset": 28}, {"referenceID": 11, "context": "the data to compact hash codes by minimizing Hamming distances of similar pairs and maximizing that of dissimilar pairs simultaneously (Liu et al., 2012).", "startOffset": 135, "endOffset": 153}, {"referenceID": 17, "context": "The sequential part of our algorithm is similar to Boosting Similarity Sensitive Coding (BSSC) (Shakhnarovich et al., 2003) and Forgiving Hash (FH) (Baluja & Covell, 2008), both of which treat each hash bit as a week classifier and learn a series of hash functions in a AdaBoost framework.", "startOffset": 95, "endOffset": 123}, {"referenceID": 15, "context": "(Tschopp & Diggavi, 2009), (Pele & Werman, 2008) and (Ozuysal et al., 2007)) are mostly restricted to approximating nearest neighbors given a distance metric to speed up large scale lookup.", "startOffset": 53, "endOffset": 75}, {"referenceID": 23, "context": "The WTA hashing is a subfamily of hashing functions introduced by (Yagnik et al., 2011).", "startOffset": 66, "endOffset": 87}, {"referenceID": 3, "context": "At first glance, this idea is similar to the family of learning-based hashing algorithms based on linear projection (Datar et al., 2004; Norouzi & Fleet, 2011).", "startOffset": 116, "endOffset": 159}, {"referenceID": 12, "context": "of the learning algorithm has been explored and empirically studied in (McAllester et al., 2010; Norouzi & Fleet, 2011).", "startOffset": 71, "endOffset": 119}, {"referenceID": 22, "context": "For comparison, we choose several state-of-the-art methods: Minimal Loss Hashing (MLH (Norouzi & Fleet, 2011)), Spectral Hashing (SH (Weiss et al., 2009)), Locality Sensitive Hashing (LSH (Datar et al.", "startOffset": 133, "endOffset": 153}, {"referenceID": 3, "context": ", 2009)), Locality Sensitive Hashing (LSH (Datar et al., 2004)), and Winner-Take-All (WTA (Yagnik et al.", "startOffset": 42, "endOffset": 62}, {"referenceID": 23, "context": ", 2004)), and Winner-Take-All (WTA (Yagnik et al., 2011)) hash.", "startOffset": 35, "endOffset": 56}, {"referenceID": 23, "context": "For WTA, we use the polynomial kernel extension and set window size K = 4 and polynomial degree p = 4, as suggested by (Yagnik et al., 2011).", "startOffset": 119, "endOffset": 140}, {"referenceID": 21, "context": "This can be explained by fact that unsupervised learning methods tend to overfit more easily with longer codes, which is consistent with the observation by (Wang et al., 2012).", "startOffset": 156, "endOffset": 175}, {"referenceID": 23, "context": "For example, although both WTA and LSH are based on data-agnostic random methods, WTA clearly outperforms LSH for most of the tests, which is similar to the results obtained in ((Yagnik et al., 2011)).", "startOffset": 178, "endOffset": 199}], "year": 2015, "abstractText": "The era of Big Data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search. Most existing work learn hash functions that are numeric quantizations of feature values in projected feature space. In this work, we propose a novel hash learning framework that encodes feature\u2019s rank orders instead of numeric values in a number of optimal low-dimensional ranking subspaces. We formulate the ranking subspace learning problem as the optimization of a piecewise linear convex-concave function and present two versions of our algorithm: one with independent optimization of each hash bit and the other exploiting a sequential learning framework. Our work is a generalization of the Winner-TakeAll (WTA) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length. We compare with several state-of-the-art hashing algorithms in both supervised and unsupervised domain, showing superior performance in a number of data sets.", "creator": "LaTeX with hyperref package"}}}