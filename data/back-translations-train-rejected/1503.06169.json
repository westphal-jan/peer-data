{"id": "1503.06169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2015", "title": "Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies", "abstract": "In this paper, we investigate a largely extended version of classical MAB problem, called networked combinatorial bandit problems. In particular, we consider the setting of a decision maker over a networked bandits as follows: each time a combinatorial strategy, e.g., a group of arms, is chosen, and the decision maker receives a reward resulting from her strategy and also receives a side bonus resulting from that strategy for each arm's neighbor. This is motivated by many real applications such as on-line social networks where friends can provide their feedback on shared content, therefore if we promote a product to a user, we can also collect feedback from her friends on that product. To this end, we consider two types of side bonus in this study: side observation and side reward. Upon the number of arms pulled at each time slot, we study two cases: single-play and combinatorial-play. Consequently, this leaves us four scenarios to investigate in the presence of side bonus: Single-play with Side Observation, Combinatorial-play with Side Observation, Single-play with Side Reward, and Combinatorial-play with Side Reward. For each case, we present and analyze a series of \\emph{zero regret} polices where the expect of regret over time approaches zero as time goes to infinity. Extensive simulations validate the effectiveness of our results.", "histories": [["v1", "Fri, 20 Mar 2015 17:21:12 GMT  (2194kb)", "http://arxiv.org/abs/1503.06169v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shaojie tang", "yaqin zhou"], "accepted": false, "id": "1503.06169"}, "pdf": {"name": "1503.06169.pdf", "metadata": {"source": "CRF", "title": "Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies", "authors": ["Shaojie Tang", "Yaqin Zhou"], "emails": [], "sections": [{"heading": null, "text": "This year, it has come to the point where you are able to live in a country where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, where you are able to govern a country, and where you are able to govern a country, where you are able to govern such a country, and where you are able to govern a country where you are able to govern it."}, {"heading": "II. MODELS AND PROBLEM FORMULATION", "text": "We are dealing, therefore, with a very complex system, in which it is a question of the extent to which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system in which it is a system, in which it is a system, in which it is a system, in which is a system, in which is a system, in which is a system, in which is a system, in which is a system, in which is a system, in which it is a system in which is a system, in which it is a system in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system, in which it is a system in which a system, in which it is a system, a system, a system, in which is a system in which is a system in which is a system, in which is a system, in which is a system, in which is a system in which is a system in which is a system in which is a system"}, {"heading": "III. SINGLE-PLAY WITH SIDE OBSERVATION", "text": "In this case, the decision-maker learns to select an arm (or strategy) with maximum reward while looking at the lateral information of his neighbors in relation to graphs. Our proposed policy, which is the first distribution threshold of regret for SSO, is presented in algorithm 1. As shown in line 2-5, the decision-maker can explore more information about the side of his neighbors, i.e., number of observations up to the current time and time-averaged reward. The key idea behind the algorithm is that lateral observation potentially reduces regret as the decision-maker can explore more without pain, i.e. gain more information about the history to analyze the benefits of lateral observation, we use the technique of graphic partition and clique coverage. The basic idea in the default regret analysis with respect to lateral cases is the use of clique covers."}, {"heading": "IV. COMBINATORIAL-PLAY WITH SIDE OBSERVATION", "text": "In this case, the key question is how we solve the problem of lateral observation of SSO.The construction process for the strategy relation is as follows: \"The strategy relation SG (F, L) for strategies in F, where F is vertex set, and L is edge set. Each strategy sx is followed by a vertex, and a link l = (sx, sy) in L connects two different vertexes sx and vice versa."}, {"heading": "V. SINGLE-PLAY WITH SIDE REWARDS", "text": "Although the single game MAB with lateral reward has the same observation as the single game MAB with lateral observation, the distinction to the reward function will make the problem different. In the case of SSR the reward function is lateral reward of the selected arm It, instead of its direct reward. Here we treat the lateral reward of each arm as a new unknown random variable, i.e. we must learn Bi, t, which is a combination of all direct rewards on the side in Ni. As direct rewards of the arms in Ni are observed asynchronously, we can update the observation on Bi, t as the way in SSO in which observation is symmetrical between two adjacent nodes. The trick is to update the number of observation on Bi, t only if direct rewards of all arms in Ni are observed asynchronously. We use gorithObi, t to distinguish this quantity from Oi, t which is the number of direct rewards observed."}, {"heading": "VI. COMBINATORIAL-PLAY WITH SIDE REWARDS", "text": "s remember that in this scenario it is necessary to select a comarm-sx with maximum side reward, with the side reward being the sum of the observed rewards of all weapons in sx. The case is more complicated than the previous three cases because: 1) Asymmetrical observations on side reward for adjacent nodes in a clique; 2) Probably exponential number of strategies created arbitrary constraints. Therefore, it is complicated to analyze regret when we apply the same techniques of combining games with side observation. Instead of learning the side reward of strategies directly, we learn the direct reward of weapons from which com-Arms.Algorithm 4 Distribution-free learning for side reward combination game (DFL-CSR) 1: For each time slot t = 0, 1, 1, Kots, \u2212 nots."}, {"heading": "VII. SIMULATION", "text": "In this section, we evaluate the performance of the proposed 4 algorithms in simulations. We mainly analyze the regret generated by each algorithm after a long period of time, n = 10000.We first evaluate the regret generated by DFL-SSO and compare it with the MOSS learning policy. The experiment is composed as follows: We randomly generate a relationship graph with 100 arms, each following an i.i.d random process over time with an average of [0, 1]. We then plot the accumulated regret and the expected regret over time, as shown in Fig. 3 (a). Although the expected regret converts over time from MOSS to a value of 0, which corresponds to its theoretical limit in Fig. 3 (a), it shows that its accumulated regret and expected regret grow over time, as shown in Fig. 3 (a)."}, {"heading": "VIII. RELATED WORKS", "text": "The classical, multi-armed bandit problem is not based on the existence of a side bonus. Recently [?] and [?] consider the problem of the networked bandit game in the presence of side observations. They examine individual cases and propose several strategies whose regret depends on the \"side bonus,\" e.g. an arbitrarily small \"min\" will invalidate the zero remorse result. In this work, we present the first non-distributional policy for single games with side observation cases. \"For the variant with combinatorial game without side bonus,\" we consider \"combinatorial\" Anantharam et al. [?] first the problem that exactly N-weapons are selected simultaneously without limitations among weapons. Gai et al. we recently extend this version to a more general problem with arbitrary limitations [?]. The model is also loosened to a linear combination of no more than N-weapons, combinatatorial, combinatorial, combinatorial. However, the results presented in [?] are distributional dependent. For this purpose, we are the first to examine the comatatatatorial case, combinatorial, combinatorial, combinatorial, combinatory, combinatory, \"we develop the\" combinatatory, \"combinatatory,\" combinatory, \"combinatatory.\" we, \"we,\" combinatatory, \"combinatatory,\" combinatatory, \"combinatorial,\" combinatory. \"we,\" combinatatory, \"combinatatory,\" combinatory, \"combinatory,\" combinatory. \"we,\" combinatory, \"we,\" combinatory, \"combinatory,\" combinatorial, \"combinatorial,\" combinatorial, \"combinatorial.\" we, \"we,\" combinatory, \"combinatorial,\" combinatory. \"we,\" combinatory, \"we,\" combinatory, \"combinatory."}, {"heading": "IX. CONCLUSION", "text": "In this thesis, we examine networked combinatorial bandit problems based on four cases. This is motivated by the presence of potential correlations or influences between adjacent weapons. We present and analyze a set of zero repentance strategies for each case. In the future, we are interested in examining some heuristics to improve the received regret in practice. For example, instead of playing the selected arm / strategy at each point in time with the maximum index value (Equation (5), (42)), we will play the arm / strategy that has the maximum experimental average observation among neighbors. Therefore, we ensure that the reward received is better than the maximum index value."}, {"heading": "X. APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Proof of Theorem 4", "text": "To prove the theory, we will use Tschernoff-Hoeffding > and the maximum inequality of Hoeffding. (...) Lemma 1: (Tschernoff-Hoeffding-border [?]). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...).). (...). (...).). (...).). (...). (...). (...).). (...). (...). (...).). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (..."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In this paper, we investigate a largely extended version of classical MAB problem, called networked combinatorial bandit problems. In particular, we consider the setting of a decision maker over a networked bandits as follows: each time a combinatorial strategy, e.g., a group of arms, is chosen, and the decision maker receives a reward resulting from her strategy and also receives a side bonus resulting from that strategy for each arm\u2019s neighbor. This is motivated by many real applications such as on-line social networks where friends can provide their feedback on shared content, therefore if we promote a product to a user, we can also collect feedback from her friends on that product. To this end, we consider two types of side bonus in this study: side observation and side reward. Upon the number of arms pulled at each time slot, we study two cases: single-play and combinatorial-play. Consequently, this leaves us four scenarios to investigate in the presence of side bonus: Single-play with Side Observation, Combinatorial-play with Side Observation, Single-play with Side Reward, and Combinatorial-play with Side Reward. For each case, we present and analyze a series of zero regret polices where the expect of regret over time approaches zero as time goes to infinity. Extensive simulations validate the effectiveness of our results.", "creator": "LaTeX with hyperref package"}}}