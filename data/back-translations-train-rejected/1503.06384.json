{"id": "1503.06384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2015", "title": "Costing Generated Runtime Execution Plans for Large-Scale Machine Learning Programs", "abstract": "Declarative large-scale machine learning (ML) aims at the specification of ML algorithms in a high-level language and automatic generation of hybrid runtime execution plans ranging from single node, in-memory computations to distributed computations on MapReduce (MR) or similar frameworks like Spark. The compilation of large-scale ML programs exhibits many opportunities for automatic optimization. Advanced cost-based optimization techniques require---as a fundamental precondition---an accurate cost model for evaluating the impact of optimization decisions. In this paper, we share insights into a simple and robust yet accurate technique for costing alternative runtime execution plans of ML programs. Our cost model relies on generating and costing runtime plans in order to automatically reflect all successive optimization phases. Costing runtime plans also captures control flow structures such as loops and branches, and a variety of cost factors like IO, latency, and computation costs. Finally, we linearize all these cost factors into a single measure of expected execution time. Within SystemML, this cost model is leveraged by several advanced optimizers like resource optimization and global data flow optimization. We share our lessons learned in order to provide foundations for the optimization of ML programs.", "histories": [["v1", "Sun, 22 Mar 2015 05:00:08 GMT  (31kb,D)", "http://arxiv.org/abs/1503.06384v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["matthias boehm"], "accepted": false, "id": "1503.06384"}, "pdf": {"name": "1503.06384.pdf", "metadata": {"source": "META", "title": "Costing Generated Runtime Execution Plans for Large-Scale Machine Learning Programs", "authors": ["Matthias Boehm"], "emails": ["mboehm@us.ibm.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most people who are able are able to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move and to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move."}, {"heading": "10: b = t(X) %*% y;", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11: beta = solve(A, b);", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "12: write(beta, $4);", "text": "Specifically, we read two matrices X and y from HDFS, where we append a column of 1s to X when asked to computear Xiv: 150 3.06 384v 1 [cs.D C] 22 March 201 5the model. The core calculation of this ML program (lines 9-11) constructs and then solves a linear equation system regularizing \u03bb. The size of the intermediate results A and b is determined by the number of features. Finally, we write the model coefficients \u03b2 to HDFS. In the rest of this paper, we discuss runtimes for different input sizes and cluster characteristics created by SystemML, as well as the cost of these generated plans. Selected details of the entire compilation chain are described in SystemML's architecture [3], SystemML's optimizer [1], and SystemML's Parfor Optimizer for task-oriented ML programs [2]."}, {"heading": "2. GENERATING RUNTIME PLANS", "text": "In this section we will discuss the basics of creating runtime schedules in SystemML. All examples are created on a 1 + 6 node cluster, i.e., a 2x4 Intel E5530 @ 2.40 GHz top node with hyper-threading enabled and 64 GB RAM and 6 2x6 Intel E5-2440 @ 2.40 GHz-2.90 GHz top node with hyper-threading enabled, 96 GB RAM, 12x2 TB disk storage, 10Gb Ethernet. We used Hadoop 2.2.0 and a static cluster configuration with 2 GB max / initial JVM heap size for the client and reduced tasks. Our HDFS capacity was 107 TB (11 disks per node), and we used an HDFS block of 128 MB. Finally, our default configurations are SystemML 12 Reducer (2x number of nodes) and a memory budget ratio of 70% of the maximum size."}, {"heading": "3. COSTING RUNTIME PLANS", "text": "In this section, we will now discuss how to calculate cost-generated runtimes that automatically reflect all optimization phases. Starting with a runtime P (with size information), we will use a white box cost model to calculate costs C (P, cc) as the estimated execution time of P given the cluster configuration cc. This time-based model allows us to linearize IO-, latence- and calculation costs in a single cost measure (see R2). Unlike similar work in MR job tuning, it also provides us with an analytical cost model for entire ML programs (see R1 and R4), as it is not based on profile runs, and the runtimes also cover the entire control flow. Finally, our approach is also aware of available resources (see R3), as the compiler already respects all memory limitations when creating runtime plans and we explicitly consider the degree of parallelism."}, {"heading": "3.1 Basic Notation", "text": "Before we can describe the skeleton of the actual cost estimator, we need a basic idea. The runtime schedule P consists of a hierarchy of program blocks bi-B and the insti-I. A matrix X is described by size information of rows m, columns n, and scanty s. We define s = nnz (X) / (m \u00b7 n), where nnz denotes the number of unequal zeros. This information allows us to calculate size estimates of the in-memory matrices M (X) and the serialized matrices M (X) (e.g. on local hard disk or HDFS). In addition, we let kl, km, and kr specify the degree of parallelism of the local control program, available card slots, and available reduction slots. In the case of YARN clusters, we correct km and kr according to the available virtual cores and memory resources of the cluster. Finally, we let T (P) specify the estimated runtime of the execution plan, then P is used."}, {"heading": "3.2 Cost Estimator Skeleton", "text": "The ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the rf for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the re"}, {"heading": "3.3 Time Estimates of Instructions", "text": "In general, we calculate the time estimate of a statement as the sum of latency, IO, and computation time based on its input and output statistics. Previous versions of this cost model [2] relied on profiled and trained cost functions for individual statements. In contrast, we now use a whitebox cost model based on IO bandwidth multipliers and company-specific floating point operations to eliminate the need for cluster-specific profile functions. A CP statement's time estimate consists of IO and computation time. We estimate the IO time based on variable state, size, format, and standard format-specific IO bandwidths. If the state of an input is in memory, then there is no IO time; otherwise, we calculate the IO time based on the serialized, format-specific size M systems (X) of that input."}, {"heading": "3.4 Examples Runtime Plan Costing", "text": "In this context, it is worth mentioning that the total planning cost of 3.31 s is calculated as the pure sum of all bond costs (which we show as a breakdown of IO and computation time).There are a few interesting observations to be made. First, the instruction, which uses a permanent input time, pays the associated IO costs (e.g. tsmm and r \"), while subsequent operations on the same data (e.g. ba + *) only take into account the computation time. Second, we see that the computation time for tsmm dominates the entire execution time.The following weights are the first reading of X and the computation cost of X."}, {"heading": "3.5 Limitations", "text": "The cost model presented works very well in practice. However, there are basic limitations. Unknown Size Information: Despite techniques for disseminating size information of dimensions and sparseness [1], there are cases where we are not able to determine the size of intermediates during the initial compilation. In this case, the compiler resorts to conservative but scalable plans to ensure the validity of the plan. Apart from the MR job latency, we cannot fully infer the IO and calculation costs of affected operators in those cases that potentially lead to a large underestimation.This problem is often addressed by the optimizer being aware of the cost model that it can often even be used for pruning.buffer pool behavior: Our cost model only partially takes into account buffer pool distributions that can contribute to the total program costs."}, {"heading": "4. CONCLUSIONS", "text": "In summary, our simple and robust cost model enables the calculation of generated schedules for ML programs. This model automatically reflects all optimization decisions along the entire compilation chain. Most importantly, it provides an analytical cost model for alternative plans without profiling or profiling, captures all relevant cost factors, recognizes data and cluster characteristics, and can be used for any complex ML program."}, {"heading": "5. REFERENCES", "text": "[1] M. Boehm, D. R. Burdick, A. V. Evfimievski, B. Reinwald, F. R. Reiss, P. Sen, S. Tatikonda, and Y. Tian. SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs. IEEE Data Eng. Bull., 37 (3), 2014. [2] M. Boehm, S. Tatikonda, B. Reinwald, P. Sen, Y. Tian, D. Burdick, and S. Vaithyanathan. Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML. PVLDB, 7 (7), 2014."}], "references": [{"title": "SystemML\u2019s Optimizer: Plan Generation for Large-Scale Machine Learning Programs", "author": ["M. Boehm", "D.R. Burdick", "A.V. Evfimievski", "B. Reinwald", "F.R. Reiss", "P. Sen", "S. Tatikonda", "Y. Tian"], "venue": "IEEE Data Eng. Bull.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Hybrid Parallelization Strategies for Large-Scale Machine Learning in SystemML", "author": ["M. Boehm", "S. Tatikonda", "B. Reinwald", "P. Sen", "Y. Tian", "D. Burdick", "S. Vaithyanathan"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "SystemML: Declarative Machine Learning on MapReduce", "author": ["A. Ghoting", "R. Krishnamurthy", "E.P.D. Pednault", "B. Reinwald", "V. Sindhwani", "S. Tatikonda", "Y. Tian", "S. Vaithyanathan"], "venue": "In ICDE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Scalable and Numerically Stable Descriptive Statistics in SystemML", "author": ["Y. Tian", "S. Tatikonda", "B. Reinwald"], "venue": "In ICDE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "In this paper, we share a simple and robust technique of costing generated runtime plans which is the result of several lessons we have learned applying earlier cost model versions in real-world use cases of SystemML [1, 2, 3].", "startOffset": 217, "endOffset": 226}, {"referenceID": 1, "context": "In this paper, we share a simple and robust technique of costing generated runtime plans which is the result of several lessons we have learned applying earlier cost model versions in real-world use cases of SystemML [1, 2, 3].", "startOffset": 217, "endOffset": 226}, {"referenceID": 2, "context": "In this paper, we share a simple and robust technique of costing generated runtime plans which is the result of several lessons we have learned applying earlier cost model versions in real-world use cases of SystemML [1, 2, 3].", "startOffset": 217, "endOffset": 226}, {"referenceID": 2, "context": "Selected details of the entire compilation chain are described in SystemML\u2019s architecture [3], SystemML\u2019s optimizer [1], and SystemML\u2019s parfor optimizer for task-parallel ML programs [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "Selected details of the entire compilation chain are described in SystemML\u2019s architecture [3], SystemML\u2019s optimizer [1], and SystemML\u2019s parfor optimizer for task-parallel ML programs [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Selected details of the entire compilation chain are described in SystemML\u2019s architecture [3], SystemML\u2019s optimizer [1], and SystemML\u2019s parfor optimizer for task-parallel ML programs [2].", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "For X>X (HOP 53), we selected again a tsmm MR operator but with final aggregation (ak+, aggregate kahan plus [4]) in order to aggregate partial mapper results.", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "We select an cpmm operator [3] instead, which requires two MR jobs.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Similar to statistics aggregation in the parfor optimizer for task-parallel ML programs [2], we aggregate the time estimate of an program block b by the sum of its childs c(b) (predicates, included program blocks, instructions) due to their sequential execution with:", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Earlier versions of this cost model [2] relied on profiled and trained cost functions for individual instructions.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Unknown Size Information: Despite techniques for propagating size information of dimensions and sparsity [1], there do exist cases where we are not able to determine sizes of intermediates during initial compilation.", "startOffset": 105, "endOffset": 108}], "year": 2017, "abstractText": "Declarative large-scale machine learning (ML) aims at the specification of ML algorithms in a high-level language and automatic generation of hybrid runtime execution plans ranging from single node, in-memory computations to distributed computations on MapReduce (MR) or similar frameworks like Spark. The compilation of large-scale ML programs exhibits many opportunities for automatic optimization. Advanced cost-based optimization techniques require\u2014as a fundamental precondition\u2014an accurate cost model for evaluating the impact of optimization decisions. In this paper, we share insights into a simple and robust yet accurate technique for costing alternative runtime execution plans of ML programs. Our cost model relies on generating and costing runtime plans in order to automatically reflect all successive optimization phases. Costing runtime plans also captures control flow structures such as loops and branches, and a variety of cost factors like IO, latency, and computation costs. Finally, we linearize all these cost factors into a single measure of expected execution time. Within SystemML, this cost model is leveraged by several advanced optimizers like resource optimization and global data flow optimization. We share our lessons learned in order to provide foundations for the optimization of ML programs.", "creator": ""}}}