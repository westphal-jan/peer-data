{"id": "1503.06468", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2015", "title": "Machine Learning Methods for Attack Detection in the Smart Grid", "abstract": "Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semi-supervised) are employed with decision and feature level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than the attack detection algorithms which employ state vector estimation methods in the proposed attack detection framework.", "histories": [["v1", "Sun, 22 Mar 2015 19:38:45 GMT  (1962kb)", "http://arxiv.org/abs/1503.06468v1", "14 pages, 11 Figures"]], "COMMENTS": "14 pages, 11 Figures", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.SY", "authors": ["mete ozay", "inaki esnaola", "fatos t yarman vural", "sanjeev r kulkarni", "h vincent poor"], "accepted": false, "id": "1503.06468"}, "pdf": {"name": "1503.06468.pdf", "metadata": {"source": "CRF", "title": "Machine Learning Methods for Attack Detection in the Smart Grid", "authors": ["Mete Ozay"], "emails": ["m.ozay@cs.bham.ac.uk).", "poor}@princeton.edu).", "vural@ceng.metu.edu.tr)."], "sections": [{"heading": null, "text": "This article addresses the question to what extent this is a system in which machine learning algorithms are used to predict the failure of system components. Anderson et al. [2] relies on machine learning algorithms to predict the energy management of loads and sources in intelligent networks. [3] Malicious activity prediction and intrusion detection problems has been analyzed with machine learning algorithms are used to predict the failure of system components. Anderson et al. [4] In this paper, we focus on the problem of incorrect data injection in the intelligent network. We use the model of Distributed Sparse Attacks Attacks Attacks Attacks at the physical level."}, {"heading": "II. PROBLEM FORMULATION", "text": "In this section, the problem of attack detection is formalized as a problem of machine learning."}, {"heading": "A. False Data Injection Attacks", "text": "Incorrect data injection attacks are defined in the following model: z = Hx + > n, (1) where x-RD contains the voltage phase angles on the buses, z-RN is the vector of the measurements, H-RN-D is the measurement threshold Jacobian matrix, and n-RN is the measurement noise that can be used as independent components [7]. The attack detection problem is defined as that of deciding whether there is an attack on the measurements or not. If the noise is normally distributed with zero mean, then a State Vector Estimation (SVE) method of computingx applied by computingx (HT-H) \u2212 1HT-Z, (2) where there is a diagonal matrix whose diagonal elements are given by the vector."}, {"heading": "B. Attack Detection using Statistical Learning Methods", "text": "This year it is more than ever before."}, {"heading": "III. ATTACK DETECTION USING MACHINE LEARNING METHODS", "text": "In this section, the problem of attack detection is modelled by the statistical classification of measurements using machine learning."}, {"heading": "A. Supervised Learning Methods", "text": "Following, the classification function f is calculated for each sample usingw (t + 1): = (w + p), (w + p), (w + p), (w + p), (w + p), (w + p), (w + p), (w + p), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), c (c), c (c), c (c), c (c), \"c,\" c, c, \"c, c, c,\" c, c, \"c, c,\" c, c, \"c,\" c, \"c, c,\" c, \"c,\" c, c, \"c, c,\" c, c, \"c, c,\" c, \"c, c,\" (c), c), c, \"(c), c,\" (c), c, c, \"(c), c, c,\" (c), c, c, c, c, c, \"(c,\" (c), c, c, c, c, c, \"(c, c,\" (c), c, \"(c), c, c, c, c,\" (c, c, \"(c), c, c, c, c,\" (c), c, c, c, c, (c, c), (c), (c, (c), (c), (c), (c, (c), (c), (c), (c), (c), (c), (c), (c, (c), (c), (c), (), (c), (c), (c), (c,"}, {"heading": "B. Semi-supervised Learning Methods", "text": "In this section, a semi-supervised support vector machine algorithm called Semi-Supervised SVM (S3VM) [26], [27] is used to establish the analytical relationship between supervised and semi-supervised learning algorithms. In this context, the unlabeled samples are included in the cost function of the optimization problem (13) to minimize the effects of the optimization problem (22)."}, {"heading": "C. Decision and Feature Level Fusion Methods", "text": "One of the challenges of statistical learning theory is to find a classification rule that is better than a set of rules for individual classifiers, or to find a set of characteristics that better represents the samples than a set of individual characteristics. An approach to solving this problem is to combine a collection of classifiers or a set of characteristics to increase the performance of each classifier. The first approach is called decision-level fusion or ensemble formation, and the second approach is called attribute level fusion. In this section, we consider Adaboost [28] and Multiple Kernel Learning (MKL) [29] for ensemble learning and attribute level fusion. 1) Ensemble Learning for Decision Level Fusion: Various methods such as bagging, boosting and stacking have been developed to combine classifiers in group learning situations [17], [30]. Below, Adaboost is explained as an ensemble learning approach that combines a collection of more weak classifiers and a stronger classifier."}, {"heading": "D. Online Learning Methods for Real-time Attack Detection", "text": "In this scenario, we loosen the distribution assumption of Section II.B, since the samples are observed in an arbitrary sequence [33]. In addition, smart PMUs using learning algorithms may be required to detect the attacks when the measurements are observed without processing the entire set of samples. To solve these challenging problems, we can use online versions of the learning algorithms specified in the preceding sections. In a general online learning environment, the learning algorithm is given a sequence of samples (or a single sample) each time the algorithm is observed or processed. Subsequently, the algorithm compresses the learning model using only the given samples and predicts the labels. The learning model is updated with respect to the error of the learning algorithm, which is compressed using a loss function on the given samples."}, {"heading": "E. Performance Analysis", "text": "In smart grid networks, the main concern is not only to detect the attacked variables, but also the attacked variables with high performance. In other words, we need the algorithms to predict the samples with high precision and avoid false alarms. To this end, we measure the true positives (tp), the true negatives (tn), the false positives (fp) and the false negatives (fn) defined in Table I. In addition, the learning abilities and characteristics of the algorithms are measured by precision (Prec), recall (Rec) and accuracy (Acc) defined as [13] Prec = tp tp + fp, Rec = tp tp tp + fn, Acc = tp + tn tp + fn + fn + fn. (23) 6Precision values provide information about the predictive performance of the algorithms. On the other hand, all values are safely classified as if the degree of attack is classified as defective."}, {"heading": "IV. EXPERIMENTS", "text": "The classification algorithms are analyzed in IEEE 9-bus, 57-bus and 118-bus test systems are not defined in the experiments.The measurement matrices H of the systems are used by the MATPOWER toolbox [36].The operating points of the test systems provided in the MATPOWER case file are generated in the generation of z-sparse attack vector with Gaussian distributed non-elements with the same mean and the same variance as the entries of z. [15] We assume that concept drift [37] and dataset shift [38] do not occur. Therefore, we use G = N in the simulations based on the results of Ozay et al."}, {"heading": "A. Results for Supervised Learning Algorithms", "text": "The performance of different algorithms is reported for the IEEE 57 bus system in Fig. 2. Accuracy values of SVE and SV1 class increase as \u03baN class. [SV1] Accuracy values of SV1 class do not increase, and accuracy and recall values increase as fn values decrease and tn values rise. In Fig. 2.c, a phase transition around the SV1 class is observed for the performance of SV1 class. As the distance between measurement vectors of the challenged and safe variables increases, we observe that accuracy, precision and recall values of k-NN class increase. 2.d Accuracy and recall values of Fig class increase in Fig-NN class. 2.d Accuracy and recall values of the recall class increase."}, {"heading": "B. Results for Semi-supervised Learning Algorithms", "text": "We use the S3VM with standard parameters, as proposed in [44]. Results of the semi-monitored SVM are shown in Figure 8. Unlike the monitored SVM, we do not observe sharp phase transitions in the semi-monitored SVM, since the information obtained from unmarked data contributes to the performance values in the calculation of the learning models. Thus, for example, the precision values of class-2 near the critical point for the monitored SVM in Figure 7. However, the semi-monitored SVM uses the unmarked samples in the calculation of the learning model in Figure 19 and partially solves this problem."}, {"heading": "C. Results for Decision and Feature Level Fusion Algorithms", "text": "In this section we analyze Adaboost and MKL. Decision stumps are used in Adaboost as weak classifiers [31]. Each decision stumps is a single-stage two-leaf binary decision tree, which is used to construct a series of dichotomies consisting of binary labels of samples [31]. The number of weak classifiers is selected based on a one-step cross-validation in the training set. We use MKL with a linear and a Gaussian core with the default parameters proposed in the simple MKL implementation [32]. The results shown in Fig. 9 show that the recall values of MKL for class-1 are lower than those of Adaboost. Furthermore, the precision values of MKL fall faster than the values of Adaboost than the DIN values for class-2. Therefore, the fn values of MKL are greater than the values of Adaboost, or in other words, the number of measurements affected by this phase is considered to be greater than the SVL in this phenomenon."}, {"heading": "D. Results for Online Learning Algorithms", "text": "We look at four online learning algorithms, namely Online Perceptron (OP), Online Perceptron with Weighted Models (OPWM), Online SVM and Online SLR. Note that these algorithms are the online versions of the batch learning algorithms specified in Section III-A and taking into account the online algorithm design approach in Section III-D. The details of the implementations of OP, OPWM, Online SVM and SLR are in [34], [35] and [45]. When the OP is used, only the model w (t) is calculated that uses the last observed measurements in due time. Details of the implementations of the test sampling are taken into account. On the other hand, we consider an average of the models wave (t) = 1T = 1 w (t), which is calculated by minimizing margin errors in the OPWM. The results are considered for the classification of the test samples."}, {"heading": "V. SUMMARY AND CONCLUSION", "text": "The attack problem has been reformulated as a machine learning problem and the performance of the monitored, semi-monitored, half-monitored, half-detected, half-detected, half-detected, half-detected, half-detected, half-detected, but in the experiments we have observed that the state of the machine learning algorithms is more sensitive than the known attack algorithms, which use a state vector estimation approach to detect both observable and unobservable attacks. We have observed that the perceptron is less sensitive and the k-NN is less sensitive to the system size than the other algorithms. Furthermore, the unbalanced data problems are more affective than the performance of the k-NN. Therefore, k-NN can perform better in small systems and worse in large systems when compared to other algorithms. The SVM performs better than the other algorithms in large systems."}], "references": [{"title": "Machine learning for the New York City power grid", "author": ["C. Rudin", "D. Waltz", "R. Anderson", "A. Boulanger", "A. Salleb-Aouissi", "M. Chow", "H. Dutta", "P. Gross", "B. Huang", "S. Ierome"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, pp. 328\u2013345, Feb. 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive stochastic control for the smart grid", "author": ["R.N. Anderson", "A. Boulanger", "W.B. Powell", "W. Scott"], "venue": "Proc. IEEE, vol. 99, pp. 1098\u2013 1115, Jun. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "An early warning system against malicious activities for smart grid communications", "author": ["Z. Fadlullah", "M. Fouda", "N. Kato", "X. Shen", "Y. Nozaki"], "venue": "IEEE Netw., vol. 25, pp. 50\u201355, Sep. 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed intrusion detection system in a multi-layer network architecture of smart grids", "author": ["Y. Zhang", "L. Wang", "W. Sun", "R. Green", "M. Alam"], "venue": "IEEE Trans. Smart Grid, vol. 2, pp. 796\u2013808, Dec. 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse attack construction and state estimation in the smart grid: Centralized and distributed models", "author": ["M. Ozay", "I. Esnaola", "F.T. Yarman Vural", "S.R. Kulkarni", "H.V. Poor"], "venue": "IEEE J. Sel. Areas Commun., vol. 31, pp. 1306\u20131318, Jul. 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "False data injection attacks against state estimation in electric power grids", "author": ["Y. Liu", "P. Ning", "M.K. Reiter"], "venue": "Proc. 16th ACM Conf. Computer and Communications Security, Chicago, Illinois, Nov. 2009, pp. 21\u201332.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Malicious data attacks on the smart grid", "author": ["O. Kosut", "L. Jia", "R.J. Thomas", "L. Tong"], "venue": "IEEE Trans. Smart Grid, vol. 2, pp. 645\u2013658, Dec. 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparing the topological and electrical structure of the North American electric power infrastructure", "author": ["E. Cotilla-Sanchez", "P. Hines", "C. Barrows", "S. Blumsack"], "venue": "IEEE Syst. J., vol. 6, pp. 616\u2013626, Dec. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Strategic protection against data injection attacks on power grids", "author": ["T.T. Kim", "H.V. Poor"], "venue": "IEEE Trans. Smart Grid, vol. 2, no. 2, pp. 326\u2013333, Jun. 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Decoding by linear programming", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Inf. Theor., vol. 51, no. 12, pp. 4203\u20134215, Dec. 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inf. Theor., vol. 52, no. 4, pp. 1289\u20131306, Apr. 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Smarter security in the smart grid", "author": ["M. Ozay", "I. Esnaola", "F. Yarman Vural", "S.R. Kulkarni", "H.V. Poor"], "venue": "Proc. 3rd IEEE Int. Conf. Smart Grid Communications, Tainan City, Nov. 2012, pp. 312\u2013317.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Cornujols, Phase Transitions in Machine Learning", "author": ["L. Saitta", "A. Giordana"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distributed models for sparse attack construction and state vector estimation in the smart grid", "author": ["M. Ozay", "I. Esnaola", "F. Yarman Vural", "S.R. Kulkarni", "H.V. Poor"], "venue": "Proc. 3rd IEEE Int. Conf. Smart Grid Communications, Tainan City, Nov. 2012, pp. 306\u2013311.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to statistical learning theory", "author": ["O. Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "Advanced Lectures on Machine Learning, O. Bousquet, U. von Luxburg, and G. Rtsch, Eds. Berlin: Springer, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "An Elementary Introduction to Statistical Learning Theory", "author": ["S. Kulkarni", "G. Harman"], "venue": "Hoboken, NJ: Wiley Publishing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Divergence estimation for multidimensional densities via k-nearest-neighbor distances", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "IEEE Trans. Inf. Theor., vol. 55, pp. 2392\u20132405, May 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Support Vector Machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "New York: Springer Publishing Company, Incorporated,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Statistical learning theory: A tutorial", "author": ["S. Kulkarni", "G. Harman"], "venue": "Wiley Interdisciplinary Reviews: Computational Statistics, vol. 3, no. 6, pp. 543\u2013556, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Training a support vector machine in the primal", "author": ["O. Chapelle"], "venue": "Neural Comput., vol. 19, no. 5, pp. 1155\u20131178, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1\u2013122, Jan. 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "Eds"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "Proc. 16th Int. Conf. Mach. Learn., Bled, Jun. 1999, pp. 200\u2013209.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimization techniques for semi-supervised support vector machines", "author": ["O. Chapelle", "V. Sindhwani", "S.S. Keerthi"], "venue": "J. Mach. Learn. Res., vol. 9, no. 6, pp. 203\u2013233, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "Proc. 21st Int. Conf. Mach. Learn., Banff, AB, Jul. 2004, pp. 6\u201313.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. Kuncheva"], "venue": "Hoboken, NJ: Wiley-Interscience,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Boosting: Foundations and Algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "J. Mach. Learn. Res., vol. 9, pp. 2491\u20132521, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "From batch to transductive online learning", "author": ["S. Kakade", "A. Kalai"], "venue": "Advances in Neural Information Processing Systems, Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, Eds. Cambridge, MA: The MIT Press, 2005, pp. 611\u2013618.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Bounded kernel-based online learning", "author": ["F. Orabona", "J. Keshet", "B. Caputo"], "venue": "J. Mach. Learn. Res., vol. 10, pp. 2643\u20132666, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "An interior-point stochastic approximation method and an l1-regularized delta rule", "author": ["P. Carbonetto", "M. Schmidt", "N.D. Freitas"], "venue": "Advances in Neural Information Processing Systems, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. Red Hook, NY: Curran Associates, Inc., 2008, pp. 233\u2013240.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "MAT- POWER: Steady-state perations, planning, and analysis tools for power systems research and education", "author": ["R.D. Zimmerman", "C.E. Murillo-S\u00e1nchez", "R.J. Thomas"], "venue": "IEEE Trans. Power Syst., vol. 26, pp. 12\u201319, Feb. 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning changing concepts by exploiting the structure of change", "author": ["P.L. Bartlett", "S. Ben-David", "S.R. Kulkarni"], "venue": "Mach. Learn., vol. 41, no. 2, pp. 153\u2013174, 2000.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Libsvm: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intell. Syst. Technol., vol. 2, no. 3, pp. 1\u201327, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Mach. Learn. Res., vol. 9, pp. 1871\u20131874, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1871}, {"title": "Asymptotic behaviors of support vector machines with Gaussian kernel", "author": ["S.S. Keerthi", "C.-J. Lin"], "venue": "Neural Comput., vol. 15, no. 7, pp. 1667\u20131689, 2003.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2003}, {"title": "An interior-point method for largescale l1-regularized logistic regression", "author": ["K. Koh", "S.-J. Kim", "S. Boyd"], "venue": "J. Mach. Learn. Res., vol. 8, pp. 1519\u20131555, 2007.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "Making large-scale support vector machine learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods, B. Sch\u00f6lkopf, C. J. C. Burges, and A. J. Smola, Eds. Cambridge, MA: The MIT Press, 1999, pp. 169\u2013 184.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1999}, {"title": "DOGMA: A MATLAB toolbox for online learning", "author": ["F. Orabona"], "venue": "2009. [Online]. Available: http://dogma.sourceforge.net", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization", "author": ["D.L. Donoho", "M. Elad"], "venue": "Proc. Nat. Acad. Sci., vol. 100, no. 5, pp. 2197\u20132202, 2003.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2003}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Dimensionality reduction via sparse support vector machines", "author": ["J. Bi", "K. Bennett", "M. Embrechts", "C. Breneman", "M. Song"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1229\u20131243, 2003.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "A direct method for building sparse kernel learning algorithms", "author": ["M. Wu", "B. Scholkopf", "G. Bakir"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 603\u2013624, 2006.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "INTRODUCTION Machine learning methods have been widely proposed in the smart grid literature for monitoring and control of power systems [1], [2], [3], [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "[1] suggest an intelligent framework for system design in which machine learning algorithms are employed to predict the failures of system components.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] employ machine learning algorithms for the energy management of loads and sources in smart grid networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Malicious activity prediction and intrusion detection problems have been analyzed using machine learning techniques at the network layer of smart grid communication systems [3], [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Malicious activity prediction and intrusion detection problems have been analyzed using machine learning techniques at the network layer of smart grid communication systems [3], [4].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "[5], where the attacks are directed by injecting false data into the local measurements observed by either local network operators or smart Phasor Measurement Units", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In addition, network operators who employ statistical learning algorithms for attack detection know the topology of the network, measurements observed in the clusters and the measurement matrix [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 4, "context": "If the residual is greater than a given threshold, a data injection attack is declared [5], [6], [7], [8].", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "If the residual is greater than a given threshold, a data injection attack is declared [5], [6], [7], [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "If the residual is greater than a given threshold, a data injection attack is declared [5], [6], [7], [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "However, exact recovery of state vectors is a challenge for state vector estimation based methods in sparse networks [5], [9], [10], where the Jacobian measurement matrix is sparse.", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "However, exact recovery of state vectors is a challenge for state vector estimation based methods in sparse networks [5], [9], [10], where the Jacobian measurement matrix is sparse.", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "However, exact recovery of state vectors is a challenge for state vector estimation based methods in sparse networks [5], [9], [10], where the Jacobian measurement matrix is sparse.", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "Sparse reconstruction methods can be employed to solve the problem, but the performance of this approach is limited by the sparsity of the state vectors [5], [11], [12].", "startOffset": 153, "endOffset": 156}, {"referenceID": 9, "context": "Sparse reconstruction methods can be employed to solve the problem, but the performance of this approach is limited by the sparsity of the state vectors [5], [11], [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Sparse reconstruction methods can be employed to solve the problem, but the performance of this approach is limited by the sparsity of the state vectors [5], [11], [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 5, "context": ", the number of nonzero elements is at most \u03ba, which is bounded by the size of the Jacobian matrix), then false data injection attacks, called unobservable attacks, cannot be detected [7], [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 6, "context": ", the number of nonzero elements is at most \u03ba, which is bounded by the size of the Jacobian matrix), then false data injection attacks, called unobservable attacks, cannot be detected [7], [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 11, "context": "[13] who employ supervised learning algorithms to predict false data injection attacks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In addition, phase transitions can be observed in the performance of Support Vector Machines (SVM) at a value of \u03ba [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "False Data Injection Attacks False Data Injection Attacks are defined in the following model: z =Hx + n, (1) where x \u2208 R contains the voltage phase angles at the buses, z \u2208 R is the vector of measurements, H \u2208 R is the measurement Jacobian matrix and n \u2208 R is the measurement noise, which is assumed to have independent components [7].", "startOffset": 331, "endOffset": 334}, {"referenceID": 5, "context": ",N [7], [13].", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": ",N [7], [13].", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "The secure variables satisfy the constraint ai = 0, \u2200i \u2208 \u0100, where \u0100 is the set complement of A [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "In order to detect an attack, the measurement residual [7], [13] is examined in l2-norm \u03c1 = \u2225z\u0303 \u2212Hx\u0302\u222522, where x\u0302 \u2208 R D", "startOffset": 55, "endOffset": 58}, {"referenceID": 11, "context": "In order to detect an attack, the measurement residual [7], [13] is examined in l2-norm \u03c1 = \u2225z\u0303 \u2212Hx\u0302\u222522, where x\u0302 \u2208 R D", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "One of the challenging problems of this approach is that the Jacobian measurement matrices of power systems in the smart grid are sparse under the DC power flow model [13], [15].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "One of the challenging problems of this approach is that the Jacobian measurement matrices of power systems in the smart grid are sparse under the DC power flow model [13], [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 9, "context": "Therefore, the sparsity of the systems determines the performance of sparse state vector estimation methods [11], [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Therefore, the sparsity of the systems determines the performance of sparse state vector estimation methods [11], [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 5, "context": "For instance, if a =Hc, where c \u2208 R is an attack vector, then the attack is unobservable by using the measurement residual \u03c1 [7], [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "For instance, if a =Hc, where c \u2208 R is an attack vector, then the attack is unobservable by using the measurement residual \u03c1 [7], [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 14, "context": ") with joint distribution P , the statistical learning problem can be defined as constructing a hypothesis function f \u2236 S \u2192 Y , that captures the relationship between the samples and labels [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "[5] is employed for attack construction where the measurements are observed in clusters in the network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": ",G [5].", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "If the attack vectors, a, are constructed in the column space of H, then they are annihilated in the computation of the residual [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 15, "context": "Two main assumptions from statistical learning theory need to be taken into account to classify measurements which satisfy (6): 1) We assume that (si, yi) \u2208 S\u00d7Y are distributed according to a joint distribution P [17].", "startOffset": 213, "endOffset": 217}, {"referenceID": 14, "context": "random variables [16].", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "1) Perceptron: Given a sample si, a perceptron predicts yi using the classification function f(si) = sign(w \u22c5 si), where w \u2208 Ri is a weight vector and sign(w \u22c5 si) is defined as [17]", "startOffset": 178, "endOffset": 182}, {"referenceID": 15, "context": "Despite its success in various machine learning applications, the convergence of the algorithm is assured only when the samples are linearly separable [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "2) k-Nearest Neighbor (k-NN): This algorithm labels an unlabeled sample si according to the labels of its k-nearest neighborhood in the feature space [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 16, "context": ", si(k)}, is constructed by computing the Euclidean distances between the samples [18], where i(1), i(2), .", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "One of the challenges of k-NN is the curse of dimensionality, which is the difficulty of the learning problem when the sample size is small compared to the dimension of the feature vector [17], [19], [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 4, "context": "If the sample size is large, distributed learning and optimization methods can be used [5], [15].", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "If the sample size is large, distributed learning and optimization methods can be used [5], [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "3) Support Vector Machines: We seek a hyperplane that linearly separates attacked and secure measurements into two half spaces using hyperplanes in a D dimensional feature space, F , which is constructed by a non-linear mapping \u03a8 \u2236 S \u2192 F [13], [21].", "startOffset": 238, "endOffset": 242}, {"referenceID": 17, "context": "3) Support Vector Machines: We seek a hyperplane that linearly separates attacked and secure measurements into two half spaces using hyperplanes in a D dimensional feature space, F , which is constructed by a non-linear mapping \u03a8 \u2236 S \u2192 F [13], [21].", "startOffset": 244, "endOffset": 248}, {"referenceID": 17, "context": "(12) The hyperplane w\u03a8 is computed by solving the following optimization problem in primal or dual form [21], [22], [23]", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "(12) The hyperplane w\u03a8 is computed by solving the following optimization problem in primal or dual form [21], [22], [23]", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "(12) The hyperplane w\u03a8 is computed by solving the following optimization problem in primal or dual form [21], [22], [23]", "startOffset": 116, "endOffset": 120}, {"referenceID": 20, "context": "4) Sparse Logistic Regression: In utilizing this approach for attack detection, we solve the classification problem using the Alternating Direction Method of Multipliers (ADMM) [24] considering the sparse state vector estimation approach of Ozay et al.", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": ", yMtr) T , the ADMM optimization problem [24] is constructed as minimize L(S,Y) + \u03bc(r) subject to w \u2212 r = 0 (18)", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "where w is a weight vector, r is a vector of optimization variables, \u03bc(r) = \u03bb\u2225r\u22251 is a regularization function, and \u03bb is a regularization parameter which is introduced to control the sparsity of the solution [24].", "startOffset": 208, "endOffset": 212}, {"referenceID": 21, "context": "Semi-supervised Learning Methods In semi-supervised learning methods, the information obtained from the unlabeled test samples is used during the computation of the learning models [25].", "startOffset": 181, "endOffset": 185}, {"referenceID": 22, "context": "In this section, a semi-supervised Support Vector Machine algorithm, called Semi-supervised SVM (S3VM) [26], [27] is employed to establish the analytical relationship between supervised and semi-supervised learning algorithms.", "startOffset": 103, "endOffset": 107}, {"referenceID": 23, "context": "In this section, a semi-supervised Support Vector Machine algorithm, called Semi-supervised SVM (S3VM) [26], [27] is employed to establish the analytical relationship between supervised and semi-supervised learning algorithms.", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "The main assumption of the S3VM is that the samples in the same cluster have the same labels and the number of subclusters is not large [27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "Moreover, this requirement is satisfied in (19) by adjusting C2 [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "A survey of the methods which are used to provide optimal C2 and solve (19) is given in [27].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "In this section, we consider Adaboost [28] and Multiple Kernel Learning (MKL) [29] for ensemble learning and feature level fusion.", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "In this section, we consider Adaboost [28] and Multiple Kernel Learning (MKL) [29] for ensemble learning and feature level fusion.", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "1) Ensemble Learning for Decision Level Fusion: Various methods such as bagging, boosting and stacking have been developed to combine classifiers in ensemble learning situations [17], [30].", "startOffset": 178, "endOffset": 182}, {"referenceID": 26, "context": "1) Ensemble Learning for Decision Level Fusion: Various methods such as bagging, boosting and stacking have been developed to combine classifiers in ensemble learning situations [17], [30].", "startOffset": 184, "endOffset": 188}, {"referenceID": 15, "context": "In the following, Adaboost is explained as an ensemble learning approach, in which a collection of weak classifiers are generated and combined using a combination rule to construct a stronger classifier which performs better than the weak classifiers [17], [28], [31].", "startOffset": 251, "endOffset": 255}, {"referenceID": 24, "context": "In the following, Adaboost is explained as an ensemble learning approach, in which a collection of weak classifiers are generated and combined using a combination rule to construct a stronger classifier which performs better than the weak classifiers [17], [28], [31].", "startOffset": 257, "endOffset": 261}, {"referenceID": 27, "context": "In the following, Adaboost is explained as an ensemble learning approach, in which a collection of weak classifiers are generated and combined using a combination rule to construct a stronger classifier which performs better than the weak classifiers [17], [28], [31].", "startOffset": 263, "endOffset": 267}, {"referenceID": 27, "context": "The distribution is initialized uniformly D1(i) = 1 M at t = 1, and is updated by a parameter \u03b1t = 12 log( 1\u2212\u01ebt \u01ebt ) as follows [31]", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "One of the feature level fusion methods is MKL in which different feature mappings are represented by kernels that are combined to produce a new kernel which represents the samples better than the other kernels [29].", "startOffset": 211, "endOffset": 215}, {"referenceID": 28, "context": "the normalized weights such that U \u2211 u=1 du = 1, then we obtain the following optimization problem of the MKL [32]:", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "B, since the samples are observed in an arbitrary sequence [33].", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "For instance, an online perceptron is implemented by predicting the label yi of a single sample si at each time t, and updating the weight vector w using \u2206w for the misclassified samples with yi \u2260 sign(f(si)) [34].", "startOffset": 209, "endOffset": 213}, {"referenceID": 30, "context": "This simple approach is applied for the development of online MKL [34] and regression algorithms [35].", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "This simple approach is applied for the development of online MKL [34] and regression algorithms [35].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "In addition, the learning abilities and memorization properties of the algorithms are measured by Precision (Prec), Recall (Rec) and Accuracy (Acc) values which are defined as [13] Prec = tp tp+fp , Rec = tp tp+fn , Acc = tp+tn tp+tn+fp+fn .", "startOffset": 176, "endOffset": 180}, {"referenceID": 32, "context": "The measurement matrices H of the systems are obtained from the MATPOWER toolbox [36].", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "In the experiments, we assume that the attacker has access to \u03ba measurements which are randomly chosen to generate a \u03ba-sparse attack vector a with Gaussian distributed nonzero elements with the same mean and variance as the entries of z [5], [13], [15].", "startOffset": 237, "endOffset": 240}, {"referenceID": 11, "context": "In the experiments, we assume that the attacker has access to \u03ba measurements which are randomly chosen to generate a \u03ba-sparse attack vector a with Gaussian distributed nonzero elements with the same mean and variance as the entries of z [5], [13], [15].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": "In the experiments, we assume that the attacker has access to \u03ba measurements which are randomly chosen to generate a \u03ba-sparse attack vector a with Gaussian distributed nonzero elements with the same mean and variance as the entries of z [5], [13], [15].", "startOffset": 248, "endOffset": 252}, {"referenceID": 33, "context": "We assume that concept drift [37] and dataset shift [38] do not occur.", "startOffset": 29, "endOffset": 33}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We analyze the behavior of each algorithm on each system for both observable and unobservable attacks by generating attack vectors with different values of \u03ba N \u2208 [0,1].", "startOffset": 162, "endOffset": 167}, {"referenceID": 4, "context": "unobservable attacks, are generated [5].", "startOffset": 36, "endOffset": 39}, {"referenceID": 34, "context": "The LIBSVM [39] implementation is used for the SVM, and the ADMM [24] implementation is used for Sparse Logistic Regression (SLR).", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "The LIBSVM [39] implementation is used for the SVM, and the ADMM [24] implementation is used for Sparse Logistic Regression (SLR).", "startOffset": 65, "endOffset": 69}, {"referenceID": 34, "context": "A grid search method [39], [40], [41] is employed to search the parameters of the SVM in an interval I = [Imin,Imax], where Imin and Imax are user defined values.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "A grid search method [39], [40], [41] is employed to search the parameters of the SVM in an interval I = [Imin,Imax], where Imin and Imax are user defined values.", "startOffset": 27, "endOffset": 31}, {"referenceID": 36, "context": "A grid search method [39], [40], [41] is employed to search the parameters of the SVM in an interval I = [Imin,Imax], where Imin and Imax are user defined values.", "startOffset": 33, "endOffset": 37}, {"referenceID": 36, "context": "In order to follow linear paths in the search space, log values of parameters are considered in the grid search method [41].", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "Keerthi and Lin [41] analyzed the asymptotic properties of the SVM for I = [0,\u221e).", "startOffset": 16, "endOffset": 20}, {"referenceID": 34, "context": "In the experiments, Imin = \u221210 is chosen to compute a lower limit 2 of the parameter values following the theoretical results given in [39] and [41].", "startOffset": 135, "endOffset": 139}, {"referenceID": 36, "context": "In the experiments, Imin = \u221210 is chosen to compute a lower limit 2 of the parameter values following the theoretical results given in [39] and [41].", "startOffset": 144, "endOffset": 148}, {"referenceID": 36, "context": "Since the classification performance of the SVM does not change for parameter values that are greater than a threshold [41], we used Imax = 10 as employed in the experimental analyses in [41].", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "Since the classification performance of the SVM does not change for parameter values that are greater than a threshold [41], we used Imax = 10 as employed in the experimental analyses in [41].", "startOffset": 187, "endOffset": 191}, {"referenceID": 8, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 147, "endOffset": 153}, {"referenceID": 0, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 154, "endOffset": 157}, {"referenceID": 20, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 159, "endOffset": 163}, {"referenceID": 37, "context": "where \u03bbmax = \u2225Hz\u0303\u2225\u221e determines the critical value of \u03bb above which the solution of the ADMM problem is 0 and \u03a9 is searched for in the interval \u03a9 \u2208 [10,1] [5], [24], [42].", "startOffset": 165, "endOffset": 169}, {"referenceID": 4, "context": "As the sparsity of the systems that generate datasets increases, lower values are calculated for \u03a9 [5], [24], [42].", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "As the sparsity of the systems that generate datasets increases, lower values are calculated for \u03a9 [5], [24], [42].", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "As the sparsity of the systems that generate datasets increases, lower values are calculated for \u03a9 [5], [24], [42].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "The absolute and relative tolerances, which determine values of upper bounds on the Euclidean norms of primal and dual residuals, are chosen as 10 and 10, respectively [24].", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "The penalty parameter is initially selected as 1 and dynamically updated at each iteration of the algorithm [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "The maximum number of iterations is chosen as 10 [5], [24].", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "The maximum number of iterations is chosen as 10 [5], [24].", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "In addition, selection of the initial value of the penalty parameter also does not affect the convergence rate if relative values of tolerance parameters are fixed [24].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "\u2225z\u0303 \u2212 Hx\u0302b\u2225 \u2264 \u03c4 is computed in order to decide whether there is an attack using the SVE and assuming a chisquare test with 95% confidence in the computation of \u03c4 [6], [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "It is worth noting that the values of \u03ba at which the phase transition occurs correspond to the minimum number of measurement variables, \u03ba, that the attacker needs to compromise in order to construct unobservable attacks [7].", "startOffset": 220, "endOffset": 223}, {"referenceID": 38, "context": "Results for Semi-supervised Learning Algorithms We use the S3VM with default parameters as suggested in [44].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Decision stumps are used as weak classifiers in Adaboost [31].", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "Each decision stump is a single-level two-leaf binary decision tree which is used to construct a set of dichotomies consisting of binary labelings of samples [31].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "We use MKL with a linear and a Gaussian kernel with the default parameters suggested in the Simple MKL implementation [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": "The details of the implementations of the OP, OPWM, Online SVM and SLR are given in [34], [35] and [45].", "startOffset": 84, "endOffset": 88}, {"referenceID": 31, "context": "The details of the implementations of the OP, OPWM, Online SVM and SLR are given in [34], [35] and [45].", "startOffset": 90, "endOffset": 94}, {"referenceID": 39, "context": "The details of the implementations of the OP, OPWM, Online SVM and SLR are given in [34], [35] and [45].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "Since the smallest number of linearly dependent measurements increases as \u03ba N increases [5], [46], the size of S decreases and the bias is decreased towards Class-1.", "startOffset": 88, "endOffset": 91}, {"referenceID": 40, "context": "Since the smallest number of linearly dependent measurements increases as \u03ba N increases [5], [46], the size of S decreases and the bias is decreased towards Class-1.", "startOffset": 93, "endOffset": 97}, {"referenceID": 41, "context": "Therefore, false negative (fn) values decrease and false positive (fp) values increase [47].", "startOffset": 87, "endOffset": 91}, {"referenceID": 30, "context": "In the results, performance values of the Online SVM and OPWM increase as the number of samples increases, since the algorithms employ margin learning approaches which provide better learning rates as the number of training samples increases [34], [45].", "startOffset": 242, "endOffset": 246}, {"referenceID": 39, "context": "In the results, performance values of the Online SVM and OPWM increase as the number of samples increases, since the algorithms employ margin learning approaches which provide better learning rates as the number of training samples increases [34], [45].", "startOffset": 248, "endOffset": 252}], "year": 2015, "abstractText": "Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semi-supervised) are employed with decision and feature level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than the attack detection algorithms which employ state vector estimation methods in the proposed attack detection framework.", "creator": "LaTeX with hyperref package"}}}