{"id": "1503.06549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "Optimum Reject Options for Prototype-based Classification", "abstract": "We analyse optimum reject strategies for prototype-based classifiers and real-valued rejection measures, using the distance of a data point to the closest prototype or probabilistic counterparts. We compare reject schemes with global thresholds, and local thresholds for the Voronoi cells of the classifier. For the latter, we develop a polynomial-time algorithm to compute optimum thresholds based on a dynamic programming scheme, and we propose an intuitive linear time, memory efficient approximation thereof with competitive accuracy. Evaluating the performance in various benchmarks, we conclude that local reject options are beneficial in particular for simple prototype-based classifiers, while the improvement is less pronounced for advanced models. For the latter, an accuracy-reject curve which is comparable to support vector machine classifiers with state of the art reject options can be reached.", "histories": [["v1", "Mon, 23 Mar 2015 08:19:17 GMT  (709kb,D)", "http://arxiv.org/abs/1503.06549v1", "19 pages"]], "COMMENTS": "19 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lydia fischer", "barbara hammer", "heiko wersing"], "accepted": false, "id": "1503.06549"}, "pdf": {"name": "1503.06549.pdf", "metadata": {"source": "CRF", "title": "Optimum Reject Options for Prototype-based Classification", "authors": ["Lydia Fischer", "Barbara Hammer", "Heiko Wersing"], "emails": [], "sections": [{"heading": null, "text": "Keywords: classification, prototype-based, distance-based, reject option, local strategies"}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Motivation", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "1.2 Related Work", "text": "The following section summarizes the state of the art for rejection options and accompanying safety measurements in the area of supervised learning. [50] The approach [50] highlights two main reasons for rejection: \u2022 Ambiguity: It is not clear how to classify the data point, e.g. the point is close to at least one decision boundary, or it is located in a region where at least two classes overlap. \u2022 Outliers: The data point differs from each data point already seen, e.g. it is caused by noise or it is an instance of an as yet unseen class or cluster. There are several approaches that explicitly address one of these reasons or a combination of both. Mostly, rejection options will be based on a measurement, the certainty whether a given data point is correctly classified. Below, we distinguish measures based on heuristics and approaches that are based on estimates of probabilities of failure or trust experiences."}, {"heading": "2 Prototype-based Classifiers", "text": "It is not only the way in which the data is based on data and data is based on data, but also the way in which the data is based on data and data is based on data and data is based on data and data is based on data and data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data is based on data on"}, {"heading": "3 Rejection Strategies", "text": "We are interested in rejection strategies for prototype-based classifiers or similar models based on two main components: 1. A security measure that assigns a certain degree of security to each data point x, indicating the certainty of the predicted class label; 2. And a strategy for rejecting a classification based on certainty; suitable rejection strategies must take into account that r (x) is not necessarily easy to interpret or uniformly scaled. That is, the exact value r (x) does not necessarily correspond to statistical certainty (which would be uniformly scaled in [0,1]), and the scaling of the value r (x) may vary depending on the location of the data point x.First, we briefly review suitable security measures r (x) before discussing optimal rejection strategies based on this."}, {"heading": "3.1 Certainty Measures", "text": "In the most recent approaches [19, 20] several safety parameters were proposed and evaluated for the prototype-based classification Q = 3. We will use three parameters that performed best in the experiments, as shown in [19, 20]: Bayesian Confidence Value: Chow analyzed the error-reject trade-off of the Bayes classification. He introduced an optimal safety parameter in the sense of error-reject trade-off [12]. The certainty parameters for a data point x \u2212 d in the case of a Bayes classifier is defined as: r (x) = Bayes (x): max 1 \u2264 j \u2264 j \u2264 Z P (j | x), where P (j | x) represents the known probability of class j for a given data point x (Fig."}, {"heading": "3.2 Global Reject Option", "text": "Suppose r (x): RM \u2192 R, x 7 \u2192 r (x) (9) refers to a level of safety where a higher value indicates higher safety. Suppose a data point x is rejected if and only ifr (x) < \u03b8. (10) An example of this refusal strategy is in Fig. 6. The refusal option works optimally if only labeling errors are rejected. Generally, this is not the case and a refusal measure leads to the rejection of some correctly classified data points along with errors. For optimal rejections, the number of false rejections should be as low as possible, while as many real rejections as possible are rejected."}, {"heading": "3.3 Local Reject Option", "text": "Global rejection options are based on the assumption that the scaling of the security level r (x) is the same for all entrances, and this assumption can be mitigated by introducing local threshold strategies. A local threshold strategy is based on dividing the entrance space into several regions and selecting the rejection threshold differently for each region; in this way, it allows a finer control of rejection [50, 18]. Following the proposal in [50], we use the natural decomposition of the entrance space into voronoi cells Vj, as introduced in Equation (2). A separate threshold is given for each voronoi cell, and the rejection option is set by a threshold vector equal to the dimension."}, {"heading": "4 Optimum Choices of Reject Thresholds", "text": "We look at ways in which we classify a threshold variant (threshold vector) by applying a threshold variant (threshold variant) optimally for a particular classification system that classifies the remaining points into a classified system. We look at ways in which we reject an option by classifying a threshold variant optimally for a particular classification system. Note: The rejection of correctly classified data is minimized. To formalize this fact and a corresponding evaluation criterion, we explain some terms that we will later use to maximize the default threshold while minimizing the rejection of correctly classified data points. Applying a classification algorithm decomposes them into a set of correctly classified data points. L and a set of incorrectly classified data points (errors). E, E = L = L = E. An optimal rejection would reject all points while we classify all points. Of course, this is usually not possible by rejecting a local or global option."}, {"heading": "4.1 Optimum Global Rejection", "text": "For a global reject option, only one parameter is selected: \"k.\" For a global reject option, only one parameter is selected: \"k.\" For a global reject option, only one parameter is created: \"k.\" For a global reject option, only one such parameter is selected: \"k.\" For a global reject option k. \"For a global reject option, only one such parameter is used:\" k. \"For a global reject option k.\" For a global reject option, only one parameter is selected: \"k.\" For a global reject option k. \"For a global reject option k.\" For a global reject option k. \"For a global reject option k.\""}, {"heading": "4.2 Optimum Local Rejection", "text": "Finding the pseudo-Pareto data in Vj is more difficult than for a global strategy, as the number of parameters (thresholds) in the optimization increases from one to two. Firstly, we will derive an optimal solution via a dynamic threshold (DP) [4, 13]. Secondly, we will introduce a faster, greedy solution that provides a good approximation to the DP. For each individual Voronoi cell, Vj, the optimal selection of a threshold and its corresponding pseudo-threshold is given exactly as for the global rejection option: We sort the security values of the points in this Voronoi cell and search for the thresholds generated by correctly classified points (possibly adding) as in Fig. 7. We use the same notation as for a global rejection option, but show via an additional index j that these values refer to Voronoi cell."}, {"heading": "4.2.1 Local Threshold Adaptation by DP", "text": "For technical reasons, it is useful to extend the range of cells to the first threshold."}, {"heading": "4.2.2 Local Threshold Adaptation by an Efficient Greedy Strategy", "text": "In this case, the number of thresholds that are not suitable for big data or online schemes is even higher. Therefore, we propose a direct, greedy approximation scheme inspired by the full DP, which leads to a (besides pre-processing) only linear method with the price of the possible sub-optimality of the solution. The basic idea is to start with the initial setting to be analogous."}, {"heading": "5 Experiments", "text": "After proposing efficient, accurate and approximate algorithms for determining optimal thresholds, we evaluate the results of the reject options for different data sets. In all cases, we use a 10-fold repeated cross-validation with ten repetitions. We evaluate the models obtained from RSLVQ, GMLVQ and LGMLVQ with one prototype per class. Thus, we can combine the models with different safety measurements depending on their performance: Since RSLVQ provides probability estimates, we can combine them with the safety measurement Conf. GMLVQ and LGMLVQ, in turn, are suitable for the safety measurement RelSim, which is already calculated during the training. We compare our results with a standard refusal measurement from SVM [36, 54] which is implemented in the LIBSVM toolbox [11]."}, {"heading": "5.1 Data Sets", "text": "For evaluation we consider the following data sets: Gaussian clusters: This data set contains two artificially created, overlapping 2D Gaussian clusters with the mean values \u00b5x = (\u2212 4.4,5), \u00b5y = (4.0.5) and standard deviations \u03c3x = (5,2,7,1) and \u03c3y = (2,5,2,1). These points are superimposed with uniform noise. Pearl chain: This data set consists of five artificially generated Gaussian clusters in two dimensions with overlapping. Means are given by \u00b5yi = 3, \u00b5x = (2,44,85,100,136), standard deviations per dimension are given by \u03c3x = (1,20,0,5,7,11), \u03c3x = \u03c3y.Image segmentation: The data set for image segmentation consists of positive positions in surgery, \u00b5x = (2,45,100,136), standard deviations per dimension are given by \u03c3x = (1,20,0,0,11), 7,x = 7,11."}, {"heading": "5.2 Comparison of DP vs. Greedy Optimization", "text": "First, we evaluate the performance of greedy optimization to calculate local scrap thresholds versus an optimal DP scheme. Results are compared in Fig. 8. Interested in heuristics \"ability to approximate optimal thresholds, ARCs are calculated based on the training set for which the thresholds are measured using DP. It is clear that the resulting curves for the data sets shown and the models provided by GMLVQ and LGMLVQ are very similar. Only for the Tecator (Haberman) dataset, the optimal DP solution beats the greedy strategy in a small region, especially for settings with a large proportion of rejected data points (which are usually less interesting in practice as almost all points in these settings are rejected). Results on the other datasets show similar behavior. We can conclude that greedy optimization provides near-optimal results for realistic settings, while requiring less time and storage complexity."}, {"heading": "5.3 Experiments on Artificial Data", "text": "This is not the first time that we have found ourselves in a situation in which we are in a situation in which we are in a crisis in which we are unable to find a solution."}, {"heading": "5.4 Experiments on Benchmarks", "text": "For these settings, we report the results obtained with an SVM and the reject option. Note: Even in the latter case, the reject capacity of the full model is about 92%, with the reject rate rising to 94%, while the reject rate rises to 10%."}, {"heading": "5.5 Medical Application", "text": "We conclude with a current example from the medical domain. Data from the adrenal glands [8] contain 147 data points consisting of 32 steroid marker values. Two classes are available: patients with benign adrenocortical adenoma (ACA) or malignant carcinoma (ACC). Our analysis of data follows the proposed evaluations in [8, 2]: We train a GMLVQ model with one prototype per class. We use the same pre-processing as in [8, 2]. Data is unbalanced with 102 ACA and 45 ACC points. Our analysis of data follows the proposed evaluations in [8, 2]."}, {"heading": "6 Conclusion", "text": "In this article, we have introduced rejection strategies for prototype-based classifiers and thoroughly evaluated the proposed methods for different sets of data. We have introduced two algorithms to derive optimal local rejection thresholds: (i) an optimal technique based on dynamic programming (DP) and (ii) a fast, greedy approach. While the first method is demonstrably optimal, the second is based on heuristics. However, we have shown that the results of both solutions are very similar, so the fast, greedy solution seems to be a reasonable choice instead of the more complex solution via DP. Their storage complexity is only linear in terms of the number of data points, while DP requires square time, and their storage complexity in terms of the number of data points, while the DPs storage size is linearly dependent on the number of data points."}, {"heading": "Acknowledgment", "text": "The authors would like to thank Stephan Hasler for the initial idea to greedily optimize local thresholds and their helpful discussion, Wiebke Arlt and Michael Biehl for providing the adrenal tumor data and their support in related issues."}], "references": [{"title": "Assessment of acrosome state in boar spermatozoa heads using n-contours descriptor and RLVQ", "author": ["E. Alegre", "M. Biehl", "N. Petkov", "L. Sanchez"], "venue": "Computer Methods and Programs in Biomedicine, 111(3):525 \u2013 536", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Urine steroid metabolomics as a biomarker  tool for detecting malignancy in adrenal tumors", "author": ["W. Arlt", "M. Biehl", "A.E. Taylor", "S. Hahner", "R. Libe", "B.A. Hughes", "P. Schneider", "D.J. Smith", "H. Stiekema", "N. Krone", "E. Porfiri", "G. Opocher", "J. Bertherat", "F. Mantero", "B. Allolio", "M. Terzolo", "P. Nightingale", "C.H.L. Shackleton", "X. Bertagna", "M. Fassnacht", "P.M. Stewart"], "venue": "Journal of Clinical Endocrinology and Metabolism, 96:3775\u20133784", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1957}, {"title": "Analysis of Flow Cytometry Data by Matrix Relevance Learning Vector Quantization", "author": ["M. Biehl", "K. Bunte", "P. Schneider"], "venue": "PLoS ONE, 8(3):e59401", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamics and Generalization Ability of LVQ Algorithms", "author": ["M. Biehl", "A. Ghosh", "B. Hammer"], "venue": "The Journal of Machine Learning Research, 8:323\u2013360", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Inter-species prediction of protein phosphorylation in the sbv IMPROVER species translation challenge", "author": ["M. Biehl", "P. Sadowski", "G. Bhanot", "E. Bilal", "A. Dayarian", "P. Meyer", "R. Norel", "K. Rhrissorrakrai", "M.D. Zeller", "S. Hormoz"], "venue": "Bioinformatics, 31(4):453\u2013461", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Matrix Relevance LVQ in Steroid Metabolomics Based Classification of Adrenal Tumors", "author": ["M. Biehl", "P. Schneider", "D. Smith", "H. Stiekema", "A. Taylor", "B. Hughes", "C. Shackleton", "P. Stewart", "W. Arlt"], "venue": "20th European Symposium on Artificial Neural Networks (ESANN), pages 423\u2013428", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "A Training Algorithm for Optimal Margin Classifiers", "author": ["B.E. Boser", "I. Guyon", "V. Vapnik"], "venue": "Proceedings of the Fifth Annual ACM Conf. on Computational Learning Theory (COLT), pages 144\u2013152", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "On Optimum Recognition Error and Reject Tradeoff", "author": ["C.K. Chow"], "venue": "IEEE Transactions in Information Theory, volume 16(1), pages 41\u201346. IEEE Transactions in Information Theory", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1970}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "Second Edition. The MIT Press and McGraw-Hill Book Company", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Support-Vector Networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, 20(3):273\u2013297", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Trans. on Information Theory, 13(1):21\u201327", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1967}, {"title": "Generating Estimates of Classification Confidence for a Casebased Spam Filter", "author": ["S.J. Delany", "P. Cunningham", "D. Doyle", "A. Zamolotskikh"], "venue": "Proceedings of the 6th international conference on Case-Based Reasoning Research and Development, ICCBR\u201905, pages 177\u2013190, Berlin, Heidelberg", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Confidence Estimation in Classification Decision: A Method for Detecting Unseen Patterns", "author": ["P.R. Devarakota", "B. Mirbach", "B. Ottersten"], "venue": "International Conference on Advances in Pattern Recognition ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Local Rejection Strategies for Learning Vector Quantization", "author": ["L. Fischer", "B. Hammer", "H. Wersing"], "venue": "Artificial Neural Networks and Machine Learning (ICANN), pages 563\u2013570", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Rejection Strategies for Learning Vector Quantization", "author": ["L. Fischer", "B. Hammer", "H. Wersing"], "venue": "22nd European Symposium on Artificial Neural Networks (ESANN), pages 41\u201346", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "and H", "author": ["L. Fischer", "B. Hammer"], "venue": "Wersing. Efficient Rejection Strategies for Prototype-based Classification", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Rejection Strategies for Learning Vector Quantization \u2013 A Comparison of Probabilistic and Deterministic Approaches", "author": ["L. Fischer", "D. Nebel", "T. Villmann", "B. Hammer", "H. Wersing"], "venue": "T. Villmann, F.-M. Schleif, M. Kaden, and M. Lange, editors, Advances in Self-Organizing Maps and Learning Vector Quantization, volume 295 of Advances in Intelligent Systems and Computing, pages 109\u2013118. Springer International Publishing", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Comprehensible Classification Models: A Position Paper", "author": ["A.A. Freitas"], "venue": "SIGKDD Exploration Newsletter, 15(1):1\u201310", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Reject option with multiple thresholds", "author": ["G. Fumera", "F. Roli", "G. Giacinto"], "venue": "Pattern Recognit., 33(12):2099\u20132101", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning and modeling big data", "author": ["B. Hammer", "H. He", "T. Martinetz"], "venue": "22th European Symposium on Artificial Neural Networks (ESANN), pages 343\u2013352", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning vector quantization for (dis-)similarities", "author": ["B. Hammer", "D. Hofmann", "F.-M. Schleif", "X. Zhu"], "venue": "Neurocomputing, 131:43\u201351", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "The Error-Reject Tradeoff", "author": ["L.K. Hansen", "C. Liisberg", "P. Salomon"], "venue": "Technical report, Electronics Institute, Technical University of Denmark, Lyngby, Denmark", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Classification with reject option", "author": ["R. Herbei", "M.H. Wegkamp"], "venue": "Canadian Journal of Statistics, 34(4):709\u2013721", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Sampling with confidence: Using k-NN confidence measures in active learning", "author": ["R. Hu", "S.J. Delany", "B.M. Namee"], "venue": "Proceedings of the UKDS Workshop at 8th International Conference on Case-based Reasoning, ICCBR\u201909, pages 181\u2013192", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Texture Feature Ranking with Relevance Learning to Classify Interstitial Lung Disease Patterns", "author": ["M.B. Huber", "K. Bunte", "M.B. Nagarajan", "M. Biehl", "L.A. Ray", "A. Wism\u00fcller"], "venue": "Artificial Intelligence in Medicine, 56(2):91 \u2013 97", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "A confidence value estimation method for handwritten Kanji character recognition and its application to candidate reduction", "author": ["E. Ishidera", "D. Nishiwaki", "A. Sato"], "venue": "International Journal on Document Analysis and Recognition, 6(4):263\u2013270", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Incremental GRLVQ: Learning Relevant Features for 3D Object Recognition", "author": ["T.C. Kietzmann", "S. Lange", "M. Riedmiller"], "venue": "Neurocomputing, pages 2868\u20132879", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "A Life- Long Learning Vector Quantization Approach for Interactive Learning of Multiple Categories", "author": ["S. Kirstein", "H. Wersing", "H.-M. Gross", "E. K\u00f6rner"], "venue": "Neural Networks, 28:90\u2013105", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Self-Organization and Associative Memory", "author": ["T. Kohonen"], "venue": "Springer Series in Information Sciences, Springer-Verlag, third edition", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1989}, {"title": "Accuracy- Rejection Curves (ARCs) for Comparing Classification Methods with a Reject Option", "author": ["M.S.A. Nadeem", "J.-D. Zucker", "B. Hanczar"], "venue": "Workshop on Machine Learning in Systems Biology (MLSB), pages 65\u201381", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Columbia Object Image Library (COIL-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "Technical Report CUCS-005-96,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1996}, {"title": "Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods. In Advances in Large Margin Classifiers, pages 61\u201374", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1999}, {"title": "Efficient Algorithms for Mining Outliers from Large Data Sets", "author": ["S. Ramaswamy", "R. Rastogi", "K. Shim"], "venue": "SIGMOD International Conference on Management of Data, pages 427\u2013438", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Machine learning for science and society", "author": ["C. Rudin", "K.L. Wagstaff"], "venue": "Machine Learning, 95(1):1\u20139", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized Learning Vector Quantization", "author": ["A. Sato", "K. Yamada"], "venue": "Adv. in Neural Inf. Proc. Syst., volume 7, pages 423\u2013429", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptive Relevance Matrices in Learning Vector Quantization", "author": ["P. Schneider", "M. Biehl", "B. Hammer"], "venue": "Neural Computation, 21(12):3532\u20133561", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Hyperparameter learning in probabilistic prototype-based models", "author": ["P. Schneider", "M. Biehl", "B. Hammer"], "venue": "Neurocomputing, 73(7-9):1117\u20131124", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularization in Matrix Relevance Learning", "author": ["P. Schneider", "K. Bunte", "H. Stiekema", "B. Hammer", "T. Villmann", "M. Biehl"], "venue": "IEEE Transactions on Neural Networks, 21(5):831\u2013840", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Soft Learning Vector Quantization", "author": ["S. Seo", "K. Obermayer"], "venue": "Neural Computation, 15(7):1589\u20131604", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic Hyperparameter Scaling Method for LVQ Algorithms", "author": ["S. Seo", "K. Obermayer"], "venue": "International Joint Conference on Neural Networks (IJCNN), pages 3196\u20133203", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "A Tutorial on Conformal Prediction", "author": ["G. Shafer", "V. Vovk"], "venue": "Journal of Machine Learning Research, 9:371\u2013421", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "The Data Replication Method for the Classification with Reject Option", "author": ["R. Sousa", "J.S. Cardoso"], "venue": "AI Communications, 26(3):281\u2013302", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "To Reject or Not to Reject: That is the Question-An Answer in Case of Neural Classifiers", "author": ["C.D. Stefano", "C. Sansone", "M. Vento"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C, 30(1):84\u201394", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2000}, {"title": "Rapid Distance-Based Outlier Detection via Sampling", "author": ["M. Sugiyama", "K.M. Borgwardt"], "venue": "Neural Information Processing Systems (NIPS), pages 467\u2013475", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Tecator data set", "author": ["H.H. Thodberg"], "venue": "contained in StatLib Datasets Archive", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1995}, {"title": "Reject Option for VQ-Based Bayesian Classification", "author": ["A. Vailaya", "A.K. Jain"], "venue": "International Conference on Pattern Recognition (ICPR), pages 2048\u20132051", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Matlab Toolbox for Dimensionality Reduction", "author": ["L.J.P. van der Maaten"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Making machine learning models interpretable", "author": ["A. Vellido", "J. Martin-Guerrero", "P. Lisboa"], "venue": "Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN), pages 163\u2013172", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Algorithmic Learning in a Random World", "author": ["V. Vovk", "A. Gammerman", "G. Shafer"], "venue": "Springer, New York", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "Probability Estimates for Multi-class Classification by Pairwise Coupling", "author": ["T.-F. Wu", "C.-J. Lin", "R.C. Weng"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2004}, {"title": "Adaptive conformal semisupervised vector quantization for dissimilarity data", "author": ["X. Zhu", "F.-M. Schleif", "B. Hammer"], "venue": "Pattern Recognition Letters, 49", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Classification constitutes one of the standard application scenarios for machine learning techniques: Its application ranges from automated digit recognition up to fraud detection, and numerous machine learning models are readily available for this task [9].", "startOffset": 254, "endOffset": 257}, {"referenceID": 30, "context": "In particular applications which require a life long learning or an adaptation to changing conditions benefit from such flexible classification models [32].", "startOffset": 151, "endOffset": 155}, {"referenceID": 10, "context": "Reject options have been pioneered by the formal framework as investigated in the approach [12]: If the costs for a misclassification versus a reject are known, one can design an optimum reject threshold based on the probability of misclassification.", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "Hence further research addresses the question whether reject options can be based on plugin rules where only empirical estimates of the misclassification probability are used [27].", "startOffset": 175, "endOffset": 179}, {"referenceID": 25, "context": "analysed in [27].", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "One principled alternative to enhance given classifiers by confidence values is offered by bootstrapping [9].", "startOffset": 105, "endOffset": 108}, {"referenceID": 51, "context": "For online learning, the theory of conformal prediction has caused quite some interest recently [53, 45].", "startOffset": 96, "endOffset": 104}, {"referenceID": 43, "context": "For online learning, the theory of conformal prediction has caused quite some interest recently [53, 45].", "startOffset": 96, "endOffset": 104}, {"referenceID": 51, "context": ", see [53, 45].", "startOffset": 6, "endOffset": 14}, {"referenceID": 43, "context": ", see [53, 45].", "startOffset": 6, "endOffset": 14}, {"referenceID": 53, "context": "[55].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "One popular example is given for the support vector machine (SVM), see the approach [36] for two-class classification and the work [54] for extensions towards multiple classes.", "startOffset": 84, "endOffset": 88}, {"referenceID": 52, "context": "One popular example is given for the support vector machine (SVM), see the approach [36] for two-class classification and the work [54] for extensions towards multiple classes.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "in the popular LIBSVM [11].", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "[7, 1, 29, 2], by offering an elegant representation which lends itself to model interpretability in a natural way [52, 22, 38].", "startOffset": 0, "endOffset": 13}, {"referenceID": 0, "context": "[7, 1, 29, 2], by offering an elegant representation which lends itself to model interpretability in a natural way [52, 22, 38].", "startOffset": 0, "endOffset": 13}, {"referenceID": 27, "context": "[7, 1, 29, 2], by offering an elegant representation which lends itself to model interpretability in a natural way [52, 22, 38].", "startOffset": 0, "endOffset": 13}, {"referenceID": 1, "context": "[7, 1, 29, 2], by offering an elegant representation which lends itself to model interpretability in a natural way [52, 22, 38].", "startOffset": 0, "endOffset": 13}, {"referenceID": 50, "context": "[7, 1, 29, 2], by offering an elegant representation which lends itself to model interpretability in a natural way [52, 22, 38].", "startOffset": 115, "endOffset": 127}, {"referenceID": 20, "context": "[7, 1, 29, 2], by offering an elegant representation which lends itself to model interpretability in a natural way [52, 22, 38].", "startOffset": 115, "endOffset": 127}, {"referenceID": 36, "context": "[7, 1, 29, 2], by offering an elegant representation which lends itself to model interpretability in a natural way [52, 22, 38].", "startOffset": 115, "endOffset": 127}, {"referenceID": 22, "context": "Further, the representation of models in terms of few representative prototypes has proved useful when dealing with online scenarios or big data sets [24, 31, 32].", "startOffset": 150, "endOffset": 162}, {"referenceID": 29, "context": "Further, the representation of models in terms of few representative prototypes has proved useful when dealing with online scenarios or big data sets [24, 31, 32].", "startOffset": 150, "endOffset": 162}, {"referenceID": 30, "context": "Further, the representation of models in terms of few representative prototypes has proved useful when dealing with online scenarios or big data sets [24, 31, 32].", "startOffset": 150, "endOffset": 162}, {"referenceID": 48, "context": "While some approaches exist to accompany nearest neighbour based classification or Gaussian mixture models (GMM) by confidence estimations [50, 28], first reject options for discriminative prototype-based methods such as learning vector quantisation have only recently been proposed [19, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 26, "context": "While some approaches exist to accompany nearest neighbour based classification or Gaussian mixture models (GMM) by confidence estimations [50, 28], first reject options for discriminative prototype-based methods such as learning vector quantisation have only recently been proposed [19, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 17, "context": "While some approaches exist to accompany nearest neighbour based classification or Gaussian mixture models (GMM) by confidence estimations [50, 28], first reject options for discriminative prototype-based methods such as learning vector quantisation have only recently been proposed [19, 21].", "startOffset": 283, "endOffset": 291}, {"referenceID": 19, "context": "While some approaches exist to accompany nearest neighbour based classification or Gaussian mixture models (GMM) by confidence estimations [50, 28], first reject options for discriminative prototype-based methods such as learning vector quantisation have only recently been proposed [19, 21].", "startOffset": 283, "endOffset": 291}, {"referenceID": 17, "context": "In this article, we will built on the insights as gained in the recent approaches [19, 21], and we will investigate how to optimally set the thresholds within intuitive reject schemes for prototype-based techniques.", "startOffset": 82, "endOffset": 90}, {"referenceID": 19, "context": "In this article, we will built on the insights as gained in the recent approaches [19, 21], and we will investigate how to optimally set the thresholds within intuitive reject schemes for prototype-based techniques.", "startOffset": 82, "endOffset": 90}, {"referenceID": 31, "context": "While the threshold selection strategies which we will investigate can be used for any prototype-based classification scheme, we will focus on the popular supervised classification technique learning vector quantisation (LVQ) and its recent more fundamental mathematical derivatives [33, 43, 40, 42].", "startOffset": 283, "endOffset": 299}, {"referenceID": 41, "context": "While the threshold selection strategies which we will investigate can be used for any prototype-based classification scheme, we will focus on the popular supervised classification technique learning vector quantisation (LVQ) and its recent more fundamental mathematical derivatives [33, 43, 40, 42].", "startOffset": 283, "endOffset": 299}, {"referenceID": 38, "context": "While the threshold selection strategies which we will investigate can be used for any prototype-based classification scheme, we will focus on the popular supervised classification technique learning vector quantisation (LVQ) and its recent more fundamental mathematical derivatives [33, 43, 40, 42].", "startOffset": 283, "endOffset": 299}, {"referenceID": 40, "context": "While the threshold selection strategies which we will investigate can be used for any prototype-based classification scheme, we will focus on the popular supervised classification technique learning vector quantisation (LVQ) and its recent more fundamental mathematical derivatives [33, 43, 40, 42].", "startOffset": 283, "endOffset": 299}, {"referenceID": 30, "context": "LVQ constitutes a powerful and efficient method for multi-class classification tasks which, due to its simple representation of models in terms of prototypes, is particularly suited for interpretability, online scenarios or life long learning [32].", "startOffset": 243, "endOffset": 247}, {"referenceID": 37, "context": "While classical LVQ models mostly rely on heuristics, modern variants are based on cost-functions such as generalized LVQ (GLVQ) [39], or the full probabilistic model robust soft LVQ (RSLVQ) [43].", "startOffset": 129, "endOffset": 133}, {"referenceID": 41, "context": "While classical LVQ models mostly rely on heuristics, modern variants are based on cost-functions such as generalized LVQ (GLVQ) [39], or the full probabilistic model robust soft LVQ (RSLVQ) [43].", "startOffset": 191, "endOffset": 195}, {"referenceID": 38, "context": "LVQ classifiers can be accompanied by strong guarantees concerning their generalization performance and learning dynamics [40, 6].", "startOffset": 122, "endOffset": 129}, {"referenceID": 4, "context": "LVQ classifiers can be accompanied by strong guarantees concerning their generalization performance and learning dynamics [40, 6].", "startOffset": 122, "endOffset": 129}, {"referenceID": 38, "context": "One particular success story links LVQ classifiers to metric learners: These enrich the classifier by feature weighting terms which opens the way towards a more flexible classification scheme, increased model interpretability, and even a simultaneous visualisation of the classifier [40, 42, 5].", "startOffset": 283, "endOffset": 294}, {"referenceID": 40, "context": "One particular success story links LVQ classifiers to metric learners: These enrich the classifier by feature weighting terms which opens the way towards a more flexible classification scheme, increased model interpretability, and even a simultaneous visualisation of the classifier [40, 42, 5].", "startOffset": 283, "endOffset": 294}, {"referenceID": 3, "context": "One particular success story links LVQ classifiers to metric learners: These enrich the classifier by feature weighting terms which opens the way towards a more flexible classification scheme, increased model interpretability, and even a simultaneous visualisation of the classifier [40, 42, 5].", "startOffset": 283, "endOffset": 294}, {"referenceID": 23, "context": "dress the setting of complex, possibly non-euclidean data which are described by pairwise similarities or dissimilarities only [25].", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "Further, also for RSLVQ, the correctness of the probability estimate is not clear since the model is not designed in order to correctly model the data probability but the conditional label probability only [6, 21].", "startOffset": 206, "endOffset": 213}, {"referenceID": 19, "context": "Further, also for RSLVQ, the correctness of the probability estimate is not clear since the model is not designed in order to correctly model the data probability but the conditional label probability only [6, 21].", "startOffset": 206, "endOffset": 213}, {"referenceID": 17, "context": "In this contribution, building on the results as recently published in [19] which proposes different real-valued certainty measures suitable for an integration in a reject option, we investigate how to devise optimum reject strategies for LVQ type classifiers, putting a particular emphasis on the choice of the threshold for a reject.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "statistical models and the SVM [10, 14].", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "statistical models and the SVM [10, 14].", "startOffset": 31, "endOffset": 39}, {"referenceID": 32, "context": "For all experiments, evaluation will rely on the full accuracy-reject curve as proposed in the approach [34].", "startOffset": 104, "endOffset": 108}, {"referenceID": 48, "context": "The approach [50] highlights two main reasons for rejection:", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "Heuristic Measures: For k-nearest neighbour (k-NN) [15] approaches a variety of simple certainty measures exist using a neighbourhood of a given data point [16, 28].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Heuristic Measures: For k-nearest neighbour (k-NN) [15] approaches a variety of simple certainty measures exist using a neighbourhood of a given data point [16, 28].", "startOffset": 156, "endOffset": 164}, {"referenceID": 26, "context": "Heuristic Measures: For k-nearest neighbour (k-NN) [15] approaches a variety of simple certainty measures exist using a neighbourhood of a given data point [16, 28].", "startOffset": 156, "endOffset": 164}, {"referenceID": 46, "context": "The approach [48] focusses on effective outlier detection, relying on the distances of a new data point from elements of a randomly chosen subset of the given data.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "The resulting method outperforms state of the art approaches such as proposed in [37] in efficiency and accuracy.", "startOffset": 81, "endOffset": 85}, {"referenceID": 44, "context": "Sousa & Cardoso [46] introduce a reject option which identifies ambiguous regions in binary classifications.", "startOffset": 16, "endOffset": 20}, {"referenceID": 45, "context": "The approach [47] addresses different neural network architectures including multi-layer perceptrons, learning vector quantisation, and probabilistic neural networks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Here an effectiveness function is introduced taking different costs for rejection and classification errors into account, very similar to the loss function as considered in [12, 27].", "startOffset": 173, "endOffset": 181}, {"referenceID": 25, "context": "Here an effectiveness function is introduced taking different costs for rejection and classification errors into account, very similar to the loss function as considered in [12, 27].", "startOffset": 173, "endOffset": 181}, {"referenceID": 34, "context": "A very popular approach to turn the activity provided by a binary SVM into an approximation of a classification confidence measure has been proposed by Platt [36].", "startOffset": 158, "endOffset": 162}, {"referenceID": 52, "context": "[54] and it is implemented in the popular LIBSVM toolbox [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[54] and it is implemented in the popular LIBSVM toolbox [11].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "As already mentioned, the approach [12] investigates optimum reject options provided the true probability density function is known.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "In the limit case, this reject strategy provides a bound for any other measure in the sense of the error-reject trade-of, as proved in [26].", "startOffset": 135, "endOffset": 139}, {"referenceID": 10, "context": "also extent Chows\u2019s rule [12] to near optimal classifiers on finite data sets, and they introduce a general scaling to compare error-reject curves of several independent experiments even with different classifiers or data sets.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "The work as presented in [23] also directly builds on [12] and more closely investigates the decomposition of data into different regions as concerns the given classes and potential errors.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "The work as presented in [23] also directly builds on [12] and more closely investigates the decomposition of data into different regions as concerns the given classes and potential errors.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "The setting that reliable class probabilities are unavailable and only empirical estimations thereof are available, is addressed in the approach [27].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "Often, GMMs are used for this purpose [17, 50].", "startOffset": 38, "endOffset": 46}, {"referenceID": 48, "context": "Often, GMMs are used for this purpose [17, 50].", "startOffset": 38, "endOffset": 46}, {"referenceID": 15, "context": "[17] extend a GMM to estimate the insecurity of a particular class membership for novel, previously unseen patterns of a new class; this estimation can yield to a reliable outlier reject option.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Vailaya & Jain [50] investigate the suitability of GMMs for both, rejection of outliers and ambiguous data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "[30] propose a suitable approximation of the probability density function for high dimensionality, which is based on a low dimensional projection of the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "We will rely on two ingredients for an efficient reject option: (I) A suitable real-valued certainty measure [19, 20] and (II) A suitable definition of how to set a threshold for rejection.", "startOffset": 109, "endOffset": 117}, {"referenceID": 18, "context": "We will rely on two ingredients for an efficient reject option: (I) A suitable real-valued certainty measure [19, 20] and (II) A suitable definition of how to set a threshold for rejection.", "startOffset": 109, "endOffset": 117}, {"referenceID": 13, "context": "Note that prototype-based models are very similar to k-NN classifiers [15] which stores all training data points as prototypes and predict a label according to the closest (k = 1) or the k closest data points.", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "Classical training techniques are often based on heuristics such as the Hebbian learning paradigm [33], yielding surprisingly good results in typical model situations, see [6].", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "Classical training techniques are often based on heuristics such as the Hebbian learning paradigm [33], yielding surprisingly good results in typical model situations, see [6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 37, "context": "More recent training schemes usually rely on a suitable cost function, including generalised LVQ (GLVQ) [39], its extension to an adaptive matrix: generalized matrix LVQ (GMLVQ) [40], its local version (LGMLVQ) [40] with local adaptive metrics, and statistical counterparts referred to as robust soft LVQ (RSLVQ) [43].", "startOffset": 104, "endOffset": 108}, {"referenceID": 38, "context": "More recent training schemes usually rely on a suitable cost function, including generalised LVQ (GLVQ) [39], its extension to an adaptive matrix: generalized matrix LVQ (GMLVQ) [40], its local version (LGMLVQ) [40] with local adaptive metrics, and statistical counterparts referred to as robust soft LVQ (RSLVQ) [43].", "startOffset": 178, "endOffset": 182}, {"referenceID": 38, "context": "More recent training schemes usually rely on a suitable cost function, including generalised LVQ (GLVQ) [39], its extension to an adaptive matrix: generalized matrix LVQ (GMLVQ) [40], its local version (LGMLVQ) [40] with local adaptive metrics, and statistical counterparts referred to as robust soft LVQ (RSLVQ) [43].", "startOffset": 211, "endOffset": 215}, {"referenceID": 41, "context": "More recent training schemes usually rely on a suitable cost function, including generalised LVQ (GLVQ) [39], its extension to an adaptive matrix: generalized matrix LVQ (GMLVQ) [40], its local version (LGMLVQ) [40] with local adaptive metrics, and statistical counterparts referred to as robust soft LVQ (RSLVQ) [43].", "startOffset": 313, "endOffset": 317}, {"referenceID": 38, "context": "GMLVQ: The Generalized Matrix Learning Vector Quantization [40] performs a stochastic gradient decent on the cost function in [39] with a more general metric d\u039b than the standard Euclidean one.", "startOffset": 59, "endOffset": 63}, {"referenceID": 37, "context": "GMLVQ: The Generalized Matrix Learning Vector Quantization [40] performs a stochastic gradient decent on the cost function in [39] with a more general metric d\u039b than the standard Euclidean one.", "startOffset": 126, "endOffset": 130}, {"referenceID": 38, "context": "The summands in this cost function are negative if and only if the classification of the corresponding point is correct, hence the costs correlate to the overall error and optimise the so-called hypothesis margin of the classifier [40].", "startOffset": 231, "endOffset": 235}, {"referenceID": 38, "context": "The algorithm which refers to these local metrics (5) is called local GMLVQ (LGMLVQ) [40].", "startOffset": 85, "endOffset": 89}, {"referenceID": 41, "context": "RSLVQ: The objective function of Robust Soft Learning Vector Quantization [43] corresponds to a statistical modelling of the setting.", "startOffset": 74, "endOffset": 78}, {"referenceID": 39, "context": "There exist schemes which also adapt the bandwidth [41, 44].", "startOffset": 51, "endOffset": 59}, {"referenceID": 42, "context": "There exist schemes which also adapt the bandwidth [41, 44].", "startOffset": 51, "endOffset": 59}, {"referenceID": 0, "context": "This means, the exact value r(x) does not necessarily coincide with the statistical confidence (which would be uniformly scaled in [0,1]), and the scaling of the value r(x) might even change depending on the location of the data point x.", "startOffset": 131, "endOffset": 136}, {"referenceID": 17, "context": "In the recent approaches [19, 20] several certainty measures have been proposed and evaluated for prototypebased classification.", "startOffset": 25, "endOffset": 33}, {"referenceID": 18, "context": "In the recent approaches [19, 20] several certainty measures have been proposed and evaluated for prototypebased classification.", "startOffset": 25, "endOffset": 33}, {"referenceID": 17, "context": "We will use three measures which scored best in the experiments as presented in [19, 20]:", "startOffset": 80, "endOffset": 88}, {"referenceID": 18, "context": "We will use three measures which scored best in the experiments as presented in [19, 20]:", "startOffset": 80, "endOffset": 88}, {"referenceID": 10, "context": "He introduced an optimal certainty measure in the sense of errorreject trade-off [12].", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "3: [18] Artificial five class data set with the contour lines of Bayes (6) (left side) and the contour lines of Conf (7) with respect to a RSLVQ model (right side, black squares are prototypes).", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "Relative Similarity: The relative similarity (RelSim) has been proposed as a certainty measure closely related to the GMLVQ cost function (3), see [39, 19].", "startOffset": 147, "endOffset": 155}, {"referenceID": 17, "context": "Relative Similarity: The relative similarity (RelSim) has been proposed as a certainty measure closely related to the GMLVQ cost function (3), see [39, 19].", "startOffset": 147, "endOffset": 155}, {"referenceID": 16, "context": "5: [18] Artificial five class data set with prototypes trained by GMLVQ (black squares) without metric adaptation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "A local threshold strategy relies on a partitioning of the input space into several regions and a different choice of the reject threshold for every region; this way, it enables a finer control of rejection [50, 18].", "startOffset": 207, "endOffset": 215}, {"referenceID": 16, "context": "A local threshold strategy relies on a partitioning of the input space into several regions and a different choice of the reject threshold for every region; this way, it enables a finer control of rejection [50, 18].", "startOffset": 207, "endOffset": 215}, {"referenceID": 48, "context": "Following the suggestion in [50], we use the natural decomposition of the input space into the Voronoi-cells Vj as introduced in Eq.", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "This multi-objective can be evaluated by a reference to the so-called accuracy reject curve (ARC) [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "First, we will derive an optimal solution via dynamic programming (DP) [4, 13].", "startOffset": 71, "endOffset": 78}, {"referenceID": 11, "context": "First, we will derive an optimal solution via dynamic programming (DP) [4, 13].", "startOffset": 71, "endOffset": 78}, {"referenceID": 34, "context": "We compare our results with a standard rejection measure of SVM [36, 54] which is implemented in the LIBSVM toolbox [11].", "startOffset": 64, "endOffset": 72}, {"referenceID": 52, "context": "We compare our results with a standard rejection measure of SVM [36, 54] which is implemented in the LIBSVM toolbox [11].", "startOffset": 64, "endOffset": 72}, {"referenceID": 9, "context": "We compare our results with a standard rejection measure of SVM [36, 54] which is implemented in the LIBSVM toolbox [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 47, "context": "Tecator: The Tecator data set [49] consists of 215 spectra of meat probes.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "Coil: The Columbia Object Image Database Library contains gray scaled images of twenty objects [35].", "startOffset": 95, "endOffset": 99}, {"referenceID": 49, "context": "The data set contains 1440 vectors with 16384 dimensions that are reduced with PCA [51] to 30.", "startOffset": 83, "endOffset": 87}, {"referenceID": 37, "context": "Thereby, RSLVQ is combined with Conf as rejection measure, while RelSim is used for deterministic LVQ models, relying on the insights as gained in the studies [39, 19, 20, 21, 18].", "startOffset": 159, "endOffset": 179}, {"referenceID": 17, "context": "Thereby, RSLVQ is combined with Conf as rejection measure, while RelSim is used for deterministic LVQ models, relying on the insights as gained in the studies [39, 19, 20, 21, 18].", "startOffset": 159, "endOffset": 179}, {"referenceID": 18, "context": "Thereby, RSLVQ is combined with Conf as rejection measure, while RelSim is used for deterministic LVQ models, relying on the insights as gained in the studies [39, 19, 20, 21, 18].", "startOffset": 159, "endOffset": 179}, {"referenceID": 19, "context": "Thereby, RSLVQ is combined with Conf as rejection measure, while RelSim is used for deterministic LVQ models, relying on the insights as gained in the studies [39, 19, 20, 21, 18].", "startOffset": 159, "endOffset": 179}, {"referenceID": 16, "context": "Thereby, RSLVQ is combined with Conf as rejection measure, while RelSim is used for deterministic LVQ models, relying on the insights as gained in the studies [39, 19, 20, 21, 18].", "startOffset": 159, "endOffset": 179}, {"referenceID": 34, "context": "For these settings, as an alternative, we report the results which are obtained with an SVM and the reject option as introduced in [36, 54].", "startOffset": 131, "endOffset": 139}, {"referenceID": 52, "context": "For these settings, as an alternative, we report the results which are obtained with an SVM and the reject option as introduced in [36, 54].", "startOffset": 131, "endOffset": 139}, {"referenceID": 34, "context": "Further, the proposed global reject option depends on the prototypes only, while the SVM technique requires a tuning of the non-linearity on the given data [36].", "startOffset": 156, "endOffset": 160}, {"referenceID": 6, "context": "The adrenal tumours data [8] contains 147", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "For further medical details we refer to [8, 2].", "startOffset": 40, "endOffset": 46}, {"referenceID": 1, "context": "For further medical details we refer to [8, 2].", "startOffset": 40, "endOffset": 46}, {"referenceID": 6, "context": "uations in [8, 2]: We train a GMLVQ model with one prototype per class.", "startOffset": 11, "endOffset": 17}, {"referenceID": 1, "context": "uations in [8, 2]: We train a GMLVQ model with one prototype per class.", "startOffset": 11, "endOffset": 17}, {"referenceID": 6, "context": "We use the same pre-processing as described in [8, 2].", "startOffset": 47, "endOffset": 53}, {"referenceID": 1, "context": "We use the same pre-processing as described in [8, 2].", "startOffset": 47, "endOffset": 53}, {"referenceID": 9, "context": "Its ARC is comparable to the ARC associated with SVM rejection computed based on LIBSVM [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "the class conditional means, following the suggestion in [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "Further, the GMLVQ model provides insight into potentially relevant biomarkers and prototypical representatives of the classes, as has been detailed in the publication [2].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "The suggested biomarkers, in particular, have been linked with biomedical insight [2].", "startOffset": 82, "endOffset": 85}], "year": 2015, "abstractText": "We analyse optimum reject strategies for prototype-based classifiers and real-valued rejection measures, using the distance of a data point to the closest prototype or probabilistic counterparts. We compare reject schemes with global thresholds, and local thresholds for the Voronoi cells of the classifier. For the latter, we develop a polynomial-time algorithm to compute optimum thresholds based on a dynamic programming scheme, and we propose an intuitive linear time, memory efficient approximation thereof with competitive accuracy. Evaluating the performance in various benchmarks, we conclude that local reject options are beneficial in particular for simple prototype-based classifiers, while the improvement is less pronounced for advanced models. For the latter, an accuracy-reject curve which is comparable to support vector machine classifiers with state of the art reject options can be reached.", "creator": "LaTeX with hyperref package"}}}