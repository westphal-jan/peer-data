{"id": "1503.06572", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "A Machine Learning Approach to Predicting the Smoothed Complexity of Sorting Algorithms", "abstract": "Smoothed analysis is a framework for analyzing the complexity of an algorithm, acting as a bridge between average and worst-case behaviour. For example, Quicksort and the Simplex algorithm are widely used in practical applications, despite their heavy worst-case complexity. Smoothed complexity aims to better characterize such algorithms. Existing theoretical bounds for the smoothed complexity of sorting algorithms are still quite weak. Furthermore, empirically computing the smoothed complexity via its original definition is computationally infeasible, even for modest input sizes. In this paper, we focus on accurately predicting the smoothed complexity of sorting algorithms, using machine learning techniques. We propose two regression models that take into account various properties of sorting algorithms and some of the known theoretical results in smoothed analysis to improve prediction quality. We show experimental results for predicting the smoothed complexity of Quicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore filling the gap between known theoretical and empirical results.", "histories": [["v1", "Mon, 23 Mar 2015 09:37:33 GMT  (1333kb,D)", "http://arxiv.org/abs/1503.06572v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CC", "authors": ["bichen shi", "michel schellekens", "georgiana ifrim"], "accepted": false, "id": "1503.06572"}, "pdf": {"name": "1503.06572.pdf", "metadata": {"source": "CRF", "title": "A Machine Learning Approach to Predicting the Smoothed Complexity of Sorting Algorithms", "authors": ["Bichen Shi", "Georgiana Ifrim"], "emails": ["bichen.shi@insight-centre.org", "m.schellekens@cs.ucc.ie", "georgiana.ifrim@insight-centre.org"], "sections": [{"heading": null, "text": "Keywords smoothed complexity \u00b7 Sorting algorithms \u00b7 Machine learning \u00b7 Regression models"}, {"heading": "1 Introduction", "text": "In fact, it is a matter of a way in which it is a matter of a way in which people live in the real world in which they move, in the world and in which they move and in which they move. (...) It is a matter of people living in the real world in which they live, in the real world and in the real world. (...) It is a matter of people living in the real world, in the real world and in the real world, in the real world and in the real world in which they live, in the real world and in the real world. (...) It is a matter of people living in the real world, in the real world and in the real world, in the real world and in the real world, in the real world, in the real world and in the real world."}, {"heading": "2 Discrete Smoothed Complexity", "text": "It is a smoothed version of the worst-case complexity that takes into account the maximum average runtime of an algorithm that acts on the disturbances of each input; the degree of disturbance is measured by a parameter of the complexity of the cases that interpolates between the worst-case and the average fall time. If the number of disturbances goes to 0, then the degree of disturbance is equal to the worst-case complexity, and in practice it is useful to understand how quickly the SC circuits switch from the worst to the average case, the less likely the worst-case comparison is in practice. SC was originally defined for ongoing cases with Gaussian perturbations (Spielman and Teng, 2001). The discrete version of SC was introduced in practice."}, {"heading": "3 Sorting Algorithms", "text": "This work focuses on the analysis and prediction of the SC of four sorting algorithms: Quicksort, M3Quicksort, Optimized Bubblesort and Mergesort. Their worst possible, average and smoothed complexity are listed in Table 2.M3Quicksort and are a variant of Quicksort. The classic version of Quicksort selects the first element of the list as the pivot point, while M3Quicksort first compares the first, the median and the last element and selects as the pivot point the element whose value is the median of three. This makes M3Quicksort 30% - 50% faster than the original algorithm. Bubblesort is easy to implement and it is also easy to track its comparisons. The normal Bubblesort has a constant runtime for all inputs, and it is not desirable as a test algorithm for the SC that analyzes the transition between the worst case and the average complexity of cases."}, {"heading": "4 Data Collection", "text": "In this section we describe our process of collecting basic truths for learning predictions (K! K!) \u3002 All of our data and codes are available on request for research purposes (N!) \u3002 There's only a limited number of data (N!) that we're able to capture the way we collect them, the way we collect them, the way we collect them in the way we collect them in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do and how they do in the way they do and how they do in the way they do and how they do in the way they do and how they do in the way they do and how they do in the way they do in the way they do and how they do in the way they do in the way they do and how they do in the way they do in the way they do and how they do in the way they do in the way they do in the way they do and how they do in the way they do in the way they do and they do in the way they do in the way they do in the way they do and they do in the way they do in the way they do in the way they do and they do in the way they do in the way they do and they do in the way they do in the way they do in the way they do in the way they do and they do in the way they do in the way they do and they do in the way they do in the way they do and they do in the way they do the way they do and they do the way they do in the way they do the way in the way they do in the way they do the way they do and they do the way in the way they do the way they do and they do the way in the way they do the way in the way they do the way they do and they do the way they do and they do the way they do the way in the way they do the way and they do the way they do the way they do the way they do the way and they do the way and they do the way they do the way they do the way and they do the way they do the way they do the way they"}, {"heading": "5 Data Analysis", "text": "Section 4 discusses collecting the basic truth data for building SC prediction algorithms, i.e., given an input list of length N and an error parameter K, we have calculated the value of SC for four sorting algorithms. For Quicksort and M3Quicksort we use the modular approach, and for Bubblesort and Mergesort, the empirical approach. Table 3 shows that the execution time for sorting algorithms will be longer if the input list length is greater. If the value of N increases, no matter what the value of N is, the value of K also increases, the SC increases as well. This is reasonable as the execution time will be longer to sort algorithms if the input list length is greater. If the value of K increases, it does not matter what value N is, the SC value decreases, it is the hybrid analysis of worst-case and average-case analysis."}, {"heading": "6 Prediction Models for the SC", "text": "In this section we present our evaluation methodology and our two approaches to predicting the SC of sorting algorithms = 30.1 evaluation MetricsWe use three classic metrics to evaluate the predictive quality of the models tested. Suppose the size of the test set is n; the actual attribute values in the test set are a1, a2,.. \u00b7 \u00b7 \u00b7 \u00b7 the predicted values on the test instances are p1, p2,., pn. The Mean Absolute Error (MAE) is (Witten et al., 2011): MAE = | p1 \u2212 a1 \u2212 a1,. \u00b7 \u00b7 pn \u2212 an \u2212 on the relative rather than absolute error values are more important, we use Mean Absolute Percentage Error (MAPE).MAPE \u2212 a1a1 \u2212 a2 a2 a2 a2 a2 a2."}, {"heading": "6.2.1 Optimization of the Proposed Feature", "text": "The new feature, expressed as (K \u2212 0.5) \u00b7 MaxRuntime, generally captures the shape of SC decreasing while K increases. We can optimize this result by parameterizing it. Using two parameters a and b, with an initial value of a = 0, b = \u2212 0.5, the new feature becomes (K + a) b) \u00b7 MaxRuntime (12) We vary the value of parameters a and b to find the best combination. We found that for lm (with characteristics (K + 2.2) \u2212 0.7) \u00b7 MaxRuntime, N and K) is trained on data with 10 \u2264 N \u2264 20.2 \u2264 K \u2264 N and tested on data of N = 40.2 \u2264 K \u2264 N, the best combination a = 2.2, b = \u2212 0.68, with a Mean Absolute Error (MAE) of 74. Additional parameters may improve the accuracy of our model, but may lead to an overmatch."}, {"heading": "6.2.2 Results", "text": "The first set contains data of 10 \u2264 N \u2264 100.2 \u2264 K \u2264 N and N increases by 5. The second set contains data of 100 \u2264 N \u2264 500.2 \u2264 K \u2264 N and N increases by 100. Table 5 shows some sample data from the first data set. We designate a training set with \u2264 N, 2 \u2264 K \u2264 N, testa \u2212 b a test set with \u2264 N \u2264 b, 2 \u2264 K \u2264 N and lma \u2212 b a lm model trained on traina \u2212 b. These notations are listed in Table 6. Figure 7 shows the predicted results from lm10 \u2212 20 to Test40 \u2212 40, the values of the data on train10 \u2212 20 and the true value of the SC from Quicksort to Test40 \u2212 40. Table 7 shows the MAE and MAPE of lm trained on different test sets. Generally speaking, the larger training set is the worse, e.g. the worse test on APN when the larger values are set to M \u2265 E."}, {"heading": "6.2.3 Discussion", "text": "The main idea behind TLR-SC is to transform a non-linear regression problem into a linear regression (possibly) by manipulating data and characteristics. The advantage is that we can use a simple algorithm - linear regression to handle a complex form of data in one step, and therefore the TLR-SC model is really simple. This model shows the importance of analyzing the data and understanding the relationship between characteristics and the predictive target. No matter how powerful the learning algorithm is (e.g. WEKA algorithms), it cannot solve the problem automatically without human knowledge. Although the TLR-SC results are very encouraging, accuracy falls quickly when the N value of the test set is large. One reason might be that the training is insufficient, so that the model cannot show its full ability to learn and predict. It is difficult to expect it to work well on test sets with N-300 if we train on only data from 20."}, {"heading": "6.3.1 NLR-SC: Fixed N", "text": "The function we use with nls isa (K N + c) b + d (14) where a, b, c, d are parameterized. Equation 14 is inspired by the function used in TLR-SC. Also, we chose KN instead of K in Equation 14, because the maximum value of K is based on the value of N, and we need the nls model to work on all N values, so it is better to use the ratio of K to N instead of increasing flexibility."}, {"heading": "6.3.2 NLR-SC: Fixed K", "text": "Submodel NLR-SC-K is created to generate the training data for nls in NLR-SC-N = = 20pages = N =. As explained in the previous section, nls requires two important training points, where K = 2 and K = N, to determine the starting point and the end point of the curve. As shown in Figure 13, when K = 2, the SC of Quicksort increases while N increases, according to the worst-case behavior O (N2). Similarly, when K = N, the SC follows the average case behavior O (N logN) N. Both experimental and theoretical results support this finding.These two patterns are so clear that we can use two lm models to adjust the curves.One model focuses on K = 2: we use N2 and N as characteristics in lm, train it on data with small N, and test it on data with large N, and fit another lm model for K = N: we use \u00d7 Log (N) and N as characteristics."}, {"heading": "8 Conclusion", "text": "In this paper we present two regression models for predicting the smoothed complexity (referred to as SC) of sorting algorithms. The first model, TLR-SC, uses linear regression to predict the complex data interface of the SC. It transfers a nonlinear relationship into a linear one by transforming the original features. TLR-SC has a simple structure and is very efficient, although it aims at quick sort and is not easily transferable to other sorting algorithms. The second model, NLR-SC, takes an iterative approach. It divides the surface problem into several curve fitting problems and by predicting curves one by one, it uses the entire interface. NLR-SC uses the theory of smoothed analysis, which results in its very good performance."}, {"heading": "9 Acknowledgements", "text": "This work was supported by Science Foundation Ireland under the 07 / CE / I1147 and 07 / IN.1 / I977 funding programmes."}], "references": [{"title": "k-means has polynomial smoothed complexity", "author": ["D. Arthur", "B. Manthey", "H. Roglin"], "venue": null, "citeRegEx": "Arthur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2009}, {"title": "Smoothed analysis of three combinatorial problems", "author": ["C. Banderier", "R. Beier", "K. Mehlhorn"], "venue": "In Mathematical Foundations of Computer Science", "citeRegEx": "Banderier et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Banderier et al\\.", "year": 2003}, {"title": "Smoothed analysis of the perceptron algorithm for linear programming", "author": ["A. Blum", "J. Dunagan"], "venue": "In Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Blum and Dunagan,? \\Q2002\\E", "shortCiteRegEx": "Blum and Dunagan", "year": 2002}, {"title": "Improved smoothed analysis of the shadow vertex simplex method", "author": ["A. Deshpande", "D.A. Spielman"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Deshpande and Spielman,? \\Q2005\\E", "shortCiteRegEx": "Deshpande and Spielman", "year": 2005}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2013}, {"title": "Modular smoothed analysis of median-of-three quicksort", "author": ["A. Hennessy", "M. Schellekens"], "venue": "Technical Report,", "citeRegEx": "Hennessy and Schellekens,? \\Q2014\\E", "shortCiteRegEx": "Hennessy and Schellekens", "year": 2014}, {"title": "The Art of Computer Programming, Volume III: Sorting and Searching", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "Knuth,? \\Q1973\\E", "shortCiteRegEx": "Knuth", "year": 1973}, {"title": "Fitting models to biological data using linear and nonlinear regression: a practical guide to curve fitting", "author": ["H. Motulsky"], "venue": "OUP USA", "citeRegEx": "Motulsky,? \\Q2004\\E", "shortCiteRegEx": "Motulsky", "year": 2004}, {"title": "A Modular Calculus for the Average Cost of Data Structuring: Efficiency-Oriented Programming in MOQA", "author": ["M. Schellekens"], "venue": null, "citeRegEx": "Schellekens,? \\Q2008\\E", "shortCiteRegEx": "Schellekens", "year": 2008}, {"title": "Modular smoothed analysis", "author": ["M. Schellekens", "A. Hennessy", "B. Shi"], "venue": "Technical Report,", "citeRegEx": "Schellekens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schellekens et al\\.", "year": 2014}, {"title": "A Machine Learning Approach For Estimating The Smoothed Complexity Of Sorting Algorithms", "author": ["B. Shi"], "venue": null, "citeRegEx": "Shi,? \\Q2013\\E", "shortCiteRegEx": "Shi", "year": 2013}, {"title": "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time", "author": ["D. Spielman", "Teng", "S.-H"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Spielman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Spielman et al\\.", "year": 2001}, {"title": "Smoothed analysis of algorithms and heuristics", "author": ["D.A. Spielman", "S. Teng"], "venue": "LONDON MATHEMATICAL SOCIETY LECTURE NOTE SERIES,", "citeRegEx": "Spielman and Teng,? \\Q2006\\E", "shortCiteRegEx": "Spielman and Teng", "year": 2006}, {"title": "Smoothed analysis of algorithms. ArXiv Mathematics e-prints", "author": ["D.A. Spielman", "Teng", "S.-H"], "venue": null, "citeRegEx": "Spielman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Spielman et al\\.", "year": 2002}, {"title": "Smoothed analysis: an attempt to explain the behavior of algorithms in practice", "author": ["D.A. Spielman", "Teng", "S.-H"], "venue": "Communications of the ACM,", "citeRegEx": "Spielman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spielman et al\\.", "year": 2009}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I.H. Witten", "E. Frank", "M.A. Hall"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "Motivated by the observation that, in practice, input parameters are often subject to a small degree of random noise, SC measures the expected performance of algorithms under slight random perturbations of the worst-case inputs (Spielman and Teng, 2006).", "startOffset": 228, "endOffset": 253}, {"referenceID": 3, "context": "The SC bounds of many algorithms have been given, including the Simplex Algorithm, that has exponential worst-case complexity but polynomial SC (Spielman and Teng, 2001; Deshpande and Spielman, 2005), Quasi-Concave Minimization, an NP-hard problem, but with polynomial SC under certain conditions (Spielman and Teng, 2009), and Quicksort, with worst-case complexity O(N2), but SC of O(p lnN), where p \u2208 [0,1] (Banderier et al.", "startOffset": 144, "endOffset": 199}, {"referenceID": 1, "context": "The SC bounds of many algorithms have been given, including the Simplex Algorithm, that has exponential worst-case complexity but polynomial SC (Spielman and Teng, 2001; Deshpande and Spielman, 2005), Quasi-Concave Minimization, an NP-hard problem, but with polynomial SC under certain conditions (Spielman and Teng, 2009), and Quicksort, with worst-case complexity O(N2), but SC of O(p lnN), where p \u2208 [0,1] (Banderier et al., 2003).", "startOffset": 409, "endOffset": 433}, {"referenceID": 9, "context": "For sorting algorithms, (Schellekens et al., 2014) has shown that the gap between the exact (empirical) value of Quicksort\u2019s SC and its known bound is significant.", "startOffset": 24, "endOffset": 50}, {"referenceID": 8, "context": "Recently, modular smoothed analysis (Schellekens, 2008) has been introduced to better estimate the value of SC for discrete cases.", "startOffset": 36, "endOffset": 55}, {"referenceID": 8, "context": "In (Schellekens, 2008) it was shown that for an algorithm that satisfies random bag preservation, its SC value can be calculated through a recurrence equation.", "startOffset": 3, "endOffset": 22}, {"referenceID": 9, "context": "Although more accurate than the theoretical bounds, modular smoothed analysis currently works only for Quicksort and its median-of-three variant (M3Quicksort) (Schellekens et al., 2014; Hennessy and Schellekens, 2014).", "startOffset": 159, "endOffset": 217}, {"referenceID": 5, "context": "Although more accurate than the theoretical bounds, modular smoothed analysis currently works only for Quicksort and its median-of-three variant (M3Quicksort) (Schellekens et al., 2014; Hennessy and Schellekens, 2014).", "startOffset": 159, "endOffset": 217}, {"referenceID": 12, "context": "For discrete cases, the SC under partial permutation perturbations is the maximum average runtime over all perturbed inputs (Spielman and Teng, 2006; Schellekens et al., 2014).", "startOffset": 124, "endOffset": 175}, {"referenceID": 9, "context": "For discrete cases, the SC under partial permutation perturbations is the maximum average runtime over all perturbed inputs (Spielman and Teng, 2006; Schellekens et al., 2014).", "startOffset": 124, "endOffset": 175}, {"referenceID": 0, "context": "Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering (Arthur et al., 2009) and Support Vector Machines (Blum and Dunagan, 2002; Spielman and Teng, 2009), however,", "startOffset": 101, "endOffset": 122}, {"referenceID": 2, "context": ", 2009) and Support Vector Machines (Blum and Dunagan, 2002; Spielman and Teng, 2009), however,", "startOffset": 36, "endOffset": 85}, {"referenceID": 4, "context": "The techniques to handle regression, curve fitting and surface fitting problems in the machine learning area are quite mature (Hastie et al., 2013).", "startOffset": 126, "endOffset": 147}, {"referenceID": 1, "context": "The discrete version of SC was introduced in (Banderier et al., 2003) and extended in (Schellekens et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 9, "context": ", 2003) and extended in (Schellekens et al., 2014).", "startOffset": 24, "endOffset": 50}, {"referenceID": 9, "context": "In this work, we use the partial permutation perturbation definition of (Schellekens et al., 2014), where \u03c3 = K N (0\u2264 K \u2264 N), N is the length of the input list:", "startOffset": 72, "endOffset": 98}, {"referenceID": 9, "context": "Definition 2 (Schellekens et al., 2014) Given a problem P with input sequence domain \u2211N , let A be an algorithm for solving P.", "startOffset": 13, "endOffset": 39}, {"referenceID": 9, "context": "1 Modular Smoothed Analysis Modular smoothed analysis was recently introduced in (Schellekens et al., 2014; Hennessy and Schellekens, 2014).", "startOffset": 81, "endOffset": 139}, {"referenceID": 5, "context": "1 Modular Smoothed Analysis Modular smoothed analysis was recently introduced in (Schellekens et al., 2014; Hennessy and Schellekens, 2014).", "startOffset": 81, "endOffset": 139}, {"referenceID": 8, "context": "For algorithms that are random bag preserving (Schellekens, 2008), the number of comparisons of the algorithm running on an input can be captured and calculated through a recurrence equation.", "startOffset": 46, "endOffset": 65}, {"referenceID": 9, "context": "The modular recurrence equation for the SC of Quicksort, f (N,K), is (Schellekens et al., 2014):", "startOffset": 69, "endOffset": 95}, {"referenceID": 5, "context": "The recurrence equation for the SC of M3Quicksort is (Hennessy and Schellekens, 2014):", "startOffset": 53, "endOffset": 85}, {"referenceID": 9, "context": "+3!(K\u22122)! ( N\u22123 K\u22123 )} (N\u2212K)! N! Modular SC values are closer to the traditional SC ones, as compared to the existing mathematical bounds (Schellekens et al., 2014).", "startOffset": 138, "endOffset": 164}, {"referenceID": 6, "context": "(Knuth, 1973).", "startOffset": 0, "endOffset": 13}, {"referenceID": 1, "context": "1 Empirical Approach Although (Banderier et al., 2003) has proven that the SC of Quicksort is O(p ln(N)), this bound is not accurate enough (see (Schellekens et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 9, "context": ", 2003) has proven that the SC of Quicksort is O(p ln(N)), this bound is not accurate enough (see (Schellekens et al., 2014) for details) to allow us training a supervised machine learning approach.", "startOffset": 98, "endOffset": 124}, {"referenceID": 9, "context": "To obtain better ground truth data for the SC, we first use an experimental approach to compute the SC exactly, by following definitions in (Schellekens et al., 2014).", "startOffset": 140, "endOffset": 166}, {"referenceID": 10, "context": "Our code uses hill climbing and Quicksort-specific worst-case permutation results (Shi, 2013), to push the input size for which we can empirically obtain SC values.", "startOffset": 82, "endOffset": 93}, {"referenceID": 9, "context": "2 Modular Smoothed Analysis Modular smoothed analysis (Schellekens et al., 2014) provides another way to calculate the SC for Quicksort and its median-of-three variant.", "startOffset": 54, "endOffset": 80}, {"referenceID": 15, "context": "The Mean Absolute Error (MAE) is (Witten et al., 2011): MAE = |p1\u2212a1|+ \u00b7 \u00b7 \u00b7+ |pn\u2212an| n (9) When the relative rather than the absolute error values are more important, we use Mean Absolute Percentage Error (MAPE).", "startOffset": 33, "endOffset": 54}, {"referenceID": 15, "context": "MAPE = | p1\u2212a1 a1 |+ | p2\u2212a2 a2 |+ \u00b7 \u00b7 \u00b7+ | pn\u2212an an | n \u00d7100% (10) We also show the Root Mean Squared Error (RMSE), which is more sensitive to outliers (Witten et al., 2011):", "startOffset": 153, "endOffset": 174}, {"referenceID": 15, "context": ", N, K, Runtime, MaxRuntime) and several built-in regression algorithms of the open source machine learning software Weka (Witten et al., 2011).", "startOffset": 122, "endOffset": 143}, {"referenceID": 7, "context": "Besides, transforming the data and features to create a linear relationship may cause the prediction error of linear regression to be distorted (Motulsky, 2004).", "startOffset": 144, "endOffset": 160}], "year": 2015, "abstractText": "Smoothed analysis is a framework for analyzing the complexity of an algorithm, acting as a bridge between average and worst-case behaviour. For example, Quicksort and the Simplex algorithm are widely used in practical applications, despite their heavy worst-case complexity. Smoothed complexity aims to better characterize such algorithms. Existing theoretical bounds for the smoothed complexity of sorting algorithms are still quite weak. Furthermore, empirically computing the smoothed complexity via its original definition is computationally infeasible, even for modest input sizes. In this paper, we focus on accurately predicting the smoothed complexity of sorting algorithms, using machine learning techniques. We propose two regression models that take into account various properties of sorting algorithms and some of the known theoretical results in smoothed analysis to improve prediction quality. We show experimental results for predicting the smoothed complexity of Quicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore filling the gap between known theoretical and empirical results.", "creator": "LaTeX with hyperref package"}}}