{"id": "1503.06666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2015", "title": "Using Generic Summarization to Improve Music Information Retrieval Tasks", "abstract": "Many Music Information Retrieval (MIR) tasks process only a segment of the whole music signal, in order to satisfy processing time constraints. This practice may lead to decreasing performance since the most important information for the task may not be in those processed segments. In this paper, we leverage generic summarization algorithms, previously applied to text and speech summarization, to summarize items in music datasets. These algorithms build summaries, that are both concise and diverse, by selecting appropriate segments from the input signal which makes them good candidates to summarize music as well. We evaluate the summarization process on binary and multiclass music genre classification tasks, by comparing the performance obtained using summarized datasets against the performances obtained using continuous segments (which is the traditional method used for addressing the previously mentioned time constraints) and full songs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA, MMR, and a Support Sets-based Centrality model improve classification performance when compared to selected 30-second baselines. We also show that these summarized datasets lead to a classification performance whose difference is not statistically significant from using full songs. Furthermore, we make an argument stating the advantages of sharing summarized datasets for future MIR research.", "histories": [["v1", "Mon, 23 Mar 2015 14:48:24 GMT  (40kb,D)", "http://arxiv.org/abs/1503.06666v1", "24 pages, 1 figure; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"], ["v2", "Thu, 3 Dec 2015 18:38:22 GMT  (346kb)", "http://arxiv.org/abs/1503.06666v2", "24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"], ["v3", "Wed, 9 Mar 2016 16:24:42 GMT  (3170kb,D)", "http://arxiv.org/abs/1503.06666v3", "24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing"]], "COMMENTS": "24 pages, 1 figure; Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.SD", "authors": ["francisco raposo", "ricardo ribeiro", "david martins de matos"], "accepted": false, "id": "1503.06666"}, "pdf": {"name": "1503.06666.pdf", "metadata": {"source": "CRF", "title": "Using Generic Summarization to improve Music Information Retrieval Tasks", "authors": ["Francisco Raposo", "Ricardo Ribeiro", "David Martins de Matos"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of us are able to set out in search of new ways to travel the world."}, {"heading": "II. MUSIC SUMMARIZATION", "text": "In this context, it should be noted that the question of whether and in what form such a measure makes sense or not is an attempt. (...) It is not the case that one can assume from the outset from the outset that one is going to start from the outset from the outset from the outset from the outset from the outset. (...) It is not the case that one is going to start from the outset from the outset from the outset from the outset from the outset from the outset. (...) It is not the case that one is going to start from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset from the outset. \""}, {"heading": "III. GENERIC SUMMARIZATION", "text": "Applying general summing algorithms to music implies segmentation of the song into musical words / terms and phrases. Since we do not take into account human aspects of music perception, we can segment songs (in sentences) according to an arbitrarily determined size. Fixed segmentation differs from structural segmentation in that it does not take into account human perception of the music structure and does not create meaningful segments (for humans). It consists of selecting a sentence size and segmenting the entire signal into sentences of the same specific size. Nevertheless, it allows us to look at the variability and repetition of the signal itself and use it to find its most important parts. As it is not aimed at human consumption, the summings generated are less prone to infringe the copyright of the original songs. This makes it easier to share data sets (instead of specific features extracted from them) for MIR research efforts, we have the following algorithmic sections evaluated."}, {"heading": "A. GRASSHOPPER", "text": "The Graph Random-Walk with Absorbing StateS that HOPs under PEaks for Ranking (GRASSHOPPER) [12] is a centrality-based method. It has been applied to the summary of texts and the analysis of social networks and focuses on improving diversity in the ranking. It takes as input a n \u00b7 n weight matrix W, which represents a graph in which each sentence is a vertex and each edge has a weight corresponding to the similarity. March 24, 2015 DRAFTbetween sentences i and j; a probability distribution r coding before the ranking (the algorithm works as a re-evaluation method); and a weighting that balances between the two. First, the matrix W is normalized in series: Oij = wij / \u2211 n k = 1wik. Then a matrix P is built in which the state N is incorporated into the state N."}, {"heading": "B. LexRank", "text": "LexRank [10] is another centrality-based method based on the similarity (usually the cosine) between pairs of sentences (usually represented as tf-idf vectors); the summary is created by taking the most central sentences from a centrality list; first, all sentences are compared with each other; then a diagram is created in which each sentence is a vertex and weighted between each set of edges (usually the similarity value must be higher than any threshold for an edge to be created); LexRank can be used on March 24, 2015, creating both the weighted (eq. 4) and the unweighted (eq. 6) edge edge; then each sentence / vertex is calculated iteratively (until convergence) according to the following equations: S (Vi) = (Vi) N + S1 (Vi) S1 (Vi) = d x."}, {"heading": "C. Latent Semantic Analysis (LSA)", "text": "LSA uses SVD to reduce the dimensionality of an original matrix representation of the text. To perform an LSA-based summary, we first build a T-term by N-sentence matrix A. Each element of A, aij = LijGi, has a local (Lij) and a global (Gi) weight. Lij is a function of the number of times a term occurs in a particular sentence, and Gi is a function of the number of sentences that contain a particular term. Normally, this translates into a matrix of tf-idf vectors, each representing a sentence. The result of applying SVD to A is A = U\u0121V T, where U (a T \u00d7 N matrix) contains the left singular vectors that contain a particular term."}, {"heading": "D. Maximal Marginal Relevance (MMR)", "text": "Sentence selection in MMR [9] is done according to its relevance and diversity compared to previously selected sentences to produce low redundancy summaries. MMR is a query-based method used in the language summary [18], [19]. It is also possible to create generic summaries by using the centric vector of all sentences as a query. MMR iteratively selects sentences that maximize the following mathematical model: \"Sim1 (Si, Q) \u2212 (1 \u2212 \u03bb) max Sj Sim2 (Si, Sj) (8) Sim1 and Sim2 may be different similarity metrics; Si are unselected sentences and Sj are previously selected; Q is the query and \u03bb is a configurable parameter with which the selection of the next sentence is based on relevance, diversity or a linear combination of the two."}, {"heading": "E. Support Sets-based Centrality", "text": "This centrality-based method was introduced in [13] and successfully applied in the text and language summary, where sentence centrality is defined by the so-called support set. A support set is the sentence that is most similar to this sentence (above a certain threshold): Si = {s-I: Sim (s, pi) > i-s 6 = pi} (9) This method calculates the support sets for each sentence and then selects the sentences that are included in the most support sets (i.e. the ones that are most recommended): argmaxs-ni = 1Si | {Si: s-Si} | (10) This is similar to the unweighted LexRank (or the degree centralization model described in Section III-B above). In degree centrality, sentences recommend each other on the basis of a globally defined similarity threshold. However, support sets allow a different threshold for each sentence (i)."}, {"heading": "IV. EXPERIMENTS", "text": "This meta-task is considered important by the MIR community, and annual conferences are held on this task, such as the International Society for Music Information Retrieval (ISMIR), which is composed of Music Information Retrieval Evaluation eXchange (MIREX) [20] to compare the performance of modern algorithms in a standardized setting. Classification is done using Support Vector Machines (SVMs) [21]. Note that there are two different steps to extract functions: the first is performed by the summarizers each time a song is summarized."}, {"heading": "A. Classification Features", "text": "The characteristics used by the SVM classifier consist of a 38-dimensional vector per song, which represents a concatenation of several statistics on characteristics used in [22]. The average of the first 20 Mel Frequency Cepstral Coefficient (MFCCs) associated with statistics (average and variance) on these spectral characteristics describes the timbral texture of the song: centric, spread, crooked, kurtosis, flow, rolloff, brightness, and entropy. These statistics are calculated across all characteristic vectors extracted on 50 ms frames without overlapping. These characteristics and a smaller group consisting solely of MFCC averages were tested in the classification task. All genres in our data set differ in time, making DRAFT these characteristics good candidates for classification. These characteristics were extracted with OpenSMILE [23]."}, {"heading": "B. Datasets", "text": "The data sets used in our experiments comprise a total of 1250 songs across 5 different genres: bass, fado, hip hop, trance and indie rock. Bass music is a generic term that refers to several specific styles of electronic music, such as dubstep, drum and bass, electro and more. Although these differ in tempo, they have similar sound characteristics such as deep bass lines and the \"wobbly\" bass effect. Fado is a Portuguese music genre whose instrumentation consists exclusively of string instruments, such as the classical guitar and the Portuguese guitar. Hip-hop consists of drum rhythms (which are usually built with samples), the use of turntables and spoken lyrics on top of it. Indie rock usually consists of guitar, drums, keyboard and vocal sounds, and has been influenced by punk, psychedelia, post-punk and country. Trance is an electronic music form that is characterised by a number of melodic and vocal phrases, each of which contains its own."}, {"heading": "C. Setup", "text": "In fact, it is such that most of us will be able to put ourselves in a different world, in which they are in a position, in which they are able to understand the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "V. RESULTS: BINARY", "text": "First, we analyze the results of the binary data sets, Bass vs. Fado and Bass vs. Trance. The reason we chose these genre pairs was because we wanted to see the effects of the summary on an easy-to-classify data set (Bass vs. Fado, because these genres are very temporally different) and a more difficult one (Bass vs. Trance, because these genres have many temporal similarities due to their electronic and dancefloor-oriented nature). Note that classification using the full 38-dimensional characteristic vector yielded better results than using only the 20 MFCCs, so we present these results only here. The best results will be in Tables I, II, III and IV. The first thing we notice in the Bass vs. Fado classification task is that the middle sections are the best continuous sections and they do a good job of distinguishing Fado from other genres."}, {"heading": "VI. RESULTS: MULTICLASS", "text": "Since we evaluate the summary extrinsically, analyzing the effects of these algorithms on the classification of music must go beyond simply comparing the final accuracy of the classification values for each scenario (as was the case with the binary classification tasks); in this context, we also look at the confusion matrices derived from the classification scenarios so that we can look at the data (in this case, you can hear the data) carefully to understand what happens when we summarize music in this way and why it improves the performance of the classification task. Since our data set consists of 250 songs per class, each confusion matrix series must yield 250. Classes are sorted identically in both rows and columns, meaning that ideally we have a diagonal confusion matrix (all zeros, except the diagonal elements, which should be all 250)."}, {"heading": "A. Full songs", "text": "First, we look at the confusion matrix resulting from the classification of complete songs (Table V). We can see that although there is some confusion between it and indie rock, fado is the most distinguishable genre within this group of genres, which makes sense because it is very different from any other genre. Trance and bass also achieve accuracies of over 90%, although they also share a certain confusion, which is explained by the fact that both genres are electronic music styles that share many of the timbre characteristics derived from the virtual instruments used in their production. The classifier performs worse when it classifies hip hop and indie rock, achieving accuracy of 84% and confusing both genres in about 10% of the tracks. This can also be explained by the fact that both genres have strong vocal presence (unlike bass and trance). Although Fado has an important vocal component, overall rock has very different instrumentation, and this instrumentation is very intuitive."}, {"heading": "B. Baseline segments", "text": "Table VI shows the results obtained by using only the 30 seconds from the beginning of the songs. Table VII shows the comparison of the opening sections with the full songs. The classification accuracy is 77.52%, which represents a decrease of 12.32 ppm compared to the use of full songs. Bass accuracy on March 24, 2015 DRAFTdecreased by 19.6 ppm, due to increasing confusion with both hip hop and indie rock. Trance was also more confused with indie rock. This is easily explained by the fact that the first 30 seconds of most bass or trance songs correspond to the intro part. These intros are energy-saving parts, which may contain a relatively strong vocal presence and much less instrumentation than other, more characteristic parts of the genres. These intros are much more similar to hip-hop and indie-rock intros than when looking at the whole songs, which explains why the classifier may confuse these classes more in this scenario than other, more distinctive parts of the genre."}, {"heading": "B -49 6 27 28 -12 -19.6%", "text": "Tables VIII and IX show the results obtained in classifying the middle 30 seconds of the songs and comparing these segments against the use of the full songs. Overall, the accuracy was 81.36%, which represents a decrease of 8.48 ppm compared to the baseline of the full songs. This time, both the bass and trance segments dropped by 16.8 pp. and 20.4 pp. respectively, and were confused by the classifier. After listening to the tracks that were mixed up in this way, the conclusion is as expected: these middle segments correspond to what is called the breakdown part of the songs. These segments correspond to lower energy segments (though not as low as an intro) of the tracks, which are not the DRAFT characteristic parts of these two genres on March 24, 2015, and, in the particular case of bass vs. trance, they are very similar in time because of their electronic character."}, {"heading": "B -42 0 12 3 27 -16.8%", "text": "Tables X and XI show the results obtained in classifying the last 30 seconds of a song and comparing those segments to the complete songs, with final sections achieving an accuracy of 76.8%, which is 13.04 pages lower than the complete songs. Again, the bass was misclassified mainly as hip hop and indie rock, and trance was misclassified mainly as bass, mainly because the last 30 seconds correspond to the outro section of the song, which has many similarities to the intro section. In trance and bass, the outro also features the puncture sections, and the fade-repeat effect at the end of many songs also adds to this confusion, meaning that the last 30 seconds of a song is not a good strategy for selecting the segments.March 24, 2015 DRAFT"}, {"heading": "B -48 3 26 18 1 -19.2%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C. Baseline Assessment", "text": "Although it seems from the above experiments that it is better to take the middle sections of the songs than the beginning or the end, it is still not good enough, at least not for all the genres considered. The characteristics used by the classifier are statistics (averages and deviations) of the characteristics that are extracted along the entire signal. These characteristics work well if you take the entire signal as input, which means that these averages and deviations should be similar to achieve a similar performance. This cannot be guaranteed at 30-second, continuous clips, since these 30 seconds happen to belong to a single (and not distinctive) structural part of the song (such as intro, breakdown and outro). If that is the case, then there is not enough diversity in the segment / summary to accurately reproduce the entire song. Moreover, some music genres can be distinguished precisely only by some of these structural parts: the best examples in this dataset are the bass and trance sections, which we should make much better differentiated from each other in 2015, and their second AFs sections."}, {"heading": "D. GRASSHOPPER", "text": "The following tables show the results obtained in classifying the data set using summaries extracted from the GRASSHOPPER algorithm. Table XII shows the specific parameter values used in this experiment: (0,5,0.5) second framing, 25-word vocabulary, 10-word sentences and binary weighting. The overall accuracy was 88.16%. Table XIII shows a comparison with the best 30-second baseline - the middle sections. As can be seen, the GRASSHOPPER summary has recovered most of what was lost by the middle sections in terms of classification accuracy for each class. Because the middle sections performed so poorly in distinguishing bass and trance, these summaries can even improve accuracy for both classes by 14.0 percentage points, with the diversity of each section being 14.4 percentage points higher than the structural one."}, {"heading": "E. LexRank", "text": "Tables XIV and XV show the LexRank confusion matrix and its difference from the middle sections. The parameter values in this experiment were: (0.5,0.5) seconds of framing, 25-word vocabulary, 5-word sentences, and a subdued tf-idf weighting. The overall accuracy was 88.40%. LexRank also significantly improved the classification accuracy compared to the middle sections (7.04 pages in total), namely for bass and trance, with an increase of 15.6 pages and 15.2 pages respectively. LexRankMarch 24, 2015 DRAFTW clearly selects different parts to be included in the 30-second summaries, as we saw when listening to them. It is also interesting that the classifier did better in another class than with full songs individually: The accuracy of the indie rock increased by 1.6 points."}, {"heading": "F. LSA", "text": "Tables XVI and XVII correspond to the confusion matrix of the LSA summary and the corresponding difference to the middle paragraphs. The following combination of parameters was used by DRAFT on March 24, 2015: (0,5,0,5) seconds of framing, 25-word vocabulary, 10-word sentences and binary weighting. Note that the use of term rate-based weighting on LSA, when applied to music, significantly deteriorates its performance, which is highly undesirable since these paragraphs achieve a very poor rating in describing this song on any latent topic, which results in them being included in the summaries. Moreover, taking into account the frequency of the reversed document, the results are even worse, because these loud terms are usually found in very few sentences, which is highly undesirable as these paragraphs do a very poor job in describing this song in any aspect as regards binary weighting."}, {"heading": "G. MMR", "text": "Table XVIII represents the confusion matrix for an MMR summary, and Table XIX shows its difference from the middle sections. (0.5,0.5) Seconds of framing was used, along with a 50-word vocabulary, 10-word sentences, 0.7 \u03bb value, and a subdued tf-idf weighting. Note that although every other parameter setup (for the other algorithms shown here) uses 20 MFCCs as characteristics, it uses the same MFCCs associated with the 9 spectral characteristics that are also used for classification (described in Section IV-A). This is because MMR, unlike all other summing algorithms shown here, performed better with this set (instead of using only MFCCs as characteristics).The overall accuracy was 88.80%, an improvement of 7.44 percentage points over the middle sections. Bass and Trance profited the most from the summary, respectively, making 16.8 percent and 16.4 percent improvements."}, {"heading": "H. Support Sets", "text": "Table XX shows the confusion matrix achieved in classifying the dataset using summaries extracted by the support rate algorithm. Specific parameters of this experiment were: (0,5,0,5) sec-March 24, 2015 DRAFTond framing, 25-word vocabulary, 10-word sentences, muted tf-idf weighting, and passage-based heuristics for generating the support rates [13] using cosinal similarity. The overall accuracy was 88.80%. Table XXI shows the comparison with the mean sections. Again, the summary recovered most of what the mean sections lacked in terms of classification accuracy for each class, which greatly affected bass and trance, with 10.8 pp. and 16.8 pp. increases, respectively. Listening to some of these summaries, we confirmed the diversity they contained, which clearly lacked in the mean sections."}, {"heading": "VII. DISCUSSION", "text": "In fact, most of them will be able to orient themselves in a different direction than in a different direction, namely the direction in which they have been moving."}, {"heading": "VIII. CONCLUSIONS AND FUTURE WORK", "text": "We have shown that generic summary algorithms do well when it comes to summarizing music records that are to be classified, and the resulting summaries are remarkably more descriptive than their continuous segments (of the same duration), and these summaries are sometimes even more discriminatory than the complete songs. We also presented an argument that demonstrates some advantages in sharing summarized data sets within the MIR community. An interesting direction of research would be to automatically determine which vocabulary sizes would fit best for each song individually, so that better modeling of the acoustic vocabulary can be used. To further confirm these conclusions, a test of the performance of the summary in different classification tasks (e.g. with more classes) is also needed. Further experiments should be performed on other MIR tasks (besides classification) that also use only a portion of the entire signal."}], "references": [{"title": "Semantic Segmentation and Summarization of Music: Methods Based on Tonality and Recurrent Structure", "author": ["W. Chai"], "venue": "IEEE Signal Processing Magazine, vol. 23, no. 2, pp. 124\u2013132, 2006. March 24, 2015  DRAFT  SUBMITTED TO IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL. X, NO. X  24", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Summarizing Popular Music via Structural Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Proc. of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2003, pp. 127\u2013130.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward Automatic Music Audio Summary Generation from Signal Analysis", "author": ["G. Peeters", "A. La Burthe", "X. Rodet"], "venue": "Proc. of the 3rd International Society for Music Information Retrieval Conference, 2002, pp. 94\u2013100.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Signal-based Music Structure Discovery for Music Audio Summary Generation", "author": ["G. Peeters", "X. Rodet"], "venue": "Proc. of the 29th International Computer Music Conference, 2003, pp. 15\u201322.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Music Summary using Key Phrases", "author": ["S. Chu", "B. Logan"], "venue": "Hewlett-Packard Cambridge Research Laboratory, Tech. Rep., 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic Music Summarization via Similarity Analysis", "author": ["M. Cooper", "J. Foote"], "venue": "Proc. of the 3rd International Society for Music Information Retrieval Conference, 2002, pp. 81\u201385.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Automatic Music Summarization: A \u201dThumbnail\u201d Approach", "author": ["J. Glaczynski", "E. Lukasik"], "venue": "Archives of Acoustics, vol. 36, no. 2, pp. 297\u2013309, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio Thumbnailing of Popular Music using Chroma-based Representations", "author": ["M.A. Bartsch", "G.H. Wakefield"], "venue": "IEEE Transactions on Multimedia, vol. 7, no. 1, pp. 96\u2013104, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "Proc. of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998, pp. 335\u2013336.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research, vol. 22, pp. 457\u2013479, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dutnais"], "venue": "Psychological Review, vol. 104, no. 2, pp. 211\u2013240, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving Diversity in Ranking using Absorbing Random Walks", "author": ["X. Zhu", "A.B. Goldberg", "J.V. Gael", "D. Andrzejewski"], "venue": "Proc. of the 5th North American Chapter of the Association for Computational Linguistics - Human Language Technologies Conference, 2007, pp. 97\u2013104.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity", "author": ["R. Ribeiro", "D.M. de Matos"], "venue": "Journal of Artificial Intelligence Research, vol. 42, pp. 275\u2013308, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "On the Application of Generic Summarization Algorithms to Music", "author": ["F. Raposo", "R. Ribeiro", "D.M. de Matos"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 1, pp. 26\u201330, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic Music Classification and Summarization", "author": ["C.X. Xu", "N.C. Maddage", "X.S. Shao"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 13, no. 3, pp. 441\u2013450, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proc. of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2001, pp. 19\u201325.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Using Latent Semantic Analysis in Text Summarization and Summary Evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proc. of ISIM, 2004, pp. 93\u2013100.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Minimizing Word Error Rate in Textual Summaries of Spoken Language", "author": ["K. Zechner", "A. Waibel"], "venue": "Proc. of the 1st North American Chapter of the Association for Computational Linguistics Conference, 2000, pp. 186\u2013193.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Extractive Summarization of Meeting Recordings", "author": ["G. Murray", "S. Renals", "J. Carletta"], "venue": "Proc. of the 9th European Conference on Speech Communication and Technology, 2005, pp. 593\u2013596.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, no. 3, pp. 27:1\u201327:27, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Using Timbre Models for Audio Classification", "author": ["F. de Leon", "K. Martinez"], "venue": "Submission to Audio Classification (Train/Test) Tasks of MIREX 2013, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Recent Developments in openSMILE, the Munich Open-source Multimedia Feature Extractor", "author": ["F. Eyben", "F. Weninger", "F. Gross", "B. Schuller"], "venue": "Proc. of the 21st ACM International Conference on Multimedia, 2013, pp. 835\u2013838.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Armadillo: An Open Source C++ Linear Algebra Library for Fast Prototyping and Computationally Intensive Experiments", "author": ["C. Sanderson"], "venue": "NICTA, Tech. Rep., 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "MARSYAS: A Framework for Audio Analysis", "author": ["G. Tzanetakis", "P. Cook"], "venue": "Organised Sound, vol. 4, no. 3, pp. 169\u2013175, 1999.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "MLPACK: A Scalable C++ Machine Learning Library", "author": ["R.R. Curtin", "J.R. Cline", "N.P. Slagle", "W.B. March", "P. Ram", "N.A. Mehta", "A.G. Gray"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 801\u2013805, 2013. March 24, 2015  DRAFT", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Music summarization has been the subject of research for at least a decade and many algorithms that address this problem, mainly for popular music, have been published in the past [1]\u2013[8].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "INTRODUCTION Music summarization has been the subject of research for at least a decade and many algorithms that address this problem, mainly for popular music, have been published in the past [1]\u2013[8].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "Generic summarization algorithms, however, focus on extracting concise and diverse summaries and have been successfully applied in text and speech summarization [9]\u2013[13].", "startOffset": 161, "endOffset": 164}, {"referenceID": 12, "context": "Generic summarization algorithms, however, focus on extracting concise and diverse summaries and have been successfully applied in text and speech summarization [9]\u2013[13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 71, "endOffset": 74}, {"referenceID": 12, "context": "We summarize music using GRASSHOPPER [12], LexRank [10], LSA [11], MMR [9], and Support Sets [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "These results complement and solidify previous work evaluated on a binary Fado classifier [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "In [1], segmentation is achieved by using a Hidden Markov Model (HMM) to detect key changes between frames and Dynamic Time Warping (DTW) to detect repeating structure.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2], a Gaussian-tempered \u201ccheckerboard\u201d kernel is correlated along the main diagonal of the song\u2019s self-similarity matrix, outputting segment boundaries.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [3] and [4], songs are segmented in 3 stages.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [3] and [4], songs are segmented in 3 stages.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "In [5], clustering is used to group and label similar segments of the song.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [6]", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "and [7], Average Similarity is used to extract a thumbnail L seconds long that is the most similar to the whole piece.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "Another method for this task, Maximum Filtered Correlation [8], starts by building a similarity matrix and then a filtered time-lag matrix, embedding the similarity between extended segments separated by a constant lag.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "In [15], music is classified as pure or vocal, in order to perform type-specific feature extraction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "The Graph Random-walk with Absorbing StateS that HOPs among PEaks for Ranking (GRASSHOPPER) [12] is a centrality-based method.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "LexRank [10] is another centrality-based method relying on the similarity (usually, the cosine) between sentence pairs (usually, represented as tf-idf vectors).", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "LSA was first applied in text summarization in [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "In [17], two limitations of this approach are discussed: if selecting K equal to the number of sentences in the summary, when it increases, the summary tends to include less significant sentences; and that sentences with high values in several dimensions (topics), but never the highest, will never be included", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "Sentence selection in MMR [9] is done according to their relevance and diversity against previously selected sentences, in order to output low-redundancy summaries.", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "MMR is a query-based method that has been used in speech summarization [18], [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "MMR is a query-based method that has been used in speech summarization [18], [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "This centrality-based method was introduced in [13] and was successfully applied in text and speech summarization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "In [13], heuristics are explored to define the support sets, specifically, a passage order heuristic which clusters all passages into two clusters, according to their distance to each cluster\u2019s centroid.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Classification is performed using Support Vector Machines (SVMs) [21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "The features used by the SVM classifier consist of a 38-dimensional vector per song, which is a concatenation of several statistics on features used in [22].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "These features were extracted using OpenSMILE [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "Every algorithm was implemented in C++ using the following libraries: OpenSMILE [23] for feature extraction (in summarization), Armadillo [24] for matrix operations, and Marsyas [25] for synthesizing the summaries.", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "Every algorithm was implemented in C++ using the following libraries: OpenSMILE [23] for feature extraction (in summarization), Armadillo [24] for matrix operations, and Marsyas [25] for synthesizing the summaries.", "startOffset": 138, "endOffset": 142}, {"referenceID": 23, "context": "Every algorithm was implemented in C++ using the following libraries: OpenSMILE [23] for feature extraction (in summarization), Armadillo [24] for matrix operations, and Marsyas [25] for synthesizing the summaries.", "startOffset": 178, "endOffset": 182}, {"referenceID": 24, "context": "mlpack\u2019s [26] implementation of the K-Means algorithm was used for this step (we experiment with", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "These features, used in several previous research efforts on music summarization in [1]\u2013[7], describe the timbre of an acoustic signal.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "These features, used in several previous research efforts on music summarization in [1]\u2013[7], describe the timbre of an acoustic signal.", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "onds framing, 25-word vocabulary, 10-word sentences, dampened tf-idf weighting, and the passage orderbased heuristic for creating the support sets [13] using the cosine similarity.", "startOffset": 147, "endOffset": 151}], "year": 2015, "abstractText": "Many Music Information Retrieval (MIR) tasks process only a segment of the whole music signal, in order to satisfy processing time constraints. This practice may lead to decreasing performance since the most important information for the task may not be in those processed segments. In this paper, we leverage generic summarization algorithms, previously applied to text and speech summarization, to summarize items in music datasets. These algorithms build summaries, that are both concise and diverse, by selecting appropriate segments from the input signal which makes them good candidates to summarize music as well. We evaluate the summarization process on binary and multiclass music genre classification tasks, by comparing the performance obtained using summarized datasets against the performances obtained using continuous segments (which is the traditional method used for addressing the previously mentioned time constraints) and full songs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA, MMR, and a Support Sets-based Centrality model improve classification performance when compared to selected 30-second baselines. We also show that these summarized datasets lead to a classification performance whose difference is not statistically significant from using full songs. Furthermore, we make an argument stating the advantages of sharing summarized datasets for future MIR research.", "creator": "LaTeX with hyperref package"}}}