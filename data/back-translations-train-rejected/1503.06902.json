{"id": "1503.06902", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2015", "title": "A Note on Information-Directed Sampling and Thompson Sampling", "abstract": "This note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers.", "histories": [["v1", "Tue, 24 Mar 2015 03:26:28 GMT  (19kb)", "http://arxiv.org/abs/1503.06902v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["li zhou"], "accepted": false, "id": "1503.06902"}, "pdf": {"name": "1503.06902.pdf", "metadata": {"source": "CRF", "title": "A Note on Information-Directed Sampling and Thompson Sampling", "authors": ["Li Zhou"], "emails": ["lizhou@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.06 902v 1 [cs.L G] 24 Mar 2This note introduces three Bayesian-style multi-armed bandit algorithms: information-driven sampling, Thompson sampling, and Generalized Thompson sampling. The aim is to provide an intuitive explanation for these three algorithms and their limitations of regret, and provide some derivatives omitted from the original essays."}, {"heading": "1 Introduction", "text": "The problem of armed bandits [1] is one of the sequential decision-making problems. Each time, the learner selects an action based on his current knowledge and policy of weapon selection and then receives a reward for the actions chosen. Since the rewards of actions that are not selected are unknown, the learner must weigh up the use of his current knowledge to select the best arm and potentially explore the best weapons. In this note, we describe three multi-armed bandit algorithms in Bavarian style: information-driven sampling [2], Thompson's sampling [3] and generalized Thompson sampling [4]. Each of these three algorithms maintains a posterior distribution that indicates the probability that each arm / policy is optimal. However, they have different rules for updating this posterior distribution based on the observed rewards."}, {"heading": "2 Information-Directed Sampling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Formulation", "text": "Information-oriented sampling (IDS) [2] takes into account a Bayesian formulation of the problem of multi-armed bandits. In this context, there are a number of actions (weapons) A \u0445, and in due course the decision-maker chooses an action. To then draw a reward ra, t from a reward distribution pa1, we assume that all rewards i.i.d are distributed and the reward distribution is stationary in relation to the time t [1, T]. To formulate a multi-armed bandit in the Bayesian way, we refer to an \"argmaxa,\" an \"era\" pa [ra], meaning that an \"arm with the highest expected reward in terms of distribution pa [1, T] where an\" A \"is designated. We also refer to the reward that comes from pa.\" The decision-maker does not know the real1In the original paper, they assume that the weapons will first result from a function, then draw a fixed one here."}, {"heading": "2.2 Algorithm", "text": "On the issue of heavily armed bandits, we want to strike a balance between exploitation and exploration. IDS handles this trade-off by defining instant regret (a) and information gains (a) through action (a) in due course."}, {"heading": "2.2.1 Immediate Regret", "text": "The immediate regret is defined as follows:..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2.2.2 Information Gain", "text": "To do this, IDS defined a term: information gain, which is called gt (a). The idea behind this is that we already have a posterior distribution over a single arm, and we hope that after we pull one of the arms, the entropy of this distribution decreases so that we get a certain amount of information about which arm has the highest expected reward. If we let one arm t + 1 and one arm t + 1 (5) and let H (a) denote the entropy of a poor t, then gt (a) is defined as arm t (a) = E [H (a) - H (a) - H (a) - H (a) - H (a) - 1) - (5) The expectation is in relation to the random reward of arm a - and the reward of arm A. To calculate this, one can feel reward of p - a and then calculate the expectation of Keria."}, {"heading": "2.2.3 Optimization", "text": "There are many ways to do this, and in the essay the author chooses the following way: \u03c0IDSt = argmin\u03c0 \u0445 D (A) {\u0435t (\u03c0) 2 gt (\u03c0)} (10) Note that \u03c0 is a distribution across all arms, and if the author assumes that g has at least 1 non-zero elements, then it is the same to solve the following optimization problem: Minimize the number of elements: = (\u03c0T) 2 \u03c0T g (11), subject yourself to the assumption that g has at least 1 non-zero elements. The author explained that \u03c0 can be very economical, with only two non-zero elements, and then try all sorts of combinations of two arms that give the lowest value. Considering that the IDS tries one arm and pulls that arm. I omit the detail here because it is well described in the IDS essay."}, {"heading": "2.3 Bernoulli Bandit Experiment", "text": "In a Bayesian learning algorithm, it is standard to model the mean reward of each arm on the basis of the beta distribution (XII, \u03b22i) (14) ri (Bernoulli, \u03b22i) (15) To calculate the mean reward of each arm, we first calculate (a). Let us leave fi = Beta.pdf (x, \u03b22i) and Fi = Beta.cdf (x, \u03b22i) and Fi = Beta.pdf (x, \u03b22i) and Fi = Beta.pdf (x, \u03b22i) and Fi = Beta.cdf (x, \u03b22i) and Fi = Beta.cdf (x, \u03b22i) for all arms i, that is, Xi and Fi are the PDF and CDF of the back part of Xi, then we will do the calculation."}, {"heading": "2.4 Regret Bound", "text": "For a defined deterministic \u03bb-R and a policy \u03c0 at the border between repentance (T, \u03c0) we have E [Regret (T, \u03c0)] \u2264 \u03bbH (\u03b11) T (35) Proof: ET = 1gt (\u03c0t) = E T = 1E [H (\u03b1t) \u2212 H (\u03b1t + 1) | Ft \u2212 1] (36) = ET; t = 1 (H (\u03b1t) \u2212 H (\u03b1t + 1)) (37) = H (\u03b11) \u2212 EH (\u03b1T + 1) (38) \u2264 H (\u03b11) (39) By definition not at the border of repentance, i.e. not at the border of definition, but at the border of repentance (E)."}, {"heading": "2.5 Potential Problems", "text": "I think the main problem is that the algorithm is very time consuming while I run it, because it has 3 integrals that we have to calculate, so we have to evaluate each integrand on a separate point network. Another problem is that the paper does not mention why you choose such a format as the trade-off between land use plan and land use plan, as there are many ways to resolve this trade-off, and it would also be nice to see some generalization on contextual land use plan."}, {"heading": "3 Thompson Sampling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Formulation", "text": "Thompson sampling (TS) [3, 5] is also a Bavarian-style bandit algorithm, it can be applied to both contextual bandits and standard multi-armed bandit problems. At this point we are talking about the non-contextual version. Again, we assume that there is an action set A, and in due course step t Thompson sampling selects action a and receives reward ra, t. We also assume that the reward of each arm ra follows a certain parametric distribution pa = P (r | a, \u03b8a) with an average \u00b5a, with phenomena being the parameters. Define past observations D consists of drawn and observed rewards. At the beginning Thompson sampling assumes a previous distribution of parameters succa, and then after each time step updates the posterior distribution P (preva | D) on the basis of past observations. Similar to IDS, the aim is to minimize regret to comply with Regregret: [E = T = T \u2212 T = T is highest."}, {"heading": "3.2 Algorithm", "text": "Similar to IDS, Thompson's sample randomly selects an action a according to its probability to be optimal. Thus, action a is chosen with the probability \u0430 I [E (r | a, \u03b8) = max a'E (r | a, \"\u03b8)] P (\u03b8 | D) d\u03b8 (45), which is just as important as \u03b1t in the IDS. However, the calculation of \u03b1t is time-consuming, and since we do not have to use \u03b1t explicitly in Thompson's sample and we only need samples from \u03b1t, it is sufficient to draw a random parameter from the posterior distribution. Algorithm 1 describes the procedure of the Thompson sample with Bernoulli bandit problem. Algorithm 1 Thompson sample with Bernoulli bandit request: \u03b1, \u03b2: previous parameter of a beta distribution For each arm i = 1,..., K sample Si = 0, Fi = 0 for t = 1,..., T sample for Beti = 1, Beta + Beta sample: Beta + Beta = 1 point, Beta + Beta = 1 point (Beta = 1 point) for each arm."}, {"heading": "3.3 Regret", "text": "Although Thompson sampling is a very old algorithm proposed by [6], but the theoretical analysis is only carried out recently. We follow [5] and hope to give an intuitive explanation of regret. Let's write the expected total regret in time T + 1 as we need the following settings [5]: Define F n, p () the cdf and fBn, p () the pdf of the binomial distribution with the parameters i, p (p) the cdf and fBn, p (p) the pdf and fBn, p (p) the pdf of the binomial distribution with the parameters n, p (p) the pdf of the betafle ki, p) the cdf of the distribution i, i."}, {"heading": "4 Generalized Thompson Sampling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Problem Formulation", "text": "Generalized Thompson sampling [4] is a contextual bandit problem, it resembles an expert learning framework and includes Thompson sampling as a special case. Let X and A be the context and the weapons, and let K = | A |. In step 1,..., T the decision maker observes the context xt-X and selects an arm on A. Then he receives a reward at c = 0, 1}, with the expectation \u00b5 (xt, at). In [4] the reward is binary, but it is easy to generalize to the continuous space. Unlike the classic Thompson sampling algorithm, generalized Thompson sampling allows the decision maker access to a number of experts E = {E1, E2,..., En}, each E gives predictions about the average reward \u00b5 (xt, at)."}, {"heading": "4.2 Algorithm", "text": "Generalized Thompson sampling is described in algorithm 2. We can see that it updates the weight wi, t + 1 by wi, t + 1, p (\u2212 \u03b7 (fi (xt, at), rt), where the loss function is. The term \"generalized\" in \"Generalized Thompson sampling\" means that we can use various types of loss functions when updating wi. [4] describes two loss functions: logarithmic loss and square loss. Logarithmic loss is defined as (r, r) = 1 (r = 1) ln 1 / r + 1 (r = 0) ln (1 / (1 \u2212 r)), and square loss is defined as (r, r) = (r \u2212 r) 2. In the next section we will show that if the loss function is logarithmic loss, a logarithmic loss is (r = 1) ln (r = 0) ln (r = 0) ln (r = 0) ln (1 \u2212 r) ln (1 \u2212 r) (1 \u2212 r), and square loss is (r)."}, {"heading": "4.3 Connection with Expert-Learning and Thompson Sampling", "text": "There are two ways to see this, and in both ways we must assume that the loss is a log loss, that is, that an expert f \u2212 \u2212 \u2212 \u2212 \u2212 p1 predicts that the probability of r = 1 is p1 and the probability of r = 0 is 1 \u2212 p1, then the log loss of each expert is ln 1p1 if reward is 1 \u2212 p1, and ln 1 \u2212 p1 if reward is 0. The first way to see this is: we can imagine the Generalized Thompson sampling as maintaining a posterior distribution of each expert, which is called wt. This posterior distribution can be interpreted as the posterior probability that fi is the reward maximizing expert. The update rule, for one step, iswi + 1, t exp (fi, at), rt exp (fi, at), rt exp (66), t exp (lp)."}, {"heading": "4.4 Regret", "text": "The basic idea of the derivative is that we assume a relationship between the loss function and regret: let us define immediate regret that there is a constant k1, so that regret (x, a) \u2212 (f, a) and average displaced loss l = Ert, in [\u2211 iwi, tl, i (r | xt, at)] we assume that there is a constant k1, so that there is a constant k1, so that it is (r | x, a). Also, let us use the self-limiting proportionality of the loss function: He [l | i (r | x, a) \u2264 k2Er [l | i (r | x, a), meaning that the second moment of displaced loss is limited. Then we can limit the expected regret by a log2 (r | x, a) \u2264 k2Er [l | i (r | x, a) \u2264 k2T (l | 1, a), meaning that the second moment of displaced loss is limited."}], "references": [{"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning to optimize via information-directed sampling", "author": ["Dan Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "An empirical evaluation of thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Generalized thompson sampling for contextual bandits", "author": ["Lihong Li"], "venue": "arXiv preprint arXiv:1310.7163,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Further optimal regret bounds for thompson sampling", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1933}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Amulti-armed bandit problem [1] is one of the sequential decision making problem.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "In this note we describe three Bayesian style Multi-armed bandit algorithms: Information-Directed Sampling[2], Thompson Sampling[3] and Generalized Thompson Sampling[4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "1 Problem Formulation Information-Directed Sampling (IDS) [2] consider a Bayesian formulation of Multi-armed bandit problem.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "1 Problem Formulation Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems.", "startOffset": 45, "endOffset": 51}, {"referenceID": 4, "context": "1 Problem Formulation Thompson sampling (TS) [3, 5] is also a Bayesian style bandit algorithm, it can apply to both contextual bandit and standard Multi-armed bandit problems.", "startOffset": 45, "endOffset": 51}, {"referenceID": 5, "context": "3 Regret Although Thompson sampling is a very old algorithm, proposed by [6], but the theoretical analysis is done very recently.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "We follow [5] and hope to give a intuitive explanation of the regret.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "To bound ki(T + 1) we need the following settings [5]: Define F B n,p(\u00b7) the cdf and fB n,p(\u00b7) the pdf of the binomial distribution with parameters n, p.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "To bound (48), [5] proved that P (i(t) = i, E i (t), E \u03b8 i (t)|Ft\u22121) \u2264 (1\u2212 pi,t) pi,t P (i(t) = 1, E i (t), E \u03b8 i (t)|Ft\u22121) (51) and so T \u2211", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "[5] provide details about how to bound (57), which is quite complicated.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Similarly, [5] bound T \u2211", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "1 Problem Formulation Generalized Thompson Sampling[4] is a contextual bandit problem, it is similar to expert-learning framework, and include Thompson Sampling as a special case.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "In [4] the reward is binary, but it is easy to generalize to continuous space.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "[4] described two loss functions: logarithmic loss and square loss.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "T \u00b7 ln 1 p1 + \u03b3T (80) Different loss has different choice of k1 and k2, and [4] proved that with square loss the expected regret bound is O( \u221a", "startOffset": 76, "endOffset": 79}], "year": 2015, "abstractText": "This note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers.", "creator": "LaTeX with hyperref package"}}}