{"id": "1503.06960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2015", "title": "Sample compression schemes for VC classes", "abstract": "We prove that proper PAC learnability implies compression. Namely, if a concept $C \\subseteq \\Sigma^X$ is properly PAC learnable with $d$ samples, then $C$ has a sample compression scheme of size $2^{O(d)}$. In particular, every boolean concept class with constant VC dimension has a sample compression scheme of constant size. This answers a question of Littlestone and Warmuth (1986). The proof uses an approximate minimax phenomenon for boolean matrices of low VC dimension.", "histories": [["v1", "Tue, 24 Mar 2015 09:30:33 GMT  (10kb,D)", "http://arxiv.org/abs/1503.06960v1", "11 pages"], ["v2", "Tue, 14 Apr 2015 12:18:14 GMT  (13kb,D)", "http://arxiv.org/abs/1503.06960v2", "14 pages. The previous version of this text contained an error; Theorem 2.1 in it is false. This error only affects the statement for multi-labeled classes, and the construction for binary-labeled classes still holds. In the new version of the text, we added a relevant discussion in Section 4"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shay moran", "amir yehudayoff"], "accepted": false, "id": "1503.06960"}, "pdf": {"name": "1503.06960.pdf", "metadata": {"source": "CRF", "title": "Proper PAC learning is compressing", "authors": ["Shay Moran", "Amir Yehudayoff"], "emails": ["shaymrn@cs.technion.ac.il.", "amir.yehudayoff@gmail.com."], "sections": [{"heading": null, "text": "We prove that a correct PAC learning ability implies compression. Namely, if a C-X concept with d samples is PAC-learnable properly, then C has a sample compression scheme of size 2O (d). Specifically, each Boolean concept class with constant VC dimension has a sample compression scheme of constant size. This answers a question by Littlestone and Warmuth (1986). The proof uses an approximate Minimax phenomenon for Boolean matrices with low VC dimensions."}, {"heading": "1 Introduction", "text": "Learning procedures perform compression, and compression is proof of this and useful in learning. For example, support vector machines, commonly used to solve classification problems, can perform compression (see Chapter 6 in [6]), and compression can be used to increase the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]). About thirty years ago, Littlestone and Warmuth [16] provided a mathematical framework for studying compression in the context of learning theory. In short, they showed that compression implies learning ability and questioned whether learning implies compression."}, {"heading": "1.1 Definitions", "text": "A concept is a function that can adapt the functionality of c / i to the set Y. We think of c / Y as a designation unit of computer science, Technion-IIT, Israel and Max Planck Institute for Informatics, Saarbru cken, Germany. shaymrn @ cs.technion.il. \u2020 Department of Mathematics, Israel amir.yehudayoff @ gmail.com. Horev fellow - supported by the Taub Foundation. Research is also supported by ISF and BSF.ar Xiv: 150 3.06 960v 1 [cs.L]."}, {"heading": "1.2 Background", "text": "Littlestone and Warmuth [16] proved that compression implies learning ability (see Theorem 1.1 below) and asked whether learning ability implies compression for Boolean concept classes: \"Are there concept classes with finite dimension for which there is no scheme with limited core size and limited additional information?\" This question and its variants lead to a rich sample material that reveals profound properties of the UK dimension and learning. This work also showed how to make connections between sample compression programs and model theory, topology, combinatories and geometry. Floyd and Warmuth [9, 10] constructed sample compression programs of size Log | C | for each concept class C. Freund [11] showed how to compress a sample of size m to a sample of size O (m) compressed (d log (m)))) with some lateral information for Boolean classes of the UK dimension.Since the study of sample compression designs was optimized and many cases were reconstructed for specific enrichment."}, {"heading": "1.3 Learning is compressing", "text": "Theorem 1.2 (Correct Learning Implies Compression): If C'X is PAC-learnable correctly with d samples, C has an example compression scheme of size 2O (d).The theorem explicitly answers Littlestone and Warmuth's question [16]; each Boolean concept class of the finite VC dimension has an example compression scheme of finite size. However, the theorem only provides an exponential dependence on d, whereas many of the known compression schemes for special cases (e.g. [10, 3, 13, 23, 17]) have size O (d). Warmuth's question [27] whether O (d) sample compression schemes always exist remains open. Our construction (see section 3) of the sample compression schemes is quite short and simple overall, but uses a different perspective of the problem than in previous work (mentioned above), with the compression schemes of the majority of the properties of the AC-11 applied being used in common with the known compression schemes."}, {"heading": "2 Preliminaries", "text": "There are many generalizations of the VC dimension to non-Boolean concept classes (see [2] and references within).Here we use the following formula: Let's all use strategies when defining the basic principles of the C-class, then a Boolean concept class becomes Bc (C) = max {VC (Bc): c).This definition of dimension is similar to the notes used in [19, 7, 2].If the C-class is then boolean, then VC (C).Vapnik and Chervonenkis [26] and Blumer et al. [4] proved that the VC dimension is synonymous with the sample complexity of PAC learning. The distinctive dimension is a lower limit of the complexity of PAC learning."}, {"heading": "3 A compression scheme", "text": "In the proof for Theorem 1.2 we use the following simple problem: The Lemma problem can be regarded as an approximate, combinatorial version of the Carathe-odorous theorem of convex geometry. Let us leave C '0, 1} n \"n\" Rn \"and designate K the convex shell of C in Rn. Carathe's odoric theorem states that each point p\" K \"is a convex combination of at most n + 1 points of C. Lemma 3.1 says that if C has a constant VC dimension, each p\" K \"can be approximated by a convex combination of small supports. Namely, if VC (C) = d then p\" by a convex combination of at most O (2 d / 2) points of C. Lemma 3.1 \"(sample for the limited VC dimension)."}, {"heading": "3.1 The construction", "text": "The proof of theorem 1.2. Since C correctly PAC is learnable with d samples, letH: LC (d) \u2192 Cbe is such that for each c (x) and for each probability distribution q on X, there is Z supp (q) of the size that q ({x) X: hZ (x) 6 = c (x)}, where hZ = H (Z).Compression. Let (Y, y) that q (\u00b2), y = {H (Z, z): Z Y (Z, z): Z Y (Z, z): Z Y (Z), z (Z). The compression is based on the following claim. Claim 3.2. There are T sets Z1, Z2,., ZT Y, each of the size at most d, with T \u2264 K (d), making the following positions."}, {"heading": "Acknowledgements", "text": "We thank Amir Shpilka and Avi Wigderson for helpful discussions and Ben Lee Volk for comments on an earlier version of this text."}], "references": [{"title": "Densite et dimension", "author": ["P. Assouad"], "venue": "Ann. Institut Fourter, 3:232\u2013282", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1983}, {"title": "and P", "author": ["S. Ben-David", "N. Cesa-Bianchi", "D. Haussler"], "venue": "M. Long. Characterizations of learnability for classes of {0,...,n}-valued functions. J. Comput. Syst. Sci., 50(1):74\u2013 86", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Combinatorial variability of Vapnik-Chervonenkis classes with applications to sample compression schemes", "author": ["S. Ben-David", "A. Litman"], "venue": "Discrete Applied Mathematics, 86(1):3\u201325", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth"], "venue": "J. Assoc. Comput. Mach., 36(4):929\u2013965", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Externally definable sets and dependent pairs", "author": ["A. Chernikov", "P. Simon"], "venue": "Israel Journal of Mathematics, 194(1):409\u2013425", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "An Introduction to Support Vector Machines and other kernel-based learning methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Universal Donsker classes and metric entropy", "author": ["R.M. Dudley"], "venue": "Ann. Probab., 15(4):1306\u20131326,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "A general lower bound on the number of examples needed for learning", "author": ["A. Ehrenfeucht", "D. Haussler", "M.J. Kearns", "L.G. Valiant"], "venue": "Inf. Comput., 82(3):247\u2013261", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "Space-bounded learning and the vapnik-chervonenkis dimension", "author": ["S. Floyd"], "venue": "COLT, pages 349\u2013364", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "Sample compression", "author": ["S. Floyd", "M.K. Warmuth"], "venue": "learnability, and the vapnikchervonenkis dimension. Machine Learning, 21(3):269\u2013304", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "Inf. Comput., 121(2):256\u2013285", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning integer lattices", "author": ["D.P. Helmbold", "R.H. Sloan", "M.K. Warmuth"], "venue": "SIAM J. Comput., 21(2):240\u2013266", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Unlabeled compression schemes for maximum classes", "author": ["D. Kuzmin", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research, 8:2047\u20132081", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Improved bounds on the sample complexity of learning", "author": ["Y. Li", "P.M. Long", "A. Srinivasan"], "venue": "SODA, pages 309\u2013318", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Playing large games using simple strategies", "author": ["R.J. Lipton", "E. Markakis", "A. Mehta"], "venue": "ACM Conference on Electronic Commerce, pages 36\u201341, New York, NY, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Relating data compression and learnability", "author": ["N. Littlewood", "M. Warmuth"], "venue": "Unpublished", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "Honest compressions and their application to compression schemes", "author": ["R. Livni", "P. Simon"], "venue": "COLT, pages 77\u201392", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Teaching and compressing for low VC-dimension", "author": ["S. Moran", "A. Shpilka", "A. Wigderson", "A. Yehudayoff"], "venue": "ECCC, TR15-025", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "On learning sets and functions", "author": ["B.K. Natarajan"], "venue": "Machine Learning, 4:67\u201397", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Zur theorie der gesellschaftsspiele", "author": ["J. von Neumann"], "venue": "Mathematische Annalen,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1928}, {"title": "Game Theory", "author": ["G. Owen"], "venue": "Academic Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Shifting: One-inclusion mistake bounds and sample compression", "author": ["B.I.P. Rubinstein", "P.L. Bartlett", "J.H. Rubinstein"], "venue": "J. Comput. Syst. Sci., 75(1):37\u201359", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "A geometric approach to sample compression", "author": ["B.I.P. Rubinstein", "J.H. Rubinstein"], "venue": "Journal of Machine Learning Research, 13:1221\u20131261", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Sharper bounds for Gaussian and empirical processes", "author": ["M. Talagrand"], "venue": "Ann. Probab., 22(1):28\u201376", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Commun. ACM, 27:1134\u20131142", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1984}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Ya. Chervonenkis"], "venue": "Theory Probab. Appl.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1971}, {"title": "Compressing to VC dimension many points", "author": ["M.K. Warmuth"], "venue": "COLT/Kernel, pages 743\u2013744", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 5, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 221, "endOffset": 229}, {"referenceID": 10, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 221, "endOffset": 229}, {"referenceID": 5, "context": "For example, support vector machines, which are commonly applied to solve classification problems, perform compression (see Chapter 6 in [6]), and compression can be used to boost the accuracy of learning procedures (see [16, 11] and Chapter 4 in [6]).", "startOffset": 247, "endOffset": 250}, {"referenceID": 15, "context": "About thirty years ago, Littlestone and Warmuth [16] provided a mathematical framework for studying compression in the context of learning theory.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "Probably approximately correct (PAC) learning was defined in Valiant\u2019s seminal work [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "The Vapnik-Chervonenkis (VC) dimension of C, denoted VC(C), is the maximum size of a shattered set in C [26].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "A fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).", "startOffset": 83, "endOffset": 86}, {"referenceID": 25, "context": "A fundamental and well-known result of Blumer, Eherenfeucht, Haussler, and Warmuth [4], which is based on an earlier work of Vapnik and Chervonenkis [26], states that every boolean concept class C can be properly PAC learned with O(VC(C)) examples (in fact the sample complexity of PAC learning for boolean classes is captured by the VC dimension).", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "Sample compression schemes were defined by Littlestone and Warmuth [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "See [9, 10, 18] for more discussions of this definition, and some insightful examples.", "startOffset": 4, "endOffset": 15}, {"referenceID": 9, "context": "See [9, 10, 18] for more discussions of this definition, and some insightful examples.", "startOffset": 4, "endOffset": 15}, {"referenceID": 17, "context": "See [9, 10, 18] for more discussions of this definition, and some insightful examples.", "startOffset": 4, "endOffset": 15}, {"referenceID": 15, "context": "2 Background Littlestone and Warmuth [16] proved that compression implies learnability (see Theorem 1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "Floyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C.", "startOffset": 18, "endOffset": 25}, {"referenceID": 9, "context": "Floyd and Warmuth [9, 10] constructed sample compression schemes of size log |C| for every concept class C.", "startOffset": 18, "endOffset": 25}, {"referenceID": 10, "context": "Freund [11] showed how to compress a sample of size m to a sample of size O(d log(m)) with some side information for boolean classes of VC dimension d.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "As the study of sample compression schemes deepened, many insightful and optimal schemes for special cases have been constructed: Floyd [9], Helmbold et al.", "startOffset": 136, "endOffset": 139}, {"referenceID": 11, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "[12], Floyd and Warmuth [10], Ben-David and Litman [3], Chernikov and Simon [5], Kuzmin and Warmuth [13], Rubinstein et al.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "[22], Rubinstein and Rubinstein [23], Livni and Simon [17] and more.", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "Finally, in our recent work with Shpilka and Wigderson [18], we constructed sample compression schemes of size O(d \u00b7 2 \u00b7 log log |C|) using some side information for every boolean concept class C of VC dimension d.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Littlestone and Warmuth proved that the sample complexity of PAC learning is at most (roughly) the size of a compression scheme [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "1 (Compression implies learnability [16]).", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "The theorem specifically answers Littlestone and Warmuth\u2019s question [16]; every boolean concept class of finite VC dimension has a sample compression scheme of finite size.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "[10, 3, 13, 23, 17]) have size O(d).", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "Warmuth\u2019s question [27] whether O(d)-sample compression schemes always exist remains open.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "It is inspired by Freund\u2019s work [11] where majority is used to boost the accuracy of learning procedures.", "startOffset": 32, "endOffset": 36}, {"referenceID": 1, "context": "There are many generalization of VC dimension to non-boolean concept classes (see [2] and references within).", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "This definition of dimension is similar to notions used in [19, 7, 2].", "startOffset": 59, "endOffset": 69}, {"referenceID": 6, "context": "This definition of dimension is similar to notions used in [19, 7, 2].", "startOffset": 59, "endOffset": 69}, {"referenceID": 1, "context": "This definition of dimension is similar to notions used in [19, 7, 2].", "startOffset": 59, "endOffset": 69}, {"referenceID": 25, "context": "Vapnik and Chervonenkis [26] and Blumer et al.", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "[4] proved that VC dimension is equivalent to the sample complexity of PAC learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).", "startOffset": 92, "endOffset": 101}, {"referenceID": 7, "context": "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).", "startOffset": 92, "endOffset": 101}, {"referenceID": 1, "context": "The distinguishing dimension is a lower bound on the sample complexity of PAC learning (see [4, 8, 2]).", "startOffset": 92, "endOffset": 101}, {"referenceID": 3, "context": "1 (Lower bound for sample complexity [4, 8, 2]).", "startOffset": 37, "endOffset": 46}, {"referenceID": 7, "context": "1 (Lower bound for sample complexity [4, 8, 2]).", "startOffset": 37, "endOffset": 46}, {"referenceID": 1, "context": "1 (Lower bound for sample complexity [4, 8, 2]).", "startOffset": 37, "endOffset": 46}, {"referenceID": 0, "context": "Assouad [1] bounded VC(C\u2217) in terms of VC(C).", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "2 (VC dimension of dual [1]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 25, "context": "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].", "startOffset": 105, "endOffset": 113}, {"referenceID": 23, "context": "This phenomenon was first proved by Vapnik and Chervonenkis [26], and was later quantitively improved in [14, 24].", "startOffset": 105, "endOffset": 113}, {"referenceID": 25, "context": "3 (Approximations for bounded VC dimension [26, 14, 24]).", "startOffset": 43, "endOffset": 55}, {"referenceID": 13, "context": "3 (Approximations for bounded VC dimension [26, 14, 24]).", "startOffset": 43, "endOffset": 55}, {"referenceID": 23, "context": "3 (Approximations for bounded VC dimension [26, 14, 24]).", "startOffset": 43, "endOffset": 55}, {"referenceID": 19, "context": "Von Neumann\u2019s minimax theorem [20] is a seminal result in game theory (see the textbook [21]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "Von Neumann\u2019s minimax theorem [20] is a seminal result in game theory (see the textbook [21]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "This implies that there is a pair of mixed strategies that form a Nash equilibrium (see [21]).", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "4 (Minimax [20]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "Lipton, Markakis and Mehta [15] call such a pair of mixed strategies an -Nash equilibrium.", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "We prove that proper PAC learnability implies compression. Namely, if a concept C \u2286 \u03a3X is properly PAC learnable with d samples, then C has a sample compression scheme of size 2O(d). In particular, every boolean concept class with constant VC dimension has a sample compression scheme of constant size. This answers a question of Littlestone and Warmuth (1986). The proof uses an approximate minimax phenomenon for boolean matrices of low VC dimension.", "creator": "LaTeX with hyperref package"}}}