{"id": "1503.07220", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2015", "title": "Individual Planning in Agent Populations: Exploiting Anonymity and Frame-Action Hypergraphs", "abstract": "Interactive partially observable Markov decision processes (I-POMDP) provide a formal framework for planning for a self-interested agent in multiagent settings. An agent operating in a multiagent environment must deliberate about the actions that other agents may take and the effect these actions have on the environment and the rewards it receives. Traditional I-POMDPs model this dependence on the actions of other agents using joint action and model spaces. Therefore, the solution complexity grows exponentially with the number of agents thereby complicating scalability. In this paper, we model and extend anonymity and context-specific independence -- problem structures often present in agent populations -- for computational gain. We empirically demonstrate the efficiency from exploiting these problem structures by solving a new multiagent problem involving more than 1,000 agents.", "histories": [["v1", "Tue, 24 Mar 2015 22:26:50 GMT  (905kb)", "https://arxiv.org/abs/1503.07220v1", "8 page article plus two page appendix containing proofs in Proceedings of 25th International Conference on Autonomous Planning and Scheduling, 2015"], ["v2", "Thu, 2 Apr 2015 22:58:57 GMT  (1215kb)", "http://arxiv.org/abs/1503.07220v2", "8 page article plus two page appendix containing proofs in Proceedings of 25th International Conference on Autonomous Planning and Scheduling, 2015"]], "COMMENTS": "8 page article plus two page appendix containing proofs in Proceedings of 25th International Conference on Autonomous Planning and Scheduling, 2015", "reviews": [], "SUBJECTS": "cs.MA cs.AI cs.GT", "authors": ["ekhlas sonu", "yingke chen", "prashant doshi"], "accepted": false, "id": "1503.07220"}, "pdf": {"name": "1503.07220.pdf", "metadata": {"source": "CRF", "title": "Individual Planning in Agent Populations: Exploiting Anonymity and Frame-Action Hypergraphs", "authors": ["Ekhlas Sonu", "Yingke Chen", "Prashant Doshi"], "emails": ["esonu@uga.edu,", "ykchen@uga.edu,", "pdoshi@cs.uga.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.07 220v 2 [cs.M A] 2A pr2 01"}, {"heading": "Introduction", "text": "We focus on the decision-making problem of a single agent operating in the presence of other self-interested agents (Doshi siez al), whose actions can influence the state of the environment and the rewards of the subject agent. In stochastic and partially observable environments, this problem is formalized by the interactive POMDP (I-POMDP) (Gmytrasiewicz and Doshi 2005). I-POMDPs cover an important part of multi-agent planning problem space (Seuken and Zilberstein 2008), and applications in various areas such as security (Ng et al. 2010; Seymour and Peterson 2009), robotics (Wang 2013), Woodward and Wood 2012), ad hoc teams (Chandrasekaran et al. 2014) and modelling human behavior (Doshi et al. 2010; Wunder et al. 2011) testify to its broad appeal while motivating greater scalability."}, {"heading": "Related Work", "text": "Building on this publication (Kearns, Littman and Singh 2001), Action Diagram Games (AGG 2001) (Jiang, Leyton-Brown, and Bhat 2011), we use problem structures such as Action Anonymity and Contextual Independence to refer to a zero point of complete information games involving multiple agents and solving themselves scalably for the Nash balance. Independence is modeled using a context-specific action diagram whose nodes are actions and an edge between two nodes, indicating that the reward of an agent performing an action is influenced by other actors performing the action of the other node. There is a lack of edges between the nodes of context-specific independence, where the context of the action anonymity is useful when the action sets of the agents overlap substantially."}, {"heading": "Factored Beliefs and Update", "text": "As we have already mentioned, the subjective actor in an IPOMDP is a belief in the physical state and common models of other actors that cannot be directly supported. (b0, l = 1Mj, l = \u00b7 \u00b7 \u2212 \u2212 \u2212 n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 r = > q. For constellations such as Example 1, where N is large, the size of the interactive state space is exponentially larger, | IS0, l \u2212 S | | | Mj, l \u2212 1 | N, and the unwieldness of the representation. However, the representation for large N becomes manageable if faith is not taken into account: b0, l (s, m1, l \u2212 1, m2, l \u2212 1,.). mN, l \u2212 1) = Pr (s) Pr (m1, l \u2212 1 | s)."}, {"heading": "Frame-Action Anonymity", "text": "As Jiang et al. (2011) notes, many uncooperative and cooperative problems are largely peaceful. (This is particularly evident in Example 1, where the outcome of policing largely depends on the number of peaceful and disruptive demonstrators moving from one place to another. (This is noted in the example below: Example 2 (Frame-action anonymes of protestors) The transient state of protests and police observations in one place is also largely influenced by the number of peaceful and disruptive demonstrators moving from one place to another. (Example 2 (Frame-action anonymes of protestors) The transient state of protests reflects the intensity of protests in each place and the number of peaceful and disruptive demonstrators entering the site.) The police (noisily) observe the intensity of protests in each place, which in turn is largely determined by the number of peaceful and disruptive demonstrators."}, {"heading": "Frame-Action Hypergraphs", "text": "Adding to the frameworks are anonymity domains where the options often have context-specific dependencies. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "Revised Framework", "text": "To benefit from the structures of anonymity and context-specific independence, we define the I-POMDP function significantly stronger than the original function of the space. (...) To benefit from the structures of anonymity and context-specific independence, we will redefine the I-POMDP function of the action space as the original configuration of the action space. (...) The physical states are regarded as, S = K = 1Xk. (...) The transition function, T0 x, a0, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "Algorithms", "text": "We present an algorithm that calculates distribution via frame action configurations and outlines our simple method for solving the previously defined Many-Agent-I-POMDP."}, {"heading": "Distribution Over Frame-Action Configurations", "text": "Algorithm 1 generalizes an algorithm developed by Jiang and Lleyton-Brown (2011) for calculating configurations about actions of mixed strategies of other agents to include frameworks and conditional beliefs about models of other agents. It calculates the probability distribution of configurations about the framework action neighborhood of an action taking into account belief in the models of agents: Pr (C\u03bd (x, a0, x) | bt0, l (M1, l \u2212 1 | s t),., bt0, l (MN, l \u2212 1 | s t)))), l (MN, l \u2212 1 | s t))))) in Equation 5, Pr (C\u03bd (x, a0, x \u00b2, x."}, {"heading": "12: cj [\u03c6] \u2190 cj [\u03c6] + 1", "text": "If an act, which by an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act, an act"}, {"heading": "Acknowledgements", "text": "This research is supported in part by a CAREER grant from NSF, IIS-0845036, and a grant from ONR, N000141310870. We thank Brenda Ng for valuable feedback that has led to improvements in her work."}, {"heading": "Games and Economic Behavior 71(1):141\u2013173.", "text": "[Jiang, Leyton-Brown, and Pfeffer 2009] Jiang, A. X.; Leyton-Brown, K.; and Pfeffer, A. 2009. Temporal actiongraph games: A new representation for dynamic games. In Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI), 268-276. [Kearns, Littman, and Singh 2001] Kearns, M.; Littman, M.; and Singh, S. 2001. Graphical models for game theory. In Uncertainty in Artificial Intelligence (UAI), 253-260. [Koller and Milch 2001] Koller, D., and Milch, B. 2001. Multi-agent influence diagrams for represent and solving games. In IJCAI, 1027-1034. [Nair et al. 2005] Nair, R.; Varakantham, P.; Tambe, M.; and Yokoo, M. 2005."}, {"heading": "Appendix", "text": "Faktored Belief Update: b0, l (s + 1, mt + 11,.., m + 1 n) = Pr (st + 1, mt + 11,.) \u2212 \u2212 pr (m + 1) \u2212 pr (m + 1, p + 1, p + 1, p + 1) \u00b7 pr (st + 1, p + 1, p + 1) \u00b7 pr (m + 1, p + 1) \u00b7 pr (m + 1, p + 1) \u00b7 pr (st + 1) \u00b7 pr (m + 1, p + 1) \u00b7 pr (t + 1) \u00b7 pr (m + 1, p + 1) \u00b7 pr (t \u2212 p + 1) \u00b7 pr (p, p + 1 \u00b7 pr) (p + 1) (p + 1) (t) (p, p + 1) (p + 1) = pr (p, p + 1) = pr (p + 1) = pr, p (t \u2212 p), p (t), p (1), p (1), pr (p), pr (1) = pr (pr) = pr (st + 1, p + 1, p + 1, p + 1 n) = pr (pr) = Pr (st + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p + 1 n) = Pr (st + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p + 1) = Pr (st + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p + 1) = Pr (m + 1, p + 1) = Pr (m + 1, p + 1, p + 1, p + 1, p + 1) = Pr (m + 1, p + 1, p + 1, p + 1) = Pr (m + 1, p + 1, p + 1, p + 1, p + 1) = Pr (m + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p + 1, p"}], "references": [{"title": "Context-specific independence in bayesian networks", "author": ["Boutilier"], "venue": "In Twelfth international conference on Uncertainty in artificial intelligence (UAI),", "citeRegEx": "Boutilier,? \\Q1996\\E", "shortCiteRegEx": "Boutilier", "year": 1996}, {"title": "Team behavior in interactive dynamic influence diagrams with applications to ad hoc teams (extended abstract)", "author": ["Chandrasekaran"], "venue": "In Autonomous Agents and MultiAgent Systems Conference (AAMAS),", "citeRegEx": "Chandrasekaran,? \\Q2014\\E", "shortCiteRegEx": "Chandrasekaran", "year": 2014}, {"title": "P", "author": ["P. Doshi", "Gmytrasiewicz"], "venue": "J.", "citeRegEx": "Doshi and Gmytrasiewicz 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Perez", "author": ["P. Doshi"], "venue": "D.", "citeRegEx": "Doshi and Perez 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling recursive reasoning in humans using empirically informed interactive POMDPs", "author": ["Doshi"], "venue": "In International Autonomous Agents and Multiagent Systems Conference (AAMAS),", "citeRegEx": "Doshi,? \\Q2010\\E", "shortCiteRegEx": "Doshi", "year": 2010}, {"title": "and Doshi", "author": ["P.J. Gmytrasiewicz"], "venue": "P.", "citeRegEx": "Gmytrasiewicz and Doshi 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "K", "author": ["T.N. Hoang", "Low"], "venue": "H.", "citeRegEx": "Hoang and Low 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and LeytonBrown", "author": ["A.X. Jiang"], "venue": "K.", "citeRegEx": "Jiang and Leyton.Brown 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "N", "author": ["A.X. Jiang", "K. LeytonBrown", "Bhat"], "venue": "A.", "citeRegEx": "Jiang. Leyton.Brown. and Bhat 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A", "author": ["Jiang"], "venue": "X.; Leyton-Brown, K.; and Pfeffer, A.", "citeRegEx": "Jiang. Leyton.Brown. and Pfeffer 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Graphical models for game theory", "author": ["Littman Kearns", "M. Singh 2001] Kearns", "M. Littman", "S. Singh"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Kearns et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2001}, {"title": "and Milch", "author": ["D. Koller"], "venue": "B.", "citeRegEx": "Koller and Milch 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs", "author": ["Nair"], "venue": null, "citeRegEx": "Nair,? \\Q2005\\E", "shortCiteRegEx": "Nair", "year": 2005}, {"title": "Towards applying interactive POMDPs to real-world adversary modeling", "author": ["Ng"], "venue": "In Innovative Applications in Artificial Intelligence (IAAI),", "citeRegEx": "Ng,? \\Q2010\\E", "shortCiteRegEx": "Ng", "year": 2010}, {"title": "and Boutilier", "author": ["P. Poupart"], "venue": "C.", "citeRegEx": "Poupart and Boutilier 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Tardos", "author": ["T. Roughgarden"], "venue": "E.", "citeRegEx": "Roughgarden and Tardos 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Zilberstein", "author": ["S. Seuken"], "venue": "S.", "citeRegEx": "Seuken and Zilberstein 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "G", "author": ["R. Seymour", "Peterson"], "venue": "L.", "citeRegEx": "Seymour and Peterson 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Doshi", "author": ["E. Sonu"], "venue": "P.", "citeRegEx": "Sonu and Doshi 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Individual planning in agent populations: Exploiting anonymity and frame-action hypergraphs", "author": ["Chen Sonu", "E. Doshi 2015] Sonu", "Y. Chen", "P. Doshi"], "venue": "Technical Report http://arxiv.org/abs/1503.07220,", "citeRegEx": "Sonu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sonu et al\\.", "year": 2015}, {"title": "Decentralized stochastic planning with anonymity in interactions", "author": ["Adulyasak Varakantham", "P. Jaillet 2014] Varakantham", "Y. Adulyasak", "P. Jaillet"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Varakantham et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Varakantham et al\\.", "year": 2014}, {"title": "Decision support for agent populations in uncertain and congested environments", "author": ["Varakantham"], "venue": "In Uncertainty in artificial intelligence (UAI),", "citeRegEx": "Varakantham,? \\Q2012\\E", "shortCiteRegEx": "Varakantham", "year": 2012}, {"title": "An I-POMDP based multiagent architecture for dialogue tutoring", "author": ["F. Wang"], "venue": "In International Conference on Advanced ICT and Education", "citeRegEx": "Wang,? \\Q2013\\E", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "R", "author": ["M.P. Woodward", "Wood"], "venue": "J.", "citeRegEx": "Woodward and Wood 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Using iterated reasoning to predict opponent strategies", "author": ["Wunder"], "venue": "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "Wunder,? \\Q2011\\E", "shortCiteRegEx": "Wunder", "year": 2011}], "referenceMentions": [], "year": 2015, "abstractText": "Interactive partially observable Markov decision processes (I-POMDP) provide a formal framework for planning for a self-interested agent in multiagent settings. An agent operating in a multiagent environment must deliberate about the actions that other agents may take and the effect these actions have on the environment and the rewards it receives. Traditional I-POMDPs model this dependence on the actions of other agents using joint action and model spaces. Therefore, the solution complexity grows exponentially with the number of agents thereby complicating scalability. In this paper, we model and extend anonymity and context-specific independence \u2013 problem structures often present in agent populations \u2013 for computational gain. We empirically demonstrate the efficiency from exploiting these problem structures by solving a new multiagent problem involving more than 1,000 agents. Introduction We focus on the decision-making problem of an individual agent operating in the presence of other self-interested agents whose actions may affect the state of the environment and the subject agent\u2019s rewards. In stochastic and partially observable environments, this problem is formalized by the interactive POMDP (I-POMDP) (Gmytrasiewicz and Doshi 2005). I-POMDPs cover an important portion of the multiagent planning problem space (Seuken and Zilberstein 2008; Doshi 2012), and applications in diverse areas such as security (Ng et al. 2010; Seymour and Peterson 2009), robotics (Wang 2013; Woodward and Wood 2012), ad hoc teams (Chandrasekaran et al. 2014) and human behavior modeling (Doshi et al. 2010; Wunder et al. 2011) testify to its wide appeal while critically motivating better scalability. Previous I-POMDP solution approximations such as interactive particle filtering (Doshi and Gmytrasiewicz 2009), point-based value iteration (Doshi and Perez 2008) and interactive bounded policy iteration (IBPI) (Sonu and Doshi 2014) scale I-POMDP solutions to larger physical state, observation and model spaces. Hoang and Low (2013) introduced the specialized IPOMDP Lite framework that promotes efficiency by Copyright c \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. modeling other agents as nested MDPs. However, to the best of our knowledge no effort specifically scales I-POMDPs to many interacting agents \u2013 say, a population of more than a thousand \u2013 sharing the environment. For illustration, consider the decision-making problem of the police when faced with a large protest. The degree of the police response is often decided by how many protestors of which type (disruptive or not) are participating. The individual identity of the protestor within each type seldom matters. This key observation of frame-action anonymity motivates us in how we model the agent population in the planning process. Furthermore, the planned degree of response at a protest site is influenced, in part, by how many disruptive protestors are predicted to converge at the site and much less by some other actions of protestors such as movement between other distant sites. Therefore, police actions depend on just a few actions of note for each type of agent. The example above illustrates two known and powerful types of problem structure in domains involving many agents: action anonymity (Roughgarden and Tardos 2002) and context-specific independence (Boutilier et al. 1996). Action anonymity allows the exponentially large joint action space to be substituted with a much more compact space of action configurations where a configuration is a tuple representing the number of agents performing each action. Context-specific independence (wherein given a context such as the state and agent\u2019s own action, not all actions performed by other agents are relevant) permits the space of configurations to be compressed by projecting counts over a limited set of others\u2019 actions. We extend both action anonymity and context-specific independence to allow considerations of an agent\u2019s frame as well. 1 We list the specific contributions of this paper below: 1. I-POMDPs are severely challenged by large numbers of agents sharing the environment, which cause an exponential growth in the space of joint models and actions. Exploiting problem structure in the form of frameaction anonymity and context-specific independence, we present a new method for considerably scaling the solution of I-POMDPs to an unprecedented number of I-POMDPs distinguish between an agent\u2019s frame and type with the latter including beliefs as well. Frames are similar in semantics to the colloquial use of types. agents. 2. We present a systematic way of modeling the problem structure in transition, observation and reward functions, and integrating it in a simple method for solving IPOMDPs that models other agents using finite-state machines and builds reachability trees given an initial belief. 3. We prove that the Bellman equation modified to include action configurations and frame-action independences continues to remain optimal given the I-POMDP with explicated problem structure. 4. Finally, we theoretically verify the improved savings in computational time and memory, and empirically demonstrate it on a new problem of policing protest with over a thousand protestors. The above problem structure allows us to emphatically mitigate the curse of dimensionality whose acute impact on I-POMDPs is well known. However, it does not lessen the impact of the curse of history. In this context, an additional step of sparse sampling of observations while generating the reachability tree allows sophisticated planning with a population of 1,000+ agents using about six hours. Related Work Building on graphical games (Kearns, Littman, and Singh 2001), action graph games (AGG) (Jiang, Leyton-Brown, and Bhat 2011) utilize problem structures such as action anonymity and context-specific independence to concisely represent single shot complete-information games involving multiple agents and to scalably solve for Nash equilibrium. The independence is modeled using a directed action graph whose nodes are actions and an edge between two nodes indicates that the reward of an agent performing an action indicated by one node is affected by other agents performing action of the other node. Lack of edges between nodes encodes the context-specific independence where the context is the action. Action anonymity is useful when the action sets of agents overlap substantially. Subsequently, the vector of counts over the set of distinct actions, called a configuration, is much smaller than the space of action profiles. We substantially build on AGGs in this paper by extending anonymity and context-specific independence to include agent frames, and generalizing their use to a partially observable stochastic game solved using decisiontheoretic planning as formalized by I-POMDPs. Indeed, Bayesian AGGs (Jiang and Leyton-Brown 2010) extend the original formulation to include agent types. These result in type-specific action sets with the benefit that the action graph structure does not change although the number of nodes grows with types: |\u0398\u0302||A| nodes for agents with |\u0398\u0302| types each having same |A| actions. If two actions from different type-action sets share a node, then these actions are interchangeable. A key difference in our representation is that we explicitly model frames in the graphs due to which context-specific independence is modeled using frame-action hypergraphs. Benefits are that we naturally maintain the distinction between two similar actions but performed by agents of different frames, and we add less additional nodes: |\u0398\u0302| + |A|. However, a hypergraph is a more complex data structure for operation. Temporal AGGs (Jiang, Leyton-Brown, and Pfeffer 2009) extend AGGs to a repeated game setting and allow decisions to condition on chance nodes. These nodes may represent the action counts from previous step (similar to observing the actions in the previous game). Temporal AGGs come closest to multiagent influence diagrams (Koller and Milch 2001) although they can additionally model the anonymity and independence structure. Overall, I-POMDPs with frame-action anonymity and context-specific independence significantly augment the combination of Bayesian and temporal AGGs by utilizing the structures in a partially observable stochastic game setting with agent types. Varakantham et al. (2014) building on previous work (Varakantham et al. 2012) recently introduced a decentralized MDP that models a simple form of anonymous interactions: rewards and transition probabilities specific to a state-action pair are affected by the number of other agents regardless of their identities. The interaction influence is not further detailed into which actions of other agents are relevant (as in action anonymity) and thus configurations and hypergraphs are not used. Furthermore, agent types are not considered. Finally, the interaction hypergraphs in networked-distributed POMDPs (Nair et al. 2005) model complete reward independence between agents \u2013 analogous to graphical games \u2013 which differs from the hypergraphs in this paper (and action graphs) that model independence in reward (and transition, observation probabilities) along a different dimension: actions. Background Interactive POMDPs allow a self-interested agent to plan individually in a partially observable stochastic environment in the presence of other agents of uncertain types. We briefly review the I-POMDP framework and refer the reader to (Gmytrasiewicz and Doshi 2005) for further details. A finitely-nested interactive I-POMDP for an agent (say agent 0) of strategy level l operating in a setting inhabited by one of more other interacting agents is defined as the following tuple: I-POMDP0,l = \u3008IS0,l, A, T0,\u03a90, O0, R0, OC0\u3009 \u2022 IS0,l denotes the set of interactive states defined as, IS0,l = S \u00d7 \u220fN j=1Mj,l\u22121, where Mj,l\u22121 = {\u0398j,l\u22121 \u222a SMj}, for l \u2265 1, and ISi,0 = S, where S is the set of physical states. \u0398j,l\u22121 is the set of computable, intentional models ascribed to agent j: \u03b8j,l\u22121 = \u3008bj,l\u22121, \u03b8\u0302j\u3009, where bj,l\u22121 is agent j\u2019s level l \u2212 1 belief, bj,l\u22121 \u2208 \u25b3(ISj,l\u22121), and \u03b8\u0302j \u25b3 = \u3008A, Tj ,\u03a9j , Oj , Rj , OCj\u3009, is j\u2019s frame. Here, j is assumed to be Bayes-rational. At level 0, bj,0 \u2208 \u25b3(S) and a level-0 intentional model reduces to a POMDP. SMj is the set of subintentional models of j, an example is a finite state automaton; \u2022 A = A0 \u00d7A1 \u00d7 . . .\u00d7AN is the set of joint actions of all agents; \u2022 T0 : S \u00d7 A0 \u00d7 \u220fN j=1 Aj \u00d7 S \u2192 [0, 1] is the transition function; \u2022 \u03a90 is the set of agent 0\u2019s observations; \u2022 O0 : S\u00d7A0 \u00d7 \u220fN j=1Aj \u00d7\u03a90 \u2192 [0, 1] is the observation function; \u2022 R0 : S\u00d7A0 \u00d7 \u220fN j=1 Aj \u2192 R is the reward function; and \u2022 OC0 is the optimality criterion, which is identical to that for POMDPs. In this paper, we consider a finite-horizon optimality criteria. Besides the physical state space, the I-POMDP\u2019s interactive state space contains all possible models of other agents. In its belief update, an agent has to update its belief about the other agents\u2019 models based on an estimation about the other agents\u2019 observations and how they update their models. As the number of agents sharing the environment grows, the size of the joint action and joint model spaces increases exponentially. Therefore, the memory requirement for representing the transition, observation and reward functions grows exponentially as well as the complexity of performing belief update over the interactive states. In the context ofN agents, interactive bounded policy iteration (Sonu and Doshi 2014) generates good quality solutions for an agent interacting with 4 other agents (total of 5 agents) absent any problem structure. To the best of our knowledge, this result illustrates the best scalability so far to N > 2 agents. Many-Agent I-POMDP To facilitate understanding and experimentation, we introduce a pragmatic running example that also forms our evaluation domain. Figure 1: Protestors of different frames (colors) and police troops at two of three sites in the policing protest domain. The state space of police decision making is factored into the protest intensity levels at the sites. Example 1 (Policing Protest) Consider a policing scenario where police (agent 0) must maintain order in 3 geographically distributed and designated protest sites (labeled 0, 1, and 2) as shown in Fig. 1. A population of N agents are protesting at these sites. Police may dispatch one or two riot-control troops to either the same or different locations. Protests with differing intensities, low, medium and high (disruptive), occur at each of the three sites. The goal of the police is to deescalate protests to the low intensity at each site. Protest intensity at any site is influenced by the number of protestors and the number of police troops at that location. In the absence of adequate policing, we presume that the protest intensity escalates. On the other hand, two police troops at a location are adequate for de-escalating protests. Factored Beliefs and Update As we mentioned previously, the subject agent in an IPOMDP maintains a belief over the physical state and joint models of other agents, b0,l \u2208 \u2206(S \u00d7 \u220fN j=1Mj,l\u22121), where \u2206(\u00b7) is the space of probability distributions. For settings such as Example 1 where N is large, the size of the interactive state space is exponentially larger, |IS0,l| = |S||Mj,l\u22121| N , and the belief representation unwieldy. However, the representation becomes manageable for large N if the belief is factored: b0,l(s,m1,l\u22121,m2,l\u22121, . . . ,mN,l\u22121) = Pr(s) Pr(m1,l\u22121|s) \u00d7 Pr(m2,l\u22121|s)\u00d7 . . .\u00d7 Pr(mN,l\u22121|s) (1) This factorization assumes conditional independence of models of different agents given the physical state. Consequently, beliefs that correlate agents may not be directly represented although correlation could be alternately supported by introducing models with a correlating device. The memory consumed in storing a factored belief is O(|S| + N |S||M j |), where |M \u2217 j | is the size of the largest model space among all other agents. This is linear in the number of agents, which is much less than the exponentially growing memory required to represent the belief as a joint distribution over the interactive state space, O(|S||M j | N ). Given agent 0\u2019s belief at time t, bt0,l, its action a t 0 and the subsequent observation it makes, \u03c9 0 , the updated belief at time step t+ 1, b 0,l , may be obtained as: Pr(s,m 1,l\u22121, . . . ,m t+1 N,l\u22121|b t 0,l, a t 0, \u03c9 t+1 0 ) = Pr(s | bt0,l, a t 0, \u03c9 t+1 0 ) Pr(m t+1 1,l\u22121|s ,m 2,l\u22121, . . . ,m t+1 N,l\u22121, bt0,l, a t 0, \u03c9 t+1 0 )\u00d7 . . .\u00d7 Pr(m t+1 N,l\u22121|s , bt0,l, a t 0, \u03c9 t+1 0 ) (2) Each factor in the product of Eq. 2 may be obtained as follows. The update over the physical state is: Pr(s|b0,l, a t 0, \u03c9 t+1 0 ) \u221d Pr(s , \u03c9 0 |b t 0,l, a t 0)", "creator": "gnuplot 4.6 patchlevel 4"}}}