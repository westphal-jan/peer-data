{"id": "1503.07274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2015", "title": "Initialization Strategies of Spatio-Temporal Convolutional Neural Networks", "abstract": "We propose a new way of incorporating temporal information present in videos into Spatial Convolutional Neural Networks (ConvNets) trained on images, that avoids training Spatio-Temporal ConvNets from scratch. We describe several initializations of weights in 3D Convolutional Layers of Spatio-Temporal ConvNet using 2D Convolutional Weights learned from ImageNet. We show that it is important to initialize 3D Convolutional Weights judiciously in order to learn temporal representations of videos. We evaluate our methods on the UCF-101 dataset and demonstrate improvement over Spatial ConvNets.", "histories": [["v1", "Wed, 25 Mar 2015 03:41:47 GMT  (483kb,D)", "http://arxiv.org/abs/1503.07274v1", "Technical Report"]], "COMMENTS": "Technical Report", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["elman mansimov", "nitish srivastava", "ruslan salakhutdinov"], "accepted": false, "id": "1503.07274"}, "pdf": {"name": "1503.07274.pdf", "metadata": {"source": "CRF", "title": "Initialization Strategies of Spatio-Temporal Convolutional Neural Networks", "authors": ["Elman Mansimov", "Nitish Srivastava", "Ruslan Salakhutdinov"], "emails": ["rsalakhu@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, learning the underlying temporal representations of information contained in videos becomes an important challenge to recognize the activities. Despite the fact that GPUs are getting faster and faster each year, learning large-scale SpatioTemporal ConvNets such as Sport-1M [7] and Facebook-380K [15] will be a very time-consuming task that takes almost a month to complete the training, while learning SpatioTemporal ConvNets on a large scale such as Sport-1M [7] and Facebook-380K [8] will be from scratch."}, {"heading": "2 Related Work", "text": "Research in the field of action detection has been driven mainly by advances in image object detection, extending or adapting these approaches to video handling. Traditional flat approaches consisted of three main stages. First, sparse spatio-temporal points of interest were identified in videos and extracted using Histogram of Oriented Gradients (HOG) [1] and Histogram of Optical Flow (HOF) [2]. Recently, Wang et al. [16] proposed dense trajectories that became handmade characteristics for action detection. Next, these characteristics were combined into a fine vector description, such as Bag of Words (BoW) or Fisher Vectors (FV). Finally, the standard classifier such as SVM was trained on BoW or FV representation to distinguish between classes of interest."}, {"heading": "3 Transforming Features", "text": "In this section, we describe several ways to convert 2D basis weights to 3D basis weights without losing the spatial information we learned from the formation of Spatial ConvNet on ImageNet. Suppose we had a 2D basis weight matrix W (2D) derived from the formation of a Spatial ConvNet on ImageNet. Our goal is to create a 3D basis weight matrix W (3D) with the temporal dimension T using these weights. Each submatrix W (3D) t has the same dimension as the original 2D basis weight matrix. In a trained Spatial ConvNet, each layer expects an input from the underlying layer within a certain range. It is important to initialize W (3D) in some way, so that the output of the 3D basis weight matrix remains roughly in the same range as the output of the originally learned 2D surface layer, we must define all as equal (D) to this 3D (D) twelve (W)."}, {"heading": "3.1 Initialization by Averaging. (IA)", "text": "Since the successive picture frames in videos have similarities in appearance, we would expect W (2D) to extract very similar spatial representations from each successive picture frame. Therefore, we can initialize any sub-matrix W (3D) t to extract the same spatial representations that are present in their respective input function board. This could be achieved by appearing each element of the 2D matrix by the time size of the wavelength layer and putting all wavelengths over the temporal dimension of W (3D) on the resulting matrix. Formally, it can be expressed as follows: W (3D) t = W (2D) T,..."}, {"heading": "3.2 Initialization by Scaling. (IS)", "text": "This method can be considered a generalization of the method of initializing weights by averaging. Instead of dividing W (2D) by the same number T and thus equating each W (3D) t, we can achieve a certain diversity by dividing each W (3D) t by a random constant. Any combination of the values of these constants could work as long as the constraint described in Section 3 is met. W (3D) therefore becomes: W (3D) t = \u03b1t \u0445 W (2D), where \u03b1t > 0 and \u2211 T = 1 \u03b1t = 1"}, {"heading": "3.3 Zero Weight Initialization. (ZWI)", "text": "It is natural to wonder what would happen if we initialized one from W (3D) t to W (2D) and initialized other submatrices to zero matrix. Would these submatrices, initialized with zeros, learn to extract meaningful representations from the input even with limited training data? Also, despite some value differences, the distribution of weights in each submatrix of W (3D) is the same in the above initializations. Therefore, the network could get stuck on the plateau achieved by learning spatial representation on images, and consequently, it could not learn the temporal representation represented in multiple images. Motivated by these points, we researched the following initialization technique, which can be expressed with W (3D) = {W (2D) if t = 1 O, otherwise. Note that for any 1 \u2264 t T, W (3D) on W (2D) differences, we could not find any of the other 2D initialized sequences as long as we are not in zero order."}, {"heading": "3.4 Negative Weight Initialization. (NWI)", "text": "We can extend the zero-weight initialization by encouraging each submatrix W (3D) t, except the first, to move even further away from the original values of W (2D), which could be achieved by dividing the values of the submatrices W (3D) t, 2 \u2264 t \u2264 T to negative sign values of W (2D), increasing the absolute values of W (3D) t, t = 1 compared to other initializations. Again, changing the order of the submatrices does not result in a change in the resulting accuracy. It can be expressed as follows: W (3D) t = \u03b1t \u0445 W (2D), where t = {2T \u2212 1 T if t = 1 \u2212 1 T, otherwise."}, {"heading": "3.5 Architecture Details.", "text": "The ConvNet with the architecture described here https: / / github.com / TorontoDeepLearning / convennet / blob / master / examples / imagenet / CLS _ net _ 20140801232522.pbtxt was trained on ImageNet ILSVRC-2012. It revealed 13.5% top 5 errors on the ILSVRC 2012 validation set. In this study, we did not focus on using the best ImageNet model. Instead, we chose one that was convenient and easy to work on and focused on investigating the relative effects of our proposed method. In fine-tuning, we removed the Softmax layer and reduced the number of units in the last fully connected hidden layer from 4096 to 2048. Otherwise, the performance of the model fell by 6-7%. We also used an aggressive dropout of 0.8 in both FC layers."}, {"heading": "4 Evaluation and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "The evaluation is based on the first part of the UCF-101 dataset. UCF-101 contains 13.2K videos (25 frames / second) divided into 101 classes, with each split containing 9.5K training videos."}, {"heading": "4.2 Effect of Different Initializations.", "text": "The results of all transfer learning experiments are presented in Table 1. Results yielded many different conclusions. First, fine tuning of 3D ConvNet, initialized by averaging or scaling, resulted in only a small marginal improvement compared to fine tuning of 2D ConvNet. Since each submatrix is initialized over time with similar weights that extract spatial characteristics, the Convolutionary Layer could not get beyond that layer. In contrast, despite the fact that all submatrices of W (3D), except one, were initialized with zero matrix, the model achieved surprisingly better results than models initialized by averaging or scaling."}, {"heading": "4.3 Two-Stream Spatio-Temporal ConvNets", "text": "We conducted further experiments and evaluated the full two-current model based on the First Split, which combines RGB and optical flow-based spatio-temporal ConvNet. Optical flow-based ConvNet, similar to the one proposed by [12], was trained on optical single frame flow and stacks of 10 optical flows, resulting in an accuracy of 72.2% and 77.5%, respectively. Additionally, we trained a slow fusion version of this model with a slow fusion build-up as in [7] and this model yielded an accuracy of 79.3%. We then combined the Softmax values of this optical flow model with the Softmax values of the RGB-based NWI + composite-LSTM model and achieved the accuracy of 85.3% on UCF-101. Even though the early fusion model's lower accuracy compared to its slow fusion version (the combination of the optical flow-based early fusion model with NWI + composite-LSTM model yielded very similar results to the 3D net-based E5.3% accuracy)"}, {"heading": "5 Conclusions", "text": "We proposed a new method of creating Spatio-Temporal ConvNets by integrating temporal information into Spatial ConvNet, which was trained on images. Although our model had limited labeled video training data, it outperformed Spatio-Temporal ConvNets, which were trained on large labeled video datasets. It is interesting to see if initializing Spatio-Temporal ConvNet with any of the initialization techniques proposed here will improve performance on large labeled video datasets. However, it is very difficult for us to process the datasets of this size."}], "references": [{"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In CVPR, pages 886\u2013893", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Human detection using oriented histograms of flow and appearance", "author": ["N. Dalal", "B. Triggs", "C. Schmid"], "venue": "In ECCV, pages 428\u2013441", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In EECV, pages 392\u2013407", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "S", "author": ["K. He", "X. Zhang"], "venue": "Ren, , and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In In EECV", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 1106\u20131114", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "HMDB: a large video database for human motion recognition", "author": ["H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "Proceedings of the International Conference on Computer Vision (ICCV)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition", "author": ["Z. Lan", "M. Lin", "X. Li", "A.G. Hauptmann", "B. Raj"], "venue": "CoRR, abs/1411.6660", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "and L", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg"], "venue": "Fei-Fei. ImageNet Large Scale Visual Recognition Challenge", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A. Roshan Zamir", "M. Shah"], "venue": "CRCV-TR-12-01", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "CoRR, abs/1502.04681", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "C3D: generic features for video analysis", "author": ["D. Tran", "L.D. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "CoRR, abs/1412.0767", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "IEEE International Conference on Computer Vision, Sydney, Australia", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting image-trained cnn architectures for unconstrained video classification", "author": ["S. Zha", "F. Luisier", "W. Andrews", "N. Srivastava", "R. Salakhutdinov"], "venue": "CoRR, abs/1503.04144", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Despite the fact that GPUs are getting faster and getting more memory every year, training SpatioTemporal ConvNets on large scale video datasets such as Sports-1M [7] and Facebook-380K [15] from scratch is a very time consuming task, which requires nearly a month to complete training.", "startOffset": 163, "endOffset": 166}, {"referenceID": 14, "context": "Despite the fact that GPUs are getting faster and getting more memory every year, training SpatioTemporal ConvNets on large scale video datasets such as Sports-1M [7] and Facebook-380K [15] from scratch is a very time consuming task, which requires nearly a month to complete training.", "startOffset": 185, "endOffset": 189}, {"referenceID": 12, "context": "At the same time, training Spatio-Temporal ConvNets on smaller datasets such as UCF-101 [13] and HMDB [9] leads to severe overfitting.", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "At the same time, training Spatio-Temporal ConvNets on smaller datasets such as UCF-101 [13] and HMDB [9] leads to severe overfitting.", "startOffset": 102, "endOffset": 105}, {"referenceID": 10, "context": "Taking a ConvNet trained on ImageNet [11] and fine-tuning it on individual video frames solves the problem of overfitting, but gives unsatisfying solution because this model doesn\u2019t learn temporal representations present in multiple frames.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "By appropriately initializing weights and using Composite LSTM that learned representations of video sequences [14] trained on labelled examples present in UCF-101 and unlabelled examples from Sports-1M, we managed to nearly match the current best classification accuracy [17] on RGB data extracted from UCF-101 which uses many additional tricks to improve their performance.", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "By appropriately initializing weights and using Composite LSTM that learned representations of video sequences [14] trained on labelled examples present in UCF-101 and unlabelled examples from Sports-1M, we managed to nearly match the current best classification accuracy [17] on RGB data extracted from UCF-101 which uses many additional tricks to improve their performance.", "startOffset": 272, "endOffset": 276}, {"referenceID": 0, "context": "Firstly, sparse spatio-temporal interest points were detected in videos and extracted using Histogram of Oriented Gradients (HOG) [1] and Histogram of Optical Flow (HOF) [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "Firstly, sparse spatio-temporal interest points were detected in videos and extracted using Histogram of Oriented Gradients (HOG) [1] and Histogram of Optical Flow (HOF) [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 15, "context": "[16] have proposed dense trajectories which became state-of-the art hand-crafted features for action recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "With the larger availability of labeled image data and advancements in parallel computing, Deep ConvNets [8] overshadowed traditional approaches in extracting features of images.", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "Motivated by those results, researchers [7] and [15] have created large scaled labelled video datasets and trained Deep Spatio-Temporal ConvNets on them, which were originally proposed by Ji et al.", "startOffset": 40, "endOffset": 43}, {"referenceID": 14, "context": "Motivated by those results, researchers [7] and [15] have created large scaled labelled video datasets and trained Deep Spatio-Temporal ConvNets on them, which were originally proposed by Ji et al.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Even though their models had access to temporal information presented in videos, they didn\u2019t perform better than simple Spatial ConvNet fine-tuned on image frames extracted from UCF-101 and HMDB datasets [7], [15].", "startOffset": 204, "endOffset": 207}, {"referenceID": 14, "context": "Even though their models had access to temporal information presented in videos, they didn\u2019t perform better than simple Spatial ConvNet fine-tuned on image frames extracted from UCF-101 and HMDB datasets [7], [15].", "startOffset": 209, "endOffset": 213}, {"referenceID": 11, "context": "[12] showed that Deep Early Fusion Spatio-Temporal ConvNet trained on dense optical flow extracted from videos significantly boosts the performance of fine-tuned spatial ConvNet trained on images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Additionally, [17], [4], [5] recently showed that spatio-temporal pooling of Deep ConvNet features gives additional improvement in performance.", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "Additionally, [17], [4], [5] recently showed that spatio-temporal pooling of Deep ConvNet features gives additional improvement in performance.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "Additionally, [17], [4], [5] recently showed that spatio-temporal pooling of Deep ConvNet features gives additional improvement in performance.", "startOffset": 25, "endOffset": 28}, {"referenceID": 16, "context": "We think that our performance can be further increased by using different tricks described in [17] and using SVM fusion instead of averaging.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "Model Performance Spatial ConvNet [12] 72.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "8 % C3D [15] 72.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "3 % C3D + fc6 [15] 76.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "4 % LRCN [3] 71.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "1 % Composite LSTM [14] 75.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "3 % ConvNet Features [17] 79.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "Optical Flow based ConvNet, similar to the one proposed by [12], was trained on single frame optical flow and stacks of 10 optical flows.", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "Additionally we trained a slow fusion version of this model, with an slow fusion setup same as in [7] and this model gave an accuracy of 79.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Method Performance LRCN [3] 82.", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "9 % Two-Stream Convolutional Net [12] (split 1) 87.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "0 % C3D + fc6 + iDT [15] 86.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "7 % ConvNet Features + iDT [17] 89.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "7 % Multi-skip feature stacking [10] 89.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "1 % Composite LSTM Model [14] (split 1) 84.", "startOffset": 25, "endOffset": 29}], "year": 2015, "abstractText": "We propose a new way of incorporating temporal information present in videos into Spatial Convolutional Neural Networks (ConvNets) trained on images, that avoids training SpatioTemporal ConvNets from scratch. We describe several initializations of weights in 3D Convolutional Layers of Spatio-Temporal ConvNet using 2D Convolutional Weights learned from ImageNet. We show that it is important to initialize 3D Convolutional Weights judiciously in order to learn temporal representations of videos. We evaluate our methods on the UCF-101 dataset and demonstrate improvement over Spatial ConvNets.", "creator": "LaTeX with hyperref package"}}}