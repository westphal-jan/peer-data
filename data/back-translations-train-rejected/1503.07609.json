{"id": "1503.07609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2015", "title": "An Evolutionary Algorithm for Error-Driven Learning via Reinforcement", "abstract": "Although different learning systems are coordinated to afford complex behavior, little is known about how this occurs. This article describes a theoretical framework that specifies how complex behaviors that might be thought to require error-driven learning might instead be acquired through simple reinforcement. This framework includes specific assumptions about the mechanisms that contribute to the evolution of (artificial) neural networks to generate topologies that allow the networks to learn large-scale complex problems using only information about the quality of their performance. The practical and theoretical implications of the framework are discussed, as are possible biological analogs of the approach.", "histories": [["v1", "Thu, 26 Mar 2015 03:33:47 GMT  (498kb)", "http://arxiv.org/abs/1503.07609v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["yanping liu", "erik d reichle"], "accepted": false, "id": "1503.07609"}, "pdf": {"name": "1503.07609.pdf", "metadata": {"source": "CRF", "title": "An Evolutionary Algorithm for Error-Driven Learning via Reinforcement", "authors": ["Yanping Liu", "Erik D. Reichle"], "emails": ["liuyp@psych.ac.cn."], "sections": [{"heading": null, "text": "An Evolutionary Algorithm for Error-Driven Learning via ReinforcementYanping Liu Key Laboratory of Behavioral ScienceInstitute of Psychology, Chinese Academy of Sciences & Erik D. Reichle School of Psychology, University of Southampton, UK.Note: This is a draft; please do not quote without permission. Addressing correspondence to: Yanping Liu, 16 Lincui Road, Key Laboratory of Behavioral Science, Institute of Psychology, Chinese Academy of Sciences, Beijing, China. E-mail: liuyp @ psych.cn.Although different learning systems are coordinated to enable complex behavior, little is known about how this happens. This article describes a theoretical framework that indicates how complex behavior could be thought of, error-guided learning could be acquired through simple amplification. This framework includes specific assumptions about the mechanisms that contribute to the development of (artificial) neural networks that allow complex learning processes to generate complex learning processes to learn complex learning processes."}, {"heading": "2. Macroscopic Evolution of the Network Architecture", "text": "Efficient biological evolution includes protective mechanisms to protect all phenotype innovations that offer a selective advantage (in terms of ecological fitness) in terms of the genotype, while simultaneously recognizing the homogeneity between genotypes and minimizing the structural complexity of the genotype. These protective mechanisms maximize the efficiency of biological evolution by minimizing the overall size of the evolutionary \"search space\" that must be \"traversed\" to produce organisms that are fit enough to compete and survive in specific ecological niches. Our Network Topology Generation Method uses an algorithm (NeuroEvolution of Augmenting Topologies, or NEAT) that has been developed specifically for these three efficiency strategies: First, it protects innovations in network topology (so that they have a reasonable chance of propagating from one generation of networks to the next), or the evolution of individual species that include populations that comprise only their unique populations."}, {"heading": "3. Microscopic Evolution (or Learning) of the Network Structure", "text": "Our algorithm for microscopic development of network connection weights is based on the problem of covariance Matrix Adaptation Evolution Strategy (CMA-ES; Hansen, 2006; Hansen & Kern, 2004; Hansen, M\u00fcller, & Koumoutsakos, 2003; Hansen & Ostermeier, 2001; Iger, Hansen, & Roth, 2007; Suttorp, Hansen, & Igel, 2009), an algorithm that provides a stochastic method for parameter optimization of non-linear, non-convex functions. As such, it is particularly useful for \"rogged\" parameters that include discontinuities and local optimizations (e.g. sharp \"ridges\") and is therefore well suited to solving non-conditioned and non-separable problems. However, some modifications to the CMA-ES algorithm were necessary to ease the problem of network connection."}, {"heading": "4. Reinforcement Learning of Connection Weights", "text": "The previous two sections have described how network topology is generated by the processes of macro and microscopy evolution. In this last section, we describe the algorithm used to train a network on the basis of reinforcement learning. As stated, reinforcement learning refers to a general class of machine learning algorithms in which performance is \"shaped,\" using a single training signal that corresponds to the reward / punishment associated with specific actions and / or the states that result from those actions (Sutton & Barto, 1998). Central to this notion is the idea of the reward predictive error, which is normally referred to by prediction error, representing the reward that represents an artificial agent anticipation in response to a particular action and the state that results from that action. This is presented in Equation 4.1, in which R represents the immediate reward that is represented by the value that can be obtained from the execution of a system in which x is represented."}, {"heading": "6. General Discussion", "text": "In fact, the fact is that you are able to be in a position without seeing yourself in a position to be in."}, {"heading": "7. References", "text": "In the USA (USA), USA (USA), Great Britain (USA), Great Britain (USA), Great Britain (USA), Great Britain (USA), Great Britain (Great Britain), France (France), Great Britain (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France), France (France)."}, {"heading": "8. Acknowledgements", "text": "Correspondence for this article should be sent to Erik Reichle, University of Pittsburgh, 635 LRDC, 3939 O'Hara St., Pittsburgh, PA, 15260; or by e-mail to reichle @ pitt.edu. The work described in this article was supported by a Chinese scholarship award to the first author and a NIH-R01 scholarship (HD053639) awarded to the second author. Source code for both evolutionary algorithms and error-driven learning networks for amplification is available upon request of the first author."}, {"heading": "9. Appendix A", "text": "This appendix contains tables listing the parameters of the macroscopic evolution algorithm (see Section 2), their functional role and their default values.c2 Scaling factor for excess genes 1 c3 Scaling factor for gene differences 2 \u03b4c Minimum distance for two genomes of the same species 3"}, {"heading": "10. Appendix B", "text": "This appendix contains a table listing the parameters of the microscopic evolution algorithm (see Section 3), their functional roles, and their default values. (Note that n represents the number of network connections and the number of generations that have stagnated.)"}, {"heading": "11. Appendix C", "text": "This appendix describes standard parameters for learning connection weights (Section 4)."}, {"heading": "11. Figure Captions", "text": "Figure 1. An example showing the coding and mapping of a single genotype (in the upper panel) to its corresponding phenotype, an artificial neural network (in the lower panel). Figure 2. An example showing two types of mutations: on the left, the mutation leads to the addition of a node, while on the right, the mutation leads to the addition of a connecting weight between two nodes. Figure 3. The upper panel shows the genomes of two parents together with the corresponding networks. The middle panel shows the two genomes aligned, as would be the case during a crossover. The lower panel shows the three different crossover methods (one-point, multi-point and multi-point comparison) and the resulting progeny phenomena."}], "references": [{"title": "Associative learning and reinforcement learning: Where animal learning and machine learning meet", "author": ["E. Alonso", "E. Mondrag\u00f3n"], "venue": "Proceedings of the Fifth Symposium on Adaptive Agents and Multi-Agent Systems", "citeRegEx": "Alonso and Mondrag\u00f3n,? \\Q2005\\E", "shortCiteRegEx": "Alonso and Mondrag\u00f3n", "year": 2005}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning (pp. 30-37)", "citeRegEx": "Baird,? \\Q1995\\E", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Dynamic programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm", "author": ["V. \u010cern\u00fd"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "\u010cern\u00fd,? \\Q1985\\E", "shortCiteRegEx": "\u010cern\u00fd", "year": 1985}, {"title": "Reinforcement learning and the brain: The Good, The Bad and The Ugly", "author": ["P. Dayan", "Y. Niv"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "Dayan and Niv,? \\Q2008\\E", "shortCiteRegEx": "Dayan and Niv", "year": 2008}, {"title": "What are the computations of the cerebellum, the basal ganglia, and the cerebral cortex", "author": ["K. Doya"], "venue": "Neural Networks,", "citeRegEx": "Doya,? \\Q1999\\E", "shortCiteRegEx": "Doya", "year": 1999}, {"title": "Complementary roles of basal ganglia and cerebellum in learning and motor control", "author": ["K. Doya"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "Doya,? \\Q2000\\E", "shortCiteRegEx": "Doya", "year": 2000}, {"title": "Learning to selectively attend", "author": ["S.J. Gershman", "J.D. Cohen", "Y. Niv"], "venue": "Proceedings of the 32nd Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society", "citeRegEx": "Gershman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2010}, {"title": "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)", "author": ["N. Hansen", "S.D. M\u00fcller", "P. Koumoutsakos"], "venue": "Evolutionary Computation,", "citeRegEx": "Hansen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2003}, {"title": "Evaluating the CMA evolution strategy on multimodal test functions", "author": ["N. Hansen", "S. Kern"], "venue": "Parallel Problem Solving from Nature - PPSN VIII (Vol", "citeRegEx": "Hansen and Kern,? \\Q2004\\E", "shortCiteRegEx": "Hansen and Kern", "year": 2004}, {"title": "The CMA evolution strategy: A comparing review", "author": ["N. Hansen"], "venue": null, "citeRegEx": "Hansen,? \\Q2006\\E", "shortCiteRegEx": "Hansen", "year": 2006}, {"title": "Adaptation in natural and artificial systems: An introductory analysis with applications to biology, control, and artificial intelligence", "author": ["J.H. Holland"], "venue": "Ann Arbor,", "citeRegEx": "Holland,? \\Q1975\\E", "shortCiteRegEx": "Holland", "year": 1975}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "Jr.", "M.P. Vecchi"], "venue": null, "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "Dynamic attention allocation during reading", "author": ["Liu", "Y.-P"], "venue": "Unpublished doctoral dissertation. Sun Yat-Sen University,", "citeRegEx": "Liu and Y..P.,? \\Q2011\\E", "shortCiteRegEx": "Liu and Y..P.", "year": 2011}, {"title": "The emergence of adaptive eye movements in reading", "author": ["Liu", "Y.-P", "E.D. Reichle"], "venue": "Proceedings of the 32nd Annual Conference of the Cognitive Science Society (pp. 1136-1131)", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory", "author": ["J.L. McClelland", "B.L. McNaughton", "R.C. O'Reilly"], "venue": "Psychological Review,", "citeRegEx": "McClelland et al\\.,? \\Q1995\\E", "shortCiteRegEx": "McClelland et al\\.", "year": 1995}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. II", "author": ["J.L. McClelland", "D.E. Rumelhart"], "venue": "PDP research group", "citeRegEx": "McClelland and Rumelhart,? \\Q1986\\E", "shortCiteRegEx": "McClelland and Rumelhart", "year": 1986}, {"title": "The predictive brain: temporal coincidence and temporal order in synaptic learning mechanisms", "author": ["P.R. Montague", "T.J. Sejnowski"], "venue": "Learning & Memory,", "citeRegEx": "Montague and Sejnowski,? \\Q1994\\E", "shortCiteRegEx": "Montague and Sejnowski", "year": 1994}, {"title": "Reinforcement learning in the brain", "author": ["Y. Niv"], "venue": "The Journal of Mathematical Psychology,", "citeRegEx": "Niv,? \\Q2009\\E", "shortCiteRegEx": "Niv", "year": 2009}, {"title": "Natural patterns of activity and long-term synaptic plasticity", "author": ["O. Paulsen", "T.J. Sejnowski"], "venue": "Current Opinions in Neurobiology,", "citeRegEx": "Paulsen and Sejnowski,? \\Q2000\\E", "shortCiteRegEx": "Paulsen and Sejnowski", "year": 2000}, {"title": "Conditional Reflexes", "author": ["I.P. Pavlov"], "venue": null, "citeRegEx": "Pavlov,? \\Q1927\\E", "shortCiteRegEx": "Pavlov", "year": 1927}, {"title": "Self-organizing neural systems based on predictive learning", "author": ["R.P. Rao", "T.J. Sejnowski"], "venue": "Philosophical Transactions: Mathematical, Physical and Engineering Sciences,", "citeRegEx": "Rao and Sejnowski,? \\Q2003\\E", "shortCiteRegEx": "Rao and Sejnowski", "year": 2003}, {"title": "Using reinforcement learning to understand the emergence of \u201cintelligent\u201d eye-movement behavior during reading", "author": ["E.D. Reichle", "P.A. Laurent"], "venue": "Psychological Review,", "citeRegEx": "Reichle and Laurent,? \\Q2006\\E", "shortCiteRegEx": "Reichle and Laurent", "year": 2006}, {"title": "The emergence of adaptive eye movement control in reading: Theory and data", "author": ["E.D. Reichle", "Liu", "Y.-P", "P.A. Laurent"], "venue": "Studies of Psychology and Behavior", "citeRegEx": "Reichle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reichle et al\\.", "year": 2011}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. I", "author": ["D.E. Rumelhart", "J.L. McClelland"], "venue": "PDP research group", "citeRegEx": "Rumelhart and McClelland,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart and McClelland", "year": 1986}, {"title": "Learning internal representations by error propagation. In Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1: Foundations (pp. 318362)", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A distributed, developmental model of word recognition and naming", "author": ["M.S. Seidenberg", "J.L. McClelland"], "venue": "Psychological Review,", "citeRegEx": "Seidenberg and McClelland,? \\Q1989\\E", "shortCiteRegEx": "Seidenberg and McClelland", "year": 1989}, {"title": "Why do we sleep", "author": ["T.J. Sejnowski", "A. Destexhe"], "venue": "Brain Research,", "citeRegEx": "Sejnowski and Destexhe,? \\Q2000\\E", "shortCiteRegEx": "Sejnowski and Destexhe", "year": 2000}, {"title": "Network oscillations: Emerging computational principles", "author": ["T.J. Sejnowski", "O. Paulsen"], "venue": "The Journal of Neuroscience,", "citeRegEx": "Sejnowski and Paulsen,? \\Q2006\\E", "shortCiteRegEx": "Sejnowski and Paulsen", "year": 2006}, {"title": "Learning optimal stragies in complex environments", "author": ["T.J. Sejnowski"], "venue": "Proceedings of the National Academy of Science,", "citeRegEx": "Sejnowski,? \\Q2010\\E", "shortCiteRegEx": "Sejnowski", "year": 2010}, {"title": "The Behavior of Organisms: An Experimental Analysis", "author": ["B.F. Skinner"], "venue": "B.F.Skinner Foundation", "citeRegEx": "Skinner.,? \\Q1938\\E", "shortCiteRegEx": "Skinner.", "year": 1938}, {"title": "Declarative and nondeclarative memory: Multiple brain systems supporting learning and memory. In Memory Concepts: Basic and Clinical Aspects, 7th Nova Nordisk Foundation Symposium (pp. 3-25)", "author": ["L.R. Squire"], "venue": null, "citeRegEx": "Squire,? \\Q1993\\E", "shortCiteRegEx": "Squire", "year": 1993}, {"title": "Evolving neural networks through augmenting topologies", "author": ["K.O. Stanley", "R. Miikkulainen"], "venue": "Evolutionary Computation,", "citeRegEx": "Stanley and Miikkulainen,? \\Q2002\\E", "shortCiteRegEx": "Stanley and Miikkulainen", "year": 2002}, {"title": "Reinforcement Learning Architecture for Animals. Proceedings of the first international conference on simulation of adaptive behavior on From animals to animats", "author": ["R.S. Sutton"], "venue": null, "citeRegEx": "Sutton,? \\Q1991\\E", "shortCiteRegEx": "Sutton", "year": 1991}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Efficient covariance matrix update for variable metric evolution strategies", "author": ["T. Suttorp", "N. Hansen", "C. Igel"], "venue": "Machine Learning,", "citeRegEx": "Suttorp et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Suttorp et al\\.", "year": 2009}, {"title": "Declarative memory consolidation in humans: A prospective functional magnetic resonance imaging study", "author": ["A. Takashima", "K.M. Petersson", "F. Rutters", "I. Tendolkar", "O. Jensen", "M.J. Zwarts", "B.L. McNaughton"], "venue": "Proceedings of the National Academy of Science,", "citeRegEx": "Takashima et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Takashima et al\\.", "year": 2006}, {"title": "Neurogammon: A neural-network backgammon program", "author": ["G. Tesauro"], "venue": "Proceedings of the International Joint Conference on Neural Networks (pp. 3340)", "citeRegEx": "Tesauro,? \\Q1990\\E", "shortCiteRegEx": "Tesauro", "year": 1990}, {"title": "Practical issues in temporal difference learning", "author": ["G. Tesauro"], "venue": "Machine Learning,", "citeRegEx": "Tesauro,? \\Q1992\\E", "shortCiteRegEx": "Tesauro", "year": 1992}, {"title": "Animal intelligence: An experimental study", "author": ["E.L. Thorndike"], "venue": null, "citeRegEx": "Thorndike,? \\Q1901\\E", "shortCiteRegEx": "Thorndike", "year": 1901}, {"title": "Tight performance bounds on greedy policies", "author": ["R. J", "L.C. Baird"], "venue": "Supplement,", "citeRegEx": "J. and Baird,? \\Q1993\\E", "shortCiteRegEx": "J. and Baird", "year": 1993}, {"title": "A fuzzy controller with supervised learning", "author": ["C. November. Ye", "Yung", "N.-C", "Wang D"], "venue": null, "citeRegEx": "Ye et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ye et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 32, "context": ", Skinner, 1938) and has instead shown that behavior is controlled by several different learning mechanisms, each operating according to its own principles (Doya, 1999, 2000; Squire, 1993).", "startOffset": 156, "endOffset": 188}, {"referenceID": 20, "context": "When viewed in this way, reinforcement learning refers to a general class of learning algorithms that has been extensively studied in psychology and that includes both classical (Pavlov, 1927) and operant (Thorndike, 1901) conditioning.", "startOffset": 178, "endOffset": 192}, {"referenceID": 40, "context": "When viewed in this way, reinforcement learning refers to a general class of learning algorithms that has been extensively studied in psychology and that includes both classical (Pavlov, 1927) and operant (Thorndike, 1901) conditioning.", "startOffset": 205, "endOffset": 222}, {"referenceID": 15, "context": "In contrast to reinforcement learning, error-driven learning refers to a type of learning in which the organism uses information about how its behavior differs from some \u201ctarget\u201d or desired behavior to modify its actual behavior so that the latter comes to resemble the former (McClelland et al., 1995; Rumelhart, Hinton, & Willams, 1986).", "startOffset": 277, "endOffset": 338}, {"referenceID": 18, "context": "But this enhanced learning requires an explicit error or teaching signal that has often undermined its applicability in both psychology (Dayan & Niv 2008; Niv, 2009; Sutton, 1991) and machine learning (Alonso & Mondrag\u00f3n, 2005; Ye, Yung, & Wang, 2003).", "startOffset": 136, "endOffset": 179}, {"referenceID": 34, "context": "But this enhanced learning requires an explicit error or teaching signal that has often undermined its applicability in both psychology (Dayan & Niv 2008; Niv, 2009; Sutton, 1991) and machine learning (Alonso & Mondrag\u00f3n, 2005; Ye, Yung, & Wang, 2003).", "startOffset": 136, "endOffset": 179}, {"referenceID": 25, "context": "In these artificial neural networks, the connection strengths between \u201cneurons\u201d are modified using a variety of different algorithms, with perhaps the most widely used of these algorithms being back-propagation (Rumelhart et al., 1986).", "startOffset": 211, "endOffset": 235}, {"referenceID": 25, "context": "Such multilayered networks are necessary to learn tasks that require non-linear mappings between input and output, such as the classic exclusive-or problem, where either of two options results to one response but neither or both of the options results in the opposite response (Rumelhart et al., 1986).", "startOffset": 277, "endOffset": 301}, {"referenceID": 11, "context": "More specifically, this approach uses a genetic algorithm (Holland, 1975) to evolve artificial neural networks that are capable of error-driven learning via reinforcement.", "startOffset": 58, "endOffset": 73}, {"referenceID": 3, "context": "Finally, the probability of mutation is determined by a simulated annealing process (\u010cern\u00fd, 1985; Kirkpatrick, Gelatt, & Vecchi, 1983) in which the overall probability is initial some large value but then declines with each successive generation.", "startOffset": 84, "endOffset": 134}, {"referenceID": 10, "context": "Microscopic Evolution (or Learning) of the Network Structure Our algorithm for microscopic evolution of network connection weights is based on the Covariance Matrix Adaptation Evolution Strategy (CMA-ES; Hansen, 2006; Hansen & Kern, 2004; Hansen, M\u00fcller, & Koumoutsakos, 2003; Hansen & Ostermeier, 2001; Iger, Hansen, & Roth, 2007; Suttorp, Hansen, & Igel, 2009).", "startOffset": 195, "endOffset": 362}, {"referenceID": 25, "context": "One way to implement this algorithm within a neural network is to use the standard error-driven back-propagation algorithm (Rumelhart et al., 1986), training the network using each state at time t, xt, as the input, and allowing the resulting state at time t+1, xt+1, as the output.", "startOffset": 123, "endOffset": 147}, {"referenceID": 1, "context": "Although this direct algorithm has been used successfully in many applications (Tesauro, 1990, 1992), it is not guaranteed to converge for general function-approximation systems (Baird, 1995).", "startOffset": 178, "endOffset": 191}, {"referenceID": 2, "context": "To develop such an algorithm, the problem can be restated as being one of predicting the outcome of a deterministic Markov chain, with the goal being to specify a value function that, for any given state, xt, will give the value of the immediate reward and the successor state, xt+1, thereby satisfying the Bellman equation (Bellman, 1957):", "startOffset": 324, "endOffset": 339}, {"referenceID": 1, "context": "The one limitation of this algorithm, however, is that it is slow (Baird, 1995; Williams & Baird, 1993).", "startOffset": 66, "endOffset": 103}, {"referenceID": 30, "context": "By instantiating error-driven reinforcement learning, our networks are capable of solving the types of large-scale and complex problems that biological organisms face in their struggle to survive and reproduce, and that are of interest to researchers in various problem domains (e.g., engineering; Sejnowski, 2010).", "startOffset": 278, "endOffset": 314}], "year": 2015, "abstractText": null, "creator": "Acrobat PDFMaker 11 for Word"}}}