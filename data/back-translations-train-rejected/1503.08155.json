{"id": "1503.08155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2015", "title": "Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories", "abstract": "This paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt. We propose IIKE (Imperfect and Incomplete Knowledge Embedding), a probabilistic model which measures the probability of each belief, i.e. $\\langle h,r,t\\rangle$, in large-scale knowledge bases such as NELL and Freebase, and our objective is to learn a better low-dimensional vector representation for each entity ($h$ and $t$) and relation ($r$) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase), so that we can use $||{\\bf h} + {\\bf r} - {\\bf t}||$ to assess the plausibility of a belief when conducting inference. We use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs, respectively. The results of extensive experiments show that IIKE achieves significant improvement compared with the baseline and state-of-the-art approaches.", "histories": [["v1", "Fri, 27 Mar 2015 17:13:03 GMT  (65kb,D)", "http://arxiv.org/abs/1503.08155v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["miao fan", "qiang zhou", "thomas fang zheng"], "accepted": false, "id": "1503.08155"}, "pdf": {"name": "1503.08155.pdf", "metadata": {"source": "CRF", "title": "Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories", "authors": ["Miao Fan", "Qiang Zhou", "Thomas Fang Zheng"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The explosive growth in the number of websites has drawn a lot of attention to research into the fundamentals of information [Sarawagi, 2008] in recent decades. The goal is to establish unstructured online texts so that we can store and exploit the distilled information as structured knowledge. Thanks to the long-term efforts of experts, crowdsourcing and even machine learning methods, several web-scale repositories have been built, such as Wordnet1, Freebase2 and NELL3, and most of them contain dozens of beliefs that are usually represented by triplets, i.e., head entity, tail entity >.Although we have collected colossal quantities of beliefs, the state of art in literature is. [West et al] reports that in this field our knowledge is baser."}, {"heading": "2 Related Work", "text": "We group current research aimed at embedding new beliefs into two categories based on knowledge repositories without additional texts, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and graph-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them \u2022 Symbolic representation v.s. Distributed representation: graph-based models view entities and relationships as atomic elements and represent them within a symbolic framework. In contrast, embedding models examine distributed representations by learning a low-dimensional continuous vector representation for each entity and relationship. \u2022 Relationspecific v.s. Open relation: graph-based models then base distributed representations on embedding models based on certain rules or rules for first specific constellations."}, {"heading": "2.1 Graph-based Inference", "text": "Graph-based inference models generally learn the representation for specific relationships from the knowledge graph. N-FOIL [Quinlan and Cameron-Jones, 1993] learns first-order horn-clause rules to derive from known new beliefs. So far, it has helped to learn about 600 such rules. However, its ability to draw conclusions about large repositories of knowledge is currently very limited. PRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random sequence model that follows the paths from the head unit to the tail unit on the local graph structure to generate nonlinear combinations of features representing the marked relationship, and uses logistical regression to select the significant features that contribute to the classification of other entity pairs belonging to the given relationship."}, {"heading": "2.2 Embedding-based Inference", "text": "However, it is all the more important to evaluate the compatibility of the triplet, and this model is a naive model that exploits the presence of information from the head and tail without taking into account the relationship between them. It defines a scoring function, and this model cannot distinguish between a pair of entities that comprise different relationships. Therefore, the unstructured is commonly regarded as the baseline. The model of distance (SE) uses a pair of beliefs (Wrh, Wrt) to characterize a relationship between two entities that involve different relationships."}, {"heading": "3 Model", "text": "The plausibility of a belief < h, r > can be regarded as the common probability of the head, which we can define as possible factors (Spain = Spain = Spain = Spain), the relation r and the entity t, namely Pr (h, r, t), Pr (h, t), Pr (r, t), Pr (h, t), Pr (h, t), Pr (h, t), Pr (r, t) and Pr (t, t), (t), (h, r) and Pr (h, r), more precisely, it corresponds to the geometric mean of Pr (h, r), t), Pr (r, h, t) shown in the following equation, Pr (h, r), t) = 3 (h | r, t) Pr (r (r, h, h), t) Pr (h, r (r, r, r)."}, {"heading": "4 Algorithm", "text": "In order to find the optimal solution to Equation (6), we use the Stochastic Gradient Descent (SGD) to update the embedding of units and relationships in an iterative way. However, it is costly to calculate the normalization terms in Pr (h | r, t), Pr (r | h, t) and Pr (t | h, r) directly. Illuminated by the work of Mikolov et al. [2013a], we have found an efficient approach that uses negative samples to transform the conditional probability functions, i.e. equation (2), (3) and (4), up to the binary classification problem, as shown in subsequent equations, logPr (h | r, t), logPr (1 | h, t) + k (1 Eh \u00b2) c \u00b2 c, c \u00b2 c, c \u00b2, c \u00b2, logPgorithm (1 Eh \u00b2 h \u00b2 h h = 1, logm = c c, c c \u00b2, c c c, c c \u00b2, c c c, c c c \u00b2, c c, c c c, c c c, logPr, c \u00b2 (h, logPr), c, c (h \u00b2 h), c, c, c, c, c, c, c, c \u00b2 h, c (1), c (h), c, c (h \u00b2 h h h, c, c, c, c, c, c \u00b2 h, c, c, c, c, c, c, c, c, c, c, c, c \u00b2 h, c, c, c, c, c, c, c, c, c, 1, c, c, c (1, c, c, c, c, c, c, c, (h \u00b2 h h, c, c, c, c, c, c, c, c, c, c (1)."}, {"heading": "Input:", "text": "Training set \u2206 = {(h, r, t, c)}, entity set E, relation set R; dimension of embedding d, number of negative samples k, learning rate \u03b1, convergence threshold \u03b7, maximum epochs n.1: / * Initialization * / 2: for each r-R do 3: r: = uniform (\u2212 6 \u221a d, 6 \u221a d) 4: r: = r | r | 5: end foreach 6: foreach e-E do 7: e: = uniform (\u2212 6 \u221a d, 6 \u221a d) 8: e: = e | e | 9: end foreach10: / * Training * /"}, {"heading": "11: i := 0", "text": "12: while Rel.loss > \u03b7 and i < n do 13: foreach < h, r, t >, do 14: foreach j (k) do 15: Negative sampling: < h, r, t >, h 16: / * \"h\" is the amount of k negative beliefs, the h * / 17: Negative sampling: < h, r \"j, t >, t >, r 18: / *\" r \"is the amount of k negative beliefs, the r * / 19: Negative sampling: < h, r, t\" j >, t 20: / * \"t\" is the amount of k negative beliefs, the t * / 21: end foreach 22: \"h, r, t, h,\" r, t, \"r,\" t \"1\" 2 (logPr (h, r, \"t,\" p, \"t),\" r, \"h,\" t."}, {"heading": "5 Experiments", "text": "Embedding entities and relationships in low-dimensional vector spaces facilitates several classical knowledge conclusions, such as link prediction and triplet classification. Specifically, link prediction performs inferences by predicting a ranking of missing entities or relationships based on the other two elements of triplet formation. For example, it may include a set of t given h and r or a bunch of h given r and t. And the triplet classification is intended to distinguish whether a triplet classification < h, r, t > is right or wrong.Several recent research papers [Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] reported that they have used subsets of freebase data (FB) to evaluate their models and have demonstrated performance in the above two tasks. To conduct solid experiments, we compare our model (IIKE) with many related studies, including those mentioned in Section 2.2. In addition, we use a larger and better data linkage to all of the NELE to perform tasks."}, {"heading": "5.1 Link prediction", "text": "One of the advantages of knowledge embedding is that we can apply simple vector calculations to many puzzles, and linking predictions is a valuable task that contributes to completing the knowledge graph. If we want to use knowledge embedding to say whether entity h has the relationship r to entity t, all we have to do is calculate the distance between h + r and t. The closer they are, the more likely there is the triplet < h, r, t >."}, {"heading": "Datasets", "text": "Bordes et al. [Bordes et al., 2013] published a large data set (FB15K) 6, extracted from freebase and constructed by crowdsourcing, in which each belief is a triplet with no confidence value. Therefore, we assign 1.0 by default to each training triplet. We have also identified a larger repository on the web called NELL7, which can be downloaded automatically from 6Related studies on this data set from the https: / / www.hds.utc.fr / everest / doku. php? id = en: transe7The entire data set of NELL can be downloaded from http: / / rtw.ml.cmu.edu / rtw / resourcesmachine learning techniques, and each triplet is labeled with a probability estimated by synthetic algorithms [Carlson et al., 2010]. We reserve the beliefs with probability ranges (0.5 - 1.0, prior to the table of 15K) that coincide with the overlaps and the examples of 15K."}, {"heading": "Evaluation Protocol", "text": "The dissimilarity of each candidate triplet is first calculated by various evaluation functions, such as | | h + r \u2212 t | |, and then sorted in ascending order. Finally, we locate the basic truth triplet and record its rank. This whole procedure proceeds in the same way when replacing the tail entity, so that we can get the mean results. We use two metrics, namely Mean Rank and Mean Hit @ 10 (the percentage of basic truth triplets ranked in the top 10), to measure performance. However, the results measured by these metrics are relatively inaccurate, as the method of doing so tends to generate false negative triplets. In other words, some of the candidate triplets rank higher than the basic truth triplets just because they also appear in the training set."}, {"heading": "Experimental Results", "text": "We compared IIKE with the state-of-the-art TransH, TransE and other models mentioned in Section 2.2, which were evaluated for FB15K and NELL. We adjusted the parameters of each previous Model 9 on the basis of the validation set and selected the combination of parameters leading to the best performance. The state-of-the-art results for FB15K are the same as those of Wang et al. [2014]. For IIKE, we tried several combinations of parameters: d = {20, 50, 100}, \u03b1 = {0,1, 0.005, 0.002}, b = {7,0, 10.0, 15.0} and norm = {L1, L2}, and finally chose d = 0.002, b = 7.0, norm = L2}, \u03b1 = L2 for the datasets of FB15K and SELE (d = 100, \u03b1 = 0.001, b = 1 for the NELL datasets)."}, {"heading": "5.2 Triplet classification", "text": "The triplet classification is another task proposed by Socher et al. [2013], which focuses on the search for a relationship-specific threshold \u03c3r to determine whether a triplet < h, r, t > is plausible."}, {"heading": "Datasets", "text": "Wang et al. [2014] constructed a standard data set FB15K from the freebase. In addition, we build another imperfect and incomplete data set, namely NELL, on the same principle that the head or tail can be replaced by another to produce a negative triplet, but in order to build very hard validation and test data sets, the principle emphasizes that the selected unit should appear at the same position once. For example, (Pablo Picaso, nationality, American) is a potential negative example and not the overtly irrational (Pablo Picaso, nationality, Van Gogh) in the face of a positive triplet (Pablo Picaso, nationality, Spanish), since American and Spanish occur more frequently than the tails of nationality. And the beliefs in the training sets are the same as in the triplet classification. Table 4 shows the statistics of the standard data sets we used to evaluate models for the triplet classification."}, {"heading": "Evaluation Protocol", "text": "The binary classification decision strategy is simple: if the dissimilarity of a test triplet calculated by fr (h, t) is below the relation-specific threshold \u03c3r, it is predicted to be positive, otherwise negative. \u03c3r, the relation-specific threshold, can be sought by maximising the classification accuracy of the validation triplets belonging to the relationship r."}, {"heading": "Experimental Results", "text": "We use the best combination of parameter settings in the link prediction task: d = 50, \u03b1 = 0.002, b = 7.0, norm = L2 for the FB15K dataset and d = 100, \u03b1 = 0.001, b = 7.0, norm = L1 for the NELL dataset to generate the entity and relation embeddings and learn the best classification threshold for each relation. Compared to several of the most recent approaches, i.e. TransH [Wang et al., 2014], TransE [Bordes et al., 2013] and Neural Tensor Network (NTN) 10 [Socher et al., 2013], the proposed IIKE approach still performs better, as shown in Table 5. We have also plotted the precision recall curves indicating the ability of global discrimination by ranking the removal of all test triplets, and Figure 2 shows that the AUC (Under the Areas) approach is much larger than IKE's other approaches."}, {"heading": "6 Conclusion", "text": "We have developed an elegant probabilistic embedding model to address this problem at the first attempt by measuring the probability of a given belief < h, r, t >. In order to efficiently learn the embedding for each entity and each relationship, we also use negative sampling technique to transform the original model and present the algorithm based on the SGD to find the optimal solution. Extensive experiments with knowledge conclusions, including link prediction and triplet classification, show that our approach achieves significant improvements in two large knowledge bases, compared to the state of the art and basic methods. We welcome further improvements to the proposed model, which leaves promising directions open for future work, such as the use of knowledge embedding to improve studies on summarizing texts and answering open questions."}, {"heading": "7 Acknowledgments", "text": "This work is supported by the National Program on Key Basic Research Project (973 Program) under Grant 2013CB329304,10Socher et al. reported higher classification accuracy in [Socher et al., 2013] for Word embedding. To make a fair comparison, the accuracy of NTN given in Table 5 is identical to the EV (Entity Vectors) results in Figure 4 of [Socher et al., 2013].National Science Foundation of China (NSFC) under Grant No.61373075."}], "references": [{"title": "A review of relation extraction", "author": ["Nguyen Bach", "Sameer Badaskar"], "venue": "Literature review for Language and Statistics II,", "citeRegEx": "Bach and Badaskar. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "et al", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "Learning structured embeddings of knowledge bases. In AAAI,", "citeRegEx": "Bordes et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multi-relational data"], "venue": "pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine Learning", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. A semantic matching energy function for learning with multi-relational data"], "venue": "94(2):233\u2013259,", "citeRegEx": "Bordes et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka Jr.", "Tom M. Mitchell"], "venue": "Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI 2010),", "citeRegEx": "Carlson et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Distant supervision for relation extraction with matrix completion", "author": ["Fan et al", "2014] Miao Fan", "Deli Zhao", "Qiang Zhou", "Zhiyuan Liu", "Thomas Fang Zheng", "Edward Y. Chang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "pages 833\u2013838", "author": ["Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M. Mitchell. Improving learning", "inference in a large knowledge-base using latent syntactic cues. In EMNLP"], "venue": "ACL,", "citeRegEx": "Gardner et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Rodolphe Jenatton", "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski"], "venue": "A latent factor model for highly multi-relational data. In NIPS, pages 3176\u20133184,", "citeRegEx": "Jenatton et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning", "author": ["Ni Lao", "William W Cohen. Relational retrieval using a combination of path-constrained random walks"], "venue": "81(1):53\u201367,", "citeRegEx": "Lao and Cohen. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing", "author": ["Ni Lao", "Tom Mitchell", "William W. Cohen. Random walk inference", "learning in a large scale knowledge base"], "venue": "pages 529\u2013539, Edinburgh, Scotland, UK., July", "citeRegEx": "Lao et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013a] Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "pages 746\u2013751", "author": ["Tomas Mikolov", "Wen tau Yih", "Geoffrey Zweig. Linguistic regularities in continuous space word representations. In HLT-NAACL"], "venue": "The Association for Computational Linguistics,", "citeRegEx": "Mikolov et al.. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al", "2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "In Proceedings of the 28th international conference on machine learning (ICML-11)", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data"], "venue": "pages 809\u2013816,", "citeRegEx": "Nickel et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Foil: A midterm report", "author": ["J. Ross Quinlan", "R. Mike Cameron-Jones"], "venue": "Proceedings of the European Conference on Machine Learning, ECML \u201993, pages 3\u201320, London, UK, UK,", "citeRegEx": "Quinlan and Cameron.Jones. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Foundations and trends in databases", "author": ["Sunita Sarawagi. Information extraction"], "venue": "1(3):261\u2013377,", "citeRegEx": "Sarawagi. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In NIPS", "author": ["Ilya Sutskever", "Ruslan Salakhutdinov", "Joshua B Tenenbaum. Modelling relational data using bayesian clustered tensor factorization"], "venue": "pages 1821\u20131828,", "citeRegEx": "Sutskever et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al", "2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, July", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Knowledge base completion via search-based question answering", "author": ["Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "WWW,", "citeRegEx": "West et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905)", "author": ["GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang. Exploring various knowledge in relation extraction"], "venue": "pages 427\u2013434, Ann Arbor, Michigan, June", "citeRegEx": "Zhou et al.. 2005", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 15, "context": "The explosive growth in the number of web pages has drawn much attention to the study of information extraction [Sarawagi, 2008] in recent decades.", "startOffset": 112, "endOffset": 128}, {"referenceID": 19, "context": "Although we have gathered colossal quantities of beliefs, state of the art work in the literature [West et al., 2014] reported that in this field, our knowledge bases", "startOffset": 98, "endOffset": 117}, {"referenceID": 20, "context": "To populate incomplete knowledge repositories, a large proportion of researchers follow the classical approach by extracting knowledge from texts [Zhou et al., 2005; Bach and Badaskar, 2007; Mintz et al., 2009].", "startOffset": 146, "endOffset": 210}, {"referenceID": 0, "context": "To populate incomplete knowledge repositories, a large proportion of researchers follow the classical approach by extracting knowledge from texts [Zhou et al., 2005; Bach and Badaskar, 2007; Mintz et al., 2009].", "startOffset": 146, "endOffset": 210}, {"referenceID": 14, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 8, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 9, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 6, "context": "The canonical approaches [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] generally conduct relationspecific random walk inference based on the local connectivity patterns learnt from the imperfect knowledge graph.", "startOffset": 25, "endOffset": 119}, {"referenceID": 17, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 7, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 1, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 2, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 16, "context": "The proposed methods [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] show promising performance, however, by means of learning from ground-truth training knowledge.", "startOffset": 21, "endOffset": 150}, {"referenceID": 14, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 8, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 9, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 6, "context": "We group recent research work related to self-inferring new beliefs based on knowledge repositories without extra texts into two categories, graph-based inference models [Quinlan and Cameron-Jones, 1993; Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] and embedding-based inference models [Sutskever et al.", "startOffset": 170, "endOffset": 264}, {"referenceID": 17, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 7, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 1, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 2, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 16, "context": ", 2013] and embedding-based inference models [Sutskever et al., 2009; Jenatton et al., 2012; Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013], and describe the principal differences between them,", "startOffset": 45, "endOffset": 155}, {"referenceID": 14, "context": "N-FOIL [Quinlan and Cameron-Jones, 1993] learns first order Horn clause rules to infer new beliefs from the known ones.", "startOffset": 7, "endOffset": 40}, {"referenceID": 8, "context": "PRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random walk model which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations representing the labeled relation, and uses logistic regression to select the significant features which contribute to classifying other entity pairs belonging to the given relation.", "startOffset": 4, "endOffset": 65}, {"referenceID": 9, "context": "PRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random walk model which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations representing the labeled relation, and uses logistic regression to select the significant features which contribute to classifying other entity pairs belonging to the given relation.", "startOffset": 4, "endOffset": 65}, {"referenceID": 6, "context": "PRA [Lao and Cohen, 2010; Lao et al., 2011; Gardner et al., 2013] is a data-driven random walk model which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations representing the labeled relation, and uses logistic regression to select the significant features which contribute to classifying other entity pairs belonging to the given relation.", "startOffset": 4, "endOffset": 65}, {"referenceID": 2, "context": "Unstructured [Bordes et al., 2013] is a naive model which exploits the occurrence information of the head and the tail entities without considering the relation between them.", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": "Distance Model (SE) [Bordes et al., 2011] uses a pair of matrices (Wrh,Wrt), to characterize a relation r.", "startOffset": 20, "endOffset": 41}, {"referenceID": 17, "context": "Bilinear Model [Sutskever et al., 2009; Jenatton et al., 2012] is another model that tries to fix the issue of weak interaction between the head and tail entities caused by Distance Model with a relation-specific bilinear form: fr(h, t) = hWrt.", "startOffset": 15, "endOffset": 62}, {"referenceID": 7, "context": "Bilinear Model [Sutskever et al., 2009; Jenatton et al., 2012] is another model that tries to fix the issue of weak interaction between the head and tail entities caused by Distance Model with a relation-specific bilinear form: fr(h, t) = hWrt.", "startOffset": 15, "endOffset": 62}, {"referenceID": 16, "context": "Neural Tensor Network (NTN) [Socher et al., 2013] designs a general scoring function: fr(h, t) = ur g(h Wrt + Wrhh+Wrtt+br), which combines the Single Layer Model and the Bilinear Model.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] is a canonical model different from all the other prior arts, which embeds relations into the same vector space of entities by regarding the relation r as a translation from h to t, i.", "startOffset": 7, "endOffset": 28}, {"referenceID": 11, "context": "Inspired by somewhat surprising patterns learnt from word embeddings [Mikolov et al., 2013b] illustrated by Figure 1, the result of word vector calculation, for instance vMadrid \u2212 vSpain + vFrance, is closer to vParis than to any other words [Mikolov et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 4, "context": "On the other hand, some imperfect repositories, such as NELL, which is automatically built by machine learning techniques [Carlson et al., 2010], assign a confidence score ([0.", "startOffset": 122, "endOffset": 144}, {"referenceID": 2, "context": "Several recent research works [Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] reported that they used subsets of Freebase (FB) data to evaluate their models and showed the performance on the above two tasks, respectively.", "startOffset": 30, "endOffset": 91}, {"referenceID": 16, "context": "Several recent research works [Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014] reported that they used subsets of Freebase (FB) data to evaluate their models and showed the performance on the above two tasks, respectively.", "startOffset": 30, "endOffset": 91}, {"referenceID": 2, "context": "[Bordes et al., 2013] released a large dataset (FB15K)6, extracted from Freebase and constructed by crowdsourcing, in which each belief is a triplet without a confidence score.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Unstructured [Bordes et al., 2014] 1,074 / 14,951 979 / 14,951 4.", "startOffset": 13, "endOffset": 34}, {"referenceID": 13, "context": "3% RESCAL [Nickel et al., 2011] 828 / 14,951 683 / 14,951 28.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "1% SE [Bordes et al., 2011] 273 / 14,951 162 / 14,951 28.", "startOffset": 6, "endOffset": 27}, {"referenceID": 3, "context": "8% SME (LINEAR) [Bordes et al., 2014] 274 / 14,951 154 / 14,951 30.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "8% SME (BILINEAR) [Bordes et al., 2014] 284 / 14,951 158 / 14,951 31.", "startOffset": 18, "endOffset": 39}, {"referenceID": 7, "context": "3% LFM [Jenatton et al., 2012] 283 / 14,951 164 / 14,951 26.", "startOffset": 7, "endOffset": 30}, {"referenceID": 2, "context": "1% TransE [Bordes et al., 2013] 243 / 14,951 125 / 14,951 34.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] 4,254 / 74,037 4,218 / 74,037 11.", "startOffset": 7, "endOffset": 28}, {"referenceID": 4, "context": "machine learning techniques, and each triplet is labeled with a probability estimated by synthetic algorithms [Carlson et al., 2010].", "startOffset": 110, "endOffset": 132}, {"referenceID": 3, "context": "Table 2 demonstrates that IIKE outperforms all the prior arts, including the baseline model Unstructured [Bordes et al., 2014], RESCAL [Nickel et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 13, "context": ", 2014], RESCAL [Nickel et al., 2011], SE [Bordes et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 1, "context": ", 2011], SE [Bordes et al., 2011], SME (LINEAR) [Bordes et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 3, "context": ", 2011], SME (LINEAR) [Bordes et al., 2014], SME (BILINEAR) [Bordes et al.", "startOffset": 22, "endOffset": 43}, {"referenceID": 3, "context": ", 2014], SME (BILINEAR) [Bordes et al., 2014], LFM [Jenatton et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": ", 2014], LFM [Jenatton et al., 2012] and TransE [Bordes et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 2, "context": ", 2012] and TransE [Bordes et al., 2013], and achieves significant improvements on the FB15K dataset, compared with the state-of-the-art TransH [Wang et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 16, "context": "NTN [Socher et al., 2013] 66.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "7% TransE [Bordes et al., 2013] 79.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": ", 2014], TransE [Bordes et al., 2013] and Neural Tensor Network (NTN)10 [Socher et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 16, "context": ", 2013] and Neural Tensor Network (NTN)10 [Socher et al., 2013], the proposed IIKE approach still outperforms them, as shown in Table 5.", "startOffset": 42, "endOffset": 63}, {"referenceID": 16, "context": "reported higher classification accuracy in [Socher et al., 2013] with word embeddings.", "startOffset": 43, "endOffset": 64}, {"referenceID": 16, "context": "parison, the accuracy of NTN reported in Table 5 is same with the EV (entity vectors) results in Figure 4 of [Socher et al., 2013].", "startOffset": 109, "endOffset": 130}], "year": 2015, "abstractText": "This paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt. We propose IIKE (Imperfect and Incomplete Knowledge Embedding), a probabilistic model which measures the probability of each belief, i.e. \u3008h, r, t\u3009, in largescale knowledge bases such as NELL and Freebase, and our objective is to learn a better lowdimensional vector representation for each entity (h and t) and relation (r) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase), so that we can use ||h + r \u2212 t|| to assess the plausibility of a belief when conducting inference. We use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs, respectively. The results of extensive experiments show that IIKE achieves significant improvement compared with the baseline and state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}