{"id": "1503.08169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2015", "title": "RankMap: A Platform-Aware Framework for Distributed Learning from Dense Datasets", "abstract": "This paper introduces RankMap, a platform-aware end-to-end framework for efficient execution of a broad class of iterative learning algorithms for massive and dense datasets. In contrast to the existing dense (iterative) data analysis methods that are oblivious to the platform, for the first time, we introduce novel scalable data transformation and mapping algorithms that enable optimizing for the underlying computing platforms' cost/constraints. The cost is defined by the number of arithmetic and (within-platform) message passing operations incurred by the variable updates in each iteration, while the constraints are set by the available memory resources. RankMap's transformation scalably factorizes data into an ensemble of lower dimensional subspaces, while its mapping schedules the flow of iterative computation on the transformed data onto the pertinent computing machine. We show a trade-off between the desired level of accuracy for the learning algorithm and the achieved efficiency. RankMap provides two APIs, one matrix-based and one graph-based, which facilitate automated adoption of the framework for performing several contemporary iterative learning applications optimized to the platform. To demonstrate the utility of RankMap, we solve sparse recovery and power iteration problems on various real-world datasets with up to 1.8 billion non-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlex platforms using up to 244 cores. The results demonstrate up to 2 orders of magnitude improvements in memory usage, execution speed, and bandwidth compared with the best reported prior work.", "histories": [["v1", "Fri, 27 Mar 2015 18:02:51 GMT  (1394kb,D)", "https://arxiv.org/abs/1503.08169v1", null], ["v2", "Thu, 27 Oct 2016 14:29:44 GMT  (988kb,D)", "http://arxiv.org/abs/1503.08169v2", "13 pages, 10 figures"]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["azalia mirhoseini", "eva l dyer", "ebrahim m songhori", "richard g baraniuk", "farinaz koushanfar"], "accepted": false, "id": "1503.08169"}, "pdf": {"name": "1503.08169.pdf", "metadata": {"source": "CRF", "title": "RankMap: A Framework for Distributed Learning from Dense Datasets", "authors": ["Azalia Mirhoseini", "Eva. L. Dyer", "Ebrahim. M. Songhori", "Richard Baraniuk", "Farinaz Koushanfar"], "emails": ["farinaz5}@rice.edu,", "edyer2@ric.org"], "sections": [{"heading": null, "text": "This year, as never before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country,"}, {"heading": "II. RANKMAP FRAMEWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Overview and approach", "text": "The main idea underlying our approach is to use the structure in large data collections to dissect the correlation (gram) matrix of the data in such a way that system costs (e.g. runtime, memory and energy) are significantly reduced in connection with iterative learning algorithms. Let the data matrix A-Rm \u00b7 n denote a collection of n signals of m dimensions, and G = ATA denote the gram matrix. Many learning algorithms iteratively update a solutionFig. 1: Schematic decomposing of a data matrix into the product of a small matrix and a large sparse matrix. Vector, denoted by x, corresponding to an update function of the following form: xiter + 1 = f (Gxiter), (1) where f () contains a large matrix and low complexity."}, {"heading": "B. Target applications", "text": "A large number of objective functions used in machine learning, such as the complexity of executing these methods, are dominated by costly iterative calculations on the gram matrix of the datasetrix. (i) We are now discussing in the real world two special learning algorithms that are evaluated in this paper: (i) economical thresholds and (ii) economical thresholds for eigenvalue decomposition. (i) thrift approximation for image denosis and classification. (i) thrift approximation for image representation is applied in a wide range of signal processing and machine learning."}, {"heading": "III. BACKGROUND AND RELATED WORK", "text": "In this section, we provide background information on methods of formatrix factorization and describe related work."}, {"heading": "A. Methods for matrix factorization", "text": "And, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,"}, {"heading": "B. Generic distributed abstractions", "text": "Some successful distributed abstractions for processing large data sets on clusters have been proposed, including MapReduce [13], Apache Spark [49], and SystemML [23]. However, these models become less efficient for applications where there is no direct data parallelism. Several new distributed abstractions have been proposed that model data dependence in graph format, in particular Pregel [35] and GraphLab [34]. They use a vertex-centric calculation model in which the user-defined programs run in parallel on each vertex. Since graph-based abstractions are suitable for sparse data sets, efficient data partitioning is not possible if the graph-based representation of the data is tightly linked. Furthermore, such tools rely mainly on communication between the vertices for the calculation. When the data are tightly connected, the resulting communication overload makes the calculation dramatically slow."}, {"heading": "IV. COLUMN SELECTION-BASED SPARSE DECOMPOSITION", "text": "(CSSD) In this section, we present a scalable matrix decomposition method (the decomposition phase in Figure 2), which we call Column Selection-Based Sparse Decomposition (CSSD)."}, {"heading": "A. Overview of CSSD method", "text": "The main idea behind CSSD is to first select a subset of columns of A, and then use that subset of columns as the basis from which we build the sparse representations of the remaining columns. \u2212 So we factorize the data as A = DV, where D is formed by subsampling, and the selected columns of A. Each column of V is then calculated by making the sparse approximation of the corresponding column of A relative to D. This sparse approximation problem can be solved by an efficient greedy routine called orthogonal matching (OMP) [12]. We provide pseudo code for CSSD in Algorithm 1.1) Step 1. Sequential column selection: To ensure that the total approximation in our factorization is sufficiently small, we must ensure that the columns selected from A to form D approximate the area of the original matrix."}, {"heading": "B. Complexity analysis", "text": "The complexity of the complexity terms corresponds to the calculation of D + or DD + A. The projection DD + a can be calculated independently for each column of A. The complexity of the sparse approximation (step 2) using the batch OMP method [43] is O (lmn + k2ln), where k < l is the average number of non-zeros per column of V. Likewise, batch OMP is applied independently for each column of A. Let us assume nc the number of parallel processing nodes. By storing D (a small m \u00b2 l matrix) and a uniform fraction of A in each node (i.e. nnc columns), the total complexity of algorithm 1 in a distributed setting can be written as O (nc (lm + k 2l) + l2m."}, {"heading": "C. Computational benefits of CSSD", "text": "In fact, the number of failures in the data structure is a function of (iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "V. TUNING DECOMPOSITION ERROR FOR A TARGET LEARNING ACCURACY", "text": "In the previous section, we discussed the computational advantage associated with introducing an approximation error into a CSSD decomposition. Of course, the accuracy of our learning algorithm is compromised if we increase the decomposition error (controlled by \u03b4D). Therefore, the key question is how much decomposition error we can afford to achieve a certain level of learning accuracy, and the answer to this question depends heavily on the specific learning algorithm and the application of interest.Previous theoretical studies have established a link between the total error in a factorization of a kernel (or graph) matrix and the accuracy of certain popular learning algorithms, including: kernel ridge regression and kernel SVM [10]. While for some learning algorithms our framework may use previous work to establish a relationship between ED and the learning error we denounce through \u0441L, the goal of this section is to suggest a generic approach to match the factorization error to achieve specified learning accuracy (we do this)."}, {"heading": "A. Error tuning", "text": "Given an already established relationship between the decomposition error and a certain algorithm, a user using our framework can easily set the decomposition error \u03b4D for CSSD to achieve a certain learning accuracy. If a user sets a target accuracy for a learning algorithm, the decomposition error \u03b4D can be set to achieve a certain learning error \u03b4L. Our strategy to ensure that we have a small learning error is to solve CSSD for a certain \u03b4D, map the resulting decomposition using the methods described in Section VI, and then calculate the accuracy of the learning algorithm \u03b4L. We then add D iteratively columns so that the decomposition error \u03b4D is small enough to ensure that it is within the error-specified tolerance. Depending on the underlying computational resources available, RankMap can be applied to multiple values of \u03b4D in parallel and the largest value of \u03b4D (the most compact representation) reaching a certain value of \u03b4L."}, {"heading": "VI. DISTRIBUTED EXECUTION AND DATA PARTITIONING", "text": "In this section, we present our approach to applying iterative updates to the decomposed data (the execution phase in Figure 2), describe a dependency-based update execution flow (i.e. Gx = VT (DTD) Vx), introduce an efficient method of partitioning the decomposed data in a distributed environment, and provide performance limits for memory usage, the number of flop operations, and the number of bytes communicated across the computing nodes."}, {"heading": "A. Computation flow", "text": "We propose two calculation models for the distributed implementation of an update in (1). Let us remember that for each iteration we have to calculate z = Gx = VT (DTD) Vx. We divide this calculation into four steps: (i) p = Vx (ii) r = Dp, (iii) p = DT r and (iv) z = VTp. The output vector z is used to generate an update of xiter + 1 = f (z + b), where b is an offset vector, and f (\u00b7) is a function of low complexity such as a soft threshold operator (sparse approximation) or a normalization (power method). To perform the calculation described above, we propose a matrix-based and texture-based model to apply iterative updates to the decomposed factors. We will now describe our implementation of both models."}, {"heading": "B. Matrix-based model", "text": "In this model, the data is stored in arrays, which enables us to capture the score in the score of the score. We assume that this is a compressed score that is capable of capturing the partition of the score in the score."}, {"heading": "C. Graph-based model", "text": "It is a very optimized machine for graph-based calculations based on iterative data, we make extensive adjustments to use our factorized graphics."}, {"heading": "VII. EVALUATIONS", "text": "In this section, we evaluate the performance of RankMap against a variety of data sets. Our evaluations examine: (i) the thescalability of CSSD and its ability to generate sparse representations; (ii) the link between decomposition error and learning accuracy for multiple learning applications, including face recognition, image denoting and PCA; (iii) the performance improvement of RankMap in terms of runtime and memory compared to previous work; and (iv) the performance of our distributed matrix and graph-based models for various structured data sets."}, {"heading": "A. Evaluation setup", "text": "In this context, it should be noted that this is a purely theoretical approach."}, {"heading": "B. Scaling of CSSD", "text": "Figure 5 shows how the runtime of CSSD increases with the number of processors for the VideoDict dataset. We increase the number of cores from 4 to 256 (on the IBM iDataPlex cluster).The dashed line shows the ideal scale-out behavior. As you can see, CSSD is highly parallel because it scales almost linearly with the number of processors and can therefore be applied to very large datasets."}, {"heading": "C. Sparse approximation", "text": "To evaluate the performance of RankMap for a low approximation, we use the fast iterative shrinkage threshold = absolute shrinkage threshold for certain algorithms (FISTA) [6] to solve the \"1 minimization problem\" in (2). We are investigating the usefulness of RankMap for two applications: frugal representation-based classification for face recognition and image analysis (see Section II-B for more details on these applications).1) Sparse representation-based classification for face recognition: Our goal is to use a collection of labeled images (training set) as our dictionary A and then form a sparse representation of a test image y in relation to A. After finding a sparse coefficient of vector x, we can then determine which signals are selected in the test sets (columns of A) to represent the test signal y. Based on the class of the selected columns, we then make a decision about which class is the signal."}, {"heading": "D. Power method", "text": "We also evaluate our performance methodology framework for three sets of data: Salinas, VideoDict and Light Field (i) (see section II-B for a more detailed discussion of the performance methodology) using the matrix-based model and running the experiments on 64 cores on an IBMiDataplex cluster. We run CSSD with various decomposition errors (\u03b4D) belonging to the following sentence: {0.4, 0.2, 0.1, 0.05, 0.001} and execute the performance methodology on each of the decomposition results. Figure 8a shows the lowness of V as we vary the error. As expected, a lower decomposition is achieved with larger error tolerances. Figure 8b shows the impact of the decomposition error on the accuracy of the performance method results. Here, the learning error (\u0441L) is defined as the normalized cumulative error of the first 100 eigenvalues. By lowering the decomposition error, we can detect significant improvements in the accuracy of the initial results of the standardization methodology."}, {"heading": "E. Graph- vs. matrix-based models", "text": "We compare the performance of the RankMap vertex and matrix-based models for different synthetic decomposed data. The purpose of these evaluations is to determine the advantages of each model in terms of the structure of the data. In all experiments, iterative updating in (3) is applied to a random input vector y. Experiments are performed on an IBM iDataPlex computing cluster. In all numbers, the runtime results for the dense matrix-based implementation (i.e., the regular provision of decomposed matrices without using the CCS format) are provided to demonstrate the efficiency achieved by using thrift in V through matrix-based and graph-based models. For the former model, we report analysis based on our C + MPI implementation and Format11the second model we base on our modification of GraphLab machine to implement RankMap."}, {"heading": "F. Memory Analysis", "text": "Table II compares the memory required to store matrices V and D. Memory usage of the original matrix A is also provided. We have also provided the memory savings for the case where D is formed in the same way as CSSD, but V is calculated using the smallest squares, as opposed to OMP.RankMap results in an improvement of up to 77.8 x (memory usage) over ATA and 8.6 x an improvement over adaptive decomposition based on standard 2 projection. The approximation error for both decomposition methods is set to \u03b4D = 0.1."}, {"heading": "G. Comparison with Spark", "text": "We have already presented the results based on our implementation of RankMap on GraphLab in Figures 9a, 9b and 9c. Now, we offer runtime comparisons between RankMap and an Apache Spark-based implementation of the power method based on baseline A versus the decomposed data DV. Remember that the iterative core update function used in the power method is provided in (4). We specify the average runtime per iteration with Spark and our implementations on the same hardware. Figure 10 shows the average runtime per iteration on a cluster of 8 nodes, 8 cores per processor for Salinas, VideoDict and Light Field (ii) datasets. As expected, our carefully tailored implementation of RankMap based on C + + MPI performed significantly better than Spark, by more than 2 orders of magnitude for Salinas, VideoDict and Light Field (ii) datasets."}, {"heading": "VIII. DISCUSSION", "text": "This year it is more than ever before."}], "references": [{"title": "SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M Aharon", "M Elad", "A Bruckstein"], "venue": "IEEE Trans Sig. Process., 54(11):4311\u20134322", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Model-based compressive sensing", "author": ["R G Baraniuk", "V Cevher", "M F Duarte", "C Hegde"], "venue": "IEEE Trans. Inf. Theory, 56(4):1982\u20132001", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A Beck", "M Teboulle"], "venue": "SIIMS, 2(1):183\u2013202", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E J Candes", "M B Wakin", "S P Boyd"], "venue": "JFAA, 14(5-6):877\u2013905", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "On a class of preconditioning methods for dense linear systems from boundary elements", "author": ["K Chen"], "venue": "SISC, 20(2):684\u2013698", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Atomic decomposition by basis pursuit", "author": ["S S Chen", "D L Donoho", "M A Saunders"], "venue": "SISC, 20(1):33\u201361", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["C Cortes", "M Mohri", "A Talwalkar"], "venue": "TAISTATS, pages 113\u2013120", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I Daubechies", "M Defrise", "C De Mol"], "venue": "Comm. Pure Appl. Math., pages 1413\u20131457", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Adaptive time-frequency decompositions", "author": ["G M Davis", "S G Mallat", "Z Zhang"], "venue": "OE, 33(7):2183\u20132191", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "MapReduce: simplified data processing on large clusters", "author": ["J Dean", "S Ghemawat"], "venue": "CACM, 51(1):107\u2013113", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A Deshpande", "L Rademacher", "S Vempala", "G Wang"], "venue": "SODA, pages 1117\u20131126. SIAM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering large graphs via the singular value decomposition", "author": ["P Drineas", "A Frieze", "R Kannan", "S Vempala", "V Vinay"], "venue": "Machine learning, 56(1-3):9\u201333", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["P Drineas", "M W Mahoney"], "venue": "JMLR, 6:2153\u2013 2175", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Self-expressive decompositions for matrix approximation and clustering", "author": ["E L Dyer", "T A Goldstein", "R Patel", "K P Kording", "R G Baraniuk"], "venue": "arXiv:1505.00824", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Greedy feature selection for subspace clustering", "author": ["E L Dyer", "A C Sankaranarayanan", "R G Baraniuk"], "venue": "JMLR, 14(1):2487\u20132517", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse subspace clustering: Algorithm", "author": ["E Elhamifar", "R Vidal"], "venue": "theory, and applications. TPAMI, 35(11):2765\u20132781", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S Fine", "K Scheinberg"], "venue": "JMLR, 2(Dec):243\u2013264", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C Fowlkes", "S Belongie", "F Chung", "J Malik"], "venue": "TPAMI, 26(2):214\u2013225", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A S Georghiades", "P N Belhumeur", "D J Kriegman"], "venue": "TPAMI, 23(6):643\u2013660", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "SystemML: Declarative machine learning on MapReduce", "author": ["A Ghoting", "R Krishnamurthy", "E Pednault", "B Reinwald", "V Sindhwani", "S Tatikonda", "Y Tian", "S Vaithyanathan"], "venue": "ICDE, pages 231\u2013242. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Revisiting the nystrom method for improved large-scale machine learning", "author": ["A Gittens", "M W Mahoney"], "venue": "ICML, pages 567\u2013575", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "PowerGraph: Distributed graph-parallel computation on natural graphs", "author": ["J E Gonzalez", "Y Low", "H Gu", "D Bickson", "C Guestrin"], "venue": "OSDI, pages 17\u201330", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "N-Body\u2019 problems in statistical learning", "author": ["A G Gray", "A W Moore"], "venue": "NIPS, pages 521\u2013527. MIT Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Video from a single coded exposure photograph using a learned over-complete dictionary", "author": ["Y Hitomi", "J Gu", "M Gupta", "T Mitsunaga", "S Nayar"], "venue": "ICCV, pages 287\u2013294. IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A E Hoerl", "R W Kennard"], "venue": "Technometrics, 12(1):55\u201367", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1970}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M Journ\u00e9e", "Y Nesterov", "P Richt\u00e1rik", "R Sepulchre"], "venue": "JMLR, 11(Feb):517\u2013553", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Motion segmentation by subspace separation and model selection", "author": ["K Kanatani"], "venue": "ICCV, volume 2, pages 586\u2013591", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C Lin"], "venue": "Neural Comput., 19(10):2756\u20132779", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["J Liu", "S J Wright", "C R\u00e9", "V Bittorf", "S Sridhar"], "venue": "JMLR, pages 285\u2013322", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "GraphLab: A new parallel framework for machine learning", "author": ["Y Low", "J E Gonzalez", "A Kyrola", "D Bickson", "C E Guestrin", "J Hellerstein"], "venue": "UAI, pages 340\u2013349", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G Malewicz", "M H Austern", "A JC Bik", "J C Dehnert", "I Horn", "N Leiser", "G Czajkowski"], "venue": "SIGMOD, pages 135\u2013146. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressive light field photography using overcomplete dictionaries and optimized projections", "author": ["K Marwah", "G Wetzstein", "Y Bando", "R Raskar"], "venue": "TOG, 32(4):46", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "LSRN: A parallel iterative solver for strongly over-or underdetermined systems", "author": ["X Meng", "M A Saunders", "M W Mahoney"], "venue": "SISC, 36(2):95\u2013 118", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Introduction to linear regression analysis", "author": ["D C Montgomery", "E A Peck", "G G Vining"], "venue": "John Wiley & Sons", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Expectation maximization and complex duration distributions for continuous time bayesian networks", "author": ["U Nodelman", "C R Shelton", "D Koller"], "venue": "arXiv preprint arXiv:1207.1402", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust sketching for multiple square-root LASSO problems", "author": ["V Pham", "L El Ghaoui"], "venue": "AISTATS", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Analytic PCA construction for theoretical analysis of lighting variability in images of a lambertian object", "author": ["R Ramamoorthi"], "venue": "TPAMI, 24(10):1322\u20131333", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2002}, {"title": "HOGWILD: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B Recht", "C Re", "S Wright", "F Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit", "author": ["R Rubinstein", "M Zibulevsky", "M Elad"], "venue": "Technion, Tech. Report, 40(8):1\u201315", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Less is more: Nystr\u00f6m computational regularization", "author": ["A Rudi", "R Camoriano", "L Rosasco"], "venue": "NIPS, pages 1657\u20131665", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "The curvelet transform for image denoising", "author": ["J Starck", "E J Cand\u00e8s", "D L Donoho"], "venue": "IEEE Trans Image Processing, 11(6):670\u2013684", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2002}, {"title": "Principal submatrices IX: Interlacing inequalities for singular values of submatrices", "author": ["R C Thompson"], "venue": "Linear Algebra and its Applications, 5(1):1\u201312", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1972}, {"title": "Sparse representation for computer vision and pattern recognition", "author": ["J Wright", "Y Ma", "J Mairal", "G Sapiro", "T S Huang", "S Yan"], "venue": "Proceedings of the IEEE, 98(6):1031\u20131044", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["J S Yedidia", "W T Freeman", "Y Weiss"], "venue": "Generalized belief propagation. NIPS, pages 689\u2013695", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2000}, {"title": "Resilient distributed datasets: A faulttolerant abstraction for in-memory cluster computing", "author": ["M Zaharia", "M Chowdhury", "T Das", "A Dave", "J Ma", "M McCauley", "M J Franklin", "S Shenker", "I Stoica"], "venue": "NSDI, pages 2\u20132. USENIX", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Spark: Cluster computing with working sets", "author": ["M Zaharia", "M Chowdhury", "M J Franklin", "S Shenker", "I Stoica"], "venue": "HotCloud, pages 10\u201310. USENIX", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse principal component analysis", "author": ["H Zou", "T Hastie", "R Tibshirani"], "venue": "J. Comp. Graph. Stat., 15(2):265\u2013286", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 43, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 149, "endOffset": 153}, {"referenceID": 33, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 184, "endOffset": 188}, {"referenceID": 34, "context": "Some prominent examples of such algorithms and their applications are linear or penalized regression [29], power iterations [30], belief propagation [48], and expectation maximization [38], [39].", "startOffset": 190, "endOffset": 194}, {"referenceID": 30, "context": ", Pregel [35], Spark [50], and GraphLab [34].", "startOffset": 9, "endOffset": 13}, {"referenceID": 45, "context": ", Pregel [35], Spark [50], and GraphLab [34].", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": ", Pregel [35], Spark [50], and GraphLab [34].", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "on each vertex [25].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Data with dense dependencies appear in a wide range of fields such as computer vision, medical image processing, boundary element methods and their applications, and N-body problems [8], [26].", "startOffset": 182, "endOffset": 185}, {"referenceID": 22, "context": "Data with dense dependencies appear in a wide range of fields such as computer vision, medical image processing, boundary element methods and their applications, and N-body problems [8], [26].", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "as the LASSO or BPDN [9], and ridge regression [29], are typically solved using iterative updates.", "startOffset": 21, "endOffset": 24}, {"referenceID": 24, "context": "as the LASSO or BPDN [9], and ridge regression [29], are typically solved using iterative updates.", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 122, "endOffset": 125}, {"referenceID": 42, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "Sparse representation is used in a wide range of signal processing and machine learning applications, including denoising [9], classification [47], clustering [18], and outlier detection [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 7, "context": "This sparse approximation problem can be solved using the following iterative soft thresholding (IST) algorithm [11]:", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": ", a soft-thresholding operator [11]) to account for the term \u03bb\u2016x\u20161 at each iteration, and \u03b3 is the step size.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "In our evaluations, we employ a variant of this algorithm called FISTA [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 27, "context": "is an example of a projected gradient descent (PGD) methods [32] which provide a generalization of standard gradient descent methods for certain classes of non-smooth objective functions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Extracting low dimensional structures not only reduces dimensionality, but also mitigates the effect of noise and improves the performance of learning and inference tasks [17], [19].", "startOffset": 171, "endOffset": 175}, {"referenceID": 15, "context": "Extracting low dimensional structures not only reduces dimensionality, but also mitigates the effect of noise and improves the performance of learning and inference tasks [17], [19].", "startOffset": 177, "endOffset": 181}, {"referenceID": 41, "context": "The truncated SVD also provides the solution to principal components analysis (PCA), which seeks to find a k-dimensional subspace that best approximates A in the least-squares sense [46].", "startOffset": 182, "endOffset": 186}, {"referenceID": 46, "context": "Two widely used sparse factorization methods include sparse PCA (SPCA) [51] and dictionary learning (DL) [4].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Two widely used sparse factorization methods include sparse PCA (SPCA) [51] and dictionary learning (DL) [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 25, "context": "To solve SPCA on big datasets, a generalized power method can be employed [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "CSS-based solutions form an approximate matrix decomposition in which one factorized component is a subset of the columns of the data itself [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "[15], Gramian matrix decomposition [16], image denoising and clustering [17], and also in spectral clustering [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 9, "context": "Examples include MapReduce [13], Apache Spark [49], and SystemML [23].", "startOffset": 27, "endOffset": 31}, {"referenceID": 44, "context": "Examples include MapReduce [13], Apache Spark [49], and SystemML [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Examples include MapReduce [13], Apache Spark [49], and SystemML [23].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "distributed abstractions have been proposed that model data dependency in a graph format, most notably Pregel [35] and GraphLab [34].", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "distributed abstractions have been proposed that model data dependency in a graph format, most notably Pregel [35] and GraphLab [34].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Because of this, most of these tools are designed based on the assumption that the input data is sparse [25], [35], [49].", "startOffset": 104, "endOffset": 108}, {"referenceID": 30, "context": "Because of this, most of these tools are designed based on the assumption that the input data is sparse [25], [35], [49].", "startOffset": 110, "endOffset": 114}, {"referenceID": 44, "context": "Because of this, most of these tools are designed based on the assumption that the input data is sparse [25], [35], [49].", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "This sparse approximation problem can be solved by an efficient greedy routine called orthogonal matching pursuit (OMP) [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Thus, we employ a sequential method to adaptively select columns that are not well approximated by the current set of columns [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 38, "context": "We employ a matching pursuit-based solver called Batch OMP [43] to solve (6).", "startOffset": 59, "endOffset": 63}, {"referenceID": 38, "context": "complexity of sparse approximation (Step 2), using the Batch OMP method [43], is O(lmn + kln), where k < l is the average number of non-zeros per column of V.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "When the data is approximately low rank, there exists a large body of work that characterizes the performance of the sequential column selection method (Step 1) used to form D [14], [24].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "When the data is approximately low rank, there exists a large body of work that characterizes the performance of the sequential column selection method (Step 1) used to form D [14], [24].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "In particular, the selection strategy in Step 2 of Algorithm 1 provides exponential decrease in the factorization error with each batch of columns that we select from A [14].", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "Then according to [14], the difference between the expected value of the approximation error, i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 36, "context": "For example, images of objects under different illumination conditions [41], motion trajectories of point-correspondences [31], neural data [17],", "startOffset": 71, "endOffset": 75}, {"referenceID": 26, "context": "For example, images of objects under different illumination conditions [41], motion trajectories of point-correspondences [31], neural data [17],", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "For example, images of objects under different illumination conditions [41], motion trajectories of point-correspondences [31], neural data [17],", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "to structured sparse and block-sparse signals [5] are all well-approximated by a union of low-dimensional subspaces.", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "When A lies on a union of subspaces, this effectively bounds the sparsity level of each column of V [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "Previous theoretical studies have established a connection between the total error in a factorization of a kernel (or Gram) matrix and the accuracy of certain popular learning algorithms, including: kernel ridge regression and kernel SVM [10].", "startOffset": 238, "endOffset": 242}, {"referenceID": 6, "context": "We observe a polynomial relationship holds both in theory [10] and in practice.", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "We use GraphLab Distributed API [34] to implement this model.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "be the master vertex and the others to be the replica vertices (these definitions are borrowed from GraphLab [25]).", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "We integrate and implement the proposed customized partitioning and distributed computation flow with the distributed GraphLab API [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "Our real datasets include Light Field data [2], hyper spectral images [1], a dictionary of video frames [28], and a collection of images of different faces under varying illumination conditions [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Our real datasets include Light Field data [2], hyper spectral images [1], a dictionary of video frames [28], and a collection of images of different faces under varying illumination conditions [22].", "startOffset": 194, "endOffset": 198}, {"referenceID": 44, "context": "We have also implemented the distributed update on the factorized data on Apache Spark [49].", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "The RankMap framework\u2019s sparse graph-based design is implemented using GraphLab, a high-level graph-parallel abstraction [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 2, "context": "To evaluate the performance of RankMap for sparse approximation, we use the fast iterative shrinkage-thresholding algorithm (FISTA) [6] to solve the `1-minimization problem in (2).", "startOffset": 132, "endOffset": 135}, {"referenceID": 31, "context": "to an overcomplete Light Field dictionary consisting of a large number of Light Field image patches collected from many scenes [36].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "Typically in image noise reduction applications, PSNR values of 30 dB and higher are desired [45], [7], [4].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Typically in image noise reduction applications, PSNR values of 30 dB and higher are desired [45], [7], [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "Typically in image noise reduction applications, PSNR values of 30 dB and higher are desired [45], [7], [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 32, "context": "There are a number of existing column sampling-based methods that aim to improve the performance of specific learning objectives, such as least-squares [37], `2-minimization with square root `1 penalty [40], and SVM [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 35, "context": "There are a number of existing column sampling-based methods that aim to improve the performance of specific learning objectives, such as least-squares [37], `2-minimization with square root `1 penalty [40], and SVM [20].", "startOffset": 202, "endOffset": 206}, {"referenceID": 16, "context": "There are a number of existing column sampling-based methods that aim to improve the performance of specific learning objectives, such as least-squares [37], `2-minimization with square root `1 penalty [40], and SVM [20].", "startOffset": 216, "endOffset": 220}, {"referenceID": 37, "context": "Our computation/communication and memory minimizing framework can also be applied to other optimization methods such as Stochastic Gradient Descent (SGD) [42] and Stochastic Coordinate Descent (SCD) [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "Our computation/communication and memory minimizing framework can also be applied to other optimization methods such as Stochastic Gradient Descent (SGD) [42] and Stochastic Coordinate Descent (SCD) [33].", "startOffset": 199, "endOffset": 203}, {"referenceID": 39, "context": "We have mainly explored its use for computational gains, however, recent theoretical results have shown that subsampling data can also be beneficial for learning [44].", "startOffset": 162, "endOffset": 166}], "year": 2016, "abstractText": "This paper introduces RankMap, a platform-aware end-to-end framework for efficient execution of a broad class of iterative learning algorithms for massive and dense datasets. Our framework exploits data structure to scalably factorize it into an ensemble of lower rank subspaces. The factorization creates sparse low-dimensional representations of the data, a property which is leveraged to devise effective mapping and scheduling of iterative learning algorithms on the distributed computing machines. We provide two APIs, one matrix-based and one graph-based, which facilitate automated adoption of the framework for performing several contemporary learning applications. To demonstrate the utility of RankMap, we solve sparse recovery and power iteration problems on various realworld datasets with up to 1.8 billion non-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlex servers using up to 244 cores. The results demonstrate up to two orders of magnitude improvements in memory usage, execution speed, and bandwidth compared with the best reported prior work, while achieving the same level of learning accuracy.", "creator": "LaTeX with hyperref package"}}}