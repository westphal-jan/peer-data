{"id": "1503.08316", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2015", "title": "A Variance Reduced Stochastic Newton Method", "abstract": "We present a new method to reduce the variance of stochastic versions of the BFGS optimization method, applied to the optimization of a class of smooth strongly convex functions. Although Stochastic Gradient Descent (SGD) is a popular method to solve this kind of problem, its convergence rate is sublinear as it is in fact limited by the noisy approximation of the true gradient. In order to recover a high convergence rate, one has to pick an appropriate step-size or explicitly reduce the variance of the approximate gradients. Another limiting factor of SGD is that it ignores the curvature of the objective function that can help greatly speed up convergence. Stochastic variants of BFGS that include curvature have shown good empirical performance but suffer from the same noise effects as SGD. We here propose a new algorithm V ITE that uses an existing technique to reduce this variance while allowing a constant step-size to be used. We show that the expected objective value converges to the optimum at a geometric rate. We experimentally demonstrate improved convergence rate on diverse stochastic optimization problems.", "histories": [["v1", "Sat, 28 Mar 2015 15:51:48 GMT  (1275kb)", "https://arxiv.org/abs/1503.08316v1", null], ["v2", "Wed, 1 Apr 2015 06:57:26 GMT  (1531kb)", "http://arxiv.org/abs/1503.08316v2", null], ["v3", "Mon, 20 Apr 2015 11:34:58 GMT  (1286kb)", "http://arxiv.org/abs/1503.08316v3", null], ["v4", "Tue, 9 Jun 2015 19:24:03 GMT  (1259kb)", "http://arxiv.org/abs/1503.08316v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aurelien lucchi", "brian mcwilliams", "thomas hofmann"], "accepted": false, "id": "1503.08316"}, "pdf": {"name": "1503.08316.pdf", "metadata": {"source": "CRF", "title": "A Variance Reduced Stochastic Newton Method", "authors": ["Aurelien Lucchi", "Brian McWilliams", "Thomas Hofmann"], "emails": ["@inf.ethz.ch"], "sections": [{"heading": null, "text": "ar Xiv: 150 3.08 316v 4 [cs.L G] 9J un2 01"}, {"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Stochastic second order optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem setting", "text": "Considering a dataset D = {(x1, y1),., (xn, yn), which consists of character vectors xi-Rd and targets yi-Rd, we consider the problem of minimizing the expected loss f (w) = E [fi (w)]. Each function fi (w) takes the form fi (w) = (h (w, xi), yi), where a loss function and h is a predictive model parameterized by w-Rd. Expectation is above the sample set and we denote w-Rd = argminw f (w).This optimization problem can be solved precisely for convex functions by means of gradient descent, whereby the gradient of the loss function is called wf (w) = E [wfi (w)]. Given the size of the dataset n, the calculation of the gradient is impractical and one must resort to stostic methods."}, {"heading": "2.2 Newton\u2019s method and BFGS", "text": "Newton's method is an iterative method that minimizes the Taylor expansion of f (w) by wt: f (w) = f (wt) + (w \u2212 wt) wf (wt) + 1 2 (w \u2212 wt) H (w \u2212 wt), (3) where H is the Hessian of the function f (w) and quantifies its curvature. Minimizing Equation 3 leads to the following update rule: wt + 1 = wt \u2212 throutH \u2212 1 t (wt), (4) where t is the step size chosen by trace line search. Given that the calculation and inversion of the Hessian matrix is a costly operation, approximate variants of the Newton method have emerged, in which H \u2212 1t is replaced by an approximate version H-1t (wt) by an approximate version H-1t."}, {"heading": "2.3 Stochastic BFGS", "text": "S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-"}, {"heading": "13: vt = \u2207fB(wt)\u2212\u2207fB(w\u0303) + \u03bd\u0303", "text": "14: wt + 1 \u2190 wt \u2212 \u03b7J \u0435t \u00b7 vt 15: Update J-t + 1 16: end for 17: w-s = wtj. 18: end for"}, {"heading": "4 Analysis", "text": "In this section, we present a convergence proof for the VITE algorithm, which builds on and generalizes previous analyses of the variance of reduced first order methods (8, 9). Specifically, we show how the variance reduction is sufficient on the stochastic evolutionary equation for J't to establish the specific convergence rates, even when performing linear transformations with a matrix. (9) Nevertheless, it shows that the variance reduction on the gradient estimate is sufficient to achieve rapid convergence as long as J't is sufficiently well behaved. Our analysis is based on the following standard assumptions: A1 Each function is differentiable and has a Lipschitz inclination with constant L > 0, i.e."}, {"heading": "5 Experimental Results", "text": "In fact, most of them are able to go in search of a solution that they have got a grip on. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them are able to go in search of a solution. (...) Most of them have gone in search of a solution. (...) Most of them have gone in search of a solution. (...) Most of them have gone in search of a solution. (...) Most of them have gone in search of a solution. (...) Most of them have gone in search of a solution. (...) Most of them have gone in search of a solution. (...)"}, {"heading": "6 Conclusion", "text": "We have introduced VITE and demonstrated that it achieves a geometric convergence rate for smooth convex functions - to our knowledge the first stochastic quasi-Newton algorithm with this property. We have experimentally demonstrated that VITE exceeds both variance-reduced SGD and stochastic BFGS. The theoretical analysis we present is quite general and additionally requires that the boundary to the eigenvalues of the inverse Hessian matrix remains in (19). Therefore, the reduced variance framework we propose can be extended to other quasinewtonian methods, including the widely used L-BFGS and ADAGRAD [7] algorithms. Finally, an important open question is how to bridge the gap between the theoretical and empirical results, in particular whether it is possible to achieve better convergence rates for stochastic BFGS algorithms that correspond to SVG results."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Proof of Lemma 2", "text": "The second discrepancy arises from the fact that the results of the last evaluation of the EIB (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIB\") (\"EIW\") (\"EIW\") (\") (\") (\")\" (\")\" (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\"(\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") (\") \") (\") (\")\") (\") (\") (\")\") \"(\") \")\" (\") (\") \"(\") (\")\") (\") (\") (\")\") \"(\") \"(\") \")\" (\")\" (\")\") \"(\") \"(\" (\")\") \"(\") \"(\") \"(\") \"(\") \")\" (\"(\") \")\" (\"(\") \"(\") \")\" (\"(\") \")\" (\"(\") \")\" (\")\" (\"(\") \")\" (\")\" (\"(\") \"(\") \"(\") \"(\") \"(\" (\")\" (\")\") \"(\" (\")\") \"(\" (\")\") \"(\") \"(\" (\"(\") \")\" (\"(\") \")\" (\"(\") \")\" (\"(\") \")\") \"("}], "references": [{"title": "et al", "author": ["F. Bach", "E. Moulines"], "venue": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pages 451\u2013459", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT, pages 177\u2013186. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A stochastic quasi-newton method for large-scale optimization", "author": ["R.H. Byrd", "S. Hansen", "J. Nocedal", "Y. Singer"], "venue": "arXiv preprint arXiv:1401.7020", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A parallel mixture of svms for very large scale problems", "author": ["R. Collobert", "S. Bengio", "Y. Bengio"], "venue": "Neural computation, 14(5):1105\u20131114", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "Advances in Neural Information Processing Systems, pages 1646\u20131654", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Jr and J", "author": ["J.E. Dennis"], "venue": "J. Mor\u00e9. Quasi-newton methods, motivation and theory. SIAM review, 19(1):46\u201389", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1977}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems, pages 315\u2013323", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Kone\u010dn\u1ef3", "P. Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method", "author": ["S. Lacoste-Julien", "M. Schmidt", "F. Bach"], "venue": "arXiv preprint arXiv:1212.2002", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, 45(1-3):503\u2013528", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1401.7625", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Numerical optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "volume 2. Springer New York", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "arXiv preprint arXiv:1109.5647", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pages 400\u2013407", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems, pages 2663\u20132671", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "Intl. Conf. Artificial Intelligence and Statistics (AIstats)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming, 127(1):3\u201330", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18].", "startOffset": 166, "endOffset": 173}, {"referenceID": 17, "context": "Stochastic gradient descent (SGD) has become the method of choice for such tasks as it only requires computing stochastic gradients over a small subset of datapoints [2, 18].", "startOffset": 166, "endOffset": 173}, {"referenceID": 0, "context": "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].", "startOffset": 73, "endOffset": 84}, {"referenceID": 9, "context": "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].", "startOffset": 73, "endOffset": 84}, {"referenceID": 13, "context": "The first line of work focuses on choosing the appropriate SGD step-size [1, 10, 14].", "startOffset": 73, "endOffset": 84}, {"referenceID": 4, "context": "This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage.", "startOffset": 23, "endOffset": 30}, {"referenceID": 15, "context": "This scheme is used in [5, 16] where the improved estimate of the gradient combines stochastic gradients computed at the current stage with others used at an earlier stage.", "startOffset": 23, "endOffset": 30}, {"referenceID": 7, "context": "A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.", "startOffset": 31, "endOffset": 37}, {"referenceID": 8, "context": "A similar approach proposed in [8, 9] combines stochastic gradients with gradients periodically re-computed at a pivot point.", "startOffset": 31, "endOffset": 37}, {"referenceID": 5, "context": "Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11].", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "Approximate variants known as quasi-Newton methods [6] have thus been developed, such as the popular BFGS or its limited memory version known as LBFGS [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Stochastic variants of BFGS have been proposed (oBFGS [17]), for which stochastic gradients replace their deterministic counterparts.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "A regularized version known as RES [12] achieves a sublinear convergence rate with a decreasing step-size by", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "SQN [3], another related method also requires a decreasing step size to achieve sub-linear convergence.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "Although stochastic second order methods have not be shown to achieve super-linear convergence, they empirically outperform SGD for problems with a large condition number [12].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "In this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients.", "startOffset": 109, "endOffset": 115}, {"referenceID": 8, "context": "In this paper, we propose and analyze a stochastic variant of BFGS that uses a multi-stage scheme similar to [8, 9] to progressively reduce the variance of the stochastic gradients.", "startOffset": 109, "endOffset": 115}, {"referenceID": 14, "context": "Given that the stochastic gradients are unbiased estimates of the gradient, Robbins and Monro [15] proved convergence of SGD to w assuming a decreasing stepsize sequence.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "A common choice for the step size is [18, 12] a) \u03b7t = \u03b70 t or b) \u03b7t = \u03b70T0 T0 + t (2) where \u03b70 is a constant initial step size and T0 controls the speed of decrease.", "startOffset": 37, "endOffset": 45}, {"referenceID": 11, "context": "A common choice for the step size is [18, 12] a) \u03b7t = \u03b70 t or b) \u03b7t = \u03b70T0 T0 + t (2) where \u03b70 is a constant initial step size and T0 controls the speed of decrease.", "startOffset": 37, "endOffset": 45}, {"referenceID": 11, "context": "Although the cost per iteration of SGD is low, it suffers from slow convergence for certain illconditioned problems [12].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "The most popular member of this class of quasi-Newton methods is BFGS [13] that incrementally updates an estimate of the inverse Hessian, denoted Jt = H\u0303 \u22121 t .", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "3 Stochastic BFGS A stochastic version of BFGS (oBFGS) was proposed in [17] in which stochastic gradients are used for both the determination of the descent direction and the approximation of the inverse Hessian.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "A regularized version of oBFGS (RES) was recently proposed in [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "Note that (10) also implies an upper and lower bound on E[\u0134t] [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "(11) The convergence proof derived in [12] shows that lower and upper bounds on the Hessian eigenvalues of the sample functions are sufficient to guarantee convergence to the optimum.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "Similar results were derived in [3] for the SQN algorithm.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "We proceed using a technique similar to the one proposed in [8, 9].", "startOffset": 60, "endOffset": 66}, {"referenceID": 8, "context": "We proceed using a technique similar to the one proposed in [8, 9].", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "Following the analysis of [8], the variance of vt goes to zero when both w\u0303 and wt converge to the same parameter w.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "4 Analysis In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9].", "startOffset": 172, "endOffset": 178}, {"referenceID": 8, "context": "4 Analysis In this section we present a convergence proof for the VITE algorithm that builds upon and generalizes previous analyses of variance reduced first order methods [8, 9].", "startOffset": 172, "endOffset": 178}, {"referenceID": 11, "context": "[12]) but allows us to remove this complication, simplifying notation in the analysis which follows.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This result follows directly from Lemma 3 in [9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "E\u2016vt\u2016 2 \u2264 4L(f(wt)\u2212 f(w ) + f(w\u0303)\u2212 f(w)) The proof is given in [8, 9] and reproduced for convenience in the Appendix.", "startOffset": 63, "endOffset": 69}, {"referenceID": 8, "context": "E\u2016vt\u2016 2 \u2264 4L(f(wt)\u2212 f(w ) + f(w\u0303)\u2212 f(w)) The proof is given in [8, 9] and reproduced for convenience in the Appendix.", "startOffset": 63, "endOffset": 69}, {"referenceID": 7, "context": "5 Experimental Results This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information.", "startOffset": 116, "endOffset": 119}, {"referenceID": 11, "context": "5 Experimental Results This section presents experimental results that compare the performance of VITE to SGD, SVRG [8] which incorporates variance reduction and RES [12] which incorporates second order information.", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "We apply least-square regression on the binary version of the COV dataset [4] that contains n = 581, 012 datapoints, each described by d = 54 input features.", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "Similarly to [12], we consider different choices for |A| and |B| and pick the best value in a limited interval {1, .", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "Finally, we compared VITE against SGD, RES and SVRG [8, 9].", "startOffset": 52, "endOffset": 58}, {"referenceID": 8, "context": "Finally, we compared VITE against SGD, RES and SVRG [8, 9].", "startOffset": 52, "endOffset": 58}, {"referenceID": 6, "context": "Therefore, the variance reduced framework we propose can be extended to other quasi-Newton methods, including the widely used L-BFGS and ADAGRAD [7] algorithms.", "startOffset": 145, "endOffset": 148}, {"referenceID": 11, "context": "A similar observation was made in [12].", "startOffset": 34, "endOffset": 38}], "year": 2015, "abstractText": "Quasi-Newton methods are widely used in practise for convex loss minimization problems. These methods exhibit good empirical performance on a wide variety of tasks and enjoy super-linear convergence to the optimal solution. For largescale learning problems, stochastic Quasi-Newton methods have been recently proposed. However, these typically only achieve sub-linear convergence rates and have not been shown to consistently perform well in practice since noisy Hessian approximations can exacerbate the effect of high-variance stochastic gradient estimates. In this work we propose VITE, a novel stochastic Quasi-Newton algorithm that uses an existing first-order technique to reduce this variance. Without exploiting the specific form of the approximate Hessian, we show that VITE reaches the optimum at a geometric rate with a constant step-size when dealing with smooth strongly convex functions. Empirically, we demonstrate improvements over existing stochastic Quasi-Newton and variance reduced stochastic gradient methods.", "creator": "LaTeX with hyperref package"}}}