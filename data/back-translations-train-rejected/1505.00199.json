{"id": "1505.00199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2015", "title": "Theory of Optimizing Pseudolinear Performance Measures: Application to F-measure", "abstract": "Non-linear performance measures are widely used for the evaluation of learning algorithms. For example, $F$-measure is a commonly used performance measure for classification problems in machine learning and information retrieval community. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes $F$-measure, \\emph{Jaccard Index}, among many others. We establish that many notions of $F$-measures and \\emph{Jaccard Index} are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the $F$-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on pseudo-linear measures, which are asymptotic in nature. We also establish the multi-objective nature of the $F$-score maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various $F$-measure optimization tasks.", "histories": [["v1", "Fri, 1 May 2015 15:25:59 GMT  (955kb,D)", "https://arxiv.org/abs/1505.00199v1", null], ["v2", "Tue, 19 May 2015 19:21:58 GMT  (0kb,I)", "http://arxiv.org/abs/1505.00199v2", "This paper has been withdrawn by the authors. There are some serious errors in the article, theoretical results were wrong"], ["v3", "Mon, 17 Aug 2015 18:53:59 GMT  (951kb,D)", "http://arxiv.org/abs/1505.00199v3", "Extended Version"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shameem a puthiya parambath", "nicolas usunier", "yves grandvalet"], "accepted": false, "id": "1505.00199"}, "pdf": {"name": "1505.00199.pdf", "metadata": {"source": "CRF", "title": "Optimizing Pseudo-Linear Performance Measures: Application to F-measure", "authors": ["Shameem A. Puthiya Parambath", "Nicolas Usunier", "Yves Grandvalet"], "emails": ["shameem.puthiya-parambath@utc.fr", "nusunier@utc.fr", "yves.grandvalet@utc.fr"], "sections": [{"heading": null, "text": "Keywords: machine learning, cost-sensitive classification, pseudo-linear performance measurements, F-score, Jaccard index"}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which"}, {"heading": "2. Background and Related Work", "text": "Here we give a brief overview of the latest methods for maximizing F measurement. We begin with the introduction of the notations used in the work; we also give the definitions of some basic quantities such as F\u03b2 measurements."}, {"heading": "2.1 Notation and Basic Definitions", "text": "We are (i) a measurable space X \u00b7 Y, where X is the attribute space and Y is the (finite) predictive set, (ii) a probability measurement \u00b5 over X \u00b7 Y, and (iii) a set of (measurable) classifiers H from the attribute space X to Y. Here we distinguish the predictive set Y from the label space L = {1,..., L}: in binary or single-line multi-class classification, the predictive set Y is the label L, but in the multi-character classification Y = 2L is the powerset of the possible labels. Within this framework, we assume that we have an i.d. sample from an underlying data distribution P on X \u00b7 Y. The empirical distribution of this finite training (or testing) is denounced by P (or test). Then we can use P as a measurement to obtain results at the population level (in terms of expected errors), or we can take the results on an endless sample."}, {"heading": "2.2 Related Work", "text": "The last few years have seen an increasing interest in this domain (?????????). The majority of work has been limited to F measurement, with very little work done on multi-label and multiclass F measurement maximization tasks (???). (??) An algorithm has been proposed to locate maximum F1 measurements for binary classification problems locally by approximating the classification results using logistic models. Since the objective function used is non-convex, the algorithm does not guarantee optimal distribution. This problem is addressed by performing the procedure multiple times and selecting the best ones in hand. The orthogonal problem of concluding the hypothesis with optimal F1 from a probability model is discussed by (?). In the scientific literature, the two problem formulations are referred to as empirical use maximization."}, {"heading": "3. Theoretical Framework and Analysis", "text": "In this section, we present the theoretical framework that lies at the heart of this work. Our results are mainly based on the maximization of F measurements for binary, multi-stage and multi-stage classification. They are based on a general property of these measures of performance, namely their pseudo-linearity in terms of false negative and false positive probabilities. For binary classification, we prove that it is sufficient to optimize F measurements to solve a problem of binary classification with different costs associated with false positive and false negative errors (Proposal 4). However, these costs are not known a priori, so in practice we propose to learn several classifiers at different costs and, in a second step, to select the best one according to the F measurements. Proposals 5 and 6 offer approximate guarantees for the F measurements that we can obtain by following this principle, depending on the granularity of the search in the cost interval."}, {"heading": "3.1 Error Profiles and Pseudo-Linearity", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Error Profiles", "text": "The performance of a classifier h by distribution \u00b5 can be summarized by the elements of the contingency table (see Table 2), which contains the summary of errors. For all classification tasks (binary, multiclass, and multilabel), the F measures we are considering here are functions of these non-diagonal elements of the contingency table, which are themselves defined by the marginal probabilities of the classes and the false negative / false positive probabilities per class. Limit probabilities of the designation k are denoted by Pk, and the false negative / false positive probabilities per class of a classifier h are defined by FNk (h) and FPk (h). Their definitions are given below: (binary / multiclass) Pk = \u00b5 ({(x, y) | y = k}), FNk (h), (x), and (x) h (x)."}, {"heading": "3.1.2 Pseudo-Linear Functions", "text": "Throughout the essay, we rely on the notion of pseudo-linearity of a function defined itself by the concept of pseudo-convexity (See?, Definition 3.2.1): a differentiable function F: D-Rd \u2192 R, defined on a convex open subset of Rd, is pseudo-convex if it is a pseudo-convex, F (e) > F (e) \u21d2 fraction. In practice, working with gradients of nonlinear functions can be cumbersome, so we will use the following characterization, which is a reformulation of?, Theorem 3.3.9, which essentially states that levels of pseudo-linear functions are hyperlinear: b > b > R functions and non-constant functions are: D-D (non-constant) and D-constant functions:"}, {"heading": "3.2 Pseudo-Linearity of F -measures", "text": "Several notions of F-measurements used in practical problems are pseudo-linear. Here, we find that binary F\u03b2 measurements and multi-class / multi-label macro / micro-F measurements are pseudo-linear functions."}, {"heading": "3.2.1 Binary Classification", "text": "In binary classification, we have FN2 = FP1, and we can write F measurements only with reference to class 1. Then, for each \u03b2 > 0 and each binary classifier h, we can perform the F\u03b2 measurement isF\u03b2 (h) = (1 + \u03b22) (P1 \u2212 FN1 (h)) (1 + \u03b22) P1 + FP1 (h) \u2212 FN1 (h). We can immediately determine that F\u03b2 is linearly fractionated and therefore pseudo-linear according to sentence 2 in FN1 and FP1. Therefore, we write the F\u03b2 measurement for binary classification as a function of vectors in R4 = R2L: (binary), i.e. R4, F\u03b2 (e) = (1 + \u03b22) (P1 \u2212 e1), where ei is the i-th element of the error profile."}, {"heading": "3.2.2 Multilabel Classification", "text": "In multi-label classification, there are several definitions of F measures. For those based on the error profiles, we first have the macro F measure (\u03b2 macro) MF\u03b2 (e) = 1L L L + k = 1 (1 + \u03b22) (Pk \u2212 e2k \u2212 1) (1 + \u03b22) Pk + e2k \u2212 1.MF\u03b2 (e) is not a pseudo-linear function of an error profile e. However, if the multi-label classification algorithm learns independent binary classifiers for each class (a method known as one-on-rest or binary relevance, see e.g.?), then the k-th binary problem alone depends on e2k \u2212 1 and e2k."}, {"heading": "3.2.3 Multiclass Classification", "text": "The last example we take is multi-class classification, which differs from multi-label classification in that for each example a single class (\u03b2) must be predicted, which imposes strong global constraints that make multi-class classification considerably more difficult. In the case of multi-label classification, there are many definitions of F measures for multi-class classification, and indeed several definitions for the micro-F measures themselves. We will focus on the following, which is used in information extraction (for example, in the BioNLP challenge?). Given the L class names, we assume that label 1 corresponds to a \"standard\" class whose prediction is not considered important. In information extraction, the standard class corresponds to the (majority) case in which no information should be extracted. Then, a false negative is an example (x, y) that y is such that y is written as y 6 = 1 and h (k) is an example (x = 6), while y is an example (y = 6)."}, {"heading": "3.3 Optimizing F -Measure by Reduction to Cost-Sensitive Classification", "text": "The F\u03b2 metrics presented above are non-linear aggregations of false negative / positive metrics that cannot be written within the usual framework of expected loss minimization; therefore, common learning algorithms are not inherently designed to optimize this type of performance metrics. In Proposition 4, we show that the optimal classifier for a cost-sensitive classification problem with brand-dependent costs (??) is also an optimal classifier for the pseudo-linear F metrics (within a specific but arbitrary classifier set H). In the cost-sensitive classification, each input of the error profile is asymmetrically weighted by a non-negative value, and the goal is to minimize the weighted average error. Efficient, consistent algorithms exist for such cost-sensitive (??). Although the costs that correspond to the optimal F metrics, we do not know of optimum prioritization with 5, we cannot prioritize the classifier."}, {"heading": "3.3.1 Reduction to Cost-Sensitive Classification", "text": "Let's F? = max e) F (e).E (H).E (H).E (H).E (H).E (H).H).E (H).E (H).E (H).E (H).E (H).E (H).E).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E (H).E).E (H).E (H).E (H).E (H).E) (.E) (.E) (.E) (.H).E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (H).E) (.E) (.E) (.E) (.E) (H).E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (H).E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (.E) (H (H (H (H).E).E) (H (H (H (H).E) ("}, {"heading": "3.3.2 Discretization Factor and Cost Interval for F\u03b2", "text": "Here we derive the values of the discretization factor (\u03a6) and the range of the cost interval (a) for the binary F\u03b2 measurement. Proposal 6 F\u03b2, defined in Section 3.2.1, fulfills the conditions of sentence 5 with: (binary) F\u03b2: \u03a6 = 1\u03b22P1 and a: t [0, 1] 7 \u2192 (1 + \u03b22 \u2212 t, 0, 0).This proposal gives the exact values of sentence 5 and the range for one in binary settings. Here, the discretization factor depends on the limit probability of the positive class (assumption 1 stands for positive class).A larger value of the discretization factor requires a smaller step size in the cost interval. If we consider the approximation guarantee in sentence 5 with a greater value of sentence 5, a reasonable approximation can be achieved by approximating the probability of the positive class (assumption 1 stands for positive class).Intuitively, we can assume that higher values of the discretization factor with a very few undesirable data."}, {"heading": "3.3.3 Algorithm for F\u03b2 Maximization", "text": "Based on the above results, we give a practical algorithm to find an optimal solution F\u03b2 = \u03b2, the cost function a: [0, 1] \u2192 Rd, which allocates the costs to the error probabilities, is Lipschitza-sensitive with the Lipschitz constant (\u03c6) = max (1, \u03b22). Therefore, it is sufficient to discredit the interval [0, 1] in order to have a series of evenly distributed values {t1,..., tC} (say, tj + 1 \u2212 tj = margin) in order to obtain \u03b50 coverage {a (t1), a (tC)} of the possible costs. Using the approximate warranty of Proposition 5, learning a cost-sensitive classification (hi) for each a (ti) and selecting the one with minimal total failure costs (< a (ti), hi (e) > on a validation quantity is sufficient to obtain an optimal solution."}, {"heading": "3.4 Beyond Binary F -measure", "text": "As already mentioned, many terms of F measures in multiclass and multi-label problems indicate that the effects of rare influences in the rare way cannot be recognized. (\u03b2 = \u03b2 = \u03b2 = \u03b2 = greater costs can be solved with our framework.) Here we derive the values of cost vector (a) and discretization factor (\u03a6) and give optimal F measurement algorithms for pseudo-linear F measurements defined in sections 3.2.2 and 3.4.1 Multilabel-F -measureProposition 8 multilabel micro-F (mF\u03b2), the conditions of Proposition 5 with: (multilabel-micro) mF\u03b2: (multi-micro-micro-micro) mF\u03b2 1 \u03b22).L k = 1 Pk and ai (t) = {1 + \u03b22 \u2212 t if i is odd, if i so. Here the discretization factor depends on the sum of the marginal probabilities of each label."}, {"heading": "4. Relationship to Multi-Objective Optimization", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr in rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr in rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "5. Experiments", "text": "This chapter illustrates the accuracy of the algorithms proposed by our theoretical framework, using F1 measurement, in binary and multilabel classification. Our experimental results for binary and multi-label macro F measurements (with binary relevance) show that (i) the selection of optimal F-classifiers is preferred by minimizing cost-insensitive classification results: a, e > is the same as the selection of classifiers with optimal F-measurements (iii), which in the case of multilabel micro F-measurements (iii) the selection of a classifier by swelling cost-sensitive values, which algorithms based on thresholds are preferred to minimize cost-insensitive classification results: to maximize F-measurements (iii) In the case of multi-label micro F-measurements, Optimal F-classifiers is the values that are implied with SVM thresholds, such as <"}, {"heading": "5.1 Importance of Thresholding", "text": "Although our theoretical developments show no need to exceed the values of the classifiers, the practical benefit of post-hoc adjustment of these values may be significant in maximizing the F1 measurement, as has already been noted in cost-sensitive learning scenarios (??). We are investigating the significance of the thresholds for classification a posteriori using didactic data called \"Galaxy.\" The data can be visualized as shown in Figure 4. Data distribution consists of four clusters of 2D examples, indexed by z-values {1, 2, 3, 4}, with a previous probability \u00b5 (z = 1) = 0.01, \u00b5 (z = 2) = 0.1, \u00b5 (z = 3) = 0.001, and \u00b5 (z = 4) = 0.889, with the respective class of previous probabilities \u00b5 (y = 1 | 1, 4, 4) being very high."}, {"heading": "5.2 Binary F\u03b2 and Multilabel MF\u03b2", "text": "The other datasets we use are Adult, RCV1, Scene, Siam and Yeast. In addition, we used a sample from the Galaxy data to demonstrate the empirical validity of the algorithm. Adult, RCV1 and Yeast are obtained from UCI repository3, and Scene and Siam from Libsvm repository4. Attributes of the data used in our empirical study are reported in Table 1.The results for binary F\u03b2 and multilabel macro-F (MF\u03b2) are reported in Tables 2 and 3 respectively. As the experimental results show, cost-sensitive learning and threshold thresholds lead to optimal results, while other methods work suboptimally, but the difference between the methods is less extreme than on the artificial Galaxy datasets."}, {"heading": "5.3 Multilabel mF\u03b2", "text": "In the case of multi-label micro-F measurement, we compare our algorithm with a commonly used method to find the best mF\u03b2 score suggested by?. In the proposed method, it is assumed that an optimal classifier for the micro-F score is an optimal classifier for the micro-F score. Therefore, the micro-F score corresponds to the optimal macro-F score obtained by executing binary relevance, as explained in Section 3.2.2.Table 5. We compare our algorithm for the micro-F score with the micro-F score. The results clearly show that the selection of the micro-F score corresponds to the maximum macro-F score (which corresponds to the Fmaxin table)."}, {"heading": "5.4 Cost Space Search Overhead", "text": "Since the actual costs associated with a misclassification differ from the costs associated with a replacement loss, it introduces an additional loop into our algorithm. Therefore, searching for an optimal cost vector in the discredited cost interval might not be a good idea, especially if the value of \u03b2 > q is large. Here, we perform an empirical analysis of the functional dependencies between the actual cost and the corresponding F measure and develop an improved version of the diagrams of micro-F measure against false negative costs contained in Section 3.4.5 of Figure 5. The diagram shows that micro-F measure is a quasi-concave function of false negative costs. A function is quasi-concave if each superplane of the function is convex (?). Formally, a function is g: D-Rd \u2192 R, quasi-interval two points (x-concave) if {x-D | g (x) is a convex value."}, {"heading": "6. Conclusion", "text": "The results are based on each data set, for each function class, and on any assumptions for data distribution (label-dependent or label-independent).We proposed algorithms for optimizing F measurements based on minimizing the total cost of the cost-sensitive classification. We demonstrated experiments with linear classifiers, showing theoretical interest in using cost-sensitive classification algorithms instead of probability thresholds. It was also shown that in F measurement maximization, even the threshold of cost-sensitive algorithms helps to achieve good performance.Empirically and algorithmically, we examined only the simplest case of our result (F\u03b2measure in binary classification and Macro-F\u03b2 measurement and Micro-F\u03b2 measurement in multi-label classification), but much more results can be obtained for inadequate measurement of F measurements (F\u03b2measure in binary classification and Micro-F\u03b2 measurement in multi-label classification)."}, {"heading": "Acknowledgments", "text": "This work was carried out and financed within the framework of the Labex MS2T and supported by the Picardy Region and the French Government under the \"Investing in the future\" programme managed by the National Research Agency (reference ANR-11IDEX-0004-02)."}, {"heading": "Appendix A. Proofs of Propositions and Corollaries", "text": "Proposition 2 A linear-fractional function F: D Rd \u2192 R is the ratio of linear functions F (e) = \u03b10 + < \u03b3, e > \u03b11 + \u00b2, e >. A non-constant linear-fractional function is pseudo-linear on open hemispace D (e) ="}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "State of the art classification algorithms are designed to minimize the misclassification error of the system, which is a linear function of the per-class false negatives and false positives. Nonetheless non-linear performance measures are widely used for the evaluation of learning algorithms. For example, F -measure is a commonly used non-linear performance measure in classification problems. We study the theoretical properties of a subset of non-linear performance measures called pseudo-linear performance measures which includes F -measure, Jaccard index, among many others. We establish that many notions of F -measures and Jaccard index are pseudo-linear functions of the per-class false negatives and false positives for binary, multiclass and multilabel classification. Based on this observation, we present a general reduction of such performance measure optimization problem to cost-sensitive classification problem with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F -measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on binary F -score, which are asymptotic in nature. Our analysis shows that thresholding cost-insensitive scores, a common technique employed to optimize F -measure, yields sub-optimal results. We also establish the multi-objective nature of the F -measure maximization problem by linking the algorithm with the weighted-sum approach used in multi-objective optimization. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F -measure optimization tasks.", "creator": "LaTeX with hyperref package"}}}