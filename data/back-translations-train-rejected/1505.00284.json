{"id": "1505.00284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2015", "title": "Bayesian Policy Reuse", "abstract": "A long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain. Acting online requires 'fast' responses, in terms of rapid convergence, especially when the task instance has a short duration, such as in applications involving interactions with humans. These requirements can be problematic for many established methods for learning to act. In domains where the agent knows that the task instance is drawn from a family of related tasks, albeit without access to the label of any given instance, it can choose to act through a process of policy reuse from a library, rather than policy learning from scratch. In policy reuse, the agent has prior knowledge of the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase. We formalise the problem of policy reuse, and present an algorithm for efficiently responding to a novel task instance by reusing a policy from the library of existing policies, where the choice is based on observed 'signals' which correlate to policy performance. We achieve this by posing the problem as a Bayesian choice problem with a corresponding notion of an optimal response, but the computation of that response is in many cases intractable. Therefore, to reduce the computation cost of the posterior, we follow a Bayesian optimisation approach and define a set of policy selection functions, which balance exploration in the policy library against exploitation of previously tried policies, together with a model of expected performance of the policy library on their corresponding task instances. We validate our method in several simulated domains of interactive, short-duration episodic tasks, showing rapid convergence in unknown task variations.", "histories": [["v1", "Fri, 1 May 2015 21:13:00 GMT  (376kb,D)", "https://arxiv.org/abs/1505.00284v1", "32 pages, submitted to the Machine Learning Journal"], ["v2", "Mon, 14 Dec 2015 15:44:51 GMT  (418kb,D)", "http://arxiv.org/abs/1505.00284v2", "32 pages, submitted to the Machine Learning Journal"]], "COMMENTS": "32 pages, submitted to the Machine Learning Journal", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["benjamin rosman", "majd hawasly", "subramanian ramamoorthy"], "accepted": false, "id": "1505.00284"}, "pdf": {"name": "1505.00284.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Benjamin Rosman", "Majd Hawasly", "Subramanian Ramamoorthy"], "emails": ["BRosman@csir.co.za.", "M.Hawasly@ed.ac.uk.", "S.Ramamoorthy@ed.ac.uk."], "sections": [{"heading": null, "text": "Benjamin Rosman Mobile Intelligent Autonomous Systems (MIAS), Council for Scientific and Industrial Research (CSIR), South Africa, and the School of Computer Science and Applied Mathematics, University of the Witwatersrand, South Africa. Email: BRosman @ csir.co.za.Majd Hawasly School of Informatics, University of Edinburgh, UK. Email: M.Hawasly @ ed.ac.uk.Subramanian Ramamoorthy School of Informatics, University of Edinburgh, UK. Email: S.Ramamoorthy @ ed.ac.uk.ar Xiv: 150 5.00 284v 2 [cs.A I] 1 4D ec2 01Keywords Policy Reuse \u00b7 Reinforcement Learning \u00b7 Online Learning \u00b7 Online Bandits \u00b7 Transfer Learning \u00b7 Bayesian Optimisation \u00b7 Bayesian Decision Theory."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves how they have behaved."}, {"heading": "2 Bayesian Policy Reuse", "text": "This could be the case where aspects of the model vary qualitatively (e.g. different personality types), or where the agent is not exposed to enough variations of the task to learn the underlying task. Bayesian Policy Reuse (BPR) builds on the intuition that in many cases the performance of a specific policy is better, relative to the other policies in the library, where there is a reward for the task for which it is known to be optimal. Thus, a model that measures the similarity between a new task and other known tasks may provide clues as to which policies are best to reuse. We learn such a model from offline experience and then use it online as Bayesian before the task, which is updated with new observations from the current task. Note that in this work we consider the general case where we parameterise the task area, which allows the construction of this model explicitly (e.g., the qualitative nature of the task is sufficiently exposed to different aspects of the personality)."}, {"heading": "2.8.1 State-Action-State Tuples", "text": "The most detailed information signal that the agent could receive is the history of all (s, a, s) tuples that occur during the implementation of a policy. Therefore, the observational model is in this case an empirical estimate of the expected transitional function of MDPs under type \u03c4. However, the significance of this signal has a disadvantage, as it is expensive to learn and maintain these models for any possible type. Furthermore, this cannot be generalized well in cases with sparse sampling. On the other hand, this form of signal is useful in cases where some environmental factors can influence the behavior of the agent in a way that is not directly related to the achievement of an episodic target. Let's take an aerial agent, for example, who can apply different navigation strategies under different wind conditions."}, {"heading": "2.8.2 Instantaneous Rewards", "text": "Another form of information is the immediate reward r-R that is received during the implementation of a policy for a state-action pair. Then, the observation model is an empirical estimate of the expected reward function for the MDPs in this way. Although this is a more abstract signal than the state-action-state tuples, it can still provide a relatively fine-grained knowledge of the behavior of the task if the interim rewards are informative. It should be useful in scenarios where the task has a number of sub-components that individually contribute to the overall performance, for example in assembly tasks."}, {"heading": "2.8.3 Episodic Returns", "text": "The observation model of such a scalar signal is much more compact and therefore easier to learn and justify than the two previous proposals. - We also note that the execution of a policy for our intended applications cannot be terminated prematurely, which means that the agent always has an episodic return signal at his disposal before he selects a new policy. -, - - This signal is useful for problems of delayed reward, where intermediate states cannot easily be evaluated, but the extent to which the task has been successfully completed defines the return. In our framework, the use of episodic return signals -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -,, -, -, -, -, -, -, -, -, -, -, -, -, -, -,, -, -, -, -,, -, -, -, -, -, -, -, -, -, -,,, -, -,, -,, -, -, -,, -, -, -, -,, -,,, -, -,, -,, -, -, -,, -, -, -,, -, -, -, -,, -,, -, -, -, -, -, -, -,, -, -, -,, -, -, -,,,, -,, -, -, -,, -, -, -, -,, -, -, -, -, -, -, -,, -,, -,, -,, -, -,, -, -, -, -,, -, -, -, -, -, -, -, -, -, -,, -, -, -,, -, -, -,,, -, -, -, -, -, -, -,,, -, -, -,, -, -, -,, -, -, -, -, -, -, -, -, -,,,,"}, {"heading": "3 Policy Selection for BPR", "text": "The selection of a political strategy for each episode (line 3 in algorithm 1) is a critical step in the evolution of the BPR, which, given the current type of belief in a possible policy, we must choose in order to achieve two simultaneous objectives: obtaining useful information about the new (current) task and, at the same time, avoiding the accumulation of additional repentance. At the core of this policy-selection problem, the trade-off between exploration and exploitation is required to gather as much information about the task as possible, so that the policy is optimally balanced into the future, while minimizing performance losses resulting from suboptimal policy choices. Our problem can be mapped to a finite horizon of total reward, in which the weapons represent the policy defined by the limited number of episodes of the limited scope of the episodes of optimism."}, {"heading": "3.1.1 Probability of Improvement and Expected Improvement", "text": "The first heuristic for policy selection uses the probability of a specific policy achieving a hypothetical performance increase. Suppose the probability of an improvement (PI) is greater than the best estimate from the current point of view, U + > U = Maximum Performance Increase (EI). In principle, the probability of an improvement (PI) opts for the policy that maximizes the term, and this choice is the primary factor influencing the performance of this exploration principle. One approach to addressing this choice is the associated idea of an expected improvement (EI). This exploration integrates the heuristic values of the improvement (U + < U + < Umax, and the policy is chosen in terms of potential benefit, namely through the associated idea of an expected improvement (EI)."}, {"heading": "3.1.2 Belief Entropy", "text": "An alternative approach is to choose the policy that will have the greatest effect in reducing uncertainty about the type space. The Entropy of Faith (BE) exploration euristics aims to estimate the impact of any policy on the reduction of uncertainty about the type space represented by the entropy of faith. For each of these strategies, estimate the expected entropy of faith after executing \u03c0 asH (\u03b2 | \u03c0) = \u2212 \u03b2\u03c0 log \u03b2\u03c0, where \u03b2\u03c0 is the updated faith after seeing the signal expected from executing \u03c0, given the probability that it will be exploited under the current faith."}, {"heading": "3.1.3 Knowledge Gradient", "text": "The final exploration euristics we describe is the knowledge gap (Powell, 2010), which aims to balance exploration and exploitation by optimizing short-sighted returns while maintaining asymptotic optimism. The principle behind this approach is to estimate a one-step knowledge gap and to select policies that maximize the benefits over both the current timeframe and the next one in terms of the information obtained. To select a policy that utilizes the knowledge gap, we choose policies that maximize the online knowledge gap at a given time."}, {"heading": "4 Experiments", "text": "4.1 Club SelectionAs an initial, illustrative simulated experiment, we look at the problem of a robot golfer making a shot with one of four golf clubs on an unknown golf course where it is not possible to reliably estimate the distance to the hole, as in this example, we look at a robot with weak sensors, which in themselves are not sufficient to reliably measure the distance. The robot is only able to make K = 3 shots, which is less than the number of clubs available, from a fixed position out of the hole. The task is evaluated by the holding distance of the ball to the hole. The robot can select one of the available clubs, and we assume that the robot will use a fixed, canonical strike with each club. In this setting, we look at the type room T to be a series of different golfing experiences of the robot before each is defined for the simplicity of the target (other factors, e.g. weather conditions, could just as well be included)."}, {"heading": "5.3.1 Bayesian Optimisation", "text": "In this context, it should be noted that this is an attempt to control the effects of the crisis on the economy, both on the economy and on the economy."}, {"heading": "5.3.2 Bayesian Reinforcement Learning", "text": "Bayesian Reinforcement Learning (BRL) is a paradigm of Reinforcement Learning that deals with uncertainty in an unknown MDP in a Bayesian way by maintaining a probability distribution over the space of possible MDPs and updating that distribution based on the observations generated from the MDP as the interaction continues (Dearden et al., 1999). In the work of Wilson et al. (2007), the problem of multi-task reinforcement learning of a possibly-infinite stream of MDPs is addressed within a Bayesian framework. The authors model the generative process of MDP using a hierarchical, infinite mixing model, assuming that each MDP is generated from a series of initially unknown classes, and a hyper-preceding one controls the distribution of classes. Bayesian Policy Reuse can be considered as a special instance of bayesian Multitask Reinforcement Learning with the following construction."}, {"heading": "5.3.3 Other Bayesian Approaches", "text": "Engel and Ghavamzadeh (2007) introduce a Bayesian treatment of the policy gradient method for reward in the affirmation of learning; the gradient of some parameterized policy spaces is modeled as a Gaussian process, and paths sampled from the MDP (completed episodes) are used to calculate the gradient and optimize policy by moving toward the power gradient; the use of Gaussian processes in the political space is similar to the interpretation of our approach, but their use serves to model the gradient rather than the performance itself. If no gradient information is available to guide the search, Wingate et al. (2011) suggest using the MCMC to search in the space of politics, which is equipped with an earlier approach; various types of hierarchical priors that can be used to narrow the search are discussed."}, {"heading": "6 Conclusion", "text": "In this paper, we deal with the problem of political re-use, which consists of responding to an unknown task by choosing between a number of strategies available to the agent to minimize his regrets about the best policy on set within a short number of episodes. This problem is motivated by many areas of application where tasks are short-lived, such as human interaction and personalization or task monitoring. Faith is updated using ancillary information (signals) available to the agent: observation signals acquired online for the new instance, and signal models acquired offline for each policy. To balance the trade between exploration and exploitation, several mechanisms for selecting strategies from faith (exploration, heuristics) are described."}], "references": [{"title": "An overview of the conservation status of and threats to rhinoceros species in the wild", "author": ["R. Amin", "K. Thomas", "R.H. Emslie", "T.J. Foose", "N. Strien"], "venue": "International Zoo Yearbook,", "citeRegEx": "Amin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2006}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["Eric Brochu", "Vlad M. Cora", "Nando De Freitas"], "venue": "arXiv preprint arXiv:1012.2599,", "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Sample complexity of multi-task reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": "In Proceedings of The 29th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Brunskill and Li.,? \\Q2013\\E", "shortCiteRegEx": "Brunskill and Li.", "year": 2013}, {"title": "Learning parameterized skills", "author": ["B.C. da Silva", "G.D. Konidaris", "A.G. Barto"], "venue": "In Proceedings of the Twenty Ninth International Conference on Machine Learning,", "citeRegEx": "Silva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "Model based bayesian exploration", "author": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In Proceedings of the fifteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Bayesian policy gradient algorithms. In Advances in Neural Information Processing Systems", "author": ["Yaakov Engel", "Mohammad Ghavamzadeh"], "venue": "Proceedings of the 2006 Conference,", "citeRegEx": "Engel and Ghavamzadeh.,? \\Q2007\\E", "shortCiteRegEx": "Engel and Ghavamzadeh.", "year": 2007}, {"title": "Probabilistic policy reuse in a reinforcement learning agent", "author": ["F. Fern\u00e1ndez", "M. Veloso"], "venue": "In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Fern\u00e1ndez and Veloso.,? \\Q2006\\E", "shortCiteRegEx": "Fern\u00e1ndez and Veloso.", "year": 2006}, {"title": "Response surface bandits", "author": ["Josep Ginebra", "Murray K. Clayton"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Ginebra and Clayton.,? \\Q1995\\E", "shortCiteRegEx": "Ginebra and Clayton.", "year": 1995}, {"title": "A dynamic allocation index for the discounted multiarmed bandit problem", "author": ["J.C. Gittins", "D. Jones"], "venue": "Progress in Statistics,", "citeRegEx": "Gittins and Jones.,? \\Q1974\\E", "shortCiteRegEx": "Gittins and Jones.", "year": 1974}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Knowledge transfer in reinforcement learning", "author": ["Alessandro Lazaric"], "venue": "PhD thesis, PhD thesis, Politecnico di Milano,", "citeRegEx": "Lazaric.,? \\Q2008\\E", "shortCiteRegEx": "Lazaric.", "year": 2008}, {"title": "Clustering markov decision processes for continual transfer", "author": ["M.M. Hassan Mahmud", "Majd Hawasly", "Benjamin Rosman", "Subramanian Ramamoorthy"], "venue": "arXiv preprint arXiv:1311.3959,", "citeRegEx": "Mahmud et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mahmud et al\\.", "year": 2013}, {"title": "Adapting interaction environments to diverse users through online action set selection", "author": ["M.M. Hassan Mahmud", "Benjamin Rosman", "Subramanian Ramamoorthy", "Pushmeet Kohli"], "venue": "In AAAI 2014 Workshop on Machine Learning for Interactive Systems,", "citeRegEx": "Mahmud et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmud et al\\.", "year": 2014}, {"title": "Latent bandits", "author": ["Odalric-Ambrym Maillard", "Shie Mannor"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Maillard and Mannor.,? \\Q2014\\E", "shortCiteRegEx": "Maillard and Mannor.", "year": 2014}, {"title": "A structured multiarmed bandit problem and the greedy policy", "author": ["Adam J. Mersereau", "Paat Rusmevichientong", "John N. Tsitsiklis"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Mersereau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mersereau et al\\.", "year": 2009}, {"title": "Computing a classic index for finite-horizon bandits", "author": ["Jos\u00e9 Ni\u00f1o-Mora"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "Ni\u00f1o.Mora.,? \\Q2011\\E", "shortCiteRegEx": "Ni\u00f1o.Mora.", "year": 2011}, {"title": "Multi-armed bandit problems with dependent arms", "author": ["Sandeep Pandey", "Deepayan Chakrabarti", "Deepak Agarwal"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Pandey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pandey et al\\.", "year": 2007}, {"title": "The knowledge gradient for optimal learning", "author": ["Warren B. Powell"], "venue": "Wiley Encyclopedia of Operations Research and Management Science,", "citeRegEx": "Powell.,? \\Q2010\\E", "shortCiteRegEx": "Powell.", "year": 2010}, {"title": "On user behaviour adaptation under interface change", "author": ["Benjamin Rosman", "Subramanian Ramamoorthy", "M.M. Hassan Mahmud", "Pushmeet Kohli"], "venue": "In International Conference on Intelligent User Interfaces,", "citeRegEx": "Rosman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rosman et al\\.", "year": 2014}, {"title": "Contextual bandits with similarity information", "author": ["Aleksandrs Slivkins"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Slivkins.,? \\Q2014\\E", "shortCiteRegEx": "Slivkins.", "year": 2014}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Niranjan Srinivas", "Andreas Krause", "Sham M. Kakade", "Matthias Seeger"], "venue": "arXiv preprint arXiv:0912.3995,", "citeRegEx": "Srinivas et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2009}, {"title": "Experience-efficient learning in associative bandit problems", "author": ["Alexander L Strehl", "Chris Mesterharm", "Michael L Littman", "Haym Hirsh"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Taylor and Stone.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone.", "year": 2009}, {"title": "Multi-task reinforcement learning: a hierarchical bayesian approach", "author": ["A. Wilson", "A. Fern", "S. Ray", "P. Tadepalli"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "Bayesian policy search with policy priors", "author": ["David Wingate", "Noah D. Goodman", "Daniel M. Roy", "Leslie P. Kaelbling", "Joshua B. Tenenbaum"], "venue": "In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Two,", "citeRegEx": "Wingate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wingate et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "For example, online personalisation (Mahmud et al., 2014) is becoming a core concept in human-computer interaction (HCI), driven largely by a proliferation of new sensors and input devices which allow for a more natural means of communicating with hardware.", "startOffset": 36, "endOffset": 57}, {"referenceID": 20, "context": "On the other hand, taking too long to calibrate is likely to frustrate the user (Rosman et al., 2014), who may then abandon the interaction.", "startOffset": 80, "endOffset": 101}, {"referenceID": 0, "context": "1 Poaching of large mammals such as rhinoceroses is a major problem throughout Africa and Asia (Amin et al., 2006).", "startOffset": 95, "endOffset": 114}, {"referenceID": 24, "context": "length is unknown), it is plausible to consider seeding the process with a set of policies of previously solved, related task instances, in what can be seen as a strategy for transfer learning (Taylor and Stone, 2009).", "startOffset": 193, "endOffset": 217}, {"referenceID": 17, "context": "Solving this problem in general is difficult as it maps into the intractable finite-horizon online bandit problem (Ni\u00f1o-Mora, 2011).", "startOffset": 114, "endOffset": 131}, {"referenceID": 23, "context": "contexts in contextual bandits (Strehl et al., 2006; Langford and Zhang, 2008)) or analytical forms of reward (e.", "startOffset": 31, "endOffset": 78}, {"referenceID": 11, "context": "contexts in contextual bandits (Strehl et al., 2006; Langford and Zhang, 2008)) or analytical forms of reward (e.", "startOffset": 31, "endOffset": 78}, {"referenceID": 18, "context": "correlated bandits (Pandey et al., 2007)) to share the credit of pulling an arm between many possible arms.", "startOffset": 19, "endOffset": 40}, {"referenceID": 2, "context": "Because we are dealing with tasks which we assume are of limited duration and which do not allow extensive experimenting, and in order to use information from previous trials to maintain belief distributions over the task space, we draw inspiration from the Bayesian optimisation/efficient global optimisation literature (Brochu et al., 2010) for an approach to this problem that is efficient in the number of policy executions, corresponding to function evaluations in the classical optimisation setting.", "startOffset": 321, "endOffset": 342}, {"referenceID": 12, "context": "A version of the policy reuse problem was described by Mahmud et al. (2013), where it is used to test a set of landmark policies retrieved through clustering in the space of MDPs.", "startOffset": 55, "endOffset": 76}, {"referenceID": 7, "context": "Additionally, the term \u2018policy reuse\u2019 has been used by Fern\u00e1ndez and Veloso (2006) in a different context.", "startOffset": 55, "endOffset": 83}, {"referenceID": 4, "context": "da Silva et al. (2012)).", "startOffset": 3, "endOffset": 23}, {"referenceID": 25, "context": "In the context of MDPs, previous work has regarded classes of MDPs as probability distributions over task parameters (Wilson et al., 2007).", "startOffset": 117, "endOffset": 138}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013).", "startOffset": 86, "endOffset": 107}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013). Similar intuitions have been developed in the multi-armed bandits literature, by examining ways of clustering bandit machines in order to allow for faster convergence and better credit assignment, e.g. Pandey et al. (2007); Bui et al.", "startOffset": 87, "endOffset": 332}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013). Similar intuitions have been developed in the multi-armed bandits literature, by examining ways of clustering bandit machines in order to allow for faster convergence and better credit assignment, e.g. Pandey et al. (2007); Bui et al. (2012); Maillard and Mannor (2014).", "startOffset": 87, "endOffset": 351}, {"referenceID": 13, "context": "A more recent work explored explicitly discovering the clustering in a space of tasks (Mahmud et al., 2013). Similar intuitions have been developed in the multi-armed bandits literature, by examining ways of clustering bandit machines in order to allow for faster convergence and better credit assignment, e.g. Pandey et al. (2007); Bui et al. (2012); Maillard and Mannor (2014). In this work we do not explicitly investigate methods of task clustering, but the algorithms presented herein are most efficient when such a cluster-based structure exists in the task space.", "startOffset": 87, "endOffset": 379}, {"referenceID": 21, "context": "This definition of types is similar to the concept of similarity-based contextual bandits (Slivkins, 2014), where a distance function can be defined in the joint space of contexts and arms given by an upper bound of reward differences.", "startOffset": 90, "endOffset": 106}, {"referenceID": 9, "context": "Multiple proposals have been widely considered in the multi-armed bandits (MAB) literature for these heuristics, ranging from early examples like the Gittins index for infinite horizon problems (Gittins and Jones, 1974) to more recent methods such as the knowledge gradient (Powell, 2010).", "startOffset": 194, "endOffset": 219}, {"referenceID": 19, "context": "Multiple proposals have been widely considered in the multi-armed bandits (MAB) literature for these heuristics, ranging from early examples like the Gittins index for infinite horizon problems (Gittins and Jones, 1974) to more recent methods such as the knowledge gradient (Powell, 2010).", "startOffset": 274, "endOffset": 288}, {"referenceID": 9, "context": "For this kind of setting Lai and Robbins (1985) show that index-based methods achieve optimal performance asymptotically.", "startOffset": 25, "endOffset": 48}, {"referenceID": 2, "context": ", Brochu et al. (2010)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 19, "context": "The final exploration heuristic we describe is the knowledge gradient (Powell, 2010), which aims to balance exploration and exploitation through optimising myopic return whilst maintaining asymptotic optimality.", "startOffset": 70, "endOffset": 84}, {"referenceID": 1, "context": "We choose two frameworks, multi-armed bandits for which we use UCB1 (Auer et al., 2002), and Bayesian optimisation where we use GP-UCB (Srinivas et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 22, "context": ", 2002), and Bayesian optimisation where we use GP-UCB (Srinivas et al., 2009).", "startOffset": 55, "endOffset": 78}, {"referenceID": 23, "context": "The optimal selection from a set of provided policies for a new task is in essence a transfer learning problem (see the detailed review by Taylor and Stone (2009)).", "startOffset": 139, "endOffset": 163}, {"referenceID": 12, "context": "One transfer approach that considers the similarity between source and target tasks is by Lazaric (2008), where generated (s, a, r, s\u2032) samples from the target task are", "startOffset": 90, "endOffset": 105}, {"referenceID": 3, "context": "More recently, Brunskill and Li (2013) consider using the (s, a, r, s\u2032) similarity to compute confidence intervals of where, in a collection of MDP classes, a new instance best fits.", "startOffset": 15, "endOffset": 39}, {"referenceID": 8, "context": "For example, in Response Surface Bandits (Ginebra and Clayton, 1995), there is a known prior over the parameters of the reward curve and the metric on the policy space is known.", "startOffset": 41, "endOffset": 68}, {"referenceID": 8, "context": "For example, in Response Surface Bandits (Ginebra and Clayton, 1995), there is a known prior over the parameters of the reward curve and the metric on the policy space is known. More recently, Mersereau et al. (2009) present a greedy policy which takes advantage of the correlation between the arms in their reward functions, assuming a linear form with one parameter, with a known prior.", "startOffset": 42, "endOffset": 217}, {"referenceID": 18, "context": "In another thread, Dependent Bandits (Pandey et al., 2007) assume that the arms in a multi-armed bandit can be clustered into different groups, such that the members of each have correlated reward distribution parameters.", "startOffset": 37, "endOffset": 58}, {"referenceID": 1, "context": "Then, each cluster is represented with one representative arm, and the algorithm proceeds in two steps: a cluster is first chosen by a variant of UCB1 (Auer et al., 2002) applied to the set of representative arms, and then the same method is used again to choose between the arms of the chosen cluster.", "startOffset": 151, "endOffset": 170}, {"referenceID": 1, "context": "Then, each cluster is represented with one representative arm, and the algorithm proceeds in two steps: a cluster is first chosen by a variant of UCB1 (Auer et al., 2002) applied to the set of representative arms, and then the same method is used again to choose between the arms of the chosen cluster. We assume in our work that the set of previously-solved tasks span and represent the space well, but we do not dwell on how this set of tasks can be selected. Clustering is one good candidate for that, and one particular example of identifying the important types in a task space can be seen in the work of Mahmud et al. (2013).", "startOffset": 152, "endOffset": 631}, {"referenceID": 23, "context": "In Contextual Bandits (Strehl et al., 2006; Langford and Zhang, 2008), the agent is able to observe side information (or context labels) that are related to the nature of the bandit machine, and the question becomes one of selecting the best arm for each possible context.", "startOffset": 22, "endOffset": 69}, {"referenceID": 11, "context": "In Contextual Bandits (Strehl et al., 2006; Langford and Zhang, 2008), the agent is able to observe side information (or context labels) that are related to the nature of the bandit machine, and the question becomes one of selecting the best arm for each possible context.", "startOffset": 22, "endOffset": 69}, {"referenceID": 15, "context": "Another related treatment is that of latent bandits (Maillard and Mannor, 2014) where, in the single-cluster arrival case, the experienced bandit machine is drawn from a single cluster with known reward distributions, and in the agnostic case the instances are drawn from many unknown clusters with unknown reward", "startOffset": 52, "endOffset": 79}, {"referenceID": 5, "context": "Bayesian Reinforcement Learning (BRL) is a paradigm of Reinforcement Learning that handles the uncertainty in an unknown MDP in a Bayesian manner by maintaining a probability distribution over the space of possible MDPs, and updating that distribution using the observations generated from the MDP as the interaction continues (Dearden et al., 1999).", "startOffset": 327, "endOffset": 349}, {"referenceID": 5, "context": "Bayesian Reinforcement Learning (BRL) is a paradigm of Reinforcement Learning that handles the uncertainty in an unknown MDP in a Bayesian manner by maintaining a probability distribution over the space of possible MDPs, and updating that distribution using the observations generated from the MDP as the interaction continues (Dearden et al., 1999). In work by Wilson et al. (2007), the problem of Multi-task Reinforcement Learning of a possibly-infinite stream of MDPs is handled in a Bayesian framework.", "startOffset": 328, "endOffset": 383}, {"referenceID": 26, "context": "When no gradient information is available to guide the search, Wingate et al. (2011) propose to use MCMC to search in the space of policies which is endowed with a prior.", "startOffset": 63, "endOffset": 85}], "year": 2015, "abstractText": "A long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain. Acting online requires \u2018fast\u2019 responses, in terms of rapid convergence, especially when the task instance has a short duration such as in applications involving interactions with humans. These requirements can be problematic for many established methods for learning to act. In domains where the agent knows that the task instance is drawn from a family of related tasks, albeit without access to the label of any given instance, it can choose to act through a process of policy reuse from a library in contrast to policy learning. In policy reuse, the agent has prior experience from the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase. We formalise the problem of policy reuse and present an algorithm for efficiently responding to a novel task instance by reusing a policy from this library of existing policies, where the choice is based on observed \u2018signals\u2019 which correlate to policy performance. We achieve this by posing the problem as a Bayesian choice problem with a corresponding notion of an optimal response, but the computation of that response is in many cases intractable. Therefore, to reduce the computation cost of the posterior, we follow a Bayesian optimisation approach and define a set of policy selection functions, which balance exploration in the policy library against exploitation of previously tried policies, together with a model of expected performance of the policy library on their corresponding task instances. We validate our method in several simulated domains of interactive, short-duration episodic tasks, showing rapid convergence in unknown task variations. ? The first two authors contributed equally to this paper. Benjamin Rosman Mobile Intelligent Autonomous Systems (MIAS), Council for Scientific and Industrial Research (CSIR), South Africa, and the School of Computer Science and Applied Mathematics, University of the Witwatersrand, South Africa. E-mail: BRosman@csir.co.za. Majd Hawasly School of Informatics, University of Edinburgh, UK. E-mail: M.Hawasly@ed.ac.uk. Subramanian Ramamoorthy School of Informatics, University of Edinburgh, UK. E-mail: S.Ramamoorthy@ed.ac.uk. ar X iv :1 50 5. 00 28 4v 2 [ cs .A I] 1 4 D ec 2 01 5 2 Rosman, Hawasly & Ramamoorthy", "creator": "LaTeX with hyperref package"}}}