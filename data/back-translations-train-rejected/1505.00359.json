{"id": "1505.00359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2015", "title": "Can deep learning help you find the perfect match?", "abstract": "Is he/she attractive or not? We can often answer this question in a split of a second, and this ability is one of the main reasons behind the success of recent dating apps. In this paper we explore if we can predict attractiveness from profile pictures with convolutional networks. We argue that the introduced task is difficult due to i) the large number of variations in profile pictures and ii) the noise in attractiveness labels. We find that our self-labeled dataset of $9364$ pictures is too small to apply a convolutional network directly. We resort to transfer learning and compare feature representations transferred from VGGNet and a self-trained gender prediction network. Our findings show that VGGNet features transfer better and we conclude that our best model, achieving $68.1\\%$ accuracy on the test set, is moderately successful at predicting attractiveness.", "histories": [["v1", "Sat, 2 May 2015 17:20:23 GMT  (358kb,D)", "http://arxiv.org/abs/1505.00359v1", null], ["v2", "Sat, 20 Jun 2015 15:41:45 GMT  (123kb,D)", "http://arxiv.org/abs/1505.00359v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harm de vries", "jason yosinski"], "accepted": false, "id": "1505.00359"}, "pdf": {"name": "1505.00359.pdf", "metadata": {"source": "CRF", "title": "Can deep learning help you find the perfect match?", "authors": ["Harm de Vries", "Jason Yosinski"], "emails": ["mail@harmdevries.com"], "sections": [{"heading": "1 Introduction", "text": "The goal of dating systems is to help you in this process by presenting the most promising profiles. The traditional way to recommend profiles is to calculate match scores based on social and physical attributes, such as body type and education level. The most popular dating app to date, Tinder1, employs an alternative matching strategy. Profiles pictures2 of geographically close users are presented one by one, and a user can quickly decide to like or dislike the profile by scrolling to the right or left. Both users like each other and have the ability to chat with each other, possibly to arrange an offline database. The success of these apps points to the importance of visual appearance in finding the ideal partner, and highlights based on matching matching matching matching algorithms are missing."}, {"heading": "2 The task and the data", "text": "I have made myself the subject of investigation. Although a person's results can never be statistically significant, we see them as a first step in investigating the feasibility of modern computer vision techniques to grasp a subtle concept such as attractiveness. Note that predicting beauty is a related but different concept: you can be beautiful but still not the type to which you are attracted."}, {"heading": "2.1 Attractiveness dataset", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2.2 Gender dataset", "text": "As we described in Section 3.1, we found that the data set collected is too small for a revolutionary network to work well directly. Therefore, we collected an additional 418,452 profile images from the dating site OKCupid labeled Gender and Age. To make training this neural network straightforward, we designed this data set to have an equal number of male and female profile images. We discard age information below because we found that the signal was too quiet. Our strategy is to train a gender prediction convention and then transfer the learned feature representations to prediction of attractiveness. The data set was collected from a real dating site that raises questions about the quality of the labels provided. For example, some images may be labeled incorrectly, or even impossible for people to distinguish. It was too time consuming to clean up the full records so that we appreciate the quality of the labels as follows."}, {"heading": "3 Experiments", "text": "In Section 3.1, we first train a convention to predict attractiveness from the small marked dataset. Section 3.2 presents the details of the formation of a convention to predict gender. Then, in Section 3.3, we examine how well the characteristics of this network can be transferred to the attractiveness forecast. We compare the characteristics of VGGNet, one of the most powerful conventions in the ImageNet."}, {"heading": "3.1 Attractiveness prediction", "text": "Our architecture is inspired by VGGNet [14] and follows the latest trends in architectural design towards very deep networks and small filter sizes. We use five revolutionary layers, all with 3x3 filter sizes and rectified linear activation functions. Each layer is followed by non-overlapping maximum pooling of size 2x2. We start with 8 function boards in the first layer and gradually increase them to 32 in the last revolutionary layer. There are two fully connected layers at 32 and 16 each. The network has in the order of the 870K parameters. The details of the architecture are shown in Table 2 (a).The only pre-processing step that is applied is to subtract the training set means from all images. We regulate the network by applying dropout [8] with the 0.5 probability to the fully connected layers."}, {"heading": "3.2 Gender prediction", "text": "The gender data set with over 400K images is much larger than the attractiveness data set. Therefore, we can afford to train a much larger network without the risk of overadjustment. The proposed Convnet architecture is similar in spirit to the attractiveness network presented in the previous paragraph. We opt for nine revolutionary layers with filter sizes of 3x3 and rectified linear activation functions. We continue to apply 2x2 max pooling after two Constitutional layers, with the exception of the first layer in which we apply pooling directly after one shift.We follow the rule of thumb introduced in [14] and double the number of function cards after each pooling layer, with the exception of the last pooling layer in which we maintain the number of function cards. The distortions (as opposed to the weights) in the Convolutionary layers are unbound, i.e. each location in a functional map has its own biological capacity."}, {"heading": "3.3 Transfer learning", "text": "We compare two transfer learning strategies: one from the gender network and the other from VGGNet, one of the most powerful ImageNet networks."}, {"heading": "3.3.1 Gender", "text": "The gender network has about 28 million parameters, and the attractiveness dataset available is relatively small, so training the entire network is likely to result in overadjustment. We therefore choose to train only the last layers of the gender network. We compare training to the last, last two and last three layers, each with 1026, 525K and 8.9M parameters. We do not use dropouts in training these last layers, but we use the same L2 regulation as in the gender network. We train with SGD for 50 epochs with a learning rate of 0.001 and 0.9 instantaneouss.The training and validation curves are shown in Figure 3 (a-c).Note that the transfer performance is quite poor. Only training the last shift hardly reduces the training error and is significantly subject to it. On the other hand, training all fully connected layers reduces the training error very quickly, but does not result in the error of only two early validation results."}, {"heading": "3.3.2 ImageNet", "text": "We choose VGGNet [14], one of the most powerful ImageNet convection networks, and use Caffe [10] to extract the properties. To feed the images into VGGNet, we resize all images to 224x224. We extract 4096 dimensional characteristics from the top layer (FC7) of the 19-layer VGGNet. We place a logistical regression with weight loss on the extracted representation. After fine-tuning the hyperparameters, we obtained the best results with an L2 regulation coefficient of 0.8, a learning rate of 0.0001 and a dynamic of 0.9. Note that a relatively large weight drop is still necessary to prevent overadjustment."}, {"heading": "4 Discussion and Conclusion", "text": "Our results confirm that ImageNet activations are excellent image characteristics for a variety of tasks. However, we did not expect that they would exceed the characteristics of the gender prediction task, as this network was trained on a similar set of images. A possible explanation for the poor transfer is that the gender network learns traits that are invariant to female traits and are therefore not suitable for distinguishing between profile images. Another reason could be that the gender network has only two classes, which does not force the network to learn highly differentiated traits. A possible direction for future research is to investigate whether adding an additional class of non-profile images leads to better transferable traits. Further studies could also explore other ways to deal with the enormous variability in profile images. For example, facial extraction could be a good way to reduce variability while preserving the most important aspect of attractiveness."}, {"heading": "5 Acknowledgement", "text": "We thank Mehdi Mirza for extracting the VGGNet features and the developers of Theano [2] and Blocks, as well as the computing resources of Compute Canada and Calcul Que \ufffd bec. We thank all members of the LISA laboratory for helpful discussions, especially Yoshua Bengio, Aaron Courville, Roland Memisevic, Kyung Hyun Cho, Yann Dauphin, Laurent Dinh, Kyle Kastner, Junyoung Chung, Julian Serban, Alexandre de Bre \ufffd bison, Ce \ufffd sar Laurent and Christopher Olah."}], "references": [{"title": "Ccr-a contentcollaborative reciprocal recommender for online dating", "author": ["J. Akehurst", "I. Koprinska", "K. Yacef", "L.A.S. Pizzato", "J. Kay", "T. Rej"], "venue": "IJCAI, pages 2199\u20132204,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., 2(1):1\u2013127, Jan.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Recommender system for online dating service", "author": ["L. Brozovsky", "V. Petricek"], "venue": "CoRR, abs/cs/0703042,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. jia Li", "K. Li", "L. Fei-fei"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "CoRR, abs/1310.1531,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1502.01852,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, abs/1502.03167,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 675\u2013678. ACM,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Collaborative filtering for people-to-people recommendation in online dating: Data analysis and user trial", "author": ["A. Krzywicki", "W. Wobcke", "Y. Kim", "X. Cai", "M. Bain", "A. Mahidadia", "P. Compton"], "venue": "International Journal of Human-Computer Studies, 76(0):50 \u2013 66,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CoRR, abs/1409.4842,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "We resort to transfer learning and compare feature representations transferred from VGGNet[14] and a self-trained gender prediction network.", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "Recently proposed matching algorithms [12, 4, 1] sidestep this problem by using collaborative filtering.", "startOffset": 38, "endOffset": 48}, {"referenceID": 3, "context": "Recently proposed matching algorithms [12, 4, 1] sidestep this problem by using collaborative filtering.", "startOffset": 38, "endOffset": 48}, {"referenceID": 0, "context": "Recently proposed matching algorithms [12, 4, 1] sidestep this problem by using collaborative filtering.", "startOffset": 38, "endOffset": 48}, {"referenceID": 10, "context": "The 2012 winning entry [11] of the ImageNet competition[5] has rapidly changed the field of computer vision.", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "The 2012 winning entry [11] of the ImageNet competition[5] has rapidly changed the field of computer vision.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "The key ingredient of the success of convnets and other deep learning models [3] is that they learn multiple layers of representations as opposed to hand-crafted or shallow features.", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].", "startOffset": 41, "endOffset": 53}, {"referenceID": 12, "context": "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].", "startOffset": 41, "endOffset": 53}, {"referenceID": 14, "context": "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].", "startOffset": 41, "endOffset": 53}, {"referenceID": 6, "context": "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].", "startOffset": 165, "endOffset": 171}, {"referenceID": 8, "context": "Since 2012, several deep learning groups [14, 13, 15] improved upon the original convnet architecture with the latest results achieving near-human level performance [7, 9].", "startOffset": 165, "endOffset": 171}, {"referenceID": 5, "context": "Several studies[6, 14] have demonstrated that high layer activations from top-performing ImageNet networks serve as excellent features for recognition tasks for which the network was not trained.", "startOffset": 15, "endOffset": 22}, {"referenceID": 13, "context": "Several studies[6, 14] have demonstrated that high layer activations from top-performing ImageNet networks serve as excellent features for recognition tasks for which the network was not trained.", "startOffset": 15, "endOffset": 22}, {"referenceID": 13, "context": "Our architecture is inspired by VGGNet [14], and follows the latest trends in architecture design to have very deep networks and small filter sizes.", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "We regularize the network by applying dropout [8] with probability 0.", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": "We follow the rule of thumb introduced in [14] and double the number of feature maps after each pooling layer, except for the last pooling layer where we kept the number of feature maps the same.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "The models were implemented in Theano[2] and took about 3 days to train on a GeForce GTX Titan Black.", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "The features extracted from ImageNet networks are known to achieve excellent transfer performance [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 13, "context": "We decide to use VGGNet [14], one of the top performing ImageNet convnets, and use Caffe[10] to extract the features.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "We decide to use VGGNet [14], one of the top performing ImageNet convnets, and use Caffe[10] to extract the features.", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "We also thank the developers of Theano[2] and Blocks and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec.", "startOffset": 38, "endOffset": 41}], "year": 2017, "abstractText": "Is he/she attractive or not? We can often answer this question in a split of a second, and this ability is one of the main reasons behind the success of recent dating apps. In this paper we explore if we can predict attractiveness from profile pictures with convolutional networks. We argue that the introduced task is difficult due to i) the large number of variations in profile pictures and ii) the noise in attractiveness labels. We find that our self-labeled dataset of 9364 pictures is too small to apply a convolutional network directly. We resort to transfer learning and compare feature representations transferred from VGGNet[14] and a self-trained gender prediction network. Our findings show that VGGNet features transfer better and we conclude that our best model, achieving 68.1% accuracy on the test set, is moderately successful at predicting attractiveness.", "creator": "LaTeX with hyperref package"}}}