{"id": "1505.00384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2015", "title": "Making Sense of Hidden Layer Information in Deep Networks by Learning Hierarchical Targets", "abstract": "This paper proposes an architecture for deep neural networks with hidden layer branches that learn targets of lower hierarchy than final layer targets. The branches provide a channel for enforcing useful information in hidden layer which helps in attaining better accuracy, both for the final layer and hidden layers. The shared layers modify their weights using the gradients of all cost functions higher than the branching layer. This model provides a flexible inference system with many levels of targets which is modular and can be used efficiently in situations requiring different levels of results according to complexity. This paper applies the idea to a text classification task on 20 Newsgroups data set with two level of hierarchical targets and a comparison is made with training without the use of hidden layer branches.", "histories": [["v1", "Sun, 3 May 2015 00:58:38 GMT  (369kb,D)", "https://arxiv.org/abs/1505.00384v1", "11 pages, 6 figures"], ["v2", "Sat, 24 Sep 2016 07:31:47 GMT  (370kb,D)", "http://arxiv.org/abs/1505.00384v2", "Updated to add a note with commentary on original (v1) submission"]], "COMMENTS": "11 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["abhinav tushar"], "accepted": false, "id": "1505.00384"}, "pdf": {"name": "1505.00384.pdf", "metadata": {"source": "CRF", "title": "Making Sense of Hidden Layer Information in Deep Networks by Learning Hierarchical Targets", "authors": ["Abhinav Tushar"], "emails": ["abhinav.tushar.vs@gmail.com"], "sections": [{"heading": null, "text": "Author's Note September 2016 This document was essentially (May 2015) a hasty drafting of a project for a course on artificial neural networks during my bachelor's degree. I add this note here to point out errors that have a detrimental effect on the writings. I have retained the original content and added only this box. First, the document does not really use the terms (information, deep networks) of the title in the analysis well. Speaking of literature itself, there is a similar concept of the auxiliary classifier in literature that uses [the same] goals at lower levels to improve performance (see arXiv: 1409.4842v1 [cs.CV] for example). -1 on literature review. Furthermore, the comparison is not rigorous enough to substantiate the claims and needs a more meaningful test.ar Xiv: 150 5.00 384v 2 [cs.N E] 2"}, {"heading": "1 Introduction", "text": "They are effective at learning features from raw data that would have taken a lot of effort to process in the case of flat networks, for example a recent paper (Zhang and LeCun 2015) that shows deep temporal revolutionary networks to learn abstract text concepts from character levels. However, deep networks are not easy to learn. Few of the problems are mired in local optimism, the problem of disappearing gradients, etc. If the hyperparameters of networks are not developed properly, deep networks tend to overlap. Choosing activation functions (Glorot and Bengio 2010) as well as the correct initialization of weights (Sutskever et al. 2013) will play an important role in the performance of deep networks."}, {"heading": "2 Proposed Network Architecture", "text": "A Simple Structure Using one1The dataset can be downloaded here qwone.com / ~ jason / 20Newsgroups / branching is in Figure 1. The targets are arranged hierarchically, with the most detailed targets farthest from the input level, while trivial targets are closer to the input level. The network learns both the final layer outputs and the hidden layer outputs. In the following subsection, the learning algorithm is explained using the example network in Figure 1."}, {"heading": "2.1 Learning Algorithm", "text": "For the network shown in Figure 1, there is a branch from the level whose output is xB. Weights and distortions from WB + 1, bB + 1 to WN + 2, bN + 2 are updated only with the cost function of the final target layer, while WH and bH are updated only with the cost function of the hidden layer. For the weights shared for both targets, i.e. weights and distortions from W 1, b1 to WB, bB, the training uses both the cost function and an average target cost function, depending on which weights are to be minimized. For the weights used for both targets, i.e. weights and distortions from W 1, b1 to WB, bB, both the cost function and an average update for these parameters are performed. If the final target costs are CF and the hidden target costs are CH, then the updates from W 1, b1 to WB, bB, are performed."}, {"heading": "2.2 Features of the network", "text": "\u2022 Performance representation of meaningful data in hidden layers controlled by hidden layer branches helps by providing features for higher layers, improving overall performance of the network. \u2022 Hierarchical Targets Different target branches, ordered hierarchically by detail, help with problems requiring scalability in the level of detail of the targets. \u2022 Modularity Hidden layer targets result in meaningful content being stored in hidden layers and thus the network can be separated (recombined) from (with) the branch joints without losing the knowledge learned."}, {"heading": "3 Experimental Results", "text": "Hidden layer taps can only be exploited if the problem has multiple and hierarchical goals. It can also work if it is possible to reduce the resolution (or other parameters related to details) of the output to generate hidden layer outputs. This section examines the performance of the proposed model on 20 newsgroups records."}, {"heading": "3.1 Data set", "text": "The dataset contains newsgroup posts from 20 newsgroups, resulting in a 20-class classification problem. According to the topics of the newsgroup, the 20 classes were divided into 5 primitive classes (details are given in Table 1); the last level of the network is designed to learn the 20 class goals, while the hidden level is used to learn the rougher, 5-class goals; the dataset includes 18846 instances, of which 14314 were selected for training, while the other 4532 instances were retained for testing."}, {"heading": "3.2 Word2Vec preprocessing", "text": "In this case, a vocabulary is built from the corpus, and each paragraph (or case) is represented by a histogram of the frequency of occurrence of words from the vocabulary. Although this representation is intuitive and simple, it has a major disadvantage when working with neural networks. Vocabulary length is usually very large, in the order of tens of thousands, while each part of text to be considered has few of the possible words, resulting in a very sparse representation. Such sparse input representation can lead to poor learning and high inefficiency in neural networks. A new tool, Word2Vec 2, is used to represent words as a dense vector.Word2Vec is a tool for calculating continuous distributed representations of vocators (v 201v)."}, {"heading": "3.3 Network Architecture", "text": "The network used had 4 hidden layers. The number of neurons in the layers was: 1000 (input) \u21d2 300 \u21d2 200 \u21d2 200 \u21d2 130 \u21d2 20 (target) \u21d2 5 (hiddentarget) From hidden layer 1 (with 300 neurons) a branch was created to learn hidden target. Weights and distortions are: WN, bN for connections from layer N \u2212 1 to layer N. WH, bH for connections from hidden layer to hidden target. Equal linear units (ReLUs) were selected as activation functions of neurons because they have a lower probability of gradients disappearing (Nair and Hinton 2010). The activation function of ReLU is by: f (x) = max (x, 0) (5) The output layers (both final and hidden branches) used Softmax logistic regression, while the cost function was multinomial loss. For hidden gas costs, the hidden weight was added by 1."}, {"heading": "3.4 Performance", "text": "Three training experiments were performed as follows: 1. With simultaneous updates for the common layers (100 epochs) + fine tuning (20 epochs) 2. Without simultaneous updates for the common layers by ignoring gradients coming from hidden shift goals (100 epochs) + fine tuning (20 epochs)."}, {"heading": "3. Training only using the hidden layer target (100 epochs) + fine tuning (20 epochs)", "text": "All three training experiments were performed with the same set of hyperparameters and were repeated 20 times to take into account the random variations.Mean training losses over the entire training course were plotted using all 20 repetitions.The representation of training losses for the final layer goal in Experiments 1 and 2 is shown in Figure 2. The diagram shows that simultaneous training appears to be better than direct training in which only the target cost function is minimized.The representation of training losses for the final layer goal in all three experiments is shown in Figure 3. Here, training with only the minimization of final cost is not able to generate a sufficient effective representation of data to help minimize the hidden cost function.The situation is clearer in Figure 4, which only represents a better representation of the data than hidden losses in Figure 6."}, {"heading": "4 Conclusion", "text": "This paper presented a branched architecture for neural networks that, when applied to a suitable problem with multiple output levels, inherently results in the hidden layers storing meaningful representations and improving performance. Training curves showed that the distributed layers learned a representation during simultaneous training that minimized both cost functions and provided better weights for hidden objectives. Industries help enforce information in hidden layers and thus the auxiliary branches can be easily added or removed from the network, providing flexibility in terms of network modularity and scalabilty. Hidden Target AccuracyTrain (%) Test (%) Hidden Training 85,320316 (1,523254) 82,822154 (0,417403) Final Training 70,205044 (4,088195) 77,763680 (1,602464) Simultaneous Training 84,051977 (1,181,86351,73,74464,736) (3,74454,473)"}, {"heading": "5 Future Work", "text": "This key concept of the proposed architecture is to exploit the hidden layers through meaningful representations. By means of a hierarchy of the target, the proposed architecture can form meaningful hidden representations. An extensive experiment can be conducted with many branches. Convolutionary networks working on computer vision problems are ideal candidates for these tests, as it is easy to visualize the weights to find connections with the desired representations. In addition, vision problems can be broken up in many levels of detail, generating a hierarchy of results from a single output level.While this essay focuses on a problem with branches from the hidden layers, an investigation can be conducted in which a few hidden neurons directly represent the hidden targets without branching out.Further, work can be useful for building multiple layers of output from a single output.This can be useful for computer vision problems where different output layers can be practical."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Pascal Lamblin"], "venue": "Advances in", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning ?", "author": ["Erhan", "Dumitru", "Aaron Courville", "Pascal Vincent"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan", "Dumitru"], "venue": "In: International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Erhan and Dumitru,? \\Q2009\\E", "shortCiteRegEx": "Erhan and Dumitru", "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Yoshua Bengio"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets.", "author": ["Hinton", "Geoffrey E"], "venue": "Neural computation", "citeRegEx": "Hinton and E,? \\Q2006\\E", "shortCiteRegEx": "Hinton and E", "year": 2006}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of NAACLHLT June,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas"], "venue": "In: NIPS,", "citeRegEx": "Mikolov and Tomas,? \\Q2013\\E", "shortCiteRegEx": "Mikolov and Tomas", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Mikolov", "Tomas"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR", "citeRegEx": "Mikolov and Tomas,? \\Q2013\\E", "shortCiteRegEx": "Mikolov and Tomas", "year": 2013}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Nair", "Vinod", "Geoffrey E Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Efficient Learning of Sparse Representations with an Energy-Based Model", "author": ["Ranzato", "Marc Aurelio"], "venue": "Advances In Neural Information Processing Systems", "citeRegEx": "Ranzato and Aurelio,? \\Q2007\\E", "shortCiteRegEx": "Ranzato and Aurelio", "year": 2007}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya"], "venue": "JMLR W&CP", "citeRegEx": "Sutskever and Ilya,? \\Q2013\\E", "shortCiteRegEx": "Sutskever and Ilya", "year": 2013}, {"title": "Optimizing word2vec in gensim", "author": ["\u0158eh\u030au\u0159ek", "Radim"], "venue": "url: http://radimrehurek", "citeRegEx": "\u0158eh\u030au\u0159ek and Radim,? \\Q2013\\E", "shortCiteRegEx": "\u0158eh\u030au\u0159ek and Radim", "year": 2013}, {"title": "Text Understanding from Scratch", "author": ["Zhang", "Xiang", "Yann LeCun (Feb"], "venue": "In: eprint: 1502.01710", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "This paper proposes an architecture for deep neural networks with hidden layer branches that learn targets of lower hierarchy than final layer targets. The branches provide a channel for enforcing useful information in hidden layer which helps in attaining better accuracy, both for the final layer and hidden layers. The shared layers modify their weights using the gradients of all cost functions higher than the branching layer. This model provides a flexible inference system with many levels of targets which is modular and can be used efficiently in situations requiring different levels of results according to complexity. This paper applies the idea to a text classification task on 20 Newsgroups data set with two level of hierarchical targets and a comparison is made with training without the use of hidden layer branches. Author\u2019s Note September, 2016 This document essentially was (May 2015) a hasty write up of a project for a course on artificial neural networks during my undergraduate studies. I am adding this note here to point out mistakes which are detrimental to writings. I have kept the original content intact, adding only this box. Firstly, the document doesn\u2019t really use the terms (information, deep networks) from the title well in the analysis. Talking about the idea itself, there is a similar concept of auxiliary classifier in literature which uses [same] targets at lower levels to improve performance (See arXiv:1409.4842v1 [cs.CV] for example). -1 to literature review. Furthermore, the comparison is not rigorous enough to back up the claims and needs more meaningful test. 1 ar X iv :1 50 5. 00 38 4v 2 [ cs .N E ] 2 4 Se p 20 16", "creator": "LaTeX with hyperref package"}}}