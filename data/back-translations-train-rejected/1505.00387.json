{"id": "1505.00387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2015", "title": "Highway Networks", "abstract": "There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on \"information highways\". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.", "histories": [["v1", "Sun, 3 May 2015 01:56:57 GMT  (311kb,D)", "http://arxiv.org/abs/1505.00387v1", "Extended Abstract. 6 pages, 2 figures"], ["v2", "Tue, 3 Nov 2015 18:15:15 GMT  (319kb,D)", "http://arxiv.org/abs/1505.00387v2", "6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop. Full paper is atarXiv:1507.06228"]], "COMMENTS": "Extended Abstract. 6 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["rupesh kumar srivastava", "klaus greff", "j\\\"urgen schmidhuber"], "accepted": false, "id": "1505.00387"}, "pdf": {"name": "1505.00387.pdf", "metadata": {"source": "META", "title": "Highway Networks", "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "emails": ["RUPESH@IDSIA.CH", "KLAUS@IDSIA.CH", "JUERGEN@IDSIA.CH"], "sections": [{"heading": "1. Introduction", "text": "In recent years, the number of those who are able to reform has increased, from 84% (2012) to 95% (2014).In theory, it is known that deep networks can represent certain functionalities that are able to establish themselves in the reformed world (2014).In the reformed world of the reformed world, it is possible that people in the reformed world in the reformed world, in the reformed world, in the reformed world, in the reformed world, in the reformed world, in the reformed world, in the reformed world and in the reformed world, in the reformed world and in the reformed world, in the reformed world and in the reformed world, in the reformed world and in the reformed world."}, {"heading": "1.1. Notation", "text": "We use bold letters for vectors and matrices and italic uppercase letters for transformation functions. 0 and 1 stand for vectors of zeros and ones, respectively, and I for an identity matrix. The function \u03c3 (x) is defined as \u03c3 (x) = 11 + e \u2212 x, x \u0445 R."}, {"heading": "2. Highway Networks", "text": "A simple forward-facing neural network typically consists of L layers in which the lth layer (l \u00b7 {1, 2,..., L}) applies a nonlinear transformation H (parameterized by WH, l) to its input xl to generate its output yl. Therefore, x1 is input to the network and yL is output of the network. Omitting the layer index and distortions for clarity, y = H (x, WH). (1) H is usually an affine transformation followed by a nonlinear activation function, but generally it can take other shapes.For a highway network, we additionally define two nonlinear transformations of T (x, WT) and C (x, WC) such an output of H (x, WH) block (x, WT) + x \u00b7 C (x, WT). We refer to T as the transformed gate and C as the carry gate, as it is expressed by much of the paper and the output of H."}, {"heading": "2.1. Constructing Highway Networks", "text": "As already mentioned, Equation (3) requires that the dimensionality of x, y, H (x, WH) and T (x, WT) must be the same. In cases where it is desirable to change the size of the representation, you can replace x with x, which is obtained by appropriate sub-sampling or zero-padding. Another alternative is to use a plane layer (without highways) to change the dimensionality and then proceed with stacking highway layers. This is the alternative we use in this study. Convolutionary highway layers are constructed similarly to fully connected layers. Weight division and local receptive fields are used for both H and T transformations. We use zero-padding to ensure that the block state and transformation of gate feature maps are the same size as the input."}, {"heading": "2.2. Training Deep Highway Networks", "text": "For simple deep networks, training with SGD stands still at the beginning, unless a specific initialization scheme for weight is used in such a way that the variance of signals during forward and backward propagation is maintained for the time being (Glorot & Bengio, 2010; He et al., 2015). This initialization depends on the exact functional shape of H. For highway layers, we use the transformation gate defined as T (x) = \u03c3 (WTTx + bT), where WT is the weight matrix and bT the bias vector for the transformation gates. This suggests a simple initialization scheme that is independent of the nature of H: bT can be initialized with a negative value (e.g. -1, -3 etc.), so that the network initially tends to carry behavior.This scheme is strongly inspired by the suggestion by Gers et al. (1999) to bridge the gates in a long-term short-memory network over-dependencies in the long-term."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Optimization", "text": "To show that road networks do not suffer from depth in the same way, we conduct a series of experiments on the MNIST digit classification dataset. We measure the entropy error on the training set to examine the optimization without confusing it with generalization problems. We train both road networks and motorway networks with the same architecture and different depth. The first layer is always a regular, fully connected layer followed by 9, 19, 49 or 99 fully connected levels or motorway layers and a single Softmax output layer. The number of units in each layer is kept constant and it is 50 for motorways and 71 for simple networks, so the number of parameters for both is roughly the same. To make the comparison, we perform a random search of 40 runs for both levels and motorway networks to find good settings for the hyperparameters."}, {"heading": "3.2. Comparison to Fitnets", "text": "To answer this question, we compared the road networks with the thin and deep architectures recently proposed by Romero et al. (2014) on the CIFAR-10 dataset, supplemented by random translations. Results are summarized in Table 1.Romero et al. (2014) and show that training using simple backpropogation was only possible for maxout networks with a depth of up to 5 layers if the number of parameters was limited to 250K and the number of multiplications to 30M. Training deeper networks was only possible by using a two-step training process and complementing soft targets produced from a pre-trained flat teacher network (switch-based training). Similarly, it was only possible to train 19-layer networks with a budget of 2.5 million."}, {"heading": "4. Analysis", "text": "The first three columns show the bias for each transformation gate, the mean activity above 10K random samples, and the activity for a single sample. Block outputs for the same single sample are shown in the last column. Transformation location distortions of the two networks were initialized to -2 and -4, respectively. Interestingly, contrary to our expectations, most of the biases actually continued to decrease during the training. For the CIFAR-100 network, the biases increase with the depth formation of a gradient. Strangely, these gradients correlate with the average activity of the transformers as seen in the second column, suggesting that the strong negative biases at low depths are not used to close the gates, but to make them more selective. This behavior is also suggested by the fact that the transformation gates as seen in the second column are strongly transformed."}, {"heading": "5. Conclusion", "text": "Learning to guide information through neural networks has helped to broaden its application to challenging problems by improving lending and facilitating training (Srivastava et al., 2015). Nevertheless, training very deep networks has remained difficult, especially without significantly increasing the overall network size.Motorway networks are novel neural network architectures that enable the formation of extremely deep networks using simple SGDs. While the traditional simple neural architectures are increasingly difficult to train with increasing network depth (even with variance-maintaining initialization), our experiments show that the optimization of motorway networks is not hampered even when network depth increases to one hundred levels. The ability to train extremely deep networks opens up the possibility of studying the effects of depth on complex problems without limitations. Various activation functions that may be more suitable for certain problems, but are not available for the most robust initialization programs, can be used in deep motorway networks."}, {"heading": "Acknowledgments", "text": "This research was supported by the EU project \"NASCENCE\" (FP7-ICT-317662) and we thank NVIDIA Corporation for their support by donating the Tesla K40 GPUs used for this research."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A committee of neural networks for traffic sign classification", "author": ["Ciresan", "Dan", "Meier", "Ueli", "Masci", "Jonathan", "Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks (IJCNN), The 2011 International Joint Conference on,", "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2011}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "In Artificial Neural Networks,", "citeRegEx": "Gers et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gers et al\\.", "year": 1999}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Computational limitations of small-depth circuits", "author": ["H\u00e5stad", "Johan"], "venue": "MIT press,", "citeRegEx": "H\u00e5stad and Johan.,? \\Q1987\\E", "shortCiteRegEx": "H\u00e5stad and Johan.", "year": 1987}, {"title": "On the power of small-depth threshold circuits", "author": ["H\u00e5stad", "Johan", "Goldmann", "Mikael"], "venue": "Computational Complexity,", "citeRegEx": "H\u00e5stad et al\\.,? \\Q1991\\E", "shortCiteRegEx": "H\u00e5stad et al\\.", "year": 1991}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "[cs],", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Technical Report FKI-207-95,", "citeRegEx": "Hochreiter et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1995}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "URL http://jmlr.org/ proceedings/papers/v38/lee15a.html", "author": ["Lee", "Chen-Yu", "Xie", "Saining", "Gallagher", "Patrick", "Zhang", "Zhengyou", "Tu", "Zhuowen"], "venue": "Deeply-supervised nets. pp", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "FitNets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "[cs],", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "URL http://arxiv.org/abs/1312", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "[cs],", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Understanding locally competitive networks", "author": ["Srivastava", "Rupesh Kumar", "Masci", "Jonathan", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "[cs],", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "For instance, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \u223c84% (Krizhevsky et al., 2012) to \u223c95% (Szegedy et al.", "startOffset": 113, "endOffset": 138}, {"referenceID": 14, "context": ", 2012) to \u223c95% (Szegedy et al., 2014; Simonyan & Zisserman, 2014) through the use of ensembles of deeper architectures and smaller receptive fields (Ciresan et al.", "startOffset": 16, "endOffset": 66}, {"referenceID": 0, "context": "As argued by Bengio et al. (2013), the use of deep networks can offer both computational and statistical efficiency for complex tasks.", "startOffset": 13, "endOffset": 34}, {"referenceID": 11, "context": "Optimization of deep networks has proven to be considerably more difficult, leading to research on initialization schemes (Glorot & Bengio, 2010; Saxe et al., 2013; He et al., 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al.", "startOffset": 122, "endOffset": 181}, {"referenceID": 6, "context": "Optimization of deep networks has proven to be considerably more difficult, leading to research on initialization schemes (Glorot & Bengio, 2010; Saxe et al., 2013; He et al., 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al.", "startOffset": 122, "endOffset": 181}, {"referenceID": 10, "context": ", 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al., 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al.", "startOffset": 60, "endOffset": 109}, {"referenceID": 14, "context": ", 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al., 2014; Lee et al., 2015).", "startOffset": 82, "endOffset": 122}, {"referenceID": 9, "context": ", 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al., 2014; Lee et al., 2015).", "startOffset": 82, "endOffset": 122}, {"referenceID": 6, "context": "For up to 100 layers we compare their training behavior to that of traditional networks with normalized initialization (Glorot & Bengio, 2010; He et al., 2015).", "startOffset": 119, "endOffset": 159}, {"referenceID": 6, "context": "For up to 100 layers we compare their training behavior to that of traditional networks with normalized initialization (Glorot & Bengio, 2010; He et al., 2015). We show that optimization of highway networks is virtually independent of depth, while for traditional networks it suffers significantly as the number of layers increases. We also show that architectures comparable to those recently presented by Romero et al. (2014) can be directly trained to obtain similar test ar X iv :1 50 5.", "startOffset": 143, "endOffset": 428}, {"referenceID": 6, "context": "For plain deep networks, training with SGD stalls at the beginning unless a specific weight initialization scheme is used such that the variance of the signals during forward and backward propagation is preserved initially (Glorot & Bengio, 2010; He et al., 2015).", "startOffset": 223, "endOffset": 263}, {"referenceID": 2, "context": "This scheme is strongly inspired by the proposal of Gers et al. (1999) to initially bias the gates in a Long Short-Term Memory recurrent network to help bridge long-term temporal dependencies early in learning.", "startOffset": 52, "endOffset": 71}, {"referenceID": 6, "context": "Very deep plain networks become difficult to optimize even if using the variance-preserving initialization scheme form (He et al., 2015).", "startOffset": 119, "endOffset": 136}, {"referenceID": 6, "context": "All other weights were initialized following the scheme introduced by (He et al., 2015).", "startOffset": 70, "endOffset": 87}, {"referenceID": 10, "context": "Deep highway networks are easy to optimize, but are they also beneficial for supervised learning where we are interested in generalization performance on a test set? To address this question, we compared highway networks to the thin and deep architectures termed Fitnets proposed recently by Romero et al. (2014) on the CIFAR-10 dataset augmented with random translations.", "startOffset": 292, "endOffset": 313}, {"referenceID": 10, "context": "25M parameter both perform similar to the teacher network of Romero et al. (2014).", "startOffset": 61, "endOffset": 82}, {"referenceID": 10, "context": "Network Number of Layers Number of Parameters Accuracy Fitnet Results reported by Romero et al. (2014) Teacher 5 \u223c9M 90.", "startOffset": 82, "endOffset": 103}, {"referenceID": 10, "context": "For comparison, results reported by Romero et al. (2014) using maxout networks are also shown.", "startOffset": 36, "endOffset": 57}, {"referenceID": 13, "context": "Learning to route information through neural networks has helped to scale up their application to challenging problems by improving credit assignment and making training easier (Srivastava et al., 2015).", "startOffset": 177, "endOffset": 202}], "year": 2015, "abstractText": "There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on information highways. The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.", "creator": "LaTeX with hyperref package"}}}