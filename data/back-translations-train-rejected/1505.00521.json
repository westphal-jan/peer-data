{"id": "1505.00521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2015", "title": "Reinforcement Learning Neural Turing Machines - Revised", "abstract": "The expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation.", "histories": [["v1", "Mon, 4 May 2015 04:14:54 GMT  (40kb)", "http://arxiv.org/abs/1505.00521v1", null], ["v2", "Sat, 21 Nov 2015 19:37:59 GMT  (2622kb,D)", "http://arxiv.org/abs/1505.00521v2", null], ["v3", "Tue, 12 Jan 2016 06:35:48 GMT  (2622kb,D)", "http://arxiv.org/abs/1505.00521v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wojciech zaremba", "ilya sutskever"], "accepted": false, "id": "1505.00521"}, "pdf": {"name": "1505.00521.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning Neural Turing Machines", "authors": ["Wojciech Zaremba"], "emails": ["woj.zaremba@gmail.com", "ilyasu@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.00 521v 1 [cs.L G] 4M ayThe expressiveness of a machine learning model is closely related to the number of sequential arithmetic steps it can learn. Deep Neural Networks, for example, have been more successful than flat networks because they can perform a greater number of sequential arithmetic steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can express an even greater number of sequential arithmetic steps in a compact way, hence it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiated; hence, the NTM can be trained in reverse propagation. Differential memory is relatively easy to implement and requires access to the entire memory content at each processing step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce Algorithm to learn where memory can be accessed."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2 The Neural Turing Machines and Related Models", "text": "The Neural Turing Machine [8] is an ambitious, computationally universal model that can essentially be trained (or \"automatically programmed\") with the back-propagation algorithm. Graves et al. [8] \"s key idea is to use interpolation to make the model differentiable. Simple Turing Machine-like models typically consist of discrete controllers that read and write discrete addresses in a large memory. NTM replaces each individual action of the discrete controller with a distribution of actions and replaces its output with the overlay of possible outcomes weighted by their probabilities. Thus, while the original discrete action was not differentiable, the new action is a linear (and therefore differentiable) function of the probabilities entered, making it possible to train NTMs with reverse development."}, {"heading": "3 The Reinforce Algorithm", "text": "The Reinforce algorithm [16] is the simplest Reinforce learning algorithm [16] because it influences the variability of variability. It takes action according to its effect distribution and observes its reward. If the reward is greater than the average, Reinforce increases its reward. While the Reinforce algorithm is not particularly efficient, it has a simple mathematical formula that can be achieved by differentiating a cost function. Suppose we have a scope for action, a parameterized distribution planning (a) of actions and a reward function r (a). Then the Reinforce target is given by J (BA) = an increase in cost effect (a) (1) and its derivatives are either J (a) or J (a)."}, {"heading": "4 The RL-NTM", "text": "In this section, we will describe the RL-NTM and explain the exact way in which it is taking the action to date. The RL-NTM has a one-dimensional input band, a one-dimensional memory and a one-dimensional output band. It has pointers to a place on the tape and a place on the output band. The RL-NTM can move its input and memory pointers in any direction, but it can only move its position on the output band in the front direction. Besides, it must predict the desired output by emitting a distribution of the possible results, and it suffers the loss of the forecast in the core of the RL-NTM."}, {"heading": "4.1 Baseline Networks", "text": "The Reinforce algorithm works much better when it has precise baselines (see section 3 for a definition).The reward baseline is calculated using a separate LSTM as follows: 1. Run the baseline LSTM over the entire input band to create a hidden state that summarizes the inputs; 2. Continue to run the baseline LSTM together with the LSTM controller so that the baseline LSTM receives exactly the same inputs as the controller LSTM, and output a baseline B value at each time step. See Figure 2. The baseline LSTM is trained to minimize the difficulty of the examples."}, {"heading": "4.2 Curriculum Learning", "text": "DNNs are successful because they are easy to optimize, while NTMs are difficult to optimize. Thus, it is plausible that curriculum learning [4], which was not helpful for DNNs because their educational goals are too simple, will be useful for NTMs because their goals are more difficult. In our experiments, we used curricula whose details were borrowed from Zaremba and Sutskever [18]. For each task, we manually define a sequence of subtasks with increasing difficulty, often measuring the difficulty of a problem instance by its size or length. During training, we maintain a distribution over task difficulties; as the performance of the RL-NTM exceeds a threshold, we shift our distribution so that it focuses on more difficult problem cases, and that is critical, the distribution over difficulties always maintains a not negligible mass over the most difficult difficulty levels."}, {"heading": "4.3 The Modified RL-NTM", "text": "For example, a typical task is to reverse a sequence (Section 5.1 lists the tasks).For such tasks, the controller would benefit from a built-in mechanism to copy a corresponding input directly into memory and output, such a mechanism would free the LSTM controller from memorizing the input symbol in its control variables (\"registers\") and would shorten the backward propagation paths, thus facilitating learning. We implemented this mechanism by inserting the input into memory and output, and also inserting the memory into the output and adjacent memory (Figure 3), while modulating this additive contribution with a dynamic scalar (sigmoid) moid, which is calculated from the state of the controller. In this way, the controller can decide not effectively insert the current input into the output at a given time. Unfortunately, the necessity of this architectural modification is not a disadvantage as we would all work it out of our implementation, as it is not dependent on the implementation of our TM."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Tasks", "text": "A generic input is x1x1x2x2x3... xL \u2212 1xLxL \u2205, while the desired output is x1x2x3.. xL \u2205. Thus, each input symbol is replicated three times, so that the RL-NTM must send out every third input symbol. The length of the input sequence is variable and may change. The input sequence and the desired output both end with a special end-of-sequence symbol. 2. Vice versa. A generic input is x1x2... xL \u2212 1xL \u2205 and the desired output is xLxL \u2212 1.. x2x1 pl.3. Repeat copy. A generic input is x1x2."}, {"heading": "5.2 Training Details", "text": "In fact, most people are able to survive themselves, and they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt \":\" I don't think we will be able to save the world. \""}, {"heading": "6 Gradient Checking", "text": "An independent contribution to this work is a simple strategy for implementing a gradient checker for Reinforce. The RL-NTM is complex, so we needed a way to verify the veracity of our implementation. We discovered a technique that makes it possible to easily implement a gradient checker for almost any model, the Reinforce.The strategy is to create a sufficiently small problem instance in which the set of all possible actions is of a manageable size (such as 1000; let's call this size N). Then we would apply the RLNTM to each action sequence, multiply the loss and gradient of each such sequence by its probability. To achieve this, we calculate a sequence of actions and overload the scanning function to produce (a) said sequence of pre-calculated actions, and (b) maintain a product of the probabilities of forced actions."}, {"heading": "7 Conclusions", "text": "We have shown that the Reinforce algorithm is capable of training an NTM-like model to solve very simple algorithmic problems. Although the Reinforce algorithm is very generic and easily applicable to a variety of problems, it seems difficult to learn memory access patterns with Reinforce. We currently believe that a differentiated approach to memory addressing will likely yield better results in the short term. And, while the Reinforce algorithm could still be useful for training NTM-like models, it would need to be used in a different way than in this paper. Our method of verifying the history of Reinforce can be applied to a variety of implementations."}, {"heading": "A. RL-NTM Execution Traces", "text": "The first column corresponds to the input band and the second column to the output band. The first line shows the input band and the desired output, while each subsequent line shows the position of the RL-NTM on the input band and its prediction for the output band. In these examples, the RL-NTM solved each task perfectly so that the predictions made in the output band corresponded perfectly to the desired output listed on the first line."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Genetic programming: an introduction, volume 1", "author": ["Wolfgang Banzhaf", "Peter Nordin", "Robert E Keller", "Frank D Francone"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Connectionist models and their properties", "author": ["Jerome A Feldman", "Dana H Ballard"], "venue": "Cognitive science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Policy gradient methods for robotics", "author": ["Jan Peters", "Stefan Schaal"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A formal theory of inductive inference", "author": ["Ray J Solomonoff"], "venue": "i. Information and control,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1964}, {"title": "Weakly supervised memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN.", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "This argument has appeared at least as early as 1982 [6] and the success of DNNs on perception tasks suggests that it may be correct.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "A model that can perform a very large number of sequential computational steps and that has an effective learning algorithm would be immensely powerful [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 2, "context": "There has been some empirical work in this direction (notably in program induction and in genetic programming [3]) but the resulting systems do not scale to large problems.", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "[8]\u2019s Neural Turing Machine (NTM), a computationally universal model that can learn to solve simple algorithmic problems from input-output examples alone.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] used interpolation to make the NTM fully differentiable and therefore trainable with backpropagation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "The goal of this work is to use the Reinforce algorithm [16] to train NTMs.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8]), reversing a sequence, and a few more tasks of comparable complexity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The Neural Turing Machine [8] is an ambitious, computationally universal model that can be trained (or \u201cautomatically programmed\u201d) with the backpropagation algorithm using only input-output examples.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "[8] is to use interpolation to make the model differentiable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "In more detail, the NTM is an LSTM [9] controller that has an external memory module.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "A predecessor of the NTM which used a similar form of differentiable attention achieved compelling results on Machine Translation [2] and speech recognition [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 4, "context": "A predecessor of the NTM which used a similar form of differentiable attention achieved compelling results on Machine Translation [2] and speech recognition [5].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "Earlier, Graves [7] used a more restricted form of differentiable attention for handwritten text synthesis which, to the best of our knowledge, is the first differentiable attention model.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "Subsequent work used the idea of interpolation in order to train a stack augmented RNN, which is essentially an NTM but with a much simpler memory addressing mechanism [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "[14] addressed this problem using differentiable attention within the Memory Network framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11], and our model uses a very similar formulation in order to learn to control the memory address.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "There have since been a number of papers on visual attention that have used both Reinforce and differentiable attention [1, 17].", "startOffset": 120, "endOffset": 127}, {"referenceID": 15, "context": "There have since been a number of papers on visual attention that have used both Reinforce and differentiable attention [1, 17].", "startOffset": 120, "endOffset": 127}, {"referenceID": 14, "context": "The Reinforce algorithm [16] is the simplest Reinforcement learning algorithm.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "In this setting, it is possible to reduce the variance of gradient estimates of action distributions near the end of an episode [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "Thus, it is plausible that curriculum learning [4], which has not been helpful for DNNs because their training objectives are too easy, will be useful for NTMs since their objectives are harder.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "[8] suggest that a differentiable memory may result in models that are easier to train.", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "The expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessitates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first model that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.", "creator": "LaTeX with hyperref package"}}}