{"id": "1505.00641", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2015", "title": "fastFM: A Library for Factorization Machines", "abstract": "Factorization Machines (FM) are only used in a narrow range of applications and are not part of the standard toolbox of machine learning models. This is a pity, because even though FMs are recognized as being very successful for recommender system type applications they are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM's for a wide field of applications. This implementation has the potential to improve our understanding of the FM model and drive new development.", "histories": [["v1", "Mon, 4 May 2015 14:06:11 GMT  (72kb,D)", "https://arxiv.org/abs/1505.00641v1", null], ["v2", "Tue, 5 May 2015 18:43:34 GMT  (72kb,D)", "http://arxiv.org/abs/1505.00641v2", "Source Code is available atthis https URL"], ["v3", "Wed, 23 Nov 2016 14:25:55 GMT  (56kb,D)", "http://arxiv.org/abs/1505.00641v3", "Source Code is available atthis https URL"]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["immanuel bayer"], "accepted": false, "id": "1505.00641"}, "pdf": {"name": "1505.00641.pdf", "metadata": {"source": "CRF", "title": "fastFM: A Library for Factorization Machines", "authors": ["Immanuel Bayer", "Cheng Soon Ong"], "emails": ["immanuel.bayer@uni-konstanz.de"], "sections": [{"heading": null, "text": "Keywords: Python, MCMC, matrix factorization, contextual recommendation"}, {"heading": "1. Introduction", "text": "This work aims to facilitate the study of matrix factorization models for machine learning (ML). Factorization machines are capable of expressing many different latent factor models and are commonly used for collaborative filter tasks (Rendle, 2012b). An important advantage of FM is that the model equationw0, x, w Rp, vi Rky FM (x): = w0 + p i = 1 p j > i < vi, vj > xixj (1) of the standard notation for vector-based ML.FM learn a factorized coefficient < vi, vj > for each functional pair xixj (eq. 1), which allows to model very sparse feature interactions, such as coding an example of vector-based ML.FM learn a factorized coefficient < vi, vj > for each functional pair xixj (exiq)."}, {"heading": "2. Design Overview", "text": "The fastFM library has a multi-layered software architecture (Figure 1) that separates the interface code from the performance-critical parts (fastFM-core); the core contains the solvers, is written in C and can be used independently; two user interfaces are available: a command-line interface (CLI) and a Python interface. Cython (Behnel et al., 2011) is used to build a Python extension from the C library. Both, the Python and C interfaces, serve as reference implementations for binding to additional languages. 2.1 fastFM-CorefastFM (Py) directly passes on Cython CLIfastFM-core (C). fastFM includes a test suite that runs on each GitHub repository with a continuous integration server4. The solvers are tested using state-of-the-art techniques such as Posterior Quantiles (Cook et al, 2006) for samplers and samplers."}, {"heading": "2.2 Solver and Loss Functions", "text": "fastFM provides a range of solutions for all supported tasks (Table 1).The MCMC Solver implements the Bayes Factorization Machine model (Freudenthaler et al., 2011) using Gibbs sampling. We use the pair-by-pair Bayesian Personalized Ranking Loss (BPR) (Rendle et al., 2009).For more details on classification and regression solvers, see Rendle (2012b).3. CXSparse is LGPL licensed. 4. https: / / travis-ci.org / ibayer / fastFM-core"}, {"heading": "2.3 Python Interface", "text": "The Python interface is compatible with the API of the widely used Scikit Learn library (Pedregosa et al., 2011), which opens the library to a large user base; the following snippet of code shows how to use MCMC sampling for an FM classifier and how to predict new data.fm = mcmc.FMClassification (init std = 0.01, rank = 8) y pred = fm.fit predict (X-move, y-move, X-test) fastFM offers additional features such as warm-starting a solver from an earlier solution (see MCMC example).fm = als.FMRegression (init std = 0.01, rank = 8, l2 reg = 2) fm.fit (X-move, y-move)."}, {"heading": "3. Experiments", "text": "libFM5 is the reference implementation for FM and the only one that provides an ALS and MCMC solver. Our experiments show that the ALS and MCMC solver in fastFM is favorable to libFM in terms of runtime (Figure 2) and indistinguishable in terms of accuracy. Experiments were performed with the MovieLens 10M dataset using the original split with a fixed number of 200 iterations for all experiments.The x axis indicates the number of latent factors (rank) and the y axis the runtime in seconds.The diagrams show that runtime is scaled linearly with the rank for both implementations.The code snippetbelow shows how easy it is to write Python code that allows model verification after each iteration.The induced Python function call Overhead occurs only once per iteration and is therefore negligible."}, {"heading": "4. Related Work", "text": "Factorization machines are available in the major machine learning libraries GraphLab (Low et al., 2014) and Bidmach (Canny and Zhao, 2013). Chen et al. \"s Svdfeatures toolkit (2012) provides a generic MF model similar to FM. Implementations in GraphLab, Bidmach, and Svdfeatures support only SGD solvers and do not result in a decline in ranking. Our goal is not to replace these distributed machine learning frameworks, but to provide an FM implementation that is easy to use and expand without sacrificing performance."}, {"heading": "Acknowledgments", "text": "This work was supported by the DFG under the grant Re 3311 / 2-1."}], "references": [{"title": "Cython: The best of both worlds", "author": ["Stefan Behnel", "Robert Bradshaw", "Craig Citro", "Lisandro Dalcin", "Dag Sverre Seljebotn", "Kurt Smith"], "venue": "Computing in Science & Engineering,", "citeRegEx": "Behnel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Behnel et al\\.", "year": 2011}, {"title": "Bidmach: Large-scale learning with zero memory allocation", "author": ["John Canny", "Huasha Zhao"], "venue": "In BigLearn Workshop,", "citeRegEx": "Canny and Zhao.,? \\Q2013\\E", "shortCiteRegEx": "Canny and Zhao.", "year": 2013}, {"title": "Svdfeature: a toolkit for feature-based collaborative filtering", "author": ["Tianqi Chen", "Weinan Zhang", "Qiuxia Lu", "Kailong Chen", "Zhao Zheng", "Yong Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Validation of software for bayesian models using posterior quantiles", "author": ["Samantha R Cook", "Andrew Gelman", "Donald B Rubin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Cook et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cook et al\\.", "year": 2006}, {"title": "Direct methods for sparse linear systems, volume 2. Siam", "author": ["Timothy A Davis"], "venue": null, "citeRegEx": "Davis.,? \\Q2006\\E", "shortCiteRegEx": "Davis.", "year": 2006}, {"title": "Bayesian factorization machines", "author": ["Christoph Freudenthaler", "Lars Schmidt-thieme", "Steffen Rendle"], "venue": "In Proceedings of the NIPS Workshop on Sparse Representation and Low-rank Approximation,", "citeRegEx": "Freudenthaler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Freudenthaler et al\\.", "year": 2011}, {"title": "Graphlab: A new framework for parallel machine learning", "author": ["Yucheng Low", "Joseph E Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos E Guestrin", "Joseph Hellerstein"], "venue": "arXiv preprint arXiv:1408.2041,", "citeRegEx": "Low et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Low et al\\.", "year": 2014}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Social network and click-through prediction with factorization machines", "author": ["Steffen Rendle"], "venue": "In KDD-Cup Workshop,", "citeRegEx": "Rendle.,? \\Q2012\\E", "shortCiteRegEx": "Rendle.", "year": 2012}, {"title": "Factorization machines with libFM", "author": ["Steffen Rendle"], "venue": "ACM Trans. Intell. Syst. Technol.,", "citeRegEx": "Rendle.,? \\Q2012\\E", "shortCiteRegEx": "Rendle.", "year": 2012}, {"title": "Factor models for tag recommendation in bibsonomy", "author": ["Steffen Rendle", "Lars Schmidt-Thieme"], "venue": "ECML/PKDD", "citeRegEx": "Rendle and Schmidt.Thieme.,? \\Q2008\\E", "shortCiteRegEx": "Rendle and Schmidt.Thieme.", "year": 2008}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In UAI", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Maximum-margin matrix factorization", "author": ["Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 12, "context": "This makes it possible to model very sparse feature interactions, as for example, encoding a sample as x = {\u00b7 \u00b7 \u00b7 , 0, xi {}}{ 1 , 0, \u00b7 \u00b7 \u00b7 , 0, xj {}}{ 1 , 0, \u00b7 \u00b7 \u00b7 } yields \u0177FM (x) = w0 + wi + wj + v T i vj which is equivalent to (biased) matrix factorization Ri,j \u2248 b0 + bi + bj + ui vj (Srebro et al., 2004).", "startOffset": 290, "endOffset": 311}, {"referenceID": 8, "context": "Please refer to Rendle (2012b) for more encoding examples.", "startOffset": 16, "endOffset": 31}, {"referenceID": 0, "context": "Cython (Behnel et al., 2011) is used to create a Python extension from the C library.", "startOffset": 7, "endOffset": 28}, {"referenceID": 4, "context": "We use the standard compressed row storage (CRS) matrix format as underlying data structure and rely on the CXSparse3 library (Davis, 2006) for fast sparse matrix / vector operations.", "startOffset": 126, "endOffset": 139}, {"referenceID": 3, "context": "Solvers are tested using state of the art techniques, such as Posterior Quantiles (Cook et al., 2006) for the MCMC sampler and Finite Differences for the SGD based solvers.", "startOffset": 82, "endOffset": 101}, {"referenceID": 5, "context": "The MCMC solver implements the Bayesian Factorization Machine model (Freudenthaler et al., 2011) via Gibbs sampling.", "startOffset": 68, "endOffset": 96}, {"referenceID": 11, "context": "We use the pairwise Bayesian Personalized Ranking (BPR) loss (Rendle et al., 2009) for ranking.", "startOffset": 61, "endOffset": 82}, {"referenceID": 11, "context": "Task Solver Loss Regression ALS, MCMC, SGD Square Loss Classification ALS, MCMC, SGD Probit (MAP), Probit, Sigmoid Ranking SGD BPR (Rendle et al., 2009) Table 1: Supported solvers and tasks", "startOffset": 131, "endOffset": 152}, {"referenceID": 5, "context": "The MCMC solver implements the Bayesian Factorization Machine model (Freudenthaler et al., 2011) via Gibbs sampling. We use the pairwise Bayesian Personalized Ranking (BPR) loss (Rendle et al., 2009) for ranking. More details on the classification and regression solvers can be found in Rendle (2012b). Task Solver Loss Regression ALS, MCMC, SGD Square Loss Classification ALS, MCMC, SGD Probit (MAP), Probit, Sigmoid Ranking SGD BPR (Rendle et al.", "startOffset": 69, "endOffset": 302}, {"referenceID": 7, "context": "3 Python Interface The Python interface is compatible with the API of the widely-used scikit-learn library (Pedregosa et al., 2011) which opens the library to a large user base.", "startOffset": 107, "endOffset": 131}, {"referenceID": 5, "context": "Please note that the MCMC solver uses Gaussian priors for the model parameter (Freudenthaler et al., 2011).", "startOffset": 78, "endOffset": 106}, {"referenceID": 6, "context": "Related Work Factorization Machines are available in the large scale machine learning libraries GraphLab (Low et al., 2014) and Bidmach (Canny and Zhao, 2013).", "startOffset": 105, "endOffset": 123}, {"referenceID": 1, "context": ", 2014) and Bidmach (Canny and Zhao, 2013).", "startOffset": 20, "endOffset": 42}, {"referenceID": 1, "context": ", 2014) and Bidmach (Canny and Zhao, 2013). The toolkit Svdfeatures by Chen et al. (2012) provides a general MF model that is similar to a FM.", "startOffset": 21, "endOffset": 90}], "year": 2016, "abstractText": "Factorization Machines (FM) are currently only used in a narrow range of applications and are not yet part of the standard machine learning toolbox, despite their great success in collaborative filtering and click-through rate prediction. However, Factorization Machines are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation (fastFM) provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM for a wide range of applications. Therefore, our implementation has the potential to improve understanding of the FM model and drive new development.", "creator": "LaTeX with hyperref package"}}}