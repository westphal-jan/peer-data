{"id": "1505.00670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2015", "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation", "abstract": "Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of representative ~216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on document- and sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of large-scale learning and prediction in electronic patient records available in most modern clinical institutions.", "histories": [["v1", "Mon, 4 May 2015 15:05:59 GMT  (28813kb,D)", "http://arxiv.org/abs/1505.00670v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hoo-chang shin", "le lu", "lauren kim", "ari seff", "jianhua yao", "ronald m summers"], "accepted": false, "id": "1505.00670"}, "pdf": {"name": "1505.00670.pdf", "metadata": {"source": "CRF", "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation", "authors": ["Hoo-Chang Shin", "Lauren Kim", "Jianhua Yao", "Ronald M. Summers", "Ari Seff", "Le Lu"], "emails": ["hoochang.shin@nih.gov", "le.lu@nih.gov", "lauren.kim2@nih.gov", "ari.seff@nih.gov", "jyao@cc.nih.gov", "rms@nih.gov"], "sections": [{"heading": null, "text": "Keywords: Deep learning, Convolutional Neural Networks, Topic Models, Natural Language Processing, Medical Imaging"}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}, {"heading": "1.1 Related Work", "text": "The ImageCLEF Medical Image Annotation tasks from 2005-2007 by Deselaers and Ney (2008) have 9,000 training and 1,000 tests two-dimensional images, converted into 32 x 32 pixel thumbnails with 57 captions. However, local image descriptors and intensity histograms are used as a bag-of-features approach in this work for this scene-detection-like problem. Grrivick et al. (2005) \"s work is based on the assignment of lung disease words (e.g. fibrosis, emphysema) to two-dimensional image blocks from axial CT breast scans of 24 patients being examined by Carrivick et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al (2005). The work of Barnard et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al (2003); Lead and Jordan (2003) using generative models of the combination of words images and a very limited word-to-24-word image scans."}, {"heading": "2. Data", "text": "To get the most comprehensive interpretation of diagnostic semantics, we use all available radiological reports of approximately 780K images stored in the PACS of the National Institutes of Health Clinical Center since 2000. Approximately 216K of important two-dimensional images are examined here, rather than using all three-dimensional image volumes. Within three-dimensional patient scans, most of the imaging information presented is normal anatomy, so they are often not at the center of radiological reports. The two-dimensional \"key images\" that radiologists manually refer to during the preparation of radiology reports (Figure 1) provide a visual reference to pathologies or other noteworthy findings. Therefore, the two-dimensional key images correlate more strongly with diagnostic semantics in the reports than the entire three-dimensional scans, but not all reports refer to key images (215,786 images of approximately 61,845 unique patients)."}, {"heading": "3. Document Topic Learning with Latent Dirichlet Allocation", "text": "Unlike images from ImageNet (Deng et al. (2009); Russakovsky et al. (2014), which often have a dominant object that appears in the center, our key images are mostly CT and MRI snippets, which typically show multiple organs with pathologies. There are large sums of intrinsic ambiguity in defining and assigning a semantic label to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-millions of radiology reports statistically defines the categories for topic mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Lead et al (2003) to find latent theme models for a collection of text labels such as newspaper articles."}, {"heading": "4. Image to Document Topic Mapping with Deep Convolutional Neural", "text": "Networks For each level of topics discussed in Section 3, we train deep CNNs to map the images into document categories using the Caffe Framework from Jia et al. (2014). We divide our entire central image set as follows: 85% are used as training data set, 5% as cross-validation (CV) and 10% as test data set. If a topic has too few images to be divided into training / resume / test for in-depth CNN learning, then this topic is neglected for CNN training. These cases are usually subjects of rare image processing protocols, for example: Topic # 5 - Abdominal ultrasound; Topic # 28 & # 49 - DEXA scans of different uses. In total, 60 topics have been used for document-level topic mapping, 385 for document-level sub-topic mapping and 717 for sentence-image mapping. Systematic diagrams that show images as being assigned to topics, as each one is learned to be assigned to topics, and each one to be assigned to 6."}, {"heading": "4.1 Implementation", "text": "All our CNN network settings are similar or equal to the ImageNet challenge \"AlexNet\" (Krizhevsky et al. (2012) and \"VGG-16 & 19\" (Simonyan and Zisserman et al. (2014)). For \"AlexNet\" we used the Caffe reference network of Jia et al. (2014), which is a slight modification of the \"AlexNet\" of Krizhevsky et al. (2012).The AlexNet model of Krizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five revolutionary layers (1st, 2nd and 5th followed by max-pooling layers) and three fully connected (FC) layers with a definitive classification layer. The VGG variations of the CNN models of Simonyan and Zisserman lower layers (2014) are significantly lower, showing 16 x 19 convolutional layers and 133 million parameters."}, {"heading": "4.2 Transfer Learning and Domain Adaptation", "text": "We found that transferring learning outcomes from the ImageNet pre-trained CNN parameters to natural images to our medical image modalities (mostly CT, MRI) significantly improves image classification performance. Furthermore, it was found that transferring learning outcomes from a CNN trained for a similar task (e.g. from CNN trained at picture-to-document level topic models to train CNN for picture-to-document subtopic models) is more effective than from a CNN trained for a less related task (e.g. from CNN trained at ImageNet level to train CNN for picture-to-document subtopic models). Examples of the classification accuracy tracked during training using CNN's random initialization, the transfer of CNN trained on ImageNet, and the transfer of learning outcomes from higher image to lower-document subtopic models are shown in Figure 7, where similar results can be reported across multiple image upta functions."}, {"heading": "4.3 Classification Results and Discussion", "text": "We would expect that the levels of difficulty for learning and classifying the images into the LDA-induced net topics will be different for each semantic level. Low-level semantic classes may have key frames of axial / sagittal / coronal sections with position variations and via MRI / CT modalities. However, mid-level to high-level concepts show much greater variation within the class in their visual appearance, as they are diseases that occur within different organs and are coherent only at a high level in semantics. Table 3 provides the top-1 and top-5 tests in classification accuracy for each level of topic models with AlexNet (Krizhevsky et al. (2012), and VGG-16 & 19 (Simonyan and Zisserman (2014)) -based deep CNN models. All top-5 accuracy values are significantly higher than top-1 values, e.g. an increase from 0.658 to net 0.907 to net 0.946 using the Alexein19 or non-Alexein19 classification."}, {"heading": "5. Generating Image-to-Text Description", "text": "Image-to-topic mapping in Section 4 is a promising first step toward automated interpretation of medical images on a large scale. However, it is too expensive and time-consuming for radiologists to examine all 1880 (80 + 800 + 1000) generated topics with their keywords and images. In addition, keywords in the topics can help to understand the semantic content of a particular image with more semantic meaning. Therefore, we suggest creating relevant keyword text descriptions similar to Kulkarni et al. (2013) using CNN models in depth language / image."}, {"heading": "5.1 Word-to-Vector Modeling and Removing Word-Level Ambiguity", "text": "In radiological reports, there are many recurring word morphisms in text identification, e.g., [mr, mri, t1- / t2-weighted] (natural language expressions for magnetic resonance imaging (MRI) imaging modalities), [cyst, cystic, cysts], [tumor, tumor, tumor, metastasis, metastatic], etc. We train a deep word-to-vector model by Mikolov et al. (2013c, b, a) to label these words ambiguously at word level, while also transforming the words into vectors. A total of 1.2 billion words from our radiological reports, as well as from biomedical research articles derived from OpenI (openi: http: / openi.nlm.nih.gov) are used to label words at word level that have an ambiguity. Words with similar meaning are mapped to similar places in the vector space or mapped to similar places."}, {"heading": "5.2 Image-to-Description Relation Mining and Matching", "text": "The sentence that refers to a key picture and its adjacent sentences may contain a variety of words, but we are largely interested in disease-related terms that are strongly correlated with diagnostic semantics. To get just the disease-related terms, we use the human disease terms and their synonyms from Disease Ontology (DO; Schriml et al. (2012)), a collection of 8,707 unique disease-related terms. While the sentences that refer to an image and its adjacent sentences have an average of 50.08 words, the number of disease-related terms in the three consecutive sentences is an average of 5.17 with a standard deviation of 2.5. Therefore, we have decided to use bi-grams for the image descriptions to achieve a good trade-off between the mediocre complexity without neglecting too many text-image pairs. Some statistics on the number of words in the documents are in the form of bi-gram image terms, so we can predict the picture-couple of disease without having to predict it."}, {"heading": "5.3 Image-to-Words Deep CNN Regression", "text": "It has been shown by Sutskever et al. (2014) that deep recurrent neural networks (RNN) can learn the language representation for machine translation. To learn the image-text representation, we map the images onto the vectors of word strings describing the image. This can be formulated as regression CNN, where the softmax costs in Section 4 are replaced by the cross-entropy cost function for the last output layer of the VGG-19 CNN model (Simonyan and Zisserman (2014): E = \u2212 1 n N-N = 1 [g (z) ng-n) + (1 \u2212 g (zn)) log (1 \u2212 g (z-n))], (2) where zn or z-n are some unielement of the target word Zn or optimized output vectors Z-n, g (x) is the sigmoid function (g) = 1 (ex), and the image is a better one than the sample number 12 of the data base."}, {"heading": "5.4 Key-Word Generation from Images and Discussion", "text": "For each key image in the test, we first predict its themes on three levels (document level, document level, subthemes, sentence level) by including the three deep CNN models by Simonyan and Zisserman (2014) in Section 4. The 50 keywords in each LDA document theme are included in the word-to-vector space of multivariate variables in Section 5.1. Then, the image is assigned to an R256 \u00d7 2 output vector using the bi-gram CNN model in Section 5.3. Finally, we match each of the 50 thematic keyword vectors in R256 \u00d7 1 with the first and second half of the R256 \u00d7 2 output vector using cosmic similarity. The closest keywords on three levels of the themes (with the highest cosmic similarity) are the cosmic words (with the two bi-gram words)."}, {"heading": "5.4.1 Discussion", "text": "The generation of keywords for images by CNN regression shows a good feasibility for the automated interpretation of patient images. The generated keywords describe what can be expected from the given image, although sometimes other words can also be generated. Finding and understanding the relationships between the generated words will be the next step to be explored, for example by more thorough text mining using sophisticated NLP parsing such as Li et al. (2011) and combining it with the specific frequent disease prediction in the next section."}, {"heading": "6. Predicting Presence or Absence of Frequent Disease Types", "text": "While the keyword generation in Section 5 can facilitate the interpretation of a patient scan, the keywords generated, such as \"spine,\" \"lung,\" are not very specific to a disease in an image. Nevertheless, one of the ultimate goals for large-scale radiological image / text analysis would be to automatically diagnose diseases from a patient scanner. To achieve the goal of automated disease detection, we added an additional pipeline of disease words instead of disease-related words using radiological semantics and predicted them in an image using CNNs with Softmax cost function."}, {"heading": "6.1 Mining Presence/Absence of Frequent Disease Terms", "text": "Disease names in Ontology of Diseases (DO) include not only disease terms, but also terms that describe a disease. Some examples of disease names in DO that do not include disease terms are: \"The concepts of gallbladder\" (DOID: 0050140), \"the concepts of gallbladder\" (DOID: 10254), and \"exocrine pancreatic insufficiency\" (DOID: 13316). Nevertheless, it is rare that \"the occurrence of gallbladder\" or \"exocrine pancreatic insufficiency\" is precisely described in radiology in such a way that it is difficult to identify specific disease terms with the presence or absence of disease. \"(UMLS) by Lindberg et al al al. (1993); Humphreys et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al e al al al al al al al al al al al al al al al al al al al key socio al al al al al al al (1998 key social and al al al al al al al al al al al al al promotion)."}, {"heading": "6.2 Predicting Disease in Images using CNN", "text": "Similar to the object detection task in the ImageNet challenge, we match and recognize disease terms found in a set of radiological reports with CNN and Softmax cost function. Softmax models the probability that an instance is the class j of the N total classes with normalized exponential. (3) Softmax is a generalization of the logistic function that summarizes an N-dimensional vector of real values in the range 0 to 1: \u03c3 (z) j = ezj \u2211 N = 1 e zk. (3) Softmax is often implemented at the last level of the neural networks used for classification and is a standard among CNN-based approaches in ImageNet object detection problems. In addition to assigning disease terms to images, we assign nested disease terms as the absence of disease terms in the images. The total number of labels is 77 (59 present, 18 absent). If more than one picture is mentioned for the number of disease terms in this section, some of the statistics section are more than one."}, {"heading": "6.3 Prediction Result and Discussion", "text": "With CNN trained to predict the presence of disease / absence, the Top1 test accuracy is achieved is 0.71, and Top 5 accuracy is 0.88. We combine this with the previous picture-to-topic mapping and keyword generation (Section 5,4) to generate the final output for comprehensive image interpretation. Some examples of test cases where top 1 probabilities match with the originally associated disease labels are shown in Figure 14. It is noteworthy that certain disease words are highly likely to be detected if there is one disease word per image, and with slightly lower top 1 probability for a disease word and the other words within the top 5 probabilities (Figure 14)."}, {"heading": "6.3.1 Discussion", "text": "Automated disease-specific term mining allows us to predict disease more specifically with promising results. However, compared to the picture-to-topic modeling in Section 4, where the caption was based on theme modeling and loose coupling of picture-to-keyword pairs, we lose about 90% of the images for analysis based on non-specific original statements. The percentage of cases where radiologists indicate a disease as strongly positive or negative is often much lower than the cases where they describe a finding rather vaguely. Mining and matching the semantic term \"T033: Find\" gives us more images of specific disease-identifier pairs. However, it is probably less specific to model an image with a generic term such as \"mass\" (which is a vague reference to a specific disease such as \"cyst\" or \"tumor\") and recognizing that term instead of labeling and identifying an image with a specific term such as \"cyst.\""}, {"heading": "7. Conclusion", "text": "It was unclear how the significant success of image classification using deep Convolutionary Neural Networks could be extended from computer vision to medical imaging; what clinically relevant image labels are to be defined; how to comment on the vast amount of medical images required by deep learning models; and to what extent and scale the deep CNN architecture is generalizable in medical image analysis are the open questions. In this paper, we present a networked text / image deep-mining system to extract the semantic interactions of radiological reports and key diagnostic images on a very large, unprecedented scale in the medical realm. Images are classified into different subject levels according to the accompanying documents, and a neural language model is learned to assign disease terms to predict what is in the image. In addition, we have degraded specific disease terms for more specific automated image labels and matched them with promising results."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Intramural Research Program of the National Institutes of Health Clinical Center and in part by a scholarship from the KRIBB Research Initiative Program (Korean Visiting Scientist Training Award), Korea Research Institute of Bioscience and Biotechnology, Republic of Korea. This study utilized the powerful computing capabilities of the Biowulf Linux Cluster at the National Institutes of Health, Bethesda, MD (http: / / biowulf.nih.gov). We thank NVIDIA for the GPU donation from K40."}], "references": [{"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Natural language processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Modeling annotated data", "author": ["D. Blei", "M. Jordan"], "venue": "In ACM SIGIR,", "citeRegEx": "Blei and Jordan.,? \\Q2003\\E", "shortCiteRegEx": "Blei and Jordan.", "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Unsupervised learning in radiology using novel latent variable models", "author": ["Luke Carrivick", "Sanjay Prabhu", "Paul Goddard", "Jonathan Rossiter"], "venue": "In CVPR,", "citeRegEx": "Carrivick et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Carrivick et al\\.", "year": 2005}, {"title": "A simple algorithm for identifying negated findings and diseases in discharge summaries", "author": ["Wendy W Chapman", "Will Bridewell", "Paul Hanbury", "Gregory F Cooper", "Bruce G Buchanan"], "venue": "Journal of biomedical informatics,", "citeRegEx": "Chapman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2001}, {"title": "Extending the negex lexicon for multiple languages", "author": ["Wendy W Chapman", "Dieter Hilert", "Sumithra Velupillai", "Maria Kvist", "Maria Skeppstedt", "Brian E Chapman", "Michael Conway", "Melissa Tharp", "Danielle L Mowery", "Louise Deleger"], "venue": "Studies in health technology and informatics,", "citeRegEx": "Chapman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2013}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deformations, patches, and discriminative models for automatic annotation of medical radiographs", "author": ["Thomas Deselaers", "Hermann Ney"], "venue": "PRL,", "citeRegEx": "Deselaers and Ney.,? \\Q2008\\E", "shortCiteRegEx": "Deselaers and Ney.", "year": 2008}, {"title": "Nonnegative matrix factorization and probabilistic latent semantic indexing: Equivalence chi-square statistic, and a hybrid method", "author": ["Chris Ding", "Tao Li", "Wei Peng"], "venue": "In Proceedings of the national conference on artificial intelligence,", "citeRegEx": "Ding et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ding et al\\.", "year": 1999}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Gregory Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Relation between plsa and nmf and implications", "author": ["Eric Gaussier", "Cyril Goutte"], "venue": "In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Gaussier and Goutte.,? \\Q2005\\E", "shortCiteRegEx": "Gaussier and Goutte.", "year": 2005}, {"title": "On an equivalence between plsi and lda", "author": ["Mark Girolami", "Ata Kab\u00e1n"], "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "Girolami and Kab\u00e1n.,? \\Q2003\\E", "shortCiteRegEx": "Girolami and Kab\u00e1n.", "year": 2003}, {"title": "Natural image bases to represent neuroimaging data", "author": ["Ashish Gupta", "Murat Ayhan", "Anthony Maida"], "venue": "In ICML,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Learning rich features from rgb-d images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbelez", "J. Malik"], "venue": "In ECCV,", "citeRegEx": "Gupta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "The unified medical language system an informatics research collaboration", "author": ["Betsy L Humphreys", "Donald AB Lindberg", "Harold M Schoolman", "G Octo Barnett"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Humphreys et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Humphreys et al\\.", "year": 1998}, {"title": "Deep features for text spotting", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "In ECCV,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei F Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Hipster wars: Discovering elements of fashion styles", "author": ["H. Kiapour", "K. Yamaguchi", "A. Berg", "T. Berg"], "venue": "In ECCV,", "citeRegEx": "Kiapour et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiapour et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A. Berg", "T. Berg"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Kulkarni et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2013}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lampert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2014}, {"title": "Radlex: A new method for indexing online educational materials 1. Radiographics", "author": ["Curtis P Langlotz"], "venue": null, "citeRegEx": "Langlotz.,? \\Q2006\\E", "shortCiteRegEx": "Langlotz.", "year": 2006}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Yann LeCun", "Fu Jie Huang", "Leon Bottou"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": null, "citeRegEx": "Lee and Seung.,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung.", "year": 1999}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T. Berg", "A. Berg", "Y. Choi"], "venue": "In ACM CoNLL,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "The unified medical language system", "author": ["Donald A Lindberg", "Betsy L Humphreys", "Alexa T McCray"], "venue": "Methods of information in Medicine,", "citeRegEx": "Lindberg et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Lindberg et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Learning high-level judgments of urban perception", "author": ["V. Ordonez", "T. Berg"], "venue": "In ECCV,", "citeRegEx": "Ordonez and Berg.,? \\Q2014\\E", "shortCiteRegEx": "Ordonez and Berg.", "year": 2014}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "arXiv preprint arXiv:1409.0575,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Multi-attribute spaces: Calibration for attribute fusion and similarity search", "author": ["W. Scheirer", "N. Kumar", "P. Belhumeur", "T. Boult"], "venue": "In CVPR,", "citeRegEx": "Scheirer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scheirer et al\\.", "year": 2012}, {"title": "Disease ontology: a backbone for disease semantic integration", "author": ["Lynn Marie Schriml", "Cesar Arze", "Suvarna Nadendla", "Yu-Wei Wayne Chang", "Mark Mazaitis", "Victor Felix", "Gang Feng", "Warren Alden Kibbe"], "venue": "Nucleic acids research,", "citeRegEx": "Schriml et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schriml et al\\.", "year": 2012}, {"title": "The umls metathesaurus: representing different views of biomedical concepts", "author": ["Peri L Schuyler", "William T Hole", "Mark S Tuttle", "David D Sherertz"], "venue": "Bulletin of the Medical Library Association,", "citeRegEx": "Schuyler et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Schuyler et al\\.", "year": 1993}, {"title": "Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4d patient data", "author": ["Hoo-Chang Shin", "Matthew R Orton", "David J Collins", "Simon J Doran", "Martin O Leach"], "venue": "Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shin et al\\.,? \\Q1930\\E", "shortCiteRegEx": "Shin et al\\.", "year": 1930}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Exploring topic coherence over many models and many topics", "author": ["Keith Stevens", "Philip Kegelmeyer", "David Andrzejewski", "David Buttler"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Stevens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stevens et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories.", "startOffset": 70, "endOffset": 89}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al.", "startOffset": 70, "endOffset": 414}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al.", "startOffset": 70, "endOffset": 445}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al. (2014). Deep CNNs can perform significantly better than traditional shallow learning methods, but usually requires much more training data as was shown by Krizhevsky et al.", "startOffset": 70, "endOffset": 468}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al. (2014). Deep CNNs can perform significantly better than traditional shallow learning methods, but usually requires much more training data as was shown by Krizhevsky et al. (2012); Russakovsky et al.", "startOffset": 70, "endOffset": 641}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al. (2014). Deep CNNs can perform significantly better than traditional shallow learning methods, but usually requires much more training data as was shown by Krizhevsky et al. (2012); Russakovsky et al. (2014). In the medical domain, however, there are no similar large-scale", "startOffset": 70, "endOffset": 668}, {"referenceID": 6, "context": "Building the ImageNet database (Deng et al. (2009)) was mainly a manual process: harvesting images returned from Google image search engine according to the WordNet (Miller (1995)) ontology hierarchy and pruning falsely tagged images using crowd-sourcing such as Amazon Mechanical Turk (AMT).", "startOffset": 32, "endOffset": 51}, {"referenceID": 6, "context": "Building the ImageNet database (Deng et al. (2009)) was mainly a manual process: harvesting images returned from Google image search engine according to the WordNet (Miller (1995)) ontology hierarchy and pruning falsely tagged images using crowd-sourcing such as Amazon Mechanical Turk (AMT).", "startOffset": 32, "endOffset": 180}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels.", "startOffset": 140, "endOffset": 159}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al.", "startOffset": 140, "endOffset": 650}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al.", "startOffset": 140, "endOffset": 677}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation.", "startOffset": 140, "endOffset": 813}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions.", "startOffset": 140, "endOffset": 953}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions. Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random field (CRF) is a feasible approach as shown by Kulkarni et al. (2013), and many useful tools for image annotation using it are available in computer vision.", "startOffset": 140, "endOffset": 1295}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions. Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random field (CRF) is a feasible approach as shown by Kulkarni et al. (2013), and many useful tools for image annotation using it are available in computer vision. Both deep feed-forward CNNs of Krizhevsky et al. (2012); Simonyan and Zisserman (2014) and recurrent neural networks of Mikolov et al.", "startOffset": 140, "endOffset": 1438}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions. Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random field (CRF) is a feasible approach as shown by Kulkarni et al. (2013), and many useful tools for image annotation using it are available in computer vision. Both deep feed-forward CNNs of Krizhevsky et al. (2012); Simonyan and Zisserman (2014) and recurrent neural networks of Mikolov et al.", "startOffset": 140, "endOffset": 1469}, {"referenceID": 5, "context": "1 Related Work The ImageCLEF medical image annotation tasks of 2005-2007 by Deselaers and Ney (2008) have 9,000 training and 1,000 testing two-dimensional images, converted to 32 \u00d7 32 pixel thumbnails with 57 labels.", "startOffset": 76, "endOffset": 101}, {"referenceID": 3, "context": ", fibrosis, emphysema) to two-dimensional image blocks from axial CT chest scans of 24 patients is studied by Carrivick et al. (2005). The works of Barnard et al.", "startOffset": 110, "endOffset": 134}, {"referenceID": 3, "context": ", fibrosis, emphysema) to two-dimensional image blocks from axial CT chest scans of 24 patients is studied by Carrivick et al. (2005). The works of Barnard et al. (2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al.", "startOffset": 8, "endOffset": 210}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al.", "startOffset": 8, "endOffset": 231}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al.", "startOffset": 8, "endOffset": 424}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds.", "startOffset": 8, "endOffset": 467}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al.", "startOffset": 8, "endOffset": 647}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al.", "startOffset": 8, "endOffset": 669}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al.", "startOffset": 8, "endOffset": 712}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)).", "startOffset": 8, "endOffset": 733}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al.", "startOffset": 8, "endOffset": 893}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al.", "startOffset": 8, "endOffset": 926}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al.", "startOffset": 8, "endOffset": 959}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain.", "startOffset": 8, "endOffset": 987}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al.", "startOffset": 8, "endOffset": 1146}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al.", "startOffset": 8, "endOffset": 1170}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.", "startOffset": 8, "endOffset": 1219}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al.", "startOffset": 8, "endOffset": 1343}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al. (2014), however the variety of the label-space is limited to image text annotations.", "startOffset": 8, "endOffset": 1368}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al. (2014), however the variety of the label-space is limited to image text annotations. We analyze and mine the medical image semantics on both document and sentence levels, and deep CNNs of Jaderberg et al. (2014); Simonyan and Zisserman (2014) are adapted to learn them from image contents.", "startOffset": 8, "endOffset": 1573}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al. (2014), however the variety of the label-space is limited to image text annotations. We analyze and mine the medical image semantics on both document and sentence levels, and deep CNNs of Jaderberg et al. (2014); Simonyan and Zisserman (2014) are adapted to learn them from image contents.", "startOffset": 8, "endOffset": 1604}, {"referenceID": 1, "context": "Software package of Bird et al. (2009) was used for basic NLP pipelines.", "startOffset": 20, "endOffset": 39}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS.", "startOffset": 153, "endOffset": 172}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al.", "startOffset": 153, "endOffset": 267}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies.", "startOffset": 153, "endOffset": 294}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles.", "startOffset": 153, "endOffset": 860}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999).", "startOffset": 153, "endOffset": 1089}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al.", "startOffset": 153, "endOffset": 1157}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al.", "startOffset": 153, "endOffset": 1384}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al.", "startOffset": 153, "endOffset": 1471}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al.", "startOffset": 153, "endOffset": 1543}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al. (2006)).", "startOffset": 153, "endOffset": 1563}, {"referenceID": 3, "context": "A lower perplexity score generally implies a better fit of the model for a given document set (Blei et al. (2003)).", "startOffset": 95, "endOffset": 114}, {"referenceID": 3, "context": "A lower perplexity score generally implies a better fit of the model for a given document set (Blei et al. (2003)). Based on the perplexity score evaluated on 80% of the total documents used for training and 20% used for testing, the number of topics chosen is 80 for the document-level model using perplexity scores for model selection (Figure 2). Although the document distribution in the topic space is approximately balanced, the distribution of image counts for the topics is more unbalanced (Figure 3). Specifically, topic #77 (non-primary metastasis spreading across a variety of body parts) contains nearly half of the 216K key images. To address this data bias, sub-topics are obtained for each of the first document-level topics, resulting in 800 topics, where the number of the sub-topics is also chosen based on the average perplexity scores evaluated on each document-level topic. Lastly, to compare the method of using the whole report with using only the sentence directly describing the key images for latent topic mining, a sentence-level LDA topics are obtained based on three sentences only: the sentence mentioning the key-image (Figure 1) and its adjacent sentences as proximal context. The perplexity scores keep decreasing with an increasing number of topics; we choose the topic count to be 1000 as the rate of the perplexity score decrease is very small beyond that point (Figure 2). We observe that LDA-generated image categorization labels are valid, demonstrating good semantic coherence among clinician observers. The lists of key words and sampled images per topic label are subjected to board-certified radiologist\u2019s review and validation. Some examples of document-level topics with their corresponding images and topic key words are shown in Figure 4. Based on radiologists\u2019 review, our LDA topics discover semantics at different levels. There are 73 low-level concepts for example, pathology examination of certain body regions and organs: topic #47 - sinus diseases; #2 - lesions of solid abdominal organs, primarily kidney; #10 - pulmonary diseases; #13 - brain MRI; #19 - renal diseases on mixed imaging modalities; #36 - brain tumors. There are 7 mid- to high-level concepts, such as: topic #77 - non-primary metastasis spreading across a variety of body parts; topic #79 - cases with high diagnosis uncertainty or equivocation; #72 - indeterminate lesions; #74 - instrumentation artifacts limiting interpretation. Low-level topic images tend to be visually more coherent than the higher-level topic images. High-level topics may be analogous to the high-level visual concepts in natural images as was studied by Kiapour et al. (2014); Ordonez and Berg (2014).", "startOffset": 95, "endOffset": 2673}, {"referenceID": 3, "context": "A lower perplexity score generally implies a better fit of the model for a given document set (Blei et al. (2003)). Based on the perplexity score evaluated on 80% of the total documents used for training and 20% used for testing, the number of topics chosen is 80 for the document-level model using perplexity scores for model selection (Figure 2). Although the document distribution in the topic space is approximately balanced, the distribution of image counts for the topics is more unbalanced (Figure 3). Specifically, topic #77 (non-primary metastasis spreading across a variety of body parts) contains nearly half of the 216K key images. To address this data bias, sub-topics are obtained for each of the first document-level topics, resulting in 800 topics, where the number of the sub-topics is also chosen based on the average perplexity scores evaluated on each document-level topic. Lastly, to compare the method of using the whole report with using only the sentence directly describing the key images for latent topic mining, a sentence-level LDA topics are obtained based on three sentences only: the sentence mentioning the key-image (Figure 1) and its adjacent sentences as proximal context. The perplexity scores keep decreasing with an increasing number of topics; we choose the topic count to be 1000 as the rate of the perplexity score decrease is very small beyond that point (Figure 2). We observe that LDA-generated image categorization labels are valid, demonstrating good semantic coherence among clinician observers. The lists of key words and sampled images per topic label are subjected to board-certified radiologist\u2019s review and validation. Some examples of document-level topics with their corresponding images and topic key words are shown in Figure 4. Based on radiologists\u2019 review, our LDA topics discover semantics at different levels. There are 73 low-level concepts for example, pathology examination of certain body regions and organs: topic #47 - sinus diseases; #2 - lesions of solid abdominal organs, primarily kidney; #10 - pulmonary diseases; #13 - brain MRI; #19 - renal diseases on mixed imaging modalities; #36 - brain tumors. There are 7 mid- to high-level concepts, such as: topic #77 - non-primary metastasis spreading across a variety of body parts; topic #79 - cases with high diagnosis uncertainty or equivocation; #72 - indeterminate lesions; #74 - instrumentation artifacts limiting interpretation. Low-level topic images tend to be visually more coherent than the higher-level topic images. High-level topics may be analogous to the high-level visual concepts in natural images as was studied by Kiapour et al. (2014); Ordonez and Berg (2014). About half of the key images are associated with topic #77, implying that the clinicians\u2019 image referencing behavior patterns heavily focuses on metastatic patients.", "startOffset": 95, "endOffset": 2698}, {"referenceID": 19, "context": "Image to Document Topic Mapping with Deep Convolutional Neural Networks For each level of topics discussed in Section 3, we train deep CNNs to map the images into document categories using the Caffe framework of Jia et al. (2014). We split our whole key image dataset as follows: 85% used as the training dataset, 5% as the crossvalidation (CV), and 10% as the test dataset.", "startOffset": 212, "endOffset": 230}, {"referenceID": 22, "context": "1 Implementation All our CNN network settings are similar or the same as the ImageNet Challenge \u201cAlexNet\u201d (Krizhevsky et al. (2012)), and \u201cVGG-16 & 19\u201d (Simonyan and Zisserman (2014)) models.", "startOffset": 107, "endOffset": 132}, {"referenceID": 22, "context": "1 Implementation All our CNN network settings are similar or the same as the ImageNet Challenge \u201cAlexNet\u201d (Krizhevsky et al. (2012)), and \u201cVGG-16 & 19\u201d (Simonyan and Zisserman (2014)) models.", "startOffset": 107, "endOffset": 183}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al.", "startOffset": 53, "endOffset": 71}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al.", "startOffset": 53, "endOffset": 148}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer.", "startOffset": 53, "endOffset": 194}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer. The VGG variations of CNN models by Simonyan and Zisserman (2014) are significantly deeper by having 16\u223c19 convolutional layers and 133\u223c144 million parameters.", "startOffset": 53, "endOffset": 476}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer. The VGG variations of CNN models by Simonyan and Zisserman (2014) are significantly deeper by having 16\u223c19 convolutional layers and 133\u223c144 million parameters. The top-1 error rates on ImageNet dataset of these models are AlexNet:15.3% (Krizhevsky et al. (2012)); VGG-16:7.", "startOffset": 53, "endOffset": 672}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer. The VGG variations of CNN models by Simonyan and Zisserman (2014) are significantly deeper by having 16\u223c19 convolutional layers and 133\u223c144 million parameters. The top-1 error rates on ImageNet dataset of these models are AlexNet:15.3% (Krizhevsky et al. (2012)); VGG-16:7.4%; and VGG-19:7.3% (Simonyan and Zisserman (2014)), respectively.", "startOffset": 53, "endOffset": 734}, {"referenceID": 40, "context": "image dataset is about one fifth the size of ImageNet (Russakovsky et al. (2014)) and is the largest annotated medical image dataset to date.", "startOffset": 55, "endOffset": 81}, {"referenceID": 45, "context": "Then we follow the approach of Simonyan and Zisserman (2014) to crop the input images from 256 \u00d7 256 to 227\u00d7 227 for training.", "startOffset": 31, "endOffset": 61}, {"referenceID": 23, "context": "Table 3 provides the top-1 and top-5 testing in classification accuracies for each level of topic models using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) based deep CNN models.", "startOffset": 120, "endOffset": 145}, {"referenceID": 23, "context": "Table 3 provides the top-1 and top-5 testing in classification accuracies for each level of topic models using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) based deep CNN models.", "startOffset": 120, "endOffset": 192}, {"referenceID": 23, "context": "Table 3: Top-1, top-5 test classification accuracies for image to document-level topics, document-level sub-topics (document-level-h2) and sentence-level topics, using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) deep CNN models.", "startOffset": 177, "endOffset": 202}, {"referenceID": 23, "context": "Table 3: Top-1, top-5 test classification accuracies for image to document-level topics, document-level sub-topics (document-level-h2) and sentence-level topics, using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) deep CNN models.", "startOffset": 177, "endOffset": 249}, {"referenceID": 19, "context": "Table 4: Training times for the CNN models used to reach 70,000 iterations,and their memory consumption, using Caffe framework (Jia et al. (2014)) on NVidia Tesla K40 GPU.", "startOffset": 128, "endOffset": 146}, {"referenceID": 45, "context": "However, comparing VGG-16 and VGG19, three additional convolutional layers seem to have contributed to raise the top-5 accuracies by a small amount (\u223c2%), which is coherent with the results reported by Simonyan and Zisserman (2014) for object recognition task on the ImageNet dataset.", "startOffset": 202, "endOffset": 232}, {"referenceID": 45, "context": "Figure 8: Confusion matrices of (a) document-level topic, (b) document-level sub-topic (documentlevel-h2), and (c) sentence- level classification Simonyan and Zisserman (2014) ((b) and (c) can be viewed best in electronic version of this document).", "startOffset": 146, "endOffset": 176}, {"referenceID": 24, "context": "We therefore propose to generate relevant key-word text descriptions similar to Kulkarni et al. (2013), using deep language/image CNN models.", "startOffset": 80, "endOffset": 103}, {"referenceID": 40, "context": "(2010, 2008); Shin et al. (2013). Some examples of query words and their corresponding closest words in terms of cosine similarity for the word-tovector models (Mikolov et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 31, "context": "Some examples of query words and their corresponding closest words in terms of cosine similarity for the word-tovector models (Mikolov et al. (2013c)), trained on radiology reports only (total of ~1 billion words) and with additional OpenI articles (total of 1.", "startOffset": 127, "endOffset": 150}, {"referenceID": 42, "context": "To obtain only the disease-related terms, we exploit the human disease terms and their synonyms from the Disease-Ontology (DO; Schriml et al. (2012)), a collection of 8,707 unique disease-related terms.", "startOffset": 127, "endOffset": 149}, {"referenceID": 24, "context": "The bi-grams of DO disease-related terms in the vector representation of R256\u00d72 are somewhat analogous to the work of Kulkarni et al. (2013) detecting multiple objects of interest and describing their spatial configurations in the image caption.", "startOffset": 118, "endOffset": 141}, {"referenceID": 47, "context": "3 Image-to-Words Deep CNN Regression It has been shown by Sutskever et al. (2014) that deep recurrent neural networks (RNN) can learn the language representation for machine translation.", "startOffset": 58, "endOffset": 82}, {"referenceID": 45, "context": "This can be formulated as a regression CNN, replacing the softmax cost in Section 4 with the cross-entropy cost function for the last output layer of VGG-19 CNN model (Simonyan and Zisserman (2014)):", "startOffset": 168, "endOffset": 198}, {"referenceID": 44, "context": "We adopt the CNN model of Simonyan and Zisserman (2014) for the image-to-text representation since it works consistently better than the other relatively simpler model of Krizhevsky et al.", "startOffset": 26, "endOffset": 56}, {"referenceID": 23, "context": "We adopt the CNN model of Simonyan and Zisserman (2014) for the image-to-text representation since it works consistently better than the other relatively simpler model of Krizhevsky et al. (2012) in our image-to-topic mapping tasks.", "startOffset": 171, "endOffset": 196}, {"referenceID": 45, "context": "4 Key-Word Generation from Images and Discussion For any key image in testing, first we predict its topics of three levels (document-level, document-level sub-topics, sentence-level) using the three deep CNN models of Simonyan and Zisserman (2014) in Section 4.", "startOffset": 218, "endOffset": 248}, {"referenceID": 42, "context": "Bi-grams are selected from the image\u2019s reference sentences containing disease-related terms from the disease ontology (DO; Schriml et al. (2012)).", "startOffset": 123, "endOffset": 145}, {"referenceID": 19, "context": "We only report R@1 score on disease-related words compared to the previous works of Karpathy et al. (2014); Frome et al.", "startOffset": 84, "endOffset": 107}, {"referenceID": 10, "context": "(2014); Frome et al. (2013), where they report from R@1 up to R@20 on the entire image caption words (e.", "startOffset": 8, "endOffset": 28}, {"referenceID": 10, "context": "(2014); Frome et al. (2013), where they report from R@1 up to R@20 on the entire image caption words (e.g. R@1=0.16 on Flickr30K dataset by Karpathy et al. (2014)).", "startOffset": 8, "endOffset": 163}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al. (2014). Also for that reason, our generated image-to-text associations are not as exact as the generated descriptions by Frome et al.", "startOffset": 24, "endOffset": 68}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al. (2014). Also for that reason, our generated image-to-text associations are not as exact as the generated descriptions by Frome et al. (2013); Karpathy et al.", "startOffset": 24, "endOffset": 202}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al. (2014). Also for that reason, our generated image-to-text associations are not as exact as the generated descriptions by Frome et al. (2013); Karpathy et al. (2014).", "startOffset": 24, "endOffset": 226}, {"referenceID": 29, "context": "Finding and understanding the relations between the generated words will be the next step to explore, for example via more thorough text mining using sophisticated NLP parsing as by Li et al. (2011) and combining them with the specific frequent disease prediction in the next section.", "startOffset": 182, "endOffset": 199}, {"referenceID": 26, "context": "The Unified Medical Language System (UMLS) of Lindberg et al. (1993); Humphreys et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 15, "context": "(1993); Humphreys et al. (1998) integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and inter-operable biomedical information systems and services, including electronic health records.", "startOffset": 8, "endOffset": 32}, {"referenceID": 15, "context": "(1993); Humphreys et al. (1998) integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and inter-operable biomedical information systems and services, including electronic health records. It is a compendium of many controlled vocabularies in the biomedical sciences, created in 1986 and maintained by the National Library of Medicine. The Metathesaurus (Schuyler et al. (1993)) forms the base of the UMLS and comprises over 1 million biomedical concepts and 5 million concept names, where all of them are collected from the over 100 incorporated controlled vocabularies and classification systems.", "startOffset": 8, "endOffset": 466}, {"referenceID": 15, "context": "(1993); Humphreys et al. (1998) integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and inter-operable biomedical information systems and services, including electronic health records. It is a compendium of many controlled vocabularies in the biomedical sciences, created in 1986 and maintained by the National Library of Medicine. The Metathesaurus (Schuyler et al. (1993)) forms the base of the UMLS and comprises over 1 million biomedical concepts and 5 million concept names, where all of them are collected from the over 100 incorporated controlled vocabularies and classification systems. The Metathesaurus is organized by concept, where each concept has specific attributes defining its meaning and is linked to the corresponding concept names. The Metathesaurus has 133 semantic types that provide a consistent categorization of all concepts represented in it. Among the 133 semantic types we chose to focus on \u201cT033: finding\u201d and \u201cT047: disease or syndrome\u201d, as they seemed most relevant to be disease specific. Examples of some other semantic types we did not focus on this study are: \u201cT017: anatomical structure\u201d, \u201cT074: medical device\u201d, and \u201cT184: sign or symptom\u201d. RadLex (Langlotz (2006)) is a unified language to organize and retrieve radiology imaging reports and medical records.", "startOffset": 8, "endOffset": 1294}, {"referenceID": 7, "context": "We hope that this study will inspire and encourage other institutions in mining other large unannotated clinical databases, to achieve the goal of establishing a central training resource and performance benchmark for large-scale medical image research, similar to the ImageNet of Deng et al. (2009) for computer vision.", "startOffset": 281, "endOffset": 300}], "year": 2015, "abstractText": "Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital\u2019s Picture Archiving and Communication System. With natural language processing, we mine a collection of representative \u223c216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on documentand sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of largescale learning and prediction in electronic patient records available in most modern clinical institutions.", "creator": "LaTeX with hyperref package"}}}