{"id": "1505.00863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2015", "title": "A Feature-based Classification Technique for Answering Multi-choice World History Questions", "abstract": "Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.", "histories": [["v1", "Tue, 5 May 2015 02:06:23 GMT  (690kb)", "http://arxiv.org/abs/1505.00863v1", "5 pages, no figure"]], "COMMENTS": "5 pages, no figure", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["shuangyong song", "yao meng", "zhongguang zheng", "jun sun"], "accepted": false, "id": "1505.00863"}, "pdf": {"name": "1505.00863.pdf", "metadata": {"source": "CRF", "title": "A Feature-based Classification Technique for Answering Multi-choice World History Questions: FRDC_QA at NTCIR-11 QA-Lab Task", "authors": ["Shuangyong Song", "Yao Meng", "Zhongguang Zheng", "Jun Sun", "Huan Zhong"], "emails": ["sunjun}@cn.fujitsu.com"], "sections": [{"heading": null, "text": "Submission of the NTCIR-11. In this paper, we describe our system for solving real-life university entrance exam questions related to world history. Wikipedia is used as the most important external resource for our system. As problems in selecting the right / wrong set of multiple-sentence decisions account for about two thirds of the total number, we individually design a classification-based model for solving these types of questions. For other types of questions, we also design some simple methods.Teamname FRDC _ QASubtask QA-Lab English SubtaskKeywords Question Answering, The National Center Test for University Admissions, World History, Feature Extraction, Classification Model."}, {"heading": "1. INTRODUCTION", "text": "The QA systems are concerned with providing relevant answers to questions proposed in natural language. QA therefore consists of three different modules: question classification, information gathering and answer extraction, each of which has a core component among other complementary components [7]. Question classification plays an essential role in QA systems by classifying the submitted question by its nature. In particular, solving real-world exam questions is an important and useful application of QA systems, and some research has been carried out on this task [1, 8-10]. The task NTCIR-11 QALab aims to provide a module-based platform for system performance assessments and comparisons to solve real-life university entrance examination questions selected from the National Center Test for University Admissions and from secondary examinations at 4 universities in Japan.FRDC _ QA."}, {"heading": "2. HASH MAP & LUCENE INDEXES OF EXTERNAL RESOURCE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 External Resource", "text": "We use Wikipedia as an external resource for our QA Lab task. Wikipedia is a well-known, freely available, multilingual encyclopedia co-authored by collaborators from around the world. [6] In this essay, we upload the Wikipedia record containing the version of \"enwiki dump progress on 20140502\" from Wikimedia Downloads 1. The downloaded record contains \"enwiki-20140502-all-title\" as a list of all Wiki articles and \"enwiki-20140502-pages-articles\" with articles from all Wiki articles. All of this data is formally processed and then stored in hash maps or Lucene2 indexes for convenient and quick searches in our QA system. Details are listed below."}, {"heading": "2.2 Hash Map of Item Title", "text": "To quickly check whether a word or phrase is a Wikipedia element, we put all the titles of Wikipedia elements into a hash map. The record of the track list contains a total of 32,877,103 titles of Wikipedia elements, and we convert all the characters from them to lowercase letters. Word or word phrases are also converted to lowercase letters when checked for exact match. If we detect elements contained in a sentence, we use a maximum matching method. For example, for a sentence of N words, we first check whether that whole sentence is a wiki element, and then check all subsets with N-1 consecutive words, then subsets with N-2 length, and so on. In particular, if a found element consists of another found element, the last one is removed and the longer one is reserved."}, {"heading": "2.3 Lucene Index of Item Page", "text": "We put the title and page contents as two Word string fields in a Lucene index file and then easily get the description of a wiki article with a simple Lucene search. This index file is used to search for the relationship between elements, since two related elements in the wiki article are shown to each other in reverse, http: / / download.wikipedia.com / enwiki / 20140502 2 2 http: / / lucene.apache.org"}, {"heading": "2.4 Lucene Index of Item Redirection", "text": "Different wiki entries can have the same meanings, such as \"AccessibleComputing\" and \"Computer Accessibility.\" For these entries with the same meanings, Wikipedia uses the redirect tag to associate one of them with another. Therefore, only one of them has a wiki article with a detailed description, and the wiki article of another entry contains only one sentence with redirect explanation. Take, for example, \"AccessibleComputing\" and \"Computer Accessibility\": The wiki article \"Computer Accessibility\" contains a detailed description of this entry, but the wiki article \"AccessibleComputing\" contains only one sentence \"Redirect Page Computer Accessibility.\" We then put these \"AccessibleComputing\" entries, such as redirected wiki entries, in a lucid index file as a word string field and take the associated entries as another word string field, then we can easily search through the actual description of these redirected wiki entries."}, {"heading": "2.5 Lucene Index of Item Time", "text": "There are two different types of time information that we should extract, one is the exact time of that point, and another is the period of that point. For example, Independence Day (United States) is the \"Fourth of July,\" and the period of the \"French and Indian War\" is \"1754-1763.\" For the period type, we note the front part as \"start time\" and the last part as \"end time,\" as many questions can ask them separately."}, {"heading": "3. FRAMEWORK FOR QUESTIONS WITH MULTIPLE SENTENCE CHOICES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Brief Description of Framework", "text": "First, we introduce the type of questions with multiple-sentence decisions, and an example is given below (italics): Background text:... (8) In India, a large-scale rebellion against colonial rule occurred in the second half of the 19th century; one of the triggers for this was the fact that Muslim soldiers revolted because of a rumor that pig fat had been used on the cartridges of their rifles.... Question: Choose below from 1-4 the most appropriate sentence regarding the underlined part (8). Selection: 1. This rebellion is also called the Sipahi (Sepoy) mutiny. 2. The Ever Victorious Army was actively involved in the suppression of this rebellion. 3. The Ever Victorious Army was actively involved in the suppression of this rebellion. 4. After this rebellion, Queen Victoria also became Empress of the Mughal Empire. We take this type of questions as an accuracy ranking probability ranking problem for all decisions, and we use classification models to handle this problem."}, {"heading": "3.2 Features", "text": "Dei eeisrrVnreeeegln rf\u00fc ide rf\u00fc ide rf\u00fc ide eeirteeVnlrrrrteeeeeeeVnlrrrteeeeeirteeVrte\u00fcgn rf\u00fc ide eeirteeeeeeVnlrrrteeeeeeeeeeeeeeVnlrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeVnlrrrrrrrteeeeeeeeeeeeeerrrrrrlrlrrlrrrlrrlrrlrrrlrrlrrlrlrrlrrlrrlrrrlrlrlrrrrlrlrlrrlrlrlrrrlrlrrlrlrlrlrrrlrlrlrlrteeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeteerrrrrrrrrrrrrln"}, {"heading": "3.3 Classifiers", "text": "In this case, it is just one of several options to look at."}, {"heading": "3.4 Choice Selection", "text": "Each classifier can obtain an accuracy probability for each election, and the average value of the accuracy probability of all classifiers is assumed to be the final accuracy probability of an election. If the question then asks us to choose the right choice using the keywords \"right,\" \"right,\" or \"appropriate,\" we choose the choice with the highest accuracy probability as the final answer. If the question asks us to choose the wrong choice using the keywords \"wrong,\" \"wrong,\" or \"error,\" we choose the choice with the lowest accuracy probability as the final answer."}, {"heading": "4. FRAMEWORKS FOR OTHER TYPES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "OF QUESTIONS", "text": "In this section we give some descriptions of our frameworks for problems besides the type with multiple-sentence selection, such as questions with chronological sequence selection, questions with term selection, etc."}, {"heading": "4.1 Framework for Questions with Chronological Sequence Choices (without images)", "text": "We will first give an example of these kinds of questions (italics): Background text:... (7) A Cold War began between the US and the Soviet Union, and the world was facing another serious conflict.... Question: Regarding the underlined section (7) select (1) - (4) below the correct chronological sequence of events related to the Cold War. Decisions: 1. Organization of the Warsaw Treaty founded - Berlin Blockade - Cuba Crisis - Japan-US Security Treaty signed (1951) 2. Berlin Blockade - Japan-US Security Treaty signed (1951) -Organization of the Warsaw Treaty founded - Cuba Crisis (1951) - Cuba Crisis - Cuba Crisis - Berlin Blockade - Organization of the Warsaw Treaty founded (1951) 4. Berlin Blockade - Organization of the Warsaw Treaty founded - Japan-US Security Treaty signed (1951) - Cuban Missile Crisis For these kinds of questions we use the Lucene Index Item Time to determine the sequence of events and then the chronological order of events."}, {"heading": "4.2 Framework for Questions with Term Choices (without images)", "text": "An example of this type of question is given below (italics): Background text:... (1) On the Eurasian continent, nomadic tribes arose on the backs of horses. Their elusive character became a major threat to settled agricultural societies, so that troops were organized on horses to counter them. Rulers also arose looking for good horses, such as...... Question: Regarding the underlined part (1), choose below from 1-4 the one name that correctly describes the nomadic tribe on the horse that gained importance in the 6th century and built a nation. Selection: 1. Scythians 2. G\u00f6kt\u00fcrks 3. Yuezhi 4. XiongnuWe recognize objects contained in the background text and the question using the maximum matching method, and then calculate the relativity between these objects and the selected object using the same method described in Section 3.2. Finally, the choice with the highest relativity and the background text of the final answer is selected."}, {"heading": "4.3 Framework for Questions with Judging True or False Sentences (without images)", "text": "An example of this type of question is given below (italics): Background text:... (3) Founder of the Kingdom - is assumed to appear in the \"Book of Wei (Weishu),\" which is a record of the Northern Wei Dynasty... Question: Regarding the underlined part (3), from 1-4 below, choose the right combination of \"right\" and \"wrong\" with respect to the following sentences Taa and b concerning the historical founder of the Kingdom. Question text: a Liu Bang defeated Xiang Yu and made Chang'an the capital city. b Yel\u00fc Dashi built the Kara Khitan Khanate.Choices: 1. a - Correction b - Correction 2. a - Correction b - Correction 3. a - False 3. a - False b - Correction 4. a - CorrectWe use the same training data with the same features as described in Section 3.2 to train vector machines."}, {"heading": "4.4 Framework for Other types of Questions", "text": "We select the final answer using the random selection method for other types of questions that normally require image analysis technology. In particular, we set a set random starting value to ensure the stability of the results given by our system."}, {"heading": "5. EVALUATION RESULTS", "text": "Table 1 gives the evaluation results of our system using the competitive data from the Phase 1 World History Exam B in 2007 at Japan University Admissions. For the types of \"multiple sentence selection questions\" and \"term selection questions (excluding pictures),\" we achieve an accuracy of about 45% in both the \"number of correct answers\" and the \"number of correct answers,\" which shows the much better effectiveness than random method, as we believe that the precision of the random method should be 25% for quadruple questions. However, the actual result of the random method for \"other types of questions\" is not as good as we thought. We received incorrect answers to all seven \"other types of questions\" using the random method, which makes our overall result a precision of 37%, well below the 45%."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "In our work on the NTCIR-11 QA-Lab task, we are designing a system for solving real university entrance exam questions related to world history. We are using Wikipedia as the most important external resource for our system, since almost all of the world history knowledge can be found in Wikipedia. In addition, we are designing different solution frameworks for different types of questions, such as questions with multiple selection, questions with time selection options, questions with non-time selection options, etc. Although our system works much better than random methods, it is still far from meeting actual needs. Several attempts can be made to improve system performance in our future work, e.g. (1) more useful external resources can be used, such as query results from Google such as search engines, electronic history books, etc. (2) more reasonable and intelligent combination methods for different classification models should be tried; (3) different ways of writing time stamps, places, and personal studies should be considered in addition."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "We sincerely thank our colleagues from Fujitsu Laboratories Ltd., Takuya Makino, Hiroko Suzuki, Tomoya Iwakura and Tetsuro Takahashi for their help in providing external training data and valuable discussions with us about feature extraction and machine learning methods."}, {"heading": "8. REFERENCES", "text": "[1] Kano, Y. 2014. Solving History Problems of the National Center Test for University Admissions 123. In Proceedings ofthe 28th Annual Conference of the Japanese Society for Artificial Intelligence (Lyon, France, April 16-20, 2012), pp. 605-606. [3] Breiman, L. 2001. Random forests. Machine Learning.45 (1): In Proceedings of the 21st International World Wide Web Conference (Lyon, France, April 16-20, 2012). [4] Friedman, J., Hastie, T. and Tibshirani, R. 2000."}], "references": [{"title": "Solving History Problems of the National Center Test for University Admissions", "author": ["Y. Kano"], "venue": "In Proceedings of  the 28th Annual Conference of the Japanese Society for Artificial Intelligence", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Detecting Dynamic Association among Twitter Topics", "author": ["S. Song", "Q. Li", "H. Bao"], "venue": "In Proceedings of the 21st International World Wide Web Conference", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Additive logistic regression: a statistical view of boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations. Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Extracting Named Entities and Relating Them over Time Based on Wikipedia", "author": ["A. Bhole", "B. Fortuna", "M. Grobelnik", "D. Mladeni\u0107"], "venue": "Informatica (Slovenia)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "The Question Answering Systems: A Survey", "author": ["A.M.N. Allam", "M.H. Haggag"], "venue": "International Journal of Research and Reviews in Information Sciences", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Answering multiple-choice questions in high-stakes medical examinations", "author": ["F. Martin R", "H. Sibyl", "K. Veronika"], "venue": "Medical Education,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Web-Based multiple choice question answering for english and arabic questions", "author": ["R. Awadallah", "A. Rauber"], "venue": "In Proceeding of the 28th European conference on Advances in Information Retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Another kind of \u2018BOLD Response\u2019: answering multiple-choice questions via online decoded single-trial brain signals", "author": ["B. Sorger", "B. Dahmen", "J. Reithler", "O. Gosseries", "A. Maudoux", "S. Laureys", "R. Goebel"], "venue": "Progress in Brain Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Logistic Model Trees", "author": ["N. Landwehr", "M.A. Hall", "E. Frank"], "venue": "In Proceeding of the 14th European Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "How to Make AdaBoost.M1 Work for Weak Base Classifiers by Changing Only One Line of the Code", "author": ["G. Eibl", "K.P. Pfeiffer"], "venue": "In Proceeding of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "MultiBoosting: A Technique for Combining Boosting and Wagging", "author": ["G.I. Webb"], "venue": "Machine Learning. Vol.40,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Locally Weighted Learning", "author": ["C.G. Atkeson", "A.W. Moore", "S. Schaal"], "venue": "Artificial Intelligence Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop", "N.M. Nasrabadi"], "venue": "Journal of Electronic Imaging,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Pattern Classification and Scene Analysis", "author": ["Duda R", "P. Hart"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1973}, {"title": "Estimating Continuous Distributions in Bayesian Classifiers", "author": ["G.H. John", "P. Langley"], "venue": "In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}], "referenceMentions": [{"referenceID": 5, "context": "QA is therefore composed of three distinct modules: question classification, information retrieval, and answer extraction, each of which has a core component beside other supplementary components [7].", "startOffset": 196, "endOffset": 199}, {"referenceID": 0, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 6, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 7, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 8, "context": "In particular, solving real-world school exam questions is an important and useful application of QA systems, and some research has been done on this task [1, 8-10].", "startOffset": 155, "endOffset": 164}, {"referenceID": 4, "context": "Wikipedia is a well-known free content, multilingual encyclopedia written collaboratively by contributors around the world [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "This formula is modified from the Euclidean Distance [2], without firstly creating the vector of words in all Wiki articles and choices, which is very time consuming and with low robustness.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 9, "context": "Logistic Model Trees: Logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning [11].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "AdaBoost M1: AdaBoost M1 is an improved version of traditional AdaBoost algorithm, which can be used to classify both binary and polynominal label with numerical, binominal and polynominal (and weighted) attributes [12].", "startOffset": 215, "endOffset": 219}, {"referenceID": 11, "context": "It is able to harness both AdaBoost's high bias and variance reduction with wagging's superior variance reduction [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "using linear regression) [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Simple Na\u00efve Bayes: Naive Bayes classifier, in which the numeric attributes are modelled by a normal distribution [17].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Numeric estimator precision values are chosen based on analysis of the training data [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "1 for numeric attributes when build Classifier is called with zero training instances [18].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "All of the classification model training procedures are realized with WEKA [5], which is a collection of machine learning algorithms for data mining tasks and contains tools for data preprocessing, classification, regression, clustering, association rules, and visualization.", "startOffset": 75, "endOffset": 78}], "year": 2015, "abstractText": "Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.", "creator": "Microsoft\u00ae Word 2013"}}}