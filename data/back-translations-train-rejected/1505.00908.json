{"id": "1505.00908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2015", "title": "Reinforced Decision Trees", "abstract": "In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen \\textit{by hand}, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm.", "histories": [["v1", "Tue, 5 May 2015 07:58:40 GMT  (714kb,D)", "http://arxiv.org/abs/1505.00908v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aur\\'elia l\\'eon", "ludovic denoyer"], "accepted": false, "id": "1505.00908"}, "pdf": {"name": "1505.00908.pdf", "metadata": {"source": "CRF", "title": "Reinforced Decision Trees", "authors": ["Aur\u00e9lia L\u00e9on", "Ludovic Denoyer"], "emails": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"], "sections": [{"heading": null, "text": "Keywords: reinforcement learning, machine learning, policy gradients, decision trees, classification"}, {"heading": "1. Introduction", "text": "The complexity of classification is usually highly related and typically linear w.r.t denotes the number of possible categories C. When confronted with a very large number of classes, such as text classification in large ontologies, object recognition or word prediction in deep language models, this becomes a critical point that makes classification methods inefficient. Thus, there is a need to develop new methods that are predicted in large production areas at low cost. Several methods have recently been developed for reducing the classification speed. They are based on the idea of organizing the possible results and allow to reduce the inference complexity."}, {"heading": "2. Notations and Model", "text": "We will look at the multi-class classification problem, where each input x-Rn must be associated with one of the possible C categories. Let us call y the designation of x, y-RC such that yi = 1 if x belongs to class i and yi = \u2212 1 elsewhere. We will call {(x1, y1),..., (xN, yN)} the set of N training examples."}, {"heading": "2.1 Reinforced Decision Trees Architecture", "text": "The architecture of the reinforced decision tree (RDT) shares common points with the decision trees. So let us designate T\u03b8, \u03b1 such a tree with the parameters \u03b8 and \u03b1 that will be defined later: \u2022 The tree consists of a series of nodes called nodes (T\u03b8, \u03b1) = {n1,..., nT}, where T is the number of nodes of the tree \u2022 n1 is the root of the tree. \u2022 Parents (ni) = nj means that node nj is the parent of ni. Each node, but n1 has only one parent, n1 has no parent because it is the root of the tree. 1. The model, of course, deals with multi-marker classification problems that are not detailed here for simplicity reasons. Forced decision trees \u2022 Leaf (ni) = true if ni is a leaf of the tree that is a leaf of the tree. Note that we have no limitations on the number of leaves or topology of the tree."}, {"heading": "2.2 Inference Process", "text": "Let us designate H as a path, H = (n (1),..., n (t), where n (i), (i), (i), (i), (i), (i), is the index of the i-th node of the path. H is thus a sequence of nodes where n (1) = n1, (i), (i), n (i \u2212 1) = parents (n (i) and sheet (n (t) = true.Algorithm 1 Stochastic course RDT Learning Procedure1: Procedure Learning (x1), (xN, yN) = parents (n).The learning set 2: is the learning rate 3: \u03b1, the random 4: Proceduretion 5: Repeat 6: i, uniform (1, N) 7: Example H = (1),..., n (t), (t), which is the learning ability, (xN, yN)."}, {"heading": "2.3 Learning Process", "text": "D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D:"}, {"heading": "2.4 Discussion", "text": "Inference complexity: The complexity of the inference process is linear with the depth of the tree. Typically, in a multi-class classification problem, the depth of the tree will be proportional to logC, resulting in a very fast inference process similar to that achieved with, for example, hierarchical SoftMax moduls.Learning Complexity: The policy gradient algorithm developed in this paper is an iterative gradient-based method. Any learning iteration complexity is O (N logC), but the number of iterations required is not known. Furthermore, the optimization problem is clearly not a convex problem, and the system can get stuck in a local minimum. As explained in Section 3, one way to avoid a problematic local minimum is to select a number of sheets higher than the number of categories, which gives modeling more degrees of freedom."}, {"heading": "3. Preliminary Experiments", "text": "In this section, we offer a series of experiments made on toy data sets to better understand the ability of RDT to make good predictions and discover a relevant hierarchy.3 Our model was compared with the same model, but where the categories associated with the leaves (the \u03b1i values) were randomly selected, with each leaf being assigned to a possible category - i.e., each vector \u03b1i is full of \u2212 1 with a 1 value at a random position - this model is called the Random Tree. In this case, the \u03b1 parameters are not updated during gradient descent. Hyperparameters (learning rate and number of iterations) were matched by cross-validation, and the results were averaged to five different runs. We also have comparisons with a linear SVM (one against all) and decisions Trees4. These preliminary experiments aim to show the ability of our integrated approach to link categories with scrolling the tread."}, {"heading": "4. Related Work", "text": "For multi-level classification problems, the classic approach is to train one-on-one classifiers. It is one of the most efficient techniques (Jr and Freitas; Babbar and Partalas, 2013), even with a very large number of classes, but the inference of complexity is linear; the number of possible categories leads to slow prediction algorithms. Hierarchical models have been proposed to reduce this complexity, and have been developed for two cases: (i) a first where a hierarchy of categories is already known; in this case, the hierarchy of classifiers is mapped to the hierarchy of classes. (ii) A second approach, closer to ours, is to automatically build a hierarchy from the training set. This is usually done as a first step, using cluster techniques such as spectral clustering on the confusion matrix (Bengio et al)."}, {"heading": "5. Conclusion and Perspectives", "text": "RDTs are sequential decision models where prediction is made via an input using O (logC) classifiers, making this method suitable for problems with a large number of categories. Furthermore, the method can easily be adapted to any learning problem such as regression or ranking by changing the loss function. RDTs are learned by using a method inspired by political gradients. Preliminary results show the effectiveness of this approach. Future work will mainly involve experiments with reinforced decision trees in the real world, but also the extension of this model to continuous output problems."}], "references": [{"title": "On Flat versus Hierarchical Classification in Large-Scale Taxonomies", "author": ["Rohit Babbar", "Ioannis Partalas"], "venue": "Neural Inf. Process. Syst., pages", "citeRegEx": "Babbar and Partalas.,? \\Q2013\\E", "shortCiteRegEx": "Babbar and Partalas.", "year": 2013}, {"title": "Logarithmic Time Online Multiclass prediction", "author": ["Anna Choromanska", "John Langford"], "venue": null, "citeRegEx": "Choromanska and Langford.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska and Langford.", "year": 2014}, {"title": "Deep sequential neural network. arXiv preprint arXiv:1410.0510 - Workshop Deep Learning", "author": ["Ludovic Denoyer", "Patrick Gallinari"], "venue": null, "citeRegEx": "Denoyer and Gallinari.,? \\Q2014\\E", "shortCiteRegEx": "Denoyer and Gallinari.", "year": 2014}, {"title": "Sequentially generated instance-dependent image representations for classification", "author": ["Gabriel Dulac-Arnold", "Ludovic Denoyer", "Nicolas Thome", "Matthieu Cord", "Patrick Gallinari"], "venue": "Internation Conference on Learning Representations - ICLR 2014,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "CoRR, abs/1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Probabilistic label trees for efficient large scale image classification", "author": ["Baoyuan Liu", "Fereshteh Sadeghi", "Marshall Tappen", "Ohad Shamir", "Ce Liu"], "venue": "Comput. Vis. Pattern Recognit.,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Sparse output coding for large-scale visual recognition", "author": ["Bin Zhao", "Eric P. Xing"], "venue": "Comput. Vis. Pattern Recognit., pages 3350\u20133357,", "citeRegEx": "Zhao and Xing.,? \\Q2013\\E", "shortCiteRegEx": "Zhao and Xing.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "But these approaches suffer from one major drawback: the structure used for prediction (error correcting codes, or tree) is usually built during a preliminary step - before learning the classifier - following hand-made heuristics, typically by using clustering algorithms or Huffman codes (Le and Mikolov, 2014).", "startOffset": 289, "endOffset": 311}, {"referenceID": 2, "context": "The difference with the classical Reinforcement Learning context is that the feedback provided to the system is a derivable loss function as proposed in (Denoyer and Gallinari, 2014) which gives more information than a reward signal, and allows fast learning of the parameters.", "startOffset": 153, "endOffset": 182}, {"referenceID": 2, "context": "g square loss or hinge loss)2 Our learning algorithm is based on an extension of policy gradient techniques inspired from the Reinforcement Learning literature and similar to Denoyer and Gallinari (2014). More precisely, our learning method is close to the methods proposed in Baxter and Bartlett (1999) with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206.", "startOffset": 175, "endOffset": 204}, {"referenceID": 2, "context": "g square loss or hinge loss)2 Our learning algorithm is based on an extension of policy gradient techniques inspired from the Reinforcement Learning literature and similar to Denoyer and Gallinari (2014). More precisely, our learning method is close to the methods proposed in Baxter and Bartlett (1999) with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206.", "startOffset": 175, "endOffset": 304}, {"referenceID": 0, "context": "It is one of the most efficient technique (Jr and Freitas; Babbar and Partalas, 2013) even with a very large number of classes, but the inference complexity is linear w.", "startOffset": 42, "endOffset": 85}, {"referenceID": 6, "context": "), using probabilistic label tree (Liu et al., 2013) or even partitioning optimization (Weston et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 7, "context": "), sparse coding (Zhao and Xing, 2013) or even using representation learning techniques, representations of categories being obtained by unsupervised models (Weinberger and Chapelle; Bengio et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 3, "context": ") or image classification (Dulac-Arnold et al., 2014; Gregor et al., 2015).", "startOffset": 26, "endOffset": 74}, {"referenceID": 4, "context": ") or image classification (Dulac-Arnold et al., 2014; Gregor et al., 2015).", "startOffset": 26, "endOffset": 74}, {"referenceID": 1, "context": "The closest work is perhaps Choromanska and Langford (2014) which discovers the hierarchy using online learning algorithms, the construction of the tree being made during learning.", "startOffset": 28, "endOffset": 60}], "year": 2015, "abstractText": "In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen by hand, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm.", "creator": "LaTeX with hyperref package"}}}