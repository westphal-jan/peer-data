{"id": "1505.01576", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Learning and Optimization with Submodular Functions", "abstract": "In many naturally occurring optimization problems one needs to ensure that the definition of the optimization problem lends itself to solutions that are tractable to compute. In cases where exact solutions cannot be computed tractably, it is beneficial to have strong guarantees on the tractable approximate solutions. In order operate under these criterion most optimization problems are cast under the umbrella of convexity or submodularity. In this report we will study design and optimization over a common class of functions called submodular functions. Set functions, and specifically submodular set functions, characterize a wide variety of naturally occurring optimization problems, and the property of submodularity of set functions has deep theoretical consequences with wide ranging applications. Informally, the property of submodularity of set functions concerns the intuitive \"principle of diminishing returns. This property states that adding an element to a smaller set has more value than adding it to a larger set. Common examples of submodular monotone functions are entropies, concave functions of cardinality, and matroid rank functions; non-monotone examples include graph cuts, network flows, and mutual information.", "histories": [["v1", "Thu, 7 May 2015 04:04:02 GMT  (271kb,D)", "http://arxiv.org/abs/1505.01576v1", "Tech Report - USC Computer Science CS-599, Convex and Combinatorial Optimization"]], "COMMENTS": "Tech Report - USC Computer Science CS-599, Convex and Combinatorial Optimization", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bharath sankaran", "marjan ghazvininejad", "xinran he", "david kale", "liron cohen"], "accepted": false, "id": "1505.01576"}, "pdf": {"name": "1505.01576.pdf", "metadata": {"source": "CRF", "title": "Learning and Optimization with Submodular Functions", "authors": ["Bharath Sankaran", "Marjan Ghazvininejad", "Xinran He", "David Kale", "Liron Cohen"], "emails": ["bsankara@usc.edu", "mghazvin@usc.edu", "xinranhe@usc.edu", "dkale@usc.edu", "lironcoh@usc.edu"], "sections": [{"heading": null, "text": "Learning and optimization with submodular functionsBharath Sankaran, Marjan Ghazvininejad, Xinran He, David Kale, Liron Cohen Department of Computer Science University of Southern California Los Angeles, CA 90034 {bsankara, mghazvin, xinranhe, dkale, lironcoh} @ usc.edu"}, {"heading": "1 Motivation", "text": "In many naturally occurring optimization problems, it is important to ensure that the definition of the optimization problem is appropriate for solutions that can be calculated. In cases where exact solutions cannot be calculated tractably, it is advantageous to have strong guarantees for tractable approximation solutions. In order to operate under this criterion, most optimization problems are summarized under the umbrella of convexity or submodularity. In this report, we examine design and optimization through a common class of functions called submodular functions.Quantity functions, and in particular submodular quantity functions, characterize a variety of naturally occurring optimization problems, and the property of submodularity of set functions has profound theoretical consequences with wide-ranging applications.Informally, the property of the submodularity of set functions relates to the intuitive principle of matularity reduction of yields, and the property of submodularity functions, which means adding a larger number of cardinality functions, rather than adding a larger number of cardinality functions."}, {"heading": "2 What is Submodularity: Formal Definition", "text": "We define submodularity as a property of the set functions f: 2V \u2192 R, which assign a value f (S) to each subset S V. Here, V is a finite set designated as a base set. We also assume that f (\u2205) = 0.Definition 1 A set function f: 2V \u2192 R is called submodular when it evaluates f (X) + f (Y) \u2265 f (X-U) + f (X-Y) \u0445 X, Y VThe function f is suitable for various forms in different fields of application. In a machine learning context, f could be a function that evaluates information of a given set, i.e. entropy. With this notion, we can easily introduce the property of reduction in return by using an equivalent definition of submodularity. Definition 2 A set function f: 2V \u2192 R is called a submodular function when it is called X-k (X-k) \u2212 f (X-k) and finally, X-k (superdular)."}, {"heading": "2.1 Notation I", "text": "In this section, we will introduce some notation, which we will keep consistently throughout this document, unless otherwise specified. The basic set by which the submodular functions are defined is referred to by ar Xiv: 150 5.01 576v 1 [cs.L G] 7M ay2 015V with cardinality n. For a vector x-RV and a subset Y V, we define x (Y) = \u2211 u-Y x (u). We can, of course, expand this definition to include the positive and negative parts of the vector x as x + RV and x \u2212 RV, where x + (u) = max {x (u), 0} and x \u2212 (u) = min {x (u), 0}. For a submodular function f, we define a convex set P (f) as the submodular polyhedron base (f)."}, {"heading": "2.2 Properties of Submodular Functions", "text": "These properties will help us to redefine many of our optimization goals as submodular optimization problems. \u2022 Lemma 2.1 (Closeness properties): Submodular functions are closed under non-negative linear combinations, i.e. if {f1, f2,..., fk} are submodular, then the function g (X) = k \u00b2 i = 1 \u03b1ifi (X) is submodular combinations. \u2022 Corollary 1.1: The sum of a modular and submodular function is a submodular function.Corollary 1.2: (Restriction / Marginalization): if Y \u00b2 V, then X \u00b2 Y) is submodular property on V and Y \u00b2.Corollary 1.3: (Contraction / Conditioning): (Miniature / Conditioning): If X \u00b2 n is modular function, then g (Y \u00b2 X) is submodular function."}, {"heading": "3 Submodular Optimization", "text": "Submodular functions have many interesting connections with convex and concave functions, as Lemma 3 has shown. Just as the minimization of convex functions can be performed efficiently, unlimited submodular minimization is also possible in strongly polynomial time. Submodular function maximization, on the other hand, is a combinatorial NP problem, but approximate solutions can be found with guarantees. In fact, a simple, greedy solution method gets a (1 \u2212 1 / e) approximation, as we maximize an undiminished submodular function under matroid constraints."}, {"heading": "3.1 Submodular Function Minimization", "text": "Submodular function minimization can be divided into two categories, exact and approximate algorithms. Exact algorithms get global minimizers for a problem, whereas approximate algorithms only achieve an approximate solution, i.e. for a set of X the solution f (X) \u2212 min Y \u0445 V f (Y) \u2264, where it is as small as possible. If the minimum absolute difference between unequal values of f is smaller, the calculated solution corresponds to the exact solution. An important practical aspect of submodular function minimization is that most algorithms have online approximation guarantees based on a duality relationship, which we will detail in the following subsections."}, {"heading": "3.1.1 Submodular Function Minimizers", "text": "For the Lemmata below we consider f = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V. \u2022 Lemma 3.1 (Minimizer for V = V = V and Y = X = Y = V = V = V and X = V = V = X = V = V = V = V = V = V and X = V = V = V = V = V = V = V = V = V = V = V and X = V = V = V = V = V = V = V and X = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V = V ="}, {"heading": "3.1.2 Minimum Norm Point Algorithm", "text": "A non-combinatorial approach proposed by Fujishige to minimize the L2 standard of a vector in a convex hull of a finite set of dots P-Rn, in this method the vector x is maintained as a convex combination of dots S and iterated in the following steps: 1. A new point of P with a norm in relation to x is projected in the affine hull of p. 2. A point with the minimum norm x is projected in the affine hull of p. 3. The minimum norm point x is projected onto the convex hull of p.: The minimum norm point x is projected onto the convex hull of p. 3: The minimum norm point x is projected onto the convex hull of p. In the case of submodular functions, one must search through the set of all bases P, which is exponentially in size."}, {"heading": "3.2 Submodular Function Maximization", "text": "Problems of this kind are known to be NP-hard. Feige and Mirrokni [4] showed that the maximization of non-negative submodular functions by a random subset of at least 1 / 4 of the optimal value and local search techniques at least 1 / 2. Although these problems are NP-hard, a (1 \u2212 1 / e) approximation can be achieved by maximizing a non-decreasing submodular function under matroid constraints. Vondrak et al. [5] recently demonstrated the solution of the Arctic matroid constraint. Nemhauser [6] showed the initial result of a (1 \u2212 1 / e) approximation in the 1970s, but the result was only applicable to uniform matroid (cardinal) constraints."}, {"heading": "3.2.1 Greedy Algorithm for Monotone Submodular Function Maximization with Uniform Matroid Constraints", "text": "Note: Maximization can also be formulated with the base polyhedron, since we have f and its lova \u0301 sz extension f. In this case, maximization is tantamount to finding the maximum l1 standard point in the base polyhedron. See [7] for more details. For the monotonous submodular maximization, which is subject to uniform matroid constraints, we must find a fixed X-V limit, so that X * = argmax \u0445 X standard point f (X) is where n is the cardinality (uniform matroid) constraint. Although this problem is NP-hard, we can get an approximate solution with an approximation of (1 \u2212 1 / e) to the optimal solution. The algorithm for achieving this solution is shown in Algorithm 2.Algorithm 2: Greedy for algorithm: 1: 2 starting point = maxy = 2."}, {"heading": "3.3 Adaptive Submodularity", "text": "The process of adaptive decision-making with uncertain outcomes is fundamental to many problems of partial observability. In such situations, the decision-maker must make a sequence of decisions, taking into account and adapting accordingly earlier observations. Golovin and Krause have shown [8] that when a problem is adaptively submodular, an adaptive greedy algorithm is guaranteed to obtain approximately optimal solutions. To clarify the notion of adaptive submodularity and adaptive monotony in the discussion of the scope, we will introduce in this section."}, {"heading": "3.3.1 Preliminaries and Notation II", "text": "If we assume that each element of the set x-V can take a number of states from a series of possible states, then we represent item states as such characterized by the random variable B. Then we can proceed from a previous probability distribution over realities as p. In cases where we observe only one realization, item (x) is at a time when we select an item x-V at a time; we can represent our observations so far with a partial realization, i.e a function of a subset of V and their observed states. Hence-V-O is (x, o): item x-o): item x-o. Here we designate the domain of items observed in items."}, {"heading": "4 Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Feature Selection", "text": "In machine learning and statistics, feature selection is one of the most important concepts. The aim of this process is to select a subset of relevant features for use in model construction and parameter matching. In real problems, we often use algorithm 3 \u03b1-Approximate Greedy Adaptive Algorithm 1: Input: Budget n, Ground Set V, p (\u03c6) and function f 2: Output: X \u00b2 V, where \"X\" = n 3: Initialize: X \u00b2 and 4: for i = 1 to n do 5: x \u00b2 V\\ X; Evaluation of features (x | 2) = E [f (dom): f (dom)."}, {"heading": "4.1.1 Submodularity", "text": "We will now show that this is submodular. Suppose there is a problem where this task does not exist. We can show that I (y; xA, xm) -I (y; xA) -I (y; xB, xm) -I (y; xB) -H (y | xA) -H (y | xA, xm) \u2265 H (y | xB) \u2212 H (y | xB, xm) (1) we can write H (y | xA) \u2212 H (y | xA) \u2212 H (y | xA, xm) = H (y | xA) \u2212 H (y, xm | xA) + H (xm | xA) \u2212 H (y | xA) \u2212 H (xm)."}, {"heading": "4.2 MAP Inference", "text": "In this section we will deal specifically with the problem of the maximum a posteriori inference on graphs. To analyze the algorithms more closely, we would like to introduce some preliminary terms, including the concept of polymatroids."}, {"heading": "4.2.1 Polymatroids", "text": "The concept of submodularity was first examined in the context of matroids. A set system (V, F) is defined by a base set V and a family of subsets F 2V. Such a system is a matroid when \u2022 \u2205 -F \u2022 when X Y-F then X-F \u2022 when X, Y-F and vice versa, Y-Y-e-e-X-Y is defined in such a way that Y + e-FNow defines a function called rank function that assigns a natural number to each subset V. This rank function is analogous to rank functions of matrices, in fact a matroid consisting of linearly independent columns of a matrix A is called a metric matroid. We define our matroid rank function as a succession function (X) = max."}, {"heading": "4.2.2 Cuts in Graphs, Energy Minimization and MAP Inference", "text": "The question whether this is a way in which the measures proposed by the EU Commission to limit the exploitation and exploitation of workers in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the EU, in the USA, in the EU, in the USA, in the EU, in the EU, in the EU, in the USA, in the EU, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "4.3 Active Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Supervised learning theory", "text": "In other words, the task is to find a response function f = 7 \u2192 Y, which is as accurate as possible the answer Y-Y for a particular input observation X-X. [13] The answers take a variety of forms in which Y = \u00b1 1. We have an unknown distribution D over the space of the observations and answers X-Y, so that observation response pairs according to (X, Y).Students choose from candidate functions or hypotheses in a hypothetical space H with the aim of minimizing the expected error or risk D (H) = E (X, Y).In other words, the goal is to minimize the expected error or risk D (H) = E (E).In other words, the goal is that we minimize the expected error or risk D (H) = E (X, Y)."}, {"heading": "4.3.2 Selective sampling as a submodular problem", "text": "Imagine the following problem, which we will call selective sampling of a budgetary problem: in the face of a large, fully designated, finite sample, we will \"buy\" a subset of \"B\" (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"B\") (\"H\") (\"H\") (\"H\") (\") (H) () () () () () () (H) () () () (H) () () (H) () () () () (H) () () () (H) () () () (H) () () () (H) () () () (H) () () () (H) () () () () (H) () () () () (H) () () () (H) () () () () (H) () () () () (H) () () () () (H) () () () () () (H) () () () () () () () ()) () () (H) () () () (H) () () () () () () ()) () () () () () () () () ()) () (H) () () () () () () () ()) (H) () () () () () () () () () () ()) (H) () () () () (() () () (())) () (())) (((()) ())) ((() ()) (()) ())) ((((())) (((()))))) ((((H () (())))) () (((()) ("}, {"heading": "4.3.3 Greedy active learning is adaptive submodular", "text": "It is about the question to what extent people are able to decide whether they are able to move in the world, and whether they are able to move in the world. (...) It is about the question to what extent people in the world are able to move in the world. (...) It is about the question to what extent people in the world are able to live in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world, in the world in the world in the world, in the world in the world, in the world in the world in"}, {"heading": "4.3.4 New directions", "text": "This is a wonderful example of cross-fertilization between research in computer science and optimization and learning theory based on submodularity and adaptive submodularity. Computer scientists have been able to rediscover and generalize previously published results from machine learning, but they have new and useful insights into the problem that they link to other problems (which we will not discuss in this section) and pave the way for new discoveries. Recently, there has been an explosion of similar work, many of which were published in 2013. [18] They describe a framework for performing distributed submodular maximizations in a shared-nothing. \""}, {"heading": "5 Submodularity in Weighted Constraint Reasoning", "text": "Many applications require efficient presentation and reasoning about factors such as fuzziness, probabilities, preferences and / or costs. Various extensions of the basic framework of Constraint Satisfaction Problems (CSPs) [25] have been introduced to include and justify such \"soft\" constraints, including variants such as fuzzy CSPs, probabilisticCSPs and weighted CSPs (WCSPs). A WCSP is an optimization version of a CSP in which the constraints are no longer \"hard,\" but extended by associating (non-negative) costs with the tuples. The goal is to find a mapping of values to all variables from their respective domains, so as to minimize total costs. For simplicity, we limit ourselves to Boolean WCSPs, which are limited to the domains of Boolean WCSPs. Note that this class can be used to preference important combinatorial problems such as the representation and user preferences [27], or overpreference arguments [26]."}, {"heading": "5.1 Weighted Constraint Satisfaction Problems", "text": "Formally, a WCSP is defined by a triplet < X, D, C >, where X = {X1, X2... XN} is a set of variables, and C = {C1, C2... CM} is a set of weighted constraints on subsets of variables. Each variable Xi is associated with a discretely weighted domain Di-D, and each constraint Ci is defined on a certain subset Si X of variables. Si is called the scope of Ci; and Ci does not set negative costs for any combination of values with the variables in Si. Uniformity of constraint Ci is equal | Si |. An optimal solution is to assign values to all variables (from their respective domains), so that the sum of costs (as defined locally by each weighted constraint) XXi and WCSP together represent an optimal edge if any variable (from their respective domains) is well known as winding."}, {"heading": "5.2 Submodular Constraints", "text": "Submodular constraints over Boolean domains correspond directly to submodular set functions. A set function \u043d: 2V \u2192 Q, which is defined on all subsets of a set V, is submodular if and only if we have for all subsets S, T V the calculation of economics (S-T) + economics (S-T) \u2264 economics (T). A submodular constraint is a weighted constraint with a submodular cost function. In this context, the agreement should be seen in the light of the observation that each subset S can be interpreted to specify the Boolean variables in V, which are set to 1. Boolean WCSPs with submodular constraints are known to be manageable [31]. However, the general algorithm for solving Boolean WCSPs with submodular constraints has a time complexity of O (N6), which is not very practical."}, {"heading": "5.3 Lifted Graphical Representations for Weighted Constraints", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them have trumped themselves. (...) Most of them are able to trump themselves."}, {"heading": "5.4 Submodular Constraints with bounded arity", "text": "The emphasis of [34] is on submodular constraints of finite nature (i.e. submodular constraints with a maximum of K, for some constant K) and providing asymptotically improved algorithms to solve them. The reason why these submodular constraints can be solved more efficiently is that the underlying maximum flow problems are staged on bipartite graphs. For Boolean WCSPs with a maximum of K, the bipartite CCG has N nodes in one partition, a maximum of 2KM nodes in the other partition, and a maximum of K2KM edges. For K, which is limited by a constant, this results in a time complexity of O (NM logM). This significantly improves the time complexity of the algorithm provided by [31]."}, {"heading": "5.5 Social Influence", "text": "With the increasing popularity of online social networking sites and apps such as Facebook and Twitter, social networks are now playing a fundamental role as a medium for people to share, exchange and receive new ideas and information. Modeling, use and understanding of social influence have become a \"hot topic\" in computer science, machine learning and computer-aided social science [35, 36, 37, 38, 39, 40, 41, 42]. It turns out that submodular functions, and in particular submodular function maximization, play a fundamental role in solving algorithmic issues related to social influence. In this section, we mainly focus on the use of submodular functional maximization techniques to solve two problems related to social influence, namely maximizing influence [36] and network inference [35]."}, {"heading": "5.5.1 Influence Maximization", "text": "Suppose that a company wants to promote its new product among individuals on a social network (real or virtual), and that the company has a limited budget to make free samples of its new product available to users on the social network. However, one natural question that needs to be asked is which group of users the company should give free choice of seeds to so that the overall adoption of the new product can be maximized. However, the question is how exactly the impact of the maximization problems can be maximized, namely the selection of a small set of seed nodes in a social network whose overall impact coverage is maximized under a particular diffusion model. Among the many diffusion models, the independent cascade model (IC) model and the linear threshold model (LT) are widely used in the study to maximize influence coverage. Both IC and LT models are stochastic models that characterize how the influence spreads across the entire network, starting from the function to the function of the seed observer, and the function of the seed observer."}, {"heading": "5.5.2 Network Inference", "text": "In most cases, however, the underlying network that enables diffusion (e.g. the timestamps in which a person posts a blog with specific information or retweeted another user tweet when a user has purchased a particular product in viral marketing applications, etc.) is the network inference problem focused on discovering the diffusion network from the observed cascades that occur between individuals in a social network. Existing approaches to this problem solve a maximum probability problem related to certain diffusion models [35, 37, 51]. It turns out that the likely function of this problem can be associated with a submodular function."}], "references": [{"title": "Lexicographically optimal base of a polymatroid with respect to a weight vector", "author": ["S. Fujishige"], "venue": "Mathematics of Operations Research, vol. 5, no. 2, pp. 186\u2013196, 1980. [Online]. Available: http: //pubsonline.informs.org/doi/abs/10.1287/moor.5.2.186", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1980}, {"title": "Finding the nearest point in a polytope", "author": ["P. Wolfe"], "venue": "Mathematical Programming, vol. 11, no. 1, pp. 128\u2013149, 1976. [Online]. Available: http://dx.doi.org/10.1007/BF01580381", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Submodular functions, matroids, and certain polyhedra", "author": ["J. Edmonds"], "venue": "Combinatorial Optimization Eureka, You Shrink!, ser. Lecture Notes in Computer Science, M. Jnger, G. Reinelt, and G. Rinaldi, Eds. Springer Berlin Heidelberg, 2003, vol. 2570, pp. 11\u201326.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V.S. Mirrokni"], "venue": "In Proceedings of 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS, 2007, p. 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal approximation for the submodular welfare problem in the value oracle model", "author": ["J. Vondrak"], "venue": "Proceedings of the 40th Annual ACM Symposium on Theory of Computing, ser. STOC \u201908. New York, NY, USA: ACM, 2008, pp. 67\u201374. [Online]. Available: http://doi.acm.org/10.1145/1374376.1374389", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Best algorithms for approximating the maximum of a submodular set function", "author": ["G.L. Nemhauser", "L.A. Wolsey"], "venue": "Mathematics of Operations Research, vol. 3, no. 3, pp. 177\u2013188, 1978. [Online]. Available: http://pubsonline.informs.org/doi/abs/10.1287/moor.3.3.177", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1978}, {"title": "Learning with submodular functions: A convex optimization perspective", "author": ["F. Bach"], "venue": "To appear in Foundations and Trends in Machine Learning, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "COLT, 2010, pp. 333\u2013345.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C.E. Guestrin"], "venue": "arXiv preprint arXiv:1207.1394, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "What energy functions can be minimized via graph cuts", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, pp. 65\u201381, 2004. 17", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficiently solving dynamic markov random fields using graph cuts", "author": ["P. Kohli", "P.H.S. Torr"], "venue": "Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, vol. 2, 2005, pp. 922\u2013929 Vol. 2.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Foundations of Machine Learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "An overview of statistical learning theory", "author": ["V. Vapnik"], "venue": "Neural Networks, IEEE Transactions on, vol. 10, no. 5, pp. 988\u2013999, 1999.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Two faces of active learning", "author": ["S. Dasgupta"], "venue": "Theor. Comput. Sci., vol. 412, no. 19, pp. 1767\u20131781, Apr. 2011. [Online]. Available: http://dx.doi.org/10.1016/j.tcs.2010.12.054", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis of a greedy active learning strategy", "author": ["\u2014\u2014"], "venue": "NIPS, 2004. [Online]. Available: http: //dblp.uni-trier.de/db/conf/nips/nips2004.html#Dasgupta04", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed submodular maximization: Identifying representative elements in massive data", "author": ["B. Mirzasoleiman", "A. Karbasi", "R. Sarkar", "A. Krause"], "venue": "To appear in Neural Information Processing Systems (NIPS), 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Near-optimal batch mode active learning and adaptive submodular optimization", "author": ["Y. Chen", "A. Krause"], "venue": "International Conference on Machine Learning (ICML), 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "J. Artif. Intell. Res. (JAIR), vol. 42, pp. 427\u2013486, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "Proc. Neural Information Processing Systems (NIPS), December 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Active learning for multi-criterion optimization", "author": ["M. Zuluaga", "A. Krause", "G. Sergent", "M. P\u00fcschel"], "venue": "International Conference on Machine Learning (ICML), 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Active classification: Theory and application to underwater inspection", "author": ["G.A. Hollinger", "U. Mitra", "G.S. Sukhatme"], "venue": "CoRR, vol. abs/1106.5829, 2011. [Online]. Available: http://dblp.uni-trier.de/db/journals/corr/ corr1106.html", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Agnostic Active Learning Without Constraints", "author": ["A. Beygelzimer", "J. Langford", "D. Hsu", "Z. Tong"], "venue": "Advances in Neural Information Processing Systems 23, 2011, pp. 199\u2013207.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Constraint Processing, ser", "author": ["R. Dechter"], "venue": "The Morgan Kaufmann Series in Artificial Intelligence. Elsevier Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "CP-nets: A tool for representing and reasoning with conditional ceteris paribus preference statements", "author": ["C. Boutilier", "R.I. Brafman", "C. Domshlak", "H.H. Hoos", "D. Poole"], "venue": "Journal of Artificial Intelligence Research, vol. 21, pp. 135\u2013191, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Planning with goal utility dependencies", "author": ["M.B. Do", "J. Benton", "M. Van Den Briel", "S. Kambhampati"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence, 2007, pp. 1872\u20131878. [Online]. Available: http://dl.acm.org/citation.cfm?id=1625275.1625578", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithm for optimal winner determination in combinatorial auctions", "author": ["T. Sandholm"], "venue": "Artificial Intelligence, vol. 135, no. 1-2, pp. 1\u201354, Feb. 2002. [Online]. Available: http://dx.doi.org/10.1016/S0004-3702(01)00159-X", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Mendelian error detection in complex pedigrees using weighted constraint satisfaction techniques", "author": ["M. Sanchez", "S. de Givry", "T. Schiex"], "venue": "Proceedings of the 2007 conference on Artificial Intelligence Research and Development. Amsterdam, The Netherlands, The Netherlands: IOS Press, 2007, pp. 29\u201337. [Online]. Available: http://dl.acm.org/citation.cfm?id=1566803.1566811", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Primal-dual algorithm for convex Markov random fields", "author": ["V. Kolmogorov"], "venue": "Microsoft Research, Tech. Rep. MSR-TR-2005-117, Sept. 2005. [Online]. Available: ftp://ftp.research.microsoft.com/pub/tr/TR-2005-117.pdf", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Classes of submodular constraints expressible by graph cuts", "author": ["S. Zivny", "P. Jeavons"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming, 2008, pp. 112\u2013127.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "A framework for hybrid tractability results in boolean weighted constraint satisfaction problems", "author": ["T.K.S. Kumar"], "venue": "Proceedings of the 14th International Conference on Principles and Practice of Constraint Programming, 2008, pp. 282\u2013297. [Online]. Available: http://dx.doi.org/10.1007/978-3-540-85958-1 19", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Lifting techniques for weighted constraint satisfaction problems", "author": ["\u2014\u2014"], "venue": "Proceedings of the International Symposium on Artificial Intelligence and Mathematics, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Submodular constraints and planar constraint networks: New results", "author": ["T.K.S. Kumar", "L. Cohen", "S. Koenig"], "venue": "Symposium on Abstraction, Reformulation and Approximation, 2013. 18", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring networks of diffusion and influence", "author": ["M. Gomez Rodriguez", "J. Leskovec", "A. Krause"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201910, 2010, pp. 1019\u20131028.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J. Kleinberg", "E. Tardos"], "venue": "Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201903, 2003, pp. 137\u2013146.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Submodular inference of diffusion networks from multiple trees", "author": ["M. Gomez-Rodriguez", "B. Sch\u00f6lkopf"], "venue": "CoRR, vol. abs/1205.1671, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Competitive influence maximization in social networks", "author": ["S. Bharathi", "D. Kempe", "M. Salek"], "venue": "Proceedings of the 3rd international conference on Internet and network economics, ser. WINE\u201907, 2007, pp. 306\u2013311.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Threshold models for competitive influence in social networks", "author": ["A. Borodin", "Y. Filmus", "J. Oren"], "venue": "Internet and Network Economics, ser. WINE\u201910, 2010, pp. 539\u2013550.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Influence maximization in social networks when negative opinions may emerge and propagate", "author": ["W. Chen", "A. Collins", "R. Cummings", "T. Ke", "Z. Liu", "D. Rincon", "X. Sun", "Y. Wang", "W. Wei", "Y. Yuan"], "venue": "Proceedings of the 11th SIAM International Conference on Data Mining, ser. SDM\u201911, 2011, pp. 379\u2013390.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Influence blocking maximization in social networks under the competitive linear threshold model", "author": ["X. He", "G. Song", "W. Chen", "Q. Jiang"], "venue": "Proceedings of the 12th SIAM International Conference on Data Mining, ser. SDM\u201912, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Limiting the spread of misinformation in social networks", "author": ["C. Budak", "D. Agrawal", "A. El Abbadi"], "venue": "Proceedings of the 20th international conference on World wide web, ser. WWW \u201911, 2011, pp. 665\u2013674.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable influence maximization for prevalent viral marketing in large-scale social networks", "author": ["W. Chen", "C. Wang", "Y. Wang"], "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201910, 2010, pp. 1029\u20131038.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Cost-effective outbreak detection in networks", "author": ["J. Leskovec", "A. Krause", "C. Guestrin", "C. Faloutsos", "J. VanBriesen", "N. Glance"], "venue": "Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201907, 2007, pp. 420\u2013429.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Celf++: optimizing the greedy algorithm for influence maximization in social networks", "author": ["A. Goyal", "W. Lu", "L.V. Lakshmanan"], "venue": "Proceedings of the 20th international conference companion on World wide web, ser. WWW \u201911, 2011, pp. 47\u201348.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable influence maximization in social networks under the linear threshold model", "author": ["W. Chen", "Y. Yuan", "L. Zhang"], "venue": "ICDM, 2010, pp. 88\u201397.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable influence maximization for prevalent viral marketing in large-scale social networks", "author": ["W. Chen", "C. Wang", "Y. Wang"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201910, 2010, pp. 1029\u20131038.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "On the submodularity of influence in social networks", "author": ["E. Mossel", "S. Roch"], "venue": "Proceedings of the Thirtyninth Annual ACM Symposium on Theory of Computing, ser. STOC \u201907, 2007, pp. 128\u2013134.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Influence maximization in social networks when negative opinions may emerge and propagate.", "author": ["W. Chen", "A. Collins", "R. Cummings", "T. Ke", "Z. Liu", "D. Rincn", "X. Sun", "Y. Wang", "W. Wei", "Y. Yuan"], "venue": "in SDM,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "Influence blocking maximization in social networks under the competitive linear threshold model", "author": ["X. He", "G. Song", "W. Chen", "Q. Jiang"], "venue": "Proceedings of the 12th SIAM International Conference on Data Mining, 2012.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "On the convexity of latent social network inference", "author": ["S.A. Myers", "J. Leskovec"], "venue": "NIPS, 2010, pp. 1741\u2013 1749.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Kronecker graphs: An approach to modeling networks", "author": ["J. Leskovec", "D. Chakrabarti", "J. Kleinberg", "C. Faloutsos", "Z. Ghahramani"], "venue": "J. Mach. Learn. Res., vol. 11, pp. 985\u20131042, Mar. 2010.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "The network completion problem: Inferring missing nodes and edges in networks", "author": ["M. Kim", "J. Leskovec"], "venue": "SDM, 2011.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified continuous greedy algorithm for submodular maximization.", "author": ["M. Feldman", "J. Naor", "R. Schwartz"], "venue": "in FOCS,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2011}, {"title": "Active sequential hypothesis testing", "author": ["M. Naghshvar", "T. Javidi"], "venue": "IEEE Transactions on Information Theory (to appear), March 2012.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2012}, {"title": "Submodular function minimization", "author": ["A. Toshev"], "venue": "WPE II Document, University of Pennsylvania, 2010.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximation algorithms for NP-complete problems on planar graphs", "author": ["B.S. Baker"], "venue": "Journal of the ACM, vol. 41, no. 1, pp. 153\u2013180, Jan. 1994. [Online]. Available: http://doi.acm.org/10.1145/174644.174650", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1994}, {"title": "Electrical flows, Laplacian systems, and faster approximation of maximum flow in undirected graphs", "author": ["P. Christiano", "J.A. Kelner", "A. Madry", "D.A. Spielman", "S.-H. Teng"], "venue": "Proceedings of the 43rd Annual ACM Symposium on Theory of Computing. ACM, 2011, pp. 273\u2013282. [Online]. Available: http://doi.acm.org/10.1145/1993636.1993674", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2011}, {"title": "An o(n log n) algorithm for maximum st-flow in a directed planar graph", "author": ["G. Borradaile", "P. Klein"], "venue": "Journal of the ACM, vol. 56, no. 2, pp. 9:1\u20139:30, Apr. 2009. [Online]. Available: http: //doi.acm.org/10.1145/1502793.1502798", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Max flows in O(nm) time, or better", "author": ["J.B. Orlin"], "venue": "ACM Symposium on the Theory of Computing, 2013, pp. 765\u2013774.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2013}, {"title": "Tutorial on submodularity in machine learning and computer vision", "author": ["S. Jegelka", "A. Krause"], "venue": "no. http://submodularity.org/submodularity-2012.pdf, 2012. [Online]. Available: http://submodularity.org/ submodularity-2012.pdf", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved algorithms for bipartite network flow", "author": ["O.J.B.R.K. Ahuja", "S. Clifford", "R.E. Tarjan"], "venue": "SIAM Journal on Computing, vol. 23, no. 5, pp. 906\u2013933, Oct. 1994. [Online]. Available: http://dx.doi.org/10.1137/S0097539791199334", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1994}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 235\u2013284, February 2008. 20", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "4 (Lov\u00e1sz Extension) A function f is submodular function, iff its Lov\u00e1sz Extension f\u0302 is convex, where f\u0302(c) = max{cx | x(U) \u2264 f(U) \u2200 U \u2286 V and c \u2208 [0, 1]}", "startOffset": 148, "endOffset": 154}, {"referenceID": 0, "context": "A non combinatorial approach proposed by Fujishige [1] is based on the norm characterization of the minima of f shown in Lemma 2.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Fujishige uses Wolfe\u2019s algorithm [2] which was developed to minimize the L2 norm of a vector in a convex hull of a finite set of points P \u2208 R.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "This issue is circumvented by using Edmonds Greedy Algorithm [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "Feige and Mirrokni [4] showed that maximizing for non-negative submodular functions a random subset achieves at least 1/4th the optimal value and local search techniques achieve at least a 1/2.", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "The solution to the arbritary matroid constraint was shown more recently by Vondrak et al [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "The initial result of an (1\u2212 1/e) approximation was shown by Nemhauser [6] in the 70s, but the result was only applicable to uniform matroid (cardinality) constraints.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "See [7] for more details.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "It has been shown by Golovin and Krause [8] that if a problem is adaptively submodular, then an adaptive greedy algorithm is guaranteed to obtain near optimal solutions.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "1 If xi\u2019s are all conditionally independent given y, then the function is submodular [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Such functions are called regular functions [11].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Another widely used application of this approach is to find the Maximum a posteriori estimate of a Markov Random Field [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "1 Supervised learning theory In classic supervised machine learning, the learning algorithm (or learner) is given the task of finding a response function f : X 7\u2192 Y that predicts as accurately as possible the output response Y \u2208 Y for a given input observation X \u2208 X [13].", "startOffset": 267, "endOffset": 271}, {"referenceID": 12, "context": "In the empirical risk minimization (ERM) paradigm, the learner assumes that the sample S is sufficiently representative of D such that choosing \u0125 = arg maxh\u2208H \u000f\u0302S will yield a hypothesis \u0125 that will also have a relatively low risk D(\u0125) [14].", "startOffset": 236, "endOffset": 240}, {"referenceID": 13, "context": "A well known theoretical result for classification that comes from Vapnik tells us if we want to learn a \u201cgood\u201d classifier from a hypothesis class H, then we need roughly |S| = \u00d5 ( d/\u03b5 log(1/\u03b4) ) points in our training sample [15].", "startOffset": 226, "endOffset": 230}, {"referenceID": 14, "context": "When err(h) > 0, the problem is not realizable [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "Rather it has access to an unlabeled data sample U = {(X, ?)}, as well as an oracle that the learner can query for the response (or label) of an observation, Y = or(X) [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 15, "context": "both of these questions is, in fact, yes [17].", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "[17] shows that in the worst case, this strategy may have to query every single label; indeed, for certain pathological cases, even the optimal query strategy will need to query every label.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "5, which rephrases Claim 4 and Theorem 3 from [17]:", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "5 (Dasgupta [17]) Suppose the optimal query policy requiresM labels in expectation for target hypotheses chosen uniformly from hypothesis class H of (VC) dimension d \u2265 e \u2248 16.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "As [17] points out, the lower bound is a bit depressing, but we derive some comfort from the fact that the upper bound matches the lower bound within a multiplicative factor.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [8], the authors show that the hypothesis space reduction problem is adaptive submodular, specifically an example of an adaptive stochastic coverage problem.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Then we can define a function that takes as input an element subset V \u2032 and a realization function \u03a6\u2032 and maps it to a real number in the interval [0, 1]: f(V \u2032,\u03a6\u2032) = f\u0302(H = {h : h(x) = \u03a6\u2032(x) for all x \u2208 V \u2032}) = 1\u2212 \u2211", "startOffset": 147, "endOffset": 153}, {"referenceID": 7, "context": "6, adapted from [8]:", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "6 (Golovin and Krause [8]) The hypothesis space reduction problem is adaptive submodular and adaptive monotone.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "7, adapted from [8]:", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "7 (Golovin and Krause [8]) Suppose the optimal query policy requires M labels in expectation for target hypotheses chosen using distribution \u03c0 from hypothesis class H.", "startOffset": 22, "endOffset": 25}, {"referenceID": 16, "context": "[18] describe a framework for performing distributed submodular maximization in a shared-nothing (MapReduce) storage setting and using it to choose a representative subsample of a massive data set for learning (similar to our selective sampling on a budget problem).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] describe a greedy batch-mode active learning algorithm that queries labels in batches of size k > 1 and show that this approach is competitive not only with optimal batch-more active learning but also with more traditional greedy active learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "There are a variety of other papers pushing the boundary in this area [20] [21] [22] [23].", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "There are a variety of other papers pushing the boundary in this area [20] [21] [22] [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "There are a variety of other papers pushing the boundary in this area [20] [21] [22] [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "There are a variety of other papers pushing the boundary in this area [20] [21] [22] [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "Aggressive active learning in the non-realizable case is a wide open problem, at least as of [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 22, "context": "We do have a variety of mellow active learners, which seek any informative label query, that are label efficient and statistically consistent [24] [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "We do have a variety of mellow active learners, which seek any informative label query, that are label efficient and statistically consistent [24] [16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "Various extensions to the basic framework of Constraint Satisfaction Problems (CSPs) [25] have been introduced to incorporate and reason about such \u201csoft\u201d constraints.", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "Note that this class can be used to model important combinatorial problems such as representing and reasoning about user preferences [26], over-subscription planning with goal preferences [27], combinatorial auctions [28], and bioinformatics [29], energy minimization problems in probabilistic settings, computer vision, Markov Random Fields [30], etc.", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "Note that this class can be used to model important combinatorial problems such as representing and reasoning about user preferences [26], over-subscription planning with goal preferences [27], combinatorial auctions [28], and bioinformatics [29], energy minimization problems in probabilistic settings, computer vision, Markov Random Fields [30], etc.", "startOffset": 188, "endOffset": 192}, {"referenceID": 26, "context": "Note that this class can be used to model important combinatorial problems such as representing and reasoning about user preferences [26], over-subscription planning with goal preferences [27], combinatorial auctions [28], and bioinformatics [29], energy minimization problems in probabilistic settings, computer vision, Markov Random Fields [30], etc.", "startOffset": 217, "endOffset": 221}, {"referenceID": 27, "context": "Note that this class can be used to model important combinatorial problems such as representing and reasoning about user preferences [26], over-subscription planning with goal preferences [27], combinatorial auctions [28], and bioinformatics [29], energy minimization problems in probabilistic settings, computer vision, Markov Random Fields [30], etc.", "startOffset": 242, "endOffset": 246}, {"referenceID": 28, "context": "Note that this class can be used to model important combinatorial problems such as representing and reasoning about user preferences [26], over-subscription planning with goal preferences [27], combinatorial auctions [28], and bioinformatics [29], energy minimization problems in probabilistic settings, computer vision, Markov Random Fields [30], etc.", "startOffset": 342, "endOffset": 346}, {"referenceID": 23, "context": "Boolean WCSPs are representationally as powerful as WCSPs; and it is well known that optimally solving Boolean WCSPs is NP-hard in general [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 29, "context": "Boolean WCSPs with submodular constraints are known to be tractable [31].", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "Specific classes of submodular constraints have been shown to be related to graph cuts, and are therefore solvable more efficiently [31].", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "They provide a unifying framework for exploiting both the graphical structure of the variable interactions as well as the numerical structure of the weighted constraints [32].", "startOffset": 170, "endOffset": 174}, {"referenceID": 31, "context": "This idea was first discussed in [33].", "startOffset": 33, "endOffset": 37}, {"referenceID": 30, "context": "Any given weighted constraint on Boolean variables can be represented graphically using a tripartite graph, which can be constructed in polynomial time [32].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "Finally, Boolean weighted constraints can be represented as multivariate polynomials on the variables participating in that constraint [31, 32].", "startOffset": 135, "endOffset": 143}, {"referenceID": 30, "context": "Finally, Boolean weighted constraints can be represented as multivariate polynomials on the variables participating in that constraint [31, 32].", "startOffset": 135, "endOffset": 143}, {"referenceID": 30, "context": "One way to build the CCG of a given weighted constraint is: (a) build the graphical representations for each of the individual terms in the multivariate polynomial; and (b) \u201cmerge\u201d these graphical representations [32].", "startOffset": 213, "endOffset": 217}, {"referenceID": 32, "context": "The focus of [34] is on bounded arity submodular constraints (that is, submodular constraints with arity at most K, for some constant K) and providing asymptotically improved algorithms for solving them.", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "This significantly improves on the O((N + M)) time complexity of the algorithm provided by [31].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 34, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 35, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 36, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 37, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 38, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 39, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 40, "context": "Modeling, utilizing, and understanding social influence has become \u201chot topic\u201d in computer science, machine learning, and computational social science [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 151, "endOffset": 183}, {"referenceID": 34, "context": "In this section, we mainly focus on using submodular function maximization techniques to solve two problems related to social influence, namely influence maximization [36] and network inference [35].", "startOffset": 167, "endOffset": 171}, {"referenceID": 33, "context": "In this section, we mainly focus on using submodular function maximization techniques to solve two problems related to social influence, namely influence maximization [36] and network inference [35].", "startOffset": 194, "endOffset": 198}, {"referenceID": 34, "context": "Among the many diffusion models, the Independent Cascade (IC) model and Linear Threshold (LT) are used widely in the study of influence maximization [36].", "startOffset": 149, "endOffset": 153}, {"referenceID": 34, "context": "It has been shown that the influence maximization problem under both IC model and LT model is NP-hard [36].", "startOffset": 102, "endOffset": 106}, {"referenceID": 34, "context": "[36]) The objective function of influence maximization problem under both IC and LT model is non-negative, monotone and submodular.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 82, "endOffset": 102}, {"referenceID": 42, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 82, "endOffset": 102}, {"referenceID": 43, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 82, "endOffset": 102}, {"referenceID": 44, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 82, "endOffset": 102}, {"referenceID": 45, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 82, "endOffset": 102}, {"referenceID": 42, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 131, "endOffset": 139}, {"referenceID": 43, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 131, "endOffset": 139}, {"referenceID": 41, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 191, "endOffset": 203}, {"referenceID": 44, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 191, "endOffset": 203}, {"referenceID": 45, "context": "Many papers have been published on how to improve the efficiency of the algorithm [43, 44, 45, 46, 47], such as by lazy evaluation [44, 45], or by approximate evaluation of the marginal gain [43, 46, 47] .", "startOffset": 191, "endOffset": 203}, {"referenceID": 46, "context": "A more general result on Generalized Linear Threshold models has been proved generalizing the results for the IC and LT models in [48].", "startOffset": 130, "endOffset": 134}, {"referenceID": 36, "context": "The influence maximization problem naturally extends to maximizing one\u2019s own influence [38, 39, 40] or minimizing the influence of the competitors [41, 42] given the choices of the initial seeds of the competitors.", "startOffset": 87, "endOffset": 99}, {"referenceID": 37, "context": "The influence maximization problem naturally extends to maximizing one\u2019s own influence [38, 39, 40] or minimizing the influence of the competitors [41, 42] given the choices of the initial seeds of the competitors.", "startOffset": 87, "endOffset": 99}, {"referenceID": 38, "context": "The influence maximization problem naturally extends to maximizing one\u2019s own influence [38, 39, 40] or minimizing the influence of the competitors [41, 42] given the choices of the initial seeds of the competitors.", "startOffset": 87, "endOffset": 99}, {"referenceID": 39, "context": "The influence maximization problem naturally extends to maximizing one\u2019s own influence [38, 39, 40] or minimizing the influence of the competitors [41, 42] given the choices of the initial seeds of the competitors.", "startOffset": 147, "endOffset": 155}, {"referenceID": 40, "context": "The influence maximization problem naturally extends to maximizing one\u2019s own influence [38, 39, 40] or minimizing the influence of the competitors [41, 42] given the choices of the initial seeds of the competitors.", "startOffset": 147, "endOffset": 155}, {"referenceID": 47, "context": "For example, on the maximization side, [49] study the influence maximization when a user can dislike the product and propagate bad news about it.", "startOffset": 39, "endOffset": 43}, {"referenceID": 48, "context": "On the minimization side, [50] study the idea of influence blocking maximization, which focuses on selecting seeds to block the propagation of rumors.", "startOffset": 26, "endOffset": 30}, {"referenceID": 34, "context": "The proof technique is similar to that in [36].", "startOffset": 42, "endOffset": 46}, {"referenceID": 33, "context": "Existing approaches to this problem solve a maximum likelihood estimation problem with respect to the network structure under certain diffusion models [35, 37, 51, 52].", "startOffset": 151, "endOffset": 167}, {"referenceID": 35, "context": "Existing approaches to this problem solve a maximum likelihood estimation problem with respect to the network structure under certain diffusion models [35, 37, 51, 52].", "startOffset": 151, "endOffset": 167}, {"referenceID": 49, "context": "Existing approaches to this problem solve a maximum likelihood estimation problem with respect to the network structure under certain diffusion models [35, 37, 51, 52].", "startOffset": 151, "endOffset": 167}, {"referenceID": 33, "context": "Thus, submodular function maximization can be used to solve this problem [35, 37].", "startOffset": 73, "endOffset": 81}, {"referenceID": 35, "context": "Thus, submodular function maximization can be used to solve this problem [35, 37].", "startOffset": 73, "endOffset": 81}, {"referenceID": 33, "context": "The extended IC model is used as the diffusion model in [35, 37].", "startOffset": 56, "endOffset": 64}, {"referenceID": 35, "context": "The extended IC model is used as the diffusion model in [35, 37].", "startOffset": 56, "endOffset": 64}, {"referenceID": 33, "context": "[35] proved that this objective function is monotone and submodular.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "This approach was later improved by by the MultiTree algorithm in [37], where the the matrix tree theorem is used to calculated the exact summation over all possible spanning trees, rather than using the maximum spanning tree approximation.", "startOffset": 66, "endOffset": 70}, {"referenceID": 50, "context": "Our current choice is the Kronecker graphs model [53, 54].", "startOffset": 49, "endOffset": 57}, {"referenceID": 51, "context": "Our current choice is the Kronecker graphs model [53, 54].", "startOffset": 49, "endOffset": 57}], "year": 2015, "abstractText": "In many naturally occurring optimization problems one needs to ensure that the definition of the optimization problem lends itself to solutions that are tractable to compute. In cases where exact solutions cannot be computed tractably, it is beneficial to have strong guarantees on the tractable approximate solutions. In order operate under these criterion most optimization problems are cast under the umbrella of convexity or submodularity. In this report we will study design and optimization over a common class of functions called submodular functions.", "creator": "LaTeX with hyperref package"}}}