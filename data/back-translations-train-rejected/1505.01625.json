{"id": "1505.01625", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Context-Aware Mobility Management in HetNets: A Reinforcement Learning Approach", "abstract": "The use of small cell deployments in heterogeneous network (HetNet) environments is expected to be a key feature of 4G networks and beyond, and essential for providing higher user throughput and cell-edge coverage. However, due to different coverage sizes of macro and pico base stations (BSs), such a paradigm shift introduces additional requirements and challenges in dense networks. Among these challenges is the handover performance of user equipment (UEs), which will be impacted especially when high velocity UEs traverse picocells. In this paper, we propose a coordination-based and context-aware mobility management (MM) procedure for small cell networks using tools from reinforcement learning. Here, macro and pico BSs jointly learn their long-term traffic loads and optimal cell range expansion, and schedule their UEs based on their velocities and historical rates (exchanged among tiers). The proposed approach is shown to not only outperform the classical MM in terms of UE throughput, but also to enable better fairness. In average, a gain of up to 80\\% is achieved for UE throughput, while the handover failure probability is reduced up to a factor of three by the proposed learning based MM approaches.", "histories": [["v1", "Thu, 7 May 2015 08:45:45 GMT  (462kb)", "http://arxiv.org/abs/1505.01625v1", null]], "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["meryem simsek", "mehdi bennis", "ismail g\\\"uvenc"], "accepted": false, "id": "1505.01625"}, "pdf": {"name": "1505.01625.pdf", "metadata": {"source": "CRF", "title": "Context-Aware Mobility Management in HetNets: A Reinforcement Learning Approach", "authors": ["Meryem Simsek", "Mehdi Bennis", "\u0130smail G\u00fcven\u00e7"], "emails": ["meryem.simsek@tu-dresden.de,", "bennis@ee.oulu.fi,", "iguvenc@fiu.edu"], "sections": [{"heading": null, "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "II. SYSTEM MODEL", "text": "We focus on transferring a two-layer HetNet, where layer 1 is modelled as macro cells and layer 2 as picocells. HetNet consists of a set of BSs K = {1,.., K} with a set M = {1,., M} of macro cells backed by a set P = {1,..., P} of picocells, where K = M-P. Macro BSs are dropped according to a hexagonal layout with three sectors. Within each macro sector m, p-P picocells are randomly positioned, and a set U = {1,..., U} of UEs are randomly dropped into a circle around each picocell p (hotspot). UEs associated with macro cells are referred to as macro UEs U (m) = {1 (m),."}, {"heading": "A. Handover Procedure", "text": "In accordance with the 3GPP standard, the transfer mechanism is based on RSRP measurements, the filtering of the measured RSRP samples, the transfer hysteresis margin and TTT mechanisms [3]. A transfer is performed if the (biased) RSRP of the target cell (plus hysteresis margin) is greater than the (biased) RSRP of the source cell. In summary, the transfer condition for a UU i (k) to BS k is defined as: Pl (i (l)) + \u03b2l < Pk (i (k) + \u03b2k + mhist, (3) with {l, k} \u043aK, mhist the UE- or cell-specific hysteresis margin, \u03b2k (\u03b2l) is the REB of BS k (l) and Pk (i (k)) (or Pl (i (l))))) [Bdm] is the R (R) or RTTi (SP)."}, {"heading": "B. Problem Formulation", "text": "Our optimization approach is aimed at maximizing the overall rate of the network. We are looking at long-term and short-term processes. The long-term load-balancing optimization approach is solved by the learning-based MM approaches proposed in Section III-A and Section III-B, which lead to REB \u03b2k value optimization and load balancing \u03c6k, tot (tn). Based on the estimated current load, the context-conscious planner selects one UU for each RB in the short term, taking into account its history and speed as described in Section II-C. This results in the immediate transfer rate of each UU inspi (k) (tn) and the RB allocation vector \u03b1i (k) (tn) (tn) (k), 1,..., \u03b1i (k), the maximum transfer rate (k), the maximum instance rate i (k), r (tk) driven by RB."}, {"heading": "C. Context-Aware Scheduler", "text": "The proposed MM approach not only optimizes the load under Section II-B, but also takes into account context ware and fairness-based UE planning. For each RB r, a UE i (k) is selected to be served by BS k to RB r according to the following planning criterion: i (k) r * = sort min (vi (k))) (arg max i (k) * Uk\u03c6i (k), r (tn) \u03c6i (tn))))), (9) where sortmin (vi (k))) sorts the candidates UEs according to their velocity starting from the slowest UE, i.e. if more than one UE can be selected for RB r, the UE is selected with minimal velocity. The reason for introducing the sorting / ranking function for candidates UEs according to their velocity UEs is that the high velocity UEs are not preferred if the UE is allocated according to (9) a resource rate that is equal to Ucity as the average (It)."}, {"heading": "III. LEARNING BASED MOBILITY MANAGEMENT ALGORITHM", "text": "To solve the optimization approach defined in Section II-B, we rely on HetNets \"self-organizing capabilities and propose an autonomous load balancing solution using reinforcement learning tools [8], in which each cell develops its own MM strategy to perform an optimal load distribution based on the learning-based approaches proposed in Section III-A and Section III-B. To realize this, we consider the game G = {K, {Ak} k-K, {uk} k-K}. Here, the amount K = {M-P} represents the group of players (i.e. BSs), and for all k-K represents the amount of actions that player k can take. For all k-K, the function uk (tn) is the service function of the player. Players learn tn at any time to optimize the load in a long-term manner and perform contextual planning based on the algorithms presented in Section III-A."}, {"heading": "A. Multi-Armed Bandit Based Learning Approach", "text": "The goal of the MAB approach is to maximize overall system performance. MAB is a machine learning technique based on an analogy with traditional slot machines (an armed bandit) [9]. The goal is to maximize the reward collected through iterative draws, i.e. the player selects his actions based on a decision function that reflects the known exploration process in learning algorithms. \u2022 The number of players, actions and utility function for our MAB-based MM approach is defined as: \u2022 Players: Macro BSs M = {1,., M} and pico BSs P = {1,. \u2022 Measures: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 18] dB as CRE-biased. We consider higher values to be equivalent to their extensive results."}, {"heading": "B. Satisfaction Based Learning Approach", "text": "Satisfaction-based learning approaches guarantee the satisfaction of the players in a system [11]. In this case, we consider the player to be satisfied when his cell reaches a certain minimum total rate and when at least 90% of the UEs in the cell reach a certain average rate. The logic behind taking these satisfaction conditions into account is to ensure the minimum rate of each individual UU while improving the overall rate of the cell. To allow a fair comparison, the players and the corresponding set of actions are selected in the proposed Satisfaction-based MM approach in the same way as in the MAB-based MM approach. The benefit function of player k at the time tn is defined as load according to Equation (11). In the Satisfaction-based learning approach, the actions are selected according to a probability distribution tn (tn) = [tn), 1 (tn), the satisfaction function of player k (tn) is the satisfaction function of player at the time tn (tn)."}, {"heading": "IV. SIMULATION RESULTS", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is not a country in which it is not a country but a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "V. CONCLUSION", "text": "We propose two learning-based MM approaches and a historically context-sensitive planning method for HetNets. The first learning approach is based on MAB methods and objectives 0 20 40 60 80 100 120 00.10.20.30.40.50.60.7Velocity [km / h] PP per babi lityClassical MAB SatisfactionTTT = 480 msTTT = 40 msFig. 6: PP probability for 30 UEs and 1 pico BS per macro cell and TTT = 40 ms and TTT = 480 ms.at System performance maximization. The second learning method aims to satisfy every cell and every unit of a cell based on satisfaction-based learning. System simulations show the performance of the proposed approaches compared to the classical MM method. While on average up to 80% increases in UE throughput are achieved, the HOF probability is reduced to a factor of three by the proposed learning-based MM approaches."}], "references": [{"title": "A Survey on 3GPP Heterogeneous Networks", "author": ["A. Damnjanovic"], "venue": "IEEE Wirel. Comm., vol.18,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Evolved Universal Terrestrial Radio Access (E- UTRA); Mobility Enhancements in Heterogeneous Networks", "author": ["3GPP TR 36.839"], "venue": "V11.1.0, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobility Performance of LTE Co-Channel Deployment of Macro and Pico Cells", "author": ["S. Barbera", "P.H. Michaelsen", "M. S\u00c3d\u2019ily", "K. Pedersen"], "venue": "Proc. IEEE Wireless Comm. and Networking Conf. (WCNC), France, Apr. 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Mobility Management Challenges in 3GPP Heterogeneous Networks", "author": ["D. Lopez-Perez", "I. Guvenc", "X. Chu"], "venue": "IEEE Comm. Mag., vol. 50, no. 12, Dec. 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Handover aware interference management in LTE small cells networks", "author": ["A. Feki", "V. Capdevielle", "L. Roullet", "A.G.Sanches"], "venue": "Proc. IEEE 11th International Symposium on Modeling & Optimization in Mobile, Ad Hoc & Wireless Networks (WiOpt), May 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimized Fairness Cell Selection for 3GPP LTE-A Macro-Pico HetNets, ", "author": ["J. Wang", "J. Liu", "D. Wang", "J. Pang", "G. Shen"], "venue": "in Proc. IEEE Vehic. Technol. Conf. (VTC),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Reinforcement Learning: A Tutorial", "author": ["M.E. Harmon", "S.S. Harmon"], "venue": "2000. Available: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.2480&rep=rep1&type=pdf.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Finite time Analysis for the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Performance Study on ABS with Reduced Macro Power", "author": ["3GPP R1-113806"], "venue": "3GPP TSG-RAN Meeting #67, Nov. 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Satisfaction Equilibrium: Achieving Cooperation in Incomplete Information Games", "author": ["S. Ross", "B. Chaib-draa"], "venue": "19th Canadian Conf. on Artificial Intelligence, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Evolved Universal Terrestrial Radio Access (EU- TRA); Further advancements for E-UTRA Physical Layer Aspects", "author": ["3GPP TR 36.814"], "venue": "V9.0.0, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "An LTE-Femtocell Dynamic System Level Simulator", "author": ["M. Simsek", "T. Akbudak", "B. Zhao", "A. Czylwik"], "venue": "International ITG Workshop on Smart Antennas (WSA), 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The deployment of Long Term Evolution (LTE) heterogeneous networks (HetNets) is a promising approach to meet the ever-increasing wireless broadband capacity challenge [1], [2].", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "However, deploying HetNets entails a number of challenges in terms of capacity, coverage, mobility management (MM), and mobility load balancing (MLB) across multiple network tiers [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "11, mobility enhancements in HetNets have been investigated through a dedicated study item [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": ", in [4]\u2013[7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": ", in [4]\u2013[7].", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "In [4], the authors evaluate the effect of different combinations of MM parameter settings for HetNets.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "The simulations in [4] consider that all UEs have the same velocity in each simulation setup.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "In [5], the authors evaluate the mobility performance of HetNets considering almost blank subframes in the presence of cell range expansion and propose a mobility based intercell interference coordination (ICIC) scheme.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [6], a handover-aware ICIC approach based on reinforcement learning is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In [7], the cell selection problem in HetNets is formulated as a network wide proportional fairness optimization problem by jointly considering the long-term channel condition and load balance in a HetNet.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "According to the 3GPP standard, the handover mechanism is based on RSRP measurements, the filtering of measured RSRP samples, Handover Hysteresis Margin, and TTT mechanisms [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 6, "context": "To solve the optimization approach defined in Section II-B, we rely on the self organizing capabilities of HetNets and propose an autonomous solution for load balancing by using tools from reinforcement learning [8].", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "MAB is a machine learning technique based on an analogy with the traditional slot machine (one armed bandit) [9].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 31, "endOffset": 40}, {"referenceID": 4, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 31, "endOffset": 40}, {"referenceID": 1, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 4, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 7, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 10, "context": "\u2022 Actions: Ak = {\u03b2k} with \u03b2m = [0, 3, 6] dB and \u03b2p = [0, 3, 6, 9, 12, 15, 18] dB being the CRE bias.", "startOffset": 53, "endOffset": 77}, {"referenceID": 8, "context": "The considered bias values rely partially on the assumptions in [10] and at the same time extensive simulation results.", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "Satisfaction based learning approaches guarantee to satisfy the players in a system [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "The scenario used in the system-level simulations is based on configuration #4b HetNet scenario in [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Simulations are performed with the picocell deployment based modified version of the system level simulator presented in [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "We consider fast-fading and shadowing effects in our simulations that are based on 3GPP assumptions [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "To compare our results with other approaches we consider a baseline MM approach as defined in [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "This slope is aligned with the results presented in [3].", "startOffset": 52, "endOffset": 55}], "year": 2015, "abstractText": "The use of small cell deployments in heterogeneous network (HetNet) environments is expected to be a key feature of 4G networks and beyond, and essential for providing higher user throughput and cell-edge coverage. However, due to different coverage sizes of macro and pico base stations (BSs), such a paradigm shift introduces additional requirements and challenges in dense networks. Among these challenges is the handover performance of user equipment (UEs), which will be impacted especially when high velocity UEs traverse picocells. In this paper, we propose a coordination-based and context-aware mobility management (MM) procedure for small cell networks using tools from reinforcement learning. Here, macro and pico BSs jointly learn their long-term traffic loads and optimal cell range expansion, and schedule their UEs based on their velocities and historical rates (exchanged among tiers). The proposed approach is shown to not only outperform the classical MM in terms of UE throughput, but also to enable better fairness. In average, a gain of up to 80% is achieved for UE throughput, while the handover failure probability is reduced up to a factor of three by the proposed learning based MM approaches.", "creator": "LaTeX with hyperref package"}}}