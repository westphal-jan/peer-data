{"id": "1505.01728", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Integrating K-means with Quadratic Programming Feature Selection", "abstract": "Several data mining problems are characterized by data in high dimensions. One of the popular ways to reduce the dimensionality of the data is to perform feature selection, i.e, select a subset of relevant and non-redundant features. Recently, Quadratic Programming Feature Selection (QPFS) has been proposed which formulates the feature selection problem as a quadratic program. It has been shown to outperform many of the existing feature selection methods for a variety of applications. Though, better than many existing approaches, the running time complexity of QPFS is cubic in the number of features, which can be quite computationally expensive even for moderately sized datasets. In this paper we propose a novel method for feature selection by integrating k-means clustering with QPFS. The basic variant of our approach runs k-means to bring down the number of features which need to be passed on to QPFS. We then enhance this idea, wherein we gradually refine the feature space from a very coarse clustering to a fine-grained one, by interleaving steps of QPFS with k-means clustering. Every step of QPFS helps in identifying the clusters of irrelevant features (which can then be thrown away), whereas every step of k-means further refines the clusters which are potentially relevant. We show that our iterative refinement of clusters is guaranteed to converge. We provide bounds on the number of distance computations involved in the k-means algorithm. Further, each QPFS run is now cubic in number of clusters, which can be much smaller than actual number of features. Experiments on eight publicly available datasets show that our approach gives significant computational gains (both in time and memory), over standard QPFS as well as other state of the art feature selection methods, even while improving the overall accuracy.", "histories": [["v1", "Thu, 7 May 2015 14:45:11 GMT  (668kb)", "https://arxiv.org/abs/1505.01728v1", "17 pages, 11 figures"], ["v2", "Tue, 11 Aug 2015 18:06:36 GMT  (668kb)", "http://arxiv.org/abs/1505.01728v2", "17 pages, 11 figures"]], "COMMENTS": "17 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yamuna prasad", "k k biswas"], "accepted": false, "id": "1505.01728"}, "pdf": {"name": "1505.01728.pdf", "metadata": {"source": "CRF", "title": "Integrating K-means with Quadratic Programming Feature Selection", "authors": ["Yamuna Prasada", "K. K. Biswasa"], "emails": ["yprasad@cse.iitd.ac.in", "kkb@cse.iitd.ac.in"], "sections": [{"heading": null, "text": "This year, it is so far that it will be able to put itself at the top of the list, \"he said.\" We are very satisfied with the development, \"he said.\" We are very satisfied, \"he said.\" We are very satisfied with the development, \"but we are not yet able to reform ourselves.\""}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. QPFS [6]", "text": "In the face of a data set with M characteristics (fi, i = 1,..., M) and N training examples (xi, i = 1,..., N) with class Y markings (yi, i = 1,..., Y), the standard QPFS formulation [6] is: f (\u03b1) = min \u03b11 2 \u03b1TQ\u03b1 \u2212 sT\u03b1Subject to the formula \u03b1i > 0, i = 1,..., I T\u03b1 = 1. (1) where \u03b1 is an M-dimensional vector, I is the vector of all ones and Q is an M \u00b7 M symmetrical positive semi-definitive formula representing the redundancy among the characteristics; s is an M-size vector representing the relevance of characteristics with corresponding class markings. In this formulation, the square term covers the dependence between each pair of characteristics and the linear term marking the relevance between each characteristic and class markings."}, {"heading": "2.2. Similarity Measure", "text": "Classification accuracy can be improved with MI because it detects non-linear dependencies between pairs of variables as opposed to the correlation coefficient, which measures only the linear relationship between a pair of variables [15, 6]. Mutual information between a pair of characteristics fi and fj can be calculated as follows: MI (fi, fj) = H (fi) + H (f2) \u2212 H (fi, f2) (3), where H (fi) reperently represents the entropy of characteristic vectors fi and H (fi, fj) represents the common entropy between characteristic vectors fi and fj [16]. The following variant of mutual information can be used as a distance metric [16]: d (fi, fj) = 1 \u2212 MI (fi, fj) max (H (fi), H (fj), H (fj), H (4)."}, {"heading": "2.3. MacQueen\u2019s K-Means Algorithm [8]", "text": "This is a k-mean cluster algorithm that runs in two passes. In the first pass, it selects the first k samples as the first k centers and assigns each of the remaining N \u2212 k samples to the cluster whose center is closest, and updates the centers. In the second pass, each of the N samples is assigned to the clusters whose center is closest, and the centers are updated. The number of distance calculations in the first and second pass are k (N \u2212 k) and Nk, respectively. Thus, the number of distance calculations required in MacQueen's k \u2212 mean algorithm is 2Nk \u2212 k2. This means that the complexity in reality is O (Nk) [7]."}, {"heading": "2.4. Two-level K-means[7] Algorithm", "text": "This cluster algorithm ensures that the radius of the produced clusters is smaller than a predefined threshold. The algorithm is outlined below: Algorithm Two-Level K Means (D, k, \u03c4) Input: dataset D, Initial Number of Clusters k and Radius Threshold. Output: Set of clusters C (c1, c2,..) (with radius ri 6) and the set of cluster centers \u00b5. 1. (Level 1:) Cluster of the given data centers points to an arbitrarily selected k \u2032 cluster C (c1, c2,.). (with radius ri 6) and the set of cluster centers \u00b5. 1. (Level 1:) Cluster of the given data centers leads to an arbitrarily selected k \u00b2 mean algorithm. 2. Calculate the radius ri of the ith cluster using ri = Maxxj \u00b2 (ci, ci), where MacQueen is the similarity metric. 3. Level: When the size of the data center ri is defined."}, {"heading": "3. Two-level K-means QPFS", "text": "The authors in [7] use two-dimensional k-mean clustering to reduce the number of data points for classification by SVM. We use a similar idea, except that we cluster a set of characteristics instead of a set of data points. We then apply QPFS to representative characteristics. Thus, the problem is transformed into the attribute space, as opposed to their formulas in the data point space. Another important distinction is that we have to work with actual characteristics, as opposed to cluster means, as in the case of [7]. This is because the topics of the attribute cluster are abstract points and may not coincide with actual characteristics through which the attribute selection could be performed. To this end, we develop two algorithms, the first by modifying the k-mean algorithm of MacQueen and the other by modifying the two-stage k-mean algorithm [7] to return cluster representatives (characteristics) in place of cluster means."}, {"heading": "3.1. Variant MacQueen\u2019s K-means", "text": "We propose a variant of MacQueen's K-mean algorithm for clustering to create clusters with redundant characteristics instead of clustering data points. In each iteration of MacQueen's K-mean algorithm, the closest point of the updated mean is selected as a new center (referred to as a cluster representative). Each iteration must calculate the distance from M-k characteristics to k centers and the distance from the center to the next feature in its cluster. Therefore, each iteration requires k (M \u2212 k) + M distance calculations. Since MacQueen's k mean uses two iterations, the total number of distance calculations would be 2Mk \u2212 2k2 + 2M. Thus, the complexity is O (Mk)."}, {"heading": "3.2. Variant Two-level K-Means(TLKM)", "text": "We propose a two-level k-Means algorithm (TLKM) by replacing MacQueen's k-Means algorithm with its variant in the k-Means algorithm at two levels, as specified in Section 2.4. It is important to note that we cluster characteristics, not data points. TLKM returns the feature clusters together with corresponding representatives. If we follow the arguments in [7], we can derive the limits in terms of the number of distance calculations for our proposed TLKM algorithm in a similar way. The only difference is that we have an additional 2M \u2212 k2 term, as explained in Section 2.4. Limits for the difference in the number of distance calculations between the k \u2212 mean of the MacQueens variant and the TLKM isU \u2212 M (((((\u03b1N + 1) R1 \u2212 R + 1) R1 \u2212 ND2 \u2212 ND2 (6 U + k \u00b2 n) that the next level of the equation is R \u00b2 K n)."}, {"heading": "3.3. Two-level K-Means QPFS (TLKM-QPFS) Algorithm", "text": "We have called this algorithm TLKM-QPFS to cluster the features in a given dataset, followed by a run of QPFS. The algorithm TLKM-QPFS illustrates our proposed two-level k-mean QPFS (TLKMQPFS) approach. The algorithm TLKM-QPFS (FS, k) Input: Feature set FS, Initial Number of Clusters kand Radius Threshold. Output: The final representative feature set F (features) in order of their \u03b1 values. 1. Find representative F using the TLKM algorithm as defined in Section 3.2. 2. Apply the QPFS to cluster representatives F. 3. Enter the rank F in the order of time and space complexities for the TLKM approach in space."}, {"heading": "4. Interleaved K-Means QPFS (IKM-QPFS)", "text": "We now propose a new algorithm by combining the advantages of the cluster approach with QPFS. In this proposed algorithm, we aim to refine the relevant attribute space from coarse to fine-grained clusters to improve accuracy while maintaining some of the computational gains achieved by TLKM-QPFS. The TLKM-QPFS algorithm uses k-means to identify clusters of characteristics that are similar (redundant), selecting a representative for each of the clusters, which is then fed into QPFS. QPFS in turn provides a ranking of these cluster representatives. Many of the representatives are classified as irrelevant to the classification (\u03b1 = 0). Among the groups of clusters whose representatives were considered irrelevant, we consider those with cluster radius r <."}, {"heading": "4.1. Interleaved K-Means QPFS (IKM-QPFS) Algorithm", "text": "These cluster representatives are then fed into QPFS to obtain a ranking of the characteristics on them. The cluster with sufficiently small radius (r < \u03c4) does not need to be further refined and can be used directly for the final level of feature selection. In practice, we need to perform the recursive splitting of clusters only on a user-defined level. In our approach, we divide each cluster into a fixed number (k) of sub-mean splits. The proposed K-mean QPFS algorithms are presented in algorithms IKM-QPFS. Algorithm IKM-QPFS (FS, k, L, L)."}, {"heading": "4.2. Convergence", "text": "In each recursive call to the IRR algorithm, all clusters whose radius is greater than Sub \u03c4 are further divided into k-sub-clusters. Since each slit is guaranteed to decrease the size of the original cluster and we have a finite number of characteristics, the algorithm is guaranteed to terminate and find clusters whose radius is smaller than it is sufficiently large. Note that in extreme cases, a cluster has only one point in it, and therefore its radius will be zero. Now, let's try to analyze what happens in an average case, i.e. if the sub-cluster induced by the MacQueen algorithm results in equally-sized clusters. Formally, ri denotes the radius of cluster i (at a certain level), which needs to be further divided. Then the volume surrounded by this cluster is C, riN is the number of original data points (this is the space in which features are embedded)."}, {"heading": "4.3. Distance Computations", "text": "In the worst case, none of the clusters will be discarded, and their radii will also be greater or equal to the threshold at each level, resulting in a recursive division of each cluster up to level L. Consider the cluster cj at level i. The number of distance calculations required by the MacQueen algorithm to further split this cluster is 2 | cj | k \u2212 2k2 + 2 | cj | (see Section 3.1). The equality results from the fact that the total number of points in clusters at each level is i (since each cluster is split up to the last level)."}, {"heading": "4.4. Time Complexity Analysis", "text": "The required time in step 1 and step 2 of the algorithm IKM-QPFS is O (Mk) and O (k3), respectively. In step 4 of the algorithm IKM-QPFS, the algorithm IRR is called, which is executed recursively. The time required for its execution can be calculated as follows: If the maximum number of levels is L, the number of clusters is k, then it can easily be shown that the time complexity of the algorithm IRR by O (LMk + k3 + L) is above the limit. The first term is derived from the number of distance calculations in k averages, and the second term is derived from the O (kL) calls of the QPFS (ki \u2212 1 calls at level i), with each call taking O (k3) time. Thus, the time required in step 4 of the algorithm IKM-QPFS is very short (MK + 3) and MK + 3 (MK)."}, {"heading": "4.5. Interleaved K-Means Aggressive QPFS (IKMAQPFS)", "text": "In this section, we present a variation of the IKMQPFS algorithm described above. The key idea is that after each step of the QPFS run, we discard all clusters whose representatives are considered irrelevant during a QPFS run (i.e. \u03b1 = 0), regardless of the radii of the corresponding clusters. This is a departure from the originally proposed algorithm, whereby we discard a cluster only if the corresponding \u03b1 = 0 and the cluster radius r \u2264 are considered irrelevant. We call this variant Interleaved K-Means Aggressive QPFS (IKMA-QPFS) because it is aggressive enough to discard the clusters whose representatives are considered irrelevant, potentially resulting in an even greater gain in terms of computational complexity, as IKMAQPFS attempts to identify the irreversible feature clusters early in the process and discards them."}, {"heading": "5. Experiments", "text": "We are comparing the performance of our proposed approaches TLKM-QPFS, IKM-QPFS and IKMA-QPFS with QPFS, FGM and GDM on eight publicly available benchmark datasets. We are comparing all methods in terms of their time and storage requirements and error rates for different number of selected top-k characteristics. FGM and GDM methods work for binary classification datasets, so the comparison with FGM and GDM is not performed for SRBCT multi-class classification datasets. We are observing improved accuracy for FGM and GDM for normalized datasets in the range [-1, 1]. Therefore, we have normalized all datasets in the range [-1, 1]. We are plotting the accuracy graphs to vary the number of top-k characteristics (1 to 100) selected for all datasets."}, {"heading": "5.1. Datasets", "text": "For our experimental study, we used eight publicly available benchmark datasets used by other researchers for trait selection; the description of these datasets is shown in Table 1. WDBC are datasets for breast cancer Wisconsin (diagnostic), Colon, SRBCT, lymphoma, leukemia and RAC are microarray datasets ([6, 17, 3, 18]) and the last two are vision datasets [18, 19]."}, {"heading": "5.2. Methodology", "text": "WDBC and USPS datasets are divided into 60% and 40% size splits for training and testing, respectively, as in [18]. The MNIST dataset is divided into 11,982 training and 1984 test cases following [19]. The reported results are an average of over 100 random splits of data. The number of samples is very small (less than 100) in microarray datasets, so leaveone-out cross-validation is used for these datasets. We use mutual information as in [6] for redundancy and relevance measures in the experiments. The data is discredited using three segments and a standard deviation for calculating mutual information as in [6]. For QPFS, the value of the scale parameter (\u03b8) is determined using cross-validation from the set of commercially available values {0.0, 0.1, 0.3, 0.5, 0.7, 0.9}. For QPFS, the error rates obtained were similar to those used under Q2K, using QM."}, {"heading": "5.3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1. Time and Memory", "text": "Tables 2 and 3 show the time and memory requirements for feature selection performed with each of the methods for all datasets. TLKM-QPFS, IKM-QPFS and IKMA-QPFS are orders of magnitude faster than QPFS for all datasets. TLKMQPFS is three times faster than GDM for RAC, MNIST and USPS datasets, while three to ten times slower than GDM for WDBC, Colon, lymphoma and leukemia datasets. TLKM-QPFS is three to five times faster than GMA-QPFFS memory, one order of magnitude faster than KMA-KMA datasets and USPS datasets. IKM-QPFS and IKMA-QPFS are three to five times faster than FGM and GDM-QPFS datasets."}, {"heading": "5.3.2. Accuracy", "text": "S & P-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-"}, {"heading": "5.4. Summary", "text": "From the above results, it is clear that our three proposed approaches to feature selection, TLKM-QPFS, IKM-QPFS and IKMA-QPFS, deliver significant gains in computing requirements (both in time and memory) while improving overall accuracy in all cases when compared to 3. Specifically, our proposed approaches help achieve the relevant features early, which is a very important feature of a good feature selection method. TLKM-QPFS, IKM-QPFS and IKMA-QPFS arithmetic requirements are similar. On the large microarray datasets, our proposed approaches are faster than FGM and GDM."}, {"heading": "6. Conclusion", "text": "In this paper, we proposed an approach to integrate k-means-based clustering with quadratic programming feature selection (QPFS), the key idea being to use k-means to cluster redundant features. Only one representative from each cluster had to be considered during the QPFS run to reduce the complexity of QPFS from cubic in the number of features to cubic in the number of features (which is much smaller). We presented two variants of our approach. TLKM-QPFS used two k-means at the level to identify a number of representative features, followed by a number of QPFS. In the more sophisticated variant, IKMA-QPFS, we combined the steps of k-means with QPFS, resulting in a very fine-grained selection of relevant features. Extensive evaluations on eight publicly available datasets showed the superior performance of our comprehensive approach to comparing an existing set of features to the state of the art."}, {"heading": "Acknowledgment", "text": "The authors thank Dr. Parag Singla, Department of Computer Science & Engineering, In-dian Institute of Technology, Delhi, India, for his valuable suggestions and support."}], "references": [{"title": "Feature selection in scientific applications", "author": ["E. Cant\u00fa-Paz", "S. Newsam", "C. Kamath"], "venue": "in: Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Stable feature selection via dense feature groups, in: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201908", "author": ["L. Yu", "C. Ding", "S. Loscalzo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "An extensive empirical study of feature selection metrics for text classification", "author": ["G. Forman", "I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Two-level k-means clustering algorithm for k-\u03c4 relationship establishment and linear-time classification, Pattern Recogn", "author": ["R. Chitta", "M. Narasimha Murty"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Classifying large data sets using svms with hierarchical clusters", "author": ["H. Yu", "J. Yang", "J. Han"], "venue": "in: Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Fast k-nearest neighbor classification using cluster-based trees", "author": ["B. Zhang", "S. Srihari"], "venue": "IEEE Transactions on Pattern Analysis and Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Fast high-dimensional kernel summations using the monte carlo multipole method", "author": ["D. Lee", "A. Gray"], "venue": "in: Proceedings of Twenty Second Annual Conference on Neural Information Processing Systems (NIPS 08),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["H. Liu", "L. Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Attribute clustering for grouping, selection, and classification of gene expression", "author": ["W.-H. Au", "K.C.C. Chan", "A.K.C. Wong", "Y. Wang"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Simultaneous gene clustering and subset selection for sample classification via mdl., Bioinformatics", "author": ["R. Jornsten", "B. Yu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Feature selection based on mutual information: criteria of max-dependency, maxrelevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Information-theoretic measures for knowledge 15  discovery and data mining, in: Karmeshu (Ed.), Entropy Measures, Maximum Entropy Principle and Emerging Applications", "author": ["Y.Y. Yao"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Design of fuzzy expert system for microarray data classification using a novel genetic swarm algorithm", "author": ["P. Ganesh Kumar", "T. Aruldoss Albert Victoire", "P. Renukadevi", "D. Devaraj"], "venue": "Expert Syst. Appl", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Learning sparse svm for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Discovering support and affiliated features from very high dimensions", "author": ["Y. Zhai", "M. Tan", "Y.S. Ong", "I.W. Tsang"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For many scientific applications, each of the dimensions (features) have an inherent meaning and one needs to keep the original features (or a representative subset) around to perform any meaningful analysis on the data [1].", "startOffset": 220, "endOffset": 223}, {"referenceID": 1, "context": "Searching for such an optimal subset is computationally intractable (search space is exponential) [2, 3].", "startOffset": 98, "endOffset": 104}, {"referenceID": 2, "context": ") [4, 5]) [6].", "startOffset": 2, "endOffset": 8}, {"referenceID": 3, "context": "In this paper, we propose a feature selection approach by first clustering the set of features using two-level k-means clustering [7] and then applying QPFS over the cluster representatives (called Twolevel K-Means QPFS).", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "Our approach is motivated by the work of Chitta and Murty [7], which proposes a two-level k-means algorithm for clustering the set of similar data points and uses it for improving classification accuracy in SVMs.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "Chitta and Murty [7] show that their approach yields linear time complexity in contrast to standard cubic time complexity for SVM training.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "In addition to Chitta and Murty [7], there is other prior literature which uses clustering to reduce the dimensionality of the data for classification and related tasks.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Examples include Clustering based SVM (CB-SVM) [9], clustering based trees for k-nearest neighbor classification [10] and use of PCA for efficient Gaussian kernel summation [11].", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "Examples include Clustering based SVM (CB-SVM) [9], clustering based trees for k-nearest neighbor classification [10] and use of PCA for efficient Gaussian kernel summation [11].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "Examples include Clustering based SVM (CB-SVM) [9], clustering based trees for k-nearest neighbor classification [10] and use of PCA for efficient Gaussian kernel summation [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 7, "context": "[12] presents a framework for categorizing existing feature selection algorithms and chosing the right algorithm for an application based on data characteristics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Considering the relative importance of non-redundancy amongst the features and their relevance,a scalar quantity \u03b8 \u2208 [0, 1] is introduced in the above formulation resulting in [6]:", "startOffset": 117, "endOffset": 123}, {"referenceID": 8, "context": "Various measures have been employed to represent similarities among features [13, 3, 14, 2].", "startOffset": 77, "endOffset": 91}, {"referenceID": 1, "context": "Various measures have been employed to represent similarities among features [13, 3, 14, 2].", "startOffset": 77, "endOffset": 91}, {"referenceID": 9, "context": "Various measures have been employed to represent similarities among features [13, 3, 14, 2].", "startOffset": 77, "endOffset": 91}, {"referenceID": 10, "context": "The classification accuracy can be improved with MI as it captures nonlinear dependencies between pair of variables unlike correlation coefficient which only measures linear relationship between a pair of variables [15, 6].", "startOffset": 215, "endOffset": 222}, {"referenceID": 11, "context": "where H(fi) reperents entropy of feature vector fi and H(fi, fj) represents the joint entropy between feature vectors fi and fj [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Following variant of mutual information can be used as distance metric [16]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "This in effect means that the complexity is O(Nk) [7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "Two-level K-means[7] Algorithm", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "Recently, a two-level k-means algorithm has been developed using MacQueen\u2019s k-means algorithm [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "[7] shows that the above two-level k-means algorithm reduces the number of distance calculations as required by the MacQueen\u2019s k-means algorithm, while guaranteeing a bound on the clustering error (details below).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The clustering error in two-level k-means algorithm is upper bounded by twice the error of optimal clustering [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "The time complexity of two-level k-means algorithm is O(Nk) and the space complexity isO(N+ k) [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "The detailed analysis of these bounds can be found in [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Authors in [7] employ two-level k-means clustering for reducing the number of data points for classification using SVM.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Another key distinction is that we need to work with actual features unlike cluster means as in the case of [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Towards this end, we develop two algorithms, the first one by modifying the MacQueen\u2019s k-means algorithm and the other one by modifying the two-level k-means algorithm [7] to return cluster representatives (features) in place of cluster means.", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "This takes the place of N which is the cardinality of (data) space in the case of Chitta and Murty [7].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "This takes place of n which is the dimensionality of the (data) space in case of Chitta and Murty [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "Following the arguments in [7], we can derive the bounds on number of distance computations for our proposed TLKM algorithm in a similar manner.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "Following [7], for reducing the number of computations in TLKM algoritm, it is necessary that", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Following the arguments in [7], it can be shown that the time and space complexities of the modified twolevel k-means for clustering features are O(Mk) and O(M + k), respectively.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "We observe an improved accuracy for FGM and GDM on normalized dataset in range [ -1, 1].", "startOffset": 79, "endOffset": 87}, {"referenceID": 0, "context": "Therefore, we normalized all the datasets in range [ -1, 1].", "startOffset": 51, "endOffset": 59}, {"referenceID": 12, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 130, "endOffset": 144}, {"referenceID": 1, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 130, "endOffset": 144}, {"referenceID": 13, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 130, "endOffset": 144}, {"referenceID": 13, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 174, "endOffset": 182}, {"referenceID": 14, "context": "WDBC is breast cancer Wisconsin (diagnostic) dataset, Colon, SRBCT, Lymphoma, Leukemia and RAC datasets are microarray datasets ( [6, 17, 3, 18]) and the last two are vision [18, 19] datasets.", "startOffset": 174, "endOffset": 182}, {"referenceID": 13, "context": "WDBC and USPS datasets are divided into 60% and 40% sized splits for training and testing, respectively as in [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "MNIST dataset is divided in 11,982 training and 1984 testing instances following [19].", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": ",1000 } with step size of 5 and k parameter in IKM-QPFS (as well in IKMA-QPFS) is choosen from the set [ 3, 150 ].", "startOffset": 103, "endOffset": 113}, {"referenceID": 13, "context": "FGM and GDM are embedded methods, so accuracy for both of these methods are obtained according to [18, 19].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "FGM and GDM are embedded methods, so accuracy for both of these methods are obtained according to [18, 19].", "startOffset": 98, "endOffset": 106}], "year": 2015, "abstractText": "Several data mining problems are characterized by data in high dimensions. One of the popular ways to reduce the dimensionality of the data is to perform feature selection, i.e, select a subset of relevant and non-redundant features. Recently, Quadratic Programming Feature Selection (QPFS) has been proposed which formulates the feature selection problem as a quadratic program. It has been shown to outperform many of the existing feature selection methods for a variety of applications. Though, better than many existing approaches, the running time complexity of QPFS is cubic in the number of features, which can be quite computationally expensive even for moderately sized datasets. In this paper we propose a novel method for feature selection by integrating k-means clustering with QPFS. The basic variant of our approach runs k-means to bring down the number of features which need to be passed on to QPFS. We then enhance this idea, wherein we gradually refine the feature space from a very coarse clustering to a fine-grained one, by interleaving steps of QPFS with k-means clustering. Every step of QPFS helps in identifying the clusters of irrelevant features (which can then be thrown away), whereas every step of k-means further refines the clusters which are potentially relevant. We show that our iterative refinement of clusters is guaranteed to converge. We provide bounds on the number of distance computations involved in the k-means algorithm. Further, each QPFS run is now cubic in number of clusters, which can be much smaller than actual number of features. Experiments on eight publicly available datasets show that our approach gives significant computational gains (both in time and memory), over standard QPFS as well as other state of the art feature selection methods, even while improving the overall accuracy.", "creator": "LaTeX with hyperref package"}}}