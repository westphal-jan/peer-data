{"id": "1505.01749", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Object detection via a multi-region & semantic segmentation-aware CNN model", "abstract": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 74.9% and 70.7% correspondingly, surpassing any other published work by a significant margin.", "histories": [["v1", "Thu, 7 May 2015 15:42:07 GMT  (8333kb,D)", "https://arxiv.org/abs/1505.01749v1", "17 pages, 14 figures"], ["v2", "Tue, 9 Jun 2015 16:49:44 GMT  (8339kb,D)", "http://arxiv.org/abs/1505.01749v2", "18 pages, 14 figures. This second version has updated experimental results on PASCAL VOC and also discusses and compares with new contemporary work"], ["v3", "Wed, 23 Sep 2015 22:24:42 GMT  (8415kb,D)", "http://arxiv.org/abs/1505.01749v3", "Extended technical report -- short version to appear at ICCV 2015"]], "COMMENTS": "17 pages, 14 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["spyros gidaris", "nikos komodakis"], "accepted": false, "id": "1505.01749"}, "pdf": {"name": "1505.01749.pdf", "metadata": {"source": "CRF", "title": "Object detection via a multi-region & semantic segmentation-aware CNN model", "authors": ["Spyros Gidaris", "Nikos Komodakis"], "emails": ["gidariss@imagine.enpc.fr", "nikos.komodakis@enpc.fr"], "sections": [{"heading": "1. Introduction", "text": "One of the most studied problems in computer vision is that of object recognition: faced with an image return of all instances of one or more types of objects in the form of boxes that enclose them, the last two years have seen enormous improvements in this task thanks to the recent advances of the deep learning community. [19, 1, 15] Among them is the work of Sermanet et et al. [30] with the Overfeat Framework and the work of Girshick et al. [10] with the R-CNN Framework.Overfeat. [30] CNN uses models that are applied to a sliding window mode on multiple levels of an image, the first being used to classify whether a window contains an object and the second to predict the true boundary box position of the object. Finally, the dense class and location predictions are merged with a greedy algorithm to determine the final amount of objects. This work was supported by the ANR SEAPOLIS project."}, {"heading": "2. Related Work", "text": "Apart from Overfeat [30] and R-CNN [10], several other recent publications deal with the problem of object recognition using deep neural networks. One is the very recent work of Zhu et al. [38], which has some conceptual similarities to ours. Specifically, they extract properties from an additional region to capture the contextual appearance of a candidate box, they use an MRF inference system to exploit the suggestions for object segmentation (obtained through parametric minimum bearings) to improve object recognition accuracy, and also use iterative box regression (based on backward development). More than that, we use multiple regions designed to diversify the appearance factors captured by our representation and to improve localization, we use semantic segmentation characteristics that we know (integrated into a unified neural network architecture), and use a deep CNN model for drilling."}, {"heading": "3. Multi-Region CNN Model", "text": "The detection model we propose consists of a multi-layer CNN network, from which each component is chosen to focus on a different region of an object. We call this a multi-region CNN model. We start by describing its general architecture first. To this end, we introduce a general CNN architecture abstraction that splits the computation into two different modules: Activation Map Module. This part of the network receives the entire image as input and issues activation maps (feature maps), passing it through a sequence of revolutionary layers. Region Adaptation Module. Faced with a region R on the image and the activation maps of the image module, this module R projects onto the activation maps, processes the activations that lie in pools them with a spatially adaptable (max.) pooling layer that forms the layer, and passes them on through a multi-layer network work."}, {"heading": "3.1. Region components and their role on detection", "text": "We use 2 types of central regions in our model: rectangles and rectangular rings, in which the latter type is defined in relation to an inner and outer rectangle. We describe below all the regions we employ, while their specifications are given in the caption of our Figure 3. Original Candidate Field: This is the candidate himself as being used on R-CNN (Figure 3a). A network trained in this type of region is guided to capture the appearance of the entire object. If used alone, the basic line of our work consists of the left / right / lower parts of a candidate field (Figure 3b, 3c, 3d, and 3e). Networks trained on each of them will, in order to learn the appearance, learn the characteristics of an object that we are standing only on each side of an object or on each side of the boundaries."}, {"heading": "4. Semantic Segmentation-Aware CNN Model", "text": "In order to further diversify the features encoded by our presentation, we are expanding the multi-region CNN model so that it also learns semantic segmentation-conscious CNN features, motivated by the close link between segmentation and recognition, as well as the fact that segmentation-related clues are empirically known, which often help with object recognition [5, 12, 25]. In the context of our multi-regional CNN network, the integration of semantic segmentation-aware features occurs by adding suitably customized versions of the two main modules of the network, i.e. the \"Activation Cards\" and \"Region Adaptation\" modules (see architecture in Figure 4). Below, we refer to the resulting modules as: \u2022 Activation Maps Module for semantic segmentation-aware features."}, {"heading": "4.1. Activation maps module for semantic", "text": "In order to fulfill the purpose of using semantic segmentation characteristics, we use a fully revolutionary network [23] for this module, hereinafter referred to as FCN, which is trained to predict class-specific foreground probabilities (we refer the interested reader to [23] for more details about FCN, where it is used for the task of semantic segmentation) Weak supervised training. To train the activation maps module for the class-specific foreground segmentation task, we use only the Object Identification Notes (to make the training of our entire system independent of the availability of segmentation notes). To this end, we follow a weakly monitored training strategy and create artificial foreground-class-specific segmentation masks using soil-specific annotations. More precisely, the soil-determining boxes of an image are marked on the spatial domain of the last layer of the underlying layer of the unmarked object."}, {"heading": "4.2. Region adaptation module for semantic segmentation-aware features", "text": "We take advantage of the above activation maps by treating them as medium features and adding a single regional adaptation module designed for our primary object detection task. In this case, we opt for a single region, which we obtain by enlarging the candidate detection box by a factor of 1.5 (such a region also contains semantic information from the vicinity of a candidate detection box).The reason that we do not repeat the same regions as in the multi-region CNN architecture is due to efficiency, as they are already used to capture the external characteristics of an object."}, {"heading": "5. Object Localization", "text": "As explained above, our Multi-Region CNN Recognition Model has the localization perception required for accurate object localization, but it is not enough on its own. To take full advantage of it, our Recognition Model needs to be presented with well-localized candidate boxes, which in turn are evaluated with high confidence by it. The solution we adopt consists of 3 main components: CNN Regional Adaptation Module for Box Regression. We present an additional Region Adaptation Module, which, instead of being used for object detection, is trained to predict the object-bounding field. It is applied to the top of the activation cards produced from the Multi-Region CNN Model, and instead of a typical single-layer return model, consists of two hidden fully connected layers and a prediction layer that gives 4 values (i.e., one Bounding field) per category."}, {"heading": "6. Implementation Details", "text": "In fact, most of them are able to play by the rules they have imposed on themselves."}, {"heading": "7. Experimental Evaluation", "text": "We evaluate our recognition system on PASCAL VOC2007 [6] and on PASCAL VOC2012 [7]. When presenting the results, we will use either the original candidate box region alone (Figure 3a) and / or the R-CNN framework with VGG-Net [31] as a starting point. We note that when using the original candidate box region alone, the resulting model is a realization of the SPP-Net [13] object recognition framework with the 16-layer VGG-Net [31]. Unless otherwise specified, we have trained our models for all results of PASCAL VOC2007 on the Trainval set and tested them on the test set of the same year."}, {"heading": "7.1. Results on PASCAL VOC2007", "text": "First, we evaluate the importance of each individual adaptation module of the region solely on the task of object detection. The results are presented in Table 1. As expected, the most powerful component is the original candidate box. Surprising is the high detection performance of individual regions such as the border region in Figure 3i 54.8% or the contextual region in Figure 3j 47.2%. Despite the fact that the area visible by them comprises only limited or no parts of the object, they exceed previous detection systems based on hand-made features. Also interesting is the high detection performance of the semantic segmentation of conscious region, 56.6%. In Table 2 we report on the detection performance of our proposed modules. The multi-region CNN model without semantic segmentation conscious CNN functions (MR-CNN) reaches 66.2% mAP system, which is 4.2 points higher than RCNN model with VGG-Net (62.0%) and net higher than the original candidate box (1.7%)."}, {"heading": "7.2. Detection error analysis", "text": "We use the tool from Hoiem et al. [16] to analyze the detection errors of our system. In Figure 8, we draw pie charts with the percentage of detection errors caused by poor localization, confusion with other categories, and an unlabeled object. We use the tool from Hoiem et al. [16] to analyze the detection errors of our system. In Figure 8, we draw pie charts with the percentage of detection errors that are false positive due to poor localization, confusion with similar categories, confusion with other categories, and caused by the background or an unlabeled object. We observe that by using the Multi-Region CNN model instead of the original Candidate Box region alone, a significant reduction in the percentage of false positives is achieved due to poor localization. This confirms our argument that focusing on multiple regions of an object increases the localization sensitivity of our model."}, {"heading": "7.3. Localization awareness of Multi-Region CNN", "text": "In this experiment we estimate the correlation between the IoU overlap and the suggestions from the box [34] (with the suggestions from the email overlap of e-mail overlap of e-mail overlap of e-mail overlap of 100 overlap of e-mail overlap of 100 overlap of 100 overlap of e-mail overlap of 100 overlap of 100 overlap of 100 overlap of e-mail overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of e-mail overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of e-mail overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of 100 overlap of"}, {"heading": "7.4. Results on PASCAL VOC2012", "text": "In Table 6, we compare our detection system with other published work on the PASCAL VOC2012 test set [7]. Our overall system includes the MultiRegion CNN model, enriched with CNN's semantic segmentation characteristics and coupled with CNN-based Bounding Box regression in the iterative localization scheme. We tested two instances of our system. Both have exactly the same components, but they were trained on different datasets. In the first case, the fine-tuning of the networks and training of SVMs to detect SVMs was performed on the VOC2007 train + val dataset with 5011 commented images. In the second case, the networks were fine-tuned on the VOC2012 train dataset with 5717 commented images and the SVM detection training was performed on the VOC2012 train + val dataset with 11540 commented images."}, {"heading": "7.5. Training with extra data and comparison with", "text": "Contemporary approaches that are consistent with ours [29, 9, 28, 27] train their models with additional data to improve the accuracy of their systems. We follow the same practice and report on the results in Tables 7 and 8. Specifically, we trained our models on VOC 2007 and 2012 Zug + Val data sets using both selective search [34] and EdgeBox [39] suggestions. During the test period, we only use EdgeBox suggestions that can be calculated more quickly. The tables show that our methods outperform other approaches even when trained with less data. Currently (08 / 06 / 15), our entries are number one and number two in the ranking of the PASCAL VOC2012 object recognition benchmark comp4 (see Table 8), and the difference between our best performing entry and the third is 3.5 points."}, {"heading": "8. Qualitative Results", "text": "In Figures 12-14, we present some of the object detections obtained through our approach. We use blue delimiters to mark the true positive detections, and red delimiters to mark the false positive detections. In Figure 9, we present a few difficult examples of this type. In Figure 10, we show some other cases of failure. Missing annotations. There have also been cases of object instances correctly detected by our approach but not included in the note of truth on the ground of PASCAL VOC2007. Figure 11 shows some of these examples of uncommented object instances."}, {"heading": "9. Conclusions", "text": "We have proposed a powerful CNN-based representation for object recognition based on two key factors: (i) the diversification of the discriminatory appearance factors it captures by focusing on different regions of the object, and (ii) the encoding of semantic segmentation-conscious features. By using it in the context of a CNN-based localization scheme, we show that it achieves excellent results that far exceed the state of the art."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Convolutional feature masking for joint object and stuff segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "arXiv preprint arXiv:1412.1283,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886\u2013893. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards unified object detection and semantic segmentation", "author": ["J. Dong", "Q. Chen", "S. Yan", "A. Yuille"], "venue": "Computer Vision\u2013ECCV 2014, pages 299\u2013314. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes challenge", "author": ["M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": "(voc", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "The pascal visual object classes challenge", "author": ["M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627\u20131645,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "arXiv preprint arXiv:1504.08083,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580\u2013587. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep CNN ensemble with data augmentation for object detection", "author": ["J. Guo", "S. Gould"], "venue": "CoRR, abs/1506.07224,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Computer Vision\u2013 ECCV 2014, pages 297\u2013312. Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1406.4729,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Diagnosing error in object detectors", "author": ["D. Hoiem", "Y. Chodpathumwan", "Q. Dai"], "venue": "Computer Vision\u2013ECCV 2012, pages 340\u2013353. Springer,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, 1(4):541\u2013551,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Features in concert: Discriminative feature selection meets unsupervised clustering", "author": ["M. Leordeanu", "A. Radu", "R. Sukthankar"], "venue": "arXiv preprint arXiv:1411.7714,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, abs/1312.4400,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4038,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision, 60(2):91\u2013110,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "The role of context for object  detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 891\u2013898. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection", "author": ["W. Ouyang", "P. Luo", "X. Zeng", "S. Qiu", "Y. Tian", "H. Li", "S. Yang", "Z. Wang", "Y. Xiong", "C. Qian"], "venue": "arXiv preprint arXiv:1409.3505,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "arXiv preprint arXiv:1506.01497,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Object detection networks on convolutional feature maps", "author": ["S. Ren", "K. He", "R. Girshick", "X. Zhang", "J. Sun"], "venue": "arXiv preprint arXiv:1504.06066,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable, high-quality object detection", "author": ["C. Szegedy", "S. Reed", "D. Erhan", "D. Anguelov"], "venue": "arXiv preprint arXiv:1412.1441,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Segmentation as selective search for object recognition", "author": ["K.E. Van de Sande", "J.R. Uijlings", "T. Gevers", "A.W. Smeulders"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Multiple kernels for object detection", "author": ["A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on, pages 606\u2013613. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving object detection with deep convolutional networks via bayesian optimization and structured prediction", "author": ["Z. Yuting", "S. Kihyuk", "V. Ruben", "P. Gang", "H. Lee"], "venue": "arXiv preprint arXiv:1504.03293,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2014, pages 818\u2013833. Springer,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "segdeepm: Exploiting segmentation and context in deep neural networks for object detection", "author": ["Y. Zhu", "R. Urtasun", "R. Salakhutdinov", "S. Fidler"], "venue": "arXiv preprint arXiv:1502.04275,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C.L. Zitnick", "P. Doll\u00e1r"], "venue": "Computer Vision\u2013ECCV 2014, pages 391\u2013405. Springer,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "The last two years, huge improvements have been observed on this task thanks to the recent advances of deep learning community [19, 1, 15].", "startOffset": 127, "endOffset": 138}, {"referenceID": 0, "context": "The last two years, huge improvements have been observed on this task thanks to the recent advances of deep learning community [19, 1, 15].", "startOffset": 127, "endOffset": 138}, {"referenceID": 14, "context": "The last two years, huge improvements have been observed on this task thanks to the recent advances of deep learning community [19, 1, 15].", "startOffset": 127, "endOffset": 138}, {"referenceID": 29, "context": "[30] with the Overfeat framework and the work of Girshick et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] with the R-CNN framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Overfeat [30] uses two CNN models that applies on a sliding window fashion on multiple scales of an image.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "R-CNN [10] uses Alex Krizhevsky\u2019s Net [18] to extract features from box proposals provided by selective search [34] and then classifies them with class specific linear SVMs.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "R-CNN [10] uses Alex Krizhevsky\u2019s Net [18] to extract features from box proposals provided by selective search [34] and then classifies them with class specific linear SVMs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 33, "context": "R-CNN [10] uses Alex Krizhevsky\u2019s Net [18] to extract features from box proposals provided by selective search [34] and then classifies them with class specific linear SVMs.", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "This simple pipeline surpasses by a large margin the detection performance of all the previously published systems, such as deformable parts models [8] or non-linear multi-kernel approaches [35].", "startOffset": 148, "endOffset": 151}, {"referenceID": 34, "context": "This simple pipeline surpasses by a large margin the detection performance of all the previously published systems, such as deformable parts models [8] or non-linear multi-kernel approaches [35].", "startOffset": 190, "endOffset": 194}, {"referenceID": 2, "context": "Their success comes from the fact that they replaced the hand-engineered features like HOG [3] or SIFT [24] with the high level object representations produced from the last layer of a CNN model.", "startOffset": 91, "endOffset": 94}, {"referenceID": 23, "context": "Their success comes from the fact that they replaced the hand-engineered features like HOG [3] or SIFT [24] with the high level object representations produced from the last layer of a CNN model.", "startOffset": 103, "endOffset": 107}, {"referenceID": 30, "context": "By employing an even deeper CNN model, such as the 16-layers VGG-Net [31], they boosted the performance another 7 points.", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.", "startOffset": 158, "endOffset": 182}, {"referenceID": 36, "context": "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.", "startOffset": 158, "endOffset": 182}, {"referenceID": 30, "context": "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.", "startOffset": 158, "endOffset": 182}, {"referenceID": 16, "context": "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.", "startOffset": 158, "endOffset": 182}, {"referenceID": 13, "context": "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.", "startOffset": 158, "endOffset": 182}, {"referenceID": 31, "context": "Besides object representation, our work is also motivated from the observation that, due to the tremendous classification capability of the recent CNN models [18, 37, 31, 17, 14, 32], the bottleneck for good detection performance is now the accurate object localization.", "startOffset": 158, "endOffset": 182}, {"referenceID": 9, "context": "Indeed, it was noticed on R-CNN [10] that the most common type of false positives is the mis-localized detections.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "9% on VOC2007 [6] and VOC2012 [7] detection challenges respectively, thus surpassing the previous state-of-art by a very significant margin.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "9% on VOC2007 [6] and VOC2012 [7] detection challenges respectively, thus surpassing the previous state-of-art by a very significant margin.", "startOffset": 30, "endOffset": 33}, {"referenceID": 29, "context": "Apart from Overfeat [30] and R-CNN [10], several other recent papers are dealing with the object detection problem using deep neural networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "Apart from Overfeat [30] and R-CNN [10], several other recent papers are dealing with the object detection problem using deep neural networks.", "startOffset": 35, "endOffset": 39}, {"referenceID": 37, "context": "[38], which shares some conceptual similarities with ours.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "As features they use the outputs of HOG [3]+SVM classifiers trained on each region separately and the 1000-class predictions of a CNN pretrained on ImageNet.", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "An \u201cadaptive max pooling\u201d layer uses spatially adaptive pooling as in [13] (but with a one-level pyramid).", "startOffset": 70, "endOffset": 74}, {"referenceID": 32, "context": "On [33], they designed a deep CNN model for object proposals generation and they use contextual features extracted from the last hidden layer of a CNN model trained on ImageNet classification task after they have applied it on large crops of the image.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "On [26], they introduce a deep CNN with a novel deformation constrained pooling layer, a new strategy for pre-training that uses the bounding box annotations provided from ImageNet localization task, and contextual features derived by applying a pre-trained on ImageNet CNN on the whole image and treating the 1000class probabilities for ImageNet objects as global contextual features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "On SPP-Net [13] detection framework, instead of applying their deep CNN on each candidate box separately as R-CNN does, they extract the convolutional feature maps from the whole image, project the candidate boxes on them, and then with an adaptive max-pooling layer, which consists of multiple pooling levels, they produce fixed length feature vectors that they pass through the fully connected layers of the CNN model.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "Contemporary to our work are the approaches of [29, 9, 28] that are also based on the SPP-Net framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 8, "context": "Contemporary to our work are the approaches of [29, 9, 28] that are also based on the SPP-Net framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 27, "context": "Contemporary to our work are the approaches of [29, 9, 28] that are also based on the SPP-Net framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 28, "context": "On [29], they improve the SPP framework by replacing the subnetwork component that is applied on the convolutional features extracted from the whole image with a deeper convolutional network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "On [9], they focus on simplifying the training phase of SPP-Net and R-CNN and speeding up both the testing and the training phases.", "startOffset": 3, "endOffset": 6}, {"referenceID": 27, "context": "Finally, on [28] they extend [9] by adding a new sub-network component for predicting class-independent proposals and thus making the system both faster and independent of object proposal algorithms.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "Finally, on [28] they extend [9] by adding a new sub-network component for predicting class-independent proposals and thus making the system both faster and independent of object proposal algorithms.", "startOffset": 29, "endOffset": 32}, {"referenceID": 12, "context": "Given a region R on the image and the activation maps of the image, this module projects R on the activation maps, crops the activations that lay inside it, pools them with a spatially adaptive (max-)pooling layer [13], and then forwards them through a multi-layer network.", "startOffset": 214, "endOffset": 218}, {"referenceID": 9, "context": "Region a: it is the candidate box itself as being used on R-CNN [10].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "Original candidate box: this is the candidate detection box itself as being used on R-CNN [10] (figure 3a).", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "Both of them were trained on PASCAL VOC2007 [6] trainval set and tested on the test set of the same challenge.", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "The motivation for this comes from the close connection between segmentation and detection as well as from the fact that segmentation related cues are empirically known to often help object detection [5, 12, 25].", "startOffset": 200, "endOffset": 211}, {"referenceID": 11, "context": "The motivation for this comes from the close connection between segmentation and detection as well as from the fact that segmentation related cues are empirically known to often help object detection [5, 12, 25].", "startOffset": 200, "endOffset": 211}, {"referenceID": 24, "context": "The motivation for this comes from the close connection between segmentation and detection as well as from the fact that segmentation related cues are empirically known to often help object detection [5, 12, 25].", "startOffset": 200, "endOffset": 211}, {"referenceID": 22, "context": "In order to serve the purpose of exploiting semantic segmentation aware features, for this module we adopt a Fully Convolutional Network [23], abbreviated hereafter as FCN, trained to predict class specific foreground probabilities (we refer the interested reader to [23] for more details about FCN where it is being used for the task of semantic segmentation).", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "In order to serve the purpose of exploiting semantic segmentation aware features, for this module we adopt a Fully Convolutional Network [23], abbreviated hereafter as FCN, trained to predict class specific foreground probabilities (we refer the interested reader to [23] for more details about FCN where it is being used for the task of semantic segmentation).", "startOffset": 267, "endOffset": 271}, {"referenceID": 22, "context": "The reason that we do not repeat the same regions as in the iniFigure 5: Illustration of the weakly supervised training of the FCN [23] used as activation maps module for the semantic segmentation aware CNN features.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "It is applied on top of the activation maps produced from the Multi-Region CNN model and, instead of a typical one-layer ridge regression model [10], consists of two hidden fully connected layers and one prediction layer that outputs 4 values (i.", "startOffset": 144, "endOffset": 148}, {"referenceID": 33, "context": "Our localization scheme starts from the selective search proposals [34] and works by iteratively scoring them and refining their coordinates.", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "For the first iteration t = 1, the box proposals Bc are coming from selective search [34] and are common between all the classes.", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "First, standard non-max suppression [10] is applied on Dc and produces the detections Yc = {(si,c, Bi,c)} using an IoU overlap threshold of 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "model [31] pre-trained on ImageNet [4] for the task of image classification3.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "model [31] pre-trained on ImageNet [4] for the task of image classification3.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Regarding the regions that are rectangular rings, both the inner and outer box are projected on the activation maps and then the activations that lay inside the inner box are masked out by setting them to zero (similar to the Convolutional Feature Masking layer proposed on [2]).", "startOffset": 274, "endOffset": 277}, {"referenceID": 9, "context": "In order to train the region adaptation modules, we follow the guidelines of R-CNN [10].", "startOffset": 83, "endOffset": 87}, {"referenceID": 33, "context": "The positive samples are defined as the selective search proposals [34] that overlap a groundtruth bounding box by at least 0.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "Its architecture consists of the 16-layers VGG-Net without the last classification layer and transformed to a FCN [23] (by reshaping the fc6 and fc7 fully connected layers to convolutional ones with kernel size of 7\u00d7 7 and 1\u00d7 1 correspondingly).", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "Its architecture consists of a spatially adaptive max-pooling layer [23] that outputs feature maps of 512 channels on a 9 \u00d7 9 grid, and a fully connected layer with 2096 channels.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "In order to train the SVMs we follow the same principles as in [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "As positive samples are considered the ground truth bounding boxes and as negative samples are considered the selective search proposals [34] that overlap with the ground truth boxes by less than 0.", "startOffset": 137, "endOffset": 141}, {"referenceID": 9, "context": "We use hard negative mining the same way as in [10, 8].", "startOffset": 47, "endOffset": 54}, {"referenceID": 7, "context": "We use hard negative mining the same way as in [10, 8].", "startOffset": 47, "endOffset": 54}, {"referenceID": 33, "context": "For training samples we use the box proposals [34] that overlap by at least 0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "The target values are defined the same way as in R-CNN [10].", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "In our system we adopt a similar multi-scale implementation as in SPP-Net [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "We evaluate our detection system on PASCAL VOC2007 [6] and on PASCAL VOC2012 [7].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "We evaluate our detection system on PASCAL VOC2007 [6] and on PASCAL VOC2012 [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 30, "context": "During the presentation of the results, we will use as baseline either the Original candidate box region alone (figure 3a) and/or the R-CNN framework with VGG-Net [31].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "We note that, when the Original candidate box region alone is used then the resulted model is a realization of the SPP-Net [13] object detection framework with the 16-layers VGG-Net [31].", "startOffset": 123, "endOffset": 127}, {"referenceID": 30, "context": "We note that, when the Original candidate box region alone is used then the resulted model is a realization of the SPP-Net [13] object detection framework with the 16-layers VGG-Net [31].", "startOffset": 182, "endOffset": 186}, {"referenceID": 35, "context": "[36], our MR-CNN & S-CNN model scores 1 point higher than their best performing method that includes generation of extra box proposals via Bayesian optimization and structured loss during the fine-tuning of the VGG-Net.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "This metric was proposed from [36] in order to reveal the localization capability of their method.", "startOffset": 30, "endOffset": 34}, {"referenceID": 35, "context": "From the table we observe that each of our modules exhibit very good localization capability, which was our goal when designing them, and our overall system exceeds in that metric the approach of [36].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "[16] to analyse the detection errors of our system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] to analyse the detection errors of our system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "In this experiment, we estimate the correlation between the IoU overlap of box proposals [34] (with the", "startOffset": 89, "endOffset": 93}, {"referenceID": 35, "context": "Best approach of [36] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "Best approach of [36] & bbox reg.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "R-CNN with VGG-Net from [36] 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "Best approach of [36] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "Best approach of [36] & bbox reg.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "Table 4: Correlation between the IoU overlap of selective search box proposals [34] (with the closest ground truth bounding box) and the scores assigned to them.", "startOffset": 79, "endOffset": 83}, {"referenceID": 6, "context": "In table 6, we compare our detection system against other published work on the test set of PASCAL VOC2012 [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 9, "context": "R-CNN [10] with VGG-Net & bbox reg.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "Network In Network [21] VOC12 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 35, "context": "Best approach of [36] & bbox reg.", "startOffset": 17, "endOffset": 21}, {"referenceID": 27, "context": "Faster R-CNN [28] VOC07+12 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "NoC [29] VOC07+12 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "Fast R-CNN [9] VOC07+12 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 27, "context": "Faster R-CNN [28] VOC07+12 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Fast R-CNN & YOLO [27] VOC07+12 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Deep Ensemble COCO [11] VOC07+12, COCO [22] 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Deep Ensemble COCO [11] VOC07+12, COCO [22] 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "NoC [29] VOC07+12 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "Fast R-CNN [9] VOC07+12 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 28, "context": "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.", "startOffset": 32, "endOffset": 47}, {"referenceID": 8, "context": "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.", "startOffset": 32, "endOffset": 47}, {"referenceID": 27, "context": "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.", "startOffset": 32, "endOffset": 47}, {"referenceID": 26, "context": "Approaches contemporary to ours [29, 9, 28, 27], train their models with extra data in order to improve the accuracy of their systems.", "startOffset": 32, "endOffset": 47}, {"referenceID": 33, "context": "Specifically, we trained our models on VOC 2007 and 2012 train+val datasets using both selective search [34] and EdgeBox [39] proposals.", "startOffset": 104, "endOffset": 108}, {"referenceID": 38, "context": "Specifically, we trained our models on VOC 2007 and 2012 train+val datasets using both selective search [34] and EdgeBox [39] proposals.", "startOffset": 121, "endOffset": 125}], "year": 2015, "abstractText": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.", "creator": "LaTeX with hyperref package"}}}