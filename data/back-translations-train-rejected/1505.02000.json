{"id": "1505.02000", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2015", "title": "Deep Learning for Medical Image Segmentation", "abstract": "This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional \"stacked 2D\" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular \"tri-planar\" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.", "histories": [["v1", "Fri, 8 May 2015 11:35:53 GMT  (271kb,D)", "http://arxiv.org/abs/1505.02000v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["matthew lai"], "accepted": false, "id": "1505.02000"}, "pdf": {"name": "1505.02000.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Medical Image Segmentation", "authors": ["Matthew Lai"], "emails": [], "sections": [{"heading": null, "text": "This report provides an overview of the current state of the art in deep learning architectures and optimization techniques, using the ADNI hippocampus MRI dataset as an example of comparing the effectiveness and efficiency of different Convolutionary Architectures to the task of patch-based three-dimensional hippocampus segmentation, which is important for diagnosing Alzheimer's disease. We found that a slightly unconventional \"stacked 2D\" approach provides much better classification performance than simple 2D patches without requiring much more computing power. We also examined the popular \"triplanar\" approach used in some recently published studies, and found that it delivers much better results than the 2D approaches, but also has a moderate increase in computing power demand. Finally, we examined a full 3D architecture of the revolutionary architecture and found that it delivers slightly better results than the triplanar approach, but at the cost of a very significant increase in computing power required."}, {"heading": "1 Introduction 5", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Traditional Neural Networks 5", "text": "2.1 Network Architectures............................................. 52.2 Network Nodes and Activation Functions......................................................................................"}, {"heading": "3 Deep Learning 9", "text": "3.1. Why build deep networks?......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Hippocampus Segmentation 14", "text": "The question of whether it is the right decision is not yet decided, the question is whether it is the right decision or whether it is the right decision, the question is whether it is the right decision."}, {"heading": "1 Introduction", "text": "Deep learning techniques have been applied to a variety of problems in recent years [1] - particularly in computer vision [2], natural language processing [3] and computer-aided audio analysis [4]. In many of these applications, algorithms based on deep learning have outperformed previous state-of-the-art performance. At the heart of all deep learning algorithms is the domain-independent idea of using hierarchical levels of learned abstraction to efficiently accomplish high-level tasks [1]. This report first provides an overview of traditional concepts of artificial neural networks introduced in the 1980s, before more recent discoveries were presented that made deep network training practical and effective. Finally, we present the results of applying several deep architectures to the ADNI hippocampus segmentation problem and comparing their classification and computing power requirements."}, {"heading": "2 Traditional Neural Networks", "text": "Artificial neural networks (ANN) are a machine learning technique inspired and loosely based on biological neural networks (BNN). Although they are similar in the sense that they both use a large number of identical and interconnected simple computing units to achieve high performance in complex tasks, modern ANNs are optimized for efficient implementation on electronic computers so that they bear little resemblance to their biological counterparts. In particular, time-dependent integrate and fire mechanisms in BNNs have been replaced by stationary state values that represent the frequency of firing, and most ANNs also have vastly simplified connection architectures that allow efficient propagation. Most current ANN architectures do not allow connecting loops (with the notable exception of recursive neural networks that use loops to model temporal correlations [5], and also allow non-evolving algorithms to break during training and architectural training [6]."}, {"heading": "2.1 Network Architectures", "text": "In a typical neural network, nodes are placed in layers, with the first layer being the input layer and the last layer being the output layer. Input nodes are special in that their outputs simply have the value of the corresponding characteristics in the input vectors. Input and output layers are generally considered to be a fixed layer that has a three-dimensional input layer (x, y, z) and a binary output, which is a possible network design to have 3 input layers, and an output node is usually fixed. With only one input layer and one output layer connecting all input layers, the network is essentially multiplied by a matrix."}, {"heading": "2.2 Network Nodes and Activation Functions", "text": "In a neural network, each node (next to the input nodes) has one or more scalar inputs and one output. Each connection between the nodes has a scalar weight, and each node has a bias to shift the point of activation.f (\u2211 wi \u0445 xi + b) (1) The output of each node is calculated, as shown in Equation 1, where Xi's inputs are to the node, wi's are the weights of the associated link, b is a bias associated with the node, and f (x) is a function associated with the node, known as an activation function.There are a few activation functions in widespread use. For output nodes in regression networks, a linear activation function (e.g. y = x) is most commonly used to give these networks a range of all real numbers."}, {"heading": "2.3 Training Neural Networks", "text": "This study focuses on gradient methods, such as those used much more frequently recently, and is generally much faster than well.As mentioned in Section 2.2, each node in the network has a weight associated with each incoming link and a scalar bias; the weight and bias of a node are the parameters of the node. If we merge the weights and biases of all nodes in a network into one vector, it completely defines the behavior of a network (for a given set of hyperparameters, ie. the network architecture).y = f (2) When the number of hyperparameters (network architecture) is encoded into a function f (), we can define the output of the network, as shown in Eq.2, where y and x are the output and input of vectors."}, {"heading": "2.4 Regularization", "text": "If the training set is limited in size, as is usually the case, it is dangerous to exercise without restriction, as the network will eventually begin to model sounds in the training set and is too specialized to generalize beyond the training set. A popular method to combat overadjustment is regulation - the idea of encouraging weights to have smaller values. The most common form of regulation is L2 regulation, where the L2 standard (the parameter vector) is added to the error function, as shown in Equation 6, which is a parameter that controls the strength of the regulation and needs to be tuned. If it is too low, the network would overfit. If it is too high, the network would be subordinated. Other standards such as L1 and L12 are also used with varying effects."}, {"heading": "3 Deep Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Why Build Deep Networks?", "text": "As already mentioned in Section 2.1, a neural network with two hidden layers is theoretically already a universal function approximator that is able to approximate any function, whether continuous or not, with arbitrary accuracy. Considering this, it may seem pointless to track networks with more hidden layers. The main advantage of using deep networks is node efficiency - it is often possible to approximate complex functions with the same accuracy by using a deeper network with much less total nodes compared to a 2-hidden layer network with very large hidden layers. In addition to the computing advantage, a model with a lower degree of freedom (number of parameters) requires a smaller dataset for training [16], and the size of the training set is often a limiting factor in neural network training. Intuitively, the reason that a smaller and deeper network may be more effective than a large number of nodes is that"}, {"heading": "3.2 Vanishing Gradients", "text": "The fact that deeper networks are more computationally efficient has been known for a very long time, and deep networks were tried as early as 1980 [17]. In 1989, LeCun et al. successfully applied a 3-layer network to ZIP code recognition, but could not scale to a higher number of layers or more complex problems due to very slow education, the cause of which was not underestimated. In 1991, Hochreiter identified the problem and called it \"vanishing gradients\" [18, 19]. Essentially, when errors are propagated back from the output layer, it multiplies with derivatives of the activation function at the time of activation. As soon as the propagation reaches a node in saturation (where the derivative is close to 0), the error is reduced to the level of noise, and nodes behind the saturated node path are extremely slow."}, {"heading": "3.2.1 Solution 1: Layer-wise Pre-training", "text": "The idea is to first train each layer in an unattended and greedy way to try to identify application-independent features from each layer, before finally training the entire network on labeled data [20]. The way he implements the idea is with a generative model, in the form of a restricted Boltzmann machine (RBM), where each layer contains a series of binary variables with associated probability distributions, and each layer is trained to predict the previous layer with an algorithm known as contrastive divergence [20]. Another idea in the same vein is to use a neural network to reproduce the original input [21], which allows the reuse of all neural network techniques that have already been developed (as opposed to RBM)."}, {"heading": "3.2.2 Solution 2: Rectified Linear Activation Units", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "3.3 DropOut", "text": "In 2012, Hinton et al. proposed a technique that greatly improves network performance in cases where there is limited training data. [22] The idea stems from the observation that if the training set is small, there will be many possible models that will do well on the training set, but only some will do well on the test set. The traditional solution to this is to train a network of different initialization and / or subsets of the training set, and then combine their results to produce the final output (often by averaging or coordination) [23]. This approach is known to work well in improving model performance, but it is often not computationally feasible in deep learning, where the training of a single network can take many hours or days, even with a fast GPU. Many trained neural networks are also used in real-time applications, and the use of an ensemble would make the model much slower to evaluate the results."}, {"heading": "3.4 Model Compression and Training Set Augmentation", "text": "As mentioned in Section 3.3 above, networks often achieve higher performance than any constituent network, but are not mathematically feasible in most real-world applications, especially for problems with small labeled datasets where individual networks are likely to be overload.Bucilua et al. suggested that for semi-monitored problems where a large dataset is available but only a small subset is labeled, it may be advantageous to train an ensemble (or another slow and accurate model) on the labeled subset and use it to label all data before eventually training a single network (or other fast model) on the entire dataset to reproduce both the original labels and the labels added by the ensemble [24]. In applications where there is a shortage of both labeled and unlabeled data, the training set can be artificially expanded."}, {"heading": "3.5 Making Deep Nets Shallow", "text": "Continuing the theme of model compression, Ba and Caruana demonstrated in 2014 that a high-performance deep net can be used to train a flat network to perform much better than a similarly flat network trained directly on the training set [26]. Their algorithm first trains a deep net (or ensemble of deep nets) on the original training set and uses it to provide \"advanced labels\" for all entries in the training set [26]. In the case of classification problems, where the output layer is often soft, the \"advanced labels\" are inputs to the Softmax layer [26]. Inputs to the Softmax layer are protocol probabilities for each class. Finally, a flat network can be trained to predict the protocol probabilities, rather than the original class designation. This makes training for the flat network much easier, as multi-class log probabilities provide much more information than a class designation."}, {"heading": "3.6 Convolutional Neural Networks", "text": "Most of them are inspired by the observation that inputs such as images (where each pixel is an input dimension), many low operations are local, and they are not location-dependent [17]. One operation that is useful in many computer applications is edge detection. In a fully networked, deep network, the edge detector needs to be trained separately for each part of the image, although they are all likely to produce similar results."}, {"heading": "4 Hippocampus Segmentation", "text": "The hippocampus is a component of the human brain responsible for the incorporation of short-term episodic and declarative memory into long-term memory and for navigation [29]. Hippocampal segmentation is important for the diagnosis of Alzheimer's disease (AD) as it is one of the components that is first affected by the disease. A reduction in hippocampal volume can be used as a marker for AD diagnosis [30]. Humans have two hippocampuses shaped like seahorses, as shown in Figure 3. Our goal is to classify each voxel in an MRI image as a non-hippocampus, left hippocampus or right hippocampus."}, {"heading": "4.1 Methodology", "text": "We examine 3 convolutionary neural network architectures for patch-based segmentation of the ADNI Alzheimer's MRI dataset [31]. In all 3 cases, the pre- and post-processing are identical. In all 3 cases, 60% (120) of the images are used as training sets, 20% (40) as validation sets, and 20% (40) as test sets. Patches from the same image are always used in only one of the sets."}, {"heading": "4.1.1 Pre-Processing", "text": "Before we start labeling an image, we first cut it down to a rectangular delimiter field so that we can perform the masking in normalized coordinates. In the case of the ADNI dataset, all images are already in the same orientation, so no rotation is required. From all images in the training set, we determined that the hippocampi are always in the region (0.42 < x < 0.81, 0.30 < y < 0.67, 0.22 < z < 0.80) relative to each dimension of the delimiter box of their respective brain. We enlarged the region on each side by 0.03 and used it (0.39 < x < 0.84, 0.27 < y < 0.70, 0.19 < z < 0.83) as a mask. All voxels outside the mask are automatically classified as non-hippocampus. All training fields are drawn inside the mask."}, {"heading": "4.1.2 Sampling", "text": "It would be dangerous to draw training voxels evenly within the mask, because even within the mask, the vast majority of voxels are not hippocampal and therefore there would be very few positive samples. Another problem is that edge voxels (voxels at the margins between positive and negative voxels) would be severely underrepresented, although they will most likely be the most difficult to classify. Therefore, we draw samples as follows - \u2022 For 50% of samples, we draw randomly until we get a voxel where the 5x5x5 delimiter box around the voxel contains more than 1 class. \u2022 For 25% of samples, we draw randomly until we get a positive voxel \u2022 For the remaining 25% of samples, we draw randomly until we get a negative voxelThis drawing scheme ensures that none of the important voxel types is underrepresented."}, {"heading": "4.1.3 Convolutional Method 1: Stacked 2D Patches", "text": "The first method we tried is to use a stack of 2D patches around every voxel we want to try. For example, with a patch size of 24 and a number of layers of 3, we would extract three patches 24x24 - one around the voxel in question, one parallel and above and one parallel and below. Each of the layers is assigned to a Convolutionary Neural 2D Network as different channels. This method gives the network a 3D context around the voxel (in the case of stack sizes larger than 1), with relatively little space. However, the network is not convolutional in the third dimension. No max pooling layer is used, as the network performs slightly worse at each Convolutionary Layer, 50 5x5 cores in the second Convolutionary Layer, a fully connected layer of 1000 nodes, and finally a Softmax layer for exponential normalization."}, {"heading": "4.1.4 Convolutional Method 2: Tri-planar Patches", "text": "The second method we have tried is the traplanar method used by Prasoon et al. and Roth et al. in other medical imaging applications [32, 33]. For each voxel, we extract 3 square patches around the voxel, perpendicular to each axis. For example, if we aim for a patch size of 24, we would extract a 24x24 patch at the x-y level in the center of the voxel in question and a 24x24 patch at the x-z level and another 24x24 patch at the y-z level. Since the corresponding pixels from the 3 patches are not spatially correlated in this case, we use a network architecture consisting of 2 sinuous layers (20 5x5 cores and 50 5x5 cores) for each of the three patches without connecting them to the very end, where we feed all their results into a fully interconnected layer for final classification."}, {"heading": "4.1.5 Convolutional Method 3: 3D Patches", "text": "This approach is an intuitive extension of the 2D approach in 3D. For each voxel we want to try, we take a 3D patch with the same length around the voxel on each side. At a patch size of 24, we would extract 24x24x24 patches. The network architecture consists of 20 5x5x5 cores in the first convective layer, 50 5x5x5 cores in the second convective layer, then a 1000 node fully connected layer as before. No max pooling is used."}, {"heading": "4.1.6 Image Labeling", "text": "After the network has been trained to label an image, patches are extracted for each voxel in the mask region (in the right format for the network architecture used) and the result is used to label the voxel. Any voxel outside the mask region is automatically classified as negative."}, {"heading": "4.1.7 Training", "text": "All network trainings are performed with a standard stochastic gradient descent with a batch size of 50 and a fixed learning rate of 1. At the beginning of the training, the discontinuation iteration is set to 1 validation period. Validation is performed after each run of the training set (24,000 patches). Each time a validation score improves the current best validation value by more than 1% (erroneously, not classification rate), the discontinuation iteration is set to double the current number of iterations, meaning that the training will only be completed if there is no significant improvement in at least the second half of the elapsed period."}, {"heading": "4.1.8 Post-Processing", "text": "In addition to comparing the raw performance after labeling by Convolutionary Neural Networks, we also want to see what performance we can achieve after simple post-labeling to clean up the results. Post-labeling is the same for all 3 Convolutionary Architectures. For each labeled image, we first calculate the center of all voxels labeled Left-Hippocampus and the center of all voxels labeled Right-Hippocampus. We then divide the image into blobs (connected voxels with the same classification) and check its size for each blob. If a blob is smaller than a certain threshold (in our case 500 voxels) and the labeling is negative (non-hippocampus), it will be re-labeled as the closest hippocampus (based on centrification). If a blob is smaller than the threshold and the labeling is positive (hippocampus), it will be re-labeled as the hippocampus (non-hippocampus) based on that hippocampus, which is clearly positioned as the next vacuum."}, {"heading": "4.2 Results", "text": "In fact, the number of layers that have little effect in terms of time per iteration is very high, but there are very clear improvements that go from 1 to 3 layers, but there are no clear improvements that go beyond 3 layers. However, we also point out that the number of layers has little effect on speed per iteration. All the labeling results are on one of the images in the test phase, with 1755 positive vocabulary, and 1393947 negative values that are able to make the total number of positive and negative vocabulary are the same that we are able to test the images in the test phase, with 1755 positive vocabulary, and 1393947 negative vocabulary."}, {"heading": "5 Conclusion and Future Work", "text": "In this project, we investigated the use of three different convolutionary network architectures for patch-based segmentation of the hippocampal region in MRI images. We discovered that the popular triplanar approach offers a good trade-off between accuracy and training time. While the 3D approach performs slightly better in patch classification, it does not seem to perform as well in the labeling of an overall image. This is most likely due to the scanning method that alters earlier probabilities of the class presented to the training algorithm, and if this problem is solved, the 3D approach should perform slightly better than the triplanar approach also in the labeling of the overall image, but with much higher computing power required. There are many possible avenues for future investigations. For example, there are available learning rate planning algorithms that can significantly shorten training time without compromising the quality of results, as Zeiler's famous ADADELTA algorithm 35 demonstrates for each [learning weight for an independent]."}], "references": [{"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["Itamar Arel", "Derek C Rose", "Thomas P Karnowski"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Honglak Lee", "Peter Pham", "Yan Largman", "Andrew Y Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Tuning of the structure and parameters of a neural network using an improved genetic algorithm", "author": ["Frank Hung-Fat Leung", "Hak-Keung Lam", "Sai-Ho Ling", "Peter Kwong-Shun Tam"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Evolving networks: Using the genetic algorithm with connectionist learning", "author": ["Richard K Belew", "John McInerney", "Nicol N Schraudolph"], "venue": "In In. Citeseer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "Combinations of genetic algorithms and neural networks: A survey of the state of the art", "author": ["J David Schaffer", "Darrell Whitley", "Larry J Eshelman"], "venue": "In Combinations of Genetic Algorithms and Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "A learning algorithm for evolving cascade neural networks", "author": ["Vitaly Schetinin"], "venue": "Neural Processing Letters,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["Moshe Leshno", "Vladimir Ya Lin", "Allan Pinkus", "Shimon Schocken"], "venue": "Neural networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "A weight initialization method for improving training speed in feedforward neural network", "author": ["Jim YF Yam", "Tommy WS Chow"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Gradient descent: Second-order momentum and saturating error", "author": ["Barak A Pearlmutter"], "venue": "Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "Rprop-a fast adaptive learning algorithm", "author": ["Martin Riedmiller", "Heinrich Braun"], "venue": "In Proc. of ISCIS VII), Universitat. Citeseer,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Estimating the dimension of a model", "author": ["Gideon Schwarz"], "venue": "The annals of statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1978}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological cybernetics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1980}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Sepp Hochreiter"], "venue": "Master\u2019s thesis, Institut fur Informatik, Technische Universitat,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Learning multiple layers of representation", "author": ["Geoffrey E Hinton"], "venue": "Trends in cognitive sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Ensembling neural networks: many could be better than all", "author": ["Zhi-Hua Zhou", "Jianxin Wu", "Wei Tang"], "venue": "Artificial intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu"], "venue": "arXiv preprint arXiv:1503.01838,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y-Lan Boureau", "Jean Ponce", "Yann LeCun"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Episodic and declarative memory: role of the hippocampus. Hippocampus", "author": ["Endel Tulving", "Hans J Markowitsch"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "Atlas-based hippocampus segmentation in alzheimer\u2019s disease and mild cognitive impairment", "author": ["Owen T Carmichael", "Howard A Aizenstein", "Simon W Davis", "James T Becker", "Paul M Thompson", "Carolyn Cidis Meltzer", "Yanxi Liu"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Standardization of analysis sets for reporting results from adni mri data", "author": ["Bradley T Wyman", "Danielle J Harvey", "Karen Crawford", "Matt A Bernstein", "Owen Carmichael", "Patricia E Cole", "Paul K Crane", "Charles DeCarli", "Nick C Fox", "Jeffrey L Gunter"], "venue": "Alzheimer\u2019s & Dementia,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network", "author": ["Adhish Prasoon", "Kersten Petersen", "Christian Igel", "Fran\u00e7ois Lauze", "Erik Dam", "Mads Nielsen"], "venue": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "A new 2.5 d representation for lymph node detection using random sets of deep convolutional neural network observations", "author": ["Holger R Roth", "Le Lu", "Ari Seff", "Kevin M Cherry", "Joanne Hoffman", "Shijun Wang", "Jiamin Liu", "Evrim Turkbey", "Ronald M Summers"], "venue": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Traffic sign recognition with multi-scale convolutional networks", "author": ["Pierre Sermanet", "Yann LeCun"], "venue": "In Neural Networks (IJCNN), The 2011 International Joint Conference on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "Deep learning techniques have been applied to a wide variety of problems in recent years [1] - most prominently in computer vision [2], natural language processing [3], and computational audio analysis [4].", "startOffset": 202, "endOffset": 205}, {"referenceID": 0, "context": "At the heart of all deep learning algorithms is the domain-independent idea of using hierarchical layers of learned abstraction to efficiently accomplish high level tasks [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 169, "endOffset": 172}, {"referenceID": 5, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 328, "endOffset": 337}, {"referenceID": 6, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 328, "endOffset": 337}, {"referenceID": 7, "context": "Most current ANN architectures don\u2019t allow loops in connections (with the notable exception of recurrent neural networks which uses loops to model temporal correlations [5]), and also don\u2019t allow connections to be made and broken during training (with the notable exception of evolving architectures based on genetic algorithms [6, 7, 8]).", "startOffset": 328, "endOffset": 337}, {"referenceID": 8, "context": "Methods have been proposed for automatic hyperparameter tuning, such as evolving cascade networks [9], which trains a large network through an iterative process, by first starting with a minimal network, and in each iteration, train a few \u201dcandidate\u201d networks that have more nodes in different layers, and keeping the best.", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "There are also tuning methods based on genetic algorithms with links turned on and off by each bit in the genes [6, 7, 8].", "startOffset": 112, "endOffset": 121}, {"referenceID": 6, "context": "There are also tuning methods based on genetic algorithms with links turned on and off by each bit in the genes [6, 7, 8].", "startOffset": 112, "endOffset": 121}, {"referenceID": 7, "context": "There are also tuning methods based on genetic algorithms with links turned on and off by each bit in the genes [6, 7, 8].", "startOffset": 112, "endOffset": 121}, {"referenceID": 9, "context": "It has been proven that a network with 1 hidden layer can approximate any continuous (in feature space) function to any accuracy, and a network with 2 hidden layers can approximate any function to any accuracy [10, 11].", "startOffset": 210, "endOffset": 218}, {"referenceID": 10, "context": "It has been proven that a network with 1 hidden layer can approximate any continuous (in feature space) function to any accuracy, and a network with 2 hidden layers can approximate any function to any accuracy [10, 11].", "startOffset": 210, "endOffset": 218}, {"referenceID": 11, "context": "In practice, although the logistic function is more biologically plausible, hyperbolic tangent usually allows faster training since being linear around 0 means nodes will not start training in saturation (which would make training much slower) even if inputs are zero or negative [12].", "startOffset": 280, "endOffset": 284}, {"referenceID": 12, "context": "One popular method is to draw from the uniform distribution (\u2212 a \u221a din , a \u221a din ), where a is chosen based on the shape of the activation function (where it starts to saturate), and din is the number of inputs to the node [13].", "startOffset": 223, "endOffset": 227}, {"referenceID": 13, "context": "down the bottom of a valley) [14].", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "5) if the gradient has changed signs [15].", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "An initial learning rate still needs to be chosen, but it doesn\u2019t significantly affect training time or result [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Besides computational benefits, a model with a smaller degree of freedom (number of parameters) requires a smaller dataset to train [16], and size of the training set is often a limiting factor in neural network training.", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "The fact that deeper networks are more computationally efficient has been known for a very long time, and deep networks have been attempted as early as 1980 [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "In 1991, Hochreiter identified the problem and called it \u201dvanishing gradients\u201d [18, 19].", "startOffset": 79, "endOffset": 87}, {"referenceID": 18, "context": "In 1991, Hochreiter identified the problem and called it \u201dvanishing gradients\u201d [18, 19].", "startOffset": 79, "endOffset": 87}, {"referenceID": 19, "context": "The idea is to first train each layer in an unsupervised and greedy fashion, to try to identify application-independent features from each layer, before finally training the entire network on labeled data [20].", "startOffset": 205, "endOffset": 209}, {"referenceID": 19, "context": "The way he implemented the idea is with a generative model, in the form of a restricted Boltzmann machine (RBM), where each layer contains a set of binary variables with associated probability distributions, and each layer is trained to predict the previous layer using an algorithm known as contrastive divergence [20].", "startOffset": 315, "endOffset": 319}, {"referenceID": 20, "context": "Another idea in the same vein is to use a neural network to reproduce the original input [21], which allows the reuse of all the neural network techniques that have already been developed (unlike for RBM).", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "The idea is to first start with just the input layer and one hidden layer, and train the network (using standard back-propagation gradient descent) to produce the original input, essentially training the network to model the identity function [21].", "startOffset": 243, "endOffset": 247}, {"referenceID": 20, "context": "This process is repeated to train each hidden layer, with the output of the previous layer as the input [21].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "In modern implementations, some artificial noise is often injected into the input of autoencoders to encourage robustness in the autoencoders, by testing their ability to reconstruct clean input from partially-corrupted input [21].", "startOffset": 226, "endOffset": 230}, {"referenceID": 20, "context": "Denoising autoencoders perform similarly to systems based on RBMs and their stacked variant - Deep Belief Networks (DBN) [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "Their solution to the vanishing gradients problem is to simply use an activation function that does not reduce the error as it\u2019s propagated back [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "The proposed function is the rectified linear activation function (ReLU), y = max(0, x) [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "This can be solved using L1 regularization, which not only limits the magnitude of weights, but also enforces sparsity [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "This can result in more efficient usage of available nodes, and also have computational advantages in networks using activation functions with a hard 0 saturation (such as the rectified linear activation) [12], since if a node\u2019s output is 0, it does not need to be broadcasted to nodes in the next layer.", "startOffset": 205, "endOffset": 209}, {"referenceID": 11, "context": "reports achieving slightly superior or similar performance to previous results based on pre-training [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "In this case, starting with pre-training using the entire dataset before training the whole network with the labeled subset can result in a network that generalizes better [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "proposed a technique that greatly improves network performance in cases where there is limited training data [22].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "The traditional solution to this is to train an ensemble of networks from different initialization and/or subsets of the training set, then combine their outputs to produce the final output (often by averaging, or voting) [23].", "startOffset": 222, "endOffset": 226}, {"referenceID": 21, "context": "When training is done, all nodes are re-enabled, but all weights are halved to maintain the same output range [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "In DropOut, nodes cannot assume other nodes exist, and are forced to be \u201dindependently useful\u201d [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "For example, in computer vision applications where the network should be translationally and rotationally invariant, additional training entries can be formed by translating or rotating images from the original training set [25].", "startOffset": 224, "endOffset": 228}, {"referenceID": 24, "context": "Continuing on the theme of model compression, in 2014, Ba and Caruana showed that with the help of a high performance deep network, a shallow network can be trained to perform much better than a similar shallow network that is trained directly on the training set [26].", "startOffset": 264, "endOffset": 268}, {"referenceID": 24, "context": "Their algorithm first trains a deep net (or an ensemble of deep nets) on the original training set, and use it to provide \u201dextended labels\u201d for all entries in the training set [26].", "startOffset": 176, "endOffset": 180}, {"referenceID": 24, "context": "In case of classification problems where the output layer is often softmax, the \u201dextended labels\u201d are inputs to the softmax layer [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "By modelling the log probabilities, the shallow network is also mimic-ing how the deep net (or ensemble) will generalize to unseen data, which mostly depend on the relative values of log probabilities for classes that are not the highest [26].", "startOffset": 238, "endOffset": 242}, {"referenceID": 24, "context": "However, the performance of these mimic-ing shallow networks are still not quite as good as the deep networks or ensembles they are mimic-ing [26].", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "Convolutional neural networks are a neural network architecture that uses extensive weight-sharing to reduce the degrees of freedom of models that operate on features that are spatially-correlated [17].", "startOffset": 197, "endOffset": 201}, {"referenceID": 25, "context": "This includes 2D and 3D images (and 2D videos, which can be seen as 3D images), but it has also very recently been successfully applied to natural language processing [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "Convolutional neural networks are inspired by the observation that for inputs like images (with each pixel being an input dimension), many low level operations are local, and they are not positiondependent [17].", "startOffset": 206, "endOffset": 210}, {"referenceID": 1, "context": "convolutional, max-pooling, and fully-connected [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 26, "context": "While averaging can also be used, empirical results suggest that downsampling by taking the maximum in each sub-region gives the best performance in most cases [28].", "startOffset": 160, "endOffset": 164}, {"referenceID": 27, "context": "The hippocampus is a component of human brains responsible for committing short-term episodic and declarative memory into long-term memory, as well as navigation [29].", "startOffset": 162, "endOffset": 166}, {"referenceID": 28, "context": "A reduction in hippocampal volume can be used as a marker for AD diagnosis [30].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "We explore 3 convolutional neural network architectures for patch-based segmentation on the ADNI Alzheimer\u2019s MRI dataset [31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "in other medical imaging applications [32, 33].", "startOffset": 38, "endOffset": 46}, {"referenceID": 31, "context": "in other medical imaging applications [32, 33].", "startOffset": 38, "endOffset": 46}, {"referenceID": 32, "context": "All timing results in this section are obtained on a single NVIDIA GeForce GTX Titan Black GPU, using Theano\u2019s CUDA implementation of convolutional neural networks [34].", "startOffset": 164, "endOffset": 168}, {"referenceID": 33, "context": "For example, there are learning rate scheduling algorithms available that may significantly shorten training time without affecting quality of results, such as Zeiler\u2019s famous ADADELTA algorithm [35], which assigns an independent learning rate to each weight depending on how often they are activated, so that while some oft-used connections can have slower learning rates to achieve higher precision, rarely-used connections can still be trained at a higher learning rate to reduce bias.", "startOffset": 195, "endOffset": 199}, {"referenceID": 34, "context": "One possible extension to the tri-planar architecture is to include images at multiple scales for each plane, similarly to how it was applied to traffic sign recognition by Sermanet and LeCun [36].", "startOffset": 192, "endOffset": 196}], "year": 2015, "abstractText": "This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer\u2019s Disease. We found that a slightly unconventional \u201dstacked 2D\u201d approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular \u201dtri-planar\u201d approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement. ar X iv :1 50 5. 02 00 0v 1 [ cs .L G ] 8 M ay 2 01 5", "creator": "LaTeX with hyperref package"}}}