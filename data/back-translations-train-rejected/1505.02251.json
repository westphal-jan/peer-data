{"id": "1505.02251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2015", "title": "Probabilistic Cascading for Large Scale Hierarchical Classification", "abstract": "Hierarchies are frequently used for the organization of objects. Given a hierarchy of classes, two main approaches are used, to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from the root to the predicted leaf. In this paper we propose a new approach, which extends cascade classification to predict the right leaf by estimating the probability of each root-to-leaf path. We provide experimental results which indicate that, using the same classification algorithm, one can achieve better results with our approach, compared to the traditional flat and cascade classifications.", "histories": [["v1", "Sat, 9 May 2015 09:39:04 GMT  (8kb)", "http://arxiv.org/abs/1505.02251v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["aris kosmopoulos", "georgios paliouras", "ion", "routsopoulos"], "accepted": false, "id": "1505.02251"}, "pdf": {"name": "1505.02251.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Cascading for Large Scale Hierarchical Classification", "authors": ["Aris Kosmopoulos", "Georgios Paliouras"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 5.02 251v 1 [csHierarchies are commonly used for organizing objects. Considering a hierarchy of classes, two main approaches are used to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from root to predicted leaf. In this paper, we propose a new approach that extends the cascade classification to predict the correct leaf by estimating the probability of each root-to-leaf path. We provide experimental results that suggest that using the same classification algorithm can achieve better results with our approach compared to traditional flat and cascade classifications."}, {"heading": "1 Introduction", "text": "If independence cannot be assumed, we either construct artificial hierarchies (hierarchical clustering) or classify new instances on a given hierarchy, typically representing a relationship. In this paper, we examine cases in which the hierarchy is already available. In addition, the hierarchy is a tree and the classification nodes are always the leaves of the hierarchy. We also assume that each instance belongs to only one category (uniform classification). Many researchers approach hierarchical classification problems [2, 8] by using flat classification, i.e. the hierarchical classification approaches are typically divided into smaller ones, usually a classification for each node of the hierarchy. For each of these problems, fewer features and instances are needed to form a good classification, compared to the respective flat approaches."}, {"heading": "2 Related Work", "text": "Although hierarchical mappings have many advantages, researchers typically resort to slightly hierarchical or even flat approaches [3]. One reason for this is that flat mappings are well studied, making it easier to transfer methods from this area. On the other hand, the flat use of traditional classifiers, such as SVMs, is often mathematically prohibitive [4]. Early work in hierarchical classifications, based on approaches such as shrinkage [5] and hierarchical mixing models [7], unfortunately, most of these approaches cannot be applied to major problems, at least not in the form described in the original papers. New methods, based on ideas similar to those of latent concepts [6], continue to crop up in the literature, taking into account scalability problems, but still most of the proposed methods are tested on rather small datasets with small hierarchies. Easy hierarchical approaches typically make limited use of the hierarchy."}, {"heading": "3 Probabilistic Cascading", "text": "In fact: It is not the first time that we are able to move in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in one direction, in which we are moving in which we are moving in one direction, in which we are moving in which we are moving in which we are moving in which we are moving in which we are moving in which we are moving in one direction, in which we are moving in which we are moving in which we are moving in which we are moving in which we, in which we are moving in which we are moving in which we are moving in which we are moving in which we are moving in which we, in which we are moving in which we are moving in which we are moving in which we, in which we are moving in which we are moving in which we are moving in which we, in which we are moving in which we are moving in which we, in which we are moving in which we are moving in which we, in which we are moving in which we, in which we are moving in which we are moving in which we, in which we are moving in which we are moving in which we, in which we are moving in which we, in which we are moving in which we, in which we are, in which we are which we are moving in which we, in which we are moving in which we, in which we are which we are which we are, in which we are which we, in which we are, in which we are in which we are we, which we are in which we are in which we, in which we, in which we are which we are which we are which we, in which we are in which we, in which we are which we, in which we are which we, in which we are which we, in which we are in which we, in which we are in which we, in which we are which we, in which we are in which we, in which we are which we are which we, in which we are which we are which we, in which we, in which we are in which we are"}, {"heading": "4 Experimental results", "text": "In fact, it is the case that most people are able to determine for themselves what they want and what they want. (...) It is not that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (...) It is that they pursue the same goals. (... \"(...) It is that they pursue the same goals. (...\" (...) It is that they pursue the same goals. \"(...\" (it is that they pursue the same goals. \"(...) It is that they pursue the same goals."}, {"heading": "5 Conclusions", "text": "In this paper, we present the Ppath method for hierarchical classification. Ppath addresses the disadvantages of traditional flat and cascading classification. Flat classification can be very demanding in large problems and also completely ignores the hierarchical information that can be used for better results. Standard cascades, on the other hand, are much more efficient in terms of computation, but suffer from the problem of early misclassification at the top hierarchical levels. Our approach has the same computational complexity as the cascade, while achieving better values by all tested evaluation standards. However, it is slower during classification because its complexity is similar to flat classification. The version presented in this paper is designed for tree hierarchies. As for future work, we plan to extend the idea of Ppath to DAG hierarchies. In addition, in this paper, we focused on single-label classification."}], "references": [{"title": "Maximum-margin framework for training data synchronization in large-scale hierarchical classification", "author": ["Rohit Babbar", "Ioannis Partalas", "Eric Gaussier", "Massih-Reza Amini"], "venue": "In Neural Information Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Document classification by computing an echo in a very simple neural network", "author": ["C. Brouard"], "venue": "In ICTAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The ecir 2010 large scale hierarchical classification workshop", "author": ["A. Kosmopoulos", "\u00c9. Gaussier", "G. Paliouras", "S. Aseervatham"], "venue": "SI- GIR Forum,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Support vector machines clas-  sification with a very large-scale taxonomy", "author": ["T. Liu", "Y. Yang", "H. Wan", "H. Zeng", "Z. Chen", "W. Ma"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Improving text classification by shrinkage in a hierarchy of classes", "author": ["A. McCallum", "R. Rosenfeld", "T.M. Mitchell", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Hierarchical text classification with latent concepts", "author": ["X. Qiu", "X. Huang", "Z. Liu", "J. Zhou"], "venue": "In ACL (Short Papers),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Text classification in a hierarchical mixture model for small training sets", "author": ["K. Toutanova", "F. Chen", "K. Popat", "T. Hofmann"], "venue": "In CIKM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Large-scale semantic indexing of biomedical publications", "author": ["G. Tsoumakas", "M. Laliotis", "N. Markantonatos", "I.P. Vlahavas"], "venue": "In BioASQ@CLEF,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Deep classification in large-scale text hierarchies", "author": ["G. Xue", "D. Xing", "Q. Yang", "Y. Yu"], "venue": "In SIGIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Many researchers approach hierarchical classification problems [2, 8] using flat classification, i.", "startOffset": 63, "endOffset": 69}, {"referenceID": 7, "context": "Many researchers approach hierarchical classification problems [2, 8] using flat classification, i.", "startOffset": 63, "endOffset": 69}, {"referenceID": 2, "context": "Although hierarchical classification has many advantages, typically researchers resort to mildly hierarchical or even flat approaches [3].", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "On the other hand on large scale problems, the flat use of traditional classifiers, such as SVMs, is often prohibitively expensive computationally [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "Early work in hierarchical classification focused on approaches such as shrinkage [5] and hierarchical mixture models [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "Early work in hierarchical classification focused on approaches such as shrinkage [5] and hierarchical mixture models [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "New methods based on similar ideas, such as that of latent concepts [6], continue to appear in the literature, taking also into account scalability issues.", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "Methods such as [9] use only some levels of the hierarchy, flattening the rest.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "Other approaches such as [1], alter the initial hierarchy before performing cascading in order to minimize errors at the upper levels of the hierarchy.", "startOffset": 25, "endOffset": 28}], "year": 2015, "abstractText": "Hierarchies are frequently used for the organization of objects. Given a hierarchy of classes, two main approaches are used, to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from the root to the predicted leaf. In this paper we propose a new approach, which extends cascade classification to predict the right leaf by estimating the probability of each root-to-leaf path. We provide experimental results which indicate that, using the same classification algorithm, one can achieve better results with our approach, compared to the traditional flat and cascade classifications.", "creator": "LaTeX with hyperref package"}}}