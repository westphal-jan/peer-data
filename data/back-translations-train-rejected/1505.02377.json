{"id": "1505.02377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2015", "title": "Bounded-Distortion Metric Learning", "abstract": "Metric learning aims to embed one metric space into another to benefit tasks like classification and clustering. Although a greatly distorted metric space has a high degree of freedom to fit training data, it is prone to overfitting and numerical inaccuracy. This paper presents {\\it bounded-distortion metric learning} (BDML), a new metric learning framework which amounts to finding an optimal Mahalanobis metric space with a bounded-distortion constraint. An efficient solver based on the multiplicative weights update method is proposed. Moreover, we generalize BDML to pseudo-metric learning and devise the semidefinite relaxation and a randomized algorithm to approximately solve it. We further provide theoretical analysis to show that distortion is a key ingredient for stability and generalization ability of our BDML algorithm. Extensive experiments on several benchmark datasets yield promising results.", "histories": [["v1", "Sun, 10 May 2015 13:27:36 GMT  (316kb,D)", "http://arxiv.org/abs/1505.02377v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["renjie liao", "jianping shi", "ziyang ma", "jun zhu", "jiaya jia"], "accepted": false, "id": "1505.02377"}, "pdf": {"name": "1505.02377.pdf", "metadata": {"source": "CRF", "title": "Bounded-Distortion Metric Learning", "authors": ["Renjie Liao", "Jianping Shi", "Ziyang Ma"], "emails": ["rjliao@cse.cuhk.edu.hk", "jpshi@cse.cuhk.edu.hk", "maziyang08@gmail.com", "dcszj@mail.tsinghua.edu.cn", "leojia@cse.cuhk.edu.hk"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that the solution to the problem is not only a purely theoretical question, but also a purely theoretical one."}, {"heading": "2 Related Work", "text": "Metric learning algorithms can be categorized according to various criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and non-probabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods. Based on the type of constraints, we can also classify them in pairs and triplets. Pair methods [41, 7] often add constraints to enforce distances between pairs of different points larger than a certain threshold. Representative methods in the triplet group are the largest distance to the nearest neighbor [39] and its variants [24]. They use local triplet constraints to ensure that the distance between any point and its neighbor in the different class is a metrix metrix margin."}, {"heading": "3 Bounded-Distortion Metric Learning", "text": "Before going into detail, we present the necessary notations. Sd = {M | M | Rd \u00b7 d, M > = M} is the space of all real symmetrical matrices equipped with the Frobenius inner product A \u2022 B = Tr (A > B).The positive semidefinitive (PSD) cone and the positive definitive (PD) cone are referred to as Sd + = {M | M \u00b2 Sd, M 0} or Sd + = {M | M \u00b2 Sd, M 0}.The convex quantity PdR = {M | M \u00b2 Sd + +, Tr (M) \u2264 R} is also used. The trace-bound R in PdR is a parameter to ensure a limited domain for M. Rd + stands for the d-dimensional non-negative orthant."}, {"heading": "3.1 Distortion of Metric Embedding", "text": "Definition 1. A pair (X > dX) is called metric space in which X is a series of points and dX is: X \u00b7 X \u2192 [0, \u221e) is a distance function that meets the following conditions for all xi, xj, xk, X: \u2022 dX (xi, xj) = 0, iff xi = xj, \u2022 dX (xi, xj) = dX (xi, xk).A Mahalanobis metric space is a metric space equipped with a Mahalanobis distance function, which is often the form of dX (xi, xj) + dX (xj) > M (xi \u2212 xj) > dX (xi, xk).A Mahalanographic space is a metric space equipped with a Mahalanobis distance function."}, {"heading": "3.2 Geometric Meaning of Distortion", "text": "Distortion can intuitively be seen as a measure of the complexity of metric embedding. From a geometric perspective, it has an intrinsically different meaning compared to other complexity measures of previous work, including the logDeterminant (logDet) [7] and the Frobenius Standard (Fnorm) [21]. Specifically, we focus on the analysis of the metric embedding fI \u2192 M and, for simplicity's sake, consider a logbook (V (E) = LogDet (M) \u2212 12 logDet (M), where \u03b3 is the volume of the unity sphere in Rd \u2264 1} the eigenvalues of M. It is well known that the logarithmic volume of E is logbook (V (E) = LogDet (M), where \u03b3 is the volume of the unity sphere in Rd."}, {"heading": "3.3 Pair and Triplet Constrained BDML", "text": "Considering a training data set D = (xi, yi) Ni = 1, where xi-Rd is a data point and yi is the corresponding class name, our task is to obtain a minimum distance scale (X, dM), in which dM is the marginal function defined as, dM (xi, xj) = (xi \u2212 xj) > M (xi \u2212 xj) = M (xi \u2212 xj), and M (xi \u2212 Sd + +). (xi \u2212 xj) > for the notation simplicity.Algorithm 1: A Bisection Method 1: Considering the distance of g as [L, U], tolerance > 0: 2: Repeat 3: (L + U) / 2. 4: Solve the convex feasibility problem (4).5: If problem (4) is feasible: U = g."}, {"heading": "4 A Bisection Algorithm with Multiplicative Weights Update", "text": "We now present a bisection algorithm to approximate our p-BDML and t-BDML, which essentially solves a sequence of convex feasibility problems. For each feasibility problem, we resort to the method of Multiplicative Weight Update (MWU) [23, 1], which is a meta-algorithm with many variants in different disciplines."}, {"heading": "4.1 Sequential Convex Feasibility Problems", "text": "In the following, we describe only the convex feasibility problem for p-BDML, since the formulation for t-BDML differs only by constraints. We refer to the objective function as g (M) = G \u00b7 M, where G = 1n (i, j) \u0445 S Xij. Its optimal value g * is assumed in the initial interval [L, U], in which L and U are set as 0 and g (I), respectively. I am the identity matrix. Our bisection algorithm estimates g and narrows the interval in each iteration by half. The procedure of the bisection algorithm is in Alg. 1. In particular, if g is not greater than g * in an iteration, we solve a convex feasibility problem by finding that M-PdR, \u03b1 > 0 s.t. G \u2022 M \u2264 g *, (i, j) I * KI."}, {"heading": "4.2 Multiplicative Weights Update Method", "text": "Before applying the MWU method, we reformulate the feasibility problem (4) to a general problem (1). (3) We present the problem of the slack variables M1 = M1 = M2 = MKI \u2212 M. (4) Then we construct a sparsely symmetrical problem from which the block diagonals (M1, M2 and M2) can be rewritten all constraints except P3d + 1R into (4) as Ji \u2022 Y + 1 R can be regarded as a simple constraint, as opposed to any hard constraint Ji \u2022 Y. With this change, we obtain the equivalent formulation of (4) asfind Y + 1R s.t. Ji \u2022 Y = 1, m. (5) The number of constraints is m = | I | I + 4d2 + 2. We also present a closely related feasibility problem + 1R s.t."}, {"heading": "5 Pseudo-Metric & Dimension Reduction", "text": "In this section we deal with the case of the pseudo-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "6 Generalization Bound for BDML", "text": "In order to theoretically investigate whether the distortion has an influence on the generalization capability, we derive the generalization limit of our BDML function."}, {"heading": "7 Experiments", "text": "We present empirical evaluations of our BDML algorithm on a wide range of tasks, including classification to multiple UCI datasets [2], domain matching to medium-sized datasets [35], and face verification to large-area LFW dataset [18]. Before presenting the results, we first discuss a practical strategy to accelerate the bisection method, as it is sometimes difficult to estimate in advance a narrow interval of the optimal lens value. Specifically, we select several fixed upper limits and then solve the convex feasibility problem in parallel (4). If the study succeeds, we use it to reduce the upper limit, otherwise we shrink the lower limit. This method provides us with a largely reduced interval with time costs as small as a call to the VAT solver. Note that we set parameters by cross validation. The effects of different parameters and space constraints are provided by the appendix."}, {"heading": "7.1 Classification", "text": "We first conduct classification experiments on several UCI data sets, including wine, iris, diabetes, segment and waveform, to validate the effectiveness of our BDML. We randomly split the data sets into 70% for training and 30% for testing, and report average test errors and standard deviations by repeating the random splits 10 times. We compare them with the baseline of the Euclidean metric and several strong competitors such as Xing [41], LMNN [39], ITML [7] and BoostMetric [38]. The neighborhood size of the kNN classification is 3 and all metric M are initialized as an identity matrix. We carefully set other parameters for these methods by cross-validation, and the results are listed in Table 1, which lists the best. Both our p-BDML and t-BDML record well on these data sets."}, {"heading": "7.2 Domain Adaptation", "text": "We also apply our BDML to domain fitting problems, both under unattended and semi-monitored settings. In the first case, we label samples in a source domain for training purposes and want to test the unlabeled samples in the target domain. While in the later setting, in addition to the labeled samples in a source domain, a small number of labeled samples in the target domain is accessible during the training, we use the same data set as in [35], which contains 2,533 images from 10 categories from 4 domains: Caltech, Amazon, Webcam and Dslr. We use the same 10 categories as [10] for all four domains. Experiments are repeated with 20 fixed tensile / test parts offered by [35]. We set the number of neighbors of kNN to 1 as other methods. Since the original SURF function has 800 dimensions, we perform pseudometric learning with t-BDML and initialize dimension reduction using MNN-1 methods other than Mapping."}, {"heading": "7.3 Face Verification", "text": "Finally, we apply our BDML to an unrestricted face-checking task using the large-area LFW dataset, which contains 13,233 facial images of 5,749 people. It is challenging due to the large variation of faces in lighting, expression, pose, resolution, etc. There are 6 standard protocols [18] to evaluate the results. We use the setting \"Image-Restricted, Label-Free Outside Data,\" where we can only access the provided labeled face pairs during training. Therefore, we only compare pseudometric learning of p-BDML since the use of triplet restrictions would violate this setting. The dataset is organized into 10 folders and each of them contains 300 similar face pairs and 300 different ones. Reported accuracy is achieved by cross-validation on the provided 10 folders. Current state-of-the-art methods from this point of view violate the provision."}, {"heading": "8 Conclusions", "text": "In this paper, we propose bounded distortion metric learning (BDML), which well balances the suitability of data and the distortion of metric embedding. In the metric space of Mahalanobis, BDML leads to a learning method of bound conditional number that possesses fascinating properties. We propose an efficient learning algorithm and furthermore provide theoretical analyses, which explains why distortion is a key component to ensure good generalization capability. In addition, we generalize pseudo-metric learning and propose an approximate solution based on semi-defined relaxation and a randomized algorithm. Empirical results confirm that our BDML leads to both better generalization and better conditionality. In the future, we would like to extend the distortion to non-Mahalanobis metrics and develop appropriate approximation algorithms. http: / laumin / guillearalp.people / dates.in.fr"}, {"heading": "9 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Proofs in Section 3", "text": "Proposal 1.Proof. Note that for two different points we have x, y, dM (f (x), f (y)) dI (x, y) = (x \u2212 y) (x \u2212 y) (x \u2212 y) > (x \u2212 y). It is easy to see that the above equation of the Rayleigh quotient is the PSD matrix M. For this reason we can determine that for each c-max\u03bbmin (f (x), f (y) dI (x, y) \u2264 dY (f (x), f (y) \u2264 dY (f (x, y) \u2264 cr \u00b7 dX (x, y) \u2264 cr \u00b7 dX (x, y).This results in the distortion of the metric embedding fI \u2192 M inf (x, y) \u2264 dY (f (x), f (y) \u2264 cr \u00b7 dX (x, y).Hence the distortion of the metric embedding."}, {"heading": "9.2 Proofs in Section 4", "text": "The proof for Lemma 1. Proof. if Y-Proof. is a viable solution to the problem (5), then Ji-Proof. i-Proof. i-Proof. i-Proof. i-Proof. i-Proof. i-Proof. i-Proof. i-Proof. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. i-Profin. Profini-Profin. Profinieri-Profin. Profinieri-Profin. Profinieri-Profin. Profinieri-finer. Profinieri-finer. Profinieri-finer. Profiner-finer. Profiner-finer. Profiner. Profiner-finer. Profiner. Profiner. Profiner. Profiner. Profiner. Profiner. Profiner."}, {"heading": "9.3 Proofs in Section 5", "text": "Proof of Proposition 2.Proof.We first restate problem (7) as below, min Q = > Q \u00b7 d1n (i, j). (i, j). (i, j). (i, j). (i, j). (i). (i). (i, j). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i). (i).). (i). (i).). (i). (i).). (i). (i). (i). (i). (i).). (i). (i). (i). (i)."}, {"heading": "9.4 Proofs in Section 6", "text": "The proof for Lemma 2.Proof. The specified loss function is: \"(A, Xij) = 1.\" First, note that, M \u2022 Xij = (xi \u2212 xj) > M (xi \u2212 xj) > M (xi \u2212 xj) > (xi \u2212 xj) > (xi \u2212 xj) > (xi \u2212 xj) > (xi \u2212 xj) > (xi \u2212 xj) > (xi \u2212 xj), then, by relying on the property of the Rayleigh quotient, you can achieve \"xi \u2212 xj) > (xi \u2212 xj) > (xi \u2212 xj) > (xi \u2212 xj), where the maximum and minimum property values of the Rayleigh quotient are."}, {"heading": "10 Impact of Parameters", "text": "In this section, we will examine how the performance of our BDML algorithm varies with several important parameters, including distortion K, trace R, and width \u03c1. Except for runtime, which requires extensive data, we experiment with all other parameters of the UCI Iris dataset. As in Section 7.1, we randomly split the dataset into 70% for training and 30% for testing, and report the average test error and its standard deviation by repeating the random splits 10 times."}, {"heading": "10.1 Distortion Bound K", "text": "We first examine the effects of distortion-bound K. We specify the number of iterations T = 1000, the margin of p-BDML \u00b5 = 1, the trace limit R = 100, and the width \u03c1 = 500. And we specify the mean number of conditions, the mean test error, and the standard deviation of the test error. The results are listed in Table 10.1. The table shows that the resulting mean number of conditions increases with K increases, in part because with increasing K increase the condition of the limited distortion is easier to fulfill, making the MWU method more heavily weighted other constraints, such as the boundary conditions. Therefore, the learned metric embedding is more distorted in order to adjust the training data. Furthermore, with K increases the test error initially decreases and then increases, which is consistent with our analysis in paragraph 6."}, {"heading": "10.2 Trace Bound R and Width \u03c1", "text": "We will now examine the effects of trace R and wide R. These two parameters are correlated in the sense that the width \u03c1 should not be much smaller than the trace R, because otherwise the restriction of width, i.e. the restriction of width, i.e. the restriction i, and the restriction Ji \u2022 Y (t) \u2212 hi-i \u2264 \u03c1 will not withstand. We specify the number of iteration T = 1000, the margin of p-BDML \u00b5 = 1, and the distortion K = 1000. The same measurements are shown in Table 10.2. This table shows that the overall quality of the solution of the MWU method, which corresponds to the analysis in [23], deteriorates. On the other hand, the resulting mean test error may tend to increase if the number of iteration, the greater the width, the lower the overall quality of the solution of the MWU method, which corresponds to the analysis in [23]."}, {"heading": "10.3 Running Time", "text": "Since the main component of our BDML algorithm is the MWU method, we are now investigating how the runtime of the LFW dataset differs in terms of the maximum number of iterations and the dimension of the input function. As the trace R and the width \u03c1 are fixed as 3d + 1 as mentioned above, we are implementing the algorithm as a one-thread MATLAB program. And all our experiments are performed on a server with Intel Xeon E5 CPU (2.6GHz) and 128G RAM. Specifically, we are testing the following dimension d of the input function, 10, 50, 100 and 300. And for each dimension, we set the maximum number of iterations as 100, 500, 1000, 2000 and 5000 and keep the corresponding runtime in mind. The natural logarithmy of all results is shown in Fig. 3."}, {"heading": "11 Full Results of Experiments", "text": "In this section we present the comprehensive results of our experiments. D rue Po s it ive Ra te"}, {"heading": "11.1 Domain Adaptation", "text": "For domain matching, we show the full results of all 12 possible combinations of source and target domains. Specifically, unattended and semi-monitored experiments are listed in Table 6 and Table 7, respectively. We refer the current results of Max Margin Domain Transformations (MMDT) [15] to this data set in Table 7. Note that the comparison with MMDT is somewhat unfair for our method, as it exploits the discriminatory power of a Max Margin Classifier while using the simple distance metric 1-NN classifier. However, it is promising that our BDML still achieves state-of-the-art results in some subtasks with such a simple classifier."}, {"heading": "11.2 Face Verification", "text": "In this section, we present full experimental comparisons to the LFW dataset. In Table 8, we list various published results for the setting \"Image-Restricted, Label-Free Outside Data\" of the LFW dataset. Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP + CSML [31], DML-eig-Combined [42], Convolutional DBN [17], Sub-SML [6], and DML-Combined [22]. Among them, \"Convolutional DBN\" and \"DML-Combined\" indicate that the metrics suffix \"CombFT,\" Sub-SML [6], and Dbined-ML [22], MbT-22."}, {"heading": "12 Useful Tail Bound for Chi-square Variables", "text": "We list the following sharp tail boundary for chi square variables: Lemma 7. [27] If we leave X \u0445 \u03c72d and \u2265 0, then P (X \u2212 d \u2265 2 \u221a d + 2) \u2264 exp (\u2212) P (X \u2212 d \u2264 \u2212 2 \u221a d) \u2264 exp (\u2212)."}], "references": [{"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning a mahalanobis metric from equivalence constraints", "author": ["A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall", "G. Ridgeway"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "On lipschitz embedding of finite metric spaces in hilbert space. Israel", "author": ["J. Bourgain"], "venue": "Journal of Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1985}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Similarity metric learning for face recognition", "author": ["Q. Cao", "Y. Ying", "P. Li"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Latent coincidence analysis: A hidden variable model for distance metric learning", "author": ["M. Der", "L.K. Saul"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Discriminant adaptive nearest neighbor classification", "author": ["T. Hastie", "R. Tibshirani"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "A geometric take on metric learning", "author": ["S. Hauberg", "O. Freifeld", "M.J. Black"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Efficient learning of domaininvariant image representations", "author": ["J. Hoffman", "E. Rodner", "J. Donahue", "T. Darrell", "K. Saenko"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Lfw results using a combined nowak plus merl recognizer", "author": ["G.B. Huang", "M.J. Jones", "E. Learned-Miller"], "venue": "In Faces in Real-Life Images Workshop in European Conference on Computer Vision (ECCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Learning hierarchical representations for face verification with convolutional deep belief networks", "author": ["G.B. Huang", "H. Lee", "E. Learned-Miller"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical Report", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Low-distortion embeddings of finite metric spaces", "author": ["P. Indyk", "J. Matousek"], "venue": "Handbook of Discrete and Computational Geometry,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Neighbourhood components analysis", "author": ["G. Jacob", "R. Sam", "H. Geoff", "S. Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Regularized distance metric learning: theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Discriminative deep metric learning for face verification in the wild", "author": ["Y.-P.T. Junlin Hu", "Jiwen Lu"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Efficient algorithms using the multiplicative weights update method", "author": ["S. Kale"], "venue": "PhD Thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Non-linear metric learning", "author": ["D. Kedem", "S. Tyree", "K.Q. Weinberger", "F. Sha", "G.R. Lanckriet"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["B. Laurent", "P. Massart"], "venue": "Annals of Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "Semidefinite relaxation of quadratic optimization problems", "author": ["Z.-q. Luo", "W.-k. Ma", "A.-C. So", "Y. Ye", "S. Zhang"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Approximation bounds for quadratic optimization with homogeneous quadratic constraints", "author": ["Z.-Q. Luo", "N.D. Sidiropoulos", "P. Tseng", "S. Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Cosine similarity metric learning for face verification", "author": ["H.V. Nguyen", "L. Bai"], "venue": "In Asian Conference of Computer Vision,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi", "D.J. Fleet", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2002}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Learnability, stability and uniform convergence", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "Positive semidefinite metric learning with boosting", "author": ["C. Shen", "J. Kim", "L. Wang", "A. Van Den Hengel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Descriptor based methods in the wild", "author": ["L. Wolf", "T. Hassner", "Y. Taigman"], "venue": "In Workshop on Faces in\u2019Real-Life\u2019Images: Detection, Alignment, and Recognition,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2003}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Y. Ying", "P. Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}], "referenceMentions": [{"referenceID": 37, "context": "For instance, in supervised learning, a common criterion is to learn a metric with a low empirical error [39], while in unsupervised learning, a good criterion is to learn a metric that minimizes the intra-cluster distance and simultaneously maximizes the inter-cluster distance [41].", "startOffset": 105, "endOffset": 109}, {"referenceID": 39, "context": "For instance, in supervised learning, a common criterion is to learn a metric with a low empirical error [39], while in unsupervised learning, a good criterion is to learn a metric that minimizes the intra-cluster distance and simultaneously maximizes the inter-cluster distance [41].", "startOffset": 279, "endOffset": 283}, {"referenceID": 2, "context": "Such an embedding intrinsically induces distortion - a concept in the theory of metric embedding [4], which intuitively measures the effort to reshape the metric space.", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "We approach the SDP via a bisection method, which involves solving a sequence of convex feasibility problems with fast multiplicative weights update [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 39, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 99, "endOffset": 110}, {"referenceID": 35, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 99, "endOffset": 110}, {"referenceID": 1, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 99, "endOffset": 110}, {"referenceID": 22, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 131, "endOffset": 143}, {"referenceID": 30, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 131, "endOffset": 143}, {"referenceID": 12, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 131, "endOffset": 143}, {"referenceID": 18, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 167, "endOffset": 174}, {"referenceID": 6, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 167, "endOffset": 174}, {"referenceID": 19, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 196, "endOffset": 204}, {"referenceID": 36, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 196, "endOffset": 204}, {"referenceID": 32, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 227, "endOffset": 231}, {"referenceID": 40, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 244, "endOffset": 248}, {"referenceID": 39, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 269, "endOffset": 273}, {"referenceID": 7, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 294, "endOffset": 297}, {"referenceID": 11, "context": "Metric learning algorithms can be categorized according to different criteria, such as Mahalanobis [41, 37, 3] and non-Mahalanobis [24, 32, 14] methods; probabilistic [20, 8] and nonprobabilistic [21, 38] methods; unsupervised [34], supervised [42] and semi-supervised [41] methods; and global [9] and local [13] methods.", "startOffset": 308, "endOffset": 312}, {"referenceID": 39, "context": "Pairwise methods [41, 7] often adds constraints to enforce distances between pairs of dissimilar points are larger than a given threshold.", "startOffset": 17, "endOffset": 24}, {"referenceID": 5, "context": "Pairwise methods [41, 7] often adds constraints to enforce distances between pairs of dissimilar points are larger than a given threshold.", "startOffset": 17, "endOffset": 24}, {"referenceID": 37, "context": "Representative methods in the triplet group are the largemargin nearest neighbor [39] and its variants [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "Representative methods in the triplet group are the largemargin nearest neighbor [39] and its variants [24].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "One line of research focuses on how to embed a finite metric space into normed spaces with a low distortion [4, 19], i.", "startOffset": 108, "endOffset": 115}, {"referenceID": 17, "context": "One line of research focuses on how to embed a finite metric space into normed spaces with a low distortion [4, 19], i.", "startOffset": 108, "endOffset": 115}, {"referenceID": 12, "context": "Metric learning is also related to manifold learning [14] and kernel learning [26].", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "Metric learning is also related to manifold learning [14] and kernel learning [26].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "The formal definition of metric embedding and its distortion are as follows [4, 19], Definition 2.", "startOffset": 76, "endOffset": 83}, {"referenceID": 17, "context": "The formal definition of metric embedding and its distortion are as follows [4, 19], Definition 2.", "startOffset": 76, "endOffset": 83}, {"referenceID": 5, "context": "From a geometric perspective, it possesses an intrinsically different meaning compared to other complexity measures in previous work, including the log determinant (logDet) [7] and Frobenius norm (Fnorm) [21].", "startOffset": 173, "endOffset": 176}, {"referenceID": 19, "context": "From a geometric perspective, it possesses an intrinsically different meaning compared to other complexity measures in previous work, including the log determinant (logDet) [7] and Frobenius norm (Fnorm) [21].", "startOffset": 204, "endOffset": 208}, {"referenceID": 37, "context": "Following [39], two types of neighbor points are distinguished.", "startOffset": 10, "endOffset": 14}, {"referenceID": 39, "context": "Here we only consider the former purpose and minimize the average distance of target neighbors as in [41].", "startOffset": 101, "endOffset": 105}, {"referenceID": 37, "context": "Note that the unit margin in [39] can be set as a arbitrarily positive constant, since it only affects the scale of M .", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "For each feasibility problem, we resort to the multiplicative weights update (MWU) method [23, 1], which is a meta algorithm and has many variants in different disciplines.", "startOffset": 90, "endOffset": 97}, {"referenceID": 0, "context": "For each feasibility problem, we resort to the multiplicative weights update (MWU) method [23, 1], which is a meta algorithm and has many variants in different disciplines.", "startOffset": 90, "endOffset": 97}, {"referenceID": 0, "context": "We have the Theorem 1 following [1] to guarantee that either \u0232 achieves a predefined accuracy or we claim that the original problem (5) is infeasible.", "startOffset": 32, "endOffset": 35}, {"referenceID": 28, "context": "By choosing \u03b3 = \u03c0/16|I|, = 40q \u221a rd and with appropriate rank reduction on Q\u0303\u2217 as [30], it can be shown that after running Alg.", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "To theoretically investigate whether the distortion has an impact on the generalization ability, we derive the generalization bound of our BDML following the stability analysis of learning algorithms [5, 36].", "startOffset": 200, "endOffset": 207}, {"referenceID": 34, "context": "To theoretically investigate whether the distortion has an impact on the generalization ability, we derive the generalization bound of our BDML following the stability analysis of learning algorithms [5, 36].", "startOffset": 200, "endOffset": 207}, {"referenceID": 3, "context": "Based on [5], we define the UniformReplace-One stability as, Definition 3.", "startOffset": 9, "endOffset": 12}, {"referenceID": 34, "context": "Note that our definition is stronger than the one proposed in [36], thus being more restrictive.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "Although \u03b2 does not depend decreasingly on the number of samples n, which may not be seen as stable in some sense [5], it is clear that the stability can be controlled by the distortion.", "startOffset": 114, "endOffset": 117}, {"referenceID": 33, "context": "We present empirical evaluations of our BDML algorithm on a wide range of tasks, including classification on several UCI datasets [2], domain adaptation on medium-scale datasets [35], and face verification on the large-scale LFW dataset [18].", "startOffset": 178, "endOffset": 182}, {"referenceID": 16, "context": "We present empirical evaluations of our BDML algorithm on a wide range of tasks, including classification on several UCI datasets [2], domain adaptation on medium-scale datasets [35], and face verification on the large-scale LFW dataset [18].", "startOffset": 237, "endOffset": 241}, {"referenceID": 39, "context": "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].", "startOffset": 90, "endOffset": 94}, {"referenceID": 37, "context": "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].", "startOffset": 112, "endOffset": 115}, {"referenceID": 36, "context": "We compare with the baseline of Euclidean metric and several strong competitors like Xing [41], LMNN [39], ITML [7] and BoostMetric [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 37, "context": "Especially, t-BDML is consistently better than p-BDML which validates the effectiveness of triplet constraints as suggested by [39].", "startOffset": 127, "endOffset": 131}, {"referenceID": 33, "context": "We use the same dataset as in [35], which contains 2,533 images of 10 categories from 4 domains: Caltech, Amazon, Webcam, and Dslr.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "We exploit the same 10 categories as [10] to all the four domains.", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "Experiments are repeated with 20 fixed train/test splits offered by [35].", "startOffset": 68, "endOffset": 72}, {"referenceID": 37, "context": "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A\u2192C means the adaptation from source domain A (i.", "startOffset": 117, "endOffset": 121}, {"referenceID": 33, "context": "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A\u2192C means the adaptation from source domain A (i.", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A\u2192C means the adaptation from source domain A (i.", "startOffset": 138, "endOffset": 142}, {"referenceID": 8, "context": "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A\u2192C means the adaptation from source domain A (i.", "startOffset": 148, "endOffset": 152}, {"referenceID": 40, "context": "Table 2 presents the mean test accuracy and standard errors of various metric learning based methods, including LMNN [39], ITML [35], SGF [11], GFK [10] and DML-eig [42], where A\u2192C means the adaptation from source domain A (i.", "startOffset": 165, "endOffset": 169}, {"referenceID": 8, "context": "And for fair competition, we adopt the best results of GFK under the PCA subspace setting reported by [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "There are 6 standard protocols [18] for evaluating results.", "startOffset": 31, "endOffset": 35}, {"referenceID": 39, "context": "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].", "startOffset": 130, "endOffset": 134}, {"referenceID": 40, "context": "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "In Table 3, we present results of p-BDML and other metric learning based methods including Xing [41], ITML [7], LDML [12], KISSME [25], DML-eig [42], Sub-ITML and Sub-ML [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 21, "context": "Then, equipped with the above inequality and Theorem 2 in [23], we have, for any constraint i,", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "Based on [29], to derive the SDP relaxation, we can easily observe the following, \u03b6X\u0303ij\u03b6 = Tr(X\u0303ij\u03b6\u03b6 >) = X\u0303ij \u2022 \u03b6\u03b6>.", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "First, we state the Lemma 1 in [30] as below which gives the polynomial tail bound of the left-side inequality \u03be>H\u03be < \u03b3E[\u03be>H\u03be].", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "To prove the Theorem 3, we first state one part of Lemma 9 in [5] as below.", "startOffset": 62, "endOffset": 65}, {"referenceID": 21, "context": "This may due to the fact that with the same number of iteration, the larger the width, the smaller the overall quality of the solution of the MWU method deteriorates which is matched to the analysis in [23].", "startOffset": 202, "endOffset": 206}, {"referenceID": 13, "context": "We include the state-of-art results of max-margin domain transformations (MMDT) [15] on this dataset in Table 7.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 61, "endOffset": 65}, {"referenceID": 39, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 72, "endOffset": 76}, {"referenceID": 5, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 127, "endOffset": 130}, {"referenceID": 23, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 152, "endOffset": 155}, {"referenceID": 29, "context": "Specifically, the abbreviations of these algorithms are MERL [16], Xing [41], ITML [7], LDML [12], DML-eig-SIFT [42], Sub-ITML [6], KISSME [25], Sub-ML [6], LBP+CSML [31], DML-eig-", "startOffset": 166, "endOffset": 170}, {"referenceID": 40, "context": "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].", "startOffset": 47, "endOffset": 50}, {"referenceID": 20, "context": "Combined [42], Convolutional DBN [17], Sub-SML [6] and DDML-Combined [22].", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": ", SIFT [28], LBP [33], TPLBP [40], etc.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": ", SIFT [28], LBP [33], TPLBP [40], etc.", "startOffset": 17, "endOffset": 21}, {"referenceID": 38, "context": ", SIFT [28], LBP [33], TPLBP [40], etc.", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "[27] Let X \u223c \u03c7d and \u2265 0, then P (X \u2212 d \u2265 2 \u221a d + 2 ) \u2264 exp (\u2212 ) P (X \u2212 d \u2264 \u22122 \u221a d ) \u2264 exp (\u2212 ).", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Metric learning aims to embed one metric space into another to benefit tasks like classification and clustering. Although a greatly distorted metric space has a high degree of freedom to fit training data, it is prone to overfitting and numerical inaccuracy. This paper presents bounded-distortion metric learning (BDML), a new metric learning framework which amounts to finding an optimal Mahalanobis metric space with a bounded-distortion constraint. An efficient solver based on the multiplicative weights update method is proposed. Moreover, we generalize BDML to pseudo-metric learning and devise the semidefinite relaxation and a randomized algorithm to approximately solve it. We further provide theoretical analysis to show that distortion is a key ingredient for stability and generalization ability of our BDML algorithm. Extensive experiments on several benchmark datasets yield promising results.", "creator": "LaTeX with hyperref package"}}}