{"id": "1505.02581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2015", "title": "Improving neural networks with bunches of neurons modeled by Kumaraswamy units: Preliminary study", "abstract": "Deep neural networks have recently achieved state-of-the-art results in many machine learning problems, e.g., speech recognition or object recognition. Hitherto, work on rectified linear units (ReLU) provides empirical and theoretical evidence on performance increase of neural networks comparing to typically used sigmoid activation function. In this paper, we investigate a new manner of improving neural networks by introducing a bunch of copies of the same neuron modeled by the generalized Kumaraswamy distribution. As a result, we propose novel non-linear activation function which we refer to as Kumaraswamy unit which is closely related to ReLU. In the experimental study with MNIST image corpora we evaluate the Kumaraswamy unit applied to single-layer (shallow) neural network and report a significant drop in test classification error and test cross-entropy in comparison to sigmoid unit, ReLU and Noisy ReLU.", "histories": [["v1", "Mon, 11 May 2015 12:14:40 GMT  (856kb,D)", "http://arxiv.org/abs/1505.02581v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jakub mikolaj tomczak"], "accepted": false, "id": "1505.02581"}, "pdf": {"name": "1505.02581.pdf", "metadata": {"source": "META", "title": "Improving neural networks with bunches of neurons modeled by Kumaraswamy units: Preliminary study", "authors": ["Jakub M. Tomczak"], "emails": ["JAKUB.TOMCZAK@PWR.EDU.PL"], "sections": [{"heading": "1. Introduction", "text": "It is fast becoming a critical element of high-performance systems in many areas (Bengio et al., 2013), e.g. speech recognition, object recognition, natural language processing, multi-task and domain adaptation. Typical neural networks are based on sigmoid hidden units (Bengio, 2009), but they can suffer from the disappearing gradient problem (Bengio et al., 1994). The problem can arise when lower layers of a neural network have gradients close to 0, because higher layers are usually saturated at 0 or 1. The disappearing gradients can drastically slow down the optimization process and ultimately lead to a bad local minimum. To overcome problems associated with sigmoid Copyright 2015, it is authorized. Non-linearity is recommended to us to use other types of hidden units. Recently, deep neural networks with reflected linear units (LU) have success in various applications, e.g. sensory processing (signal processing)."}, {"heading": "2. Modeling bunch of neurons: Kumaraswamy unit", "text": "It is a way in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, in which we behave in a way, and in which we behave in a way, and in which we behave in a way, and in which we behave in a way, and in which we behave in a way, and in which we behave in which way we behave in a way, and in which we behave in which way we behave in which way we behave in which way we behave in which way we behave in which way we behave in which way we behave in which way we behave in which we behave in which way we behave in which way we behave in which way and in which way we behave in which way we behave in which way we behave in which way we behave in which way we behave in which way we behave in which way we behave in which way we behave and in which way we behave in which we behave in which we behave in which we behave in which way we behave in which way we behave in which way we behave in which way we behave in which we behave in which we behave in which we behave in which way we behave in which we behave in which way we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which way we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in"}, {"heading": "3. Experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4. Conclusion", "text": "In this paper, we presented a new idea for improving neural networks with bundles of neurons, i.e. replicas of the same neurons that consist of independent elements, which can easily be modelled by generalised Kumaraswamy distribution, leading to the formulation of a new nonlinear activation function, which we call the Kumaraswamy unit. A nice feature of the Kumaraswamy unit is that it can be roughly shaped as a ReLU when parameters are correctly selected and provides values between 0 and 1. In the experiment, the performance of the neural network with the Kumaraswamy unit was compared with other activation functions, namely the Sigmoid unit, ReLU and Noisy ReLU. The results obtained at MNIST seem to confirm the superiority of the Kumaraswamy unit, although this statement needs to be confirmed by more thorough studies."}, {"heading": "Acknowledgments", "text": "The research carried out by the author was partly co-financed by the Ministry of Science and Higher Education of the Republic of Poland with grant number B40020 / I32."}], "references": [{"title": "Learning activation functions to improve deep neural networks", "author": ["Agostinelli", "Forest", "Hoffman", "Matthew", "Sadowski", "Peter", "Baldi", "Pierre"], "venue": "arXiv preprint arXiv:1412.6830,", "citeRegEx": "Agostinelli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agostinelli et al\\.", "year": 2015}, {"title": "Learning Deep Architectures for AI", "author": ["Bengio", "Yoshua"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio and Yoshua.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A new family of generalized distributions", "author": ["Cordeiro", "Gauss M", "de Castro", "M\u00e1rio"], "venue": "Journal of Statistical Computation and Simulation,", "citeRegEx": "Cordeiro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cordeiro et al\\.", "year": 2011}, {"title": "Deep sparse rectifier networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "M Ranzato", "LeCun", "Yann"], "venue": "In Computer Vision,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Kumaraswamys distribution: A beta-type distribution with some tractability advantages", "author": ["Jones", "MC"], "venue": "Statistical Methodology,", "citeRegEx": "Jones and MC.,? \\Q2009\\E", "shortCiteRegEx": "Jones and MC.", "year": 2009}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "General results for the kumaraswamy-g distribution", "author": ["Nadarajah", "Saralees", "Cordeiro", "Gauss M", "Ortega", "Edwin MM"], "venue": "Journal of Statistical Computation and Simulation,", "citeRegEx": "Nadarajah et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nadarajah et al\\.", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Rate-coded restricted boltzmann machines for face recognition", "author": ["Teh", "Yee Whye", "Hinton", "Geoffrey E"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Teh et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2001}, {"title": "On rectified linear units for speech processing", "author": ["Zeiler", "Matthew D", "M Ranzato", "Monga", "Rajat", "M Mao", "K Yang", "Le", "Quoc Viet", "Nguyen", "Patrick", "A Senior", "Vanhoucke", "Vincent", "Dean", "Jeffrey", "Hinton", "Geoffrey E"], "venue": "In ICASSP,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Deep neural networks are quickly becoming a crucial element of high performance systems in many domains (Bengio et al., 2013), e.", "startOffset": 104, "endOffset": 125}, {"referenceID": 2, "context": "Typical neural networks are based on sigmoid hidden units (Bengio, 2009), however, they can suffer from the vanishing gradient problem (Bengio et al., 1994).", "startOffset": 135, "endOffset": 156}, {"referenceID": 14, "context": ", signal processing (Zeiler et al., 2013), sentiment analysis (Glorot et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 5, "context": ", 2013), sentiment analysis (Glorot et al., 2011), object recognition (Jarrett et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 7, "context": ", 2011), object recognition (Jarrett et al., 2009), image analysis (Nair & Hinton, 2010).", "startOffset": 28, "endOffset": 50}, {"referenceID": 10, "context": "It has been shown that piece-wise linear units, such as ReLU, can compute highly complex and structured functions (Montufar et al., 2014).", "startOffset": 114, "endOffset": 137}, {"referenceID": 9, "context": "In (Maas et al., 2013) a leaky version of ReLU was proposed.", "startOffset": 3, "endOffset": 22}, {"referenceID": 6, "context": "Further investigations with parametrized leaky ReLU (called Parametric ReLU) in (He et al., 2015) confirmed the presumption that simple ReLU may be to restrictive to learn fully successful representation.", "startOffset": 80, "endOffset": 97}, {"referenceID": 0, "context": "Recently, Agostinelli et al. (2015) went even further and proposed Adaptive Piecewise Linear Units (APLU), ReLU with a piecewise linear part for negative values.", "startOffset": 10, "endOffset": 36}, {"referenceID": 11, "context": "A suitable fashion of modeling such bunch of neurons is application of a generalized Kumaraswamy distribution (KUM-G) (Cordeiro & de Castro, 2011; Nadarajah et al., 2012).", "startOffset": 118, "endOffset": 170}, {"referenceID": 7, "context": "Recently, several alternatives to the sigmoid function have been used in numerous applications, such as, rectified linear unit (ReLU) (Jarrett et al., 2009):", "startOffset": 134, "endOffset": 156}, {"referenceID": 11, "context": "It turns out that a suitable manner of modeling a bunch of neurons as described above is a generalized Kumaraswamy distribution (Cordeiro & de Castro, 2011; Nadarajah et al., 2012).", "startOffset": 128, "endOffset": 180}, {"referenceID": 11, "context": "KUM-G perfectly fits to modeling chosen property of a complex system, such as, lifetime of an entire system (Nadarajah et al., 2012).", "startOffset": 108, "endOffset": 132}], "year": 2015, "abstractText": "Deep neural networks have recently achieved state-of-the-art results in many machine learning problems, e.g., speech recognition or object recognition. Hitherto, work on rectified linear units (ReLU) provides empirical and theoretical evidence on performance increase of neural networks comparing to typically used sigmoid activation function. In this paper, we investigate a new manner of improving neural networks by introducing a bunch of copies of the same neuron modeled by the generalized Kumaraswamy distribution. As a result, we propose novel nonlinear activation function which we refer to as Kumaraswamy unit which is closely related to ReLU. In the experimental study with MNIST image corpora we evaluate the Kumaraswamy unit applied to single-layer (shallow) neural network and report a significant drop in test classification error and test cross-entropy in comparison to sigmoid unit, ReLU and Noisy ReLU.", "creator": "LaTeX with hyperref package"}}}