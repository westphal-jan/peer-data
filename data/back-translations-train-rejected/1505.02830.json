{"id": "1505.02830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2015", "title": "Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search", "abstract": "The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree Search (MCTS), is currently the most widely used variant of MCTS. Recently, a number of investigations into applying other bandit algorithms to MCTS have produced interesting results. In this research, we will investigate the possibility of combining the improved UCB algorithm, proposed by Auer et al. (2010), with MCTS. However, various characteristics and properties of the improved UCB algorithm may not be ideal for a direct application to MCTS. Therefore, some modifications were made to the improved UCB algorithm, making it more suitable for the task of game tree search. The Mi-UCT algorithm is the application of the modified UCB algorithm applied to trees. The performance of Mi-UCT is demonstrated on the games of $9\\times 9$ Go and $9\\times 9$ NoGo, and has shown to outperform the plain UCT algorithm when only a small number of playouts are given, and rougly on the same level when more playouts are available.", "histories": [["v1", "Mon, 11 May 2015 22:59:31 GMT  (21kb)", "http://arxiv.org/abs/1505.02830v1", "To appear in the 14th International Conference on Advances in Computer Games (ACG 2015)"]], "COMMENTS": "To appear in the 14th International Conference on Advances in Computer Games (ACG 2015)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yun-ching liu", "yoshimasa tsuruoka"], "accepted": false, "id": "1505.02830"}, "pdf": {"name": "1505.02830.pdf", "metadata": {"source": "CRF", "title": "Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search", "authors": ["Yun-Ching Liu", "Yoshimasa Tsuruoka"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 150 5.02 830v 1 [cs.A I] 1 1M ay2 015"}, {"heading": "1 Introduction", "text": "The development of the Monte-Carlo Tree Search (MCTS) has significant implications for various fields of the computer game, in particular the field of the Go computer game. [6] The UCT algorithm [3] is an MCTS algorithm that combines the UCB algorithm [4] and MCTS, treating each node as a single instance of the multi-armed bandit problem. UCT algorithm is one of the most prominent variants of the UCT algorithm [6]. Several studies have recently been conducted to explore the possibility of applying other bandit algorithms to MCTS. Applying the simple regret of the bandit algorithms has shown the potential to overcome some weaknesses of the UCT algorithm [7]. Sequential halving of trees (SHOT) [8] applies the sequential halving of the algorithm [11] to MCTS."}, {"heading": "2 Applying Modified Improved UCB Algorithm to Trees", "text": "In this section we will first present the improved UCB algorithm, then make some modifications to the improved UCB algorithm and finally show how to apply the modified algorithm to the Monte Carlo tree search."}, {"heading": "2.1 Improved UCB Algorithm", "text": "The goal of the player is to maximize the total amount of the reward over T, which is defined as the total amount of the reward over T games. Bandit algorithms are guidelines that the player can pursue to achieve this goal. The distribution of the reward of each arm is unknown to the player. (The goal of the player is to maximize the total amount of the reward over T games. (The goal of the player is to minimize the total amount of the reward, which is defined as asRt = total amount of the reward.) T = 1 r-rIt is the expected mean reward of the optimal arm, and rIt is the received reward when the player decides to play arm. (The goal of the bandit algorithm may limit the cumulative regret.)"}, {"heading": "2.2 Modification of the Improved UCB Algorithm", "text": "The improved UCB algorithm tries to find the optimal arm through the process of elimination. Therefore, in order to eliminate sub-optimal arms as early as possible, it tends to devote more games to sub-optimal arms in the early stages. - This might not be ideal when MCTS occurs, especially in situations where time and resources are rather limited, because it may end up spending most of its time exploring irrelevant parts of the game tree rather than moving into more promising subtrees. - The improved UCB algorithm requires the total number of games that need to be specified beforehand, and its key properties or theoretical guarantees cannot be maintained if stopped prematurely. Since we consider each node as a single instance of the MAB problem in MCTS, internal nodes are the most likely to be stopped prematurely."}, {"heading": "2.3 Modified Improved UCB applied to Trees (Mi-UCT)", "text": "We will now introduce the application of the modified improved UCB algorithm to the Monte Carlo tree search or the Mi-UCT algorithm. Details of the MiUCT algorithm are given in Algorithm 3.The Mi-UCT algorithm adopts the same game tree expansion paradigm as the UCT algorithm, i.e., the game tree is expanded by a number of iterations, and each iteration consists of four steps: selection, expansion, simulation, and backpropagation [3]. The difference is that the tree policy is replaced by the modified improved UCB algorithm. The modified improved UCB on each node is executed in an episodic manner; overall, T0 = 2 plays on the N. algorithm in the first episode, and T + 1 = T 2 plays off the N.T point in subsequent episodes. The Mi-UCT algorithm tracks when N. should be updated, and the starting point of an anticipated update of an episode to the N.T and the expected number of the N.T by the N.T."}, {"heading": "3 Experimental Results", "text": "We will first examine how the various modifications we have made to the improved UCB algorithm affect its performance in the multi-armed bandit problem. Next, we will demonstrate the performance of the Mi-UCT algorithm against the simple UCT algorithm in the game of 9 \u00d7 9 Go and 9 \u00d7 9 NoGo."}, {"heading": "3.1 Performance on Multi-armed Bandits Problem", "text": "The experimental settings follow the multi-armed bandit test bed shown in [5]. Results are averaged over 2000 randomly generated bandit tasks. We set K = 60 to determine the conditions in which the bandit algorithms are applied in more detail. Means of each generated bandit task were randomly selected, and the reward was distributed over a normal (gaze) distribution with the mean and variability. Means of each generated bandit task were randomly selected."}, {"heading": "3.2 Performance of Mi-UCT against Plain UCT on 9 \u00d7 9 Go", "text": "We will demonstrate the performance of the Mi-UCT algorithm against the pure UCT algorithm at the game Go, which is played on a 9 x 9 board. No performance-enhancing heuristics have been used for an effective comparison of the two algorithms; the simulations are all pure random simulations with no pattern or simulation guidelines. In total, 1000 games have been played for each constant C setting of the UCT algorithm, each playing alternately in black. The total number of playouts has been set at 1000, 3000 and 5000 for both algorithms.The results are shown in Table 1. It can be observed that the performance of the Mi-UCT algorithm is quite stable compared to various constant C settings of the pure UCT algorithm and is approximately on the same level.The Mi-UCT algorithm appears to perform better when only 1000 playouts are given, but slightly deteriorates when more playouts are available."}, {"heading": "3.3 Performance of Mi-UCT against Plain UCT on 9 \u00d7 9 NoGo", "text": "We will demonstrate the performance of the Mi-UCT algorithm against the simple UCT algorithm on the game of NoGo played on a 9 x 9 board. NoGo is an abusive version of the game of Go, in which the first player who has no legal moves other than to capture the opponent's stone losses. All simulations are purely random simulations and no additional heuristics or simulation guidelines have been applied. A total of 1000 games were played for each constant C setting of the UCT algorithm, with everyone having to take turns playing black. The total number of plays was set at 1000, 3000 and 5000 for both algorithms. Results are shown in Table 2. We can observe that the Mi-UCT algorithm clearly dominates the simple UCT algorithm when only 1000 plays were given, and performance deteriorates rapidly when more plays are available, although it is roughly at the same level as the UCT simple algorithm 9 x the UCT algorithm on the Mi problem 9 x both."}, {"heading": "4 Conclusion", "text": "The improved UCB algorithm is a modification of the UCB algorithm and has a better upper limit than the UCB algorithm. Various characteristics of the improved UCB algorithm, such as early exploration and the fact that it is not an algorithm available at all times, are not ideal for direct application to MCTS. Therefore, we have made some changes to the improved UCB algorithm, making it more suitable for the task of searching for game trees. We have investigated the effects and effects of each modification through an empirical study within the framework of conventional multi-armed bandit problems.The Mi-UCT algorithm is the application of the modified improved UCB algorithm to tree search in Monte-Carlo. We have shown that it surpasses the simple UCT algorithm in both games of 9 x 9 Go and 9 x 9 NoGo, if only a small number of playouts are given, and at a comparable level with increased playouts, another way of improving the UCT algorithm would be an attempt to improve the confidence in the UCT algorithm."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics 6 (1): 4", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["P. Auer", "R. Ortner"], "venue": "Periodica Mathematica Hungarica 61, pp. 1-2", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Bandit Based Monte-carlo Planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 17th European Conference on Machine Learning (ECML\u201906), pp. 282-293", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem.Machine Learning", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Issue", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "A Survey of Monte Carlo Tree Search Methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Trans. Comp. Intell. AI Games 4(1), pp. 1-43", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "MCTS Based on Simple Regret", "author": ["D. Tolpin", "S.E. Shimony"], "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence, pp. 570-576", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequential Halving applied to Trees", "author": ["T. Cazenave"], "venue": "IEEE Trans. Comp. Intell. AI Games volPP, no.99, pp.1-1", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Minimizing Simple and Cumulative Regret in Monte-Carlo Tree Search", "author": ["T. Pepels", "T. Cazenave", "M.H.M. Winands", "M. Lanctot"], "venue": "Proceedings of Computer Games Workshop at the 21st European Conference on Artificial Intelligence", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Applying Multi Armed Bandit Algorithms to MCTS and Those Analysis", "author": ["T. Imagawa", "T. Kaneko"], "venue": "Proceedings of the 19th Game Programming Workshop (GPW-14), pp.145-150", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Almost Optimal Exploration in Multi-Armed Bandits", "author": ["Z. Karnin", "T. Koren", "S. Oren"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML\u201913), pp.1238-1246", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Cappe, A.:The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["A. Garivier"], "venue": "Proceedings of 24th Annual Conference on Learning Theory (COLT \u201911),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Thompson Sampling: An Asymptotically Optimal Finite-Time Analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "Proceedings of 23rd Algorithmic Learning Theory (ALT\u201912), pp.199-213", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "[2], with MCTS.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "The development of Monte-Carlo Tree Search (MCTS) has made significant impact on various fields of computer game play, especially the field of computer Go [6].", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "The UCT algorithm [3] is an MCTS algorithm that combines the UCB algorithm [4] and MCTS, by treating each node as a single instance of the multiarmed bandit problem.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "The UCT algorithm [3] is an MCTS algorithm that combines the UCB algorithm [4] and MCTS, by treating each node as a single instance of the multiarmed bandit problem.", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "The UCT algorithm is one of the most prominent variants of the Monte-Carlo Tree Search [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "The application of simple regret minizing bandit algorithms has shown the potential to overcome some weaknesses of the UCT algorithm [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "The sequential halving on trees (SHOT) [8] applies the sequential halving algorithm [11] to MCTS.", "startOffset": 39, "endOffset": 42}, {"referenceID": 10, "context": "The sequential halving on trees (SHOT) [8] applies the sequential halving algorithm [11] to MCTS.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "The H-MCTS algorithm [9] performs selection by the SHOT algorithm for nodes that are near to the root and the UCT algorithm for deeper nodes.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "Applications of the KLUCB [12] and Thompson sampling [13] to MCTS have also been investigated and produced some interesting results[10].", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "Applications of the KLUCB [12] and Thompson sampling [13] to MCTS have also been investigated and produced some interesting results[10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "Applications of the KLUCB [12] and Thompson sampling [13] to MCTS have also been investigated and produced some interesting results[10].", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "The improved UCB algorithm [2] is a modification of the UCB algorithm, and it has been shown that the improved UCB algorithm has a tighter regret upper bound than the UCB algorithm.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Algorithm 1 The Improved UCB Algorithm [2]", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "The bandit will produce a reward r \u2208 [0, 1] according to the arm that has been pulled.", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": "If a bandit algorithm can restrict the cumulative regret to the order of O(log T ), it is said to be optimal [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "The UCB algorithm [4], which is used in the UCT algorithm [3], is an optimal algorithm which restricts the cumulative regret to O( log(T ) \u2206 ), where \u2206 is the difference of expected reward between a suboptimal arm and the optimal arm.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "The UCB algorithm [4], which is used in the UCT algorithm [3], is an optimal algorithm which restricts the cumulative regret to O( log(T ) \u2206 ), where \u2206 is the difference of expected reward between a suboptimal arm and the optimal arm.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "The improved UCB algorithm [2] is a modification of the UCB algorithm, and it can further restrict the growth of the cumulative regret to the order of O( log(T\u2206 )", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "The Mi-UCT algorithm adopts the same game tree expansion paradigm as the UCT algorithm, that is, the game tree is expanded over a number of iterations, and each iteration consists of four steps: selection, expansion, simulation, and backpropagation [3].", "startOffset": 249, "endOffset": 252}, {"referenceID": 4, "context": "The experimental settings follow the multi-armed bandit testbed that is specified in [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "It would also be interesting to investigate the possibility of enhancing the performance of the Mi-UCT algorithm by combining it with commonly used heuristics [6] or develop new heuristics that are unique to the Mi-UCT algorithm.", "startOffset": 159, "endOffset": 162}], "year": 2015, "abstractText": "The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree Search (MCTS), is currently the most widely used variant of MCTS. Recently, a number of investigations into applying other bandit algorithms to MCTS have produced interesting results. In this research, we will investigate the possibility of combining the improved UCB algorithm, proposed by Auer et al. [2], with MCTS. However, various characteristics and properties of the improved UCB algorithm may not be ideal for a direct application to MCTS. Therefore, some modifications were made to the improved UCB algorithm, making it more suitable for the task of game tree search. The Mi-UCT algorithm is the application of the modified UCB algorithm applied to trees. The performance of Mi-UCT is demonstrated on the games of 9\u00d7 9 Go and 9\u00d7 9 NoGo, and has shown to outperform the plain UCT algorithm when only a small number of playouts are given, and rougly on the same level when more playouts are available.", "creator": "gnuplot 4.4 patchlevel 3"}}}