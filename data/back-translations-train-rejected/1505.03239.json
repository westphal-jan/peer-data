{"id": "1505.03239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2015", "title": "Feature selection using Fisher's ratio technique for automatic speech recognition", "abstract": "Automatic Speech Recognition involves mainly two steps; feature extraction and classification . Mel Frequency Cepstral Coefficient is used as one of the prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC coefficients is used as the feature vector in the classification step. But the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 MFCC as feature vector. In this paper, Fisher's ratio technique is used for selecting a subset of 12 MFCC coefficients that contribute more in discriminating a pattern. The selected coefficients are used in classification with Hidden Markov Model algorithm. The classification accuracies that we get by using 12 coefficients and by using the selected coefficients are compared.", "histories": [["v1", "Wed, 13 May 2015 04:50:27 GMT  (225kb)", "http://arxiv.org/abs/1505.03239v1", "in International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 4, No. 2, April 2015"]], "COMMENTS": "in International Journal on Cybernetics &amp; Informatics (IJCI) Vol. 4, No. 2, April 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sarika hegde", "k k achary", "surendra shetty"], "accepted": false, "id": "1505.03239"}, "pdf": {"name": "1505.03239.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sarika Hegde", "K. K. Achary", "Surendra Shetty"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijci.2015.4204 45Automatic Speech Recognition (ASR) essentially comprises two steps: feature extraction and classification (pattern recognition). Mel Frequency Cepstral Coefficient (MFCC) is used as one of the leading feature extraction techniques in ASR. Normally, the set of all 12 MFCC coefficients is used as a feature vector in the classification level. However, the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 MFCC as a feature vector. In this paper, Fisher's ratio technique is used to select a subset of 12 MFCC coefficients that contribute more to the distinction of a pattern. The selected coefficients are used in the classification with the Hidden Markov Model (HMM) algorithm. The classification accuracies we obtain by using 12 coefficients and the selected coefficients are compared with each other."}, {"heading": "KEYWORDS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Automatic Speech Recognition, Statistical Technique, Fisher\u2019s Ratio, Hidden Markov Model, Mel Frequency Cepstral Coeffecients (MFCC), Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1. INTRODUCTION", "text": "The efficient feature extraction technique extracts the feature that is able to accurately distinguish one pattern from another. There are many feature extraction techniques that are available for the representation of speech patterns in the Automatic Speech Recognition (ASR) task. Mel Frequency Cepstral Coefficients (MFCC) and Linear Predictive Coefficients (LPC) have been identified as the most successful feature of representation for the speech signal. However, recently, MFCC gained more importance than LPC due to its high discriminatory capability [2]. MFCC is one of the signal processing techniques when applied to speech sounds, extracting the 12 coefficients of speech. This set of 12 MFCC coefficients is used as a feature vector in the classification task.If the feature vector vector is of high dimension, it is reduced to lower dimensions of subspace using techniques such as the Principal Component Analysis and Discrimination Analysis (PCA)."}, {"heading": "2. METHODOLOGY", "text": "In this section, the techniques for feature extraction, F-ratio calculation and classification are explained."}, {"heading": "2.1. Mel Frequency Cepstral Coefficient", "text": "The Mel Frequency Cepstral Coefficient (MFCC) is a feature extracted by applying more than one Fourier transformation sequentially to the original signal [6] [12]. The first step is preprocessing, which consists of framing and windowing the signal. Framing is the process of breaking up the series of sample observations of an entire audio file into smaller chunks called frames [13]. We have used a frame size of 30 ms, which is normally used in speech recognition applications. For each frame range, when the size N, DFT coefficients are calculated by applying the following equation. (1) The resulting value] [miX is a complex number and the power spectrum for this is considered as, (2) The power spectrum is then transformed into a frequency scale using a filter bank consisting of triangular filters whose frequency is considered on the scale."}, {"heading": "2.3. Hidden Markov Model (HMM)", "text": "A Hidden Markov Model (HMM) is a statistical model in which the modeled system is regarded as a Markov chain with unobserved (hidden) states [6]. A Markov chain is a stochastic process in which the future state of the process is based solely on its current state. HMM is a very common technique in classification, especially in the case of sequential data processes such as speech, music, and text. HMMs are used to determine a common probability distribution via hidden state sequences S = {s1, s2,.., sT} and observed output sequences X = {x1, x2,.., xT}. The logarithm of this common distribution is defined by: (7) Here, the hidden states and observed output conveniences xt are designated as target markers and the corresponding feature vectors. The distributions P (xt | st) are typically modelled by multivariate Gaussian mixed models (GMMs)."}, {"heading": "3. RESULTS AND DISCUSSIONS", "text": "For the experiment in which most people are in a position to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in, to be in."}, {"heading": "3. CONCLUSIONS", "text": "The experimental results show that classification accuracy is improved when eight MFCC coefficients are selected on the basis of the F ratio criterion, compared to the traditional method of using twelve MFCC coefficients. Classification accuracy is influenced by the number of MFCC coefficients selected, and the F ratio technique has proven to be promising in selecting a subset of the MFCC attribute. We are conducting further experiments to analyze whether the same effect is found for consonants and simple syllables."}, {"heading": "AUTHORS", "text": "Sarika Hegde received her B.E (Information Science) degree from NMAM.I.T Nitte in 2003 and her M.Tech (Computer Science) degree from NMAM.I.T Nitte in 2007. She is currently an associate professor in the Department of Computer Applications at NMAM.I.T Nitte and also holds a Ph.D. in Automatic Speech Recognition from Mangalore University, Mangalore, India. Her area of interest includes speech recognition, audio data mining, big data and pattern recognition. Dr. K. Achary is Professor of Statistics and Biostatistics at Yenepoya Research Centre, Yenepoya University, Mangalore, India. He holds an M.Sc degree in Statistics and Ph.D. in Applied Mathematics from the Indian Institute of Science, Bangalore, India. His current research interests include Stochastic Models, Inventory Theory, Face Recognition Sciences."}], "references": [{"title": "Statistical analysis of features and classification of alphasyllabary sounds in Kannada language", "author": ["Hegde", "K.K. Achary", "S. Shetty"], "venue": "International Journal of Speech Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Pattern recognition and machine learning (Vol", "author": ["C.M. Bishop"], "venue": "4, No. 4, p. 12). New York: springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Heteroscedastic discriminant analysis and reduced rand HMMs for improved speech recognition", "author": ["N. Kumar", "A.G. Andreou"], "venue": "Speech Communication, vol. 26, pp. 283\u2013297", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": " D", "author": ["R.O. Duda", "P.E. Hart"], "venue": "G. Stork, Pattern classification, John Wiley & Sons", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": " B", "author": ["L.R. Rabiner"], "venue": "H. Juang, Fundamentals of speech recognition, Prentice Hall", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": " H", "author": ["R. Haeb-Umbach"], "venue": "Ney, \u201cLinear discriminant analysis for improved large vocabulary continuous speech recognition\u201d, In Acoustics, Speech, and Signal Processing", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Optimal linear feature transformations for semi-continuous hidden markov models.\" Acoustics, Speech, and Signal Processing", "author": ["Schukat-Talamazzini", "E. Gunter", "J. Hornegger", "H. Niemann"], "venue": "IEEE International Conference on", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Feature combination using linear discriminant analysis and its pitfalls,", "author": ["R. Schluter", "A. Zolnay", "H. Ney"], "venue": "Proceedings of the 9th International Conference on Spoken Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Automatic recognition of speakers from their voices", "author": ["B.S. Atal"], "venue": "Proceedings IEEE, 64(4), pp. 460\u2013 476", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1976}, {"title": "Auditory toolbox: A MATLAB Toolbox for auditory modeling work", "author": ["M. Slaney"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Applied Speech and Audio Processing", "author": ["I. McLoughlin"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Introduction to Machine Learning", "author": ["E. Alpaydin"], "venue": "PHI Publications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": " L", "author": ["F. Sha"], "venue": "K. Saul., Large Margin Training of Continuous Density Hidden Markov Models. In J. Keshet and S. Bengio (Eds.), Automatic speech and speaker recognition: Large margin and kernel methods. Wiley-Blackwell", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "K K Achary and S", "author": ["S. Hegde"], "venue": "Shetty., \u201cA Multiple Classifier System for Automatic Speech Recognition\u201d, International Journal of Computer Applications, 101(9):38-43, September 2014. International Journal on Cybernetics & Informatics (IJCI) Vol. 4, No. 2, April 2015 52  AUTHORS Sarika Hegde received B.E (Information Science) degree from NMAM.I.T Nitte in 2003 and M.Tech ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "But recently, MFCC gained more importance than LPC due to its high discriminating capability [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "Linear Discriminant Analysis [3][4][5] finds a linear transform by maximizing the Fisher\u2019s ratio and is found to be good in discriminating patterns.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "Linear Discriminant Analysis [3][4][5] finds a linear transform by maximizing the Fisher\u2019s ratio and is found to be good in discriminating patterns.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Linear Discriminant Analysis [3][4][5] finds a linear transform by maximizing the Fisher\u2019s ratio and is found to be good in discriminating patterns.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "46 PCA technique computes the feature vector subspace with maximum variance [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "These techniques are also used successfully in classification with HMM [7][8][9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "These techniques are also used successfully in classification with HMM [7][8][9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "These techniques are also used successfully in classification with HMM [7][8][9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "It is also sometimes used to compare the two feature extraction technique for their capability of discriminating patterns in classification [2][10][11].", "startOffset": 140, "endOffset": 143}, {"referenceID": 8, "context": "It is also sometimes used to compare the two feature extraction technique for their capability of discriminating patterns in classification [2][10][11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "The Mel Frequency Cepstral Coefficient (MFCC) is a feature extracted by applying more than one Fourier Transform sequentially to the original signal [6][12].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "The Mel Frequency Cepstral Coefficient (MFCC) is a feature extracted by applying more than one Fourier Transform sequentially to the original signal [6][12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "Framing is the process of breaking the set of sample observations of an entire audio file into smaller chunks called as frame [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "A more detailed description of the mel-frequency cepstral coefficients can be found in [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "For a feature value, a good measure of effectiveness would be the ratio of inter vowel to intra-vowel (within class) variance, often referred to as the F-ratio [10][14].", "startOffset": 160, "endOffset": 164}, {"referenceID": 8, "context": "Higher F-ratio value for an MFCC coefficient indicates that it can be used for good classification [10][11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "A Hidden Markov Model (HMM) is a statistical model in which the system being modelled is assumed to be a Markov chain with unobserved (hidden) states [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 11, "context": "The goal of parameter estimation in HMMs is to compute the optimal * \u03b8 given N pairs of observations and target label sequences {Xi, Yi} where Yi is the class label [15][16].", "startOffset": 165, "endOffset": 169}, {"referenceID": 12, "context": "The goal of parameter estimation in HMMs is to compute the optimal * \u03b8 given N pairs of observations and target label sequences {Xi, Yi} where Yi is the class label [15][16].", "startOffset": 169, "endOffset": 173}, {"referenceID": 13, "context": "More detail on how HMM is used for classification can be found in [17].", "startOffset": 66, "endOffset": 70}], "year": 2015, "abstractText": "Automatic Speech Recognition (ASR) involves mainly two steps; feature extraction and classification (pattern recognition). Mel Frequency Cepstral Coefficient (MFCC) is used as one of the prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC coefficients is used as the feature vector in the classification step. But the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 MFCC as feature vector. In this paper, Fisher\u2019s ratio technique is used for selecting a subset of 12 MFCC coefficients that contribute more in discriminating a pattern. The selected coefficients are used in classification with Hidden Markov Model (HMM) algorithm. The classification accuracies that we get by using 12 coefficients and by using the selected coefficients are compared.", "creator": "PScript5.dll Version 5.2.2"}}}