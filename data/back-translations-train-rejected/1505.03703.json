{"id": "1505.03703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2015", "title": "A PCA-Based Convolutional Network", "abstract": "In this paper, we propose a novel unsupervised deep learning model, called PCA-based Convolutional Network (PCN). The architecture of PCN is composed of several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a convolutional layer and a feature pooling layer. In the convolutional layer, the filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For the higher convolutional layers, the filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-the-art deep learning models. More importantly, since there is no back propagation for supervised finetuning, PCN is much more efficient than existing deep networks.", "histories": [["v1", "Thu, 14 May 2015 12:35:19 GMT  (3701kb,D)", "http://arxiv.org/abs/1505.03703v1", "8 pages,5 figures"]], "COMMENTS": "8 pages,5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["yanhai gan", "jun liu", "junyu dong", "guoqiang zhong"], "accepted": false, "id": "1505.03703"}, "pdf": {"name": "1505.03703.pdf", "metadata": {"source": "CRF", "title": "A PCA-Based Convolutional Network", "authors": ["Yanhai Gan", "Jun Liu", "Junyu Dong", "Guoqiang Zhong", "Qing Dao"], "emails": ["gyh5421@163.com", "liujunqd@163.com", "dongjunyu@ouc.edu.cn", "gqzhong@ouc.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for only one person to be able to integrate into politics."}, {"heading": "2 Related Work", "text": "In recent years, variations of the Constellation Network in terms of bundling and Convolutionary Operation have been proposed. Lately, unsupervised learning has been used at each stage for pre-training, which would reduce the need for convolution. As all stages have been pre-trained, the network has been refined by using stochastic gradient downward method. Many methods have been proposed to prepare filter banks of convolution in an uncontrolled feature learning mode. Convolutionary versions of sparse RBMs [Jarrett et al., 2009] [Lee et al., 2009a], sparse coding [Bruna and Mallat, 2013] and predictive sparse decomposition (PSD) [Jarrett et al., 2009] [Henaff et al., 2011] [Kavukcuoglu et al., 2009] [Kavukcuoglu et al.] constellation constellations have been reported and achieved high accuracy on several constellations."}, {"heading": "3 The PCA-Based Convolutional Network", "text": "The PCN is essentially a multi-level Convolutionary Network that can be trained in layers and unattended. It consists of cascaded extraction stages and a nonlinear output stage. Figure 1 illustrates the structure of a typical PCN with three stages, including the output stage. Each extraction stage consists of a Convolutionary Layer and a Pooling Layer. Inputs are first interwoven with PCAbased filters to create a series of Characteristic Cards. Generally, the Pooling Layer calculates the average or maximum value over a neighborhood. The purpose of a Pooling Layer is to build robustness against small distortions and to reduce the resolution of Characteristic Cards by a factor p horizontal and q vertical. Characteristic Cards propagated by the Pooling Layer are then fed as input into the next level. The final output level of the PCN includes binary hash and block histogram statistics."}, {"heading": "3.1 The first feature extraction stage in PCN", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "3.2 The second feature extraction stage in PCN", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "3.3 The output stage in PCN", "text": "In the output stage, we reconstruct character cards to form definitive representations of the input image. We use binary hash and histogram statistics (called \"hashingHist\") as in PCANet [Chan et al., 2014]. Each input character card Sl \"i up to the second stage generates L2 output cards. We binarize these output cards and calculate H (Sl\" i \"Vl), where H (.) is a heaviside step function whose value is one for positive entries and zero otherwise. For each pixel location, we treat the vector of L2 binary bits as a decimal number. This converts the L2 output values generated in the second stage back into a single integer image. For each of the L1 integer images, we partition it into B blocks. We calculate the histogram of the decimal values in each block and link all of the integer images to a single histogram according to this application."}, {"heading": "4 Experiments", "text": "In all experiments, for simplicity, a three-stage PCN (including the final output stage) is applied to different datasets, and the final output characteristics of the PCN are sent to a linear SVM for classification, and all of these configurations remain unchanged. We compared the efficiency of the PCN for different detection tasks with the same desktop PC with an Intel i5-3570 CPU and 32GB of memory."}, {"heading": "4.1 Digit Recognition based on the MNIST Datasets", "text": "Since the images in the MNIST datasets are small, we set the patching sampling interval to 1, i.e. we try a patch at each pixel location, the patch size is set to 7 x 7. In the output stage, we set the block size to 7 x 7, and we set the block overlap ratio to 0.5. The three parameters remain unchanged during the experiment. Specifically, the pooling layer is disabled at each stage of the feature extraction, and it can easily be controlled by a parameter in our code. We select an identity matrix as the indexing matrix, i.e. we make each group contain only a subset in the second stage. The digit detection in the basic MNIST dataset is a smaller subset of MNIST. It contains 10,000 training images, 2000 validation images, and 50,000 test images. We first run our experiment on the basic dataset to maximize the performance of the system."}, {"heading": "4.2 Face Recognition on the Extended Yale B Dataset", "text": "The extended Yale B dataset contains 2414 frontal face images of 38 individuals; the cropped and normalized 192 x 168 face images were taken under different lighting conditions; for each subject, we randomly selected 5 images as our test images and the rest for training; at the end, a validation group of 5 images per subject was taken from the training sets; hyperparameters were selected to maximize performance on the validation set; the system was then trained throughout the training set; at the end, the patch size was set to 5 x 5; and the number of filters in the first and second stages were set to 11 and 8; the patch sampling interval was set to 1; the Max Pooling Module used a 2 x-2 boxcar filter with a 2 x-2 down sampling step; we used non-overlapping blocks in the output stage; and the block size was used as an 8 x filter in the second stage."}, {"heading": "4.3 Texture Classification on CUReT Dataset", "text": "The CUReT texture dataset contains 61 categories of textures. Each category contains images of the same material with different poses and illumination conditions. In this experiment, according to PCANet [Chan et al., 2014], a subset of the original data with azimuthal viewing angles of less than 60 degrees was selected, generating 92 images in each class. A central 200 x 200 region was truncated from each of the selected images. The dataset was randomly divided into a training set and a test set, with 46 training images for each class. Hyperparameters were selected according to the literature. The filter size was set as 5 x 5; the patch sampling interval was set as 1. The number of filters in both stages was set to 8, and the non-overlapping block size was 50 x 50. The pooling layer was deactivated at each extraction stage. Identity matrix was used as an indexing matrix in the second stage. Accuracy reached 971%, which was higher than the AN61% result."}, {"heading": "4.4 Texture Classification on Outex Dataset", "text": "Outex is a framework for empirical evaluation of texture classification and segmentation algorithms. Problems are encapsulated in well-defined test suites with precise specifications of input and output data. Outex database contains surface textures and natural scenes. The collection of surface textures is continuously expanding. At this moment, the database contains 320 surface textures, both macro and micro textures. Many textures have variations in local color content, resulting in challenging local grayscale variations in intensity images. Some of the source textures have a large tactile dimension, which can cause significant local grayscale distortion. Each source texture is mapped using a specific process. Images used in a texture classification series are extracted from the given set of source images (certain texture classes, illuminations, spatial resolutions and rotation windings), so that the centrifugation of each side of the biampling is defined."}, {"heading": "4.5 Texture Classification on Our Dataset", "text": "A number of procedural models have been proposed and these models can produce different textures. Rendering these textures renders surface images. In view of a surface image, it is important to know which model can produce such a type of texture. Textures generated by a particular method are usually different from those generated by other methods. However, some textures generated by other models can be perceived similarly, making this a challenging classification task. Figure 4 shows sample examples of our texture data. The size of the surface images in our dataset 256 * 256. In this experiment, we are using a total of 3600 surface images that will be available along with the source code in the near future."}, {"heading": "5 Conclusion", "text": "We propose a PCA-based Convolutional Network (PCN), which essentially has the advantages of CNN [Jarrett et al., 2009] and PCANet [Chan et al., 2014], i.e. it can achieve competitive performance compared to modern methods, but is much more efficient in terms of computation. The PCN used in our experiments consists of only two levels of trait extraction and one non-linear output level. Instead of training the network through iteration methods, PCN simply uses PCA to learn filters in the folding layer. eigenvectors are used as filters to blend with the input images. Similar to other deep networks, it should be noted that proper configuration of PCN for different types of input is very important. If the training images are relatively simple in structure and have a large interval, we can use a relatively large interval to try out the patches and quickly activate the pick layer to reduce the size of the input layer."}, {"heading": "Acknowledgment", "text": "This work was supported by the National Natural Science Foundation of China (NSFC) (No. 61271405); the Ph.D. Program Foundation Of Ministry Of Education Of China (No. 20120120132110018);"}], "references": [{"title": "IEEE Transactions on", "author": ["Serge Belongie", "Jitendra Malik", "Jan Puzicha. Shape matching", "object recognition using shape contexts. Pattern Analysis", "Machine Intelligence"], "venue": "24(4):509\u2013522,", "citeRegEx": "Belongie et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "IEEE Transactions on", "author": ["Joan Bruna", "St\u00e9phane Mallat. Invariant scattering convolution networks. Pattern Analysis", "Machine Intelligence"], "venue": "35(8):1872\u20131886,", "citeRegEx": "Bruna and Mallat. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Yi Ma", "author": ["Tsung-Han Chan", "Kui Jia", "Shenghua Gao", "Jiwen Lu", "Zinan Zeng"], "venue": "Pcanet: A simple deep learning baseline for image classification? arXiv preprint arXiv:1404.3606,", "citeRegEx": "Chan et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE Transactions on", "author": ["Zhenhua Guo", "David Zhang. A completed modeling of local binary pattern operator for texture classification. Image Processing"], "venue": "19(6):1657\u20131663,", "citeRegEx": "Guo and Zhang. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Journal of Physiology-Paris", "author": ["Michael U Gutmann", "Aapo Hyv\u00e4rinen. A three-layer model of natural image statistics"], "venue": "107(5):369\u2013398,", "citeRegEx": "Gutmann and Hyv\u00e4rinen. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "volume 11", "author": ["Mikael Henaff", "Kevin Jarrett", "Koray Kavukcuoglu", "Yann LeCun. Unsupervised learning of sparse features for scalable audio classification. In ISMIR"], "venue": "page 276,", "citeRegEx": "Henaff et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "M Ranzato", "Yann LeCun"], "venue": "2009 IEEE 12th International Conference on, pages 2146\u2013 2153. IEEE,", "citeRegEx": "Jarrett et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "2009", "author": ["Koray Kavukcuoglu", "MarcAurelio Ranzato", "Rob Fergus", "Yann Le-Cun. Learning invariant features through topographic filter maps. In Computer Vision", "Pattern Recognition"], "venue": "CVPR 2009. IEEE Conference on, pages 1605\u20131612. IEEE,", "citeRegEx": "Kavukcuoglu et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Advances in neural information processing systems", "author": ["Koray Kavukcuoglu", "Pierre Sermanet", "Y-Lan Boureau", "Karol Gregor", "Micha\u00ebl Mathieu", "Yann L Cun. Learning convolutional feature hierarchies for visual recognition"], "venue": "pages 1090\u20131098,", "citeRegEx": "Kavukcuoglu et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Pca-sift: A more distinctive representation for local image descriptors", "author": ["Yan Ke", "Rahul Sukthankar"], "venue": "Computer Vision and Pattern Recognition, 2004. CVPR", "citeRegEx": "Ke and Sukthankar. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "IEEE Transactions on", "author": ["Daniel Keysers", "Thomas Deselaers", "Christian Gollan", "Hermann Ney. Deformation models for image recognition. Pattern Analysis", "Machine Intelligence"], "venue": "29(8):1422\u20131435,", "citeRegEx": "Keysers et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee et al", "2009a] Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Lee et al", "2009b] Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "localized features", "author": ["Jim Mutch", "David G Lowe. Multiclass object recognition with sparse"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, volume 1, pages 11\u201318. IEEE,", "citeRegEx": "Mutch and Lowe. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "3d human posture estimation using the hog features from monocular image", "author": ["Katsunori Onishi", "Tetsuya Takiguchi", "Yasuo Ariki"], "venue": "Pattern Recognition, 2008. ICPR 2008. 19th International Conference on, pages 1\u20134. IEEE,", "citeRegEx": "Onishi et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "2005", "author": ["Thomas Serre", "Lior Wolf", "Tomaso Poggio. Object recognition with features inspired by visual cortex. In Computer Vision", "Pattern Recognition"], "venue": "CVPR 2005. IEEE Computer Society Conference on, volume 2, pages 994\u20131000. IEEE,", "citeRegEx": "Serre et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "scaling and deformation invariant scattering for texture discrimination", "author": ["Laurent Sifre", "St\u00e9phane Mallat. Rotation"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1233\u20131240. IEEE,", "citeRegEx": "Sifre and Mallat. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IEEE Transactions on", "author": ["Dacheng Tao", "Xuelong Li", "Xindong Wu", "Stephen J Maybank. General tensor discriminant analysis", "gabor features for gait recognition. Pattern Analysis", "Machine Intelligence"], "venue": "29(10):1700\u20131715,", "citeRegEx": "Tao et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In Computer Vision and Pattern Recognition (CVPR)", "author": ["Kai Yu", "Yuanqing Lin", "John Lafferty. Learning image representations from the pixel level via hierarchical sparse coding"], "venue": "2011 IEEE Conference on, pages 1713\u20131720. IEEE,", "citeRegEx": "Yu et al.. 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 19, "context": "The most popular hand-crafted features include Gabor features [Tao et al., 2007], locally binary patterns (LBP) [Guo and Zhang, 2010], Hog [Onishi et al.", "startOffset": 62, "endOffset": 80}, {"referenceID": 3, "context": ", 2007], locally binary patterns (LBP) [Guo and Zhang, 2010], Hog [Onishi et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 16, "context": ", 2007], locally binary patterns (LBP) [Guo and Zhang, 2010], Hog [Onishi et al., 2008] and SIFT [Ke and Sukthankar, 2004].", "startOffset": 66, "endOffset": 87}, {"referenceID": 10, "context": ", 2008] and SIFT [Ke and Sukthankar, 2004].", "startOffset": 17, "endOffset": 42}, {"referenceID": 6, "context": "using \u201ddropout\u201d for regulation [Hinton et al., 2012].", "startOffset": 31, "endOffset": 52}, {"referenceID": 2, "context": "PCANet is such a variation of deep convolutional networks of which convolution filter banks in each stage are simply chosen from PCA filters [Chan et al., 2014].", "startOffset": 141, "endOffset": 160}, {"referenceID": 7, "context": "The convolutional versions of sparse RBMs [Jarrett et al., 2009] [Lee et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 1, "context": ", 2009a] , sparse coding [Bruna and Mallat, 2013] and predictive sparse decomposition(PSD) [Jarrett et al.", "startOffset": 25, "endOffset": 49}, {"referenceID": 7, "context": ", 2009a] , sparse coding [Bruna and Mallat, 2013] and predictive sparse decomposition(PSD) [Jarrett et al., 2009] [Henaff et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 5, "context": ", 2009] [Henaff et al., 2011] [Kavukcuoglu et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 8, "context": ", 2011] [Kavukcuoglu et al., 2009] [Kavukcuoglu et al.", "startOffset": 8, "endOffset": 34}, {"referenceID": 9, "context": ", 2009] [Kavukcuoglu et al., 2010] were reported and achieved high accuracy on several benchmarks.", "startOffset": 8, "endOffset": 34}, {"referenceID": 17, "context": "In [Serre et al., 2005] [Mutch and Lowe, 2006], Gabor filters were used in the first convolution layer.", "startOffset": 3, "endOffset": 23}, {"referenceID": 15, "context": ", 2005] [Mutch and Lowe, 2006], Gabor filters were used in the first convolution layer.", "startOffset": 8, "endOffset": 30}, {"referenceID": 1, "context": "Meanwhile, wavelet scattering networks (ScatNet) [Bruna and Mallat, 2013] [Sifre and Mallat, 2013] also used pre-fixed convolutional filters which were called scattering operators.", "startOffset": 49, "endOffset": 73}, {"referenceID": 18, "context": "Meanwhile, wavelet scattering networks (ScatNet) [Bruna and Mallat, 2013] [Sifre and Mallat, 2013] also used pre-fixed convolutional filters which were called scattering operators.", "startOffset": 74, "endOffset": 98}, {"referenceID": 2, "context": "One more closely related work is called PCANet [Chan et al., 2014], which simply use PCA filters in an unsupervised learning mode at the convolution layer.", "startOffset": 47, "endOffset": 66}, {"referenceID": 7, "context": "Inspired by weight sharing of receptive fields in ConvNets [Jarrett et al., 2009], for each input image, we sample a number of patches with a size of k1\u00d7k2 at every k pixel locations, i.", "startOffset": 59, "endOffset": 81}, {"referenceID": 12, "context": "This operation is reminiscent to the local contrast normalization used by ImageNet [Krizhevsky et al., 2012].", "startOffset": 83, "endOffset": 108}, {"referenceID": 4, "context": "Since high level features are the combinations and abstract of low level features [Gutmann and Hyv\u00e4rinen, 2013], we combine subsets {Sl}1 l=1 according to certain rule to form several groups.", "startOffset": 82, "endOffset": 111}, {"referenceID": 2, "context": "We use binary hashing and histogram statistics (called \u201dhashingHist\u201d) as in PCANet [Chan et al., 2014].", "startOffset": 83, "endOffset": 102}, {"referenceID": 20, "context": "HSC [Yu et al., 2011] 99.", "startOffset": 4, "endOffset": 21}, {"referenceID": 0, "context": "23 K-NN-SCM [Belongie et al., 2002] 99.", "startOffset": 12, "endOffset": 35}, {"referenceID": 11, "context": "37 K-NN-IDM [Keysers et al., 2007] 99.", "startOffset": 12, "endOffset": 34}, {"referenceID": 7, "context": "18 ConvNet [Jarrett et al., 2009] 99.", "startOffset": 11, "endOffset": 33}, {"referenceID": 1, "context": "47 ScatNet-2(SVMrbf ) [Bruna and Mallat, 2013] 99.", "startOffset": 22, "endOffset": 46}, {"referenceID": 2, "context": "In this experiment, following PCANet [Chan et al., 2014], a subset of the original data with azimuthal viewing angles less than 60 degrees was selected, thereby yielding 92 images in each class.", "startOffset": 37, "endOffset": 56}, {"referenceID": 7, "context": "We propose a PCA-based Convolutional Network (PCN), which essentially has the advantage of both CNN [Jarrett et al., 2009] and PCANet [Chan et al.", "startOffset": 100, "endOffset": 122}, {"referenceID": 2, "context": ", 2009] and PCANet [Chan et al., 2014], i.", "startOffset": 19, "endOffset": 38}], "year": 2015, "abstractText": "In this paper, we propose a novel unsupervised deep learning model, called PCA-based Convolutional Network (PCN). The architecture of PCN is composed of several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a convolutional layer and a feature pooling layer. In the convolutional layer, the filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For the higher convolutional layers, the filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-theart deep learning models. More importantly, since there is no back propagation for supervised finetuning, PCN is much more efficient than existing deep networks.", "creator": "LaTeX with hyperref package"}}}