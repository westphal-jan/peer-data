{"id": "1505.04150", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2015", "title": "Reinforcement Learning applied to Single Neuron", "abstract": "This paper extends the reinforcement learning ideas into the multi-agents system, which is far more complicated than the previously studied single-agent system. We studied two different multi-agents systems. One is the fully-connected neural network consists of multiple single neurons. Another one is the simplified mechanical arm system which is controlled by multiple neurons. We suppose that each neuron is like an agent and it can do Gibbs sampling of the posterior probability of stimulus features. The policy is optimized in a way that the cumulative global rewards are maximized. The algorithm for the second system is based on the same idea but we incorporate the physics model into the constraints. The simulation results show that for the first system our algorithm converges well. For the second system it does not converge well in a reasonable simulation time length. In summary, we took the initial endeavor to study the reinforcement learning for multi-agents system. The computational complexity is always an issue and significant amount of works have to be done in order to better understand the problem.", "histories": [["v1", "Fri, 15 May 2015 18:36:20 GMT  (138kb,D)", "http://arxiv.org/abs/1505.04150v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["zhipeng wang", "mingbo cai"], "accepted": false, "id": "1505.04150"}, "pdf": {"name": "1505.04150.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning Applied to Single Neuron", "authors": ["Zhipeng Wang", "Mingbo Cai"], "emails": ["zw12@rice.edu", "mingbo.cai@gmail.com"], "sections": [{"heading": null, "text": "I. However, INTRODUCTIONReinforcement Learning (RL) is a sub-area of machine learning through which the agent interacts with the environment in order to dynamically maximize its long-term expected rewards (1, 2). It has broad applications in a variety of emerging areas such as robotics, artificial intelligence, brain and cognitive sciences. In reality, animals make decisions through the neural system. The neural system consists of individual neurons. Each of them has some available measures, the collective action of the entire neural network determines the effect of the system (animals, robots, etc.) and through which the system makes decisions. The task for strengthening the learning algorithm is to optimize the policies under which the system takes measures to maximize its cumulative long-term rewards."}, {"heading": "II. ALGORITHMS", "text": "The merit of the algorithm lies in the application of reinforcement learning to each individual neuron in a network \u03b2 = only in the improvement of individual weight. Specifically, our idea is inspired by the gradient ascent approach for reinforcement learning. A neuron is considered an agent, receiving inputs from other neurons and external inputs.ar Xiv: 150 5.04 150v 1 [cs.A I] 1 5M ay2 015Its output is action potentials. For simplicity, we can model each neuron as a single compartment that has a membrane potential. Spike count is generated by a Poisson process, of which the shooting rate is a sigmoid function of the membrane potential. The neurons have a leaky current to reduce their membrane potential towards Resting Potential. A spike of a neuron generates either an increase or a decrease in the membrane potential in the neuron we receive input from it. The size of the change depends on the synaptic weights between them."}, {"heading": "III. SIMULATION RESULTS", "text": "First, we simulate a simple network of 5 neurons. All neurons are initially connected to random weights. Neuron 1 receives external current injection so that its shot rate largely follows the size of the injected current. Timing of the injected current is a sinusoidal signal with random frequency and initial phase in each epoch, so that the frequency across epochs follows 1 / f distribution. Global reward is defined as the dot product of spike numbers between neuron 1 and neuron 2, which are normalized by the average of their spike numbers. This reward definition achieves random detection to maximize global reward, Neuron 2 should have strong input weight of neurons to follow their activity. Neuron 1 may also have strong input weight of neurons, Neuron 2 may have strong input weight of neurons, depending on whether the neurons are shifting from other neurons to strong."}, {"heading": "IV. CONCLUSION", "text": "Our algorithm does not require prior adoption of network topology. Based on all networked neural networks, our algorithm could be applied in a favorable manner to the formation of simple networks. However, for more complex networks, the convergence time of our algorithm is unknown. To find a better reward function for more complex neural networks, rigorous mathematical proof is required."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Dr. Richard Baraniuk for his guidance and suggestions for this work. We would also like to thank the Department of Electrical and Computer Engineering at Rice University for hosting the amplification seminar."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Kernelized Value Function Approximation for Reinforcement Learning", "author": ["G. Taylor", "R. Parr"], "venue": "26th International Conference on Machine Learning (ICML),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Regularization and Feature Selection in Least-Squares Temporal Difference Learning", "author": ["J.Z. Kolter", "A. Ng"], "venue": "26th International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Greedy Algorithms for Sparse Reinforcement Learning", "author": ["C.P. Wakefield", "R. Parr"], "venue": "29th International Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Reinforcement Learning in POMDPs via Direct Gradient Ascent", "author": ["J Baxter", "P. Bartlett"], "venue": "17th International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Reinforcement Learning for Sensing Strategies", "author": ["C Kwok", "D. Fox"], "venue": "International Conference on Intelligent Robots and Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}], "referenceMentions": [], "year": 2015, "abstractText": "This paper extends the reinforcement learning ideas into the multi-agents system, which is far more complicated than the previously studied single-agent system. We studied two different multi-agents systems. One is the fully-connected neural network consists of multiple single neurons. Another one is the simplified mechanical arm system which is controlled by multiple neurons. We suppose that each neuron is like an agent and it can do Gibbs sampling of the posterior probability of stimulus features. The policy is optimized in a way that the cumulative global rewards are maximized. The algorithm for the second system is based on the same idea but we incorporate the physics model into the constraints. The simulation results show that for the first system our algorithm converges well. For the second system it does not converge well in a reasonable simulation time length. In summary, we took the initial endeavor to study the reinforcement learning for multi-agents system. The computational complexity is always an issue and significant amount of works have to be done in order to better understand the problem.", "creator": "LaTeX with hyperref package"}}}