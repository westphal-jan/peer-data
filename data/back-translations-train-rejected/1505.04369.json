{"id": "1505.04369", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2015", "title": "Shrinkage degree in $L_2$-re-scale boosting for regression", "abstract": "Re-scale boosting (RBoosting) is a variant of boosting which can essentially improve the generalization performance of boosting learning. The key feature of RBoosting lies in introducing a shrinkage degree to re-scale the ensemble estimate in each gradient-descent step. Thus, the shrinkage degree determines the performance of RBoosting.", "histories": [["v1", "Sun, 17 May 2015 08:09:43 GMT  (152kb,D)", "http://arxiv.org/abs/1505.04369v1", "11 pages, 27 figures"]], "COMMENTS": "11 pages, 27 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lin xu", "shaobo lin", "yao wang", "zongben xu"], "accepted": false, "id": "1505.04369"}, "pdf": {"name": "1505.04369.pdf", "metadata": {"source": "CRF", "title": "Shrinkage degree in L2-re-scale boosting for regression", "authors": ["Lin Xu", "Shaobo Lin", "Yao Wang", "Zongben Xu"], "emails": [], "sections": [{"heading": null, "text": "The question that has arisen in recent years is whether and how such a development can occur, and how such a development can occur. (...) The aim of this paper is to develop a concrete analysis of how to determine the degree of shrinkage. (...) It is about how to determine the degree of shrinkage. (...) It is about how to get a grip on the meaning of shrinkage. (...) It is about how to get a grip on shrinkage. (...) It is about how to get a grip on shrinkage. (...) It is about how to get a grip on shrinkage. (...) It is about how to get a grip on shrinkage. (...) It is about how to get a grip on shrinkage. (...) It is about how to get a grip on shrinkage. (...) It is about how to get a grip on shrinkage."}, {"heading": "III. THEORETICAL BEHAVIORS", "text": "In this section we present some theoretical results regarding the degree of shrinkage. First, we examine the relationship between shrinkage and generalization capability of L2RBoosting. Theoretical results show that shrinkage plays a crucial role in L2-RBoosting for regression with finite samples. Second, we analyze the advantages and disadvantages of L2RBoosting and L2-DDRBoosting. It turns out that the potential performance of L2-RBoosting is slightly better than that of L2-DDRBoosting. Finally, we propose an adaptive Parameter 4 selection strategy for shrinkage and theoretically verify its feasibility."}, {"heading": "A. Relationship between the generalization capability and shrinkage degree", "text": "It is not as if we have the infinite number of learning processes in such a space (X). The infimum of all such B's defines a norm for f'Lr1. It follows from [29] that (III.1) an interpolation space is defined that is widely used in nonlinear approximation. [26].Let L1 (S).1) Definition of the interpolation that we have used in nonlinear approximation. [29] It follows from [29] that (II.1) is an interpolation space used in nonlinear approximation."}, {"heading": "IV. NUMERICAL RESULTS", "text": "In this section, a series of simulations and real data experiments are performed to illustrate our theoretical claims."}, {"heading": "A. Simulation experiments", "text": "In this part, we first present the simulation settings, including data sets, weak learners and experimental environments. Secondly, we analyze the relationship between shrinkage and generalizability for the proposed L2-RBoosting number using an ideal performance curve. Thirdly, we compare performance of L2-Boosting, L2-RBoosting and L2-DDRBoosting. The results show that L2-RBoosting with an appropriate shrinkage rate outperforms other simulations, especially for high-dimensional data simulations. Finally, we justify the feasibility of the adaptive parameter selection strategy for shrinkage in L2RBoosting.1) Simulation Settings: In the following simulations, we generate data from the following model: Y = m (X) + class by class, (IV.1) where it proves to be standard noise and independent of X."}, {"heading": "B. Real data experiments", "text": "We have proven that L2-RBoosting is better than L2-Boosting and L2-DDRBoosting based on 3 \u00d7 9 = 27 different distributions in the previous simulations. We are now comparing the learning performance of these enhanced algorithms on six real data sets. The first data set is a subset of the Shanghai Share Index (SSPI), which can be extracted from a dependent variable. This data set contains 2000 trading days stock index, which has five independent variables, i.e., minimum prices, closing prices, daily trading volumes, and a dependent variable, i.e., the opening price is the diabetes data set."}, {"heading": "V. PROOFS", "text": "In this section we provide the results of the main results."}, {"heading": "VI. CONCLUSION", "text": "The contributions can be inferred in four aspects. First, we have theoretically derived the generalization error limit of L2RBoosting and demonstrated the importance of the degree of shrinkage. Furthermore, it has been shown that the shrinkage rate of L2-RBoosting O (m \u2212 1 / 2 logm) can be reached, which is the optimal \"record\" for greedy learning and stimulation algorithms. Furthermore, our result showed that although the shrinkage rate did not affect the learning rate, it determines the constant of the generalization error and therefore played a decisive role in boosting L2 learning with finite samples. Then, we proposed two schemes to determine the shrinkage rate: the first is conventional parameterized L2-RBoosting, and the second is to learn the degree of boosting directly from the samples (L2DRDRBoosting)."}], "references": [{"title": "Greedy function approximation: a gradient boosting machine", "author": ["J. Friedman"], "venue": "Ann. Stat., vol. 29, no. 5, pp. 1189\u20131232, 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosting with the l2 loss: regression and classification", "author": ["P. Buhlmann", "B. Yu"], "venue": "J. Amer. Stat. Assoc., vol. 98, no. 462, pp. 324\u2013339, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "ICML, vol. 96, pp. 148\u2013156, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Ann. Stat., vol. 28, no. 2, pp. 337\u2013407, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Boosting algorithms: regularization, prediction and model fitting", "author": ["P. Buhlmann", "T. Hothorn"], "venue": "Stat. Sci., vol. 22, no. 4, pp. 477\u2013505, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Boosting methods for regression", "author": ["N. Duffy", "D. Helmbold"], "venue": "Mach. Learn., vol. 47, no. 2-3, pp. 153\u2013200, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "Inform. Comput., vol. 121, no. 2, pp. 256\u2013285, 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "The strength of weak learnability", "author": ["R.E. Schapire"], "venue": "Mach. Learn., vol. 5, no. 2, pp. 1997\u20132027, 1990.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Adaboost is consistent", "author": ["P. Bartlett", "M. Traskin"], "venue": "J. Mach. Learn. Res., vol. 8, pp. 2347\u20132368, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Lower bounds for the rate of convergence of greedy algorithms", "author": ["E.D. Livshits"], "venue": "Izvestiya: Mathematics, vol. 73, no. 6, pp. 1197\u20131215, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibsirani"], "venue": "Ann. Stat., vol. 32, no. 2, pp. 407\u2013451, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Re-scale boosting for regression and classification", "author": ["S.B. Lin", "Y. Wang", "L. Xu"], "venue": "ArXiv:1505.01371, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Characterizing l2 boosting", "author": ["J. Ehrlinger", "H. Ishwaran"], "venue": "Ann. Stat., vol. 40, no. 2, pp. 1074\u20131101, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Boosting with early stopping: convergence and consistency", "author": ["T. Zhang", "B. Yu"], "venue": "Ann. Stat., vol. 33, no. 4, pp. 1538\u20131579, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Forward stagewise regression and the monotone lasso", "author": ["T. Hastie", "J. Taylor", "R. Tibshirani", "G. Walther"], "venue": "Electron. J. Stat., vol. 1, pp. 1\u201329, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Stagewise lasso", "author": ["P. Zhao", "B. Yu"], "venue": "J. Mach. Learn. Res., vol. 8, pp. 2701\u20132726, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Relaxation in greedy approximation", "author": ["V. Temlyakov"], "venue": "Constr. Approx., vol. 28, no. 1, pp. 1\u201325, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequential greedy approximation for certain convex optimization problems", "author": ["T. Zhang"], "venue": "IEEE Trans. Inform. Theory, vol. 49, no. 3, pp. 682\u2013691, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "A working guide to boosted regression trees", "author": ["J. Elith", "J.R. Leathwick", "T. Hastie"], "venue": "J. Anim. Ecol., vol. 77, no. 4, pp. 802\u2013813, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn., vol. 24, no. 2, pp. 123\u2013140, 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "Linearly combining density estimators via stacking", "author": ["P. Smyth", "D. Wolpert"], "venue": "Mach. Learn., vol. 36, no. 1-2, pp. 59\u201383, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Bayesian methods for adaptive models", "author": ["D. MacKay"], "venue": "PhD thesis, California Institute of Technology, 1991.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1991}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Mach. Learn., vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "An l2 boosting algorithm for estimation of a regression function", "author": ["A. Bagirov", "C. Clausen", "M. Kohler"], "venue": "IEEE Trans. Inform. Theory, vol. 56, no. 3, pp. 1417\u20131429, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Some theory for generalized boosting algorithms", "author": ["P. Bickel", "Y. Ritov", "A. Zakai"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 705\u2013732, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning capability of relaxed greedy algorithms", "author": ["S.B. Lin", "Y.H. Rong", "X.P. Sun", "Z.B. Xu"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 10, pp. 1598\u20131608, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Some remarks on greedy algorithms", "author": ["R. DeVore", "V. Temlyakov"], "venue": "Adv. Comput. Math., vol. 5, no. 1, pp. 173\u2013187, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Greedy approximation", "author": ["V. Temlyakov"], "venue": "Acta Numer., vol. 17, pp. 235\u2013 409, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation and learning by greedy algorithms", "author": ["A. Barron", "A. Cohen", "W. Dahmen", "R. DeVore"], "venue": "Ann. Stat., vol. 36, no. 1, pp. 64\u201394, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "A comparison study between different re-scale boosting", "author": ["L. Xu", "S.B. Lin", "Y. Wang", "Z.B. Xu"], "venue": "Manuscript, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Greedy approximation in convex optimization", "author": ["V. Temlyakov"], "venue": "Constr. Approx., pp. 1\u201328, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximation with polynomial kernels and svm classifiers", "author": ["D.X. Zhou", "K. Jetter"], "venue": "Adv. Comput. Math., vol. 25, no. 1-3, pp. 323\u2013344, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Concentration estimates for learning with l1-regularizer and data dependent hypothesis spaces", "author": ["L. Shi", "Y.L. Feng", "D.X.Zhou"], "venue": "Appl. Comput. Harmon. Anal., vol. 31, no. 2, pp. 286\u2013302, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "A distributionfree theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer Science & Business Media, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "The elements of statistical learning", "author": ["T. Hastie", "R.Tibshirani", "J. Friedman"], "venue": "New York: springer, 2001.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "C. Stone", "R. Olshen"], "venue": "CRC press, 1984.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1984}, {"title": "UCI} repository of machine learning databases", "author": ["C. Blake", "C. Merz"], "venue": "1998.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Hedonic prices and the demand for clean air", "author": ["D. Harrison", "D.L. Rubinfeld"], "venue": "J. Environ. Econ., vol. 5, no. 1, pp. 81\u2013102, 1978.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1978}, {"title": "Modeling of strength of high performance concrete using artificial neural networks", "author": ["I.C. Ye"], "venue": "Cement and Concrete Research, vol. 28, no. 12, pp. 1797\u20131808, 1998.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1808}, {"title": "The population biology of abalone (haliotis species) in tasmania. i. blacklip abalone (h. rubra) from the north coast and islands of bass strait", "author": ["W.J. Nash", "T.L. Sellers", "S.R. Talbot", "A.J. Cawthorn", "W.B. Ford"], "venue": "Sea Fisheries Division, Technical Report, no. 48, 1994.  12", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1994}, {"title": "Greedy metric in orthogonal greedy learning", "author": ["L. Xu", "S.B. Lin", "J.S. Zeng", "Z.B. Xu"], "venue": "Manuscript, 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient greedy learning for massive data", "author": ["C. Xu", "S.B. Lin", "J. Fang", "R.Z. Li"], "venue": "Manuscript, 2014.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-kernel regularized classifiers", "author": ["Q. Wu", "Y. Ying", "D.X. Zhou"], "venue": "J. Complex., vol. 23, no. 1, pp. 108\u2013134, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "From the statistical viewpoint, boosting can be viewed as a form of functional gradient decent [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Typically, L2-Boosting [2], [3] can be interpreted as an stepwise additive learning scheme that concerns the problem of minimizing the L2 risk.", "startOffset": 23, "endOffset": 26}, {"referenceID": 2, "context": "Typically, L2-Boosting [2], [3] can be interpreted as an stepwise additive learning scheme that concerns the problem of minimizing the L2 risk.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "Boosting is resistant to overfitting [4] and thus, has triggered enormous research activities in the past twenty years [5], [6], [7], [1], [8].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "Although the universal consistency of boosting has already been verified in [9], the numerical convergence rate of boosting is a bit slow [9], [10].", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "Although the universal consistency of boosting has already been verified in [9], the numerical convergence rate of boosting is a bit slow [9], [10].", "startOffset": 138, "endOffset": 141}, {"referenceID": 9, "context": "Although the universal consistency of boosting has already been verified in [9], the numerical convergence rate of boosting is a bit slow [9], [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": "The main reason for such a drawback is that the step-size derived via linear search in boosting can not always guarantee the most appropriate one [11], [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "The main reason for such a drawback is that the step-size derived via linear search in boosting can not always guarantee the most appropriate one [11], [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China the regularized boosting via shrinkage (RSBoosting) [13], regularized boosting via truncation (RTBoosting) [14] and \u03b5Boosting [15] have been developed via introducing additional parameters to control the step-size.", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China the regularized boosting via shrinkage (RSBoosting) [13], regularized boosting via truncation (RTBoosting) [14] and \u03b5Boosting [15] have been developed via introducing additional parameters to control the step-size.", "startOffset": 213, "endOffset": 217}, {"referenceID": 14, "context": "Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China the regularized boosting via shrinkage (RSBoosting) [13], regularized boosting via truncation (RTBoosting) [14] and \u03b5Boosting [15] have been developed via introducing additional parameters to control the step-size.", "startOffset": 232, "endOffset": 236}, {"referenceID": 0, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "Both experimental and theoretical results [1], [13], [16], [5] showed that these variants outperform the classical boosting within a certain extent.", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "Motivated by the recent development of relaxed greedy algorithm [17] and sequential greedy algorithm [18], Lin et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Motivated by the recent development of relaxed greedy algorithm [17] and sequential greedy algorithm [18], Lin et al.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "[12] introduced a new variant of boosting named as the rescale boosting (RBoosting).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Both theoretical analysis and experimental results in [12] implied that RBoosting is better than boosting, at least for the L2 loss.", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "[19] showed that 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "1 is a feasible choice of \u03b5 in \u03b5-Boosting; B\u00fchlmann and Hothorn [5] recommended the selection of 0.", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "1 for the regularization parameter in RSBoosting; Zhang and Yu [14] proved that O(k\u22122/3) is a good value of the truncated parameter in RTBoosting, where k is the number of iterations.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 96, "endOffset": 99}, {"referenceID": 20, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "L2-BOOSTING, L2-RBOOSTING AND L2-DDRBOOSTING Ensemble techniques such as bagging [20], boosting [7], stacking [21], Bayesian averaging [22] and random forest [23] can significantly improve performance in practice and benefit from favorable learning capability.", "startOffset": 158, "endOffset": 162}, {"referenceID": 23, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 109, "endOffset": 112}, {"referenceID": 24, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 125, "endOffset": 128}, {"referenceID": 25, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "In particular, boosting and its variants are based on a rich theoretical analysis, to just name a few, [24], [9], [25], [2], [4], [26], [12], [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "where \u03c6 : R \u00d7 R \u2192 R+ is called a loss function [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "Previous study [2] showed that L2Boosting can successfully tackle this problem.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Then the gradient descent view of L2-Boosting [1] can be interpreted as follows.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 83, "endOffset": 86}, {"referenceID": 26, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "In spite of L2-Boosting was proved to be consistent [9] and overfitting resistance [2], multiple studies [27], [10], [28] also showed that its population convergence rate is far slower than the best nonlinear approximant.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "The main reason is that the linear search in Algorithm 1 makes fk+1 to be not always the greediest one [11], [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "The main reason is that the linear search in Algorithm 1 makes fk+1 to be not always the greediest one [11], [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Thus, various variants of boosting, such as the \u03b5-Boosting [15] which specifies the step-size as a fixed small positive number \u03b5 rather than using the linear search, RSBoosting[13] which multiplies a small regularized factor to the step-size deduced from the linear search and RTBoosting[14] which truncates the linear search in a small interval have been developed.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "Thus, various variants of boosting, such as the \u03b5-Boosting [15] which specifies the step-size as a fixed small positive number \u03b5 rather than using the linear search, RSBoosting[13] which multiplies a small regularized factor to the step-size deduced from the linear search and RTBoosting[14] which truncates the linear search in a small interval have been developed.", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Thus, various variants of boosting, such as the \u03b5-Boosting [15] which specifies the step-size as a fixed small positive number \u03b5 rather than using the linear search, RSBoosting[13] which multiplies a small regularized factor to the step-size deduced from the linear search and RTBoosting[14] which truncates the linear search in a small interval have been developed.", "startOffset": 287, "endOffset": 291}, {"referenceID": 11, "context": "[12] also derived a new backward type strategy, called the re-scale boosting (RBoosting), to improve the population convergence rate and consequently, the generalization capability of boosting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "L2-RBoosting stems from the \u201cgreedy algorithm with fixed relaxation\u201d [28] in nonlinear approximation.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "It is different from the L2-Boosting algorithm proposed in [24], which adopts the idea of \u201cX-greedy algorithm with relaxation\u201d [29].", "startOffset": 59, "endOffset": 63}, {"referenceID": 28, "context": "It is different from the L2-Boosting algorithm proposed in [24], which adopts the idea of \u201cX-greedy algorithm with relaxation\u201d [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "Such a difference makes the design principles of RBoosting and the boosting algorithm in [24] to be totally distinct.", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "However, the boosting algorithm in [24] only concerns the optimization problem", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "When faced with other loss, the boosting algorithm in [24] cannot be efficiently numerical solved.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "However, it can be found in [12] that RBoosting is feasible for arbitrary loss.", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "We are currently studying the more concrete comparison study between these two re-scale boosting algorithms [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 256, "endOffset": 260}, {"referenceID": 12, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 300, "endOffset": 304}, {"referenceID": 13, "context": "It is known that L2-RBoosting can improve the population convergence rate and generalization capability of L2-Boosting [12], but the price is that there is an additional parameter, the shrinkage degree \u03b1k, just like the step-size parameter \u03b5 in \u03b5-Boosting [15], regularized parameter v in RSBoosting [13] and truncated parameter T in RTBoosting [14].", "startOffset": 345, "endOffset": 349}, {"referenceID": 30, "context": "The above Algorithm 3 is motivated by the \u201cgreedy algorithm with free relaxation\u201d [31].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "As far as the L2 loss is concerned, it is easy to deduce the close-form representation of f \u2032 k+1 [28].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "It follows from [29] that (III.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "1) defines an interpolation space which has been widely used in nonlinear approximation [29], [26], [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "1) defines an interpolation space which has been widely used in nonlinear approximation [29], [26], [28].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "1) defines an interpolation space which has been widely used in nonlinear approximation [29], [26], [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 31, "context": "Then it is obvious that [32] for all t \u2208 R and y \u2208 [\u2212M,M ] there holds", "startOffset": 24, "endOffset": 28}, {"referenceID": 28, "context": "This rate is independent of the dimension and is the same as the optimal \u201crecord\u201d for greedy learning [29] and boosting-type algorithms [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "This rate is independent of the dimension and is the same as the optimal \u201crecord\u201d for greedy learning [29] and boosting-type algorithms [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "Furthermore, under the same assumptions, this rate is faster than those of boosting [9] and RTBoosting [14].", "startOffset": 84, "endOffset": 87}, {"referenceID": 13, "context": "Furthermore, under the same assumptions, this rate is faster than those of boosting [9] and RTBoosting [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "The l2-empirical covering number of a function set is defined by means of the normalized l2-metric d2 on the Euclidean space R given in [33] with d2(a,b) = ( 1 m \u2211m i=1 |ai \u2212 bi| ) 1 2 for a = (ai)i=1,b = (bi) m i=1 \u2208 R.", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "For example, in [33], Shi et al.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The second half, denoted by D m (the validation set), is used to choose \u03b1k by picking \u03b1k \u2208 I := [0, 1] to minimize the empirical risk", "startOffset": 96, "endOffset": 102}, {"referenceID": 23, "context": "9 typical regression functions are considered in this set of simulations, where these functions are the same as those in section IV of [24].", "startOffset": 135, "endOffset": 139}, {"referenceID": 34, "context": "[35] suggest that 4 \u2264 J \u2264 8 generally works well and the estimate is typically not sensitive to the exact choice of J within that range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Thus, in the following simulations, we use the CART [36] (with the number of splits J = 4) to build up the week learners for regression.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "2) Relationship between shrinkage degree and generalization performance : For each given re-scale factor u \u2208 [1, 10], we employ L2-RBoosting to train the corresponding estimates on the whole training samples Dm, and then use the independent test samples Dm\u2032 to evaluate their generalization performance.", "startOffset": 109, "endOffset": 116}, {"referenceID": 9, "context": "2) Relationship between shrinkage degree and generalization performance : For each given re-scale factor u \u2208 [1, 10], we employ L2-RBoosting to train the corresponding estimates on the whole training samples Dm, and then use the independent test samples Dm\u2032 to evaluate their generalization performance.", "startOffset": 109, "endOffset": 116}, {"referenceID": 10, "context": "The second one is the Diabetes data set[11].", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "The fourth one is the Boston Housing data set created form a housing values survey in suburbs of Boston by Harrison[38].", "startOffset": 115, "endOffset": 119}, {"referenceID": 38, "context": "The fifth one is the Concrete Compressive Strength (CCS) data set created from[39].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "The sixth one is the Abalone data set, which comes from an original study in [40] for predicting the age of abalone from physical measurements.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "Using the similar methods that in [26], [41], we construct an f\u2217 k \u2208 span(Dn) as follows.", "startOffset": 34, "endOffset": 38}, {"referenceID": 40, "context": "Using the similar methods that in [26], [41], we construct an f\u2217 k \u2208 span(Dn) as follows.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "The first one can be found in [12], which is a direct generalization of [17, Lemma 2.", "startOffset": 30, "endOffset": 34}, {"referenceID": 42, "context": "The following concentration inequality [43] plays a crucial role in the proof.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "Assume that there are constants B, c > 0 and \u03b1 \u2208 [0, 1] such that \u2016f\u2016\u221e \u2264 B and Ef \u2264 c(Ef) for every f \u2208 F .", "startOffset": 49, "endOffset": 55}], "year": 2015, "abstractText": "Re-scale boosting (RBoosting) is a variant of boosting which can essentially improve the generalization performance of boosting learning. The key feature of RBoosting lies in introducing a shrinkage degree to re-scale the ensemble estimate in each gradient-descent step. Thus, the shrinkage degree determines the performance of RBoosting. The aim of this paper is to develop a concrete analysis concerning how to determine the shrinkage degree in L2-RBoosting. We propose two feasible ways to select the shrinkage degree. The first one is to parameterize the shrinkage degree and the other one is to develope a data-driven approach of it. After rigorously analyzing the importance of the shrinkage degree in L2-RBoosting learning, we compare the pros and cons of the proposed methods. We find that although these approaches can reach the same learning rates, the structure of the final estimate of the parameterized approach is better, which sometimes yields a better generalization capability when the number of sample is finite. With this, we recommend to parameterize the shrinkage degree of L2RBoosting. To this end, we present an adaptive parameterselection strategy for shrinkage degree and verify its feasibility through both theoretical analysis and numerical verification. The obtained results enhance the understanding of RBoosting and further give guidance on how to use L2-RBoosting for regression tasks.", "creator": "LaTeX with hyperref package"}}}