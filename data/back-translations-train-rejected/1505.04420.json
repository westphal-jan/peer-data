{"id": "1505.04420", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2015", "title": "CCG Parsing and Multiword Expressions", "abstract": "This thesis presents a study about the integration of information about Multiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar (CCG). We build on previous work which has shown the benefit of adding information about MWEs to syntactic parsing by implementing a similar pipeline with CCG parsing. More specifically, we collapse MWEs to one token in training and test data in CCGbank, a corpus which contains sentences annotated with CCG derivations. Our collapsing algorithm however can only deal with MWEs when they form a constituent in the data which is one of the limitations of our approach.", "histories": [["v1", "Sun, 17 May 2015 17:26:36 GMT  (98kb,D)", "http://arxiv.org/abs/1505.04420v1", "MSc thesis, The University of Edinburgh, 2014, School of Informatics, MSc Artificial Intelligence"]], "COMMENTS": "MSc thesis, The University of Edinburgh, 2014, School of Informatics, MSc Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["miryam de lhoneux"], "accepted": false, "id": "1505.04420"}, "pdf": {"name": "1505.04420.pdf", "metadata": {"source": "CRF", "title": "CCG Parsing and Multiword Expressions", "authors": ["Miryam de Lhoneux"], "emails": [], "sections": [{"heading": null, "text": "This thesis presents a study on the integration of information about Multiword Expressions (MWEs) into analysis with Combinatory Categorial Grammar (CCG). We build on previous work by Nivre and Nilsson (2004) and Korkontzelos and Manandhar (2010), which showed how useful it is to add information about MWEs to syntactical analysis by implementing a similar pipeline of CCG analyses. Specifically, we collate MWEs into a single symbol in training and test data in CCGbank, a corpus that was automatically created by Hockenmaier (2003) and contains sentences commented on with CCG derivatives. However, our collapsing algorithm can only deal with MWEs if they form a component in the data that is one of the limitations of our approximation.We investigate the effect of collapsing training and test data that is not standard."}, {"heading": "Acknowledgements", "text": "First and foremost, I would like to thank my supervisors Mark Steedman and Omri Abend. I visited Mark in January with the story of where I came from and where I wanted to go, and I knew immediately that I had knocked on the right door. He listened attentively to my story, put me in touch with Omri, and together they helped me connect the dots so that my thesis fitted well into the picture. Omri was a very enthusiastic supervisor full of interesting ideas, and I admire the patience he had with me. I am grateful to both of them for all the discussions and for taking the time to correct my drafts and comment on them in depth. I would also like to thank Bharat Ram Ambati, who was very helpful to me on some technical issues. I am not sure if I would have made it through this dissertation, let alone this mastery would not have been for my AT colleagues, whom I would also like to thank."}, {"heading": "1 Introduction 1", "text": "1.1 Motivation............................................................................................................................"}, {"heading": "2 Related Work 3", "text": "2.1. Introduction......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Methodology 14", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Experiments and results 32", "text": "4.1. Introduction......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5 Discussion 40", "text": "5.1 Introduction.............................................................................................................................................................................................................................."}, {"heading": "6 Conclusion 44", "text": "The question of whether it is the right strategy is not new; the question of whether it is the right strategy is not new; the question of whether it is the right strategy, but the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy - the right strategy, the right, the right, the right, the right, the right, the right, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy, the right strategy"}, {"heading": "1.1 Motivation", "text": "It is a central task in the processing of natural language (henceforth, NLP) and is used in many applications. It is a central but difficult task, partly because it requires a lot of annotated data. Pioneering research in the 1990s showed impressive results in this task, but the great efforts were directed at a particular domain (news) and a specific language (Enlglish) and the problem is far from being solved. Multi-word expressions (henceforth, MWE (s) are increasingly able to analyze a variety of phenomena with different properties (Enlglish) and the problem is far from solving itself."}, {"heading": "1.2 Aims", "text": "So far, we have not tried to use MWE information to improve CCG parsing, which is what we intend to do in this work. Different approaches to using MWE information to improve syntactical parsing have so far been done using different syntactical models, and we will do one that is far from ideal, but will be a necessary first step that will be useful to form a solid baseline. The approach we will take is to change training and test data, i.e. MWEs to break down into a lexical element in them. By collapse, we mean grouping MWEs so that they form a symbol and therefore retokenize the sentence. We will use this term in the rest of the work. A change in the approach that we will propose is to recognize MWE as an experimental part of the pipeline so that we can find out whether or not implementing this type of approach leads to different types of MWEs."}, {"heading": "1.3 Overview", "text": "We will provide an overview of the background literature to further support our research and elaborate on the research questions in Chapter 2. Subsequently, we will explain and motivate the methodology we propose to answer the research questions in Chapter 3. We will present our experiments and results in Chapter 4 and discuss the results and limitations of the current approach in Chapter 5. We will draw conclusions from our study and propose research opportunities in Chapter 6.Chapter 2Related Work."}, {"heading": "2.1 Introduction", "text": "This chapter gives an overview of the research areas on which this thesis is based: syntactic parsing (Section 2.2), multiword expressions (Section 2.3) and combinatorial categorical grammar (Section 2.4), which for time and space reasons are introduced only briefly with the background knowledge needed to understand the rest of the work. Subsequently, the work on the interaction between the three frameworks will be presented in Section 2.5 (between multiword expressions and syntactic parsing) and 2.6 (between CCG and syntactical parsing), the objectives and research questions of the work will be presented in Section 2.7."}, {"heading": "2.2 Syntactic Parsing", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2.3 Multiword Expressions", "text": "The most commonly accepted definition of this term since Sag et al. (2001) is that it is a group of multiple lexemes that exhibit a certain degree of idiomaticism or irregularity; the multiple lexemes in an MWE are referred to as MWE units in the rest of this thesis for convenience; this idiomaticism can be lexicosyntactic, as in the unusual coordination of a preposition and an adjective in the \"whole.\" It can be semantic, as in the idiom \"kick the bucket,\" in which the meaning of the whole cannot be divided into the meaning of the parts; it can be pragmatic, as in \"Good Morning,\" which has a meaning attached to the situation in which it is said; and finally, it can be statistical, as in the collocation of \"strong coffee,\" in which both units occur more frequently than expected."}, {"heading": "2.4 Combinatory Categorial Grammar", "text": "It was built with the intention of being linguistically conscious and arithmetically comprehensible, partly in response to the transformational ideas that prevailed in formal grammars at the time. It differs mainly in that it has a component that includes syntactical and semantic information, rather than having separate modules for each in grammar. Instead of having a large set of rules and a lexicon, as is the case with traditional grammars, it has a small set of universal rules and a lexicon that implements the universal rules."}, {"heading": "2.5 Syntactic Parsing and Multiword Expressions", "text": "As mentioned in Section 2.3, the identification of MWEs is important for syntatic analysis, but because they have unusual properties, their analysis can be quite problematic; the question of how to deal with MWEs for syntactic parsing has been raised by many and has been approached in different ways. Researchers working with precision grammatics such as HPSG, for example, have included the lexical entries for MWEs in the lexicon so that MWEs do not pose a problem for parsing. Researchers working with data-induced grammars have adjusted the test and / or training data prior to parsing. Recent research has suggested changing both the lexicon and the parsing algorithm. I will briefly describe each of these approaches. As I will use the second approach for reasons explained in 3.2.1, I will describe it in more detail than the other two."}, {"heading": "2.5.1 Changing the lexicon", "text": "Different types of lexical entries have been suggested for MWEs in grammar. Many research suggests that all MWEs should simply be analyzed as \"words with spaces,\" i.e. Chapter 2. Related paper 9groups the MWE units in syntactic analysis, an analysis that has been disputed by many. Sag et al. (2001) have proposed sophisticated methods to present the various MWE types in a grammar that has been partially implemented within the framework of the precision grammar HPSG as described by Copestake et al. (2002). Zhang et al. (2006) have recently found that MWEs are an enormous source of parsing failures with precision grammar such as HPSG."}, {"heading": "2.5.2 Changing the data", "text": "Since the groundbreaking work of Nivre and Nilsson (2004), research has shown that treating MWEs as a character or \"word with spaces\" in test and / or training data before parsing and / or training improves parsing accuracy. Nivre and Nilsson (2004) have shown that this is true for deterministic dependency analysis, and Korkontzelos and Manandhar (2010) have shown that it is true for shallow parsing."}, {"heading": "2.5.2.1 Changing training and test data", "text": "Nivre and Nilsson (2004) created two versions of a tree bank, one in which MWE's are commented compositively and one in which they are linked together as a lexical element. They demonstrated that training a parser on the second version of the tree bank leads to better analytical accuracy. They used a corpus of manual MWE annotations to create both versions of the tree bank, thereby simulating a \"perfect\" MWE detection. MWE annotations, however, consist of only a few types of MWE and are therefore not comprehensive. They reported improvements in the analytical accuracy of the MWEs themselves, but also in their surrounding syntactical structure. They opened the door to improving the syntactic parse with MWE information, but left many questions unanswered: whether their results can be applied to other syntactical analysis models or not, whether the full potential they have achieved with the \"perfect\" detection of MWE cannot be achieved with an automated detection device."}, {"heading": "2.5.2.2 Changing test data", "text": "Korkontzelos and Manandhar (2010) reported similar precision improvements in shallow parsing. They showed that the results from Nivre and Nilsson (2004) seemed to port to at least one other parsing model, but their technique is very different. They created a corpus that contained a large number of pre-selected MWEs (randomly selected from WordNet) and converted it into a version in which the MWE units collapsed onto a lexical element. They marked the two versions of the corpus before parsing. They then analyzed the differences in output. To do this, they randomly selected a sample of output from both analyzed corpora and created a taxonomy of changes they observed from one to the other. For each class in taxonomy, they determined whether the change in output led to increased accuracy, or did not change accuracy. They automatically classified the rest of output data and observed an increase in accuracy."}, {"heading": "2.5.3 Changing the lexicon and the parsing algorithm", "text": "Many studies have shown that MWE information improves syntactic parsing, but vice versa: syntactical analysis improves MWE identification. Green et al. (2013) successfully tuned a parser for MWE identification, Weller and Heid (2010) and Martens and Vandeghinste (2010) have shown that using analyzed corpora for MWE identification is beneficial. These results have led Seretan (2013) to suggest that neither the adaptation of grammar to MWE information nor the recognition of MWE in the raw text as an aid to parsing are appropriate methods for dealing with the issue of MWE in syntactic parsing, since neither approach exploits the fact that MWE information and syntactic analysis are informative to each other. Instead, it suggests having an MWE lexicon and treating potential MWEs during parsinging.Chapter 2."}, {"heading": "2.5.4 Advantages and caveats of the three approaches", "text": "All of these researchers have demonstrated the importance of MWEs for syntactical analysis, but all approaches presented have reservations. Research at HPSG seems to have found the most sophisticated methods for dealing with MWEs, but parsing with precise grammars is known to be much less robust (Zhang and Kordoni, 2008) (i.e., it does not succeed in analyzing many more sentences) than parsing with data-induced grammars, which make it a less than ideal solution for practical parsing. As for other solutions, they are often very limited by the type of MWEs that have been treated. All other solutions presented as a fact focus on a few types of MWEs. However, as Kim (2008) has argued, due to the different but interrelated properties of MWEs, it is not appropriate to try and generalize, to generalize MWEs, and to find a single representation that works for all types of MWEs, nor is it appropriate to deal with each type of MWEs to syntactical improvement."}, {"heading": "2.6 CCG Parsing", "text": "It was mentioned in Section 2.4 that CCG has computational and linguistic dependencies, which have a high value in the NLP community. Therefore, the desire to build efficient parsers with this formalism grew naturally. Recent research has developed efficient tools for the analysis of CCG and CCG dependencies, which are increasingly used in NLP applications. It is, to cite just a few examples, for question analysis (Clark et al., 2004) and semantic parsing. The first efficient statistical model was the generative model built by Hockenmaier and Steedman (2002) and expanded by Clark and Curran (2007) into a discriminatory model. Hockenmaier's first step (2003) was to translate the PTB into a treadmill of CCG derivatives, called CCGbank. Both models work close to art, although using simpler statistical models of PCs, which are used by the authors as the result of a more expressive technique, the grammatics are used by the state."}, {"heading": "2.7 Research questions and objectives", "text": "Section 2.2 said that syntactic parsing work is important for many NLP applications, and it was said that one of the promising directions was to try and improve it as a joint task with another NLP task. Section 2.5 said that MWE identification information improves syntactic parsing, although current approaches leave a lot of room for improvement. Therefore, trying to improve syntactic parsing with MWE information looks like a promising research path. Section 2.2 explains the benefits of working with lexicalized grammar formalism for syntactic parsing, and Section 2.6 said that parsing with highly lexicalized grammar formalism is almost state-of-the-art CCG. CCG parsing therefore looks like an ideal framework for improving current approaches to improving syntactical parsing with MWE information."}, {"heading": "3.1 Introduction", "text": "As explained in the previous chapter, the aim of this work is to test whether information about MWE can improve CCG parsing or not, and to improve earlier approaches to adding MWE information to syntactic parsing by avoiding focusing on a limited number of MWE types, and instead trying to determine whether different types of MWE lead to different outcomes. In this chapter, the methodology used in this thesis is described. Section 3.2 describes the general approach and the way it relates to earlier approaches to integrating MWE information on syntactic parsing. Section 3.3 describes the parsing model on which we are building the study and which will serve as the basis for the thesis. Section 3.4 describes the data used. Section 3.5 describes the implementation of the changes to the parsing model we are proposing, and Section 3.6 describes the way we train the updated model and analyze the test data."}, {"heading": "3.2 Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Type of approach", "text": "Section 2.5 describes three different approaches to adding MWE information to a syn-tactical analysis model: one is to directly modify lexical entries in the lexicon; another is to recognize MWE information in the raw text and fold MWEs together in Chapter 3. Methodology 15Units to one tokens that can be used for both training and test data for use with an analysis model; a final approach is to create a dictionary of MWEs and modify the analysis algorithm to make decisions about MWE representation during analysis; the first approach has been suggested for precision grammar and is not readily applicable to data-induced grammar; the third approach is more sophisticated than the second and has the advantage of dealing with disjointed MWEs that the second does not or at least does not directly address; but the second approach is already a solid foundation for other formalities that we implement for CCG analysis in this thesis."}, {"heading": "3.2.2 Description of approach", "text": "In the eeisrmtlrrrrVee rf\u00fc ide eeisrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrr\u00fc rf\u00fc ide eeisrmtlrVnlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerlr\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc ide eeisreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeer\u00fc rf\u00fc rf\u00fc rf\u00fc"}, {"heading": "3.3 Parsing model", "text": "As stated in Section 2.6, there are both generative and discriminatory models for parsing with CCG. There are different generative models with different characteristics. We chose StatOpenCCG, which was developed by Christodoulopoulos (2008) and recently expanded by Deoskar et al. (2014) because it is easy to use, flexible and quick to train. The extension of the parser by Deoskar et al. (2014) is therefore particularly well suited for our purposes: it has been expanded to work better on unknown lexical objects. The collapse of lexical objects increases data sparseness and the ability to deal with unknown data. In particular, the model proposed by Christodoulopoulos (2008) and Deoskar et al. (2014) is based on one of Hockenmaier's models (2003) called LexCat and the ability to deal with unknown data is a problem for our approach."}, {"heading": "3.4 Data", "text": "All statistical CCG analysis models are based on the CCGbank, which, as mentioned in Section 2.6, is a translation of the PTB into CCG derivatives developed by Hockenmaier (2003) with the aim of creating parsing models for CCG. Sections 01-22 are usually used for training, Section 00 for development and Section 23 for testing. We will follow tradition to achieve comparable results with previous models."}, {"heading": "3.5 Extending the parsing model with MWE information", "text": "As explained in Section 3.2, the initial goal is to recognize MWEs in the unlabeled version of the CCGbank and then reduce them to a lexical point in the annotated version of the Treebank and in the unlabeled test data. MWE detection is described in Section 3.5.1 and CCGbank conversion in Section 3.5.2."}, {"heading": "3.5.1 Recognizing MWEs", "text": "It is a flexible library that provides many useful tools for our purposes. It can be used to create an index of MWEs with information about their probability. It can also be used with a standard index that contains all the MWEs and inflections extracted from Word3.0 and Semcor 1.6 and statistics for each MWE. There are three different tools that are of interest to us. There are filters that filter the results of the detectors in the text. There is a detector to find Proper Nouns, one to find all types of MWEs, one that only finds stop words, etc. These simple detectors can also be combined to form a complex detector. There are filters that filter the results of the detectors. For example, one accepts only MWEs that are continuous, one ejects MWEs that have a score below a certain threshold, one holds MWEs only below a certain length. The last tool that we need is a \"solver,\" if we do not resolve conflicts on MWE. \""}, {"heading": "3.5.2 Modifying CCGbank", "text": "This section describes most of our work. The idea is to use the list of MWEs that come from the detection system and collapse the MWE units in the tree. We have processed trees and dependencies separately from the trees and I will describe each collapsing algorithm one by one."}, {"heading": "3.5.2.1 Collapsing MWE units in a CCG parse tree", "text": "The algorithm takes a tree and a list of MWE's. It iterates through the list of MWE = MWE's records, finds them in the tree, and collapses MWE's units only if they form a component in the tree, otherwise the MWE NP is discarded. First, the algorithm searches the tree leaves to find the index position of each unit of each MWE. Based on these index positions, it finds the tree position that dominates all units of an MWE. If this tree position dominates all and only the MWE's units, the MWE's units are siblings in the tree and they are collapsed into a lexical element. To the collapsed MWE, the label of the dominant node is assigned as a category. For example, if the MWE publisher information bureau dominates and the sub-tree in Figure 3.3, the algorithm returns the sub-tree in Figure 3.4.NBureauN's units."}, {"heading": "3.5.2.2 Collapsing dependencies", "text": "In fact, it is the case that one sees oneself as being able to correct and correct the aforementioned errors without being able to retaliate."}, {"heading": "3.6 Training and parsing", "text": "To ensure that we were working on improving a model that was already working well, we first tried to reproduce the results obtained from Deoskar et al. (2014) on the non-collapsed data. We used the same parameters to train and analyze, and obtained similar results (about 87% of the correct lexical categories); this model will serve as a basis and will henceforth be called modelA for convenience. For each experiment, we run the MWE detection mechanism on an unlabeled version of CCGbank1. We then apply the cascaded algorithms 1 and 2 to each set of 1We have created an unlabeled version of CCGbank from the tree leaves to ensure that the data is compatible with the trees with which we work in the event of collapse. Chapter 3. Methodology 24CCGbank. We divide the data in the traditional way, as explained in 3.4, train the model and analyze the test file, a version of the trees that were not labeled, a broken version of the tree was created."}, {"heading": "3.7 Evaluation", "text": "After we have trained our models A and B, we are able to answer the research questions from Section 2.7, i.e. we just have to evaluate them in different ways. In this section, I will first describe the evaluation metrics we will use, and then I will describe the different evaluation schemes we have implemented to answer each of the research questions."}, {"heading": "3.7.1 Evaluation metrics", "text": "As mentioned in Section 2.6, the evaluation of dependencies has often been preferred to the evaluation of dependencies in brackets for CCG analysis. Alternatively, supertags are sometimes evaluated. In our case, we are not particularly interested in supertags. We have changed some of them to the gold standard, as explained in Section 3.5.2.1, but our approach changes the structure of the tree in the gold standard and it is in this structure that we believe we have proposed a better data representation. Therefore, we have decided to follow the former tendency to apply the dependency evaluation scheme in this Chapter 3. Methodology 25. Specifically, dependencies on output parses are compared with the gold standard and labeled and blank precision and recall is calculated as in Equation 3.1, recall is calculated as in Equation 3.2, and a harmonious F density can be achieved with the formula in Equation 3.3. Since we have not tried to work with dependencies in the collapsing formula, only dependencies will be evaluated with PRR (PRR = 3.m)."}, {"heading": "3.7.2 Evaluation schemes", "text": "In this context, it should be noted that this model is not a model, but a model that is a model."}, {"heading": "3.7.2.1 Assessing training and parsing effects", "text": "It's not like it's an attempt to analyze the results. It's not like it's an attempt. It's like it's an attempt. It's like it's an attempt. It's like it's an attempt. It's like it's an attempt. It's like it's an attempt. It's like it's an attempt. It's like it's an attempt. It's like it's an attempt. It's like it's an experiment. It's like it's like it's an experiment. It's like it's an experiment. It's like it's an experiment. It's like it's an experiment. It's like it's an experiment. It's like it's an experiment."}, {"heading": "3.7.2.2 Verifying whether or not automatic recognition of MWEs can improve", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3.7.2.3 Testing whether or not different MWE types impact the results differently", "text": "As mentioned before, we have used different MWE detection systems to create different versions of CCGbankB and thus different versions of ModellB. Since we have created a different gold standard for each of them, the results of different models are not directly comparable, but we can convert the results using the combination algorithm described in Section 3.7.2.1 and test each model against gold standard A. In this way, different versions of ModellB can be compared."}, {"heading": "3.8 Summary of the pipeline", "text": "The entire pipeline of an experiment is summarized in Figure 3.12. First, the CCGbankA is split, and the parser is trained and tested, and then the MWEs are detected in the CCGbank as described in Section 3.5.1, which is then broken down as described in Section 3.5.2. Data is split, the parser is trained, and the test set is analysed.The results of Model A and Model B are combined in the three possible ways described in Section 3.7.2.1 to enable the evaluation of parts of Model B and Model A against the gold standardA. Output A 2They are joined by a \"+\" symbol. Chapter 3. Method 30 is broken down to allow the evaluation of Model B and Model A against the gold standardB. The \"pre-parsing breakdown\" of Model A is not included in Figure A, but is depicted in Figure 3.13. In this case, the test data is broken down before the parse and the output of Model A is compared against the gold standardB."}, {"heading": "3.9 Conclusion", "text": "In Chapter 2, we motivated our study and identified two research questions: whether information about MWEs can improve CCG parsing or not and whether different types of MWEs can affect parsing accuracy in different ways or not. In this chapter, we proposed a methodology to test this. We refined the initial research questions: what we want to find out is whether or not the automatic detection of MWEs can improve CCG parsing. In addition, we proposed to divide it into two further research questions: whether we can observe a parsing effect (the parser is helped in its decisions by broken data) and / or whether we can observe a training effect (the parser learns something useful). We proposed to use different MWE detection mechanisms to answer the second question: when defining an algorithm to break down the treadmill, however, we could not find a simple algorithm to collapse MWEs that are not siblings in the tree, and decide to only one algorithm to break down."}, {"heading": "4.1 Introduction", "text": "As already mentioned, the objectives of this thesis are firstly to find out if automatic MWE detection can be useful for analyzing with CCG, and secondly to find out whether applying the same approach using different types of MWE detection can lead to different results in the MWE detection phase or not. More practically, the first question is whether we can build a model with MWE information that does not have ModelA and that performs better than ModelA? In Chapter 3, this question has also been broken down into two subquestions: whether there is a training effect from the data collapse or not, i.e. the parser learns something useful and whether there is a parsing effect, i.e. the parser is helped in its decisions by broken data. The second research question is less precise: Do we observe differences in results when we use different MWE detection devices that we use for different MWE detection purposes? What would this tell us about the general chapter on MWE detection and how we will deal with each of these two questions first?"}, {"heading": "4.2 MWE recognition", "text": "As mentioned in Section 3.5.1, we used three different tools from the MWErecognition library: detectors that recognize MWEs in the text, filters that filter through the results of one or more detectors and resolutions that resolve conflicts between MWEs when a word is assigned more than one MWEs by the detector. We built 5 different MWE detection features with these three tools. This means that the study is by no means exhaustive, we tried combinations that are comparable to each other and that made intuitive sense. Building all possible combinations went beyond the scope of the theories. We used two different resolutions: one that always selects the longest matchingMWE (which is called the \"longest\"), one that selects the most leftmost MWE (which is called the \"Leftmost,\" i.e. the one that started earliest in the sentence."}, {"heading": "4.3.1 Can MWE collapsing introduce training and parsing effects?", "text": "I will first try to determine whether we can observe a training effect when we train ModelB or not, then I will try to determine whether we can observe a parsing effect when data collapses before and after parsing with ModelA or not."}, {"heading": "4.3.1.1 Can MWE collapsing introduce a training effect?", "text": "As described in Section 3.7.2.1, in order to determine whether or not training data on an MWE-informed body can lead to improved accuracy, i.e. a training effect, we compared the results of Model B with the results of Model A, which was tested on the basis of the \"gold test\" data. 3 of our 5 models outperform Model A on unlabeled B, although generally by a small margin. The best results are obtained by Model B3 and are given in Table 4.2. Model B significantly outperforms Model A by 0.24% (p =.006), supporting the hypothesis that there is indeed a training effect. Chapter 4. Experiments and Results 35However, as the use of gold test data may distort the results of Model B, we also tested these models on the basis of the \"completely collapsed test data.\" In this case, 3 of our 5 models Model A perform better again, albeit by an even smaller margin. The largest difference in results is obtained with Model B1 and the results in Table B1 are significantly smaller than the 4.3, although the model B performs well."}, {"heading": "4.3.1.2 Can MWE collapsing introduce parsing effect?", "text": "It is as if we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in which we are in a time, in which we are in which we are in which are in which we are in a time, in which are in which we are in which we are in which are in which we are in which are in which we are in which are in which are in which are in which we are in which are in a time, in which are in which are in which are in which are in which we are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in which are in"}, {"heading": "4.4 Does using different MWE recognizers impact pars-", "text": "As explained in Section 3.7.2.3, the last experiment we tried to test our model with different detectors was to combine the results using the combination algorithm explained in Section 3.7.2.1 and compare it with the gold standard A. This provides an opportunity to compare different versions of our model. As shown in Table 4.7, different MWE detection methods appear to make a difference in the results. There is a significant difference between our best model (based on detector3) and our worst model (based on detector2) of.26 (p =.01). Some detection methods reduce detection accuracy while others increase it. It is interesting to note from the table that the use of a linkest resolver (a resolver that always chooses the linkiest MWE when there is a conflict) has a bad effect on detection accuracy. If you look at the different models, it is interesting to note that there is a much lower percentage of tree types based on IR, so there are only a lot of WR and MR spaces in the tree, so the changes are not interesting."}, {"heading": "4.5 Conclusion", "text": "When asked if we can improve the accuracy of CCG analysis with information about MWEs obtained through an automatic detection mechanism, we replied that we obviously can. We have shown that ModelB can outperform ModelA on many evaluation models. We also said that there can be both a training effect and a parsing effect. However, we could not show that we can significantly beat ModelA on the gold standard A and the improvements we have made are not dramatic. When asked if the way MWE detection is performed makes a difference in improving the analytical model, we replied that it seems to make a difference. We have achieved very encouraging but limited results and we will discuss them in the next chapter.Chapter 5Discussion"}, {"heading": "5.1 Introduction", "text": "We saw that working with lexicalized formalisms was an appropriate thing because they are linguistically motivated and work with them has shown encouraging results. We also saw that using MWE information for syntactic parsing was a useful direction. Two hypotheses were made: that CCG parsing can be improved by adding information about MWE, and that using different types of detectors could show different images. In Chapter 3, we refined the first of these hypotheses: we said that we wanted to find out whether automatic MWE detection can be useful for parsing CCG or not. We also decoupled this question in two subquestions: whether there is a parsing effect or not (the parser is supported in its decision by collapsed data) and whether there is a training effect (the parser is learning something useful)."}, {"heading": "5.2 Data collapsing", "text": "Collapsing siblings showed a beneficial influence on the accuracy of the analysis, albeit with a margin that is not dramatic. This is not surprising, since our collapsing algorithm 40Chapter 5. Discussion 41 does not change much information in the tree. It only changes the probability distributions of the lexical categories of the nodes involved, i.e. children and parents. It does not change the rest of the tree. One of the limitations mentioned is the fact that we MWEs only broke down when they are siblings in the data. An algorithm that would collapse non-siblings and return trees, as described in Section 3.5.2.1, would make more radical changes to the tree and probably affect the probability model much more. Chapter 3 is an example of MWE, for which units are not siblings in the tree, along with its ideal collapsed tree. Both figures are reproduced in Figure 5.1 and 5.2 to deprive the ability of MKnot forming nodes. In this example, not only the category of the node in the tree, but the \"node\" (but the \"node that often disappear in the category)."}, {"heading": "5.3 MWE recognition", "text": "A few experiments were conducted with different MWE detectors, but due to time constraints, we could not conduct a comprehensive study of the effect of each MWE detection method on parsing accuracy. It seemed that using the resolver that selects the longest matching MWE worked better for our purposes than the one that selects the leftmost MWE detection method, but more experiments would be needed to investigate this and better understand the various effects. It seemed that using only correct nouns for our purposes was the best MWE detection type, but a more comprehensive study would involve more experiments on different types of MWE. Such a study would give a better insight into the type of MWE data that best collapsed as a lexical element and into the reasons why this is the case. In addition, while we used several layers of the MWE detection library that we used through experiments with the different tools, the results would have been more interesting than the data available in the 2001 study, but only a limited set of data available to the MWE."}, {"heading": "5.4 Evaluation", "text": "It has been said a few times that the fact that we did not have an algorithm for the breakdown of non-sibling MWE units made the evaluation part more difficult. We get the best results when we have gold information about siblings, but these results cannot be fully attributed to our modified models. We have implemented evaluation programs to verify the results of our models against data where all MWEs have collapsed, and we have shown improvements to our models even in these evaluation programs, but with less effects.However, when using these cross-validation programs, we do not have reliable gold standards, so our results could have a greater impact. In any case, improvements in all evaluation programs show that our models are useful, but our imperfect evaluation could hide greater effects.Chapter 5. In addition, we have used a purely quantitative evaluation, but it would be very interesting to perform a more qualitative evaluation such as that of Korelkontzos is described in section 2.2 and 2.2 (2010)."}, {"heading": "5.5 Overall approach", "text": "The general approach could also be improved and possibly lead to better results. In Section 3.2.1, it was mentioned that we chose the approach of changing training and test data before parsing because it is a method that has proven useful in the past because it is a solid foundation for future research and because of time constraints. It was mentioned that a more sophisticated approach is not only to change training and test data, but also to change the parsing algorithm. This would be an interesting future work. Another thing that might also be interesting is to integrate MWE detection into the collapsing algorithm. In fact, conflicts between MWEs are resolved without information about the parsing structure. In the event of a breakdown, we can make the decision to discard MWEs because MWE units are not siblings in the tree. However, these MWEs that are discarded could embed smaller MWEs and it would be useful if we could access this information during the breakdown."}, {"heading": "5.6 Conclusion", "text": "This year, we will be able to go in search of a solution that is capable, that we are able, that we are able to find a solution that is capable of finding a solution, that we are able to find a solution that enables us to find a solution, that we are able to find a solution, that we are able to find a solution that enables us to find a solution, that we are able to find a solution that enables us to find a solution that is capable of finding a solution. \""}], "references": [{"title": "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars", "author": ["E. Black", "S.P. Abney", "D. Flickenger", "C. Gdaniec", "R. Grishman", "P. Harrison", "D. Hindle", "R. Ingria", "F. Jelinek", "J.L. Klavans", "M. Liberman", "M.P. Marcus", "S. Roukos", "B. Santorini", "T. Strzalkowski"], "venue": "Proceedings of the workshop on Speech and Natural Language (HLT \u201991), pages 306\u2013311, Pacific Grove, CA,", "citeRegEx": "Black et al\\.,? 1991", "shortCiteRegEx": "Black et al\\.", "year": 1991}, {"title": "Two languages are better than one (for syntactic parsing)", "author": ["D. Burkett", "D. Klein"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP 08, pages 877\u2013886. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Burkett and Klein,? 2008", "shortCiteRegEx": "Burkett and Klein", "year": 2008}, {"title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["E. Charniak", "M. Johnson"], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 173\u2013180, Ann Arbor, Michigan. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Charniak and Johnson,? 2005", "shortCiteRegEx": "Charniak and Johnson", "year": 2005}, {"title": "Creating a Natural Logic Inference System with Combinatory Categorial Grammar", "author": ["C. Christodoulopoulos"], "venue": "Master\u2019s thesis, University of Edinburgh. (Not cited.)", "citeRegEx": "Christodoulopoulos,? 2008", "shortCiteRegEx": "Christodoulopoulos", "year": 2008}, {"title": "Coping with Syntactic Ambiguity or How to Put the Block in the Box on the Table", "author": ["K. Church", "R. Patil"], "venue": "Comput. Linguist., 8(3-4):139\u2013149. (Not cited.)", "citeRegEx": "Church and Patil,? 1982", "shortCiteRegEx": "Church and Patil", "year": 1982}, {"title": "Statistical Parsing", "author": ["S. Clark"], "venue": "Clark, F. and Lappin, editors, The Handbook of Computational Linguistics and Natural Language Processing, pages p. 333\u2013363. Blackwell, Oxford. (Not cited.)", "citeRegEx": "Clark,? 2010", "shortCiteRegEx": "Clark", "year": 2010}, {"title": "Log-Linear Models for Wide-Coverage CCG Parsing", "author": ["S. Clark", "J.R. Curran"], "venue": "Computational Linguistics, 4(33). (Not cited.) 47", "citeRegEx": "Clark and Curran,? 2007", "shortCiteRegEx": "Clark and Curran", "year": 2007}, {"title": "Evaluating a wide-coverage CCG parser", "author": ["S. Clark", "J. Hockenmaier"], "venue": "Proceedings of the LREC 2002 Beyond PARSEVAL workshop, pages 60\u201366. (Not cited.)", "citeRegEx": "Clark and Hockenmaier,? 2002", "shortCiteRegEx": "Clark and Hockenmaier", "year": 2002}, {"title": "Object-Extraction and QuestionParsing using CCG", "author": ["S. Clark", "M. Steedman", "J.R. Curran"], "venue": "EMNLP, pages 111\u2013118. ACL. (Not cited.)", "citeRegEx": "Clark et al\\.,? 2004", "shortCiteRegEx": "Clark et al\\.", "year": 2004}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["M. Collins"], "venue": "Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, EACL \u201997, pages 16\u201323, Stroudsburg, PA, USA. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Collins,? 1997", "shortCiteRegEx": "Collins", "year": 1997}, {"title": "Head-Driven Statistical Models for Natural Language Parsing", "author": ["M. Collins"], "venue": "Computational Linguistics, 29(4):589\u2013637. (Not cited.)", "citeRegEx": "Collins,? 2003", "shortCiteRegEx": "Collins", "year": 2003}, {"title": "A New Statistical Parser Based on Bigram Lexical Dependencies", "author": ["M.J. Collins"], "venue": "Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL \u201996, pages 184\u2013191, Stroudsburg, PA, USA. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Collins,? 1996", "shortCiteRegEx": "Collins", "year": 1996}, {"title": "Integrating Verb-Particle Constructions into CCG Parsing", "author": ["J. Constable", "J. Curran"], "venue": "Proceedings of the Australasian Language Technology Association Workshop2009, pages 114\u2013118, Sydney, Australia. (Not cited.)", "citeRegEx": "Constable and Curran,? 2009", "shortCiteRegEx": "Constable and Curran", "year": 2009}, {"title": "Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing", "author": ["M. Constant", "A. Sigogne", "P. Watrin"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912, pages 204\u2013212, Stroudsburg, PA, USA. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Constant et al\\.,? 2012", "shortCiteRegEx": "Constant et al\\.", "year": 2012}, {"title": "Multiword expressions: linguistic precision and reusability", "author": ["A. Copestake", "F. Lambeau", "A. Villavicencio", "F. Bond", "T. Baldwin", "I.A. Sag", "D. Flickinger"], "venue": "LREC 2002, pages 1941\u20131947. (Not cited.)", "citeRegEx": "Copestake et al\\.,? 2002", "shortCiteRegEx": "Copestake et al\\.", "year": 2002}, {"title": "Linguistically Motivated Large-Scale NLP with C&C and Boxer", "author": ["J. Curran", "S. Clark", "J. Bos"], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33\u201336, Prague, Czech Republic. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Curran et al\\.,? 2007", "shortCiteRegEx": "Curran et al\\.", "year": 2007}, {"title": "Generalizing a Strongly Lexicalized Parser using Unlabeled Data", "author": ["T. Deoskar", "C. Christodoulopoulos", "A. Birch", "M. Steedman"], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126\u2013134, Gothenburg. (Not cited.)", "citeRegEx": "Deoskar et al\\.,? 2014", "shortCiteRegEx": "Deoskar et al\\.", "year": 2014}, {"title": "Detecting Multi-word Expressions Improves Word Sense Disambiguation", "author": ["M.A. Finlayson", "N. Kulkarni"], "venue": "Proceedings of the Workshop on Multiword Expressions: From Parsing and Generation to the Real World, MWE \u201911, pages 20\u2013 24, Stroudsburg, PA, USA. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Finlayson and Kulkarni,? 2011", "shortCiteRegEx": "Finlayson and Kulkarni", "year": 2011}, {"title": "Parsing Models for Identifying Multiword Expressions", "author": ["S. Green", "de Marneffe", "M.-C", "C.D. Manning"], "venue": "Computational Linguistics,", "citeRegEx": "Green et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Green et al\\.", "year": 2013}, {"title": "Data and Models for Statistical Parsing with Combinatory Categorial Grammar", "author": ["J. Hockenmaier"], "venue": "PhD thesis, The University of Edinburgh. (Not cited.)", "citeRegEx": "Hockenmaier,? 2003", "shortCiteRegEx": "Hockenmaier", "year": 2003}, {"title": "Generative Models for Statistical Parsing with Combinatory Categorial Grammar", "author": ["J. Hockenmaier", "M. Steedman"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 335\u2013342, Stroudsburg, PA, USA. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Hockenmaier and Steedman,? 2002", "shortCiteRegEx": "Hockenmaier and Steedman", "year": 2002}, {"title": "The Handbook of Construction Grammar", "author": ["T. Hoffmann", "G. Trousdale"], "venue": "Hoffmann, T. and Trousdale, G., editors, The Oxford Handbook of Construction Grammar, Oxford Handbooks in Linguistics, chapter Construction Grammar: Introduction, pages 1\u201314. Oxford University Press. (Not cited.)", "citeRegEx": "Hoffmann and Trousdale,? 2013", "shortCiteRegEx": "Hoffmann and Trousdale", "year": 2013}, {"title": "Statistical Modeling of Multiword Expressions", "author": ["S.N. Kim"], "venue": "PhD thesis, University of Melbourne. (Not cited.)", "citeRegEx": "Kim,? 2008", "shortCiteRegEx": "Kim", "year": 2008}, {"title": "Can recognising multiword expressions improve shallow parsing? In HLT-NAACL, pages 636\u2013644", "author": ["I. Korkontzelos", "S. Manandhar"], "venue": "(Not cited.)", "citeRegEx": "Korkontzelos and Manandhar,? 2010", "shortCiteRegEx": "Korkontzelos and Manandhar", "year": 2010}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Comput. Linguist., 19(2):313\u2013 330. (Not cited.)", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "An Efficient, Generic Approach to Extracting Multi-Word Expressions from Dependency Trees", "author": ["S. Martens", "V. Vandeghinste"], "venue": "Proceedings of the Workshop", "citeRegEx": "Martens and Vandeghinste,? 2010", "shortCiteRegEx": "Martens and Vandeghinste", "year": 2010}, {"title": "Statistical Parsing", "author": ["J. Nivre"], "venue": "Indurkhya, N. and Damerau, F. J., editors, Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group, Boca Raton, FL. ISBN 978-1420085921. (Not cited.)", "citeRegEx": "Nivre,? 2010", "shortCiteRegEx": "Nivre", "year": 2010}, {"title": "Multiword units in syntactic parsing", "author": ["J. Nivre", "J. Nilsson"], "venue": "Workshop on Methodologies and Evaluation of Multiword Units in Real-World Applications, pages 39\u201346. (Not cited.)", "citeRegEx": "Nivre and Nilsson,? 2004", "shortCiteRegEx": "Nivre and Nilsson", "year": 2004}, {"title": "Idioms", "author": ["Nunberg", "Geoffrey", "Sag", "Ivan A.", "Wasow", "Thomas"], "venue": "Language, 70(3):491\u2013538. (Not cited.)", "citeRegEx": "Nunberg et al\\.,? 1994", "shortCiteRegEx": "Nunberg et al\\.", "year": 1994}, {"title": "User\u2019s guide to sigf: Significance testing by approximate randomisation", "author": ["S. Pad\u00f3"], "venue": "(Not cited.)", "citeRegEx": "Pad\u00f3,? 2006", "shortCiteRegEx": "Pad\u00f3", "year": 2006}, {"title": "Multiword Expressions: A Pain in the Neck for NLP", "author": ["I.A. Sag", "T. Baldwin", "F. Bond", "A. Copestake", "D. Flickinger"], "venue": "In Proc. of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing2002, pages 1\u201315. (Not cited.)", "citeRegEx": "Sag et al\\.,? 2001", "shortCiteRegEx": "Sag et al\\.", "year": 2001}, {"title": "Comprehensive Annotation of Multiword Expressions in a Social Web Corpus", "author": ["N. Schneider", "S. Onuffer", "N. Kazour", "E. Danchik", "M.T. Mordowanec", "H. Conrad", "N.A. Smith"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014), European Language Resources Association (ELRA). (Not cited.)", "citeRegEx": "Schneider et al\\.,? 2014", "shortCiteRegEx": "Schneider et al\\.", "year": 2014}, {"title": "On Collocations and Their Interaction with Parsing and Translation", "author": ["V. Seretan"], "venue": "Informatics, 1(1):11\u201331. (Not cited.)", "citeRegEx": "Seretan,? 2013", "shortCiteRegEx": "Seretan", "year": 2013}, {"title": "Baby Steps: How Less is More in unsupervised dependency parsing", "author": ["V.I. Spitkovsky", "H. Alshawi", "D. Jurafsky"], "venue": "In NIPS: Grammar Induction, Representation of Language and Language Learning. (Not cited.)", "citeRegEx": "Spitkovsky et al\\.,? 2009", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2009}, {"title": "The Syntactic Process", "author": ["M. Steedman"], "venue": "MIT Press, Cambridge, MA, USA. (Not cited.)", "citeRegEx": "Steedman,? 2000", "shortCiteRegEx": "Steedman", "year": 2000}, {"title": "Extraction of German Multiword Expressions from Parsed Corpora Using Context Features", "author": ["M. Weller", "U. Heid"], "venue": "Calzolari, N., Choukri, K., Maegaard,", "citeRegEx": "Weller and Heid,? 2010", "shortCiteRegEx": "Weller and Heid", "year": 2010}, {"title": "Robust parsing with a large HPSG grammar", "author": ["Y. Zhang", "V. Kordoni"], "venue": "In Proceedings of the Sixth International Language Resources and Evaluation(LREC08. (Not cited.)", "citeRegEx": "Zhang and Kordoni,? 2008", "shortCiteRegEx": "Zhang and Kordoni", "year": 2008}, {"title": "Automated Multiword Expression Prediction for Grammar Engineering", "author": ["Y. Zhang", "V. Kordoni", "A. Villavicencio", "M. Idiart"], "venue": "Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE \u201906, pages 36\u201344, Stroudsburg, PA, USA. Association for Computational Linguistics. (Not cited.)", "citeRegEx": "Zhang et al\\.,? 2006", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": "We build on previous work by Nivre and Nilsson (2004) and by Korkontzelos and Manandhar (2010) who have shown the benefit of adding information about MWEs to syntactic parsing by implementing a similar pipeline with CCG parsing.", "startOffset": 29, "endOffset": 54}, {"referenceID": 22, "context": "We build on previous work by Nivre and Nilsson (2004) and by Korkontzelos and Manandhar (2010) who have shown the benefit of adding information about MWEs to syntactic parsing by implementing a similar pipeline with CCG parsing.", "startOffset": 61, "endOffset": 95}, {"referenceID": 19, "context": "More specifically, we collapse MWEs to one token in training and test data in CCGbank, a corpus automatically created by Hockenmaier (2003) which contains sentences annotated with CCG derivations.", "startOffset": 121, "endOffset": 140}, {"referenceID": 30, "context": "They represent a wide variety of phenomena with different properties but are generally agreed to be a group of multiple lexemes which have some level of idiomaticity or irregularity (Sag et al., 2001).", "startOffset": 182, "endOffset": 200}, {"referenceID": 27, "context": "Recent research is showing that information about MWEs help the syntactic parsing task (Nivre and Nilsson, 2004; Korkontzelos and Manandhar, 2010) and inversely, information about syntactic analysis helps MWE identification (Green et al.", "startOffset": 87, "endOffset": 146}, {"referenceID": 23, "context": "Recent research is showing that information about MWEs help the syntactic parsing task (Nivre and Nilsson, 2004; Korkontzelos and Manandhar, 2010) and inversely, information about syntactic analysis helps MWE identification (Green et al.", "startOffset": 87, "endOffset": 146}, {"referenceID": 18, "context": "Recent research is showing that information about MWEs help the syntactic parsing task (Nivre and Nilsson, 2004; Korkontzelos and Manandhar, 2010) and inversely, information about syntactic analysis helps MWE identification (Green et al., 2013; Weller and Heid, 2010; Martens and Vandeghinste, 2010).", "startOffset": 224, "endOffset": 299}, {"referenceID": 35, "context": "Recent research is showing that information about MWEs help the syntactic parsing task (Nivre and Nilsson, 2004; Korkontzelos and Manandhar, 2010) and inversely, information about syntactic analysis helps MWE identification (Green et al., 2013; Weller and Heid, 2010; Martens and Vandeghinste, 2010).", "startOffset": 224, "endOffset": 299}, {"referenceID": 25, "context": "Recent research is showing that information about MWEs help the syntactic parsing task (Nivre and Nilsson, 2004; Korkontzelos and Manandhar, 2010) and inversely, information about syntactic analysis helps MWE identification (Green et al., 2013; Weller and Heid, 2010; Martens and Vandeghinste, 2010).", "startOffset": 224, "endOffset": 299}, {"referenceID": 4, "context": "Early research (Church and Patil, 1982) has shown that existing grammars are highly ambiguous and has called for the need for two more components: a probability model for scoring the different", "startOffset": 15, "endOffset": 39}, {"referenceID": 4, "context": "Clark (2010)) to be a central task in NLP, it is used in many applications such as Tree-based Machine Translation, Question Answering etc.", "startOffset": 0, "endOffset": 13}, {"referenceID": 24, "context": "Most work on syntactic parsing is based on a manually annotated corpus, the Penn Treebank (Marcus et al., 1993) which has been used to infer a probabilistic model together with a Context Free Grammar (henceforth CFG) or a PCFG .", "startOffset": 90, "endOffset": 111}, {"referenceID": 0, "context": "1) Syntactic parsing has traditionally been evaluated by computing parsing accuracy and parsing accuracy has traditionally been measured with the PARSEVAL metric (Black et al., 1991).", "startOffset": 162, "endOffset": 182}, {"referenceID": 0, "context": "1) Syntactic parsing has traditionally been evaluated by computing parsing accuracy and parsing accuracy has traditionally been measured with the PARSEVAL metric (Black et al., 1991). This parsing accuracy measure is obtained by comparing output parse trees with the treebank gold standard: labelled precision and recall of nodes in the trees are computed as well as their F-measure. Collins reported parsing accuracy results of over 90% which were then slightly improved by Charniak and Johnson (2005). Since these early results however, a lot has been tried but not a lot of improvement has been made which highlights the difficulty of the task at hand.", "startOffset": 163, "endOffset": 503}, {"referenceID": 0, "context": "1) Syntactic parsing has traditionally been evaluated by computing parsing accuracy and parsing accuracy has traditionally been measured with the PARSEVAL metric (Black et al., 1991). This parsing accuracy measure is obtained by comparing output parse trees with the treebank gold standard: labelled precision and recall of nodes in the trees are computed as well as their F-measure. Collins reported parsing accuracy results of over 90% which were then slightly improved by Charniak and Johnson (2005). Since these early results however, a lot has been tried but not a lot of improvement has been made which highlights the difficulty of the task at hand. However, as pointed out by Clark (2010), research has shifted the focus away from building complex generative models to discriminative models, which estimate the probability of a parse tree for a sentence directly instead of estimating parse tree and sentence together, and which avoid independence assumptions.", "startOffset": 163, "endOffset": 696}, {"referenceID": 0, "context": "1) Syntactic parsing has traditionally been evaluated by computing parsing accuracy and parsing accuracy has traditionally been measured with the PARSEVAL metric (Black et al., 1991). This parsing accuracy measure is obtained by comparing output parse trees with the treebank gold standard: labelled precision and recall of nodes in the trees are computed as well as their F-measure. Collins reported parsing accuracy results of over 90% which were then slightly improved by Charniak and Johnson (2005). Since these early results however, a lot has been tried but not a lot of improvement has been made which highlights the difficulty of the task at hand. However, as pointed out by Clark (2010), research has shifted the focus away from building complex generative models to discriminative models, which estimate the probability of a parse tree for a sentence directly instead of estimating parse tree and sentence together, and which avoid independence assumptions. Research has also drawn attention to building grammars based on expressive formalisms such as Tree Adjoining Grammar (henceforth TAG), Lexical Functional Grammar (henceforth LFG), HPSG and CCG which offer more linguistically accurate representations of language. According to Nivre (2010), the tendency of using supervised methods for parsing is moving towards unsupervised methods such as in Spitkovsky et al.", "startOffset": 163, "endOffset": 1257}, {"referenceID": 0, "context": "1) Syntactic parsing has traditionally been evaluated by computing parsing accuracy and parsing accuracy has traditionally been measured with the PARSEVAL metric (Black et al., 1991). This parsing accuracy measure is obtained by comparing output parse trees with the treebank gold standard: labelled precision and recall of nodes in the trees are computed as well as their F-measure. Collins reported parsing accuracy results of over 90% which were then slightly improved by Charniak and Johnson (2005). Since these early results however, a lot has been tried but not a lot of improvement has been made which highlights the difficulty of the task at hand. However, as pointed out by Clark (2010), research has shifted the focus away from building complex generative models to discriminative models, which estimate the probability of a parse tree for a sentence directly instead of estimating parse tree and sentence together, and which avoid independence assumptions. Research has also drawn attention to building grammars based on expressive formalisms such as Tree Adjoining Grammar (henceforth TAG), Lexical Functional Grammar (henceforth LFG), HPSG and CCG which offer more linguistically accurate representations of language. According to Nivre (2010), the tendency of using supervised methods for parsing is moving towards unsupervised methods such as in Spitkovsky et al. (2009). Some authors have given up on improv-", "startOffset": 163, "endOffset": 1386}, {"referenceID": 1, "context": "ing parsing accuracy directly but have tried working on it indirectly by working on it as a joint task together with another task such such as Burkett and Klein (2008) who show the benefit of combining Machine Translation and Syntactic Parsing as well as the work which has tried to improve Multiword Expression identification together with syntactic parsing, described in Section 2.", "startOffset": 143, "endOffset": 168}, {"referenceID": 1, "context": "ing parsing accuracy directly but have tried working on it indirectly by working on it as a joint task together with another task such such as Burkett and Klein (2008) who show the benefit of combining Machine Translation and Syntactic Parsing as well as the work which has tried to improve Multiword Expression identification together with syntactic parsing, described in Section 2.5. In any case, although the percentages obtained by the seminal works on generative models may seem very high, syntactic parsing is far from being a solved problem, as argued by Clark (2010), because these percentages are inflated by recurring derivations and cannot deal with many important constructions.", "startOffset": 143, "endOffset": 575}, {"referenceID": 30, "context": "The most commonly acknowledged definition of this term since Sag et al. (2001) is that it is a group of multiple lexemes which have some level of idiomaticity or irregularity.", "startOffset": 61, "endOffset": 79}, {"referenceID": 22, "context": "As described at length in Kim\u2019s (2008) thesis, \u2018dealing with\u2019 MWEs consists in developing systems and models for various kinds of tasks.", "startOffset": 26, "endOffset": 39}, {"referenceID": 34, "context": "Combinatory Categorial Grammar (Steedman, 2000) is a strongly lexicalized grammar formalism which is currently gaining popularity in the NLP community.", "startOffset": 31, "endOffset": 47}, {"referenceID": 21, "context": "It is a tenet of the recently emerging framework of Construction Grammar (Hoffmann and Trousdale, 2013).", "startOffset": 73, "endOffset": 103}, {"referenceID": 29, "context": "Sag et al. (2001) have suggested sophisticated ways of representing the different MWE types in a grammar, which have been partly implemented within the framework of the precision grammar HPSG, as described by Copestake et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "(2001) have suggested sophisticated ways of representing the different MWE types in a grammar, which have been partly implemented within the framework of the precision grammar HPSG, as described by Copestake et al. (2002). Zhang et al.", "startOffset": 198, "endOffset": 222}, {"referenceID": 14, "context": "(2001) have suggested sophisticated ways of representing the different MWE types in a grammar, which have been partly implemented within the framework of the precision grammar HPSG, as described by Copestake et al. (2002). Zhang et al. (2006) recently established that MWEs are a tremendous source of parse failures when parsing with a precision grammar such as HPSG and henceforth proposed a way of using this information to identify new MWEs and enrich a lexicon: they suggested using parse failures to predict the existence of a MWE.", "startOffset": 198, "endOffset": 243}, {"referenceID": 25, "context": "Since the seminal work of Nivre and Nilsson (2004), research has shown that treating MWEs as one token or a \u2018word-with-spaces\u2019 in test and/or in training data before parsing and/or training leads to an improvement in parsing accuracy.", "startOffset": 26, "endOffset": 51}, {"referenceID": 25, "context": "Since the seminal work of Nivre and Nilsson (2004), research has shown that treating MWEs as one token or a \u2018word-with-spaces\u2019 in test and/or in training data before parsing and/or training leads to an improvement in parsing accuracy. Nivre and Nilsson (2004) have shown that to be true for deterministic dependency parsing and Korkontzelos and Manandhar (2010) have shown that to be true for shallow parsing.", "startOffset": 26, "endOffset": 260}, {"referenceID": 23, "context": "Nivre and Nilsson (2004) have shown that to be true for deterministic dependency parsing and Korkontzelos and Manandhar (2010) have shown that to be true for shallow parsing.", "startOffset": 93, "endOffset": 127}, {"referenceID": 13, "context": "Constant et al. (2012) have recently shown that with an automatic recognizer, the parsing accuracy improvement is not as dramatic as predicted by Nivre and Nilsson (2004), giving a negative answer to the second of these questions.", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "Constant et al. (2012) have recently shown that with an automatic recognizer, the parsing accuracy improvement is not as dramatic as predicted by Nivre and Nilsson (2004), giving a negative answer to the second of these questions.", "startOffset": 0, "endOffset": 171}, {"referenceID": 18, "context": "Green et al. (2013) successfully tuned a parser for MWE identification, Weller and Heid (2010) and Martens and Vandeghinste (2010) showed that using parsed corpora for MWE identification is beneficial.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "Green et al. (2013) successfully tuned a parser for MWE identification, Weller and Heid (2010) and Martens and Vandeghinste (2010) showed that using parsed corpora for MWE identification is beneficial.", "startOffset": 0, "endOffset": 95}, {"referenceID": 18, "context": "Green et al. (2013) successfully tuned a parser for MWE identification, Weller and Heid (2010) and Martens and Vandeghinste (2010) showed that using parsed corpora for MWE identification is beneficial.", "startOffset": 0, "endOffset": 131}, {"referenceID": 18, "context": "Green et al. (2013) successfully tuned a parser for MWE identification, Weller and Heid (2010) and Martens and Vandeghinste (2010) showed that using parsed corpora for MWE identification is beneficial. These findings led Seretan (2013) to propose that neither accommodating the grammar with MWE information, nor recognizing MWEs in raw text as a help to parsing are appropriate ways of dealing with the issue of MWEs in syntactic parsing because neither approach takes advantage of the fact that MWE information and syntactic analysis are mutually informative.", "startOffset": 0, "endOffset": 236}, {"referenceID": 36, "context": "Research on HPSG seems to have found the most sophisticated methods of dealing with MWEs but parsing with precision grammars is known to be much less robust (Zhang and Kordoni, 2008) (i.", "startOffset": 157, "endOffset": 182}, {"referenceID": 22, "context": "However, as argued by Kim (2008), because of the different but interrelated properties of MWEs, it is neither appropriate to try and generalize from MWEs and find a single representation which works for all types, nor is it appropriate to deal with each MWE type at a time.", "startOffset": 22, "endOffset": 33}, {"referenceID": 8, "context": "It is, to name just a couple of examples, used for question parsing (Clark et al., 2004) and semantic parsing.", "startOffset": 68, "endOffset": 88}, {"referenceID": 5, "context": "It is, to name just a couple of examples, used for question parsing (Clark et al., 2004) and semantic parsing. The first efficient statistical model was the generative model built by Hockenmaier and Steedman (2002) and extended by Clark and Curran (2007) to a discriminative model.", "startOffset": 69, "endOffset": 215}, {"referenceID": 5, "context": "It is, to name just a couple of examples, used for question parsing (Clark et al., 2004) and semantic parsing. The first efficient statistical model was the generative model built by Hockenmaier and Steedman (2002) and extended by Clark and Curran (2007) to a discriminative model.", "startOffset": 69, "endOffset": 255}, {"referenceID": 5, "context": "It is, to name just a couple of examples, used for question parsing (Clark et al., 2004) and semantic parsing. The first efficient statistical model was the generative model built by Hockenmaier and Steedman (2002) and extended by Clark and Curran (2007) to a discriminative model. The first step made by Hockenmaier (2003) was to translate the PTB into a Treebank with CCG derivations, called CCGbank.", "startOffset": 69, "endOffset": 324}, {"referenceID": 5, "context": "The traditional parsing accuracy metric PARSEVAL has been argued (for example by Clark and Hockenmaier (2002)) to be too harsh on CCG derivation trees because they are always binary, as opposed to PTB-style trees which can have flat constructions with more than one branching node.", "startOffset": 81, "endOffset": 110}, {"referenceID": 5, "context": "The traditional parsing accuracy metric PARSEVAL has been argued (for example by Clark and Hockenmaier (2002)) to be too harsh on CCG derivation trees because they are always binary, as opposed to PTB-style trees which can have flat constructions with more than one branching node. This binary nature of CCG trees make them prone to having more errors. Consequently evaluation of dependencies has generally been preferred for CCG parsing. For this type of evaluation, labelled and/or unlabelled precision and recall as well as F-measure are computed by comparing dependencies extracted from the output parse trees with dependencies extracted from gold standard trees. As further argued by Clark and Hockenmaier (2002), it also makes sense to use dependencies to evaluate CCG parsing since one of the advantages of CCG over other formalisms is precisely its treatment of long-range dependencies.", "startOffset": 81, "endOffset": 718}, {"referenceID": 12, "context": "Constable and Curran (2009) modified CCGbank to have a better representation of verb particle constructions but did not report any parsing accuracy improvement.", "startOffset": 0, "endOffset": 28}, {"referenceID": 13, "context": "As also said, Constant et al. (2012) have shown that with an automatic recognizer, the parsing accuracy improvement is not as dramatic as predicted by Nivre and Nilsson (2004) using gold standard MWE recognition.", "startOffset": 14, "endOffset": 37}, {"referenceID": 13, "context": "As also said, Constant et al. (2012) have shown that with an automatic recognizer, the parsing accuracy improvement is not as dramatic as predicted by Nivre and Nilsson (2004) using gold standard MWE recognition.", "startOffset": 14, "endOffset": 176}, {"referenceID": 13, "context": "As also said, Constant et al. (2012) have shown that with an automatic recognizer, the parsing accuracy improvement is not as dramatic as predicted by Nivre and Nilsson (2004) using gold standard MWE recognition. We will be conducting the type of approach described by Nivre and Nilsson (2004) using an automatic recognizer to answer the first of these questions in the context of CCG parsing.", "startOffset": 14, "endOffset": 294}, {"referenceID": 13, "context": "As also said, Constant et al. (2012) have shown that with an automatic recognizer, the parsing accuracy improvement is not as dramatic as predicted by Nivre and Nilsson (2004) using gold standard MWE recognition. We will be conducting the type of approach described by Nivre and Nilsson (2004) using an automatic recognizer to answer the first of these questions in the context of CCG parsing. We will also experiment with the recognizer by using different versions of it to answer the second of these two questions. This approach is especially interesting in that, as has been shown in Schneider et al. (2014) who attempted a comprehensive annotation of MWEs in a corpus, even manual annotation of MWEs is a difficult task and making automatic MWE recognition an experimental part of the pipeline could lead to interesting results.", "startOffset": 14, "endOffset": 611}, {"referenceID": 23, "context": "The qualitative analysis of output proposed by Korkontzelos and Manandhar (2010) would add a lot of insight to this thesis but will not be conducted for reasons of time and space.", "startOffset": 47, "endOffset": 81}, {"referenceID": 23, "context": "The qualitative analysis of output proposed by Korkontzelos and Manandhar (2010) would add a lot of insight to this thesis but will not be conducted for reasons of time and space. As a matter of fact, Korkontzelos and Manandhar (2010) built a taxonomy of output differences and used it to quantify each change in the output data but their taxonomy is not readily usable for another syntactic model.", "startOffset": 47, "endOffset": 235}, {"referenceID": 15, "context": "(2014) in using the C&C tools (Curran et al., 2007) to POS-tag our test data so as to have a model that is comparable with theirs.", "startOffset": 30, "endOffset": 51}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al.", "startOffset": 42, "endOffset": 68}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training.", "startOffset": 42, "endOffset": 123}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training. The expansion of the parser by Deoskar et al. (2014) is particularly well suited to our purposes: it was extended so that it works better on unknown lexical items.", "startOffset": 42, "endOffset": 235}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training. The expansion of the parser by Deoskar et al. (2014) is particularly well suited to our purposes: it was extended so that it works better on unknown lexical items. Collapsing lexical items will increase the sparsity of data and being able to deal with unknown data is therefore a concern for our approach. More particularly, the model proposed by Christodoulopoulos (2008) and Deoskar et al.", "startOffset": 42, "endOffset": 555}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training. The expansion of the parser by Deoskar et al. (2014) is particularly well suited to our purposes: it was extended so that it works better on unknown lexical items. Collapsing lexical items will increase the sparsity of data and being able to deal with unknown data is therefore a concern for our approach. More particularly, the model proposed by Christodoulopoulos (2008) and Deoskar et al. (2014) is based on one of Hockenmaier (2003)\u2019s models called LexCat and which conditions probabilities on lexical categories.", "startOffset": 42, "endOffset": 581}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training. The expansion of the parser by Deoskar et al. (2014) is particularly well suited to our purposes: it was extended so that it works better on unknown lexical items. Collapsing lexical items will increase the sparsity of data and being able to deal with unknown data is therefore a concern for our approach. More particularly, the model proposed by Christodoulopoulos (2008) and Deoskar et al. (2014) is based on one of Hockenmaier (2003)\u2019s models called LexCat and which conditions probabilities on lexical categories.", "startOffset": 42, "endOffset": 619}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training. The expansion of the parser by Deoskar et al. (2014) is particularly well suited to our purposes: it was extended so that it works better on unknown lexical items. Collapsing lexical items will increase the sparsity of data and being able to deal with unknown data is therefore a concern for our approach. More particularly, the model proposed by Christodoulopoulos (2008) and Deoskar et al. (2014) is based on one of Hockenmaier (2003)\u2019s models called LexCat and which conditions probabilities on lexical categories. Deoskar et al. (2014) make use of this LexCat model instead of the fully lexicalized model which conditions it on words precisely so that the parser is better equipped to deal with unseen lexical items.", "startOffset": 42, "endOffset": 722}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training. The expansion of the parser by Deoskar et al. (2014) is particularly well suited to our purposes: it was extended so that it works better on unknown lexical items. Collapsing lexical items will increase the sparsity of data and being able to deal with unknown data is therefore a concern for our approach. More particularly, the model proposed by Christodoulopoulos (2008) and Deoskar et al. (2014) is based on one of Hockenmaier (2003)\u2019s models called LexCat and which conditions probabilities on lexical categories. Deoskar et al. (2014) make use of this LexCat model instead of the fully lexicalized model which conditions it on words precisely so that the parser is better equipped to deal with unseen lexical items. They introduce a smoothed lexicon to deal with these. They POS-tag the test data in a pre-processing stage and use POS-information to determine the lexical categories of words by using probabilities of lexical categories that appear with each POS-tag of unseen word in the seen data. Because, as mentioned in Section 2.5.2.2, Korkontzelos and Manandhar (2010) have shown that POS-tags assigned automatically to MWEs were useful when parsing, the LexCat model therefore looks ideal for our purposes.", "startOffset": 42, "endOffset": 1263}, {"referenceID": 3, "context": "We chose to use StatOpenCCG, developed by Christodoulopoulos (2008) and recently further expanded by Deoskar et al. (2014) because of its ease of use, flexibility and fast training. The expansion of the parser by Deoskar et al. (2014) is particularly well suited to our purposes: it was extended so that it works better on unknown lexical items. Collapsing lexical items will increase the sparsity of data and being able to deal with unknown data is therefore a concern for our approach. More particularly, the model proposed by Christodoulopoulos (2008) and Deoskar et al. (2014) is based on one of Hockenmaier (2003)\u2019s models called LexCat and which conditions probabilities on lexical categories. Deoskar et al. (2014) make use of this LexCat model instead of the fully lexicalized model which conditions it on words precisely so that the parser is better equipped to deal with unseen lexical items. They introduce a smoothed lexicon to deal with these. They POS-tag the test data in a pre-processing stage and use POS-information to determine the lexical categories of words by using probabilities of lexical categories that appear with each POS-tag of unseen word in the seen data. Because, as mentioned in Section 2.5.2.2, Korkontzelos and Manandhar (2010) have shown that POS-tags assigned automatically to MWEs were useful when parsing, the LexCat model therefore looks ideal for our purposes. We follow Deoskar et al. (2014) in using the C&C tools (Curran et al.", "startOffset": 42, "endOffset": 1434}, {"referenceID": 19, "context": "6 is a translation of the PTB into CCG derivations built by Hockenmaier (2003) with the objective of building parsing models for CCG.", "startOffset": 60, "endOffset": 79}, {"referenceID": 17, "context": "1 Recognizing MWEs For MWE recognition, we used an existing tool developed by Finlayson and Kulkarni (2011). It is a flexible library which offers many tools useful to our purposes.", "startOffset": 78, "endOffset": 108}, {"referenceID": 16, "context": "To make sure we were working on improving a model that already works well, we first attempted to reproduce the results obtained by Deoskar et al. (2014) on the non-collapsed data.", "startOffset": 131, "endOffset": 153}, {"referenceID": 29, "context": "We used the software created by Pad\u00f3 (2006) (slightly modified in order to make it a one-tailed test instead of a two-tailed one) for our tests.", "startOffset": 32, "endOffset": 44}, {"referenceID": 30, "context": "For other types of MWE, an analysis as word-with-spaces might not be the most appropriate, as argued by many (Sag et al. (2001) to give just one example, see Chapter 2).", "startOffset": 110, "endOffset": 128}, {"referenceID": 30, "context": "In addition, MWEs have been said to be quite domain-dependent (Sag et al., 2001) and working on different data might impact the results differently.", "startOffset": 62, "endOffset": 80}, {"referenceID": 23, "context": "In addition, we used a purely quantitative evaluation but it would be very interesting to conduct a more qualitative evaluation such as the one by Korkontzelos and Manandhar (2010) described in Section 2.", "startOffset": 147, "endOffset": 181}], "year": 2015, "abstractText": "This thesis presents a study about the integration of information about Multiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar (CCG). We build on previous work by Nivre and Nilsson (2004) and by Korkontzelos and Manandhar (2010) who have shown the benefit of adding information about MWEs to syntactic parsing by implementing a similar pipeline with CCG parsing. More specifically, we collapse MWEs to one token in training and test data in CCGbank, a corpus automatically created by Hockenmaier (2003) which contains sentences annotated with CCG derivations. Our collapsing algorithm however can only deal with MWEs when they form a constituent in the data which is one of the limitations of our approach. We study the effect of collapsing training and test data. A parsing effect can be obtained if collapsed data help the parser in its decisions and a training effect can be obtained if training on the collapsed data improves results. We also collapse the gold standard and show that our model significantly outperforms the baseline model on our gold standard, which indicates that there is a training effect. We show that the baseline model performs significantly better on our gold standard when the data are collapsed before parsing than when the data are collapsed after parsing which indicates that there is a parsing effect. We show that these results can lead to improved performance on the non-collapsed standard benchmark although we fail to show that it does so significantly. We conclude that despite the limited settings, there are noticeable improvements from using MWEs in parsing. We discuss ways in which the incorporation of MWEs into parsing can be improved and hypothesize that this will lead to more substantial results. We finally show that turning the MWE recognition part of the pipeline into an experimental part is a useful thing to do as we obtain different results with different recognizers.", "creator": "LaTeX with hyperref package"}}}