{"id": "1505.04497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "A Definition of Happiness for Reinforcement Learning Agents", "abstract": "What is happiness for reinforcement learning agents? We seek a formal definition satisfying a list of desiderata. Our proposed definition of happiness is the temporal difference error, i.e. the difference between the value of the obtained reward and observation and the agent's expectation of this value. This definition satisfies most of our desiderata and is compatible with empirical research on humans. We state several implications and discuss examples.", "histories": [["v1", "Mon, 18 May 2015 03:14:39 GMT  (37kb,D)", "http://arxiv.org/abs/1505.04497v1", "AGI 2015"]], "COMMENTS": "AGI 2015", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mayank daswani", "jan leike"], "accepted": false, "id": "1505.04497"}, "pdf": {"name": "1505.04497.pdf", "metadata": {"source": "CRF", "title": "A Definition of Happiness for Reinforcement Learning Agents\u2217", "authors": ["Mayank Daswani", "Jan Leike"], "emails": [], "sections": [{"heading": null, "text": "Keywords: temporal difference error, prediction error, pleasure, well-being, optimism, machine ethics"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 Reinforcement Learning", "text": "In Reinforcement Learning (RL), an agent interacts with an environment in cycles: in the time step t, the agent selects an action at the boundary of A and receives an observation at the boundary of O and a real-world reward at the boundary of R; the cycle then repeats for the time step t + 1 [SB98]. The list of interactions a1o1r1a2o2r2.. is called history. We use ht to denote a story of length t, and we use the abbreviation h: = ht \u2212 1 and h \u2032: = ht \u2212 1atotrt. The objective of the agent is to choose measures to maximize cumulative rewards. To avoid infinite sums, we use a discount factor g with 0 < < < < 1 and maximize the discounted sum we expect t = 1 categorized trt. A policy is a function that sets each story on the boundary action, categorizes an environment."}, {"heading": "3 A Formal Definition of Happiness", "text": "The goal of an enhanced learning agent is to maximize rewards, so it seems natural to assume that the more rewards an agent receives, the happier he will be. However, this does not correspond to our intuition: Sometimes we simply cannot equate joy to happiness, and vice versa, sustained suffering does not necessarily mean dissatisfaction (see Example 3 and Example 7). In fact, it is empirically proven that rewards and happiness cannot be equated [RSDD14] (p-value < 0.0001).There is also a formal problem with the definition of happiness in terms of reward: we can add a constant c-R to each reward. No matter how the agent-environment interaction goes, the agent will have received additional rewards."}, {"heading": "4 Matching the Desiderata", "text": "In this case, it is as if people are able to save themselves. (...) It is not as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...)"}, {"heading": "5 Discussion and Examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Off-policy Agents", "text": "A common difference between RL algorithms is whether they learn outside politics or inside politics. A local agent evaluates the value of the policy he is currently pursuing. For example, if the policy that the agent is supposed to follow is reduced to zero over time, the agent's learned policy in MDPs tends to be the optimal policy. Alternatively, an agent can learn outside politics, that is, he can learn about a policy (say, the optimal one) while following a different behavioral policy. Behavioral policy (\u03c0b) determines how the agent acts while learning the optimal policy. Once an agent has learned the optimal value function outside politics, he is not happy when he is acting according to another (possibly suboptimal) policy."}, {"heading": "5.2 Increasing and Decreasing Rewards", "text": "Intuitively, it seems that if things keep getting better, it should increase satisfaction. However, this is generally not the case: even a drug that receives monotonously increasing rewards can be unhappy if it thinks that these rewards mean even higher negative rewards in the future. Example 7. Alice has enrolled in a questionable drug study investigating the effects of a potentially harmful drug. This drug gives the user transitory pleasure every time it is used, and increased consumption leads to increased pleasure. However, the drug reduces quality of life in the long term. Alice has been informed of the potential side effects of the drug. She can be either part of a placebo group or the group that is administered the drug. Every morning, Alice gets a syringe of an unknown liquid. She feels temporary, but intense feelings of satisfaction. This is evidence that she is in the non-placebo group and therefore has a potentially impaired quality of life (so she may also have a very poor sense of satisfaction)."}, {"heading": "5.3 Value Function Initialisation", "text": "Example 8 (Increasing pessimism does not increase happiness). Consider the deterministic MDP example in Figure 1. Suppose that the agent has an initial value function Q-0 (s0, \u03b1) = 0, Q-0 (s0, \u03b2) = \u2212 \u03b5, Q-0 (s1, \u03b1) = \u03b5 and Q-0 (s1, \u03b2) = 0. If no forced exploration is carried out by the agent, he has no incentive to visit s1. Happiness achieved by such an agent for some time is t, (s0\u03b1s00, V-0) = 0, where V-0 (s0): = Q-0 (s0, \u03b1) = 0. Suppose, however, that the agent (more optimistic) arrives with Q-0 (s0, \u03b1) = 0, Q-0 (s0, \u03b2) = 1 (s1, \u03b1) = 12, the agent would take action and arrive in the state s1."}, {"heading": "5.4 Maximising Happiness", "text": "How can an agent increase his own happiness? The first source of happiness, happiness, depends entirely on the outcome of a random event over which the agent has no control. However, the agent could change his learning algorithm to be systematically pessimistic about the environment. For example, if he sets the estimate of the value function for all stories below rmin / (1 \u2212 \u03b3), happiness is positive at every step, but this agent would not actually take any meaningful action. In the same way that optimism is often used to artificially increase exploration, pessimism discourages exploration that leads to poor performance. As shown in Example 8, a pessimistic agent can be less happy than a more optimistic one. Moreover, an agent who explicitly tries to maximize his own happiness is no longer an amplifier. So, instead of asking how an agent can increase his own happiness, we should set an amplification learning algorithm and ask about the environment that would make the algorithm happy."}, {"heading": "6 Conclusion", "text": "An artificial super-intelligence could contain suffering sub-routines, a phenomenon that Bostrom calls a mental crime [Bos14, chap. 8]. More broadly, Tomasik argues that even current reinforcement-learning agents could have moral weight [Tom14]. If this is the case, then a general theory of happiness is essential for reinforcement-learning; it would allow us to derive ethical standards in the treatment of algorithms. Our theory is very preliminary and should be viewed as a small step in this direction. Many questions remain unanswered, and we hope to see more research into the suffering of AI agents in the future. We thank Marcus Hutter and Brian Tomasik for careful reading and detailed feedback. Data from the smartphone experiment were kindly provided by Robb Rutledge. We are also grateful to many of our friends for encouragement and interesting discussions."}, {"heading": "A Omitted Proofs", "text": "Proposition 2, (h), k), k), k), k), k), k), k), k), k), k), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c)), c), c), c), c), c), c)), c)), c)), c)), c)), c)), c)), c))), c))), c))))), c))))), c)))), c), c)))))), c))))), c)))), c))))), c)))))), c))))), c))))))))), c)))))))))))))), c)))))), c)))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))"}, {"heading": "B Data Analysis", "text": "In this section we describe the details of the data analysis on the Great Brain Experiment 2, conducted by Rutledge et al. [RSDD14]. This experiment measures the subjective well-being on a smartphone app and had 18,420 subjects. Each subject goes through 30 studies and starts with 500 points. At the beginning of a study, the subject is given an option to choose between a certain reward (CR) and a 50-50 game of gambling between two outcomes. Gambling is depicted to have an equal probability between the outcomes, making it easy for the subject to choose between the particular reward and gambling. Before the studies begin, the agent is asked to rate their current happiness on a scale of 0-100. The slider for the scale is initialized randomly. Every two to three studies and 12 times in each game, the question is asked \"How happy are you at this moment?\" We model this experiment as an amplification problem that can represent the 150 learning states that sperm may be."}], "references": [{"title": "Hedonic relativism and planning the good society", "author": ["Philip Brickman", "Donald T Campbell"], "venue": "Adaptation-Level Theory, pages 287\u2013305,", "citeRegEx": "Brickman and Campbell.,? \\Q1971\\E", "shortCiteRegEx": "Brickman and Campbell.", "year": 1971}, {"title": "Lottery winners and accident victims: Is happiness relative", "author": ["Philip Brickman", "Dan Coates", "Ronnie Janoff-Bulman"], "venue": "Journal of Personality and Social Psychology,", "citeRegEx": "Brickman et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Brickman et al\\.", "year": 1978}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2014\\E", "shortCiteRegEx": "Bostrom.", "year": 2014}, {"title": "Beyond the hedonic treadmill: Revising the adaptation theory of well-being", "author": ["Ed Diener", "Richard E Lucas", "Christie Napa Scollon"], "venue": "American Psychologist,", "citeRegEx": "Diener et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Diener et al\\.", "year": 2006}, {"title": "distress, hope, and fear in reinforcement learning", "author": ["Elmer Jacobs", "Joost Broekens", "Catholijn Jonker. Joy"], "venue": "In Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Jacobs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 2014}, {"title": "Reinforcement learning in the brain", "author": ["Yael Niv"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Niv.,? \\Q2009\\E", "shortCiteRegEx": "Niv.", "year": 2009}, {"title": "A computational and neural model of momentary subjective well-being", "author": ["Robb B Rutledge", "Nikolina Skandali", "Peter Dayan", "Raymond J Dolan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Rutledge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rutledge et al\\.", "year": 2014}, {"title": "Time-derivative models of Pavlovian reinforcement", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1990\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1990}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Do artificial reinforcement-learning agents matter morally", "author": ["Brian Tomasik"], "venue": "Technical report,", "citeRegEx": "Tomasik.,? \\Q2014\\E", "shortCiteRegEx": "Tomasik.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "What is happiness for reinforcement learning agents? We seek a formal definition satisfying a list of desiderata. Our proposed definition of happiness is the temporal difference error, i.e. the difference between the value of the obtained reward and observation and the agent\u2019s expectation of this value. This definition satisfies most of our desiderata and is compatible with empirical research on humans. We state several implications and discuss examples.", "creator": "LaTeX with hyperref package"}}}