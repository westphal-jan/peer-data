{"id": "1505.04636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning", "abstract": "Distributed computing excels at processing large scale data, but the communication cost for synchronizing the shared parameters may slow down the overall performance. Fortunately, the interactions between parameter and data in many problems are sparse, which admits efficient partition in order to reduce the communication overhead.", "histories": [["v1", "Mon, 18 May 2015 13:43:46 GMT  (194kb,D)", "http://arxiv.org/abs/1505.04636v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["mu li", "dave g", "ersen", "alexander j smola"], "accepted": false, "id": "1505.04636"}, "pdf": {"name": "1505.04636.pdf", "metadata": {"source": "CRF", "title": "Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning", "authors": ["Mu Li", "Dave G. Andersen", "Alexander J. Smola"], "emails": ["muli@cs.cmu.edu", "dga@cs.cmu.edu", "alex@smola.org"], "sections": [{"heading": null, "text": "In this thesis we formulate data placement as a problem of graph partitioning. We propose a distributed partitioning algorithm. We give both theoretical guarantees and a highly efficient implementation. We also offer a highly efficient implementation of the algorithm and demonstrate its promising results both on text records and in social networks. We show that the proposed algorithm leads to a 1.6-fold acceleration of a modern distributed machine learning system by eliminating 90% of network communication."}, {"heading": "1. INTRODUCTION", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. GRAPH PARTITIONING", "text": "In this section, we first present the problem of inference and the model of dependencies in the distributed inference, then the formulation of the problem of data partitioning in the distributed inference, and finally a brief overview of related work."}, {"heading": "2.1 Inference in Machine Learning", "text": "In machine learning, many consequence problems have graphically structured dependencies. For example, in risk minimization [14] we strive to solve w R [w]: = m \u2211 i = 1 l (xi, yi, w) + [w], (1) where l (xi, yi, w) is a loss function that measures the model's adjustment error in the data (xi, yi), and b [w] is a regulator of the model parameter w. Data and parameters often correlate only with the unequal terms in xi, which have sparse patterns in many applications. For example, in e-mail spam filtering, elements of xi 'correspond with words and attributes in e-mails, while in computer-mail advertising they correlate words in ads and user behavioral patterns. For undirected graphical models [4, 17] the common distribution of random variables in log-letter summaries can be shown as a potential learning chart."}, {"heading": "2.2 Bipartite Graphs", "text": "The dependencies in the above inference problems can be modeled by a two-part graph G (U, V, E) with vertex sets U and V and edge set E. We designate the edge between two nodes u, U and v, V according to (u, v), E. Figure 2 illustrates the case of risk minimization (1) where U consists of samples {(xi, yi)} mi = 1 and V consists of the parameters w. There is an edge (xi, yi), wj) if and only if the j element of xi is not zero. Therefore, {wj: ((xi, yi), wj) is the working set of w to evaluate the loss function l (xi, yi, w) on the sample (xi, yi). We can construct such a two-part graph G (U, V, E) to code the dependencies in undirected graphical models."}, {"heading": "2.3 Distributed Inference", "text": "One solution is to divide the additive form of R [w] to split the optimization into smaller problems, and then deploy multiple machines to solve these sub-problems while keeping the solutions (parameters) consistent. Several frameworks exist to facilitate the development of efficient distributed algorithms, such as Hadoop [11] and its in-memory variant Spark [34] to execute MapReduce programs, and Graphlab for distributed graph compressor nodes: workers nodes: dataschedulerFigure 3: Simplified parameters server architecture.Machine 0 Machine 1 Machine 2V: U: Each machine contains a server and a worker containing a part of U and V. The inter-machine dependencies (edges) are highlighted and the communication costs for these machines are distributed among three machines, with costs 3 and 3 respectively distributed among frame V."}, {"heading": "2.4 Multiple Objectives of Partitioning", "text": "In the eeisn eeisrmnmnlhUeercnh hacu ufa nde eeisrmtlrVnreeu ni rde eeisrmnlhUe\u00fccnh ufa nde eeisrmtlrVnlrteeu uaf nde eeisrmnlrmtlrVnlrteeu uaf ende eeisrmnlrteeu ni der eeisrmnlrteeu nI \"W\" s \"so asds the eeisrrmnlrteeu ide eeisrrdne eeisrmnlrmnllteeutnlrVnlrteeu.nlrdwnei nI\" W \"s\" so asds the eeisrrf\u00fc ide eeisrrrrf\u00fc ide ide ide eeisrrrrneos \"nllrneos,\" nlllrneos \"eaf eeeeeu Veirrrrnu nu neirrrrrrrrrrrrru nu ni ni\" lrrrrrrrrrrid no."}, {"heading": "2.5 Related Work", "text": "Graph partitioning has aroused great interest in scientific computing [16, 6, 9], the scaling of large-scale calculations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], the search and analysis of social networks [24, 31, 2], and streaming processing [28, 27, 22, 30]. Most previous work, such as METIS [16], dealt with edge sections, but few of them solved the vertex cut problem, which is closely related to this paper, in order to directly minimize network traffic. PaToH [6] and Zoltan [9] used multi-level partitioning algorithms in connection with METIS, while PowerGraph [12] used a greedy algorithm. Only recently [5] investigated the relationship between edge section and vertex cut.In contrast to this work, we propose a new algorithm based on the subjecting to solve the problem."}, {"heading": "3. ALGORITHM", "text": "In this section, we introduce our Parsa algorithm to solve the partitioning problem with multiple targets in (4), (6) and (7). Note that (6) a K-way graph partition problem on the vertex U equals merit with vertex intersection. This problem is NP-Complete [6]. Furthermore, (7) is more complex due to the involvement of V. Instead of solving all these goals together, Parsa splits this problem into two tasks: the partition of data U by solving (4) and (6), and, given the partition of U, parameters V by solving (7). Intuitively, we first assign data workers to balance the CPU load and minimize memory consumption, and then distribute the parameters to servers to minimize communication between machines."}, {"heading": "3.1 Partitioning U over Worker Nodes", "text": "It is a submodular function that is similar to the convex and concave functions in real variables. Although the problem is in (4), there are several algorithms to roughly solve it by taking advantage of submodularity. [29] In our algorithm, we have modified the algorithms to solve (4) and (6). The main difference is that we build the sets Ui step by step, which is important for both partition qualities and computing efficiency at a later stage. As shown in algorithm 1, the algorithms proceed as follows: in each turn, we select the smallest partition Ui and find the best elements to add it. To do this, we first draw a small subset of candidates R and select the best subset using a minimum incremental weight over minT."}, {"heading": "3.2 Partitioning V over Server Nodes", "text": "Next, given the partition of U, we find a mapping of parameters in V to servers. We reformulate (7) as a convex integer programming problem with completely unimodular constraints [15], which are then solved by means of a sequential optimization algorithm that performs a sweep through the variables. We define index variables, the variables vij [0, 1}, j = 1,.. k, to indicate which server maintains a certain parameter vi. you must meet the requirements j = 1 vij = 1 vij = 1. In addition, we denote variables that record whether j \u00b2 n (Ui). Then algorithm 3 partitioning U efficientlyInput: Graph G (U, V, E), # Partitions k = 1 vij = 1 vij = optimal neighbor: Ui."}, {"heading": "4. EFFICIENT IMPLEMENTATION", "text": "The temporal complexity of algorithm 2 is O (k (| U | + | V |), but it could be O (k | U | 6) for algorithm 1, which is unfeasible in practice. We will now discuss how to implement algorithm 1 efficiently. We will first present how to find the optimal T * and Example R. Then we will address parallel implementation with the parameter server and finally describe the initialization of neighboring sets to improve partition quality."}, {"heading": "4.1 Finding T \u2217 efficiently", "text": "The most expensive operation in the inner loop of the algorithm 1 is step 10, which determines which vertices, T \u0445, are to be added to a partition. (D) The most expensive operation in the inner loop of the algorithm 1 is step 10, which determines which vertices, T \u0445. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D (D). (D). (D). (D). (D). (D). (D). (D). (D (D). (D). (D). (D (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D"}, {"heading": "4.2 Division into Subgraphs", "text": "One of the goals of the sampling strategy used in Algorithm 1 is to keep the U partitions balanced, since the indentations of a partition are limited at a given time. Consequently, the additional constraint | T | = 1 introduced in the previous section ensures that only a single vertex is assigned each time that relates to the balance. Consequently, we want to capture as many indentations as possible to widen the search area of the optimal U-shaped partition. Sampling remains attractive as it is a trade-off between calculation efficiency and partition quality."}, {"heading": "4.3 Parallelize with the Parameter Server", "text": "Although Parsa can partition very large graphs in a single process by taking advantage of scanning, parallelization is desirable as both the CPU and I / O times on each machine are reduced. Parsa parallelizes partitioning by processing different subgraphs in parallel (on different nodes) using shared neighbor sets. To implement the algorithm using the parameter server, we need the following three sets of nodes: The planner provides workers with partitioning tasks and monitors their progress. Server nodes maintain the global Shared Neighbor sets. They handle workers \"push and pull requests. Workers nodes partition subgraphs in parallel. Each time a worker reads a subgraph from the (distributed) file system first, he then pulls the newest neighboring sets associated with this subgraph from the servers. Finally, Parsa partitions the subgraphers with these adjacent algorithms and then modifies the 3."}, {"heading": "4.4 Initializing the Neighbor Sets", "text": "Neighboring sets play a similar role to cluster centers on cluster methods, both of which affect the assignment of vertices. Well-initialized neighbor sets potentially improve partition results, but initialization by empty sets that prefer to assign vertices with small degrees first often does little or even worsen the resulting assignment. Parsa uses several initialization strategies to improve the results: Individual initialization. Given a graph that has been split into b subgraphs, we can perform a + b iteration using the results for the first a iteration. In other words, before processing the (j + 1) -tenth subgraph, j \u2264 a + 1, we reset the neighboring set to Si = N (Ui, j), where {Ui, j} ki = 1 are the partitions of the j-tenth subgraph. The old results are dropped, because otherwise a vertex of its old partition will be assigned back to its old partition, since the small neighbor will be assigned to the Si = N, and the small neighbors will be assigned to the Ski = N."}, {"heading": "4.5 Puting it all together", "text": "Algorithm 4 shows Parsa which partitions U into k parts in parallel. Then, we can V with algorithm 2 ifAlgorithm 4 ifAlgorithm 4 Parsa: parallel submodular approximationInput: Graph G, initial neighbor sets {Si} ki = 1, # partitions k, max delay \u03c4, initialization from a, # subgraphs b.Output: partitions U = k = 1 Ui Scheduler: 1: divide G into b subgraphs 2: ask all workers to partition with (a, \u03c4, true) 3: ask all workers to partition with (b, \u03c4, false) servers: 1: start with a part of {Si} ki = 1 2: if you receive a pull request then 3: reply with the requested neighbor set {Si} ki = 1 4: end if you receive a push request that {Snewi} ki = 1 then 6: if initialized."}, {"heading": "5. EXPERIMENTS", "text": "We selected 7 datasets of different types and sizes, summarized in Table 1. The first three are text datasets 1; Livebuch and Orkut are social network2; and the last two are click rate datasets from a large Internet company. The number of corners and edges ranges from 104 to 1010."}, {"heading": "5.1 Setup", "text": "We have implemented Parsa in the parameter server [18]; the source is available at https: / / github.com / mli / parsa. We1 http: / / www.csie.ntu.edu.tw / ~ cjlin / libsvmtools / datasets / 2 http: / / snap.stanford.edu / data / compared Parsa with the popular diagram partition toolboxes Zoltan3 and PaToH4, which can also use bipartition diagrams as inputs. We have also used the well-known chart partition package METIS5 and the greedy algorithm of Powergraph6, even though they only handle normal diagrams. All algorithms are implemented in C / C + +. We report both runtime and partition results. The default measurement of the latter is the maximum individual traffic volume. We counted the improvement against random (proposed) / 100% improvement, with 100% moderate traffic, which means that 50% of the partition pressure is over the university."}, {"heading": "5.2 Comparison to other Methods", "text": "This year, it is so far that it is not so far before it goes to the next round."}, {"heading": "5.4 Scalability", "text": "We are testing the scalability of Parsa on CTRb with 10 billion edges by increasing the number of machines. We operate 4 workers and 4 servers on each machine with endless maximum delay. The results are shown in Figure 1. As you can see, the acceleration is linear with the number of machines and close to ideal. In particular, we achieved 13.7 times the acceleration by increasing the number of machines from 1 to 16. Therefore, the main reason that Parsa scales well is due to the ultimate consistency model.However, this consistency model potentially leads to inconsistency between workers, and each worker does not even wait for the expected results. Therefore, workers make full use of the computing resources and network bandwidth and do not waste time waiting for data synchronization. However, this consistency model potentially leads to inconsistency of neighboring quantities between workers. However, we found that Parsa is robust against this type of inconsistency."}, {"heading": "5.5 Accelerating Distributed Inference", "text": "Finally, we examine how much Parsa can accelerate distributed machine learning applications through better data and parameter placement. We look at \"1-regulated logistic regression, which is one of the most widely used machine learning algorithms for large-area text records. We choose a distributed inference algorithm, DBPG [19], to solve this application. It is based on the block proximal gradient method to improve efficiency: it supports the maximum presence of consistency models, and uses several custom filters, such as key caching, value compression, and an algorithmized kit filter to reduce costs. This algorithm has been implemented in the parameter server and is well optimized."}, {"heading": "6. CONCLUSION", "text": "This paper presented a new parallel vertex-cut graph partition algorithm, Parsa, to solve the problem of data and parameter placement. Our contributions are the following: \u2022 We provide theoretical analyses and approximate guarantees for both decomposition phases of what is generally a hard NP problem. \u2022 We show that the algorithm can be implemented very efficiently by using a duplicate list in O (k | E |) time. \u2022 We provide technologies such as sampling, initialization and parallelization to improve speed and partition quality. \u2022 Experiments show that Parsa works well in practice and beats (or matches) all competing algorithms both in terms of memory requirements and communication costs, while providing a very fast runtime. \u2022 We used Parsa to provide a distributed state-of-the-art solver for \"1-regulated logistic regression, which was implemented in the parameter server, with a relatively fast algorithm of 16 billion times, thanks to a 1.6 billion-observed data acceleration."}, {"heading": "7. REFERENCES", "text": "[1] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola. Scalable inference in latent variable models. In WSDM, 2012. [2] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In WWWW, 2013. [3] R. Andersen, D. Gleich, and V. Mirrokni. Overlapping clusters for distributed computation. In WSDM, 2012. [4] J. Besag. Spatial interaction and the statistical analysis of lattice systems (with discussion). JRSS Series B, 2): 192-236, 1974. [5] F. Bourse, M. Lelarge, and M. Vojnovic."}], "references": [{"title": "Scalable inference in latent variable models", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A.J. Smola"], "venue": "In WSDM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Distributed large-scale natural graph factorization", "author": ["A. Ahmed", "N. Shervashidze", "S. Narayanamurthy", "V. Josifovski", "A.J. Smola"], "venue": "In WWW,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Overlapping clusters for distributed computation", "author": ["R. Andersen", "D. Gleich", "V. Mirrokni"], "venue": "In WSDM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Spatial interaction and the statistical analysis of lattice systems (with discussion)", "author": ["J. Besag"], "venue": "JRSS Series B,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1974}, {"title": "Balanced graph edge partition", "author": ["F. Bourse", "M. Lelarge", "M. Vojnovic"], "venue": "In KDD,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Hypergraph partitioning based decomposition for parallel sparse-matrix vector multiplication", "author": ["U. Catalyurek", "C. Aykanat"], "venue": "IEEE TPDS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Unicorn: A system for searching the social", "author": ["M. Curtiss", "I. Becker", "T. Bosman", "S. Doroshenko", "L. Grijincu", "T. Jackson", "S. Kunnatur", "S. Lassen", "P. Pronin", "S. Sankar"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Parallel hypergraph partitioning for scientific computing", "author": ["K. Devine", "E. Boman", "R. Heaphy", "R. Bisseling", "U. Catalyurek"], "venue": "In Parallel and Distributed Processing Symposium, pages 10\u2013pp. IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "On power-law relationships of the internet topology", "author": ["M. Faloutsos", "P. Faloutsos", "C. Faloutsos"], "venue": "In SIGCOMM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Powergraph: Distributed graph-parallel computation on natural graphs", "author": ["J.E. Gonzalez", "Y. Low", "H. Gu", "D. Bickson", "C. Guestrin"], "venue": "In OSDI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "GraphX: Graph processing in a distributed dataflow framework", "author": ["J.E. Gonzalez", "R.S. Xin", "A. Dave", "D. Crankshaw", "M.J. Franklin", "I. Stoica"], "venue": "In OSDI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "An extension of a theorem of Dantzig\u2019s", "author": ["I. Heller", "C. Tompkins"], "venue": "In Linear Inequalities and Related Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1956}, {"title": "Multilevel k-way partitioning scheme for irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "J. Parallel Distrib. Comput.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F. Kschischang", "B.J. Frey", "H. Loeliger"], "venue": "IEEE ToIT,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "J. Park", "A.J. Smola", "A. Amhed", "V. Josifovski", "J. Long", "E. Shekita", "B.Y. Su"], "venue": "In OSDI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "A.J. Smola", "K. Yu"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "GraphLab: A new parallel framework for machine learning", "author": ["Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J.M. Hellerstein"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Distributed graphlab: A framework for machine learning and data mining in the cloud", "author": ["Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J.M. Hellerstein"], "venue": "In PVLDB,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Restreaming graph partitioning: Simple versatile algorithms for advanced balancing", "author": ["J. Nishimura", "J. Ugander"], "venue": "In KDD,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J. Orlin"], "venue": "Mathematical Programming,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "The little engine (s) that could: scaling online social networks", "author": ["J.M. Pujol", "V. Erramilli", "G. Siganos", "X. Yang", "N. Laoutaris", "P. Chhabra", "P. Rodriguez"], "venue": "ACM SIGCOMM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Trinity: A distributed graph engine on a memory cloud", "author": ["B. Shao", "H. Wang", "Y. Li"], "venue": "In ACM SIGMOD,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "An architecture for parallel topic models", "author": ["A.J. Smola", "S. Narayanamurthy"], "venue": "In VLDB,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Streaming balanced graph partitioning for random graphs", "author": ["I. Stanton"], "venue": "CoRR, abs/1212.1121,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Streaming graph partitioning for large distributed graphs", "author": ["I. Stanton", "G. Kliot"], "venue": "In KDD,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Z. Svitkina", "L. Fleischer"], "venue": "SIAM Computing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Fennel: Streaming graph partitioning for  massive scale graphs", "author": ["C. Tsourakakis", "C. Gkantsidis", "B. Radunovic", "M. Vojnovic"], "venue": "In WSDM, pg", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Balanced label propagation for partitioning massive graphs", "author": ["J. Ugander", "L. Backstrom"], "venue": "In WSDM,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Tao: how Facebook serves the social graph", "author": ["V. Venkataramani", "Z. Amsden", "N. Bronson", "G. Cabrera III", "P. Chakka", "P. Dimov", "H. Ding", "J. Ferris", "A. Giardullo", "J. Hoon"], "venue": "In ACM SIGMOD,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Towards effective partition management for large graphs", "author": ["S. Yang", "X. Yan", "B. Zong", "A. Khan"], "venue": "In ACM SIGMOD,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Fast and interactive analytics over Hadoop data with Spark", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. Mccauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Advanced partitioning techniques for massively distributed computation", "author": ["J. Zhou", "N. Bruno", "W. Lin"], "venue": "In ACM SIGMOD,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "For instance, for very large scale graph factorization [2], one needs to partition a natural graph in a way such that the memory, which is required for storing local state of the partition and caching the adjacent variables, is bounded within the capacity of each machine.", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "GraphLab [20, 12], where vertex-specific updates are carried out while keeping other variables synchronized between machines.", "startOffset": 9, "endOffset": 17}, {"referenceID": 10, "context": "GraphLab [20, 12], where vertex-specific updates are carried out while keeping other variables synchronized between machines.", "startOffset": 9, "endOffset": 17}, {"referenceID": 0, "context": "Likewise, in distributed inference for graphical models with latent variables [1, 26], the distributed state variables must be synchronized efficiently between machines.", "startOffset": 78, "endOffset": 85}, {"referenceID": 24, "context": "Likewise, in distributed inference for graphical models with latent variables [1, 26], the distributed state variables must be synchronized efficiently between machines.", "startOffset": 78, "endOffset": 85}, {"referenceID": 16, "context": "Furthermore, general purposed distributed machine learning framework such as the parameter server [18, 8] face similar issues when it comes to data and parameter layout.", "startOffset": 98, "endOffset": 105}, {"referenceID": 7, "context": "Furthermore, general purposed distributed machine learning framework such as the parameter server [18, 8] face similar issues when it comes to data and parameter layout.", "startOffset": 98, "endOffset": 105}, {"referenceID": 26, "context": "Due to its practical importance, even though the dataset partitioning problems are often NP hard [28], it is still worth seeking practical solutions that outperform random partitioning, which typically leads to poor performance.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "Experiments on text datasets and social networks of various scales show that, on both partition quality and time efficiency, Parsa outperforms state-of-the-art methods, including METIS [16], PaToH [6] and Zoltan [9].", "startOffset": 185, "endOffset": 189}, {"referenceID": 5, "context": "Experiments on text datasets and social networks of various scales show that, on both partition quality and time efficiency, Parsa outperforms state-of-the-art methods, including METIS [16], PaToH [6] and Zoltan [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "Experiments on text datasets and social networks of various scales show that, on both partition quality and time efficiency, Parsa outperforms state-of-the-art methods, including METIS [16], PaToH [6] and Zoltan [9].", "startOffset": 212, "endOffset": 215}, {"referenceID": 12, "context": "For instance, in risk minimization [14], we strive to solve", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "For undirected graphical models [4, 17], the joint distribution of the random variables in logscale can be written as x1 = (.", "startOffset": 32, "endOffset": 39}, {"referenceID": 15, "context": "For undirected graphical models [4, 17], the joint distribution of the random variables in logscale can be written as x1 = (.", "startOffset": 32, "endOffset": 39}, {"referenceID": 2, "context": "Similar problems occur in the context of inference on natural graphs [3, 12, 2], where we have sets of interacting parameters represented by vertices on the graph, and manipulating a vertex affects all of its neighbors computationally.", "startOffset": 69, "endOffset": 79}, {"referenceID": 10, "context": "Similar problems occur in the context of inference on natural graphs [3, 12, 2], where we have sets of interacting parameters represented by vertices on the graph, and manipulating a vertex affects all of its neighbors computationally.", "startOffset": 69, "endOffset": 79}, {"referenceID": 1, "context": "Similar problems occur in the context of inference on natural graphs [3, 12, 2], where we have sets of interacting parameters represented by vertices on the graph, and manipulating a vertex affects all of its neighbors computationally.", "startOffset": 69, "endOffset": 79}, {"referenceID": 32, "context": "There exist several frameworks to simplify the developing of efficient distributed algorithms, such as Hadoop [11] and its in-memory variant Spark [34] to execute MapReduce programs, and Graphlab for distributed graph com-", "startOffset": 147, "endOffset": 151}, {"referenceID": 19, "context": "putation [21].", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "In this paper, we focus on the parameter server framework [18], a high-performance general-purpose distributed machine learning framework.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 71, "endOffset": 81}, {"referenceID": 5, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 71, "endOffset": 81}, {"referenceID": 8, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 71, "endOffset": 81}, {"referenceID": 10, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 120, "endOffset": 143}, {"referenceID": 33, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 120, "endOffset": 143}, {"referenceID": 31, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 120, "endOffset": 143}, {"referenceID": 4, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 120, "endOffset": 143}, {"referenceID": 11, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 120, "endOffset": 143}, {"referenceID": 29, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 120, "endOffset": 143}, {"referenceID": 23, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 161, "endOffset": 172}, {"referenceID": 30, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 161, "endOffset": 172}, {"referenceID": 6, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 161, "endOffset": 172}, {"referenceID": 22, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 209, "endOffset": 220}, {"referenceID": 29, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 209, "endOffset": 220}, {"referenceID": 1, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 209, "endOffset": 220}, {"referenceID": 26, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 247, "endOffset": 263}, {"referenceID": 25, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 247, "endOffset": 263}, {"referenceID": 20, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 247, "endOffset": 263}, {"referenceID": 28, "context": "Graph partitioning has attracted much interest in scientific computing [16, 6, 9], scaling out large-scale computations [12, 35, 33, 5, 13, 31], graph databases [25, 32, 7], search and social network analysis [24, 31, 2], and streaming processing [28, 27, 22, 30].", "startOffset": 247, "endOffset": 263}, {"referenceID": 14, "context": "Most previous work, such as METIS [16], is concerned with edge cuts.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "PaToH [6] and Zoltan [9] used multilevel partitioning algorithms related to METIS, while PowerGraph [12] adopted a greedy algorithm.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "PaToH [6] and Zoltan [9] used multilevel partitioning algorithms related to METIS, while PowerGraph [12] adopted a greedy algorithm.", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "PaToH [6] and Zoltan [9] used multilevel partitioning algorithms related to METIS, while PowerGraph [12] adopted a greedy algorithm.", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "Very recently [5] studied the relation between edge cut and vertex cut.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "This problem is NP-Complete [6].", "startOffset": 28, "endOffset": 31}, {"referenceID": 27, "context": "Although the problem in (4) is NP-Complete, there exist several algorithms to solve it approximately by exploiting the submodularity [29].", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "In our algorithm, we modified [29] to solve (4) and (6).", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "The proof is near-identical as [29].", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "We reformulate (7) as a convex integer programming problem with totally unimodular constraints [15], which is then solved using a sequential optimization algorithm performing a sweep through the variables.", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "modular, since they satisfy the conditions of [15].", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "As a consequence every vertex solution is integral and we may relax the condition vij \u2208 {0, 1} to vij \u2208 [0, 1] to obtain a convex optimization problem.", "startOffset": 104, "endOffset": 110}, {"referenceID": 21, "context": "Submodular minimization problems incur O(n) time [23].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "We implemented Parsa in the parameter server [18]; the source is available at https://github.", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": "We choose a state-of-the-art distributed inference algorithm, DBPG [19], to solve this application.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "This algorithm has been implemented in the parameter server [18], and is well optimized.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "It can use 1,000 machines to train `1regularized logistic regression on 500 terabytes data within a hour [19].", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "We enabled all optimization options described in [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Even though the prior work reported very low communication cost for DBPG [19], we observe that a significant amount of time was spent on data synchronization.", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "First, [19] pre-processed the data to remove tail features (tail vertices in V ) before training.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "Second, the network bandwidth of the university cluster we used is 20 times less than the industrial data-center used by [19].", "startOffset": 121, "endOffset": 125}], "year": 2015, "abstractText": "Distributed computing excels at processing large scale data, but the communication cost for synchronizing the shared parameters may slow down the overall performance. Fortunately, the interactions between parameter and data in many problems are sparse, which admits efficient partition in order to reduce the communication overhead. In this paper, we formulate data placement as a graph partitioning problem. We propose a distributed partitioning algorithm. We give both theoretical guarantees and a highly efficient implementation. We also provide a highly efficient implementation of the algorithm and demonstrate its promising results on both text datasets and social networks. We show that the proposed algorithm leads to 1.6x speedup of a state-of-the-start distributed machine learning system by eliminating 90% of the network communication.", "creator": "LaTeX with hyperref package"}}}