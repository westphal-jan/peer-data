{"id": "1505.04637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2015", "title": "Ensemble of Example-Dependent Cost-Sensitive Decision Trees", "abstract": "Several real-world classification problems are example-dependent cost-sensitive in nature, where the costs due to misclassification vary between examples and not only within classes. However, standard classification methods do not take these costs into account, and assume a constant cost of misclassification errors. In previous works, some methods that take into account the financial costs into the training of different algorithms have been proposed, with the example-dependent cost-sensitive decision tree algorithm being the one that gives the highest savings. In this paper we propose a new framework of ensembles of example-dependent cost-sensitive decision-trees. The framework consists in creating different example-dependent cost-sensitive decision trees on random subsamples of the training set, and then combining them using three different combination approaches. Moreover, we propose two new cost-sensitive combination approaches; cost-sensitive weighted voting and cost-sensitive stacking, the latter being based on the cost-sensitive logistic regression method. Finally, using five different databases, from four real-world applications: credit card fraud detection, churn modeling, credit scoring and direct marketing, we evaluate the proposed method against state-of-the-art example-dependent cost-sensitive techniques, namely, cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision trees. The results show that the proposed algorithms have better results for all databases, in the sense of higher savings.", "histories": [["v1", "Mon, 18 May 2015 13:43:53 GMT  (150kb,D)", "http://arxiv.org/abs/1505.04637v1", "13 pages, 6 figures, Submitted for possible publication"]], "COMMENTS": "13 pages, 6 figures, Submitted for possible publication", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alejandro correa bahnsen", "djamila aouada", "bjorn ottersten"], "accepted": false, "id": "1505.04637"}, "pdf": {"name": "1505.04637.pdf", "metadata": {"source": "CRF", "title": "Ensemble of Example-Dependent Cost-Sensitive Decision Trees", "authors": ["Alejandro Correa Bahnsen", "Djamila Aouada", "Bj\u00f6rn Ottersten"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Cost-sensitive classification, ensemble methods, credit rating, fraud detection, churn modelling, direct marketing. F"}, {"heading": "1 INTRODUCTION", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2 BACKGROUND AND PROBLEM FORMULATION", "text": "This work relates to two research groups in the field of machine learning: (i) example-based cost-sensitive classification and (ii) ensemble learning."}, {"heading": "2.1 Example-dependent cost-sensitive classification", "text": "The classification deals with the problem of predicting class Yi of a set of examples or instances, defining the k-characteristics xi-Rk costs (> costs). The aim is to construct a function f (\u00b7) that makes a prediction ci of the class of each example using its variable xi. Traditionally, machine learning methods are designed to minimize a kind of misclassification rate such as the F1Score [21]; therefore, provided that different misclassification errors have the same costs. As discussed earlier, this is not appropriate in many real-world applications. In fact, two classifiers with the same misclassification rate but different numbers of false positives and false negatives that do not have the same impact on costs, as CFPi = CFNi errors; therefore, there is a need for a measure that takes into account the actual costs of each example, i.In this context, binary classification costs can be represented using a 2x2 cost matrix."}, {"heading": "2.2 Ensemble learning", "text": "Ensemble learning is a common theme in the machine learning community. The main idea behind the ensemble methodology is to combine several individual classifiers, referred to as basic classifiers, to have a classifier that outperforms all of them [12]. There are three main reasons why ensemble methods are better than individual models: statistical, computational and representative [23]. First, from a statistical point of view, there is a risk that if the learning set is too small, an algorithm may find several good models within the search space that relate to the same performance in the S training set. Nevertheless, without a validation set, there is a risk of choosing the wrong model. Second, there is a computational reason; in general, algorithms rely on local search optimization and can get stuck in a local optimum. Then, an ensemble can solve this by focusing different algorithms on different spaces within the training set. The last reason is representative."}, {"heading": "3 ENSEMBLES OF COST-SENSITIVE DECISIONTREES", "text": "In this section, we present our proposed framework for Ensembles of Example Cost-Sensitive Decision Trees (ECSDT), based on the extension of our previous contribution to Example Cost-Sensitive Decision Trees (CSDT) [10]. In particular, we create many different CSDT on random subsamples of the training set and then combine them with different combination methods. In addition, we propose two new cost-sensitive combination approaches, cost-sensitive weighted decisions and cost-sensitive stacking, the latter being an extension of our previously proposed cost-sensitive logistic regression (CSLR) [9]. The rest of the section is organized as follows: First, we introduce the example-sensitive decision tree and cost-sensitive stacking, then we introduce the various random inducers and combination methods, and finally, we define our proposed algorithms to introduce cost-sensitive decision trees (CSDT) in the training class (DT) into the decision-making process."}, {"heading": "3.2 Algorithms", "text": "With the aim of creating an ensemble of sample-dependent decision trees, at each location, different random sub-samples Sj = 1,.., T, of the training set S are first created, and a CSDT algorithm is developed on each basis, although we create the different sub-samples using four different methods: Bagging [14], Pasting [15], Random Forests [15] and Random Patches [11]. In Bagging [14], base classifiers are built on randomly drawn sub-samples of the original data, producing different base classifiers. Similarly, base classifiers are built on random samples without replacing the training set. In random forests, the decision trees are expanded as base learners and combined with a randomization of the input functions used when considering candidates for splitting internal nodes."}, {"heading": "4 THEORETICAL ANALYSIS OF THE COSTSENSITIVE ENSEMBLE", "text": "Although the algorithm proposed above is simple, there is no work that formally examines ensemble performance in terms of anything other than accuracy. (In this section, our goal is to theoretically prove that the combination of individual cost-sensitive classifiers achieves better results in terms of higher savings. (We refer to Sa, where an S0, 1}, as the subset of S, where the examples belong to class a: Sa = {x \u00b2 Cost i | yi = a, i \u0445 1,..), where S = S0, S0, 1}, and Na = | Sa, where the examples belong to class a. Also, we define the average cost of the basic classifiers as: Cost (M) = 1 T, j = 1 Cost (Mj (S))). (20) We first prove that the cost of an ensemble H on the subset Sa is lower than the average cost of the basic classifiers."}, {"heading": "5 EXPERIMENTAL SETUP", "text": "In this section, we present the datasets used to evaluate the proposed Ensembles of Example-Dependent Cost-Sensitive Decision-Trees algorithms. We used five datasets from four different sample-based real-world cost-sensitive problems: credit card fraud detection, churn modeling, credit scoring, and direct marketing. For each dataset, we used a predefined cost matrix that we had previously proposed in various publications. Additionally, we perform a subsample, cost-proportional rejection, and cost-proportional oversample process."}, {"heading": "5.1 Credit card fraud detection", "text": "A credit card fraud detection algorithm consists of identifying these transactions with a high probability of fraud, based on historical fraud patterns. Various detection systems based on machine learning techniques have been successfully used for this problem, see [2]. Credit card fraud detection is by definition a cost-sensitive problem, since the cost of failing to detect a fraud is significantly different from when a false alarm is triggered. We used the example cost matrix for detecting fraud that we have proposed in [8], where the cost of failing to detect a fraud is equal to the amount of the transaction (Amti) and the cost of correctly classifying and false positives is equivalent to the administrative cost of investigating a fraud alert (Ca). The cost table is shown in Table 2. For further discussion see [8], [33]. For this paper, we used a data set that consists of a large European card processor provided between January 2012 and January 2013."}, {"heading": "5.2 Churn modeling", "text": "The churn prediction problem has been extensively investigated by the data mining and machine learning communities, and is usually addressed through the use of classification algorithms to learn the different patterns of both churners and non-churners. For a review, see [34]. However, the current state-of-the-art classification algorithms are not well aligned with commercial objectives in the sense that the models do not take into account the real financial costs and benefits during the training and evaluation phases [3]. We then follow the example-based, cost-sensitive methodology for churn modelling that we have proposed in [35]. When a customer is predicted to be a churner, an offer is made with the aim of preventing the customer from having the churn faulty."}, {"heading": "5.3 Credit scoring", "text": "The goal in the credit assessment is to classify which potential customers are likely to fail to meet a contracted financial obligation based on the customer's past financial experience, and to use this information to decide whether to approve or reject a loan. [36] In constructing credit assessments, it is common practice to use cost-insensitive binary classification algorithms by default, such as logistic regression, neural networks, discriminatory analysis, genetic programming, decision tree, among others [37]. In practice, however, the costs associated with approving a bad customer are significantly different from the costs associated with declining a good customer. In addition, the cost matrix is not constant among customers, as customers have different credit line amounts, conditions, and even interest rates. In this paper, we used the credit scoring exampledependent cost-sensitive cost matrix that we have proposed in [9]. The cost matrix is presented in TABLE 4. Firstly, the cost of each customer is zero."}, {"heading": "5.4 Direct Marketing", "text": "In direct marketing, the goal is to classify those customers who are more likely to have a specific response to a marketing campaign [34]. We used a direct marketing data set of [38]. The data set includes 45,000 customers of a Portuguese bank who were contacted by telephone between March 2008 and October 2010 and received an offer to open a long-term deposit account with attractive interest rates. The data set includes features such as age, occupation, marital status, education, average annual balance and current credit status, as well as the label indicating whether the customer has accepted the offer or not. This problem is exemplarily cost-sensitive, as there are different costs for false positives and false negatives. In direct marketing, in particular, false positives have the cost of contacting the customer, and false negatives have the cost of losing income because they do not contact a customer who would otherwise have opened a long-term deposit. We used the example of direct marketing-dependent cost matrix proposed by us to determine the cost of contacting the customer to determine the cost matrix for the marketing campaign."}, {"heading": "5.5 Database partitioning", "text": "For each database, 3 different data sets are extracted: training, validation, and testing. Each contains 50%, 25%, and 25% of transactions, respectively. As classification algorithms suffer when the label distribution is distorted in the direction of one of the classes [21], a subsample of positive examples is performed to achieve a balanced class distribution. In addition, we perform cost-proportional repulsion and cost-proportional oversampling procedures. Table 6 summarizes the different data sets. It is important to note that the sampling procedures were applied only to the training data set, as the validation and test records must reflect the real distribution."}, {"heading": "6 RESULTS", "text": "This year, it has come to the point that it is only a matter of time before it is ready, until it is ready, until it is ready."}, {"heading": "7 CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we proposed a new framework of ensembles of example-based cost-sensitive decision trees by creating cost-sensitive decision trees using four different methods of random induction and then mixing them with three different combination approaches. The proposed method was tested using five databases, four real-world applications: credit card fraud detection, churn modeling, credit evaluation, and direct marketing. Overall, our framework consists of 12 different algorithms, as the example-based cost-sensitive ensemble can be constructed by equipping the basic classifiers with either dredging, inserting random forest areas, or random patches, and then combining them with majority votes, cost-sensitive weighted votes, or cost-sensitive stacks."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was financed by the Fonds National de la Recherche, Luxembourg."}], "references": [{"title": "The Foundations of Cost-Sensitive Learning", "author": ["C. Elkan"], "venue": "Seventeenth International Joint Conference on Artificial Intelligence, 2001, pp. 973\u2013978.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "The application of data mining techniques in financial fraud detection: A classification framework and an academic review of literature", "author": ["E. Ngai", "Y. Hu", "Y. Wong", "Y. Chen", "X. Sun"], "venue": "Decision Support Systems, vol. 50, no. 3, pp. 559\u2013569, Feb. 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A novel profit maximizing metric for measuring classification performance of customer churn prediction models", "author": ["T. Verbraken", "W. Verbeke", "B. Baesens"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 5, pp. 961\u2013973, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost-sensitive learning by cost-proportionate example weighting", "author": ["B. Zadrozny", "J. Langford", "N. Abe"], "venue": "Third IEEE International Conference on Data Mining. IEEE Comput. Soc, 2003, pp. 435\u2013442.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Development and application of consumer credit scoring models using profitbased classification measures", "author": ["T. Verbraken", "C. Bravo", "R. Weber", "B. Baesens"], "venue": "European Journal of Operational Research, vol. 238, no. 2, pp. 505\u2013513, Oct. 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "C4.5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling", "author": ["C. Drummond", "R. Holte"], "venue": "Workshop on Learning from Imbalanced Datasets II, ICML, Washington, DC, USA, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees", "author": ["O.M. Aodha", "G.J. Brostow"], "venue": "2013 IEEE International Conference on Computer Vision, Washington, DC, USA, Dec. 2013, pp. 193\u2013200.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Cost Sensitive Credit Card Fraud Detection Using Bayes Minimum Risk", "author": ["A. Correa Bahnsen", "A. Stojanovic", "D. Aouada", "B. Ottersten"], "venue": "2013 12th International Conference on Machine Learning and Applications. Miami, USA: IEEE, Dec. 2013, pp. 333\u2013338.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Example- Dependent Cost-Sensitive Logistic Regression for Credit Scoring", "author": ["A. Correa Bahnsen", "D. Aouada", "B. Ottersten"], "venue": "2014 13th International Conference on Machine Learning and Applications. Detroit, USA: IEEE, 2014, pp. 263\u2013269.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Example-Dependent Cost-Sensitive Decision Trees", "author": ["\u2014\u2014"], "venue": "Expert Systems with Applications, vol. inpress, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensembles on random patches", "author": ["G. Louppe", "P. Geurts"], "venue": "ECML PKDD\u201912 Proceedings of the 2012 European conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2012, pp. 346\u2013361.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review, vol. 33, no. 1-2, pp. 1\u201339, Nov. 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Ensemble Methods Foundations and Algorithms", "author": ["Z.-H. Zhou"], "venue": "Boca Raton, FL, US: CRC Press,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 24, no. 2, pp. 123\u2013140, Aug. 1996.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Pasting small votes for classification in large databases and on-line", "author": ["\u2014\u2014"], "venue": "Machine Learning, vol. 103, pp. 85\u2013103, 1999.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Random Forests", "author": ["\u2014\u2014"], "venue": "Machine Learning, vol. 45, pp. 5\u201332, 2001.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Cost-Sensitive Boosting", "author": ["H. Masnadi-shirazi", "N. Vasconcelos"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 2, pp. 294\u2013309, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Bagging with Adaptive Costs", "author": ["W. Street"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 20, no. 5, pp. 577\u2013588, May 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Cost-Sensitive Tree-Stacking : Learning with Variable Prediction Error Costs", "author": ["T.A. Nesbitt"], "venue": "Ph.D. dissertation, University of California, Los Angeles, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey of cost-sensitive decision tree induction algorithms", "author": ["S. Lomax", "S. Vadera"], "venue": "ACM Computing Surveys, vol. 45, no. 2, pp. 1\u201335, Feb. 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Transaction aggregation as a strategy for credit card fraud detection", "author": ["C. Whitrow", "D.J. Hand", "P. Juszczak", "D.J. Weston", "N.M. Adams"], "venue": "Data Mining and Knowledge Discovery, vol. 18, no. 1, pp. 30\u201355, Jul. 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Ensemble methods in machine learning", "author": ["T. Dietterich"], "venue": "Multiple classifier systems. Springer, 2000, pp. 1\u201315.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 12, no. October, pp. 993\u20131001, 1990.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Application of majority voting to pattern recognition: an analysis of its behavior and performance", "author": ["L. Lam", "S.Y. Suen"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part A Systems and Humans, vol. 27, no. 5, pp. 553\u2013568, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Goal-directed classification using linear machine decision trees", "author": ["B. Draper", "C. Brodley", "P. Utgoff"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 16, no. 9, pp. 888\u2013893, 1994.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "An instance-weighting method to induce cost-sensitive trees", "author": ["K. Ting"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 14, no. 3, pp. 659\u2013665, 2002.  13", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Decision trees with minimal costs", "author": ["C.X. Ling", "Q. Yang", "J. Wang", "S. Zhang"], "venue": "Twenty-first international conference on Machine learning - ICML \u201904, no. Icml. New York, New York, USA: ACM Press, 2004, p. 69.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Cost-Sensitive Classification with Genetic Programming", "author": ["J. Li", "X. Li", "X. Yao"], "venue": "2005 IEEE Congress on Evolutionary Computation, vol. 3. IEEE, 2005, pp. 2114\u20132121.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Evolutionary induction of costsensitive decision trees", "author": ["M. Kretowski", "M. Grze\u015b"], "venue": "Foundations of Intelligent Systems. Springer Berlin Heidelberg, 2006, pp. 121\u2013126.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "CSNL: A cost-sensitive non-linear decision tree algorithm", "author": ["S. Vadera"], "venue": "ACM Transactions on Knowledge Discovery from Data, vol. 4, no. 2, pp. 1\u201325, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks, vol. 5, pp. 241\u2013259, 1992.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Improving Credit Card Fraud Detection with Calibrated Probabilities", "author": ["A. Correa Bahnsen", "A. Stojanovic", "D. Aouada", "B. Ottersten"], "venue": "Proceedings of the fourteenth SIAM International Conference on Data Mining, Philadelphia, USA, 2014, pp. 677 \u2013 685.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Application of data mining techniques in customer relationship management: A literature review and classification", "author": ["E. Ngai", "L. Xiu", "D. Chau"], "venue": "Expert Systems with Applications, vol. 36, no. 2, pp. 2592\u20132602, Mar. 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A novel costsensitive framework for customer churn predictive modeling", "author": ["A. Correa Bahnsen", "D. Aouada", "B. Ottersten"], "venue": "Decision Analytics, vol. inpress, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-Driven Classification Based on Multiple Criteria and Multiple Constraint- Level Programming for Intelligent Credit Scoring", "author": ["J. He", "Y. Zhang", "Y. Shi", "S. Member", "G. Huang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 6, pp. 826\u2013838, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey of the issues in consumer credit modelling research", "author": ["L.C. Thomas", "R.M. Oliver", "D.J. Hand"], "venue": "Journal of the Operational Research Society, vol. 56, no. 9, pp. 1006\u20131015, Jul. 2005.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Using data mining for bank direct marketing: An application of the crisp-dm methodology", "author": ["S. Moro", "R. Laureano", "P. Cortez"], "venue": "European Simulation and Modelling Conference, Guimares, Portugal, 2011, pp. 117\u2013121.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical Comparisons of Classifiers over Multiple Data Sets", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 1\u201330, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Typical costsensitive approaches assume a constant cost for each type of error, in the sense that, the cost depends on the class and is the same among examples [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "For example in credit card fraud detection, failing to detect a fraudulent transaction may have an economical impact from a few to thousands of Euros, depending on the particular transaction and card holder [2].", "startOffset": 207, "endOffset": 210}, {"referenceID": 2, "context": "In this context, failing to identify a profitable or unprofitable churner has a significant different economic result [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "Similarly, in direct marketing, wrongly predicting that a customer will not accept an offer when in fact he will, may have a different financial impact, as not all customers generate the same profit [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 4, "context": "Lastly, in credit scoring, accepting loans from bad customers does not have the same economical loss, since customers have different credit lines, therefore, different profit [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 3, "context": "Standard solutions consist in re-weighting the training examples based on their costs, either by cost-proportionate rejection-sampling [4], or cost-proportionate-sampling [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "Standard solutions consist in re-weighting the training examples based on their costs, either by cost-proportionate rejection-sampling [4], or cost-proportionate-sampling [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 5, "context": "However, costproportionate over-sampling increases the training set and it also may result in over-fitting [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "Moreover, the literature on example-dependent costsensitive methods is limited, often because there is a lack of publicly available datasets that fit the problem [7].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "Recently, we have proposed different methods that take into account the different example-dependent costs, in particular: Bayes minimum risk (BMR) [8], cost-sensitive logistic regression [9], and cost-sensitive decision tree (CSDT ) [10].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Recently, we have proposed different methods that take into account the different example-dependent costs, in particular: Bayes minimum risk (BMR) [8], cost-sensitive logistic regression [9], and cost-sensitive decision tree (CSDT ) [10].", "startOffset": 187, "endOffset": 190}, {"referenceID": 9, "context": "Recently, we have proposed different methods that take into account the different example-dependent costs, in particular: Bayes minimum risk (BMR) [8], cost-sensitive logistic regression [9], and cost-sensitive decision tree (CSDT ) [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 10, "context": "However, the CSDT algorithm only creates one tree in order to make a classification, and as noted in [11], individual decision trees typically suffer from high variance.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "The main idea behind the ensemble methodology is to combine several individual base classifiers in order to have a classifier that outperforms each of them [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Nowadays, ensemble methods are one of the most popular and well studied machine learning techniques [13], and it can be noted that since 2009 all the first-place and second-place winners of the KDD-Cup competition1 used ensemble methods.", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 14, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 169, "endOffset": 173}, {"referenceID": 15, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 190, "endOffset": 194}, {"referenceID": 10, "context": "In order to induce the random permutations and therefore create the different base classifiers, several methods have been proposed, in particular: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 214, "endOffset": 218}, {"referenceID": 12, "context": "Finally, after the base classifiers are trained, they are typically combined using either majority voting, weighted voting or stacking [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "In [17], the authors proposed a framework for costsensitive boosting that is expected to minimized the losses by using optimal cost-sensitive decision rules.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18], a bagging algorithm with adaptive costs was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In his doctoral thesis, Nesbitt [19], proposed a method for costsensitive tree-stacking.", "startOffset": 32, "endOffset": 36}, {"referenceID": 19, "context": "Lastly in [20], a survey of application of cost-sensitive learning with decision trees is shown, in particular including other methods that create cost-sensitive ensembles.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "Traditionally, machine learning classification methods are designed to minimize some sort of misclassification measure such as the F1Score [21]; therefore, assuming that different misclassification errors have the same cost.", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "In this context, binary classification costs can be represented using a 2x2 cost matrix [1], that introduces the costs associated with two types of correct classification, true positives (CTPi ), true negatives (CTNi ), and the two types of misclassification errors, false positives (CFPi ), false negatives (CFNi ), as defined in TABLE 1.", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "These are referred to as the reasonableness conditions [1], and are defined as CFPi > CTNi and CFNi > CTPi .", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Using the cost matrix, an example-dependent cost statistic [8], is defined as:", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "In [22], a normalized cost measure was proposed, by dividing the total cost by the theoretical maximum cost, which is the cost of misclassifying every example.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "We proposed a similar approach in [9], where the savings corresponding to using an algorithm are defined as the cost of the algorithm versus the cost of using no algorithm at all.", "startOffset": 34, "endOffset": 37}, {"referenceID": 11, "context": "The main idea behind the ensemble methodology is to combine several individual classifiers, referred to as base classifiers, in order to have a classifier that outperforms everyone of them [12].", "startOffset": 189, "endOffset": 193}, {"referenceID": 22, "context": "There are three main reasons regarding why ensemble methods perform better than single models: statistical, computational and representational [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 23, "context": "Moreover, if we assume that each one of the T base classifiers has a probability \u03c1 of being correct, the probability of an ensemble making the correct decision, denoted by Pc, can be calculated using the binomial distribution [24]", "startOffset": 226, "endOffset": 230}, {"referenceID": 24, "context": "Furthermore, as shown in [25], if T \u2265 3 then:", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "The framework is based on expanding our previous contribution on example-dependent cost-sensitive decision trees (CSDT ) [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "The latter being an extension of our previously proposed cost-sensitive logistic regression (CSLR) [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 19, "context": "Introducing the cost into the training of a decision tree has been a widely study way of making classifiers costsensitive [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 25, "context": "However, in most cases, approaches that have been proposed only deal with the problem when the cost depends on the class and not on the example [26]\u2013[31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "However, in most cases, approaches that have been proposed only deal with the problem when the cost depends on the class and not on the example [26]\u2013[31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "In [10], we proposed an example-dependent cost-sensitive decision trees (CSDT ) algorithm, that takes into account the example-dependent costs during the training and pruning of a tree.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "In particular we create the different subsets using four different methods: bagging [14], pasting [15], random forests [16] and random patches [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "In bagging [14], base classifiers are built on randomly drawn bootstrap subsets of the original data, hence producing different base classifiers.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "Similarly, in pasting [15], the base classifiers are built on random samples without replacement from the training set.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "In random forests [16], using decision trees as the base learner, bagging is extended and combined with a randomization of the input features that are used when considering candidates to split internal nodes.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "In the random patches algorithm [11], base classifiers are created by randomly drawn bootstrap subsets of both examples and features.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "First, in the traditional approach, a similar comparison of the votes of the base classifiers is made, but giving a weight \u03b1j to each classifier Mj during the voting phase [13]", "startOffset": 172, "endOffset": 176}, {"referenceID": 31, "context": "The staking method consists in combining the different base classifiers by learning a second level algorithm on top of them [32].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Even though there is no restriction on which algorithm can be used as a second level learner, it is common to use a linear model [13], such as fs(S,M, \u03b2) = g \uf8eb\uf8ed T \u2211", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "Moreover, following the logic used in [19], we propose learning the set of parameters \u03b2 using our proposed costsensitive logistic regression (CSLR) [9].", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "Moreover, following the logic used in [19], we propose learning the set of parameters \u03b2 using our proposed costsensitive logistic regression (CSLR) [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "However, as discussed in [9], this cost function is not convex for all possible cost matrices, therefore, we use genetic algorithms to minimize it.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Different detection systems that are based on machine learning techniques have been successfully used for this problem, for a review see [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "We used the fraud detection example-dependent cost matrix we proposed in [8], in which the cost of failing to detect a fraud is equal to the amount of the transaction (Amti), and the costs of correct classification and false positives is equal to the administrative cost of investigating a fraud alert (Ca).", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "For a further discussion see [8], [33].", "startOffset": 29, "endOffset": 32}, {"referenceID": 32, "context": "For a further discussion see [8], [33].", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "TABLE 2 Credit card fraud detection cost matrix [8]", "startOffset": 48, "endOffset": 51}, {"referenceID": 34, "context": "TABLE 3 Churn modeling cost matrix [35]", "startOffset": 35, "endOffset": 39}, {"referenceID": 33, "context": "For a review see [34].", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Nevertheless, current state-of-the-art classification algorithms are not well aligned with commercial goals, in the sense that, the models miss to include the real financial costs and benefits during the training and evaluation phases [3].", "startOffset": 235, "endOffset": 238}, {"referenceID": 34, "context": "We then follow the example-dependent cost-sensitive methodology for churn modeling we proposed in [35].", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "The objective in credit scoring is to classify which potential customers are likely to default a contracted financial obligation based on the customer\u2019s past financial experience, and with that information decide whether to approve or decline a loan [36].", "startOffset": 250, "endOffset": 254}, {"referenceID": 36, "context": "When constructing credit scores, it is a common practice to use standard cost-insensitive binary classification algorithms such as logistic regression, neural networks, discriminant analysis, genetic programing, decision tree, among others [37].", "startOffset": 240, "endOffset": 244}, {"referenceID": 8, "context": "In this paper, we used the credit scoring exampledependent cost-sensitive cost matrix we proposed in [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "First, the costs of a TABLE 4 Credit scoring cost matrix [9]", "startOffset": 57, "endOffset": 60}, {"referenceID": 33, "context": "In direct marketing the objective is to classify those customers who are more likely to have a certain response to a marketing campaign [34].", "startOffset": 136, "endOffset": 140}, {"referenceID": 37, "context": "We used a direct marketing dataset from [38].", "startOffset": 40, "endOffset": 44}, {"referenceID": 32, "context": "We used the direct marketing example-dependent cost matrix we proposed in [33].", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "TABLE 5 Direct marketing cost matrix [33]", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "Afterwards, because classification algorithms suffer when the label distribution is skewed towards one of the classes [21], an under-sampling of the positive examples is made, in order to have a balanced class distribution.", "startOffset": 118, "endOffset": 122}, {"referenceID": 38, "context": "Using the implementation of Scikitlearn [39], each algorithm is trained using the different training sets: training (t), under-sampling (u), cost-proportionate rejection-sampling (r) [4] and cost-proportionate oversampling (o) [1].", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Using the implementation of Scikitlearn [39], each algorithm is trained using the different training sets: training (t), under-sampling (u), cost-proportionate rejection-sampling (r) [4] and cost-proportionate oversampling (o) [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 0, "context": "Using the implementation of Scikitlearn [39], each algorithm is trained using the different training sets: training (t), under-sampling (u), cost-proportionate rejection-sampling (r) [4] and cost-proportionate oversampling (o) [1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 32, "context": "Afterwards, we evaluate the results of the algorithms using BMR [33].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Then, the cost-sensitive logistic regression (CSLR) [9] and cost-sensitive decision tree (CSDT ) [10] were also evaluated.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "Then, the cost-sensitive logistic regression (CSLR) [9] and cost-sensitive decision tree (CSDT ) [10] were also evaluated.", "startOffset": 97, "endOffset": 101}, {"referenceID": 39, "context": "Subsequently, in order to statistically sort the classifiers we computed the Friedman ranking (F-Rank) statistic [40].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "It is observed that the model with the highest savings is not the same as the one with the highest F1Score in all of the databases, corroborating the conclusions from [8], as selecting a method by a traditional statistic does not give the same result as selecting it using a business oriented measure such as financial savings.", "startOffset": 167, "endOffset": 170}], "year": 2015, "abstractText": "Several real-world classification problems are example-dependent cost-sensitive in nature, where the costs due to misclassification vary between examples and not only within classes. However, standard classification methods do not take these costs into account, and assume a constant cost of misclassification errors. In previous works, some methods that take into account the financial costs into the training of different algorithms have been proposed, with the example-dependent cost-sensitive decision tree algorithm being the one that gives the highest savings. In this paper we propose a new framework of ensembles of example-dependent cost-sensitive decision-trees. The framework consists in creating different example-dependent cost-sensitive decision trees on random subsamples of the training set, and then combining them using three different combination approaches. Moreover, we propose two new cost-sensitive combination approaches; cost-sensitive weighted voting and cost-sensitive stacking, the latter being based on the cost-sensitive logistic regression method. Finally, using five different databases, from four real-world applications: credit card fraud detection, churn modeling, credit scoring and direct marketing, we evaluate the proposed method against state-of-the-art example-dependent cost-sensitive techniques, namely, cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision trees. The results show that the proposed algorithms have better results for all databases, in the sense of higher savings.", "creator": "LaTeX with hyperref package"}}}