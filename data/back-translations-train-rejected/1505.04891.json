{"id": "1505.04891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph", "abstract": "Word embedding, which refers to low-dimensional dense vector representations of natural words, has demonstrated its power in many natural language processing tasks. However, it may suffer from the inaccurate and incomplete information contained in the free text corpus as training data. To tackle this challenge, there have been quite a few works that leverage knowledge graphs as an additional information source to improve the quality of word embedding. Although these works have achieved certain success, they have neglected some important facts about knowledge graphs: (i) many relationships in knowledge graphs are \\emph{many-to-one}, \\emph{one-to-many} or even \\emph{many-to-many}, rather than simply \\emph{one-to-one}; (ii) most head entities and tail entities in knowledge graphs come from very different semantic spaces. To address these issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet models the relationships between head and tail entities after transforming them with different low-rank projection matrices. The low-rank projection can allow non \\emph{one-to-one} relationships between entities, while different projection matrices for head and tail entities allow them to originate in different semantic spaces. The experimental results demonstrate that ProjectNet yields more accurate word embedding than previous works, thus leads to clear improvements in various natural language processing tasks.", "histories": [["v1", "Tue, 19 May 2015 07:08:10 GMT  (23kb)", "https://arxiv.org/abs/1505.04891v1", null], ["v2", "Sun, 14 Jun 2015 08:21:24 GMT  (23kb)", "http://arxiv.org/abs/1505.04891v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fei tian", "bin gao", "enhong chen", "tie-yan liu"], "accepted": false, "id": "1505.04891"}, "pdf": {"name": "1505.04891.pdf", "metadata": {"source": "CRF", "title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of Knowledge Graph", "authors": ["Fei Tian"], "emails": ["tianfei@mail.ustc.edu.cn", "bingao@microsoft.com", "cheneh@ustc.edu.cn", "tyliu@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.04 891v 2 [cs.C L] 14 Jun 2015Better word embedding by asymmetric low-rank projection of knowledge GraphFei Tian University of Science and Technology of Chinatianfei @ mail.ustc.edu.cnBin Gao Microsoft Researchbingao @ microsoft.com Enhong Chen University of Science and Technology of Chinacheneh @ ustc.edu.cnTie-Yan Liu Microsoft Researchtyliu @ microsoft.com"}, {"heading": "Abstract", "text": "Word embedding, which refers to low-dimensional, dense vector representations of natural words, has proven its power in many tasks of natural language processing. However, it may suffer from the inaccurate and incomplete information contained in the free-text corpus as training data. To meet this challenge, there is a slew of papers that use knowledge diagrams as an additional source of information to improve the quality of text embedding. Although these papers have achieved some success, they have neglected some important facts about knowledge diagrams: (i) Many relationships in knowledge diagrams are much-to-one, one-to-many, or even much-to-many, rather than just one-to-one; (ii) most head and tail units in knowledge diagrams come from very different semantic spaces. To address these problems, in this paper we propose a new algorithm called ProjectNet that calculates the relationships between head and tail units according to the translation of different matrices."}, {"heading": "1 Introduction", "text": "In recent years, research into word embedding (or distributed word representation) may not have yielded semantically accurate results, which means promising advances in many natural language processing tasks [1; 7; 11; 15; 18]. Unlike the traditional embedding of words, word embedding vectors, which in many cases have proven their power, it is beginning to be recognized that embedding techniques on a large-format free text corpus, which suffers from incomplete and inaccurate information contained in the free text files. On the one hand, due to the restrictive topics and coverage of a text corpus, some words may not have sufficient contexts and therefore may not have reliable word embedding. On the other hand, even if a word has sufficient contextual data, the free texts may not be precise, providing a semantically precise result."}, {"heading": "2 Related Work", "text": "In fact, research in this field has grown very rapidly in recent years [11; 15; 18; 6]. Among them, word2vec [15; 16] is getting quite a lot of attention from the community due to its simplicity and effectiveness. An interesting result of word2vec is that the word it produces can reflect human knowledge through some simple arithmetic operations, e.g. v (Japan) \u2212 v (France) \u2212 v, as already mentioned, word embedding models such as word2vec that reflect human knowledge."}, {"heading": "3 The ProjectNet Algorithm", "text": "In this section, we present our proposed ProjectNet model in detail. In general, following [23; 21], with a training text corpus D and a set of K triples in form (head unit, relation, tail unit) extracted from a knowledge diagram, our model collectively minimizes a linear combination of loss positions on text and knowledge: L = \u03b1LD + (1 \u2212 \u03b1) LK, (1), where \u03b1 [0, 1] is used to weigh the two loss concepts against each other. LD and LK have the same parameters, i.e. the embedding vectors for words and their corresponding entities are the same. In the following subsections, we will present the text model for specifying LD and the knowledge model for specifying LK."}, {"heading": "3.1 Text Model", "text": "Similar to [23; 21], we use the Skip-Gram model [16] as a text model. In Skip-Gram, the probability of observing the target word wO based on its context word wI is modeled as P (wO | wI) = exp (w \ufffd O \u00b7 wI) \u2211 w \ufffd V exp (w \ufffd \u00b7 wI), where w \ufffd Rd and w \ufffd Rd denote the input and output embed vectors for word w, V is the dictionary and d is the dimension of embedding. In view of the training corpus D, consisting of the symbol words {p1, \u00b7 \u00b7 \u00b7, pk, \u00b7 \u00b7 \u00b7, p | D |}, the loss LD is specified by: LD = | D | \u2211 k = 1 \u0445 j {\u2212 M, \u00b7, M}, j 6 = 0P (pk | pk + j), where 2M is the size of the reduction strategy we apply to minimizing efficiency."}, {"heading": "3.2 Knowledge Model", "text": "The knowledge model in ProjectNet is based on an asymmetric low-rank projection that projects the original entity embedding vectors into a new semantic space. It is designed to be asymmetric to address the heterogeneity between head and tail units, and is designed to be low to deal with non-one-to-one relationships in knowledge diagrams."}, {"heading": "Asymmetric Projection", "text": "As already mentioned, head and tail are usually very different in knowledge diagrams, both from a semantic and a statistical perspective. Therefore, we argue that it is unreasonable to apply the same projection to these two types of entities (as TransH [22] does). Instead, it would be better to adopt different projection matrices called Lr, Rd and Rr, Rd and Rd. (3) Starting from the transformed embeddings, we therefore define a scoring function fd to reflect the confidence level that the triple (h, r, t) vectors for h and t are transformed into h and t as follows: fd (h, r, t) = Lrh, t, r and prt."}, {"heading": "Low-Rank Projection", "text": "As mentioned in the introduction, many relationships in the knowledge diagrams are not one-to-one. In this case, in order to achieve reasonable results in minimizing LK defined above, it is necessary to limit the projection matrices Lr and Rr to a low rank, which is described in the following sentence. Sentence 3.1 As soon as linear projections are imposed on head and tail units, the necessary condition for overcoming the non-one-to-one imaging problem is that the projection matrices"}, {"heading": "Lr and Rr should not be full-ranked.", "text": "Proof Consider the following least quadratic problem w.r.t. The optimization variable h: min | | Lrh \u2212 c | | 2 2, (6), where Lrh = h \u2032 and we consider c = t \u2032 \u2212 r as a constant vector. It is easy to achieve that the optimal solution h \u0445 fulfills the following linear system: LTr Lrh \u0445 = LTr c. (7) In order to avoid the not one-to-one mapping problem, the above equation must have several solutions. Then, it is necessary that LTr is a low-ranking matrix. Furthermore, as rank (LTr Lr) = rank (Lr), we also use the linear projection matrix Lr not to be complete. The same conclusion applies to the projection matrix Rr for the entityx. In view of the above preproduction, we use the following tricks to ensure that Lr and Rr are the real matrices (whose lengths we are all mR and mR)."}, {"heading": "3.3 Discussions", "text": "In this context, it is also possible that the requests made by the EU Commission to the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU Commission, the EU Commission, the Commission, the Commission, the Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU, the EU, the EU Commission, the EU Commission, the EU Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the EU"}, {"heading": "4 Experiments", "text": "In this section, we will conduct a series of experiments to test the effectiveness of the ProjectNet model."}, {"heading": "4.1 Experiments Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Training Data", "text": "For the free-text corpus, we used a public snapshot of English Wikipedia called enwik9.2 The corpus contains about 1202http: / / mattmahoney.net / dc / enwik9.zipmillion word marks. We removed digital words and words with a frequency of less than 5. Then we used a knowledge graph FB13 [19] to impose relationships on those units that fall under enwik9. Since FB13 contains many units whose names have multiple words, we merged these words into enwik9 phrases and looked at both single words and embedded units in the dictionary. Finally, the dictionary size is about 230k."}, {"heading": "Baseline Methods", "text": "As a basis, we consider the following algorithms (we used the codes approved by the authors of this paper for implementation): 1. Skip-Gram (SG): the original Skip-Gram model in word2vec, corresponding to \u03b1 = 0 in (1).2. RNet: the common embedding model in [23] and [21], which includes the objective min | h + r \u2212 t | | 22 in the knowledge model. 33. Skip-Gram + TransH (SG + TransH): the combination of Skip-Gram (for the text model) and TransH (for the knowledge model)."}, {"heading": "Parameter Setting", "text": "For the knowledge model in ProjectNet, we initialized the projection matrices Lr and Rr as diagonal matrices with randomly assigned 0, 1 elements (with mL or mR non-elements). For mL and mR, we varied their values according to the quantity {10, 20, \u00b7 \u00b7, 80, 90, 95, 100}. For all common embedding models, we varied the trade-off parameter \u03b1 in (1) according to the quantity {0.01, 0.05, 0.1, 0.2, 0.5}. The margin value is set to q = 1.We used two tasks to evaluate our algorithm and the baseline models, one being the analog reasoning task and the other word showing the similarity of the two tasks in the following sections."}, {"heading": "4.2 Analogical Reasoning Task", "text": "The analogous world of thought is able to hide itself in order to change the world, and to be able to change the world in order to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy, to destroy,"}, {"heading": "Sensitivity to different ranks", "text": "The best performance of ProjectNet was achieved with \u03b1 = 0.2, mL = 50 and mR = 90. To show the influence of the ranks of the projection matrices, we have shown in Figure 1 two curves reflecting the performance of ProjectNet w.r.t., which reflect different rank values mL and mR: One curve corresponds to the change of mR while specifying mL = 50 and the other corresponds to the change of mL while specifying mR = 90. The following observations result from the figure. (i) The performance becomes bad if the rank is too low. This is because in this case the expressive power of the model becomes bad due to a small number of free parameters in the projection matrices. (ii) For the projection matrix for head units, the mean values of mL correspond to the better performances (the dashdot line), 4We have not used the analog argument sets in [16], because these values are specific to the projection line, while the L is higher in almost all senses."}, {"heading": "4.3 Word Similarity Task", "text": "In our experiments, we used three word similarity tasks, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14]. In these data sets, there are 353, 2003 and 2034 pairs of words, respectively. From the word similarity values assigned by human labellers, we obtain the similarity values (e.g. cosmic similarity) for each word pair, on the basis of which a ranking is derived based on the word pairs. Subsequently, the generated ranking is compared with the ranking values generated by the basic truth similarity values assigned by human labels. To evaluate the consistency between two rankings, we used Spearman's ranking correlation, which results from the ranking correlation (expressed as 1, 1]."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, we have proposed a novel word embedding algorithm called ProjetNet that uses knowledge diagrams to improve the quality of word embedding. In ProjetNet, we use different asymmetrical low-level projections to successfully maintain head and tail entities in an entity relationship triple, resulting in both non-one-to-one mapping and heterogeneous head-tail entity characteristics of the knowledge diagram. Experimental results show that ProjetNet significantly outperforms earlier embedding models. For future work, we plan to apply the proposed approach to perform knowledge-mining tasks, such as triplet classification and link prediction [22]. In addition, we plan to use the word embedding vectors generated by ProjectNet in some real-world applications such as document classification and web search ranking."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Knowledgepowered deep learning for word embedding", "author": ["Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia- Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A. Botha", "Phil Blunsom"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Learning effective word embedding using morphological word similarity", "author": ["Qing Cui", "Bin Gao", "Jiang Bian", "Siyu Qiu", "Tie-Yan Liu"], "venue": "CoRR, abs/1407.1687,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A latent factor model for highly multi-relational data", "author": ["Rodolphe Jenatton", "Nicolas L Roux", "Antoine Bordes", "Guillaume R Obozinski"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "C Manning"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Distributed representations of  words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Knowledge graph and text jointly embedding", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "To tackle this problem, recently some researchers have proposed to leverage knowledge graphs, such as WordNet [17] and Freebase [3], as additional data sources to improve word embedding [2; 21; 23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 2, "context": "To tackle this problem, recently some researchers have proposed to leverage knowledge graphs, such as WordNet [17] and Freebase [3], as additional data sources to improve word embedding [2; 21; 23].", "startOffset": 128, "endOffset": 131}, {"referenceID": 18, "context": "Take a widely used benchmark dataset FB13 [19], which is a subset of Freebase, as an instance.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "For example, in [22], it is proposed to project the embedding vectors of both entities onto a relation-dependent hyperplane before computing the loss function LK.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "Furthermore, the projection matrix used in [22] has a fixed rank for all types of relationships, which could not express various degrees of non one-to-one mappings.", "startOffset": 43, "endOffset": 47}, {"referenceID": 4, "context": "In [5], different transformations are adopted to head and tail entities respectively, however, no consideration is taken to address the issue of non one-to-one mappings.", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "Actually, it can be proven that the TransH model in [22] is our special case in the sense that it also adopts a projection matrix of low (and fixed) rank.", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "In [24], the authors used semantic relational knowledge between words as a constraint in learning word embedding vectors.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [23], the authors leveraged knowledge graphs, the most widely used structured knowledge, to help improve word representations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21], the authors proposed a very similar method to [23], but with a different objective of improving knowledge graph understanding with the help of text corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[21], the authors proposed a very similar method to [23], but with a different objective of improving knowledge graph understanding with the help of text corpus.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "Actually, both the models in [23] and [21] are inspired by the TransE model [4], which is a state-of-the-art work in the literature of computing distributed representations for knowledge graphs [5; 12; 19].", "startOffset": 29, "endOffset": 33}, {"referenceID": 20, "context": "Actually, both the models in [23] and [21] are inspired by the TransE model [4], which is a state-of-the-art work in the literature of computing distributed representations for knowledge graphs [5; 12; 19].", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "Actually, both the models in [23] and [21] are inspired by the TransE model [4], which is a state-of-the-art work in the literature of computing distributed representations for knowledge graphs [5; 12; 19].", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "To tackle the problem, in [22], the authors proposed a simple projection method named TransH.", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "where \u03b1 \u2208 [0, 1] is used to trade off the two loss terms.", "startOffset": 10, "endOffset": 16}, {"referenceID": 15, "context": "Similar to [23; 21], we leverage the Skip-Gram model [16] as the text model.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "P (wO|wI), we adopt the negative sampling strategy [16] to boost the computation efficiency.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "Therefore, we argue that it is unreasonable to adopt the same projection to these two kinds of entities (as TransH [22] does).", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "RNet refers to the knowledge models proposed in [23] and [21].", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "RNet refers to the knowledge models proposed in [23] and [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "In [23], a large margin ranking loss is adopted for the minimization of fd(h, r, t), whereas in [21], an approximate softmax loss is used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [23], a large margin ranking loss is adopted for the minimization of fd(h, r, t), whereas in [21], an approximate softmax loss is used.", "startOffset": 96, "endOffset": 100}, {"referenceID": 21, "context": "TransH [22] is proposed to overcome the non oneto-one mapping problem.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "LrLr = Lr) and wr is a unit length vector, it holds that rank(Lr) = trace(Lr) = d \u2212 1 [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "SE [5] adopts the following scoring function:", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "[13] TransR treats relationships and entities as different objects and thus separates their embeddings into different spaces, fd(h, r, t) = ||Mrh+ r\u2212Mrt|| 2 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Then we leveraged a knowledge graph FB13 [19] to impose relationships onto those entities covered by enwik9.", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "RNet: the joint embedding model in [23] and [21], which adopts the objective min ||h + r \u2212 t||2 in the knowledge model.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "RNet: the joint embedding model in [23] and [21], which adopts the objective min ||h + r \u2212 t||2 in the knowledge model.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "The analogical reasoning task is a word relationship inference task proposed in [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "We did not use the analogical reasoning dataset given in [16] because this dataset is too special in the sense that almost all the relationships in it are one-to-one mappings.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "We used three wordsimilarity tasks in our experiments, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14].", "startOffset": 90, "endOffset": 93}, {"referenceID": 10, "context": "We used three wordsimilarity tasks in our experiments, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "We used three wordsimilarity tasks in our experiments, namely Word Similarity 353 (WS353) [9], SCWS [11] and Rare Word (RW) [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "For the future work, we plan to apply the proposed approach to fulfill knowledge mining tasks, such as triplet classification and link prediction [22].", "startOffset": 146, "endOffset": 150}], "year": 2015, "abstractText": "Word embedding, which refers to low-dimensional dense vector representations of natural words, has demonstrated its power in many natural language processing tasks. However, it may suffer from the inaccurate and incomplete information contained in the free text corpus as training data. To tackle this challenge, there have been quite a few works that leverage knowledge graphs as an additional information source to improve the quality of word embedding. Although these works have achieved certain success, they have neglected some important facts about knowledge graphs: (i) many relationships in knowledge graphs are many-to-one, oneto-many or even many-to-many, rather than simply one-to-one; (ii) most head entities and tail entities in knowledge graphs come from very different semantic spaces. To address these issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet models the relationships between head and tail entities after transforming them with different low-rank projection matrices. The low-rank projection can allow non one-to-one relationships between entities, while different projection matrices for head and tail entities allow them to originate in different semantic spaces. The experimental results demonstrate that ProjectNet yields more accurate word embedding than previous works, thus leads to clear improvements in various natural language processing tasks.", "creator": "LaTeX with hyperref package"}}}