{"id": "1505.05008", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Boosting Named Entity Recognition with Neural Character Embeddings", "abstract": "Most state-of-the-art named entity recognition (NER) systems rely on the use of handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking. In this work we propose a language-independent NER system that uses automatically learned features only. Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classification. We perform an extensive number of experiments using two annotated corpora in two different languages: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002, which contains texts in Spanish. Our experimental results shade light on the contribution of neural character embeddings for NER. Moreover, we demonstrate that the same neural network which has been successfully applied for POS tagging can also achieve state-of-the-art results for language-independet NER, using the same hyper-parameters, and without any handcrafted features. For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes), and by 7.2 points in the F1 for the selective scenario (five NE classes).", "histories": [["v1", "Tue, 19 May 2015 14:21:37 GMT  (126kb,D)", "http://arxiv.org/abs/1505.05008v1", "10 pages"], ["v2", "Mon, 25 May 2015 11:35:32 GMT  (125kb,D)", "http://arxiv.org/abs/1505.05008v2", "9 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cicero nogueira dos santos", "victor guimar\\~aes"], "accepted": false, "id": "1505.05008"}, "pdf": {"name": "1505.05008.pdf", "metadata": {"source": "CRF", "title": "Boosting Named Entity Recognition with Neural Character Embeddings", "authors": ["C\u0131\u0301cero Nogueira dos Santos", "Victor Guimar\u00e3es"], "emails": ["cicerons@br.ibm.com,", "victorguimaraes@id.uff.br"], "sections": [{"heading": "1. Introduction", "text": "Named Unit Recognition is a natural language processing (NLP) task, which consists of finding names in a text and classifying them under several predefined categories of interest such as person, organization, place, and time. Although machine learning-based systems have been the dominant approach to achieving state-of-the-art results for NER, most of these NER systems rely on the use of expensive handcrafted functions and the results of other NLP tasks [Tjong Kim Sang 2002, Tjong Kim Sang and De Meulder 2003, Doddington et al. 2004, Finkel et al. 2005, Milidiu \u0301 et al. 2007]. On the other hand, some recent work on NLP tasks uses deep learning strategies that minimize the need for these costly functions [Chen et al. 2010, Collobert et al. 2011, Tang et al. 2014]. As far as we know, there is still no work on deep learning approaches for NER, the character-level emttingsbettings.In this paper we approach independently."}, {"heading": "2. CharWNN", "text": "CharWNN expands Collobert et al.'s (2011) neural network architecture for sequential classification by adding a revolutionary level to extract representations at the character level [dos Santos and Zadrozny 2014]. When a sentence is given, the network gives a score for each word (tag). As shown in Figure 1, the network uses a fixed-size window centralized in the target word to enter a word. It is entered through a sequence of layers in which features are extracted with increasing complexity, and the output of the entire sentence is then processed using the Viterbi algorithm [Viterbi 1967] to make structured predictions. For a detailed description of the neural network CharWNN, refer the reader to [dos Santos and Zadrozny 2014]."}, {"heading": "2.1. Character- and Word-level Representations", "text": "As illustrated in Figure 1, the first layer of the network transforms words into real-value feature vectors (embedding), which are used to gather morphological, syntactic and semantic information about the words. We use a fixed-size vocabulary V would, and we assume that words are composed of a fixed-size vocabulary V chr. In a sentence consisting of N-words {w1, w2,..., wN}, each word wn is converted to a vector un = [rwrd; rwch], which consists of two subvectors: the word-level embedding rwrd-Rdwrd and the character-level embedding rwch-Rclu of wn. While word-level embedding capture syntactical and semantic information, letter-level embedding capture multiplicit information. Word-level embedding is embedded by word."}, {"heading": "2.2. Scoring and Structured Inference", "text": "We follow Collobert et al.'s [Collobert et al. 2011] Window approach to evaluate all tags T for each word in a sentence. | This approach follows the assumption that in the sequential classification the tag of a word mainly depends on its adjacent words. In view of a sentence with N words {w1, w2,..., wN} that have been converted into a common word and character plane, we first use a vector z, which results from the concatenation of a sequence of kwrd embeddings centered in the n-th word, z = (kwrd \u2212 1) / 2,..., un + (kwrd \u2212 1) / 2) T. We use a special sentence z, which results from the concatenation of a sequence of kwrd embeddings."}, {"heading": "2.3. Network Training", "text": "We train CharWNN by minimizing a negative probability over the training set D. In the same way as in [Collobert et al. 2011], we interpret the sentence value (2) as a conditional probability over a path. To this end, we exponentialize the value (2) and normalize it in relation to all possible paths. If we take the log, we arrive at the following conditional log probability: log p ([t] N1 | [w] N1, \u03b8) = S ([w] N1, [t] N 1, \u03b8) \u2212 log. We use stochastic gradient lineage (SGD) to efficiently calculate the negative log probability in relation to Equation 3 by means of dynamic programming [Collobert 2011]."}, {"heading": "3. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Unsupervised Learning of Word Embeddings", "text": "The word embeddings used in our experiments are initialized by unsupervised pre-training. In our experiments at the Portuguese level, we use the word embeddings previously trained by [dos Santos and Zadrozny 2014]. They have used a corpus that consists of the Portuguese Wikipedia, the CETENFolha2 corpus and the CETEMPublico3 corpus. In our experiments at the Spanish level, we use the Spanish Wikipedia. We edit the Spanish Wikipedia corpus with the same steps used by [dos Santos and Zadrozny 2014]: (1) remove paragraphs that are not in Spanish; (2) replace non-Roman characters with a special character; (3) tokenize the text using a tokenizer that we have implemented; (4) remove sentences that are not at the Spanish level; (2) replace non-Roman characters with a specific character; (2) we all have (1) letters."}, {"heading": "3.2. Corpora", "text": "We use the corpus from the first HAREM evaluation [Santos and Cardoso 2007] in our experiments in Portuguese NER. This corpus is commented on with ten named entity categories: Person (PESSOA), Organization (ORGANIZACAO), Location (LOCAL), Value (VALOR), Date (TEMPO), Abstraction (ABSTRACCAO), Title (OBRA), Event (ACONTECIMENTO), Thing (COISA) and Miscellaneous (OUTRO). The HAREM corpus is already divided into two subgroups: First HAREM and MiniHAREM. Each subset corresponds to a different Portuguese NER competition. In our experiments we call HAREM I the setup in which we use the First HAREM Corpus NO 2002 as a training set and the MiniHAREM Corpus as a test set."}, {"heading": "3.3. Model Setup", "text": "In most of our experiments, we use the same hyperparameters used by dos Santos and Zadrozny (2014) for the marking of parts of the language, except for the learning rate for SPA CoNLL-2002, which we set to 0.005 to avoid divergences. Hyperparameter values are shown in Table 2. We use the development kits to determine the number of training periods, the six for HAREM and sixteen for SPA CoNLL-2002.We compare CharWNN with two similar neural network architectures: CharNN and WNN. CharNN corresponds to character-level embeddings, i.e. it only uses character-level embeddings without characters. WNN corresponds to character-level embeddings, i.e. it only uses word embeddings. Additionally, suffix embeddings are used in the same way as in [Collobert et al. 2011]."}, {"heading": "4. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Results for Spanish NER", "text": "In Table 3, we report on the performance of the various NNs for the SPA CoNLL-2002 Corpus. All results for this corpus were calculated using the CoNLL-2002 evaluation script4. CharWNN achieves the best precision, recall and F1 in both development and test sets. For the test set, CharWNN's F1 is 3 points higher than WNN's F1, which uses two additional handmade features: suffixes and capital letters. This result suggests that for the NER task, character-level embedding is as or more effective as the two character traits used in WNN. Similar results were tasked by dos Santos and Zadrozny (2014) in the POS tagging. In the last two lines of Table 3, we can see the results of the use of word embedding on level NN."}, {"heading": "4.2. Results for Portuguese NER", "text": "The results in this table were calculated using the CoNLL-2002 evaluation script. We report the results in two scenarios: total and selective. In the overall scenario, all ten categories are taken into account if they achieve similar results. In the selective scenario, only five selected categories (person, organization, date and value) are taken into account. We can see in Table 5 that CharWNN and WNN have two additional craftsmanship characteristics. We think that by increasing the training data, CharWNN has the potential to learn better character embedding and outperform WNN, as happens in the SPA CoNLL-2002-Corpus, which is larger than the HAREM-Corpus-I-Corpus."}, {"heading": "4.3. Impact of unsupervised pre-training of word embeddings", "text": "In Table 8, we evaluate the impact of unattended pre-training of word embedding on CharWNN performance for both SPA CoNLL-2002 and HAREM I. Results were calculated using the CoNLL-2002 evaluation script. For both corporations, CharWNN results improve through unattended pre-training; the impact of unattended pre-training is greater for the HAREM-I corpus (13.2 points in Formula 1) than for the SPA CoNLL-2002 (4.3 points in Formula 1)."}, {"heading": "5. Conclusions", "text": "In this paper, we approach the language-independent NER using a DNN that uses word and character embedding to perform a sequential classification. We show that the same DNN that was successfully used for POS marking can also achieve state-of-the-art results for the NER using the same hyperparameters and without any handicraft characteristics. We also highlight the contribution of neural character embedding to the NER and define new state-of-the-art results for the Portuguese and Spanish NER."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy)", "citeRegEx": "Bergstra,? \\Q2010\\E", "shortCiteRegEx": "Bergstra", "year": 2010}, {"title": "Named entity extraction using adaboost", "author": ["Carreras"], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "Carreras,? \\Q2002\\E", "shortCiteRegEx": "Carreras", "year": 2002}, {"title": "Using deep belief nets for chinese named entity categorization", "author": ["Chen"], "venue": "In Proceedings of the Named Entities Workshop,", "citeRegEx": "Chen,? \\Q2010\\E", "shortCiteRegEx": "Chen", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert,? \\Q2011\\E", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "The automatic content extraction (ace) program tasks, data, and evaluation", "author": ["Doddington"], "venue": "In Proceedings of the Fourth International Conference on Language Resources and Evaluation", "citeRegEx": "Doddington,? \\Q2004\\E", "shortCiteRegEx": "Doddington", "year": 2004}, {"title": "Entropy Guided Transformation Learning - Algorithms and Applications", "author": ["dos Santos", "C.N. Milidi\u00fa 2012] dos Santos", "R.L. Milidi\u00fa"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2012}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "C.N. Zadrozny 2014] dos Santos", "B. Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning, JMLR: W&CP", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Incorporating nonlocal information into information extraction systems by gibbs sampling", "author": ["Finkel"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Finkel,? \\Q2005\\E", "shortCiteRegEx": "Finkel", "year": 2005}, {"title": "Gradientbased learning applied to document recognition", "author": ["Lecun"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun,? \\Q1998\\E", "shortCiteRegEx": "Lecun", "year": 1998}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov"], "venue": "In Proceedings of Workshop at International Conference on Learning Representations", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Machine learning algorithms for portuguese named entity recognition", "author": ["Milidi\u00fa"], "venue": "Revista Iberoamericana de Inteligencia Artificial,", "citeRegEx": "Milidi\u00fa,? \\Q2007\\E", "shortCiteRegEx": "Milidi\u00fa", "year": 2007}, {"title": "Reconhecimento de entidades mencionadas em portugu\u00eas", "author": ["Santos", "D. Cardoso 2007] Santos", "N. Cardoso"], "venue": null, "citeRegEx": "Santos et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2007}, {"title": "Evaluating word representation features in biomedical named entity recognition", "author": ["Tang"], "venue": "tasks. BioMed Research International,", "citeRegEx": "Tang,? \\Q2014\\E", "shortCiteRegEx": "Tang", "year": 2014}, {"title": "Introduction to the conll-2002 shared task: Language-independent named entity recognition", "author": [], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "Sang,? \\Q2002\\E", "shortCiteRegEx": "Sang", "year": 2002}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "F. De Meulder"], "venue": "Proceedings of CoNLL-2003,", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "author": ["J. A"], "venue": "[Viterbi", "citeRegEx": "A.,? \\Q1967\\E", "shortCiteRegEx": "A.", "year": 1967}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["Waibel"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Waibel,? \\Q1989\\E", "shortCiteRegEx": "Waibel", "year": 1989}], "referenceMentions": [{"referenceID": 15, "context": "We perform an extensive number of experiments using two annotated corpora: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002, which contains texts in Spanish. In our experiments, we compare the performance of the joint and individual use of character-level and word-level embeddings. We provide information on the impact of unsupervised pre-training of word embeddings in the performance of our proposed NER approach. Our experimental results evidence that CharWNN is effective and robust for Portuguese and Spanish NER. Using the same CharWNN configuration used by dos Santos and Zadrozny (2014) for POS Tagging, we achieve state-of-the-art results for both corpora.", "startOffset": 76, "endOffset": 622}, {"referenceID": 3, "context": "CharWNN extends Collobert et al.\u2019s (2011) neural network architecture for sequential classification by adding a convolutional layer to extract character-level representations [dos Santos and Zadrozny 2014].", "startOffset": 16, "endOffset": 42}, {"referenceID": 3, "context": "The log-likelihood in Equation 3 can be computed efficiently using dynamic programming [Collobert 2011].", "startOffset": 87, "endOffset": 103}, {"referenceID": 10, "context": "This is the same setup used by dos Santos and Milidi\u00fa (2012). Additionally, we tokenize the HAREM corpus and create a development set that comprises 5% of the training set.", "startOffset": 46, "endOffset": 61}, {"referenceID": 15, "context": "In Table 3, we report the performance of different NNs for the SPA CoNLL-2002 corpus. All results for this corpus were computed using the CoNLL-2002 evaluation script4. CharWNN achieves the best precision, recall and F1 in both development and test sets. For the test set, the F1 of CharWNN is 3 points larger than the F1 of the WNN that uses two additional handcrafted features: suffixes and capitalization. This result suggests that, for the NER task, the character-level embeddings are as or more effective as the two character-level features used in WNN. Similar results were obtained by dos Santos and Zadrozny (2014) in the POS tagging task.", "startOffset": 65, "endOffset": 623}], "year": 2017, "abstractText": "Most state-of-the-art named entity recognition (NER) systems rely on the use of handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking. In this work we propose a language-independent NER system that uses automatically learned features only. Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classification. We perform an extensive number of experiments using two annotated corpora in two different languages: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002, which contains texts in Spanish. Our experimental results shade light on the contribution of neural character embeddings for NER. Moreover, we demonstrate that the same neural network which has been successfully applied for POS tagging can also achieve state-of-the-art results for language-independet NER, using the same hyper-parameters, and without any handcrafted features. For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes), and by 7.2 points in the F1 for the selective scenario (five NE classes).", "creator": "LaTeX with hyperref package"}}}