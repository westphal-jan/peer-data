{"id": "1505.05063", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2015", "title": "Necessary and Sufficient Conditions for Surrogate Functions of Pareto Frontiers and Their Synthesis Using Gaussian Processes", "abstract": "This paper introduces the necessary and sufficient conditions that surrogate functions must satisfy to properly define frontiers of non-dominated solutions in multi-objective optimization problems. Given that this is the first time that those conditions are elicited, there is no reason to believe that the surrogates already proposed in the literature meet them. As a consequence, dominated solutions can be suggested by already proposed surrogates as valid candidates to represent the Pareto frontier. Conceptually speaking, the new conditions we are introducing work directly on the objective space, thus being agnostic on the evaluation methods. Therefore, real objectives or user-designed objectives' surrogates are allowed, opening the possibility of linking independent objective surrogates. To illustrate the practical consequences of adopting the proposed conditions, an oversimplified model for the surrogate is shown to be capable of suggesting a valid frontier of non-dominated solutions, though not the expect one from the data provided. On the other hand, when applying Gaussian processes as surrogates endowed with monotonicity soft constraints and with an adjustable degree of flexibility, the necessary and sufficient conditions proposed here are finely managed by the multivariate distribution, guiding to high-quality surrogates capable of suitably synthesizing an approximation to the Pareto frontier in challenging instances of multi-objective optimization.", "histories": [["v1", "Tue, 19 May 2015 16:09:23 GMT  (347kb)", "https://arxiv.org/abs/1505.05063v1", null], ["v2", "Wed, 20 May 2015 22:45:29 GMT  (347kb)", "http://arxiv.org/abs/1505.05063v2", null], ["v3", "Fri, 18 Dec 2015 06:01:11 GMT  (341kb)", "http://arxiv.org/abs/1505.05063v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["conrado silva miranda", "fernando jos\\'e von zuben"], "accepted": false, "id": "1505.05063"}, "pdf": {"name": "1505.05063.pdf", "metadata": {"source": "CRF", "title": "Necessary and Sufficient Conditions for Surrogate Functions of Pareto Frontiers and Their Synthesis Using Gaussian Processes", "authors": ["Conrado S. Miranda", "Fernando J. Von Zuben"], "emails": ["contact@conradomiranda.com,", "vonzuben@dca.fee.unicamp.br"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. MULTI-OBJECTIVE OPTIMIZATION", "text": "A multi-objective optimization problem (MOO) is defined by a decision-making X and a set of objective functions gi (x): X \u2192 Yi, i, i, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o"}, {"heading": "III. NECESSARY AND SUFFICIENT CONDITIONS FOR SURROGATE SCORE FUNCTIONS", "text": "In this section, we will show how a score function f (y) can trigger an estimated Pareto boundary function (xi) and the conditions it must fulfill, so that the set it defines is actually an estimated Pareto boundary function, that is, no point in it strongly dominates another point in it. 4 The most important theory developed is based on the most general concept of a function f, but the conditions can be difficult to evaluate in a general case. Therefore, we will also provide conclusions that place additional constraints on the results for functions, such as continuous derivatives Taylor. Since some of these results depend on Taylor's approximations and the first derivative at the required points, which can be zero, we must define a generalized gradient f. Definition 9 (Generalized Gradient), where Ck is the class of functions in which the first k derivatives exist and are continuous, with the first k derivatives being null."}, {"heading": "A. Necessary Conditions", "text": "The necessary conditions derived from this are direct applications of the estimated Pareto boundary definition and lay the foundations for how a function f should be defined from a given estimated boundary. Lemma 1 (General Necessity) < \u2212 \u2212 F is an estimated Pareto boundary. Let f (y): RM \u2192 R is a score function for F. Then f (y + \u03b4u) > 0 and f (y \u2212 \u03b4u) < 0 for all y-F, u (0, 1] M, and \u03b4 (R, \u03b4 > 0.Proof. Let us suppose there is y, u, and \u03b4 > 0 so that f (y + \u03b4u) \u2264 0. Let y = y + \u03b4u so that y y y y. \"< If f\" y \") < 0, then from the definition of a score function there are some y\" F \"and\" y, \"so that y.\" From the transitivity of dominance we have this \"y.\""}, {"heading": "B. Sufficient Conditions", "text": "Once we define how the estimated pareto boundary relates to a given score function, we will show that a function that meets the results of the previous problem and the resulting consequences actually clearly defines an estimated pareto boundary. < Let f (y): RM \u2192 R is a function. Let F = {y (y) RM | f (y) = 0} is an estimated pareto boundary. Let f (y) > 0 and f (y) < 0 for all y (F, u (0, 1) M, and f (i) = 0) is an estimated pareto boundary. Proof. For F to be an estimated pareto boundary, we must prove that for each y boundary, y \"F, y\" 6 = \"y\" y \"y\" y \"y\" y \"y\" y \"y\" y. \""}, {"heading": "C. Necessary and Sufficient Conditions", "text": "Since the symmetry between Lemmas 1 and 2 is clear, we can build a theorem to fuse these two and create the necessary and sufficient conditions for defining an estimated Pareto boundary. (Let f (RM). (Let f (RM). (Let). (Let). (Let f (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO. (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO). (LO)."}, {"heading": "IV. LEARNING SURROGATE FUNCTIONS FROM SAMPLES", "text": "After showing what conditions the f function must fulfill, one might ask how to build such a function for a given problem, and above all how to learn one from a series of non-dominated points. This can be a generally difficult question to answer, but we can ask an additional problem that can be helpful in many cases. Lemma 3 (Strictly increasing sufficiency) Let f (y): RM \u2192 R be a strictly increasing function on each coordinate. Let F = {y problem | f (y) = 0}. Then F is an estimated Pareto limit. For F to be an estimated Pareto limit, we must prove that for every y, y border F, y-6 = y-y can be a limit. Let's say that there are y and y-y cases that are y-shaped. Let's let's let P = (p0 = y, p1, pM \u2212 1, y-f) be a limit that is not."}, {"heading": "A. Gaussian Process As a Function Approximation Problem", "text": "Since the flexibility is sufficient to do justice to the given samples, an appropriate choice for a replacement function is = = = Gaussian process (which always has enough capacity to fit the data). Before describing how a Gaussian process is used to approximate the Pareto boundary, we offer the reader an overview of how it works. However, for a more detailed description we refer the reader to [15].A Gaussian process (GP) is a generalization of the multivariate normal distribution to infinite dimensions and can be used to solve a regression problem. A GP defines a probability distribution across functions so that the outputs are jointly normally distributed. To better understand this concept, we consider an infinite column vector y and an infinite matrix x x x x x x x x R. Then a function f: RD \u2192 R can be described by associating the row indexes so that f (xi) = yi."}, {"heading": "B. Gaussian Processes with Monotonicity Soft Constraint as Surrogates", "text": "As in the previous section, we consider the zero mean function \u00b5 (x) = 0 and the square exponential size Y = 2-1.5-1-2-2-101234xf (x) after observing the function distribution by means of a Gaussian process. However, prior to the observations, the distribution is the same across the entire space. According to the observations, the distribution adapts to the limitation of the possible functions. The distribution mean is given by the black line and the 95% confidence interval. Three function samples are also provided for each case. (2) Since we map a value in R from the objective space RM, the input values are the targets and the outputs."}, {"heading": "C. Comparison to Existing SVM Surrogate", "text": "The surrogate method used in [10], like the method proposed in this paper for optimizing the hyperparameters by approximating the probability we use, is based on approximating the limits directly from the values in objective space. [10] This makes it a good candidate for comparing and validating the assumption that the existing methods can also arbitrarily violate the conditions described in this paper. [10] One class of SVM applied in [10] is defined by the following optimization problems: \"The other way in which validation of the validation is performed is implicitly by the kernel K (x, y) = exp.\" (\u2212 y, x \u2212 y), which is similar to the kernel used for the GP. (0, 1) An important difference between training at the SVM and a GP is that the GP optimizes its hyperparameters."}, {"heading": "V. CONCLUSION", "text": "This year, it is more than ever before that it will be able to take the lead."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank CNPq for the financial support."}], "references": [{"title": "Multiple Criteria Optimization: State of the Art Annotated Bibliographic Surveys, ser", "author": ["X. Gandibleux"], "venue": "International Series in Operations Research & Management Science. Springer US,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Nonlinear Multiobjective Optimization", "author": ["K. Miettinen"], "venue": "Springer US,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Multi-Objective Optimization Using Evolutionary Algorithms", "author": ["K. Deb"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Performance Assessment of Multiobjective Optimizers: An Analysis and Review", "author": ["E. Zitzler", "L. Thiele", "M. Laumanns", "C.M. Fonseca", "V.G. Da Fonseca"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 7, no. 2, pp. 117\u2013132, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A Comprehensive Survey of Fitness Approximation in Evolutionary Computation", "author": ["Y. Jin"], "venue": "Soft computing, vol. 9, no. 1, pp. 3\u201312, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Meta-Modeling in Multiobjective Optimization", "author": ["J. Knowles", "H. Nakayama"], "venue": "Multiobjective Optimization. Springer, 2008, pp. 245\u2013284.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-objective Optimization Using Surrogates", "author": ["I. Voutchkov", "A. Keane"], "venue": "Computational Intelligence in Optimization. Springer, 2010, pp. 155\u2013175.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "A Gaussian process surrogate model assisted evolutionary algorithm for medium scale expensive optimization problems", "author": ["B. Liu", "Q. Zhang", "G. Gielen"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 18, no. 2, pp. 180\u2013192, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiobjective estimation of distribution algorithm based on joint modeling of objectives and variables", "author": ["H. Karshenas", "R. Santana", "C. Bielza", "P. Larranaga"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 18, no. 4, pp. 519\u2013542, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Generation of Pareto frontiers using support vector machine", "author": ["Y. Yun", "H. Nakayama", "M. Arakava"], "venue": "International Conference on Multiple Criteria Decision Making, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "A Mono Surrogate for Multiobjective Optimization", "author": ["I. Loshchilov", "M. Schoenauer", "M. Sebag"], "venue": "Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation. ACM, 2010, pp. 471\u2013478.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "An efficient approach to nondominated sorting for evolutionary multiobjective optimization", "author": ["X. Zhang", "Y. Tian", "R. Cheng", "Y. Jin"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 19, no. 2, pp. 201\u2013213, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Dominance-Based Pareto- Surrogate for Multi-Objective Optimization", "author": ["I. Loshchilov", "M. Schoenauer", "M. Sebag"], "venue": "Simulated Evolution and Learning. Springer, 2010, pp. 230\u2013239.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The attainment-function approach to stochastic multiobjective optimizer assessment and comparison", "author": ["V.G. da Fonseca", "C.M. Fonseca"], "venue": "Experimental methods for the analysis of optimization algorithms. Springer, 2010, pp. 103\u2013130.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian Process for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Multi-objective optimization", "author": ["K. Deb"], "venue": "Search methodologies. Springer, 2014, pp. 403\u2013449.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Some Bayesian numerical analysis", "author": ["A. O\u2019Hagan"], "venue": "Bayesian statistics, vol. 4, pp. 345\u2013363, 1992.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "Gaussian processes to speed up hybrid Monte Carlo for expensive Bayesian integrals", "author": ["C.E. Rasmussen", "J.M. Bernardo", "M.J. Bayarri", "J. Berger", "A.P. Dawid", "D. Heckerman", "A.F.M. Smith", "M. West"], "venue": "Bayesian Statistics 7, 2003, pp. 651\u2013659.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Gaussian processes with monotonicity information", "author": ["J. Riihim\u00e4ki", "A. Vehtari"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 645\u2013652.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 2001, pp. 362\u2013369.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["K. Deb", "A. Pratap", "S. Agarwal", "T.A.M.T. Meyarivan"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 6, no. 2, pp. 182\u2013197, 2002.  13", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A review of multiobjective test problems and a scalable test problem toolkit", "author": ["S. Huband", "P. Hingston", "L. Barone", "L. While"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 10, no. 5, pp. 477\u2013506, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "MULTI-OBJECTIVE optimization (MOO), also called multiple criteria optimization [1], is an extension of the standard single-objective optimization, where the objectives may be conflicting with each other [2], [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "MULTI-OBJECTIVE optimization (MOO), also called multiple criteria optimization [1], is an extension of the standard single-objective optimization, where the objectives may be conflicting with each other [2], [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 2, "context": "MULTI-OBJECTIVE optimization (MOO), also called multiple criteria optimization [1], is an extension of the standard single-objective optimization, where the objectives may be conflicting with each other [2], [3].", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "The Pareto frontier is at the core of MOO algorithms, being the foundation of many methods devoted to evaluating the performance and comparing the solutions to each other [4].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "However, the frontier is defined by the objectives, which can be expensive to compute [5], [6], [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "However, the frontier is defined by the objectives, which can be expensive to compute [5], [6], [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "However, the frontier is defined by the objectives, which can be expensive to compute [5], [6], [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 7, "context": "[8], [9], thus saving computational resources.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8], [9], thus saving computational resources.", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "[10] is the closest to the surrogate described in this paper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] presented a similar SVM approach, but the function learnt is defined over the decision space, which allows direct comparison with the Pareto frontier approximation without requiring evaluation of the objectives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "However, contrary to the one-class SVM that learns a model to fit all samples on one side of the approximate frontier, the proposed SVM is also able to consider points that dominate the frontier being approximated, allowing approximation of multiple Pareto frontiers, each defined by a class of points in non-dominated sorting [12].", "startOffset": 327, "endOffset": 331}, {"referenceID": 12, "context": "[13] approximated the Pareto dominance instead of the Pareto frontier by using a rank-based SVM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "However, both [11] and [13] try to estimate the Pareto frontier using generic function approximation models, which do not take into account the particularities of the Pareto frontier.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "However, both [11] and [13] try to estimate the Pareto frontier using generic function approximation models, which do not take into account the particularities of the Pareto frontier.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "For instance, using a binary random field over the objective space to model the boundary between dominated and non-dominated regions, Da Fonseca and Fonseca [14] described a theory that can be used to assess the statistical performance of a stochastic optimization algorithm and compare different algorithms.", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "As an example of how to integrate the theoretical conditions in a surrogate design, we show how to introduce the theoretical conditions as soft constraints in Gaussian processes [15], which are nonparametric models, thus being able to adjust to variable number of samples, and whose hyper-parameters can be easily optimized.", "startOffset": 178, "endOffset": 182}, {"referenceID": 9, "context": "To validate the hypothesis that surrogate methods that do not consider this theory may define invalid Pareto frontier approximations, the constrained Gaussian process is compared to a regular Gaussian process and to an existing SVM-based surrogate [10] and results show that the soft constrained Gaussian process finds good approximations maximally obeying the constraints according to the degree of flexibility of the model.", "startOffset": 248, "endOffset": 252}, {"referenceID": 15, "context": ",M}, where Yi \u2286 R [16].", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "The definition of dominance used in this paper is the same provided in [4], which allows a point to dominate itself.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "A set S is path-connected if there is a path joining any two points x and y in S and a path is defined by a continuous function p : [0, 1] \u2192 S with p(0) = x and p(1) = y.", "startOffset": 132, "endOffset": 138}, {"referenceID": 10, "context": "From the partition of the objective space in three sets, one estimated Pareto frontier, one dominated and one nondominated set, we can define a score function similarly to [11], [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 12, "context": "From the partition of the objective space in three sets, one estimated Pareto frontier, one dominated and one nondominated set, we can define a score function similarly to [11], [13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "For a more detailed description, we refer the reader to [15].", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Usual choices for the mean and covariance functions are the null mean [15], such that \u03bc(x) = 0, and the squared exponential kernel, defined by:", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Besides the observations of f(y) at the desired points, the GP framework also accepts observations of its derivative, since differentiation is a linear operator [18], [19], that is, the derivative of a GP is also a Gaussian process.", "startOffset": 161, "endOffset": 165}, {"referenceID": 17, "context": "Besides the observations of f(y) at the desired points, the GP framework also accepts observations of its derivative, since differentiation is a linear operator [18], [19], that is, the derivative of a GP is also a Gaussian process.", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "Another option is to introduce a probability distribution over the gradient in order to favor positive values, introducing monotonicity information [20].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "In this paper, following the suggestion of [20], we use \u03bd = 10.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "Therefore, the distribution p(M|L) must be approximated by a normal distribution, which can be achieved using the expectation propagation algorithm [21], with the update equations described in [20].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Therefore, the distribution p(M|L) must be approximated by a normal distribution, which can be achieved using the expectation propagation algorithm [21], with the update equations described in [20].", "startOffset": 193, "endOffset": 197}, {"referenceID": 9, "context": "Therefore, this approximation is still close to the correct frontier and could be used to evaluate proposed solutions because it was built with the theoretical developments of this paper in mind and tries to approximate them, which most likely provides better frontier estimates than methods that use traditional regression solutions, such as [10], [11], [13], where the manifold f(y) = 0 can have any shape.", "startOffset": 343, "endOffset": 347}, {"referenceID": 10, "context": "Therefore, this approximation is still close to the correct frontier and could be used to evaluate proposed solutions because it was built with the theoretical developments of this paper in mind and tries to approximate them, which most likely provides better frontier estimates than methods that use traditional regression solutions, such as [10], [11], [13], where the manifold f(y) = 0 can have any shape.", "startOffset": 349, "endOffset": 353}, {"referenceID": 12, "context": "Therefore, this approximation is still close to the correct frontier and could be used to evaluate proposed solutions because it was built with the theoretical developments of this paper in mind and tries to approximate them, which most likely provides better frontier estimates than methods that use traditional regression solutions, such as [10], [11], [13], where the manifold f(y) = 0 can have any shape.", "startOffset": 355, "endOffset": 359}, {"referenceID": 9, "context": "Comparison to Existing SVM Surrogate The surrogate method introduced in [10], like the method proposed in this paper, is based on approximating the frontier directly from values in the objective space.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "The one-class SVM used in [10] is defined by the following optimization problem:", "startOffset": 26, "endOffset": 30}, {"referenceID": 2, "context": "To compare the surrogate methods, we use one test problem from [3], which is also used in [10] to show the behavior of the proposed SVM surrogate.", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": "To compare the surrogate methods, we use one test problem from [3], which is also used in [10] to show the behavior of the proposed SVM surrogate.", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "x1 \u2208 [0, 1], x2 \u2208 [\u22122, 2].", "startOffset": 5, "endOffset": 11}, {"referenceID": 9, "context": "To validate this hypothesis and the conjecture that existing surrogate methods may violate the conditions described in this paper, we compared the proposed GP with a one-class SVM used in [10] on one of the test problems described in the same paper.", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "Further investigations involve studying the behavior of the GP to approximate the Pareto frontier with real benchmarks and using some multi-objective optimization algorithm, such as NSGA-II [23], to provide the points.", "startOffset": 190, "endOffset": 194}, {"referenceID": 21, "context": "Since the objectives tend to be smoother than in the example frontier provided [24], we expect the estimated Pareto frontier described by a GP to fit the true Pareto frontier even better in these problems.", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "Standard performance measures in multi-objective optimization, such as the class in non-dominated sorting [23] and the dominance count [25], satisfy this property and can be used as targets of the regression.", "startOffset": 106, "endOffset": 110}], "year": 2015, "abstractText": "This paper introduces the necessary and sufficient conditions that surrogate functions must satisfy to properly define frontiers of non-dominated solutions in multi-objective optimization problems. These new conditions work directly on the objective space, thus being agnostic about how the solutions are evaluated. Therefore, real objectives or user-designed objectives\u2019 surrogates are allowed, opening the possibility of linking independent objective surrogates. To illustrate the practical consequences of adopting the proposed conditions, we use Gaussian processes as surrogates endowed with monotonicity soft constraints and with an adjustable degree of flexibility, and compare them to regular Gaussian processes and to a frontier surrogate method in the literature that is the closest to the method proposed in this paper. Results show that the necessary and sufficient conditions proposed here are finely managed by the constrained Gaussian process, guiding to high-quality surrogates capable of suitably synthesizing an approximation to the Pareto frontier in challenging instances of multi-objective optimization, while an existing approach that does not take the theory proposed in consideration defines surrogates which greatly violate the conditions to describe a valid frontier.", "creator": "LaTeX with hyperref package"}}}