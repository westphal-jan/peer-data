{"id": "1505.05215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Learning with a Drifting Target Concept", "abstract": "We study the problem of learning in the presence of a drifting target concept. Specifically, we provide bounds on the error rate at a given time, given a learner with access to a history of independent samples labeled according to a target concept that can change on each round. One of our main contributions is a refinement of the best previous results for polynomial-time algorithms for the space of linear separators under a uniform distribution. We also provide general results for an algorithm capable of adapting to a variable rate of drift of the target concept. Some of the results also describe an active learning variant of this setting, and provide bounds on the number of queries for the labels of points in the sequence sufficient to obtain the stated bounds on the error rates.", "histories": [["v1", "Wed, 20 May 2015 00:41:23 GMT  (41kb)", "http://arxiv.org/abs/1505.05215v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["steve hanneke", "varun kanade", "liu yang"], "accepted": false, "id": "1505.05215"}, "pdf": {"name": "1505.05215.pdf", "metadata": {"source": "CRF", "title": "Learning with a Drifting Target Concept", "authors": ["Steve Hanneke", "Varun Kanade"], "emails": ["steve.hanneke@gmail.com", "varun.kanade@ens.fr", "yangli@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.05 215v 1 [cs.L G] 20 M"}, {"heading": "1 Introduction", "text": "Much of the work on statistical learning has focused on learning settings where the concept to be learned is static over time. However, there are many areas of application where this is not the case. For example, in the problem of facial recognition, the concept that needs to be learned is actually changed over time, as each individual's facial features evolve over time. In this work, we are investigating the problem of learning with a drifting target concept. Specifically, we are looking at a statistical learning environment in which data arrives in a stream, and for each data point the learner needs to predict a label for the data point at that time. We are then interested in getting low error rates for these predictions. Target labels are generated from a function that is known to reside in a given concept dream, and at any given time the target function is allowed to change at most distances: this is the probability that the new target function does not match the previous target function."}, {"heading": "2 Definitions and Notation", "text": "X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X.) X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. Xt. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. Xt. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. X. Xt. X. X. X. Xt. X. X. X. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt. Xt"}, {"heading": "3.1 Bounded Drift Rate", "text": "The simplest and perhaps most elegant results for the tracking algorithms are for the series of S sequences with a limited drift rate. Investigation of this problem was initiated in the original work of [HL94]. The best-known general results are due to [Lon99]: namely, that for each [HL94], for each [HL94], for each [HL94], for each [HL94], for each (HL94], for each (HL94), for each (HL94), for each (HL94), for each (HL94), for each (HL94), for each (HL94), for each (HL94), for each (H94), for each (H94), for each (H94), for each (H94), for each (H94), for each (H94)."}, {"heading": "3.2 Varying Drift Rate: Nonadaptive Algorithm", "text": "In addition to the specific limits in the case of h-S-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T T-T T-T-T-T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T T"}, {"heading": "3.3 Computational Efficiency", "text": "[HL94] also proposed a reduction-based approach, which sometimes produces mathematically efficient methods even though the tolerable \u2206 value is smaller. Specifically, given any (randomized) polynomial time algorithm for which such a classifier h-C exists (the so-called consistency problem), they propose a polynomial time algorithm that is 0 for each sequence (x1, y1) = 0 (xm, ym) for which such a classifier h exists (the so-called dLog problem), which is slightly worse (by a factor of dLog (1 / 2)) than the drift rate tolerated by the above-mentioned (typically inefficient) algorithm."}, {"heading": "3.4 Lower Bounds", "text": "[HL94] also have lower boundaries for specific concept spaces: namely linear delimiters and axial rectangles. They explicitly argue that for C, a concept space BASICn = 1 [i / n, (i + ai) / n): a [0, 1] n on [0, 1], under P, there is a uniform distribution on [0, 1], for any quantity [0, 1 / e2] and b [0, 1 / e2] and c \u00b2 2 / n, for any algorithm A and any T \u00b2 N, a selection of h \u00b2 S \u00b2, so that the prediction Y \u00b2 T produced by A at any time fulfils T, P (Y \u00b2 T 6 = YT) >, concluding that there is no tracking algorithm. Furthermore, they find that space BASICn can be embedded in many frequently studied concept spaces, including halves and axes that are equal in both ranges."}, {"heading": "4 Adapting to Arbitrarily Varying Drift Rates", "text": "This section represents a general limit of the error rate, expressed as a function of the drift rates, which may be arbitrary. In contrast to the methods discussed above in the literature, the method that achieves this general result is adapted to the drift rates, so that it does not require information about the drift rates in advance. This is an attractive property, since it essentially allows the algorithm to learn from target concepts under any sequence h \u0445; the difficulty of the task is then simply reflected in the resulting limits of the error rates: that is, faster changing sequences of target functions lead to greater limits of the error rates, but do not require any modification of the algorithm itself."}, {"heading": "4.1 Adapting to a Changing Drift Rate", "text": "Remember that method (1) (based on the work of [HL94]) requires access to the sequence of changes in order to achieve the specified number of errors. (1) This method is based on the choice of a classifier to predict Y numbers by minimizing the number of errors among the previous mt samples, where mt is a value chosen on the basis of the order chosen. (1) The key to modifying this algorithm to fit the order Xi numbers is to determine a suitable choice of mt without reference to the order. The strategy we are using here is to use the data to determine a reasonable value m \u00b2 t."}, {"heading": "4.2 Conditions Guaranteeing a Sublinear Number of Mistakes", "text": "An immediate implication of Theorem 1 is that if the sum of the T values becomes sublinear = = then there is an algorithm that achieves an expected number of errors that grow sublinear in the number of predictions. Formally, we have the following Korollary.Korollary.Korollary.Korollary.Korollary.Korollary1 = o (T).Proof. For each T question N = o (T).Proof. For each T question N = o (T = 1).Proof. For each T question N (T = argmin 1) such an algorithm A is such that for each question T = argmin 1 \u2264 m (T \u2212 m)."}, {"heading": "5 Polynomial-Time Algorithms for Linear Separators", "text": "In this section, we assume that \"t\" = \"for each t\" N, \"for a fixed constant\" > 0, \"and we consider the specific case of learning homogeneous linear delimiters in Rk under a uniform distribution on the origin-centric sphere of unity. In this case, the analysis of [HL94] mentioned in Section 3.3 implies that it is possible to achieve for each prediction a limit on the error rate that is\" O \"(d) by using an algorithm running in time poly (d, 1 / \u0445, log (1 / \u0445)) (and independent of t), which also implies that it is possible to achieve an expected number of errors among T predictions that is\" O \"(d). [CMEDV10] 6 have since proven that a variant of the perceptron algorithm is capable of achieving an expected number of errors O (d).\""}, {"heading": "5.1 An Improved Guarantee for a Polynomial-Time Algorithm", "text": "We have the following result. Theorem 3: If C is the space of homogeneous linear delimiters (with d + 4) and P is the uniform distribution on the surface of the origin-centered sphere in Rd, then there is an algorithm that runs in time (d, 1 / 3, log (1 / 3)) for each time t, so that the expected number of errors among the first T instances is O (with a probability of at least 1 \u2212 3), ert (h), d (h), d), d (c). Besides, this algorithm runs with an expected number of errors among the first T instances is O (1)."}, {"heading": "6 General Results for Active Learning", "text": "As already mentioned, the above results on linear separators are also relevant to the number of queries in active learning. (...) One can also give very general results on the expected number of queries and errors that can be achieved by an active learning algorithm. (...) We assume that we will apply a few definitions to a certain number of queries and errors. (...) We assume that a certain number of queries and errors will be applied to a certain number of queries and errors. (...) We define the region of inconsistencies. (...) We define the region of inconsistencies. (...) We define the region of inconsistencies. (...) The analysis in this section is based on the following algorithms. (...) The Active subroutine is derived from the work of Han12 (slightly modified) and is a variant of the deviations. (...) The analysis in this section is circled around the following algorithms. (...)"}, {"heading": "7 One could alternatively proceed as in DriftingHalfspaces, using the final classifier from the previous batch, which would also add a guarantee on the error rate achieved at all sufficiently large t.", "text": "The proof for Theorem 4 is based on an analysis of the behavior of the Active Subroutine, which is presented in the following table: \"There is a universal constant for each k + 1,.., log2 (M / 2),\" with a probability of at least 1 \u2212 2, if there is a universal constant, \"says Lemma 9.,., log2 (M / 2), with a probability of at least 1 \u2212 2, if h (T + 1), then h (T + 1), then h (T + 1), then h (T + 1), then h (T + 2), then h (T / 2), then h (T / 2), then h (T / 2), then h (T / 2), then h), then h (T / 2), then h (T / 2), then h)."}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["ABL13. P. Awasthi", "M.-F. Balcan", "P.M. Long"], "venue": null, "citeRegEx": "Awasthi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2013}, {"title": "Learning changing concepts by exploiting the structure of change", "author": ["BBDK00. P.L. Bartlett", "S. Ben-David", "S.R. Kulkarni"], "venue": "Machine Learning,", "citeRegEx": "Bartlett et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2000}, {"title": "Agnostic active learning", "author": ["BBL06. M.F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In Proc. of the 23rd International Conference on Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Margin based active learning", "author": ["BBZ07. M.-F. Balcan", "A. Broder", "T. Zhang"], "venue": "In Proceedings of the 20 Conference on Learning Theory,", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "On the complexity of learning from drifting distributions", "author": ["R.D. Barve", "P.M. Long"], "venue": "In Proceedings of the ninth annual conference on Computational learning theory,", "citeRegEx": "Barve and Long.,? \\Q1996\\E", "shortCiteRegEx": "Barve and Long.", "year": 1996}, {"title": "On the complexity of learning from drifting distributions", "author": ["BL97. R.D. Barve", "P.M. Long"], "venue": "Inf. Comput.,", "citeRegEx": "Barve and Long.,? \\Q1997\\E", "shortCiteRegEx": "Barve and Long.", "year": 1997}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["BL13. M.-F. Balcan", "P.M. Long"], "venue": "In Proceedings of the 26 Conference on Learning Theory,", "citeRegEx": "Balcan and Long.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Long.", "year": 2013}, {"title": "Concentration Inequalities", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "Regret minimization with concept drift", "author": ["CMEDV10. K. Crammer", "Y. Mansour", "E. Even-Dar", "J. Wortman Vaughan"], "venue": "In COLT,", "citeRegEx": "Crammer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2010}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A. Kalai", "C. Monteleoni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2009}, {"title": "Active learning via perfect selective classification", "author": ["EYW12. R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "El.Yaniv and Wiener.,? \\Q2012\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2012}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Han07. S. Hanneke"], "venue": "In Proceedings of the 24 International Conference on Machine Learning,", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Activized learning: Transforming passive to active with improved label complexity", "author": ["Han12. S. Hanneke"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hanneke.,? \\Q2012\\E", "shortCiteRegEx": "Hanneke.", "year": 2012}, {"title": "Tracking drifting concepts using random examples", "author": ["HL91. D.P. Helmbold", "P.M. Long"], "venue": "In COLT, pages", "citeRegEx": "Helmbold and Long.,? \\Q1991\\E", "shortCiteRegEx": "Helmbold and Long.", "year": 1991}, {"title": "Tracking drifting concepts by minimizing disagreements", "author": ["HL94. D.P. Helmbold", "P.M. Long"], "venue": "Machine Learning,", "citeRegEx": "Helmbold and Long.,? \\Q1994\\E", "shortCiteRegEx": "Helmbold and Long.", "year": 1994}, {"title": "Predicting {0, 1}-functions on randomly drawn points", "author": ["HLW94. D. Haussler", "N. Littlestone", "M. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Haussler et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1994}, {"title": "Concise formulas for the area and volume of a hyperspherical cap", "author": ["Li11. S. Li"], "venue": "Asian Journal of Mathematics and Statistics,", "citeRegEx": "Li.,? \\Q2011\\E", "shortCiteRegEx": "Li.", "year": 2011}, {"title": "The complexity of learning according to two models of a drifting environment", "author": ["Lon99. P.M. Long"], "venue": "Machine Learning,", "citeRegEx": "Long.,? \\Q1999\\E", "shortCiteRegEx": "Long.", "year": 1999}, {"title": "Estimation of Dependencies Based on Empirical Data", "author": ["Vap82. V. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1982\\E", "shortCiteRegEx": "Vapnik.", "year": 1982}, {"title": "Statistical Learning Theory", "author": ["Vap98. V. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}], "referenceMentions": [], "year": 2015, "abstractText": "We study the problem of learning in the presence of a drifting target concept. Specifically, we provide bounds on the error rate at a given time, given a learner with access to a history of independent samples labeled according to a target concept that can change on each round. One of our main contributions is a refinement of the best previous results for polynomial-time algorithms for the space of linear separators under a uniform distribution. We also provide general results for an algorithm capable of adapting to a variable rate of drift of the target concept. Some of the results also describe an active learning variant of this setting, and provide bounds on the number of queries for the labels of points in the sequence sufficient to obtain the stated bounds on the error rates.", "creator": "LaTeX with hyperref package"}}}