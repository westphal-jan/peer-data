{"id": "1505.05231", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks", "abstract": "We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning. We additionally extend this setting to real-valued function, where we establish consistency of an estimator for the prior, and discuss an additional application to a preference elicitation problem in algorithmic economics.", "histories": [["v1", "Wed, 20 May 2015 02:43:24 GMT  (40kb)", "http://arxiv.org/abs/1505.05231v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["liu yang", "steve hanneke", "jaime carbonell"], "accepted": false, "id": "1505.05231"}, "pdf": {"name": "1505.05231.pdf", "metadata": {"source": "CRF", "title": "Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks", "authors": ["Liu Yang", "Steve Hanneke", "Jaime Carbonell"], "emails": ["yangli@us.ibm.com", "steve.hanneke@gmail.com", "jgc@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.05 231v 1 [cs.L G"}, {"heading": "1 Introduction", "text": "In this context, it should be noted that the measures in question are measures that have been taken in recent years."}, {"heading": "2 The Setting", "text": "Let us define (X, BX) a measurable space (where X is called the instance space), and let D be a distribution on X (called the data distribution), let C be a VC class of measurable classifiers, formulated by the pseudo-metric method (h, g) = D (called the concept space), and designate the UK dimension of C [10]. We assume that our results can be formulated for general D (with slightly more complicated theorem statements) to simplify the statement of the results, which is actually a metric that would follow the corresponding topological conditions on C relative to D.For two probability measures on a measurable space."}, {"heading": "3 An Upper Bound", "text": "We now have the following theory, which applies to any VC class C and data distribution D; it is the main result of this work. Theorem 1. For any class of priors there is an estimator, for any region there is an estimator, for any region there is an estimator, for any region there is an estimator, for every region there is an estimator, for every region there is an estimator, for every region there is an estimator, for every region there is an estimator, for every region there is an estimator, for every region there is an estimator. According to the standard PAC analysis [9,3], for every region with a probability greater than 1, with a probability greater than 1, there is a sample of k = O (d / 3)."}, {"heading": "4 A Minimax Lower Bound", "text": "A natural question is whether theorem 1 can be generally improved. While we expect this to apply to some fixed UK classes (e.g. those of finite size), and in any case we expect that some of the constant factors in the exponent can be improved, it is currently not clear whether the general form of UK classes (e.g. those of finite size) is sometimes optimal. One way to investigate this question is to construct specific spaces C and distributions D for which a lower limit can be reached. In particular, we are generally interested in pointing out lower limits that are worse than those applicable to the usual problem of density estimation, based on direct access to the h-shaped values tA values (see theorem 3 below).Here we present a lower limit that is interesting for this reason. However, although it is greater than the optimal rate for methods with direct access to the target concepts, it is still far removed from the adjustment to the upper limit so that the question of the question remains open."}, {"heading": "5 Real-Valued Functions and an Application in Algorithmic Economics", "text": "In this section, we present results that generalize the analysis of [12] to classes of real functions. We also present an application of this generalization to a problem that generates preferences."}, {"heading": "5.1 Consistent Estimation of Priors over Real-Valued Functions at a Bounded Rate", "text": "In this section, we leave B a \u03c3 algebra on X \u00b7 R, and again we leave BX the corresponding \u03c3 algebra on X. Also for measurable functions h, g: X \u00b7 R, let \u03c1 (h, g) = \u0432 | h \u2212 g | dPX, where PX is a distribution over X. We assume that F is a class of functions X \u2192 R with the Borel algebra BF induced by \u03c1. Let it be a set, and let it come to a probability measurement (F, BF). We assume that the probability is high on (F, BF)."}, {"heading": "5.2 Maximizing Customer Satisfaction in Combinatorial Auctions", "text": "The fact is that you are able to put yourself in a situation where you are able, in which you are able to assert yourself, and in which you are able, in which you are able to assert yourself."}, {"heading": "6 Open Problems", "text": "There are some interesting questions that remain open at this point: Can the lower limit or upper limit in general be improved? If we use m \u2265 d samples instead of d samples per task, how does the Minimax risk vary with m? In this context, what is the optimal value of m to optimize the convergence rate as a function of mT, the total number of samples? More generally, if an estimator is allowed to use N samples taken from any number of tasks, what is the optimal convergence rate as a function of N?"}, {"heading": "A Proofs for Section 5", "text": "The proof for Theorem 4 is based on the following sequence of lemmas, which are used in parallel to obtain the analog result for the consistent estimation of priors via binary functions; the last of these (namely, lemmas 3) requires significant changes to the original reasoning of [12]; the others use arguments more-directly based on those of [12]. Lemma 1. Leave X = {Xt1, Xt2,.}, Y (both) = {Yt1 (both), PZt (both). \u2212 PZt (both). \u2212 Proof. Fix. Leave X = {Xt1, Xt2,.}, Y (both), Yt1 (both), Yt1 (both), Yt2 (both)."}], "references": [{"title": "Sampling lower bounds via information theory", "author": ["Z. Bar-Yossef"], "venue": "Proceedings of the 35th Annual ACM Symposium on the Theory of Computing. pp. 335\u2013344", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A Bayesian/information theoretic model of learning to learn via multiple task sampling", "author": ["J. Baxter"], "venue": "Machine Learning 28, 7\u201339", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M. Warmuth"], "venue": "Journal of the Association for Computing Machinery 36(4), 929\u2013965", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Combinatorial Auctions", "author": ["P. Cramton", "Y. Shoham", "R. Steinberg"], "venue": "The MIT Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Combinatorial Methods in Density Estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": "Springer, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Empirical Processes in M-Estimation", "author": ["S. van de Geer"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "MDL convergence speed for Bernoulli sequences", "author": ["J. Poland", "M. Hutter"], "venue": "Statistics and Computing 16, 161\u2013175", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Theory of Statistics", "author": ["M.J. Schervish"], "venue": "Springer, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Estimation of Dependencies Based on Empirical Data", "author": ["V. Vapnik"], "venue": "Springer-Verlag, New York", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1982}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory of Probability and its Applications 16, 264\u2013280", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1971}, {"title": "Sequential tests of statistical hypotheses", "author": ["A. Wald"], "venue": "The Annals of Mathematical Statistics 16(2), 117\u2013186", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1945}, {"title": "A theory of transfer learning with applications to active learning", "author": ["L. Yang", "S. Hanneke", "J. Carbonell"], "venue": "Machine Learning 90(2), 161\u2013189", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Rates of convergence of minimum distance estimators and Kolmogorov\u2019s entropy", "author": ["Y.G. Yatracos"], "venue": "The Annals of Statistics 13, 768\u2013774", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1985}, {"title": "On polynomial-time preference elicitation with value queries", "author": ["M. Zinkevich", "A. Blum", "T. Sandholm"], "venue": "Proceedings of the 4 ACM Conference on Electronic Commerce. pp. 175\u2013185", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "Among the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12].", "startOffset": 343, "endOffset": 349}, {"referenceID": 11, "context": "Among the several proposed models for transfer learning, one particularly appealing model supposes the learning problems are independent and identically distributed, with unknown distribution, and the advantage of transfer learning then comes from the ability to estimate this shared distribution based on the data from past learning problems [2,12].", "startOffset": 343, "endOffset": 349}, {"referenceID": 11, "context": "In recent work, [12] have shown that under mild conditions on the family of possible distributions, if the target concepts reside in a known VC class, then it is possible to estimate this distribtion using only a bounded number of training samples per task: specifically, a number of samples equal the VC dimension.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "The present work continues that of [12], bounding the rate of convergence for estimating this distribution, under a smoothness condition on the distribution.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "We additionally include an extension of the results of [12] to the setting of real-valued functions, establishing consistency (at a uniform rate) for an estimator of a prior over any VC subgraph class.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "In addition to the application to transfer learning, analogous to the original work of [12], we also discuss an application of this result to a preference elicitation problem in algorithmic economics, in which we are tasked with allocating items to a sequence of customers to approximately maximize the customers\u2019 satisfaction, while permitted access to the customer valuation functions only via value queries.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "2 The Setting Let (X ,BX ) be a measurable space [8] (where X is called the instance space), and let D be a distribution on X (called the data distribution).", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "Let C be a VC class of measurable classifiers h : X \u2192 {\u22121,+1} (called the concept space), and denote by d the VC dimension of C [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "We suppose there exists a probability measure \u03c00 on C (the reference measure) such that every \u03c0\u03b8 is absolutely continuous with respect to \u03c00, and therefore has a density function f\u03b8 given by the Radon-Nikodym derivative d\u03c0\u03b8 d\u03c00 [8].", "startOffset": 228, "endOffset": 231}, {"referenceID": 11, "context": "In previous work, [12] showed that, if \u03a0\u0398 is a totally bounded family, then even with only d number of samples per task, the minimax risk (as a function of the number of tasks T ) converges to zero.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "By the standard PAC analysis [9,3], for any \u03b3 > 0, with probability greater than 1 \u2212 \u03b3, a sample of k = O((d/\u03b3) log(1/\u03b3)) random points will partition C into regions of width less than \u03b3 (under L1(D)).", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "By the standard PAC analysis [9,3], for any \u03b3 > 0, with probability greater than 1 \u2212 \u03b3, a sample of k = O((d/\u03b3) log(1/\u03b3)) random points will partition C into regions of width less than \u03b3 (under L1(D)).", "startOffset": 29, "endOffset": 34}, {"referenceID": 11, "context": "Following analogous to the inductive argument of [12], suppose I \u2286 {1, .", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "[12] showed that E [ |PYd(\u03b8)|Xd(\u0233 )\u2212 PYd(\u03b8\u2032)|Xd(\u0233 )| ] \u2264 4 \u221a \u2016PZd(\u03b8) \u2212 PZd(\u03b8\u2032)\u2016, so that in total we have \u2016\u03c0\u03b8 \u2212 \u03c0\u03b8\u2032\u2016 < (L+1)\u03b3+4(2ek) \u221a \u2016PZd(\u03b8)\u2212PZd(\u03b8\u2032)\u2016.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "IfN(\u03b5) is the \u03b5-covering number of {PZd(\u03b8) : \u03b8 \u2208 \u0398}, then taking \u03b8\u0302T\u03b8\u22c6 as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance \u03b5 from PZd(\u03b8\u22c6), for some T = O((1/\u03b5 ) logN(\u03b5/4)).", "startOffset": 116, "endOffset": 122}, {"referenceID": 4, "context": "IfN(\u03b5) is the \u03b5-covering number of {PZd(\u03b8) : \u03b8 \u2208 \u0398}, then taking \u03b8\u0302T\u03b8\u22c6 as the minimum distance skeleton estimate of [13,5] achieves expected total variation distance \u03b5 from PZd(\u03b8\u22c6), for some T = O((1/\u03b5 ) logN(\u03b5/4)).", "startOffset": 116, "endOffset": 122}, {"referenceID": 11, "context": "Furthermore, the covering number of \u03a0\u0398 upper bounds N(\u03b5) [12], so that N(\u03b5) \u2264 (1/\u03b5) d/\u03b1).", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "d Bernoulli(p) random variables, for each p \u2208 [0, 1]; then it is known that, for any (possibly nondeterministic) decision rule", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "(1) This easily follows from the results of [1], combined with a result of [7] bounding the KL divergence (see also [11]) To use this result, we construct a learning problem as follows.", "startOffset": 116, "endOffset": 120}, {"referenceID": 7, "context": ", i \u2217 T ) = q\u0302 \u2032 i(CiT (\u03b8\u22c6)), for some q\u0302 \u2032 i [8]: that is, estimators that are a function of the NiT (\u03b8\u22c6) = |CiT (\u03b8\u22c6)| Bernoulli(pi) random variables, which we should note are conditionally i.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "samples from a H\u00f6lder smooth density in a d-dimensional space [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "Prior Estimation 11 5 Real-Valued Functions and an Application in Algorithmic Economics In this section, we present results generalizing the analysis of [12] to classes of real-valued functions.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The proof parallels that of [12] (who studied the special case of binary functions), with a few important twists (in particular, a significantly different approach in the analogue of their Lemma 3).", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": ",Z T d (\u03b8\u22c6)), and functions R : N0 \u00d7 (0, 1] \u2192 [0,\u221e) and \u03b4 : N0 \u00d7 (0, 1] \u2192 [0, 1] such that, for any \u03b1 > 0, lim T\u2192\u221e R(T, \u03b1) = lim T\u2192\u221e \u03b4(T, \u03b1) = 0 and for any T \u2208 N0 and \u03b8\u22c6 \u2208 \u0398, P (", "startOffset": 74, "endOffset": 80}, {"referenceID": 11, "context": "2 Maximizing Customer Satisfaction in Combinatorial Auctions Theorem 4 has a clear application in the context of transfer learning, following analogous arguments to those given in the special case of binary classification by [12].", "startOffset": 225, "endOffset": 229}, {"referenceID": 3, "context": "However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 277, "endOffset": 280}, {"referenceID": 13, "context": "However, we are not permitted direct observation of the customer valuation functions; rather, we may query for the value of any given bundle of items; this is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 282, "endOffset": 286}, {"referenceID": 3, "context": "This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 126, "endOffset": 129}, {"referenceID": 13, "context": "This is referred to as a value query in the literature on preference elicitation in combinatorial auctions (see Chapter 14 of [4], [14]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "The proof of this result parallels that of [12] for the transfer learning setting, but is included here for completeness.", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "16 Liu Yang, Steve Hanneke, and Jaime Carbonell A Proofs for Section 5 The proof of Theorem 4 is based on the following sequence of lemmas, which parallel those used by [12] for establishing the analogous result for consistent estimation of priors over binary functions.", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "The last of these lemmas (namely, Lemma 3) requires substantial modifications to the original argument of [12]; the others use arguments more-directly based on those of [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 9, "context": "Note that since F is a uniformly bounded VC subgraph class, so is the collection of functions {|h\u2212 g| : h, g \u2208 F}, so that the uniform strong law of large numbers implies that with probability one, \u2200h, g \u2208 F , \u03c1X(h, g) exists and has \u03c1X(h, g) = \u03c1(h, g) [10].", "startOffset": 253, "endOffset": 257}, {"referenceID": 11, "context": "This proof follows identically to a proof of [12], but is included here for completeness.", "startOffset": 45, "endOffset": 49}, {"referenceID": 7, "context": "Thus, Carath\u00e9odory\u2019s extension theorem (specifically, the version presented by [8]) implies that there exist disjoint sets {Ai}i\u2208N in A such that B \u2286 \u22c3 i\u2208N Ai and PZt(\u03b8)(B) \u2212 PZt(\u03b8\u2032)(B) < \u2211", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "By Carath\u00e9odory\u2019s extension theorem (specifically, the version presented by [8]), there exists a disjoint sequence of sets {Bi(\u03b8, \u03b8)}i=1 such that PZt k (\u03b8)(B\u03b8,\u03b8\u2032)\u2212 PZt k (\u03b8\u2032)(B\u03b8,\u03b8\u2032) < \u03b3 + \u221e \u2211", "startOffset": 76, "endOffset": 79}, {"referenceID": 11, "context": "Therefore, the results of [12] (in the proof of their Lemma 3) imply that", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "[12] argue that for all y \u2208 {0, 1} and t1, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The estimator \u03b8\u0302T\u03b8\u22c6 we will use is precisely the minimum-distance skeleton estimate of PZt d(\u03b8\u22c6) [13,5].", "startOffset": 97, "endOffset": 103}, {"referenceID": 4, "context": "The estimator \u03b8\u0302T\u03b8\u22c6 we will use is precisely the minimum-distance skeleton estimate of PZt d(\u03b8\u22c6) [13,5].", "startOffset": 97, "endOffset": 103}, {"referenceID": 12, "context": "[13] proved that if N(\u03b5) is the \u03b5-covering number of {PZt d(\u03b8\u22c6) : \u03b8 \u2208 \u0398}, then taking this \u03b8\u0302T\u03b8\u22c6 estimator, then for some T\u03b5 = O((1/\u03b5 ) logN(\u03b5/4)), any T \u2265 T\u03b5 has E [", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Abstract. We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning. We additionally extend this setting to real-valued function, where we establish consistency of an estimator for the prior, and discuss an additional application to a preference elicitation problem in algorithmic economics.", "creator": "LaTeX with hyperref package"}}}