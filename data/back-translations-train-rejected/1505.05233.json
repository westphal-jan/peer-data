{"id": "1505.05233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Visual Understanding via Multi-Feature Shared Learning with Global Consistency", "abstract": "Image/video data is usually represented by multiple visual features. Fusion of multi-sources information for establishing the identity has been widely recognized. Multi-feature visual recognition has recently received attention in multimedia applications. This paper studies visual understanding via a newly proposed l_2-norm based multi-feature jointly sharing learning framework, which can simultaneously learn the global label matrix and explicit classifiers from the labeled visual data represented by multiple feature modalities. Additionally, a multi-modal group graph manifold regularizer formed by mixed Laplacian and Hessian graph is proposed for better preserving the manifold structure of different features on the labeled data, while preserving the label consistency and improving the label prediction power via semi-supervised learning. The merits of the proposed multi-feature learning framework lie in jointly sharing the structural information from multiple features in global classifier learning phase based on a mixed graph regularizer on one hand, and an efficient alternating optimization method for fast classifier training on the other hand. Experiments on several benchmark visual datasets, such as 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, YouTube &amp; Consumer Videos dataset and large-scale NUS-WIDE dataset for multimedia understanding all demonstrate that the proposed approach compares favorably with state-of-the-art algorithms.", "histories": [["v1", "Wed, 20 May 2015 03:01:08 GMT  (1767kb)", "http://arxiv.org/abs/1505.05233v1", "13 pages,8 figures"], ["v2", "Wed, 9 Sep 2015 10:07:11 GMT  (1660kb)", "http://arxiv.org/abs/1505.05233v2", "13 pages,6 figures, this paper is accepted for publication in IEEE Transactions on Multimedia"]], "COMMENTS": "13 pages,8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["lei zhang", "david zhang"], "accepted": false, "id": "1505.05233"}, "pdf": {"name": "1505.05233.pdf", "metadata": {"source": "CRF", "title": "multi-modal group graph manifold regularizer formed by mixed Laplacian and Hessian graph is proposed for better preserving the manifold structure of different features on the labeled data, while preserving the label consistency and improving the label prediction power via semi-supervised learning. The merits of the proposed multi-feature learning framework lie in jointly sharing the structural information from multiple features in global classifier learning phase based on a mixed graph regularizer on one hand, and an efficient alternating optimization method for fast classifier training on the other hand. Experiments on several benchmark visual datasets, such as 17-category Oxford Flower", "authors": [], "emails": ["leizhang@cqu.edu.cn).", "csdzhang@comp.polyu.edu.hk)."], "sections": [{"heading": null, "text": "In fact, it is so that most people are able to understand themselves and understand what they are doing to change the world and that they are able to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world in order to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world in order to change the world, to change the world, to change the world in order to change the world, to change the world in order to change the world, to change the world, to change the world in order to change the world, to change the world, to change the world in order to change the world, to change the world in order to change the world, to change the world in order to change the world, to change the world, to change the world in order to change the world, to change the world, to change the world in order to change the order to change the world, to change the world in the order to change the world, to change the world in order to change the world, to change the world in order to change the order to change the world, to change the world, to change the world in order to change the world, to change the world, to change the world in order to change the world, to change the world in order to change the world in the order to change the world, to change the world, to change the world in the order to change the world, to change the world in the order to change the world, to change the world in the order to change the world in order to change the world, to change the world in the order to change the world in the order to change the world in the order to change the world, to change the world, to change the world in the order to change the world, to change the world"}, {"heading": "II. RELATED WORKS", "text": "As previously discussed, this work is closely related to visual recognition efforts and multimodal graph-based learning. In this section, we will briefly review the current dominant approaches.3 http: / / vc.sce.ntu.edu.sg / transfer learning domain adaptation / domain adaptation home.html 4http: / / lms.comp.nus.edu.sg / research / NUS-WIDE.htmlA. Visual recognition A number of methods have been developed for visual recognition, such as face recognition, gender recognition, age assessment, scene categories and object recognition in the computer vision community. The bag-of-features (BoF) model is a popular image categorization, but it rejects the spatial order of local descriptors that limits the descriptive power of image representation. In [2], spatial pyramid matching (SPM) is to achieve beyond sacks and features."}, {"heading": "B. Graph based Semi-supervised Learning", "text": "Semi-supervised learning has been widely used in the recognition task due to the fact that the formation of a small amount of labeled data is prone to overadjustment, while the manual labeling of a large amount of accurately labeled data is lengthy and time-consuming. Zhou et al. [31] proposed a semi-supervised multiple feature analysis method for action recognition based on a subspace [8], in which both global and local structural consistency was taken into account in discriminatory classification training. Zhou et al. [31] proposed a semi-supervised method (LGC) for learning local and global consistency by a regularization framework. In [7] a laplacian graph manifold based semi-supervised learning was proposed, in which a varied assumption that the diverse structural information of the unlabeled data can be obtained. Assuming consistency means that the nearby points probably have the same label and the same points are inconsistent on multiple clusters."}, {"heading": "C. Multi-view Graph based Learning", "text": "Belkin et al. [41] proposed a diverse regulatory framework for semi-supervised learning. Laplacian-regulated least square and Laplacian support vector machines were discussed in their work, but in a single view. Tong et al. [42] proposed a graph-based multimodal learning method with linear and sequential fusion schemes, but the mapping function in the objective function is implicit. Xia et al. [35] proposed a multi-view graphics embedding that calculates an eigenvalue problem in optimization, but for dimension reduction. Wu et al. [43] proposed a frugal multimodal dictionary that learns with laplactic hyper-graph as regulation. Wang et al. [8] Therefore, under multi-view learning and graph diversity, a semi-supervised multimodal learning system was proposed, in which laplactic hyper-graphs were studied as regulation."}, {"heading": "III. MULTI-FEATURE GLOBAL LABEL CONSISTENT CLASSIFIER", "text": "In this section, the standards-based GLCC framework with model formulation, optimization, training algorithm and recognition is presented."}, {"heading": "A. Notations", "text": "Suppose that there are n training samples with d-dimensional vector from c-classes. Describe as training set of i-th characteristic modality, as global identification information of the training samples with c-classes and as predicted identification matrix of the training data. di denotes the dimension. In this thesis and denote Frobenius norm and norm, Tr (\u00b7) denotes the trace operator. In an example vector xi, if xi belongs to the j-th class, and otherwise. The learned classifier of the i-th feature is parameterized as distorted. The laplactic and the Hessian graph matrix are represented as respectively."}, {"heading": "B. Formulation of GLCC", "text": "This year, the time has come for such a process to take place in the first half of the year, in which such a process will take place."}, {"heading": "C. Classifier Training", "text": "From the structure of the proposed GLCC framework (11) we observe that the solutions can be solved by a very efficient alternating optimization approach. First, we repair. The initialized F can be solved by setting the derivative of the following objective function w.r.t. F to 0, (12) Then the initial value of F can be obtained as (13), and the optimization problem shown in (11) becomes (14) by setting the derivatives of the objective function (14) w.r.t.Pi and Bi to 0, or we can have (15) (16) where I am an identity matrix and a complete vector. Note that in the calculation of Pi the initial Bi as a zero vector. After specification, the optimization problem becomes (17).By specifying the derivative of the objective function (17) w.r.F to 0, the predicted designation matrix."}, {"heading": "D. Recognition", "text": "Once they have been obtained, the label of a sample with m-attribute modalities can be determined as follows: (23) which index is the maximum value of the output vector. Specifically, the detection method of the proposed GLCC frame in Algorithm 2.Algorithm 2. Detection of the GLCC frameInput: training set, training labels Y and a test sample of the m-modalities; Procedure: Obtaining and solving the model (11) using the proposed algorithm 1. Output:"}, {"heading": "E. Convergence", "text": "To explore the convergence behavior of the proposed algorithm 1, we first present a problem as follows: Lemma 1: For alternative optimization, if an update with other specified variables is possible, i.e., update, and (t denotes the index of iterations) the objective function value is not increased. Three claims are given: Claim 1. Evidence. On correction, and update, the objective function is solved convex, as in (15), by setting the derivative of the objective function w.r.t. to 0, then it becomes' sclear that.Claim 2. Evidence. Similar to the proof of Claim 1, the objective function convex w.r.t. is solved when fixed, as in (16). Then it is solved. Claim 3.Evidence. When solved, the optimization problem (17), the convex w.r.t. F. By specifying the derivative of the objective function, the convergence can be solved."}, {"heading": "F. Computational Complexity", "text": "Before entering the learning phase, we briefly analyze the computational complexity of the GLCC method, which includes T iterations and m modalities. Before entering the learning phase, the time complexity of the calculation of the laplac and Hessian energy matrices is O (mn 3) + O (m 2 ndT). In learning, each iteration comprises four update steps, and the time complexity in all iterations is O (m 2 ndT). Therefore, the computational complexity of our method is O (mn 3) + O (m 2 ndT). Note that the laplactic and Hessian energy matrices are not involved in iterations, and can therefore be calculated prior to learning algorithms in such a way that the computational complexity O (mn 3) can be avoided in order to reduce the total computing costs. Specifically, the total computational time for different datasets is presented in experiments in Section IV and discussed in Section V."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, the experiments with the Oxford Flowers 17 dataset, the Caltech 101 dataset, the YouTube & Consumer Videos dataset, and an extensive NUS-WIDE dataset for multimedia understanding will be conducted to investigate the effectiveness of our GLCC method."}, {"heading": "A. Datasets, Features and Experimental Setup", "text": "This year, it has come to the point where there is only one person who is able to take care of another person."}, {"heading": "B. Parameter Settings", "text": "In the GLCC model there are two regularization parameters \u03bband \u03b3. The parameters \u03bb and \u03b3 are adjusted during the entire experiments from the set {10 -4, 10 -2, 1, 10 2, 10 4} and provide the best results. The maximum number of training siterations is set to 5. Parameter sensitivity analysis is shown in Section G."}, {"heading": "C. Experimental Results on Flower 17 Data", "text": "The comparative experiments of the Flower 17 data set are conducted in two parts. First, we compare the results of this data set of 11 methods mentioned in the previous paper with basic and state-of-the-art results. Second, we compare the effectiveness of the proposed semi-supervised learning model with four challenging methods such as FSNM [52], FSSI [48], SFSS [51] and MLHR [9], which are closely related to the proposed GLCC. The brief description of these methods is presented in Table I. In experiments, we have fine-tuned the parameters of each method and provide their best results for discussion. The test results of all methods are described in Table II, which specifies the average detection accuracy and standard deviation of the predefined three pull / test columns, as well as the total training and test time in seconds."}, {"heading": "D. Experimental Results on Caltech 101 Data", "text": "Similarly, we first show the basic and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons. From Table III, we can deduce that our proposed GLCC achieves an average detection accuracy of 73.5%, which is above the state of the art of the KMTJSRC. Secondly, we can see that the four multifunctional and semi-monitored methods such as FSNM, FSSI, SFSS and MLHR are also tested using this data set, and the best results are reported after parameter setting in Table III."}, {"heading": "E. Experimental Results for Video Event Recognition", "text": "For this YouTube & Consumer Videos dataset, as claimed in the experimental protocol of this dataset, all methods are compared in three cases: a) classifiers learned on SIFT features with L = 0 and L = 1; b) classifiers learned on ST features with L = 0 and L = 1; c) classifiers learned on SIFT and ST features with L = 0 and L = 1. Results of the three cases are presented in Table VI with Mean Average Precision (MAP) as a yardstick. Firstly, we compare our GLCC method with SVMs, MKL, adaptive SVM (A-SVM) [46] and FR [47] methods as a basis."}, {"heading": "F. Experimental Results on Large-Scale NUS-WIDE Data", "text": "For these large-scale NUS-WIDE web image data, we compare our GLCC with existing multifunctional learning and semi-supervised methods using the experimental protocol. Test results, which were trained on all 3000 training data, are presented in Table V. The results of MAP show the better performance of the GLCC than other related methods. Performance fluctuations of 10%, 30%, 50%, 70% and 90% of the marked training data are shown in Fig.5-d. The results of FSSI, MLHR and GLCC are much better than FSNM and SFSS, which demonstrate the importance of multifunctional learning."}, {"heading": "G. Weights of Laplacian and Hessian Graph", "text": "The learned IEF-IEF-IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0 IEF-0.0-0.0 IEF-0.0 IEF-0.0 IEF-0.0-0.0 IEF-0.0 IEF-0.0 IEF-0.0-0.0-0.0 IEF-0.0 IEF-0.0-0.0-0.0 IEF-0.0-0.0 IEF-0.0 IEF-0.0-0.0 IEF-0.0-0.0-0.0-0.0 IEF-0.1-0.0-0.0-0.0-0.0-0.1-0.0-0.1-0.0-0.1 IEF-0.1 IEF-0.0-0.0-0.0-0.1 IEF-0.0-0.0-0.0-0.0-IEF-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1-0.1 IEF-0."}, {"heading": "H. Parameter Analysis", "text": "In our proposed GLCC method for Flower 17, Caltech 101, YouTube & Consumer Videos and large-scale NUS-WIDE experiments, we examine the effects of the model parameters \u03bb and \u03b3 from the set {10 -4, 10 -2, 1, 10 2, 10 4}. Performance variations (i.e. recognition accuracy / MAP) with the parameters \u03bb and \u03b3 are described in Fig. 6, from which we have the following observations: 1) a smaller value of the parameters \u03bb and \u03b3 show significantly better performance for Flower 17 and Caltech 101 data (see Fig. 6-a and Fig. 6-b) and performance deteriorates drastically when larger than 1; 2) for YouTube & Consumer videos (see Fig. 6-c), a greater value of the parameters \u03bb and \u03b3 shows better performance for Flower 17 and Caltech 101 data (see Fig. 6-b) and performance deteriorates drastically when larger than 1; 2) for YouTube & Consumer videos (see Fig. 6)."}, {"heading": "V. CONVERGENCE AND COMPUTATIONAL TIME ANALYSIS", "text": "This section presents the convergence analysis of the objective function and the classifier (i.e. the mapping matrix P) and analyses the computational complexity of the proposed approach."}, {"heading": "A. Convergence Analysis", "text": "The proof of the GLCC convergence is provided in Section III.D. The convergence of the GLCC, which is shown by the objective function (11) via iterations on three benchmark data sets used in this paper for object detection and video event detection, is described in Fig.7. It can be observed that our GLCC algorithm always converges empirically after some iterations. Furthermore, we have calculated the convergence of the learned classifier P by calculating the difference between iteration t and t-1. The convergence of the proposed GLCC via iterations is described in Fig.8. It is clear that the learned classifier P always converges to a small value for each data set after several iterations."}, {"heading": "B. Computational Time Analysis", "text": "The total computing time for the Flower 17, Caltech 101 and YouTube & Consumer Video datasets, together with the detection performance, were presented in Table II, Table V and Table VI, which clearly show that the proposed method has efficient computing power. Note that the experiments on Flower 17, Caltech 101 and YouTube & Consumer Videos are performed in a laptop with an Inter Core i5 CPU (2.50 GHz) and 4 GB of RAM. Experiments on large-area NUS-WIDE web image data are performed in a computer with an Inter Core i7 CPU and 32 GB of RAM."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we propose a global label classifier that uses multiple feature matrices representing visual data, and common learning for object detection or video event detection. First, the proposed GLCC can fully take into account the multiple view characteristics to achieve better detection performance, which differs from the simple feature concatenation that is not robust and the structural information cannot be used from different angles and can be shared in classification learning. Second, inspired by the semi-supervised multiple regression, the GLCC presents a group chart regulator combined with the laplactic and Hessian energy of multiple views of the said data. It assumes that the label prediction for each view is consistent with the global prediction based on multiple views. Third, a standards-based global classifier with an alternating optimization method is effectively used to demonstrate the multifunctional nature of the multifunctional method. Finally, the same framework of data sets was used for the large-scale selection in this paper model."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the Hong Kong Scholar Program (No XJ2013044), the National Natural Science Foundation of China (No 61401048) and also funded by the China Postdoctoral Science Foundation (No 2014M550457)."}, {"heading": "APPENDIX A", "text": "The entire Hessian energy estimate of individual views / properties can be represented as follows [5]: Where is the sparse Hessian energy matrix of the training set? Proof: First define a local tangent space of the Xi data point. In order to estimate the local tangent space, a PCA is carried out on the adjacent k space, then m leading eigenvectors can be used as an orthogonal basis for. The Hessian regulator, defined from data point Xi, is the square standard of the second covariant derivative, which corresponds to the Frobenius standard of the Hessian for normal coordinates."}, {"heading": "APPENDIX B", "text": "To solve the group of equations (21) in the essay, we first solve \u03b1i as follows. Combine the first and the third equations as \u2462. To the first equation in \u2463, there is the second equation in \u03a6 and the equation \u0442, we have the replacement equation that we can get as (22). Similarly, we can also calculate in the same way."}], "references": [{"title": "On feature combination for multiclass objective classification,", "author": ["P. Gehler", "S. Nowozin"], "venue": "in Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories,", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "in Proc. CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Visual Classification with Multi-Task Joint Sparse Representation,", "author": ["X.T. Yuan", "X. Liu", "S. Yan"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Joint Sparse Representation for Robust Multimodal Biometrics Recognition,", "author": ["S. Shekhar", "V.M. Patel", "N.M. Nasrabadi", "R. Chellappa"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Semi-supervised Regression using Hessian Energy with an Application to Semi-supervised Dimensionality Reduction,", "author": ["K.I. Kim", "F. Steinke", "M. Hein"], "venue": "in Proc. NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation,", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Semi-supervised learning on manifolds,", "author": ["M. Belkin", "P. Niyogi"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Semi-Supervised Multiple Feature Analysis for Action Recognition,", "author": ["S. Wang", "Z. Ma", "Y. Yang", "X. Li", "C. Pang", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Multi-Feature Fusion via Hierarchical Regression for Multi-media Analysis,", "author": ["Y. Yang", "J. Song", "Z. Huang", "Z. Ma", "N. Sebe", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Alpaydn, \u201cMultiple Kernel Learning Algorithms,", "author": ["E.M. G\u04e7nen"], "venue": "J. Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Shawetaylor, \u201cTwo View Learning: SVM-2k, Theory and Practice,", "author": ["J. Farquhar", "H. Meng", "S. Szedmak", "D. Hardoon"], "venue": "Proc. Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A visual vocabulary for flower classification,", "author": ["M. Nilsback", "A. Zisserman"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Column-generation boosting methods for mixture of kernels,", "author": ["J. Bi", "T. Zhang", "K.P. Bennett"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories,", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "CVPR Workshop on Generative-Model Based Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Linear programming boosting via column generation,", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Vehicle Classification on Multi-Sensor Smart Cameras Using Feature- and Decision-Fusion,", "author": ["A. Klausner", "A. Tengg", "B. Rinner"], "venue": "Proc. IEEE Conf. Distributed Smart Cameras,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Retrieval based interactive cartoon synthesis via unsupervised bi-distance metric learning,", "author": ["Y. Yang", "Y. Zhuang", "D. Xu", "Y. Pan", "D. Tao", "S. Maybank"], "venue": "in Proc. ACM MM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Feature Level Fusion of Hand and Face Biometrics,", "author": ["A.A. Ross", "R. Govindarajan"], "venue": "Proc. SPIE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Feature Fusion of Face and Gait for Human Recognition at a Distance in Video,", "author": ["X. Zhou", "B. Bhanu"], "venue": "Proc. Int. Conf. Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Learning Mid-Level  Features for Recognition,", "author": ["Y.L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recogntion,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Two-view transductive support vector machines,", "author": ["G. Li", "S. Hoi", "K. Chang"], "venue": "in Proc. SDM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Discriminative learning and recognition of image set classess using canonical correlations,", "author": ["T. Kim", "J. Kittler", "R. Cipolla"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Multiple Kernels for Object Detection,", "author": ["A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Robust face recognition via sparse representation,", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent K-SVD,", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Fisher Discrimination Dictionary Learning for sparse representation,", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Discriminative K-SVD for dictionary learning in face recognition,", "author": ["Q. Zhang", "B. Li"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Jointly Learning Visually Correlated Dictionaries for Large-Scale Visual Recognition Applications,", "author": ["N. Zhou", "J. Fan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Classification and Clustering via Dictionary Learning with Structured Incoherence and Shared Features,", "author": ["I. Ram\u00edrez", "P. Sprechmann", "G. Sapiro"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Multi-Observation Visual Recognition via Joint Dynamic Sparse Representation,", "author": ["H. Zhang", "N.M. Nasrabadi", "Y. Zhang", "T.S. Huang"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Scholkopf, \u201cLearning with local and global consistency,", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston"], "venue": "in Proc. NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,", "author": ["S. Yan", "D. Xu", "B. Zhang", "H.J. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding,", "author": ["S. Roweis", "L. Saul"], "venue": "Science, vol. 290,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction,", "author": ["J. Tenenbaum", "V. Silva", "J. Langford"], "venue": "Science, vol. 290,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Multiview Spectral Embedding,", "author": ["T. Xia", "T. Mei", "Y. Zhang"], "venue": "IEEE Trans. Systems, Man, and Cybernetics-part B: Cybernetics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Cost-Sensitive Subspace Analysis and Extensions for Face Recognition,", "author": ["J. Lu", "Y.P. Tan"], "venue": "IEEE Trans. Information Forensics and Security,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "More efficiency in multiple kernel learning,", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Sch\u04e7lkopf, \u201cLarge scale multiple kernel learning,", "author": ["S. Sonnenburg", "G. R\u00e4tch", "C. Sch\u00e4fer"], "venue": "JMLR, vol", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Learning the discriminative power-invariance trade-off,", "author": ["M. Varma", "D. Ray"], "venue": "in ICCV, pp", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Linear Spatial Pyramid Matching Using Sparse Coding for Image Classification,", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples,", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Graph based multi-modality learning,", "author": ["H. Tong", "J. He", "M. Li", "C. Zhang", "W.Y. Ma"], "venue": "Proc. ACM Int. Conf. Multimedia,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "Sparse Multi-Modal Hashing,", "author": ["F. Wu", "Z. Yu", "Y. Yang", "S. Tang", "Y. Zhang", "Y. Zhuang"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Automated flower classification over a large number of classes,", "author": ["M. Nilsback", "A. Zisserman"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Visual event recognition in videos by learning from web data,", "author": ["L. Duan", "D. Xu", "I.W. Tsang", "J. Luo"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Cross-domain video concept detection using adaptive svms,", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "Proc. ACM Int\u2019l Conf. Multimedia,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "Frustratingly easy domain adaption,", "author": ["H. Daum\u00e9"], "venue": "Proc. Ann. Meeting Assoc. for Computational Linguistics,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2007}, {"title": "Feature Selection for Multimedia Analysis by Sharing Information Among Multiple Tasks,", "author": ["Y. Yang", "Z. Ma", "A.G. Hauptmann", "N. Sebe"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Multiple Features But Few Labels? A Symbiotic Solution Exemplified for Video Analysis,", "author": ["Z. Ma", "Y. Yang", "N. Sebe", "A.G. Hauptmann"], "venue": "ACM MM,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "A New Approach to Cross-Modal Multimedia Retrieval,", "author": ["N. Rasiwasia", "J.C. Pereira", "E. Coviello", "G. Doyle", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACM MM,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Discriminating Joint Feature Analysis for Multimedia Data Understanding,", "author": ["Z. Ma", "F. Nie", "Y. Yang", "J.R.R. Uijlings", "N. Sebe", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Efficient and Robust Feature Selection via Joint l2,1-norms Minimization,", "author": ["F. Nie", "H. Huang", "X. Cai", "C. Ding"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "NUS-WIDE: A Real-World Web Image Database from National University of Singapore,", "author": ["T.S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y.T. Zheng"], "venue": "ACM International Conference on Image and Video Retrieval,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "For example, given a face image or a video frame, its visual content can be represented by several kinds of weak modalities such as left and right periocular, mouth and nose regions [4] for robust face recognition, or different feature types such as histogram, SIFT, HSV, etc.", "startOffset": 182, "endOffset": 185}, {"referenceID": 8, "context": "[9] for action recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In early, information fusion can be done at three levels: feature level, score level and decision level, whereas feature level fusion can be more discriminative than other two level-fusions [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 17, "context": "Feature concatenation is a prevalent fusion method which has been used in patter recognition [18], [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Feature concatenation is a prevalent fusion method which has been used in patter recognition [18], [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "However, it is less effective in multimedia content analysis, especially when the features are independent or heterogeneous [17], and in particular, simple feature concatenation for high dimensional feature vectors may also become inefficient and non-robust.", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "One popular work is the two-view based support vector machine (SVM-2k) [11], [21], [22], which jointly learns SVM with two views.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "One popular work is the two-view based support vector machine (SVM-2k) [11], [21], [22], which jointly learns SVM with two views.", "startOffset": 77, "endOffset": 81}, {"referenceID": 21, "context": "One popular work is the two-view based support vector machine (SVM-2k) [11], [21], [22], which jointly learns SVM with two views.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Another popular work is multiple kernel learning (MKL) [10], [20], which focus on the information integration from multiple features by combining multiple kernels with respective weights.", "startOffset": 55, "endOffset": 59}, {"referenceID": 19, "context": "Another popular work is multiple kernel learning (MKL) [10], [20], which focus on the information integration from multiple features by combining multiple kernels with respective weights.", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 27, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Some representative works under the framework of dictionary learning such as [25], [26], [27], [28], [29] have been proposed for visual recognition including face, digit, action, and object recognition, which demonstrate that training more discriminative dictionaries and jointly learning visually correlated dictionaries can effectively improve the recognition performance of the reconstruction based classifier proposed in [24].", "startOffset": 425, "endOffset": 429}, {"referenceID": 2, "context": "For examples, in [3], a multi-task joint sparse representation classifier (MTJSRC) was proposed for visual classification in which group sparsity was used to combine multiple features.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "In [4], a multimodal joint sparse representation and kernel space multimodal sparse model were proposed for robust face recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 29, "context": "In [30], a joint dynamic sparse representation classifier model was proposed for object recognition.", "startOffset": 3, "endOffset": 7}, {"referenceID": 47, "context": "In [48], a very efficient multi-task feature selection model (FSSI) with information sharing in low-rank solution was proposed for multimedia analysis.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Comparatively, Hessian regularization has better extrapolating power shown in two facets [5]: first, it has a richer null space and second, it can exploit the intrinsic local geometry of the data manifold very well.", "startOffset": 89, "endOffset": 92}, {"referenceID": 11, "context": "The visual experiments have been conducted on the benchmark visual datasets, including the Oxford flower 17 dataset 1 from [12], the Caltech 101 dataset 2", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "uk/~vgg/software/MKL/ from [14], the YouTube & Consumer video dataset 3 from [45], and the large scale real-world NUS-WIDE web image dataset 4", "startOffset": 27, "endOffset": 31}, {"referenceID": 44, "context": "uk/~vgg/software/MKL/ from [14], the YouTube & Consumer video dataset 3 from [45], and the large scale real-world NUS-WIDE web image dataset 4", "startOffset": 77, "endOffset": 81}, {"referenceID": 52, "context": "from [53] for multimedia analysis.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "In [2], a spatial pyramid matching (SPM) beyond bags of features was proposed for natural scene categories and object recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 39, "context": "[40] also proposed a linear SPM based on sparse coding (ScSPM) for visual classification and obtained significant improvement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 204, "endOffset": 208}, {"referenceID": 36, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 210, "endOffset": 214}, {"referenceID": 37, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 216, "endOffset": 220}, {"referenceID": 12, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 260, "endOffset": 264}, {"referenceID": 14, "context": "In [1], Gehler et al described several feature combination methods including average kernel support vector machine (AK-SVM), product kernel support vector machine (PK-SVM), multiple kernel learning (MKL) [23], [37], [38], column generation boosting (CG-Boost) [13], and linear programming boosting (LP-B and LP-\u03b2) [15] for object recognition.", "startOffset": 314, "endOffset": 318}, {"referenceID": 2, "context": "[3] proposed a multi-task joint sparse representation (MTJSRC) using mixed-norm for visual", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 115, "endOffset": 119}, {"referenceID": 25, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 27, "context": "classification, and obtained better performance by comparing with several sparse dictionary learning methods [24], [25], [26], [27], [28].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "[30] proposed a multi-observation joint dynamic sparse representation for visual recognition, and obtain comparable performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Most related to the paper, a subspace sharing based semi-supervised multiple feature analysis method for action recognition was proposed [8], in which both the global and local structural consistency has been considered in discriminative classifier training.", "startOffset": 137, "endOffset": 140}, {"referenceID": 30, "context": "[31] proposed a graph based semi-supervised method (LGC) for learning local and global consistency by a regularization framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "In [7], a Laplacian graph manifold based semi-supervised learning was proposed, in which a manifold assumption that the manifold structure information of the unlabeled data can be preserved was given.", "startOffset": 3, "endOffset": 6}, {"referenceID": 50, "context": "In [51], a semi-supervised feature selection algorithm SFSS for multimedia analysis based on Laplacian graph and l2,1-norm regularization was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 109, "endOffset": 112}, {"referenceID": 31, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 120, "endOffset": 124}, {"referenceID": 33, "context": "Laplacian eighenmap based manifold learning was usually proposed for dimension reduction and graph embedding [6], [32], [33], [34], but all of them were implemented in single view/modality.", "startOffset": 126, "endOffset": 130}, {"referenceID": 35, "context": "In [36], a graph Laplacian based multi-view spectral embedding (MSE) method was proposed for dimension reduction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "[9] proposed a multi-feature Laplacian graph based hierarchical semi-supervised regression (MLHR) for multimedia analysis and achieved better performance in video concept annotation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Throughout these manifold based methods presented above, Laplacian graph and single feature are the mainstream of semi-supervised learning, however, it has been identified in [5] that it suffers from the fact that the solution is biased towards a constant with weaker extrapolating power.", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "Hessian graph exploited for semi-supervised dimension reduction [5] was proved to have a good extrapolating power.", "startOffset": 64, "endOffset": 67}, {"referenceID": 40, "context": "[41] proposed a manifold regularization framework for semi-supervised learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] proposed a graph based multi-modality learning method with linear and sequential fusion schemes, but the mapping function in the objective function is implicit.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] proposed a multi-view graph embedding which calculates an eigenvalue problem in optimization, but for dimension reduction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] proposed a sparse multi-modal dictionaries learning with Laplacian hyper-graph as regularization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] proposed a semi-supervised multiple feature learning framework in which graph Laplacian regularizer and subspace sharing were studied for action recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 12, "endOffset": 15}, {"referenceID": 34, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 17, "endOffset": 21}, {"referenceID": 40, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 23, "endOffset": 27}, {"referenceID": 41, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 29, "endOffset": 33}, {"referenceID": 42, "context": "Inspired by [8], [35], [41], [42], [43], the multi-feature GLCC is generally formulated as follows.", "startOffset": 35, "endOffset": 39}, {"referenceID": 4, "context": "As denoted in [5], the graph Laplacian based semi-supervised regression suffers from the fact that the solution is biased towards a constant and the extrapolating power is lost, and further proposed a second-order Hessian energy regularizer that shows a better extrapolation capability than Laplacian regularizer in semi-supervised learning, particularly if only few labeled points are available.", "startOffset": 14, "endOffset": 17}, {"referenceID": 35, "context": "\u03b1i=1, \u03b2j=1), such that the complementary structure information of different feature modalities cannot be exploited [36].", "startOffset": 115, "endOffset": 119}, {"referenceID": 43, "context": "The authors in [44] provide seven distance matrices of features, such as clustered HSV, HOG, SIFT on the foreground internal region (SIFTint), SIFT on the foreground boundary (SIFTbdy) and three matrices derived from color, shape and texture vocabularies, along with three predefined splits of training (40 images per class), validation (20 images per class) and testing (20 images per class) sets.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 70, "endOffset": 74}, {"referenceID": 37, "context": "We strictly follow the experimental settings in [1], [3], [13], [15], [37], [38] which contain three predefined train/test splits for fair comparison, and explore the performance of the proposed GLCC in this paper for 17-class object recognition task.", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "Four kinds of kernel matrices, such as geometric blur (GB), Phow-gray (L=0, 1, 2), Phow-color (L=0, 1, 2), and SSIM (L=0, 1, 2) extracted using MKL code package [39] have been used in this paper, where L is the spatial pyramid level.", "startOffset": 161, "endOffset": 165}, {"referenceID": 2, "context": "For all algorithms, 15 training images per category and 15 testing images per category according to the three predefined training/testing splits [3] are employed for verification.", "startOffset": 145, "endOffset": 148}, {"referenceID": 44, "context": "web video domain) including six events such as birthday, picnic, parade, show, sports and wedding is developed for testing those semi-supervised domain adaptation and transfer learning methods in [45], as shown in Fig.", "startOffset": 196, "endOffset": 200}, {"referenceID": 44, "context": "We strictly follow the experimental setting in [45] for all methods.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "2 AK-SVM [1] 84.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "2 2 PK-SVM [1] 85.", "startOffset": 11, "endOffset": 14}, {"referenceID": 37, "context": "2 10 MKL(SILP) [38] 85.", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "5 97 MKL(simple) [37] 85.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "CG-Boost [13] 84.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "2e3 LP-\u03b2 [15] 85.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "0 80 LPBoost [15] 85.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "4 98 FDDL [26] 86.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "9e3 KMTJSRC [3] 86.", "startOffset": 12, "endOffset": 15}, {"referenceID": 51, "context": "FSNM [52] 85.", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "7 24 FSSI [48] 86.", "startOffset": 10, "endOffset": 14}, {"referenceID": 50, "context": "4 12 SFSS [51] 85.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "MLHR [9] 86.", "startOffset": 5, "endOffset": 8}, {"referenceID": 51, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 185, "endOffset": 189}, {"referenceID": 47, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 196, "endOffset": 200}, {"referenceID": 50, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 207, "endOffset": 211}, {"referenceID": 8, "context": "Second, to further demonstrate the effectiveness of the proposed multi-feature semi-supervised learning model, in this paper, we also compare with four challenging methods such as FSNM [52], FSSI [48], SFSS [51], and MLHR [9] that have close relation with the proposed GLCC.", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "8% obtained by KMTJSRC [3], while the proposed GLCC obtains the highest recognition accuracy of 87.", "startOffset": 23, "endOffset": 26}, {"referenceID": 47, "context": "2% which is also better than the multi-feature and semi-supervised learning method FSSI [48] and MLHR [9], respectively.", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "2% which is also better than the multi-feature and semi-supervised learning method FSSI [48] and MLHR [9], respectively.", "startOffset": 102, "endOffset": 105}, {"referenceID": 38, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 44, "endOffset": 47}, {"referenceID": 51, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 53, "endOffset": 57}, {"referenceID": 47, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 63, "endOffset": 67}, {"referenceID": 50, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "Method NS SRC MKL [39] LPBoost [15] KMTJSRC [3] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 83, "endOffset": 86}, {"referenceID": 38, "context": "Similarly, we first show the baseline and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Similarly, we first show the baseline and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons.", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "Similarly, we first show the baseline and state-of-the-art results reported in previous work, such as NS, SRC, MKL [39], LPBoost [15] and KMTJSRC [3] for comparisons.", "startOffset": 146, "endOffset": 149}, {"referenceID": 45, "context": "First, we compare our GLCC method with SVMs, MKL, adaptive SVM (A-SVM) [46], and FR [47] methods as baseline.", "startOffset": 71, "endOffset": 75}, {"referenceID": 46, "context": "First, we compare our GLCC method with SVMs, MKL, adaptive SVM (A-SVM) [46], and FR [47] methods as baseline.", "startOffset": 84, "endOffset": 88}, {"referenceID": 44, "context": "It\u2019s worth noting that the state-of-the-art domain adaptation method reported in [45] for this dataset are not compared because our method does not belong to such transfer learning framework, and only exploit the data for research.", "startOffset": 81, "endOffset": 85}, {"referenceID": 46, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 23, "endOffset": 27}, {"referenceID": 45, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 34, "endOffset": 38}, {"referenceID": 38, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 43, "endOffset": 47}, {"referenceID": 51, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 53, "endOffset": 57}, {"referenceID": 47, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 63, "endOffset": 67}, {"referenceID": 50, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "Method SVM_T SVM_AT FR [47] A-SVM [46] MKL [39] FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 83, "endOffset": 86}, {"referenceID": 51, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 12, "endOffset": 16}, {"referenceID": 47, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 22, "endOffset": 26}, {"referenceID": 50, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "Method FSNM [52] FSSI [48] SFSS [51] MLHR [9] GLCC", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "The total Hessian energy estimation of single view/feature can be represented as [5]", "startOffset": 81, "endOffset": 84}], "year": 2015, "abstractText": "Image/video data is usually represented by multiple visual features. Fusion of multi-sources information for establishing the identity has been widely recognized. Multi-feature visual recognition has recently received attention in multimedia applications. This paper studies visual understanding via a newly proposed -norm based multi-feature jointly sharing learning framework, which can simultaneously learn the global label matrix and explicit classifiers from the labeled visual data represented by multiple feature modalities. Additionally, a multi-modal group graph manifold regularizer formed by mixed Laplacian and Hessian graph is proposed for better preserving the manifold structure of different features on the labeled data, while preserving the label consistency and improving the label prediction power via semi-supervised learning. The merits of the proposed multi-feature learning framework lie in jointly sharing the structural information from multiple features in global classifier learning phase based on a mixed graph regularizer on one hand, and an efficient alternating optimization method for fast classifier training on the other hand. Experiments on several benchmark visual datasets, such as 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, YouTube & Consumer Videos dataset and large-scale NUS-WIDE dataset for multimedia understanding all demonstrate that the proposed approach compares favorably with state-of-the-art algorithms.", "creator": "Microsoft\u00ae Office Word 2007"}}}