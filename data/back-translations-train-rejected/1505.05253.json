{"id": "1505.05253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Knowlege Graph Embedding by Flexible Translation", "abstract": "Knowledge graph embedding refers to projecting entities and relations in knowledge graph into continuous vector spaces. State-of-the-art methods, such as TransE, TransH, and TransR build embeddings by treating relation as translation from head entity to tail entity. However, previous models can not deal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or lack of scalability and efficiency. Thus, we propose a novel method, flexible translation, named TransF, to address the above issues. TransF regards relation as translation between head entity vector and tail entity vector with flexible magnitude. To evaluate the proposed model, we conduct link prediction and triple classification on benchmark datasets. Experimental results show that our method remarkably improve the performance compared with several state-of-the-art baselines.", "histories": [["v1", "Wed, 20 May 2015 05:57:32 GMT  (801kb,D)", "http://arxiv.org/abs/1505.05253v1", null], ["v2", "Thu, 10 Sep 2015 03:48:55 GMT  (0kb,I)", "http://arxiv.org/abs/1505.05253v2", "This paper has been withdraw by the author due to an error in sec3.1"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jun feng", "mantong zhou", "yu hao", "minlie huang", "xiaoyan zhu"], "accepted": false, "id": "1505.05253"}, "pdf": {"name": "1505.05253.pdf", "metadata": {"source": "CRF", "title": "Knowlege Graph Embedding by Flexible Translation", "authors": ["Jun Feng", "Mantong Zhou", "Yu Hao", "Minlie Huang", "Xiaoyan Zhu"], "emails": ["zmt.keke}@gmail.com,", "haoyu@mail.tsinghua.edu.cn,", "zxy-dcs}@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "There is a knowledge graph that contains highly structured and well-organized data and is usually represented by steered graphs in which the nodes correspond to entities and edges of the relationship, or simply by a series of triples (head entity, relationship, tail entity) (h, r, t) for short periods of time. Although there has been considerable success in building knowledge on a large scale, the general paradigm for supporting computer systems is not clear. In fact, traditional knowledge graphs are symbolic and logical frameworks that are not flexible enough to be fertile enough to export statistical learning approaches that require knowledge to be computable in numerical formats. Recently, knowledge graph, the projects of entity or / and relationships to be in continuous vector spaces."}, {"heading": "2 Related Work", "text": "There are a variety of models for embedding knowledge graphs. Each model projects units and relationships into a continuous vector space and the triples are assigned scores to show their accuracy."}, {"heading": "2.1 Translation-based Models", "text": "It assumes that if (h, t) the embedding of the tail entity t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t"}, {"heading": "2.2 Other Related Models", "text": "In addition to TransE, TransH and TransR, there are many other models proposed for embedding knowledge variables. We present several typical models. Unstructured model [Bordes et al., 2014] is considered the naive case of TransE, which sets all translations r = 0, i.e. the score function becomes fr (h, t) = h \u2212 t l1 / 2. Obviously, it cannot distinguish between different relationships. Structured embedding [Bordes et al., 2011] introduces two independent projections for entities in a relationship. The basic idea is that if two entities belong to the same triple, their embedding should be close to each other in some subspace. The score function for the triple is called fr (h, t) = Wr, 1h \u2212 Wr, 2t l1 l1. As pointed out by [Socher et al., 2013], this model is weak to capture the correlations between entities and due to the introduction of two separate matrices."}, {"heading": "3 Method", "text": "To address the aforementioned problems of TransE, TransH and TransR, we propose a Knowledge Diagram Embedding Model based on Flexible Translation (TransF). TransF can essentially overcome the problems of TransE in modeling reflexive / one-to-many / many-to-one / many-to-many relationships. Details of the model are described in Section 3.1. Following this principle of flexible translation, we also propose the TransRF Model, which is an improved variant of TransR, as illustrated in Section 3.2. Let's introduce some notations: S stands for a set of golden triples, S for a set of corrupt triples. A triple (h, r, t) consists of two entities h, t-E (the set of entities) and the relationship r-R (the set of relationships). We use the bold letters h, r, and to denote the corresponding embedding representations."}, {"heading": "3.1 Flexible Translation: TransF", "text": "We first analyze the limitations of TransE, TransR and TransH models. Then we explain our TransF model in detail. As mentioned before, TransE and TransR work well on irreflexive and one-to-one relationships, but they have problems with reflexive / one-to-many / many-to-many relationships. The reason for this is that both TransE and TransR take over the score function when (h, r, t) the score function is low, which means that h + r leads to many-to-many relationships. More specifically, if we use the ideal embedding with the function h + r = t, if (h, r) holds the score function, we can get if (t, r, h) both are correct, r is a reflexive relationship and r = 0, h = 2) if a set of triples (h, r, ti),"}, {"heading": "3.2 Enhancement of TransR: TransRF", "text": "We can improve the TransE-related models to TransF-related models, because TransF outperforms TransE in the ability to discriminate. Contrary to TransR, which takes hr + r \u2248 tr when (h, r, t) is a golden triple, TransRF takes hr + r \u2248 \u03b1tr.Accordingly, we can define the score function asfr (h, t) = (h + r) > tr + h > r (tr \u2212 r) (3), where hr = hMr and tMr = tMr. For each relation r, Mr is the projection matrix that projects units from the entity space into the relationship space. We follow the constraints on the norms of embedding h, r and t and the mapping matrices, as well as ig h, r, t, we have the entities from the entity space into the relationship space."}, {"heading": "3.3 Training Objective", "text": "All models are trained with contrastive maximum margin objective functions, the aim being to ensure that a triple (h, r, t) \"S\" in the golden sentence has a higher score than a triple (h, r, t) \"S\" in the corrupt triple sentence, as follows: L = \u2211 (h, r, t) \"S\" (h, r, t) \"S\" max (0, \u03b3 \u2212 fr (h, t) + fr (h, t \") (4), where\" g > 0 \"is a marginal hyperparameter.\" S \"is the training set of golden triples.\" S \"is the set of corrupt triples.\" The corrupt triples are generated from the training triples, with either the head or tail entity being replaced by a random entity (but not both at the same time). We take over the minisset of golden triples. \"S\" is the set of corrupt triples, with either the head or tail entity being replaced by a random entity."}, {"heading": "4 Experiments", "text": "In this section, we conduct extensive experiments to justify the proposed models. First, we evaluate our models in terms of link prediction [Bordes et al., 2013] and triple classification [Socher et al., 2013]. Second, to explain the remarkable improvements, we assess how well the models can distinguish the golden triples and corrupt triples with the score function by comparing them with TransE. Three benchmark datasets are used in the experiments: WN18 [Bordes et al., 2013] taken from Wordsworth [Miller, 1995]; and two dense subgraphs of the freebase [Bollacker et al., 2008], FB15K [Bordes et al., 2013] and FB13 [Socher et al., 2013]. Table 1 shows the statistics of these datasets."}, {"heading": "4.1 Link Prediction", "text": "As reported in [Bordes et al., 2011; Bordes et al., 2013], Link prediction is actually the prediction of missing h or t given (h, r) or (r, t). In this task, we perform the evaluation by ranking the number of candidates in the knowledge diagram, rather than offering a best matching entity. This experiment is performed on two datasets, WN18 and FB15K.Evaluation protocol. According to the protocol in TransE [Bordes et al., 2013], for each test triple (h, r, t), we replace the head entity h with each entity in the knowledge graph, and rank these corrupt triples in descending order according to the similarity score given by fr. We repeat this procedure by replacing the tail entity t. After collecting all these triples, we use two correct metrics: the middle rank of the correct entities (called Mean Rank)."}, {"heading": "4.2 Triple Classification", "text": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our triple classification model. Triple classification is a binary classification task that predicts whether a given triple (h, r, t) is correct or not. This task is used to answer questions such as: Does Michael Jackson publish the song Beat it?. We use three sets of data in this task: WN11 and FB13 published in NTN [Socher et al., 2013]; FB15K used in TransR [Lin et al., 2015].evaluation protocol. Following the protocol in NTN [Socher et al., 2013] we set a relationship-specific threshold Tr for predicting and then for a triple (h, r, t) when the similarity value achieved by fr, transformation value above Tr, is predicted to be triple negative (h, otherwise)."}, {"heading": "4.3 Score Distribution", "text": "To explain the remarkable improvements that our model achieves in these tasks, we assess whether our model can better distinguish the positive and negative triples compared to TransE. For embedding vectors, we directly use the embedding results that are trained for the link prediction task on WN18 and FB15K. Subsequently, two triple sets of positive and negative values are created for comparison, the positive triples being those in the test dataset. The negative triples generated by the first two methods are constructed in three ways: replacing the head unit randomly from each positive tripel; replacing the tail entity randomly; and randomly selecting two entities and a relationship to the composition of a new trip. The triples generated by the first two methods are called seminative triples and the negative triples by the third method. While we create negative triples, we guarantee that they do not exist in the knowledge graph."}, {"heading": "5 Conclusion", "text": "The idea of flexible translation is to ensure that the sum vector of a head vector and a relation vector has the same direction as a tail entity vector, but with a flexible size. The proposed models, TransF and TransRF, not only address some of the existing problems in previous models when it comes to reflexive / one-to-many / much-to-many relationships, but also maintain low complexity and high efficiency. We conduct extensive experiments with benchmark data sets for link prediction tasks and triple classification, and the results show that our TransF and TransRF models achieve substantial improvements over baselines."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker et al", "2008] Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "et al", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "Learning structured embeddings of knowledge bases. In AAAI,", "citeRegEx": "Bordes et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multi-relational data"], "venue": "pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine Learning", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. A semantic matching energy function for learning with multi-relational data"], "venue": "94(2):233\u2013259,", "citeRegEx": "Bordes et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In International Conference on Artificial Intelligence and Statistics", "author": ["Xavier Glorot", "Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks"], "venue": "pages 249\u2013256,", "citeRegEx": "Glorot and Bengio. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Hoffmann et al", "2011] Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Rodolphe Jenatton", "Nicolas L Roux", "Antoine Bordes", "Guillaume R Obozinski. A latent factor model for highly multi-relational data"], "venue": "pages 3167\u2013 3175,", "citeRegEx": "Jenatton et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Xuan Zhu", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu"], "venue": "Learning entity and relation embeddings for knowledge graph completion.", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM, 38(11):39\u201341,", "citeRegEx": "Miller. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al", "2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "In Proceedings of the 28th international conference on machine learning (ICML-11)", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data"], "venue": "pages 809\u2013816,", "citeRegEx": "Nickel et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "Proceedings of the 21st international conference on World Wide Web, pages 271\u2013280. ACM,", "citeRegEx": "Nickel et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "pages 148\u2013163", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum. Modeling relations", "their mentions without labeled text. In Machine Learning", "Knowledge Discovery in Databases"], "venue": "Springer,", "citeRegEx": "Riedel et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiinstance multi-label learning for relation extraction", "author": ["Surdeanu et al", "2012] Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computa-", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "In Advances in neural information processing systems", "author": ["Ilya Sutskever", "Joshua B Tenenbaum", "Ruslan Salakhutdinov. Modelling relational data using bayesian clustered tensor factorization"], "venue": "pages 1821\u20131828,", "citeRegEx": "Sutskever et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph embedding by translating on hyperplanes"], "venue": "pages 1112\u20131119,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier"], "venue": "arXiv preprint arXiv:1307.7973,", "citeRegEx": "Weston et al.. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "Knowledge graphs such as Wordnet [Miller, 1995] and Freebase [Bollacker et al.", "startOffset": 33, "endOffset": 47}, {"referenceID": 1, "context": "A variety of approaches have been explored for knowledge graph embedding, such as neural network based [Bordes et al., 2011; Socher et al., 2013], and translation based [Bordes et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 14, "context": "A variety of approaches have been explored for knowledge graph embedding, such as neural network based [Bordes et al., 2011; Socher et al., 2013], and translation based [Bordes et al.", "startOffset": 103, "endOffset": 145}, {"referenceID": 2, "context": ", 2013], and translation based [Bordes et al., 2013] approaches.", "startOffset": 31, "endOffset": 52}, {"referenceID": 2, "context": "Some notable works, including TransE [Bordes et al., 2013], TransH [Wang et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 17, "context": ", 2013], TransH [Wang et al., 2014] , and TransR [Lin et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 7, "context": ", 2014] , and TransR [Lin et al., 2015] are simple, efficient, and effective.", "startOffset": 21, "endOffset": 39}, {"referenceID": 8, "context": "Inspired by [Mikolov et al., 2013], TransE learns vector embedding for both entities and relations.", "startOffset": 12, "endOffset": 34}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] represents relationships by translation vectors in an embedding space.", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "TransH [Wang et al., 2014] attempts to alleviate the problems of TransE when dealing with reflexive /one-tomany/many-to-one/many-to-many relations by modeling a relation as a relation-specific hyperplane with wr as the normal vector together with a translation operation dr on it.", "startOffset": 7, "endOffset": 26}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] addresses the issue that some entities are similar in the entity space but comparably different in other specific aspects.", "startOffset": 7, "endOffset": 25}, {"referenceID": 3, "context": "Unstructured Model [Bordes et al., 2014] regards as the naive case of TransE, which sets all translation r = 0, i.", "startOffset": 19, "endOffset": 40}, {"referenceID": 1, "context": "Structured Embedding [Bordes et al., 2011] introduces two independent projections for the entities in a relation.", "startOffset": 21, "endOffset": 42}, {"referenceID": 14, "context": "As pointed out by [Socher et al., 2013],this model is weak to capture the correlations between entities and relations because of the introduction of two separate matrices.", "startOffset": 18, "endOffset": 39}, {"referenceID": 3, "context": "Semantic Matching Energy [Bordes et al., 2014] captures the interactions of relation vectors and entity vectors through multiple matrix Hadamard products.", "startOffset": 25, "endOffset": 46}, {"referenceID": 14, "context": "Neural Tensor Network [Socher et al., 2013] defines an expressive score function as fr(h, t) = ur g(h Mrt+Mr,1h+Mr,2t+br) where ur is a relationspecific linear layer, g() is the tanh operation.", "startOffset": 22, "endOffset": 43}, {"referenceID": 14, "context": "We also treat Single Layer Model [Socher et al., 2013], Latent Factor Model [Jenatton et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 6, "context": ", 2013], Latent Factor Model [Jenatton et al., 2012] and RESCAL [Nickel et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 11, "context": ", 2012] and RESCAL [Nickel et al., 2011; Nickel et al., 2012] as our baseline in experiments.", "startOffset": 19, "endOffset": 61}, {"referenceID": 12, "context": ", 2012] and RESCAL [Nickel et al., 2011; Nickel et al., 2012] as our baseline in experiments.", "startOffset": 19, "endOffset": 61}, {"referenceID": 4, "context": "For TransF, all embeddings are randomly initialized with a similar process of [Glorot and Bengio, 2010].", "startOffset": 78, "endOffset": 103}, {"referenceID": 2, "context": "First, we evaluate our models on link prediction [Bordes et al., 2013] and triple classification [Socher et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 14, "context": ", 2013] and triple classification [Socher et al., 2013] respectively.", "startOffset": 34, "endOffset": 55}, {"referenceID": 2, "context": "Three benchmark datasets are adopted in the experiments: WN18 [Bordes et al., 2013] which is extracted from Wordnet [Miller, 1995]; and two dense subgraphs of Freebase[Bollacker et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 9, "context": ", 2013] which is extracted from Wordnet [Miller, 1995]; and two dense subgraphs of Freebase[Bollacker et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 2, "context": ", 2008], FB15K [Bordes et al., 2013] and FB13 [Socher et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 14, "context": ", 2013] and FB13 [Socher et al., 2013].", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": "As reported in [Bordes et al., 2011; Bordes et al., 2013], link prediction is to predict the missing h or t given (h, r) or (r, t) respectively.", "startOffset": 15, "endOffset": 57}, {"referenceID": 2, "context": "As reported in [Bordes et al., 2011; Bordes et al., 2013], link prediction is to predict the missing h or t given (h, r) or (r, t) respectively.", "startOffset": 15, "endOffset": 57}, {"referenceID": 2, "context": "Following the protocol in TransE [Bordes et al., 2013], for each test triple (h, r, t), we replace the head entity h by every entity in the knowledge graph, and rank these corrupted triples in descending order by the similarity score which is given by fr.", "startOffset": 33, "endOffset": 54}, {"referenceID": 7, "context": "By sharing the same data sets, we directly refer to the baselines and experimental results reported in [Lin et al., 2015].", "startOffset": 103, "endOffset": 121}, {"referenceID": 3, "context": "Raw Filter Raw Filter Raw Filter Raw Filter Unstructured [Bordes et al., 2014] 315 304 35.", "startOffset": 57, "endOffset": 78}, {"referenceID": 11, "context": "3 RESCAl [Nickel et al., 2011] 1,180 1,163 37.", "startOffset": 9, "endOffset": 30}, {"referenceID": 1, "context": "1 SE [Bordes et al., 2011] 1,011 985 68.", "startOffset": 5, "endOffset": 26}, {"referenceID": 3, "context": "8 SME(linear) [Bordes et al., 2014] 545 533 65.", "startOffset": 14, "endOffset": 35}, {"referenceID": 3, "context": "8 SME(bilinear) [Bordes et al., 2014] 526 509 54.", "startOffset": 16, "endOffset": 37}, {"referenceID": 6, "context": "3 LFM [Jenatton et al., 2012] 469 456 71.", "startOffset": 6, "endOffset": 29}, {"referenceID": 2, "context": "1 TransE [Bordes et al., 2013] 263 251 75.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "1 TransH [Wang et al., 2014] 318 303 75.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] 232 219 78.", "startOffset": 7, "endOffset": 25}, {"referenceID": 3, "context": "Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N Unstructured [Bordes et al., 2014] 34.", "startOffset": 87, "endOffset": 108}, {"referenceID": 1, "context": "6 SE [Bordes et al., 2011] 35.", "startOffset": 5, "endOffset": 26}, {"referenceID": 3, "context": "3 SME(linear) [Bordes et al., 2014] 35.", "startOffset": 14, "endOffset": 35}, {"referenceID": 3, "context": "3 SME(bilinear) [Bordes et al., 2014] 30.", "startOffset": 16, "endOffset": 37}, {"referenceID": 2, "context": "8 TransE [Bordes et al., 2013] 43.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "0 TransH [Wang et al., 2014] 66.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] 76.", "startOffset": 7, "endOffset": 25}, {"referenceID": 2, "context": "We classify the relations by following the same rules in [Bordes et al., 2013].", "startOffset": 57, "endOffset": 78}, {"referenceID": 2, "context": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our model on triple classification.", "startOffset": 28, "endOffset": 86}, {"referenceID": 17, "context": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our model on triple classification.", "startOffset": 28, "endOffset": 86}, {"referenceID": 7, "context": "Following the experiment in [Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015], we also evaluate our model on triple classification.", "startOffset": 28, "endOffset": 86}, {"referenceID": 1, "context": "DataSets WN11 FB13 FB15K SE [Bordes et al., 2011] 53.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "2 SME(bilinear) [Bordes et al., 2014] 70.", "startOffset": 16, "endOffset": 37}, {"referenceID": 14, "context": "7 SLM [Socher et al., 2013] 69.", "startOffset": 6, "endOffset": 27}, {"referenceID": 6, "context": "3 LFM [Jenatton et al., 2012] 73.", "startOffset": 6, "endOffset": 29}, {"referenceID": 14, "context": "3 NTN [Socher et al., 2013] 70.", "startOffset": 6, "endOffset": 27}, {"referenceID": 2, "context": "5 TransE [Bordes et al., 2013] 75.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "6 TransH [Wang et al., 2014] 77.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "TransR [Lin et al., 2015] 85.", "startOffset": 7, "endOffset": 25}, {"referenceID": 14, "context": "sets in this task: WN11 and FB13 released in NTN [Socher et al., 2013]; FB15K used in TransR [Lin et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 7, "context": ", 2013]; FB15K used in TransR [Lin et al., 2015].", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": "Following the protocol in NTN [Socher et al., 2013], we set a relation-specific threshold Tr for prediction and then, for a triple (h, r, t), if the similarity score obtained by fr is above Tr, the triple (h, r, t) is predicted as positive, otherwise negative.", "startOffset": 30, "endOffset": 51}, {"referenceID": 7, "context": "We compare our models with the baseline methods reported in [Lin et al., 2015] for WN11, FB13 and FB15K.", "startOffset": 60, "endOffset": 78}, {"referenceID": 17, "context": "As described in [Wang et al., 2014; Lin et al., 2015], FB13 is much denser than WN11 and FB15K where strong correlations exist between entities, and NTN can achieve better results by learning complicated correlations using tensor transformation from dense graph of FB13.", "startOffset": 16, "endOffset": 53}, {"referenceID": 7, "context": "As described in [Wang et al., 2014; Lin et al., 2015], FB13 is much denser than WN11 and FB15K where strong correlations exist between entities, and NTN can achieve better results by learning complicated correlations using tensor transformation from dense graph of FB13.", "startOffset": 16, "endOffset": 53}], "year": 2017, "abstractText": "Knowledge graph embedding refers to projecting entities and relations in knowledge graph into continuous vector spaces. State-of-the-art methods, such as TransE, TransH, and TransR build embeddings by treating relation as translation from head entity to tail entity. However, previous models can not deal with reflexive/one-to-many/manyto-one/many-to-many relations properly, or lack of scalability and efficiency. Thus, we propose a novel method, flexible translation, named TransF, to address the above issues. TransF regards relation as translation between head entity vector and tail entity vector with flexible magnitude. To evaluate the proposed model, we conduct link prediction and triple classification on benchmark datasets. Experimental results show that our method remarkably improve the performance compared with several state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}