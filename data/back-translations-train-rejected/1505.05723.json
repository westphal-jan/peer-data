{"id": "1505.05723", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "On the relation between accuracy and fairness in binary classification", "abstract": "Our study revisits the problem of accuracy-fairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of non-discriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination.", "histories": [["v1", "Thu, 21 May 2015 13:20:06 GMT  (49kb)", "http://arxiv.org/abs/1505.05723v1", "Accepted for presentation to the 2nd workshop on Fairness, Accountability, and Transparency in Machine Learning (this http URL)"]], "COMMENTS": "Accepted for presentation to the 2nd workshop on Fairness, Accountability, and Transparency in Machine Learning (this http URL)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["indre zliobaite"], "accepted": false, "id": "1505.05723"}, "pdf": {"name": "1505.05723.pdf", "metadata": {"source": "META", "title": "On the relation between accuracy and fairness in binary classification", "authors": ["Indr\u0117 \u017dliobait\u0117"], "emails": ["INDRE.ZLIOBAITE@AALTO.FI"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.05 723v 1 [cs.L G] 21 May 201 5"}, {"heading": "1. Introduction", "text": "Discrimination-conscious machine learning is an emerging area of research exploring how to make predictive models free of discrimination when historical data on which they are based may contain biased, incomplete or even discriminatory decisions of the past. Research assumes that the protected grounds against which discrimination is prohibited are given by law. Machine learning aims to develop algorithmic techniques to incorporate these non-discriminatory limitations into predictive models.A number of studies on discrimination-conscious machine learning and data mining (Pedreschi et al., 2009; Kamiran et al., 2010; Calders & Verwer, 2010) focus on achieving equal acceptance rates (proportions of positive decisions) for preferential and protected groups of people in binary classification."}, {"heading": "2. Problem setting and assumptions", "text": "In the face of a data set that contains discrimination, the goal is to build a classifier that would be as accurate as possible and obey non-discrimination restrictions. For example, a model could decide whether to grant a credit because demographic information and financial situation are given and the ethnicity of an applicant (native, foreigner) is considered protected land. We assume that the values of the target variables (labels) in the historical data set are objectively correct, for example, whether the credit was usually repaid or not. In order for discrimination to occur, the target variable must be polar, that is, one result (accept) should be preferred (reject) to the other. Let X specify a set of input variables (e.g. salary, assets), s denote the protected characteristic (e.g. ethnicity: native (w) or foreign (b)) discrimination, and y denote the target variable (e.g. credit decision): accept (+) or reject."}, {"heading": "3. Accuracy and fairness", "text": "The performance of discrimination-conscious classifiers is typically compared by plotting discrimination vs. accuracy. An attempt to eliminate discrimination can easily generate classifiers with different acceptance rates \u03c0 from those in the original dataset, especially when the evaluation of non-discriminatory classifiers must take into account the acceptance of discrimination thresholds (e.g. WEKA1), which simply circulate the numerical probability values without limitations in terms of positive output rates. Our main message is that the evaluation of non-discriminatory classifiers is not comparable to the acceptance of high-income or low-income people because the acceptance rate changes. We randomly divide the dataset into two halves: training and testing. We train a logistical regression (similar results were obtained with Naive Baytree and Decision)."}, {"heading": "4. Baselines and tradeoffs", "text": "It has been observed (Kamiran et al., 2010) that under the assumption that the labels in the data are correct, eliminating discrimination comes with costs - it reduces the accuracy of the prediction. \u2212 The authors have found no limitations on acceptance \u2212 an oracle is a fictitious base classifier that has the highest possible intelligence (as if it knows the true labels) and strives to meet the non-discrimination limitations. \u2212 A random classifier is the opposite, it does not use intelligence. For each individual, a random classifier makes a random prediction with the likelihood of acceptance. The accuracy of the oracle is A0 = 1, kappa will be the discrimination rate 0 = 1, the discrimination would be as in the data d0 and kappa 0. The random classifier defines the other basis of performance with the assumption that the label is firm."}, {"heading": "5. Interesting cases", "text": "We conclude our study with an experiment to illustrate the difference between the raw data and normalized metrics when comparing non-discriminatory classifiers. The experiment compares the performance of three classifiers (Logistic Regression, Naive Bayes and Decision Tree J48 from WEKA) who are discriminated against with three different strategies: including the protected feature under classifier inputs, with the protected feature of classifier inputs and the exclusion of the protected feature from classifier inputs plus the massaging of discrimination data removes the simplest discrimination removal strategy introduced in (Kamiran & Calders, 2009). Training labels are converted from binary to numerical with an anchoring function, we use a logistic regression adjustment to the same training data. A number of the lowest-classified deficiencies that have a positive label are altered in a negative way, and the negative label is transformed from positive to negative, which will have a negative label."}, {"heading": "6. Conclusion", "text": "The evaluation of non-discriminatory classifiers must take positive output rates into account, otherwise the comparison may be misleading and conclusions about comparative performance may be invalid.We have introduced a normalization factor for discrimination standards that takes into account the maximum possible discrimination in a given acceptance.Maximum discrimination occurs when the protected persons are only accepted when everyone from the preferred community is accepted.Acceptance rates may be limited by resources and are not free to make decisions. If the acceptance rate is determined in the data and outputs of the classifier, then classifiers are comparable in relation to A and d, otherwise they must be compared in relation to B and B."}], "references": [{"title": "Three naive bayes approaches for discrimination-free classification", "author": ["Calders", "Toon", "Verwer", "Sicco"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "Calders et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Calders et al\\.", "year": 2010}, {"title": "A coefficient of agreement for nominal scales", "author": ["Cohen", "Jacob"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "Cohen and Jacob.,? \\Q1960\\E", "shortCiteRegEx": "Cohen and Jacob.", "year": 1960}, {"title": "Classification without discrimination", "author": ["Kamiran", "Faisal", "Calders", "Toon"], "venue": "In Proc. of the 2nd IC4 conf. on Computer, Control and Communication,", "citeRegEx": "Kamiran et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kamiran et al\\.", "year": 2009}, {"title": "Discrimination aware decision tree learning", "author": ["Kamiran", "Faisal", "Calders", "Toon", "Pechenizkiy", "Mykola"], "venue": "In Proc. of the 2010 IEEE International Conference on Data Mining,", "citeRegEx": "Kamiran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kamiran et al\\.", "year": 2010}, {"title": "Measuring discrimination in socially-sensitive decision records", "author": ["Pedreschi", "Dino", "Ruggieri", "Salvatore", "Turini", "Franco"], "venue": "In Proc. of the SIAM Int. Conf. on Data Mining,", "citeRegEx": "Pedreschi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pedreschi et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "A number of studies in discrimination-aware machine learning and data mining (Pedreschi et al., 2009; Kamiran et al., 2010; Calders & Verwer, 2010) focus on achieving equal acceptance rates (proportions of positive decisions) for favored and protected groups of individuals in binary classification.", "startOffset": 77, "endOffset": 147}, {"referenceID": 3, "context": "A number of studies in discrimination-aware machine learning and data mining (Pedreschi et al., 2009; Kamiran et al., 2010; Calders & Verwer, 2010) focus on achieving equal acceptance rates (proportions of positive decisions) for favored and protected groups of individuals in binary classification.", "startOffset": 77, "endOffset": 147}, {"referenceID": 3, "context": "It has been observed (Kamiran et al., 2010) that, assuming the labels in data are correct, discrimination removal comes at a cost \u2013 it reduces prediction accuracy.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": "As suggested in (Kamiran et al., 2010), the oracle would either reduce the 0 0.", "startOffset": 16, "endOffset": 38}], "year": 2015, "abstractText": "Our study revisits the problem of accuracyfairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of nondiscriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination.", "creator": "LaTeX with hyperref package"}}}