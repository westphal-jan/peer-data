{"id": "1505.05841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "Translation Memory Retrieval Methods", "abstract": "Translation Memory (TM) systems are one of the most widely used translation technologies. An important part of TM systems is the matching algorithm that determines what translations get retrieved from the bank of available translations to assist the human translator. Although detailed accounts of the matching algorithms used in commercial systems can't be found in the literature, it is widely believed that edit distance algorithms are used. This paper investigates and evaluates the use of several matching algorithms, including the edit distance algorithm that is believed to be at the heart of most modern commercial TM systems. This paper presents results showing how well various matching algorithms correlate with human judgments of helpfulness (collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm based on weighted n-gram precision that can be adjusted for translator length preferences consistently returns translations judged to be most helpful by translators for multiple domains and language pairs.", "histories": [["v1", "Thu, 21 May 2015 18:57:34 GMT  (37kb,D)", "http://arxiv.org/abs/1505.05841v1", "9 pages, 6 tables, 3 figures; appeared in Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, April 2014"]], "COMMENTS": "9 pages, 6 tables, 3 figures; appeared in Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, April 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael bloodgood", "benjamin strauss"], "accepted": false, "id": "1505.05841"}, "pdf": {"name": "1505.05841.pdf", "metadata": {"source": "CRF", "title": "Translation Memory Retrieval Methods", "authors": ["Michael Bloodgood", "Benjamin Strauss"], "emails": ["meb@umd.edu", "bstrauss@umd.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that one sees oneself as being able to live in a country where people are able to develop, and where people are able to develop, in order to develop, and in which they are able to develop, in order to develop."}, {"heading": "2 Translation Memory Similarity Metrics", "text": "All metrics calculate values between 0 and 1, with higher values indicating better match. All metrics relate to two inputs: M and C, where M is a workload from the MTBT (Material To Be Translated) and C is the source language side of an existing translation of a candidate from the TM Bank. Measures range from simple baseline to the presumed current industry standard to new methods."}, {"heading": "2.1 Percent Match", "text": "Perhaps the simplest metric that could be imagined for TM similarity matching is percent match (PM), the percentage of tokens in the MTBT segment that can be found on the source language side of the candidate translation pair to be translated by the TM Bank. Formally, PM (M, C) = | Munigrams Cunigrams | | Munigrams |, (1) where M is the sentence from the MTBT to be translated, C is the source language side of the candidate translation from the TM Bank, Munigrams is the set of unigrams in M and Kunigrams is the set of unigrams in C."}, {"heading": "2.2 Weighted Percent Match", "text": "One disadvantage of PM is that it weights the match of the individual unigrams in an MTBT segment equally, but it is not the case that the value of support for the translator is the same for each unigram of the MTBT segment. The parts that are most valuable to the translator are the parts that he / she does not yet know how to translate. Weighted Percentage Agreement (WPM) uses the inverse frequency of the document (IDF) as a proxy for trying to weight words based on how much value their translations should offer the translators. The use of IDF-based weighting is motivated by the assumption that common words that permeate the entire language will be easy for translators to translate, words that occur in relatively rare situations, but more difficult to translate and therefore more valuable than they match in the TM bank."}, {"heading": "2.3 Edit Distance", "text": "A disadvantage of both PM and WPM metrics is that they only take into account the coverage of the words from the workload sentence in the candidate set of the TM bank and not the context of the words. However, words can be translated very differently depending on their context. Therefore, it can be expected that a TM metric that matches sentences with more than just (weighted) percentage coverage of lexical items will perform better for evaluating and querying the TM bank. In fact, as discussed in Section 1, it is generally assumed that most TM similarity metrics used in existing systems are based on string spacing. Our implementation of the editing distance (Levenshtein, 1966), which is calculated at word level, is similar to the version defined in (Koehn and Senell Type, 2010). Formally, our TM metric, which is based on editing distance ED (), as a max, C, C, C (Editor, C | where Mundit) and M (Mundit)."}, {"heading": "2.4 N-Gram Precision", "text": "In fact, it is the case that most people are in a position to put themselves in the world, in which they are able to put themselves in the position, in which they are able to put themselves in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to put themselves, in which they are able to put themselves, in which they are able to put"}, {"heading": "2.5 Weighted N-Gram Precision", "text": "Similar to the way in which we have improved PM with WPM, we are trying to improve NGP in a similar way. As can be seen from the counter of Eq.5, NGP weights the correspondence of all n-grams as uniformly important. However, it is not the case that each n-gram is of equal value to the translator. Similar to WPM, we use IDF as the basis of our representative for the weighting of n-grams according to the value that their translations should provide to the translators. Specifically, we define the weight of an n-gram as the sum of the IDF values for each contiguous unigram that the n-gram consists of. Accordingly, we formally define the method weighted N-gram precision (WNGP) as follows: WNGP = N-gram = 1 N-wpn = 1 N-wpn = 1 N-wpn = 1 N-wpn, (6) where N-gram i i i i i i i is defined as in Equation 4, andwpn-g = 1-g g-gram} (w g [w-g-g-i] [w-i-i] (i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i"}, {"heading": "2.6 Modified Weighted N-gram Precision", "text": "Modified Weighted NGram Precision (MWNGP) improves the WNGP by weighting the contribution of each wpn, so that shorter n-grams contribute more than longer ngs. Intuition is that it is more helpful for TM settings to get more high-quality shorter n-gram matches at the expense of less longer n-gram matches, as translators get relatively more support by recognizing new high-quality vocabularies. Since translators probably already know the rules of the language regarding the correct order of words, the loss of the longer n-gram matches will be mitigated. Formally, we define MWNGP as follows: MWNGP = 2N2N \u2212 1 N-n = 1 1 2n wpn, (9) where N and wpn are as defined for Equation 6."}, {"heading": "3 Experimental Setup", "text": "We conducted experiments on two corpora from two different technical areas with two language pairs, French-English and Chinese-English. Section 3.1 discusses the specifics of the corpora and the processing we perform. Section 3.2 discusses the specifics of our human assessments, how helpful segments found for translation are."}, {"heading": "3.1 Corpora", "text": "The first step in our experiments was the pre-processing of corporations. For the Chinese corporations, we compared each sentence with the Stanford Chinese Word Segmenter (Tseng et al., 2005) with the Chinese Treebank Standard. The first step in our experiments was the pre-processing of corporations. We compared it with the Stanford Chinese Word Segmenter (Tseng et al., 2005) with the Chinese Treebank Standard (Xia, 2000). We have all segments that have less than 5 tokens or more than 100 tokens."}, {"heading": "3.2 Human Evaluations", "text": "To evaluate how helpful the translations retrieved from the various TM retrieval metrics would be for the translation of the MTBT segments, we used Amazon Mechanical Turk, which has been used productively in the past for related work related to machine translation (Bloodgood and Callison-Burch, 2010b; Bloodgood and CallisonBurch, 2010a; Callison-Burch, 2009).For each pair (MTBT segment, best-matching TMB segment) generated, as discussed in Section 3.1, we collected assessments from Turks (i.e. the workers on MTurk) about how helpful the TMB translation would be for the translation of the MTBT segment on a 5-point scale. The 5-point scale was as follows: \u2022 5 = Extremely helpful. The sample is so similar that with trivial modifications I can perform the translation. \u2022 4 = Very helpful."}, {"heading": "4 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Main Results", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "4.2 Adjusting for length preferences", "text": "Table 5 shows how the average length, measured by the number of tokens on the source side of the translation pairs returned by MWNGP, changes when the Z parameter is changed. Table 6 shows an example of how the optimal translation pair returned by MWNGP changes from Z = 0.00 to Z = 1.00. The example illustrates the effects of changing the Z value on the type of translation matches returned by MWNGP. As discussed in Section 2, smaller settings of Z are suitable for preferences of shorter matches that are more precise because a larger percentage of their content will be relevant. Bigger settings of Z are suitable for preferences for longer matches that are more remembered because they have more overall matches with the content in the MTBT segment, albeit at a possible cost of more irrelevant content."}, {"heading": "5 Conclusions", "text": "Translation memory is one of the most widely used translation technologies. One of the most important aspects of the technology is the system for evaluating translation candidates from the TM bank for retrieval. Although detailed descriptions of the device used in commercial systems are lacking, it is widely believed that they are based on an edit distance approach. We have defined and studied several TM retrieval approaches, including a new method that uses a modified weighted Ngram precision TM, which is better than processing distances according to human translator judgments of helpfulness. The MWNGP method is based on the following premise: local context matching is desired; a weighting of words and phrases according to expected helpfulness for translators is desired; and enabling shorter n-gram precisions contributes more to the end result than longer n-gram precisions. One advantage of the method is that they can be adapted to the preferences of translators who are aware of the translation work returned to them."}], "references": [{"title": "Machine translation and computerized terminology systems: a translator\u2019s viewpoint", "author": ["Peter J Arthern."], "venue": "Translating and the Computer: Proceedings of a Seminar, pages 77\u2013108.", "citeRegEx": "Arthern.,? 1978", "shortCiteRegEx": "Arthern.", "year": 1978}, {"title": "The effects of word order and segmentation on translation retrieval performance", "author": ["Timothy Baldwin", "Hozumi Tanaka."], "venue": "Proceedings of the 18th conference on Computational linguistics-Volume 1, pages 35\u201341. Association for Computational Lin-", "citeRegEx": "Baldwin and Tanaka.,? 2000", "shortCiteRegEx": "Baldwin and Tanaka.", "year": 2000}, {"title": "Bucking the trend: Large-scale cost-focused active learning for statistical machine translation", "author": ["Michael Bloodgood", "Chris Callison-Burch."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854\u2013864.", "citeRegEx": "Bloodgood and Callison.Burch.,? 2010a", "shortCiteRegEx": "Bloodgood and Callison.Burch.", "year": 2010}, {"title": "Using mechanical turk to build machine translation evaluation sets", "author": ["Michael Bloodgood", "Chris Callison-Burch."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, pages 208\u2013", "citeRegEx": "Bloodgood and Callison.Burch.,? 2010b", "shortCiteRegEx": "Bloodgood and Callison.Burch.", "year": 2010}, {"title": "A comparative evaluation of bilingual concordancers and translation memory systems", "author": ["Lynne Bowker", "Michael Barlow."], "venue": "Topics in Language Resources for Translation and Localization, \u00c1msterdam-Filadelfia: John Benjamins, pages 1\u201322.", "citeRegEx": "Bowker and Barlow.,? 2008", "shortCiteRegEx": "Bowker and Barlow.", "year": 2008}, {"title": "Fast, cheap, and creative: Evaluating translation quality using Amazon\u2019s Mechanical Turk", "author": ["Chris Callison-Burch."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286\u2013295, Singapore, August. As-", "citeRegEx": "Callison.Burch.,? 2009", "shortCiteRegEx": "Callison.Burch.", "year": 2009}, {"title": "Translation-memory (tm) research: what do we know and how do we know", "author": ["Tina Paulsen Christensen", "Anne Gram Schjoldager"], "venue": null, "citeRegEx": "Christensen and Schjoldager.,? \\Q2010\\E", "shortCiteRegEx": "Christensen and Schjoldager.", "year": 2010}, {"title": "Automatic evaluation of machine translation quality using n-gram cooccurrence statistics", "author": ["George Doddington."], "venue": "Proceedings of the second international conference on Human Language Technology Research, HLT \u201902, pages 138\u2013145, San", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Power shifts in web-based translation memory", "author": ["Ignacio Garcia."], "venue": "Machine Translation, 21(1):55\u201368.", "citeRegEx": "Garcia.,? 2007", "shortCiteRegEx": "Garcia.", "year": 2007}, {"title": "Integrating n-best smt outputs into a tm system", "author": ["Yifan He", "Yanjun Ma", "Andy Way", "Josef Van Genabith."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 374\u2013382. Association for Computational Lin-", "citeRegEx": "He et al\\.,? 2010", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "The proper place of men and machines in language translation", "author": ["Martin Kay."], "venue": "Research Report", "citeRegEx": "Kay.,? 1980", "shortCiteRegEx": "Kay.", "year": 1980}, {"title": "Convergence of translation memory and statistical machine translation", "author": ["Philipp Koehn", "Jean Senellart."], "venue": "Proceedings of AMTA Workshop on MT Research and the Translation Industry, pages 21\u201331.", "citeRegEx": "Koehn and Senellart.,? 2010", "shortCiteRegEx": "Koehn and Senellart.", "year": 2010}, {"title": "The significance of recall in automatic metrics for mt evaluation", "author": ["Alon Lavie", "Kenji Sagae", "Shyamsundar Jayaraman."], "venue": "In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA-2004.", "citeRegEx": "Lavie et al\\.,? 2004", "shortCiteRegEx": "Lavie et al\\.", "year": 2004}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["Vladimir I Levenshtein."], "venue": "Soviet physics doklady, volume 10, page 707.", "citeRegEx": "Levenshtein.,? 1966", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Extra: a system for examplebased translation assistance", "author": ["Federica Mandreoli", "Riccardo Martoglia", "Paolo Tiberio."], "venue": "Machine Translation, 20(3):167\u2013197.", "citeRegEx": "Mandreoli et al\\.,? 2006", "shortCiteRegEx": "Mandreoli et al\\.", "year": 2006}, {"title": "A bilingual concordance system and its use in linguistic studies", "author": ["Alan K Melby"], "venue": "The Eighth Lacus Forum, pages 541\u2013549, Columbia, SC.", "citeRegEx": "Melby,? 1981", "shortCiteRegEx": "Melby", "year": 1981}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A poor man\u2019s translation memory using machine translation evaluation metrics", "author": ["Michel Simard", "Atsushi Fujita."], "venue": "Conference of the Association for Machine Translation in the Americas 2012, San Diego, California, USA, October.", "citeRegEx": "Simard and Fujita.,? 2012", "shortCiteRegEx": "Simard and Fujita.", "year": 2012}, {"title": "Computers and translation: a translator\u2019s guide, volume 35", "author": ["Harold L Somers."], "venue": "John Benjamins Publishing Company.", "citeRegEx": "Somers.,? 2003", "shortCiteRegEx": "Somers.", "year": 2003}, {"title": "News from OPUS - A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, vol-", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A conditional random field word segmenter for sighan bakeoff 2005", "author": ["Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning."], "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume", "citeRegEx": "Tseng et al\\.,? 2005", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Evaluation metrics for a translation memory system", "author": ["Edward K. Whyman", "Harold L. Somers."], "venue": "Software-Practice and Experience, 29:1265\u20131284.", "citeRegEx": "Whyman and Somers.,? 1999", "shortCiteRegEx": "Whyman and Somers.", "year": 1999}, {"title": "The segmentation guidelines for the penn chinese treebank (3.0)", "author": ["Fei Xia"], "venue": "Technical Report IRCS-00-06,", "citeRegEx": "Xia.,? \\Q2000\\E", "shortCiteRegEx": "Xia.", "year": 2000}], "referenceMentions": [{"referenceID": 6, "context": "The most widely used computer-assisted translation (CAT) tool for professional translation of specialized text is translation memory (TM) technology (Christensen and Schjoldager, 2010).", "startOffset": 149, "endOffset": 184}, {"referenceID": 0, "context": "The main conceptions of TM technology occurred in the late 1970s and early 1980s (Arthern, 1978; Kay, 1980; Melby and others, 1981).", "startOffset": 81, "endOffset": 131}, {"referenceID": 10, "context": "The main conceptions of TM technology occurred in the late 1970s and early 1980s (Arthern, 1978; Kay, 1980; Melby and others, 1981).", "startOffset": 81, "endOffset": 131}, {"referenceID": 4, "context": "TM has been widely used since the late 1990s and continues to be widely used today (Bowker and Barlow, 2008; Christensen and Schjoldager, 2010; Garcia, 2007; Somers, 2003).", "startOffset": 83, "endOffset": 171}, {"referenceID": 6, "context": "TM has been widely used since the late 1990s and continues to be widely used today (Bowker and Barlow, 2008; Christensen and Schjoldager, 2010; Garcia, 2007; Somers, 2003).", "startOffset": 83, "endOffset": 171}, {"referenceID": 8, "context": "TM has been widely used since the late 1990s and continues to be widely used today (Bowker and Barlow, 2008; Christensen and Schjoldager, 2010; Garcia, 2007; Somers, 2003).", "startOffset": 83, "endOffset": 171}, {"referenceID": 18, "context": "TM has been widely used since the late 1990s and continues to be widely used today (Bowker and Barlow, 2008; Christensen and Schjoldager, 2010; Garcia, 2007; Somers, 2003).", "startOffset": 83, "endOffset": 171}, {"referenceID": 11, "context": "The retrieval algorithm used by commercial TM systems is typically not disclosed (Koehn and Senellart, 2010; Simard and Fujita, 2012; Whyman and Somers, 1999).", "startOffset": 81, "endOffset": 158}, {"referenceID": 17, "context": "The retrieval algorithm used by commercial TM systems is typically not disclosed (Koehn and Senellart, 2010; Simard and Fujita, 2012; Whyman and Somers, 1999).", "startOffset": 81, "endOffset": 158}, {"referenceID": 21, "context": "The retrieval algorithm used by commercial TM systems is typically not disclosed (Koehn and Senellart, 2010; Simard and Fujita, 2012; Whyman and Somers, 1999).", "startOffset": 81, "endOffset": 158}, {"referenceID": 1, "context": "However, the bestperforming method used in current systems is widely believed to be based on edit distance (Baldwin and Tanaka, 2000; Simard and Fujita, 2012; Whyman and Somers, 1999; Koehn and Senellart, 2010; Christensen and Schjoldager, 2010; Mandreoli et al., 2006; He et al., 2010).", "startOffset": 107, "endOffset": 286}, {"referenceID": 17, "context": "However, the bestperforming method used in current systems is widely believed to be based on edit distance (Baldwin and Tanaka, 2000; Simard and Fujita, 2012; Whyman and Somers, 1999; Koehn and Senellart, 2010; Christensen and Schjoldager, 2010; Mandreoli et al., 2006; He et al., 2010).", "startOffset": 107, "endOffset": 286}, {"referenceID": 21, "context": "However, the bestperforming method used in current systems is widely believed to be based on edit distance (Baldwin and Tanaka, 2000; Simard and Fujita, 2012; Whyman and Somers, 1999; Koehn and Senellart, 2010; Christensen and Schjoldager, 2010; Mandreoli et al., 2006; He et al., 2010).", "startOffset": 107, "endOffset": 286}, {"referenceID": 11, "context": "However, the bestperforming method used in current systems is widely believed to be based on edit distance (Baldwin and Tanaka, 2000; Simard and Fujita, 2012; Whyman and Somers, 1999; Koehn and Senellart, 2010; Christensen and Schjoldager, 2010; Mandreoli et al., 2006; He et al., 2010).", "startOffset": 107, "endOffset": 286}, {"referenceID": 6, "context": "However, the bestperforming method used in current systems is widely believed to be based on edit distance (Baldwin and Tanaka, 2000; Simard and Fujita, 2012; Whyman and Somers, 1999; Koehn and Senellart, 2010; Christensen and Schjoldager, 2010; Mandreoli et al., 2006; He et al., 2010).", "startOffset": 107, "endOffset": 286}, {"referenceID": 14, "context": "However, the bestperforming method used in current systems is widely believed to be based on edit distance (Baldwin and Tanaka, 2000; Simard and Fujita, 2012; Whyman and Somers, 1999; Koehn and Senellart, 2010; Christensen and Schjoldager, 2010; Mandreoli et al., 2006; He et al., 2010).", "startOffset": 107, "endOffset": 286}, {"referenceID": 9, "context": "However, the bestperforming method used in current systems is widely believed to be based on edit distance (Baldwin and Tanaka, 2000; Simard and Fujita, 2012; Whyman and Somers, 1999; Koehn and Senellart, 2010; Christensen and Schjoldager, 2010; Mandreoli et al., 2006; He et al., 2010).", "startOffset": 107, "endOffset": 286}, {"referenceID": 17, "context": "A limitation of the work of (Simard and Fujita, 2012) was that the evaluation of the performance of the TM similarity algorithms was also conducted using the same MT evaluation metrics.", "startOffset": 28, "endOffset": 53}, {"referenceID": 13, "context": "Our implementation of edit distance (Levenshtein, 1966), computed on a word level, is sim-", "startOffset": 36, "endOffset": 55}, {"referenceID": 11, "context": "ilar to the version defined in (Koehn and Senellart, 2010).", "startOffset": 31, "endOffset": 58}, {"referenceID": 16, "context": "We note that n-gram precision forms a fundamental subcomputation in the computation of the corpus-level MT evaluation metric BLEU score (Papineni et al., 2002).", "startOffset": 136, "endOffset": 159}, {"referenceID": 7, "context": "Since large ngram matches are unlikely on a segment level, using a geometric mean can be a poor method to use for matching on a segment level, as has been described for the related task of MT evaluation (Doddington, 2002; Lavie et al., 2004).", "startOffset": 203, "endOffset": 241}, {"referenceID": 12, "context": "Since large ngram matches are unlikely on a segment level, using a geometric mean can be a poor method to use for matching on a segment level, as has been described for the related task of MT evaluation (Doddington, 2002; Lavie et al., 2004).", "startOffset": 203, "endOffset": 241}, {"referenceID": 7, "context": "Since large ngram matches are unlikely on a segment level, using a geometric mean can be a poor method to use for matching on a segment level, as has been described for the related task of MT evaluation (Doddington, 2002; Lavie et al., 2004). Additionally, for the related task of MT evaluation at a segment level, Lavie et al. (2004) have found that using an arithmetic mean correlates better with human judgments than using a geometric mean.", "startOffset": 204, "endOffset": 335}, {"referenceID": 19, "context": "For Chinese-English experiments, we used the OpenOffice3 (OO3) parallel corpus (Tiedemann, 2009), which is OO3 computer office productivity software documentation.", "startOffset": 79, "endOffset": 96}, {"referenceID": 19, "context": "For French-English experiments, we used the EMEA parallel corpus (Tiedemann, 2009), which are medical documents from the European Medecines Agency.", "startOffset": 65, "endOffset": 82}, {"referenceID": 19, "context": "The corpora were produced by a suite of automated tools as described in (Tiedemann, 2009) and come sentence-aligned.", "startOffset": 72, "endOffset": 89}, {"referenceID": 20, "context": "For Chinese corpora we tokenize each sentence using the Stanford Chinese Word Segmenter (Tseng et al., 2005) with the Chinese Penn Treebank standard (Xia, 2000).", "startOffset": 88, "endOffset": 108}, {"referenceID": 22, "context": ", 2005) with the Chinese Penn Treebank standard (Xia, 2000).", "startOffset": 48, "endOffset": 59}, {"referenceID": 3, "context": "used Amazon Mechanical Turk, which has been used productively in the past for related work in the context of machine translation (Bloodgood and Callison-Burch, 2010b; Bloodgood and CallisonBurch, 2010a; Callison-Burch, 2009).", "startOffset": 129, "endOffset": 224}, {"referenceID": 5, "context": "used Amazon Mechanical Turk, which has been used productively in the past for related work in the context of machine translation (Bloodgood and Callison-Burch, 2010b; Bloodgood and CallisonBurch, 2010a; Callison-Burch, 2009).", "startOffset": 129, "endOffset": 224}], "year": 2015, "abstractText": "Translation Memory (TM) systems are one of the most widely used translation technologies. An important part of TM systems is the matching algorithm that determines what translations get retrieved from the bank of available translations to assist the human translator. Although detailed accounts of the matching algorithms used in commercial systems can\u2019t be found in the literature, it is widely believed that edit distance algorithms are used. This paper investigates and evaluates the use of several matching algorithms, including the edit distance algorithm that is believed to be at the heart of most modern commercial TM systems. This paper presents results showing how well various matching algorithms correlate with human judgments of helpfulness (collected via crowdsourcing with Amazon\u2019s Mechanical Turk). A new algorithm based on weighted n-gram precision that can be adjusted for translator length preferences consistently returns translations judged to be most helpful by translators for multiple domains and language pairs.", "creator": "LaTeX with hyperref package"}}}