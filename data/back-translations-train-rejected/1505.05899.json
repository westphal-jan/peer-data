{"id": "1505.05899", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "The IBM 2015 English Conversational Telephone Speech Recognition System", "abstract": "We describe the latest improvements to the IBM English conversational telephone speech recognition system. Some of the techniques that were found beneficial are: maxout networks with annealed dropout rates; networks with a very large number of outputs trained on 2000 hours of data; joint modeling of partially unfolded recurrent neural networks and convolutional nets by combining the bottleneck and output layers and retraining the resulting model; and lastly, sophisticated language model rescoring with exponential and neural network LMs. These techniques result in an 8.0% word error rate on the Switchboard part of the Hub5-2000 evaluation test set which is 23% relative better than our previous best published result.", "histories": [["v1", "Thu, 21 May 2015 20:49:32 GMT  (35kb,D)", "http://arxiv.org/abs/1505.05899v1", "Submitted to Interspeech 2015"]], "COMMENTS": "Submitted to Interspeech 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["george saon", "hong-kwang j kuo", "steven rennie", "michael picheny"], "accepted": false, "id": "1505.05899"}, "pdf": {"name": "1505.05899.pdf", "metadata": {"source": "CRF", "title": "The IBM 2015 English Conversational Telephone Speech Recognition System", "authors": ["George Saon", "Hong-Kwang J. Kuo", "Steven Rennie", "Michael Picheny"], "emails": ["gsaon@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "Since 2000, we have seen the great precision gains from using deep neural network acoustic models compared to Gauss mixing models, the Switchboard Corpus has become the de facto standard experimental test bed for reporting credible and, more importantly, reproducible results for LVCSR. We suspect that this is the largest publicly available data set (up to 2,300 hours of training data) consisting of truly dialogical statements, and that techniques that lead to improvements on the Switchboard work well on small and large vocabulary tasks. Think of LDA / STC, VTLN, FMLR and grid model-based discriminatory training that was first developed on the Switchboard and then serves as a prime example of such techniques."}, {"heading": "2. General processing", "text": "Here we describe the usual processing steps for all models detailed in this paper. In particular, we discuss front-end processing, loudspeaker matching and specifics in the field of neural network training that are broadly similar to those [14, 13]."}, {"heading": "2.1. Training and test data", "text": "The training data consists of 1975 hours of segmented audio from English telephone conversations between two strangers on a given topic and is divided as follows: 262 hours from data collection from Switchboard 1, 1698 hours from data collection from Fisher and 15 hours from CallHome Audio. The test kit is the Hub5 2000 evaluation kit and consists of two parts: 2.1 hours (21.4 K words, 40 speakers) of switching data and 1.6 hours (21.6 K words, 40 speakers) of CallHome Audio. The decoding vocabulary includes 30.5 K words and 32.9K pronunciations and all decoding was performed using a 4M 4 gram language model (and rescorded with various LMs in Section 3.4)."}, {"heading": "2.2. Feature extraction", "text": "Each image is represented by a feature vector of 13 VTL-distorted perceptual linear prediction coefficients (PLP), which normalize mean and variance per conversation side. All 9 consecutive ceptral frames are spliced together using LDA and projected down to 40 dimensions. The extent of this transformation is further diagonalized using a global semi-bound covariance transformation. Next, the LDA features are transformed using a feature space MLLR (FMLR), per conversation side, both during training and during the test period. Folding networks are trained using VTL-distorted log features, supplemented by first and second temporal derivatives. The Mel filter bank has 40 filters, and input to the CNNs are blocks of 11 consecutive 40 x 3-dimensional frames (as described in [13].ar Xiv: 150 V.05V [899V] components each]."}, {"heading": "2.3. Neural network training", "text": "All models have sigmoid hidden layers and Softmax output layers (with the exception of the models in subsection 3.1) and are trained with 10-15 Epochs SGD on frame-randomized minibatches with 250 images and a cross-entropy criterion. Targets correspond to the context-dependent HMM states achieved by matching the audio with a GMM-HMM system with 300K Gaussians trained with the maximum probability of FMLLR characteristics. The same states are assigned to the sheets of different phonetic decision trees, which differ in telephone context size (\u00b1 2 or \u00b1 3) and the number of sheets (16K, 32K and 64K). Prior to CE training, the networks are initialized with layerwise discriminatory pretraining, as proposed in [1]. Additionally, we have used 20-30 iterations of Hesse-free sequence discriminatory training ST (minimum) as a state-based MBR-risk scenario."}, {"heading": "3. System improvements", "text": "In this section we discuss specific improvements related to acoustic and linguistic modeling. More specifically, we describe the following techniques: Maxout models with annealed dropout (Section 3.1); training DNNs, CNNs and RNNNs with a very large number of outputs (Section 3.2); improved joint training of revolutionary and non-revolutionary networks (Section 3.3); and the rescoring of speech models with exponential and neural network LMs (Section 3.4)."}, {"heading": "3.1. Maxout networks with annealed dropout", "text": "Maxout networks [16] generalize linear (ReLU, max [0, a]) units using nonlinearities of form: sj = max i-C (j) ai (1), where the activations ai = wTi x + bi are, as usual, based on internal products, and the sets of activations {C (j)} used by various hidden units are typically fragmented. Maxout networks are conditionally linear, thus avoiding the problem of disappearing progression, and are well suited to the exit process [17], which forms an exponentially large set of models for a linear model (2D models for input dimension D), the geometric average of which can be calculated by simple renormalization in the test period. Maxout networks for ASR have recently been studied by several researchers and found that significant increases in training are achieved when the group is limited [18], but negligible gains in our personal experience approximately exceed 100 hours."}, {"heading": "3.2. Networks with very large output layers", "text": "It is indeed the case that we are able to get to grips with the problems mentioned in order to solve them."}, {"heading": "3.3. Improved joint training of recurrent and convolutional nets", "text": "In [13], we proposed a method for jointly modelling and training a CNN and a DNN. The crux of the method is that the first layers are network-specific (folding and pooling for CNNoperating on spectral characteristics and an input layer for DNN operating on PLP-based and i-vector characteristics) and the remaining layers are split. Outputs of the network-specific layers are merged into a common hidden layer, followed by additional (common) hidden layers and an initial layer. This graph structure for the common network extends to the standard linear sequence of layers for DNNs (or CNNs). By using this architecture, we reported a relative gain on a switchboard of 300 hours setup on the best single model (from 11.8% for CNN to 10.4% for the common CNN / DNN). We also showed that performing the fusion of a CNN and a DNN is trained separately."}, {"heading": "3.4. Language model", "text": "In experiments comparing acoustic models reported in earlier sections, we used a basic language model already used for previous publications: a 4M 4 gram language model with a vocabulary of 30.5K words. While maintaining the same vocabulary, we rebuilt the LM using publicly available (e.g. LDC) training data, including Switchboard, Fisher, Gigaword, and Broadcast News and Conversations. The most relevant data is the transcripts of the 1975 hour audio data used for the acoustic model, consisting of approximately 24M words. To create the new n gram language model, we trained a 4 gram model with modified Kneser-Ney smoothing [23] for each corpus, and then interpolated the component models linearly with weights selected to optimize perplexity on a presented n gram language model. [We then applied an optimized Neser-Ney polishing pattern to each corpus, with modified Knesh-23]."}, {"heading": "LM WER SWB WER CH", "text": "For the rescoring of LM, we used two types of LMs: Model M, a class-based exponential model [25] and Neural Network LM (NNLM) [26, 27, 28, 29]. We built one model M LM on each corpus and interpolated the models together with the 37M ngram LM. As shown in Table 5, the use of Model M resulted in an improvement of 0.4% on SWB and 1.0% on CallHome. We built two NLMs for interpolation. One was trained on only the most relevant data: the 24M word corpus (control panel / Fisher / CallHome acoustic transcripts). Another was mixed on a 560M word subset of LM training data: In order to accelerate the training for this larger group, we used a hierarchical NLM approach [27, 30]. Table 5 shows that we were compared to the n-gram base line of LM-NM and LM-NM-B, respectively, with 0.8% NM-NM-NM-NM-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N"}, {"heading": "4. Discussion", "text": "We have presented a number of improvements to our English switching system that significantly reduced the error rate compared to our previous best [13]. In decreasing order of importance, these are: recourse to strong voice models trained on different data sources; joint training of an RNN and a CNN with 32,000 outputs on 2000 hours of annealed dropout audio and maxout networks. Additional accuracy gains are expected from the formation of maxout networks and larger CNNs with a 512 / 512 filter configuration across all data. Based on historical trends, we believe that human accuracy can be achieved in this task within the next decade. We think that getting there will most likely require an increase of several orders of magnitude in training data and the use of more complex neural network architectures that tightly integrate multiple sources of knowledge (acoustics, language, pragmatics, etc.)."}, {"heading": "5. Ackowledgment", "text": "The authors thank the following current and former IBM colleagues: H. Soltau, D. Povey, S. Chen, A. Emami, V. Goel, B. Kingsbury, L. Mangu, B. Ramabhadran, T. Sainath and G. Zweig for their significant contributions to the switching system."}, {"heading": "6. References", "text": "[1] F. Seide, G. Li, X. Chien, and D. Yu 61, \"Feature engineeringin context-dependent deep neural networks for conversations speech transcription,\" in Proc. ASRU, 2011. [2] R. P. Lippmann, \"Speech recognition by machines and people,\" Speech communication, vol. 22, no. 1, pp. 1-15, 1997. [3] F. Liu, M. Monkowski, M. Picheny, and P. Rao, \"Performance of the IBM LVCSRsystem on the Switchboard, pp.,\" in Proceedings of Speech Research Symposium, 1995, p. 189. [4] T. Hain, P. Woodland, G. Evermann, and D. Povey, \"The CU-HTK march 2000 HUB5E transcription system,\" in Proc. Speech. Speech Research Symposium, vol. 1. Baltimore, 2000."}], "references": [{"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G. Li", "X. Chien", "D. Yu"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition by machines and humans", "author": ["R.P. Lippmann"], "venue": "Speech communication, vol. 22, no. 1, pp. 1\u201315, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Performance of the IBM LVCSR  system on the Switchboard corpus", "author": ["F. Liu", "M. Monkowski", "M. Novak", "M. Padmanabhan", "M. Picheny", "P. Rao"], "venue": "Proceedings of Speech Research Symposium, 1995, p. 189.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "The CU-HTK march 2000 HUB5E transcription system", "author": ["T. Hain", "P. Woodland", "G. Evermann", "D. Povey"], "venue": "Proc. Speech Transcription Workshop, vol. 1. Baltimore, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "The IBM 2004 conversational telephony system for rich transcription.", "author": ["H. Soltau", "B. Kingsbury", "L. Mangu", "D. Povey", "G. Saon", "G. Zweig"], "venue": "in Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "The IBM Attila speech recognition toolkit", "author": ["H. Soltau", "G. Saon", "B. Kingsbury"], "venue": "Proc. of IEEE Workshop on Spoken Language Technology (SLT), 2010, pp. 97\u2013102.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "fMPE: Discriminatively trained features for speech recognition", "author": ["D. Povey", "B. Kingsbury", "L. Mangu", "G. Saon", "H. Soltau", "G. Zweig"], "venue": "Proc. of ICASSP, 2005, pp. 961\u2013964.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesely", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proc. Interspeech, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "1-bit stochastic gradient descent and its application to dataparallel distributed training of speech dnns", "author": ["F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepspeech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence training of multiple deep neural networks for better performance and faster training speed", "author": ["P. Zhou", "L. Dai", "H. Jiang"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5627\u20135631.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Increasing deep neural network acoustic model size for large vocabulary continuous speech recognition", "author": ["A.L. Maas", "A.Y. Hannun", "C.T. Lengerich", "P. Qi", "D. Jurafsky", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1406.7806, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint training of convolutional and non-convolutional neural networks", "author": ["H. Soltau", "G. Saon", "T.N. Sainath"], "venue": "to Proc. ICASSP, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization", "author": ["B. Kingsbury", "T. Sainath", "H. Soltau"], "venue": "Proc. Interspeech, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1302.4389, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1929}, {"title": "Improving deep neural network acoustic models using generalized maxout networks", "author": ["X. Zhang", "J. Trmal", "D. Povey", "S. Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 215\u2013219.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Annealed dropout training of deep networks", "author": ["S. Rennie", "V. Goel", "S. Thomas"], "venue": "Spoken Language Technology (SLT), IEEE Workshop on. IEEE, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "Proc. of ICASSP, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "A.-r. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8614\u20138618.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Unfolded recurrent neural networks for speech recognition", "author": ["G. Saon", "H. Soltau", "A. Emami", "M. Picheny"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech & Language, vol. 13, no. 4, pp. 359\u2013393, 1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Entropy-based pruning of backoff language models", "author": ["A. Stolcke"], "venue": "Proc. DARPA Broadcast News Transcription and Understanding Workshop, 1998, pp. 270\u2013274.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Shrinking exponential language models", "author": ["S.F. Chen"], "venue": "Proc. NAACL-HLT, 2009, pp. 468\u2013476.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "A neural syntactic language model", "author": ["A. Emami"], "venue": "Ph.D. dissertation, Johns Hopkins University, Baltimore, MD, USA, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Continuous space language models", "author": ["H. Schwenk"], "venue": "Computer Speech & Language, vol. 21, no. 3, pp. 492\u2013518, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Empirical study of neural network language models for Arabic speech recognition", "author": ["A. Emami", "L. Mangu"], "venue": "Proc. ASRU, 2007, pp. 147\u2013152.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Large scale hierarchical neural network language models", "author": ["H.-K.J. Kuo", "E. Ar\u0131soy", "A. Emami", "P. Vozila"], "venue": "Proc. Interspeech, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Ever since [1] demonstrated the large accuracy gains from using deep neural network acoustic models versus Gaussian mixture models, the Switchboard corpus has become the de facto standard experimental testbed for reporting believable and, more importantly, reproducible results for LVCSR.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "To set the baseline, the human word error rate on this task is estimated to be around 4% [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Quoting [2] again, in 1995, \u201ca high-performance HMM recognizer\u201d achieved a 43% WER on Switchboard [3].", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "Quoting [2] again, in 1995, \u201ca high-performance HMM recognizer\u201d achieved a 43% WER on Switchboard [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "3% during the Hub5e DARPA evaluation [4] which they attributed to \u201ccareful engineering\u201d.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "2% WER during the 2004 DARPA EARS Rich Transcription evaluation [5] largely due to the Attila ASR toolkit [6] and fMPE [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "2% WER during the 2004 DARPA EARS Rich Transcription evaluation [5] largely due to the Attila ASR toolkit [6] and fMPE [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "2% WER during the 2004 DARPA EARS Rich Transcription evaluation [5] largely due to the Attila ASR toolkit [6] and fMPE [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "Nowadays, deep neural networks have levelled the playing field and multiple sites can easily reach 12-14% WER using much simpler systems [8, 9, 10, 11, 12] as shown in Table 6.", "startOffset": 137, "endOffset": 155}, {"referenceID": 8, "context": "Nowadays, deep neural networks have levelled the playing field and multiple sites can easily reach 12-14% WER using much simpler systems [8, 9, 10, 11, 12] as shown in Table 6.", "startOffset": 137, "endOffset": 155}, {"referenceID": 9, "context": "Nowadays, deep neural networks have levelled the playing field and multiple sites can easily reach 12-14% WER using much simpler systems [8, 9, 10, 11, 12] as shown in Table 6.", "startOffset": 137, "endOffset": 155}, {"referenceID": 10, "context": "Nowadays, deep neural networks have levelled the playing field and multiple sites can easily reach 12-14% WER using much simpler systems [8, 9, 10, 11, 12] as shown in Table 6.", "startOffset": 137, "endOffset": 155}, {"referenceID": 11, "context": "Nowadays, deep neural networks have levelled the playing field and multiple sites can easily reach 12-14% WER using much simpler systems [8, 9, 10, 11, 12] as shown in Table 6.", "startOffset": 137, "endOffset": 155}, {"referenceID": 5, "context": "Last but not least, it is our experience that having a strong GMM-HMM baseline system [6, 13] which provides high-quality alignments used for the various speaker adaptation techniques and for DNN crossentropy training helps.", "startOffset": 86, "endOffset": 93}, {"referenceID": 12, "context": "Last but not least, it is our experience that having a strong GMM-HMM baseline system [6, 13] which provides high-quality alignments used for the various speaker adaptation techniques and for DNN crossentropy training helps.", "startOffset": 86, "endOffset": 93}, {"referenceID": 13, "context": "In particular, we discuss frontend processing, speaker adaptation and neural network training specifics which are largely similar to [14, 13].", "startOffset": 133, "endOffset": 141}, {"referenceID": 12, "context": "In particular, we discuss frontend processing, speaker adaptation and neural network training specifics which are largely similar to [14, 13].", "startOffset": 133, "endOffset": 141}, {"referenceID": 12, "context": "The Mel filterbank has 40 filters and the input to the CNNs are blocks of 11 consecutive 40\u00d73-dimensional frames (as described in [13]).", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "In addition to VTLN and FMLLR, DNNs are adapted to the speaker by appending 100-dimensional i-vectors to every block of 11 FMLLR frames as described in [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "Prior to CE training, the networks are initialized with layerwise discriminative pretraining as suggested in [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 14, "context": "Additionally, we applied 20-30 iterations of hessian-free sequence discriminative training (ST) by using the state-based minimum Bayes risk (MBR) objective function as described in [15].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "Maxout networks [16] generalize rectified linear (ReLU, max[0, a]) units, employing non-linearities of the form:", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "Maxout networks are conditionally linear and so avoid the vanishing gradient problem, and are well suited for the dropout training procedure [17], which for a linear model, trains an exponentially sized model ensemble (2 models for input dimension D), whose geometric average can be computed by simply renormalizing at test time.", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "Maxout networks for ASR have recently been investigated by several researchers, and found to produce significant gains when training data is limited [18], but negligible gains in our personal experience when the amount of training data exceeds approximately 100 hours.", "startOffset": 149, "endOffset": 153}, {"referenceID": 18, "context": "However, recently we showed that by annealing the dropout rate over the course of training, Maxout networks can produce substantial gains, even in big data scenarios [19].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "Table 1 compares the performance of our annealed dropout Maxout networks (Maxout-AD) to corresponding sigmoidbased DNNs and CNNs from [13] learned using our standard training procedure, using only the SWB-1 training data (262 hours).", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "To keep the computation and the number of parameters in check, we also had to use a bottleneck layer before the output layer [20] with typically 512 neurons.", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "Back in the days when we were training GMM-based acoustic models, we did not notice accuracy improvements when using more than, say, 10000 HMM states [5].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "This was a distinct feature of our EARS RT\u201904 evaluation system which made a significant difference [5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 20, "context": "The convolution and pooling layer configuration is taken from [21] and the architecture is also shown on the left side of Figure 1.", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "\u2022 Partially unfolded recurrent neural networks [22] which operate on a sliding window of 6 40-dimensional FMLLR frames (from t .", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "All nets are trained with 10-15 passes of cross-entropy on 2000 hours of audio and 30 iterations of sequence discriminative training using Hessian-free optimization [15].", "startOffset": 165, "endOffset": 169}, {"referenceID": 12, "context": "In [13], we proposed a method for jointly modeling and training a CNN and a DNN.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Hence, the main benefit of the joint model in [13] over the score fusion approach is the shared computation for the common hidden and output layers which is considerably faster than having to do two separate forward passes.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "For both scenarios we generate numerator and denominator lattices with the initial joint model and optimize the lattice-based MBR loss using distributed hessian-free training [15].", "startOffset": 175, "endOffset": 179}, {"referenceID": 22, "context": "To build the new n-gram language model, we trained a 4gram model with modified Kneser-Ney smoothing [23] for each corpus, and then linearly interpolated the component models with weights chosen to optimize perplexity on a held-out set.", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "Then we applied entropy pruning [24], resulting in a single 4gram LM consisting of 37M n-grams.", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "For LM rescoring, we used two types of LMs: model M, a class-based exponential model [25] and neural network LM (NNLM) [26, 27, 28, 29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "For LM rescoring, we used two types of LMs: model M, a class-based exponential model [25] and neural network LM (NNLM) [26, 27, 28, 29].", "startOffset": 119, "endOffset": 135}, {"referenceID": 26, "context": "For LM rescoring, we used two types of LMs: model M, a class-based exponential model [25] and neural network LM (NNLM) [26, 27, 28, 29].", "startOffset": 119, "endOffset": 135}, {"referenceID": 27, "context": "For LM rescoring, we used two types of LMs: model M, a class-based exponential model [25] and neural network LM (NNLM) [26, 27, 28, 29].", "startOffset": 119, "endOffset": 135}, {"referenceID": 28, "context": "For LM rescoring, we used two types of LMs: model M, a class-based exponential model [25] and neural network LM (NNLM) [26, 27, 28, 29].", "startOffset": 119, "endOffset": 135}, {"referenceID": 26, "context": "Another was trained on a 560M word subset of the LM training data: in order to speed up training for this larger set, we employed a hierarchical NNLM approximation [27, 30].", "startOffset": 164, "endOffset": 172}, {"referenceID": 29, "context": "Another was trained on a 560M word subset of the LM training data: in order to speed up training for this larger set, we employed a hierarchical NNLM approximation [27, 30].", "startOffset": 164, "endOffset": 172}, {"referenceID": 7, "context": "[8] SWB 12.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] SWB+Fisher+other 13.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] SWB+Fisher 12.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] SWB 14.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] SWB 14.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] SWB+Fisher 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] SWB 10.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "1% CallHome WER is not reported in [13]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "We have presented a set of improvements to our English Switchboard system that lowered the error rate substantially compared to our previous best result [13].", "startOffset": 153, "endOffset": 157}], "year": 2015, "abstractText": "We describe the latest improvements to the IBM English conversational telephone speech recognition system. Some of the techniques that were found beneficial are: maxout networks with annealed dropout rates; networks with a very large number of outputs trained on 2000 hours of data; joint modeling of partially unfolded recurrent neural networks and convolutional nets by combining the bottleneck and output layers and retraining the resulting model; and lastly, sophisticated language model rescoring with exponential and neural network LMs. These techniques result in an 8.0% word error rate on the Switchboard part of the Hub5-2000 evaluation test set which is 23% relative better than our previous best published result.", "creator": "LaTeX with hyperref package"}}}