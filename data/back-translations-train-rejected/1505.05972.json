{"id": "1505.05972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2015", "title": "Instant Learning: Parallel Deep Neural Networks and Convolutional Bootstrapping", "abstract": "Although deep neural networks (DNN) are able to scale with direct advances in computational power (e.g., memory and processing speed), they are not well suited to exploit the recent trends for parallel architectures. In particular, gradient descent is a sequential process and the resulting serial dependencies mean that DNN training cannot be parallelized effectively. Here, we show that a DNN may be replicated over a massive parallel architecture and used to provide a cumulative sampling of local solution space which results in rapid and robust learning. We introduce a complimentary convolutional bootstrapping approach that enhances performance of the parallel architecture further. Our parallelized convolutional bootstrapping DNN out-performs an identical fully-trained traditional DNN after only a single iteration of training.", "histories": [["v1", "Fri, 22 May 2015 07:24:14 GMT  (336kb)", "http://arxiv.org/abs/1505.05972v1", null], ["v2", "Sat, 21 May 2016 09:33:33 GMT  (262kb)", "http://arxiv.org/abs/1505.05972v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1505.05972"}, "pdf": {"name": "1505.05972.pdf", "metadata": {"source": "CRF", "title": "Instant Learning: Parallel Deep Neural Networks and Convolutional Bootstrapping", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "This year, we have reached the point where we are able to live in a country where it is not a country, but a country where it is a country."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["GE Hinton", "S Osindero", "Y Teh"], "venue": "Neural Computation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Deep big simple neural nets excel on handwritten digit recognition", "author": ["Ciresan C. D", "U. Meier", "M. Gambardella L", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "The Loss Surface of Multilayer Networks\u201d, arXiv preprint arXiv:1412.0233", "author": ["A Choromanska", "M Henaff", "M Mathieu", "GB Arous", "Y LeCun"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arXiv.org abs/1502.04042", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of  feature detectors,", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": "The Computing Research Repository (CoRR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["GE Dahl", "TN Sainath", "GE Hinton"], "venue": "in Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Over-Sampling in a Deep Neural Network\u201d, arXiv.org abs/1502.03648", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "process of back-propagated gradient descent [1]-[4].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "process of back-propagated gradient descent [1]-[4].", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": "DNNs present a non-convex optimization problem and hence the ultimate local solution depends upon the random starting weights [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "It has been demonstrated that the various local solutions, that may be sampled in this way, are typically equivalent [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": ", such as \u2018data augmentation\u2019 [4]) would lead to further enhancements via the cumulative sampling process.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "Within the context of the well-known MNIST [3] hand-written digit classification problem, we replicate Hinton\u2019s original MNIST DNN [1] a large number of times and train each from different starting weights.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "Within the context of the well-known MNIST [3] hand-written digit classification problem, we replicate Hinton\u2019s original MNIST DNN [1] a large number of times and train each from different starting weights.", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "classification using the MNIST dataset [3], [1], [4].", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "classification using the MNIST dataset [3], [1], [4].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "classification using the MNIST dataset [3], [1], [4].", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Hinton\u2019s [1] architecture, but using the biased sigmoid activation function [6], we built a fully connected network of", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "Hinton\u2019s [1] architecture, but using the biased sigmoid activation function [6], we built a fully connected network of", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "on the same 60,000 training examples from the MNIST dataset [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "This is a form of \u2018data augmentation\u2019 [4].", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "However, unlike data augmentation approaches featuring arbitrary transforms [4] or assumptions about the representation of the data, the described convolutional bootstrapping can be applied without prior knowledge or assumptions about the nature and/or structure of the data.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "It remains to be seen how techniques such as dropout [7], [8] and interpretations from sampling theory [9] might be related or integrated.", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "It remains to be seen how techniques such as dropout [7], [8] and interpretations from sampling theory [9] might be related or integrated.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "It remains to be seen how techniques such as dropout [7], [8] and interpretations from sampling theory [9] might be related or integrated.", "startOffset": 103, "endOffset": 106}], "year": 2015, "abstractText": "Although deep neural networks (DNN) are able to scale with direct advances in computational power (e.g., memory and processing speed), they are not well suited to exploit the recent trends for parallel architectures. In particular, gradient descent is a sequential process and the resulting serial dependencies mean that DNN training cannot be parallelized effectively. Here, we show that a DNN may be replicated over a massive parallel architecture and used to provide a cumulative sampling of local solution space which results in rapid and robust learning. We introduce a complimentary convolutional bootstrapping approach that enhances performance of the parallel architecture further. Our parallelized convolutional bootstrapping DNN out-performs an identical fully-trained traditional DNN after only a single iteration of training.", "creator": "PDFCreator Version 1.7.1"}}}