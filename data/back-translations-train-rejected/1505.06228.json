{"id": "1505.06228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2015", "title": "Keyphrase Based Evaluation of Automatic Text Summarization", "abstract": "The development of methods to deal with the informative contents of the text units in the matching process is a major challenge in automatic summary evaluation systems that use fixed n-gram matching. The limitation causes inaccurate matching between units in a peer and reference summaries. The present study introduces a new Keyphrase based Summary Evaluator KpEval for evaluating automatic summaries. The KpEval relies on the keyphrases since they convey the most important concepts of a text. In the evaluation process, the keyphrases are used in their lemma form as the matching text unit. The system was applied to evaluate different summaries of Arabic multi-document data set presented at TAC2011. The results showed that the new evaluation technique correlates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4, and AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENG MeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667 respectively.", "histories": [["v1", "Fri, 22 May 2015 21:12:35 GMT  (461kb)", "http://arxiv.org/abs/1505.06228v1", "4 pages, 1 figure, 3 tables"]], "COMMENTS": "4 pages, 1 figure, 3 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fatma elghannam", "tarek el-shishtawy"], "accepted": false, "id": "1505.06228"}, "pdf": {"name": "1505.06228.pdf", "metadata": {"source": "META", "title": "Keyphrase based Evaluation of Automatic Text Summarization", "authors": ["Fatma Elghannam", "Tarek El-Shishtawy"], "emails": [], "sections": [{"heading": null, "text": "The present study introduces a new Keyphrase-based Summary Evaluator (KpEval) for the evaluation of automatic summaries. KpEval relies on the keyphrases because they convey the most important concepts of a text. In the evaluation process, the keyphrases in their term form are used as a suitable text unit. KpEval has been used to evaluate various summaries of Arabic multi-document sets presented at TAC2011. Results showed that the new evaluation technology correlates well with the known evaluation systems: Rouge-1, Rouge-2, Rouge-SU4 and AutoSummENG-MeMoG. KpEval has the strongest correlation with AutoSummENG-MeMoG, Pearson and Spearman keyphrase correlation coefficient measures."}, {"heading": "1. INTRODUCTION", "text": "Evaluating automated text summaries using human evaluators requires costly human effort, and this hard, expensive effort has hampered researchers in their search for methods to automatically evaluate summaries. Current automated methods compare fragments of the summary to be evaluated in the evaluation process with one or more reference summaries (typically produced by humans) and measure how many fragments in the reference summary are present in the created summary. One method is to view the sentence as a fragmented text unit in the evaluation process, but the problem is that these sentences contain many individual pieces of information that cannot be used by humans for reference summaries. Selecting an appropriate fragment length and appropriate comparison is a critical problem in the evaluation process. The problem is to extract the matching units that express the informative content of a text. Misleading the informative content of a text results in an unfortunate matching between two parts of text in a comparison and reference summary."}, {"heading": "2. PREVIOUS WORKS", "text": "It is not only a question of the expression, but also a question of the expression, the expression, the expression, the expression, the expression, the expression, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the language, the word, the word, the word, the word, the word, the language, the word, the language, the language, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the language, the word, the language, the word, the word, the word, the word, the word, the word, the word, the word, the language, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the language, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the"}, {"heading": "3. KEYPHRASE BASED SUMMARY EVALUATOR", "text": "One problem with methods that use a defined n-gram matching is that they are based only on characteristics at the level of peer candidates, and the absence of deep characteristics that express the informative content of the matching units [10]. Neglecting the linguistic characteristics in the matching units misleads the matching process. We define two major types of errors that can occur between the found units in a summary of peer and reference data: 1) sub-matching, in which non-identical formats covering the same concept are considered to be non-identical units. This problem can occur at all levels of the NLP (lexical, syntactic and semantic). For example, the units (educational levels, educational levels, educational levels) convey the same concepts, but with different syntactical structures or synonyms. Recognition of this problem requires different NLP analytical levels 2) Over-matching of the units found in the peer summary, no overmatch of the units found in the peer summary."}, {"heading": "3.1 Features of the Keyphrase Based Summary Evaluator", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3.2 Steps of the Keyphrase Based Summary Evaluator", "text": "The KpEval process consists of the following steps: 1. Extract indicative keyphrases at the Lemma level using the LBAKE module for both peer sums and reference sums; 2. Count the matching keyphrase lemma forms that occur between the peer summary and each of the reference sums; 3. Calculate precision, callback, and F-measure to measure peer summary performance."}, {"heading": "3.2.1 Keyphrase Extraction", "text": "The first step is to follow the peer-and-reference summaries of the keyphrase extractor LBAKE to extract the indicative keyphrases at the level. LBAKE is an accompanying learning module for extracting keyphrases of individual Arabic documents. It is based on three main steps: linguistic processing, candidate phrase extraction, and functionality vector calculation. LBAKE starts by splitting the text into words and extracting their lexical and syntactic properties [1], and then extracts the keyphrases in their form for peer and reference summaries. The extractor is supplied with linguistic knowledge and statistical information. Any possible phrases of one, two, or three consecutive words that occur in a given document are generated as n-gram units. These n-gram words are accepted as keyphrases if they follow the following tactical ones."}, {"heading": "3.2.2 Precision, Recall, and F measure calculation", "text": "The KpEval technique is based on the evaluation of the precision, callback and F-measurement between the reference systems and the reference systems using the extracted key phrases. For a peer summary, the number of overlapping key phrases is calculated with each of the reference summaries. P, recall R and F-measurements are then evaluated using the following formulas [7]: skpummariesreferencessskpmatchummariesreferencessKPrefcountKPcountRecall) (_) () (skpskpskpmatchummariesreferencesreferencesnoKPsyscountKPcountprecision _) (2RPRP F Where) (_ KPrefcount,) (_ KPsyscount is the number of key phrases in their Lemma form in the reference summary KPsyscountcountcountmaximtKPprecision (KP) compared to the number of other (KpEval) summaries."}, {"heading": "4.1 Data Set", "text": "The well-known TAC 2011 MultiLing Pilot 2011 Dataset1 is used; the package contains all data set files related to the MultiLing 2011 Pilot. The data includes the peer summaries, human summaries and results of Rouge-1, Rouge-2, RougeSU4 and AutoSummENG-MeMoG evaluation results. The data set is available in 7 languages, including Arabic. We apply our test to Arabic documents. In addition to the two base systems, there were seven participants (peers) for the Arabic language, giving a total of nine results. Source texts contain ten collections of related news and newspaper articles. Each collection contains ten related articles. The MultiLing task requires the participant to produce a single summary for each collection. Human summaries contain three human (reference) summaries for each collection."}, {"heading": "4.2 Evaluation Results", "text": "Table 2 illustrates KpEval's average evaluation results for the summarization systems participating in the 2011 TAC. We compared our summary performance results with four other systems: Rouge-1, Rouge-2, Rouge-SU4 and AutoSummENG-MeMoG. Pearson's correlation coefficient is used to measure match with the results, and the Spearman coefficient to measure correlation with the rankings. Results showed that KpEval correlates with the other four techniques; MeMoG has the strongest correlation with KpEval in both measures (0.8840 and 0.9667), as shown in Table 3.8ID8 0.26476ID9 0.23494 Figure 1."}, {"heading": "5. CONCLUSION AND FUTURE WORK", "text": "In this paper, we have introduced a keyphrase-based evaluation system KpEval for the evaluation of automatic summaries. KpEval is based on counting the matching keyphrase-Lemma form of the summary to be evaluated on the basis of reference summaries. KpEval has three main steps: i) extracting the keyphrases for peer and reference summaries ii) counting the matching keyphrases that occur between the peer and each of the reference summaries, and iii) calculating precision, retrieval, and F-measure. To measure the validity of the new system, Peare and Spearman correlation coefficient measurements were tested between the results of KpEval and other evaluation systems: Rouge-1, Rouge-2, Rouge-SU4, and MeMoG using the data set TAC 2011. Results showed that KpEval correlates with the four techniques, Me88Spearman has the strongest correlation for the other languages, and Me88Spearman, respectively, and Pearson with the strongest correlation for 6val, in this paper."}], "references": [{"title": "An Accurate Arabic Root-Based Lemmatizer for Information Retrieval Purposes", "author": ["T. El-Shishtawy", "F. El-Ghannam"], "venue": "International Journal of Computer Science Issues (IJCSI),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Keyphrase based Arabic summarizer (KPAS)", "author": ["T. El-Shishtawy", "El-Ghannam", "May"], "venue": "Informatics and Systems (INFOS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Summarization system evaluation revisited: N-gram graphs", "author": ["G. Giannakopoulos", "V. Karkaletsis", "G. Vouros", "P. Stamatopoulos"], "venue": "ACM Transactions on Speech and Language Processing (TSLP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Evaluating duc 2005 using basic elements", "author": ["E. Hovy", "C.Y. Lin", "L. Zhou"], "venue": "In Proceedings of DUC (Vol", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "July. Rouge: A package for automatic evaluation of summaries", "author": ["C.Y. Lin"], "venue": "In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop (pp. 74-81)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "July. BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics (pp. 311-318)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Summarization evaluation using transformed basic elements. In Proceedings of the 1 Text Analysis Conference", "author": ["S. Tratz", "E. Hovy"], "venue": "MeMoG KP-Eval IJCATM : www.ijcaonline.org", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "Bleu (Bilingual Evaluation Understudy) [9] is an ngram precision based evaluator metric initially designed for the evaluation of machine translation.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "ROUGE [9] stands for RecallOriented Understudy for Gisting Evaluation is a recall measure that counts the number of overlapping n-gram units between the peer summary generated by computer and several reference summaries.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "AutoSummENG\u2013MeMoG (MeMoG) [3] is a summarization", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "[4] developed (BE) method, BE is a very small syntactic unit of content.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "KEYPHRASE BASED SUMMARY EVALUATOR A problem with methods using fixed n-gram matching is that they rely only on surface-level features, and the nonexistence of deep features that express the informative contents of the matching units [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 1, "context": "We adopted the existing lemma based keyphrase extractor LBAKE module [2] which is based on statistical techniques in addition to linguistic knowledge to extract the candidate keyphrases.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "[4] extracts the basic elements (BE) \u2013 which are used in the matching process- based on the words syntactic feature.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "It starts by breaking the text into words and extracting their lemma forms and the associated lexical and syntactic features using the Arabic Lemmatizer [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "Precision P, recall R, and F-measure are then evaluated using the following formulas [7]:", "startOffset": 85, "endOffset": 88}], "year": 2015, "abstractText": "The development of methods to deal with the informative contents of the text units in the matching process is a major challenge in automatic summary evaluation systems that use fixed n-gram matching. The limitation causes inaccurate matching between units in a peer and reference summaries. The present study introduces a new Keyphrase based Summary Evaluator (KpEval) for evaluating automatic summaries. The KpEval relies on the keyphrases since they convey the most important concepts of a text. In the evaluation process, the keyphrases are used in their lemma form as the matching text unit. The system was applied to evaluate different summaries of Arabic multi-document data set presented at TAC2011. The results showed that the new evaluation technique correlates well with the known evaluation systems: Rouge-1, Rouge-2, Rouge-SU4, and AutoSummENG\u2013MeMoG. KpEval has the strongest correlation with AutoSummENG\u2013MeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667 respectively. General Terms Automatic summary evaluation, Automatic summarization, Keyphrase extraction, Natural language processing, computational linguistics, Information retrieval.", "creator": "Microsoft\u00ae Office Word 2007"}}}