{"id": "1505.06378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2015", "title": "Monotonic Calibrated Interpolated Look-Up Tables", "abstract": "Real-world machine learning applications may require functions that are fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of the learned function can be critical to user trust. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to train monotonic look-up tables by solving a convex problem with appropriate linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and propose random sampling of additive regularizer terms. Experiments on seven real-world problems with five to sixteen features and thousands to millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy on practical problems while providing greater transparency to users.", "histories": [["v1", "Sat, 23 May 2015 20:57:58 GMT  (1000kb,D)", "https://arxiv.org/abs/1505.06378v1", null], ["v2", "Tue, 3 Nov 2015 21:49:53 GMT  (1007kb,D)", "http://arxiv.org/abs/1505.06378v2", "To appear (with minor revisions), Journal Machine Learning Research 2016"], ["v3", "Wed, 20 Jan 2016 22:54:21 GMT  (1300kb,D)", "http://arxiv.org/abs/1505.06378v3", "To appear (with minor revisions), Journal Machine Learning Research 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maya gupta", "rew cotter", "jan pfeifer", "konstantin voevodski", "kevin canini", "alexander mangylov", "wojtek moczydlowski", "alex van esbroeck"], "accepted": false, "id": "1505.06378"}, "pdf": {"name": "1505.06378.pdf", "metadata": {"source": "CRF", "title": "Monotonic Calibrated Interpolated Look-Up Tables", "authors": ["Maya Gupta", "Andrew Cotter", "Jan Pfeifer", "Konstantin Voevodski", "Kevin Canini", "Wojciech Moczydlowski", "Alexander van Esbroeck"], "emails": ["mayagupta@google.com", "acotter@google.com", "janpf@google.com", "kvodski@google.com", "canini@google.com", "amangy@google.com", "wojtekm@google.com", "alexve@google.com"], "sections": [{"heading": null, "text": "Keywords: interpretation, interpolation, reference tables, monotony"}, {"heading": "1. Introduction", "text": "We have found that an important question of interpretation in practice is whether the learned model can be guaranteed to be monotonous in terms of some characteristics. For example, we assume that the goal is to estimate the value of a used car, and one of the characteristics is the number of kilometers it has driven. If all other characteristics are driven in practice, we expect the value of the Thear Xiv to be fixed: 150 5.06 378v 3 [cs.L used car, so as never to increase the number of kilometers driven. But, a model that we have learned from a small group of noisy samples, in fact, we do not respect this prior knowledge."}, {"heading": "2. Related Work", "text": "We give a brief overview of related work in the field of interpretable machine learning and then give an overview of related work in the field of learning monotonous functions."}, {"heading": "2.1 Related Work in Interpretable Machine Learning", "text": "Two key themes of the previous work on interpretable machine learning are (i) interpretable function classes and (ii) the preference for simpler functions within a function class."}, {"heading": "2.1.1 Interpretable Function Classes", "text": "Similarly, linear models form an interpretable function class, in that the parameters dictate the relative meaning of each attribute. Linear approaches can be generalized to summarize nonlinear components, as in generalized additive models (Hastie and Tibshirani, 1990) and some core methods, while retaining some of their interpretable aspects. Interpolated reference tables can be interpreted in that the function parameters are the reference values and are therefore semantically significant: they are only examples of the results of the function that are regularly stored in the domain. In the face of two reference tables with the same structure and characteristics, one can analyze how their functions differ by analyzing how the reference table parameters differ. If the examples that change help \"which parameters such as training change.\""}, {"heading": "2.1.2 Prefer Simpler Functions", "text": "Another area of work focuses on the selection of simpler functions within a function class, thereby optimizing a goal of form: minimizing empirical errors and maximizing simplicity, with simplicity usually defined as a manifestation of Occam's Razor or a variant of Kolmogorov complexity. Thus, Ishibuchi and Nojima (2007) minimize the number of blurred rules in a set of rules, Osei-Bryson (2007) clips a decision tree for interpretability, Ra \u00bc tsch et al. (2006) finds a sparse convex combination of cores for a multi-core support vector machine, and Nock (2002) favors smaller committees of ensemble classifiers. Similarly, Garcia et al. (2009) measure the interpretability of rule-based classifiers in terms of the number of rules used and the number of features used. More generally, such a category of interpretability is based on simplicity, as the modalities and simplification."}, {"heading": "2.2 Related Work in Monotonic Functions", "text": "A function f (x) increases monotonously with respect to the features d if f (xi) \u2265 f (xj) for any two feature vectors xi, xj, and RD, where xi [d] \u2265 xj [d] and xi [m] = xj [m] for m 6 =. A number of approaches have been proposed to enforce and promote monotonicity in machine learning, the computational complexity of these algorthims tends to be high, and most methods scale poorly in the number of features D and samples n, as in Table 1. We describe the related work in the following sections, which are organized according to the type of machine learning, but these methods could instead be organized by strategy that falls largely into one of four categories: 1. Restrict a more flexible function class to monotonous, such as linear functions with positive coefficients or a sigmoidal neural network with positive weights."}, {"heading": "2.2.1 Monotonic Linear and Polynomial Functions", "text": "Linear functions can easily be forced to be monotonous in certain inputs by requiring that the corresponding slope coefficients do not have to be negative, but linear functions are not flexible enough for many problems. Polynomial functions (synonymous, linear functions with predefined crosses of characteristics) can also easily be forced to be monotonous by requiring that all coefficients must be positive. However, this is only a sufficient and unnecessary condition: there are monotonous polynomials whose coefficients are not all positive. Consider, for example, the simple case of second-degree multilinear polynomials defined above unit f: [0, 1] 2 \u2192 R so that: f (x) = a0 + a1x [0] + a3x [0] x [1]. (1) The forcing of the derivatives to the positivity of domain x [0, 1] 2, shows that the complete set of monotonic functions is the form (monoton)."}, {"heading": "2.2.2 Monotonic Splines", "text": "In this work, we expand the grid regression, a spline method with fixed nodes on a regular grid and a linear core (Garcia et al., 2012), to be monotonous. There were a number of suggestions to learn monotonous one-dimensional splines. For example, Shively et al. (2009), building on Ramsay (1998), parameterized the set of all smooth and strictly monotonous one-dimensional functions using an integrated exponential form f (x) = a + 0 eb + u (t) dt, and showed better performance than the monotonous function estimators of Neelon and Dunson (2004) and Holmes and Heard (2003) for smooth functions. In other related spline works, Villalobos and Wahba (1987) looked at smooth splines with linear inequality limitations but did not treat monotonics."}, {"heading": "2.2.3 Monotonic Decision Trees and Forests:", "text": "For deeper or wider trees, however, all pairs of leaves must be checked to check for monotony (Potharst and Feelders, 2002b). Non-monotonous trees can be pruned to monotonous trees by various strategies that iteratively reduce the non-monotonous branches (Ben-David, 1992; Potharst and Feelders, 2002b). Monotonicity can also be promoted during construction by punishing the splitting criterion to reduce the number of non-monotonous leaves that would cause splitting (Ben-David, 1995). Potharst and Feelders (2002a) achieved completely flexible monotonous trees with a strategy similar to that of bogosort (Gruber et al., 2007): train many trees on different random subsets of training samples and then select one that is monoton.7"}, {"heading": "2.2.4 Monotonic Support Vector Machines", "text": "With a linear core, it may be easy to verify and enforce the monotonicity of support vector machines, but for nonlinear cores it will be a greater challenge. Lauer and Bloch (2008) encouraged support vector machines to be more monotonous by limiting the derivation of function in the training samples. Riihima \u00bfki and Vehtari (2010) used the same strategy to promote monotonous Gaussian processes."}, {"heading": "2.2.5 Monotonic Neural Networks", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to move, to fight, to move, to move, to fight, to fight, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2.2.6 Isotonic Regression and Monotonic Nearest Neighbors", "text": "The isotonic regressions can then be used, for example, to define a monotonic, constant, or piecemeal linear surface. This is an old approach; see Barlow et al. (1972) Isotonic regression can be solved in O (n) time if monotonicity implies a total order of n samples. However, for ordinary multidimensional machine learning problems, monotonicity implies only a partial order, and the solution of the nparameter square programs is generally O (n), and monotonicity implies the monotonic learning processes."}, {"heading": "3. Review of Lattice Regression", "text": "Before proposing a monotonic grid regression, we check the grid regression (Garcia and Gupta, 2009; Garcia et al., 2012).The key notation in Table 2.Let Md-N is a hyperparameter that indicates the number of vertices in the summary table (i.e. the grid) for the dth characteristic. Then the grid is a regular grid of M 4 = M1 \u00b7 M2 \u00b7.. MD parameter (an overview table) that is placed with natural numbers so that the grid spans the hyperrectangle M 4 = [0, M1 \u2212 1] \u00d7 [0, M2 \u2212 1] \u00b7. [0, MD \u2212 1]. See Figure 1 for examples of 2 \u00d7 2 grids, and Figure 2 for an example 3 \u00d7 2 grids that encompass the interlatency. For machine learning problems, we find Md = 2 = x for all d lattes to be a good default."}, {"heading": "3.1 Multilinear Interpolation", "text": "The familiar bilinear interpolation commonly used for up-sample images is the D = 2 case of multilinear interpolation = 1 x, which we check here. See Figure 2 for a visual explanation. For notational simplicity, we assume a 2D grid such that x [0, 1] D. For multicellular grids, however, the same mathematics and logic is applied to the grid cell x x, which contains x. Name the kth component of \u03c6 (x) as \u03c6k (x). Let vk [0, 1} D be the lowest vertex of the unit hypercube. The multilinear interpolation weight on the vertex vk is i. (x) = D \u2212 d = 0 x [d] vk [d] (1 \u2212 x]). We note the exponents in (2) are vk and 1 \u2212 vk [d]."}, {"heading": "3.2 The Lattice Regression Objective", "text": "Consider the standard supervised structure of machine learning of a series of randomly sampled pairs {(xi, yi)}, where xi-M and yi-R, for i = 1,..., n. Historically, people have created lookup tables by first adjusting a function h (x) to {xi, yi} by using a regression algorithm such as a neural network or local linear regression, and then evaluating h (x) in a regular grid to generate the lookup table values (Sharma and Bala, 2002), but even when using the function to minimize empirical risk in the training samples, they have not minimized the actual empirical risk, because these approaches did not take into account that the produced lookup table would be interpolated at runtime, and that interpolation would match the error in the samples.Garcia and Gupta (2009) training samples with a direct lookup function that would be the lookup function for polarity polarity polarity polarity in the 2009 workout."}, {"heading": "4. Monotonic Lattices", "text": "In this section we propose to restrict the grid regression to learn monotonous functions."}, {"heading": "4.1 Monotonicity Constraints For a Lattice", "text": "In general, checking whether a non-linear function is monotonous is quite difficult (see related work in Section 2.2), but for a linear interpolated search table, testing for monotonicity is relatively simple: if the grid values rise in a particular direction, the function increases in that direction (see Figure 1 for examples. Specifically, one must check whether these properties apply to each pair of adjacent nighttime table parameters hastr and hasts. If all properties for a 2D grid are specified as monotonous, this leads to D2D \u2212 1 picturesquely linear inequalitative constraints for checks.These same linear inequalitative constraints can be imposed when learning the hastr and hastr parameters to ensure that a monotonic function is learned, and the following result states that these constraints are sufficient and necessary for a 2D grid to become monotonical."}, {"heading": "4.2 Monotonic Lattice Regression Objective", "text": "We relax the strict monotonicity to monotonicity by allowing equality in the adjacent parameter constraints (for an example see the second function from the left in Figure 1).Then the set of paired constraints can be expressed as A\u03b8 \u2264 0 for the corresponding sparse matrix A with a 1 and \u2212 1 per row of A and a row per constraint. Each feature can independently be left unlimited or forced to either monotonously increase or decrease by the specification of A.Thus, the proposed monotonic grating regression object with linear inequality convex: arg min \u03b8 n = 1 '(yi, Tennessee) + R (Tennessee), s.t. A\u03b8 \u2264 b. (8) Additional linear constraints can be included in A\u03b8 \u2264 b to restrict the adapted function in other practical ways, such as f (x \u2212 \u03b8) or f \u2212 latstic case."}, {"heading": "5. Faster Linear Interpolation", "text": "The interpolation of a lookup table has long been considered an efficient method for specifying and evaluating a low-dimensional nonlinear function (Sharma and Bala, 2002; Garcia et al., 2012), but calculating linear interpolation weights with (3) requires O (D) operations for each of the 2D interpolation weights, for the total cost of O (D2D). In Section 5.1, we show that the multilinear interpolation weights of (3) in O (2D) operations can be calculated. Subsequently, in Section 5.2, we review and analyze another linear interpolation that we call simple interpolation that requires only O (D logD) operations."}, {"heading": "5.1 Fast Multilinear Interpolation", "text": "A large part of the calculation in (3) can be divided between the different weights. In algorithm 1 we enter a dynamic programming solution that encompasses D times, with the dth loop lasting 2d times, so that a total of D-1 d = 0 2 d = O (2D) operations. Algorithm 1 calculates the multilinear interpolation weights and the corresponding vertex indices for a unit grid cell [0, 1] D and an x number [0, 1] D. Let the grid parameters index so that sd = 2d is the difference in the indices of the parameters corresponding to any two d numbers adjacent in the dth dimension, for example, to index the 2 \u00b7 2 grid parameters [0 0 0 0], [1 0 1], [1], [1] and the index of the associated grid parameters d \u2212 x."}, {"heading": "5.2 Simplex Linear Interpolation", "text": "For velocity, we propose to use a more efficient linear interpolation for grid regression, which interpolates each x linearly from only D + 1 of the 2D environment corners. However, many different linear interpolation strategies have been proposed to interpolate the reference books with only a subset of the 2D corners (for a check, see Kang (1997)). However, for most strategies it is too mathematically expensive to determine exactly which of the corners should be used to interpolate each x. The wonder of Simplex interpolation is that it only takes O (D logD) operations to determine the D + 1 corners needed to interpolate a given x, and then only O (D) operations to interpolate the identified D + 1 corners. A comparison of simplex and multilinear interpolation is suggested in Figure 3 for the same reference table parameters Simplex interpolation in the literature."}, {"heading": "5.2.1 Partitioning of the Unit Hypercube Into Simplices", "text": "The simplex interpolation implicitly divides the hypercube into the group of D! congruent simplicities that meet the following requirements: Each simplex comprises the vertex of all 0, a vertex consists of all zeros but has a single 1, a vertex is all zeros but has two ones, etc., and ends with a vertex that is all 1, i.e. a total of D + 1 vertices in each simplex. Figure 4 shows the division for the D = 2 and D = 3 unit hypercube. This division can also be described by the hyperplanes xk = xr for 1 \u2264 k \u2264 r \u2264 D (Schimdt and Simon, 2007). Knop (1973) discussed this division as a special case of the Euler's division of the hypercube, and Mead (1979) showed that this is the smallest possible division of the unit hypercube."}, {"heading": "5.2.2 Simplex Interpolation", "text": "Given that x [0, 1] D, the D + 1 vertices specifying the simple unit containing x, can be calculated in O (D logD) operations by sorting the D values of the characteristic vector x, and then the dth simplex vertice has one in the first d-sorted components of x. For example, if x = [.8.2.3] is the D + 1 vertices of its simplex index [0 0 0 0], [1 0 0], [1 0 1], [1 1 1 1], let V be the D + 1 after D matrix, whose dth row is the dth vertice of the simplex vertice containing x. Then the simplex interpolation weights (x) must be the linear interpolation equation specified in (5) so satisfy that [V T1T] vertices () vertices [) -x [x] are the high-weighted T (x) = 1x (1)."}, {"heading": "5.2.3 Simplex Interpolation and Monotonicity", "text": "We show that the same linear inequality constraints that guarantee monotonicity for multilinear interpolation also guarantee monotonicity for single interpolation: Lemma 3 (Monotonic constraints for single interpolation) Let f (x) = \u03b8T\u03c6 (x) for \u03c6 (x), which is specified in algorithm 2. Partial derivative \u2202 f (x) / \u2202 x [d] > 0 iff \u03b8k > \u03b8k \u2032 for all k, k \u2032 so that vk [d] = 0, vk \u2032 [d] = 1 and vk [m] = vk \u2032 [m] for all m 6 = d.Evidence that algorithm 2 interpolates linearly from D + 1 vertices at a time, and thus the resulting function is linear over each simplex. Since the parameters are limited to increase, each such linear function increases monotonically."}, {"heading": "5.2.4 Using Simplex Interpolation for Machine Learning", "text": "Simplex interpolation generates a locally linear continuous function composed of D! hyperplanes defined around the diagonal main axis of the hypercube. Compared to multilinear interpolation, simplex interpolation is not as smooth (though continuous), and is roughly rotation dependent. For low-dimensional regression problems using a three-dimensional regression table with many cells, it has been found that the performance of the two interpolation methods is similar, especially when using a fine-grained grid with many cells. In a comparison of Sun and Zhou (2012) for the three-dimensional regression problem of color management of an LCD monitor, multilinear interpolation of a 9 x-9-9-9-9 look-up table (also referred to as trilinear interpolation in the special case of three-dimensional interpolector) we will generate 1% worse average errors than a simple maximum interpolation of 60%, but with only a multipolar error of 60%."}, {"heading": "6. Regularizing the Lattice Regression To Be More Linear", "text": "We propose a new regulator that takes advantage of the grid structure and encourages the adapted function to be more linear by penalizing differences in parallel edges: Rtorsion (\u03b8) = D-D = 1-D-D-D-D-6 = d-D-R, s, t, u-D-D and vs-D-D, vt and vu-D in dimensions d-D, vr and vt-D-D-D (\u03b8r \u2212 \u03b8s) \u2212 (\u03b8t \u2212 \u03b8u))) 2. (11) This regulator punishes how much the grid function rotates from side to side, and therefore we speak of it as a torsion regulator. The greater the weight on the torsion regulator in the objective function, the more linear the grid function will be over each 2D grid cellar. Figure 6 illustrates the torsion regulator and compares it with the previously proposed torsion regulator, the grid function will be expressed narrowly over each 2D grid function, whereby the 2D grid function will be the lens function."}, {"heading": "7. Jointly Learning Feature Calibrations", "text": "You can learn arbitrarily limited functions with a sufficiently fine-grained grid, but increasing the number of grid corners Md for the dth attribute results in a multiplicative increase in the total number of parameters M = sed dMd. However, in practice, we find that if the characteristics are first appropriately transformed, many problems require only a 2D grid to capture the interactions of the characteristics. A attribute that measures distance, for example, could be better specified as a distance log. Instead of relying on a user to determine how each attribute can best be transformed, we automate this attribute before processing by adding D one-dimensional transformations cd (x [d]) to our function class, which we learn together with the grid, as shown in Figure 7."}, {"heading": "7.1 Calibrating Continuous Features", "text": "We calibrate each continuous feature with a one-dimensional monotonous, piecewise linear function, as shown in Figure 8. Our approach is similar to the work of Howard and Jebara (2007), who jointly learn monotonous, piecewise linear one-dimensional transformations and a linear function. This joint estimation does not make the lens convex, as discussed in Section 9.3. To simplify the estimation of the parameters, we then treat the number of Cd alternating points for the dth characteristic as hyperparameters and fix the Cd alternating point positions (also called nodes) with uniform quantities of the characteristic values. The alternating point values are then optimized together with the grid parameters detailed in Section 9.3.x 2... x (2) 2 R x (1) 2 R x (2) 2 R x (2) 2 Rclateral f functions, each of which are one Rc1 (4)... c3 (2) c (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2) (1) (1) (1) (1) (1) (1) (2) (1) (1) (1) (1) (1) (1) (2) (2) (2) (2) (2) (2) () (2) () (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) (2) (2)"}, {"heading": "7.2 Calibrating Categorical Features", "text": "If the dth attribute is categorical, we suggest using a calibration function cd (\u00b7) to map each category to a real value in [0, Md \u2212 1], i.e., let the set of possible categories for the dth attribute be called Gd, then cd: Gd \u2192 [0, Md \u2212 1]. Figure 9 shows an example grid with a categorical country attribute calibrated to [0, 2]. If there is prior knowledge of the arrangement of the original discrete values or categories, partial or complete pictorial constraints can be added to the mapped values to respect the known ordering information, which can be expressed as additional sparse linear constraints on parameter pairs."}, {"heading": "8. Calibrating Missing Data and Using Missing Data Vertices", "text": "We propose two monitored approaches to detect missing values in training or in the testset.First, one can detect a monitored imputation of missing data values by calibrating a missing data value for each feature. This is the same approach proposed for calibrating categorical values in Section 7.2: learn to imply the numerical value in [0, Md \u2212 1] if the dth function is missing, which minimizes the structural risk obediently. In this approach, missing data are treated by a calibration function cd (\u00b7), and like the other calibration parameters. Other researchers have also considered a joint training of classifiers and imputations for missing data, for example van Esbroeck et al. (2014) and Liao et al. (2007) A more flexible option is to enter missing data its own missing data into the grid, as shown in Figure 10."}, {"heading": "9. Large-Scale Training", "text": "For convex loss functions (\u03b8) and convex regulators R (\u03b8), any solver for convex problems with linear inequality constraints can be used to optimize the grid parameters \u03b8 in (8). However, for large n and even relatively small D, training the proposed calibrated monotonous grids is a challenge due to the number of constraints, the number of terms in the graph regulators, and the non-convexity generated by the use of calibration functions. In this section, we will discuss various standard and new strategies that we found useful in practice: our use of stochastic gradient descences (SGD), stochastic handling of the regulators, parallelization and averaging for distributed training, dealing with the large number of constraints in the context of the SGD, and finally, some details on how we proceed with the non-convex problem of calibrator function training and the generator parameters straight from this (that we are optimizing the north9)."}, {"heading": "9.1 SGD and Reducing Variance of the Subgradients", "text": "To scale to a large number of samples n, we used SGD for all our experiments. For each SGD iteration t, a labeled training sample (xi, yi) is sampled evenly from the set of training sample pairs. We find the corresponding subgradient of (8) and take a tiny step in its negative gradient direction. (The resulting parameters can then violate the limitations we discuss in Section 9.4.) A simple implementation of the SGD for (8) would use the subgradient: (1) + (2) + (2), (3), with the respective operator finding an arbitrary subgradient in its reasoning. (3) Ideally, these subgradients should be cheap to calculate so that each iteration is fast. Calculation costs are dominated by the calculation of the subgradient-subgradient-subgradient-subgradient-subgradient-subgradient."}, {"heading": "9.1.1 Mini-Batching", "text": "We reduce the variance of the loss duration of the stochastic subgradient by minibatch over several samples (Dekel et al., 2012). Let us call S \"a series of k\" training indices, which are uniformly sampled by substitutes of 1,..., n, then the subgradient in minibatches is: \u2206 = 1k \"\u2211 i \u0445 S\" \u044b \"(\u03b8T\u03c6 (xi), yi) + \u043e\u043d\u043e\u043dR (\u03b8). (13) This simultaneously reduces the variance and increases the calculation costs of the loss duration by a factor k. For sufficiently small k\" this is a net gain, since the differentiation of the regulator is the prevailing computational term."}, {"heading": "9.1.2 Stochastic Subgradients for Regularizers", "text": "We propose to reduce the calculation costs of each SGD iteration by randomly selecting the additive conditions of the regularizer, for regularizers that can be expressed as the sum of terms: R (\u03b8) = \u2211 m j = 1 rj (\u03b8). For example, for a 2-D grid, each calculation of the diagram Laplacian Regularizer Subgradient sums over m = D2D \u2212 1 term, and the diagram Torsion Regulator Subgradient sums over m = D (D \u2212 1) 2D \u2212 3 term. Let SR denote a series of kR indices that are uniformly sampled by substituting 1, l. m, then define the subgradient: \u2206 = 1k'i-S 'circulating \"(successT\u03c6 (Xi), Yi) + m kR-j-SR-SR by determining the subgradient in relation to the R and the subgradient in reference to the R."}, {"heading": "9.2 Parallelizing and Averaging", "text": "For a large number of training samples n can be divided into K-sets and then trained independently and in parallel a grid on each of the K-sets. Once trained, the vector grid parameters for the K-grids can be easily averaged. This approach of parallelization and transmission was investigated by Mann et al. (2009) for the large-scale training of linear models. Their results showed similar accuracies as for distributed gradient descent, but 1000 x less network traffic and reduced time for wall clocks for large datasets. In our implementation of the Parallelise-and-Average approach we perform multiple synchronizations: averaging the grids and then sending the averaged grid to parallelized workers to improve with further trainings.We illustrate the performance and acceleration of this simple parallel-and-average approach to learning multimount grids, and the subsequent sending of the averaged grid to parallelized workers to improve with further trainings."}, {"heading": "9.3 Jointly Optimizing Lattice and Calibration Functions", "text": "Let's mark a feature vector with D components, each of which is either a continuous or a categorical value (discrete properties can be modeled either as continuous properties or as a categorical attribute, as the user deems appropriate). Let's assume that there is a limited domain that x [d] can be a calibration function that acts on the dth component of x and has parameters. If the dth attribute is continuous, let's assume that there is a limited domain, so that x [d] is such a domain that x [d] is for finite ld, ud] for the dth calibration function cd [d]; alpha) is a monotonic linear form with fixed nodes at ld, ud, and the Cd \u2212 2 is equally spatial quantities of the dth function via education."}, {"heading": "9.4 Large-Scale Projection Handling", "text": "By default, stochastic gradient reduction projects the parameters onto the constraints after each stochastic gradient update. Given the extremely large number of linear imbalance constraints required to enforce monotonicity even for small D, we found a complete projection of each iteration impractical and unnecessary. We avoid the complete projection of each iteration by using one of two strategies."}, {"heading": "9.4.1 Suboptimal Projections", "text": "We found that the modification of the SGD update to approximate the projection worked well. Specifically, for each new stochastic subgradient \u03b7 \u0445 we create a set of active constraints initialized to \u2205, and move, based on the last parameter values, along the section of \u03b7 \u0445 that is orthogonal to the current active set until we encounter a constraint, add this constraint to the active set and then continue until the update \u03b7 \u0445 is exhausted or it is not possible to move orthogonally to the current active set. At all times, the parameters satisfy the constraints. This can be particularly fast because it is possible to find the scarcity of the monotonicity constraints (on which each depends on only two parameters) and the scarcity of the simple interpolation to optimize the implementation.But this strategy is suboptimal because we do not remove constraints from the active statistic (on which each of only two parameters depend) to optimize the interpolarity of the strategy."}, {"heading": "9.4.2 Stochastic Constraints with LightTouch", "text": "An optimal approach we have compared to handle large-scale constraints is called LightTouch (Cotter et al., 2015). At each iteration, LightTouch does not project constraints, but shifts the constraints to the target, applying a random subset of constraints as stochastic gradient updates to the parameters where distribution is learned through the constraints, while optimization focuses on constraints that are more likely to be active, replacing pro-iteration forecasts with cheap gradient updates. Intermediate solutions may not meet all constraints, but a full projection is performed at the very end to ensure final satisfaction of the constraints. Experimentally, we found that LightTouch generally leads to faster convergence (see Cotter et al. (2015) for its theoretical convergence rate), while it provides experimental results similar to those projected by LightTouch that require a more straighttouch."}, {"heading": "9.4.3 Adapting Stepsizes with Adagrad", "text": "One can generally improve the speed of the SGD with adagrad (Duchi et al., 2011), even with non-convex problems (Gupta et al., 2014). Adagrad reduces the increment adaptively for each parameter, so that parameters that are updated more frequently or with larger gradients have a smaller increment size. We found that adagrad does accelerate the convergence somewhat, but requires more complicated implementations to handle the constraints correctly, since the projections must be related to the Adagrad norm and not to the Euclidean norm. We experimented with the approximation of the Adagrad norm projection to the Euclidean projection, but found that this approximation led to poor convergence."}, {"heading": "10. Case Studies", "text": "We present a series of experimental case studies on real-world problems to highlight various aspects of the proposed methods, followed by some sample interpolation and training times in Section 10.7. Previous data sets used to evaluate monotonous algorithms were small, both in the number of samples and in the number of dimensions, as shown in Table 1. In order to achieve statistically significant experimental results and to better demonstrate the practical need for monotonous limitations, we use real-world case studies with relatively large data sets, and for which application engineers have confirmed that they expect or want the learned function to be monotonous in relation to some sub-characteristics. The data sets used are detailed in Table 3 and comprise data sets with eight thousand to 400 million samples and nine to sixteen characteristics, most of which are limited to being monotonous."}, {"heading": "10.1 General Experimental Details", "text": "We used 10-fold cross validation on each training set to select hyperparameters, including: whether to apply capplactic or torsion regulation, how much regulation (in powers of ten), whether to calibrate missing data or use a vertex of missing data, the number of change points when selecting feature calibration from the choices: {2, 3, 5, 10, 20, 50}, and the number of vertices for each feature was started at 2 and increased by 1 as long as the accuracy of cross validation increased. The step size was matched by 10-fold cross validation and the decisions were powers of 10; it was usually chosen as one of {.01, 1, 1}. When calibration functions were used, a hyperparameter was used to scale the step size for calibration function losses, unless it was set differently compared to the grid function gradients; this cross-calibration was determined by 10-scale validation and a 10-scale validation was also used under 1."}, {"heading": "10.2 Case Study: Business Entity Resolution", "text": "In fact, it is the case that most people are able to survive themselves if they do not see themselves able to survive themselves, \"he said.\" But it is not the case that they feel able to survive themselves. \"In fact,\" it is not the case that they are able to survive themselves. \"In the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century and in the second half of the 20th century."}, {"heading": "10.3 Case Study: Scoring Ad\u2013Query Pairs", "text": "In this case study, we show the potential of the calibration functions, the goal being to evaluate how well a display matches a web search query, based on five different characteristics, each measuring a different idea of good match; the score must be monotonous in relation to all five characteristics; the labels are binary, so this is trained and tested as a classification problem; the pull and test sets were independent and identically distributed, and the results are presented in Table 5; the cross-validated grid size was 2 \u00d7 2 \u00d7 2 \u00d7 2, and the calibration functions were each used 5 alternating points; the distance of the calibration functions and the re-validation of the grid size resulted in a larger grid with a size of 4 \u00d7 4 \u00d7 4 \u00d7 4, and slightly worse (but not statistically significantly worse)."}, {"heading": "10.4 Case Study: Rendering Classifier", "text": "This case study demonstrates the formation of a flexible function (by means of a grid) that is monotonous in terms of fifteen characteristics. The goal is to evaluate whether a particular display element should be displayed on a webpage. The score must be monotonous in fifteen characteristics, and there is a sixteenth Boolean characteristic that is not restricted. The training and testing sets (detailed in Table 3) consisted almost exclusively of samples that are known to be difficult to classify correctly (hence the rather low accuracies).We used a fixed grid size of 216, fixed 5 alternating points per characteristic for the six continuous signals (the other ten signals were boolean), and no regulation of diagrams, so no hyperparameters were optimized for this case study.Simple interpolation was used for the velocity. A single training loop through the 20,000 training samples took about five minutes on a Xeon-type Intel desktop, with a one-thread + C + impairment problem clearly explained by the time constraint of dealing with the impairment."}, {"heading": "10.5 Case Study: Fusing Pipelines", "text": "While this work focuses on learning monotonous functions, we believe it is also the first work to suggest applying lattice regression to classification problems, not just regression problems. With this in mind, we include this case study, which shows that lattice regression also functions without limitations in a similar way to random forests on a large multi-class problem in the real world. The goal of this case study is to merge the predictions of two pipelines, each of which makes a prediction of the probability of seven user categories based on different high-dimensional characteristics. Since the probability estimates of each pipeline produce one, only the first six probability estimates from each pipeline are needed as characteristics for fusion, for a total of twelve characteristics. The training and test set have been split by time, with the older 1.6 million samples used for training, and the latest 390,000 samples used as a test set."}, {"heading": "10.6 Case Study: Video Ranking and Large-Scale Learning", "text": "The goal is to learn a feature that a user likes to watch, based on the video he has just watched. Experiments were conducted on anonymized data from YouTube.Each feature vector xi is a feature vector over a pair of videos, xi = h (vj, vk), where VJ is the video watched, and h is a function that takes a pair of videos and returns a twelve-dimensional feature vector xi. For example, a feature could be the number of times video vj and video vk were viewed in the same session. Each of the twelve features was specified to be positively correlated with users viewing preferences, and so we have the model increase monotonically."}, {"heading": "10.6.1 Which Pairs of Candidate Videos?", "text": "A key question is which sample pairs of candidate videos v + k and v \u2212 k should be used as preferred and non-preferred pairs for a given video vj. We used anonymized click data from YouTube's current video suggestion system. If a user clicked on a proposed video in the second position or below, we took the clicked video as the preferred video v + k and the video proposed directly above the clicked video as the unpreferred video v \u2212 j. We designate this choice of v + k and v \u2212 k as a pair clicked from below. This choice is consistent with the results of Joachims et al. (2005), whose eye-tracking experiments on Web site search results showed that users on average consider at least one result above the clicked result and that these pairs of preferred / non-preferred samples correlated strongly with explicit relevance assessments. Also, the use of pairs clicked below removes the trust list from those who know that they are being presented with a higher preference, even though they are presented with a large sample)."}, {"heading": "10.6.2 More Experimental Details", "text": "The data set was randomly divided into mutually exclusive training, testing and validation sets of 400 million, 25 million and 25 million pairs. To ensure privacy, the data set contained only the feature vector and no information to identify the video or the user. The disadvantage is that the test and validation sets are likely to contain some samples from the same videos and the same users. Overall, however, the data sets capture millions of unique users and unique videos seen. We used a fixed 312 grid with a total of 531,441 parameters. In this case, the pre-processing functions were set so that no calibration functions were learned. We compared the training with ever larger randomly sampled subsets of the 400 million training set (see Figure 13 for the size of the training set). We compared training on a single worker with the parallelization and average strategies explained in Section 9.2."}, {"heading": "10.6.3 Results", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "10.7 Run Times", "text": "Figure 14 shows average evaluation times for multilinear and simple interpolation of a sample from a 2D grid for D = 4 to D = 20 with a single-wire 3.5 GHz Intel Ivy Bridge processor. Note the multilinear evaluation times on a log scale, and on a log scale the evaluation time increases approximately linearly in D, corresponding to the theoretical O (2D) complexity given in Section 5.1. Simple evaluation times scale roughly linearly with D, which is in line with the theoretical O (D logD) complexity. For D = 6 features, single interpolation is already three times faster than multilinear features. At D = 20 features, single interpolation is only 750 nanoseconds long, but multilinear interpolation is approximately 15,000 times slower, at about 12 milliseconds."}, {"heading": "11. Discussion and Some Open Questions", "text": "In fact, most of them are in a position to move into a different world, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live."}, {"heading": "12. Acknowledgments", "text": "We thank Sugato Basu, David Cardoze, James Chen, Emmanuel Christophe, Brendan Collins, Mahdi Milani Fard, James Muller, Biswanath Panda and Alex Vodomerov for their help with experiments and helpful discussions."}], "references": [{"title": "A method for learning from hints", "author": ["Y.S. Abu-Mostafa"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Abu.Mostafa.,? \\Q1993\\E", "shortCiteRegEx": "Abu.Mostafa.", "year": 1993}, {"title": "Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems", "author": ["N.P. Archer", "S. Wang"], "venue": "Decision Sciences,", "citeRegEx": "Archer and Wang.,? \\Q1993\\E", "shortCiteRegEx": "Archer and Wang.", "year": 1993}, {"title": "Learning with submodular functions: A convex optimization perspective", "author": ["F. Bach"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "Statistical inference under order restrictions; the theory and application of isotonic regression", "author": ["R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk"], "venue": null, "citeRegEx": "Barlow et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Barlow et al\\.", "year": 1972}, {"title": "Automatic generation of symbolic multiattribute ordinal knowledge based DSS: methodology and applications", "author": ["A. Ben-David"], "venue": "Decision Sciences,", "citeRegEx": "Ben.David.,? \\Q1992\\E", "shortCiteRegEx": "Ben.David.", "year": 1992}, {"title": "Monotonicity maintenance in information-theoretic machine learning algorithms", "author": ["A. Ben-David"], "venue": "Machine Learning,", "citeRegEx": "Ben.David.,? \\Q1995\\E", "shortCiteRegEx": "Ben.David.", "year": 1995}, {"title": "Learning and classification of monotonic ordinal concepts", "author": ["A. Ben-David", "L. Sterling", "Y.H. Pao"], "venue": "Computational Intelligence,", "citeRegEx": "Ben.David et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1989}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2010}, {"title": "The Mythical Man-Month", "author": ["F. Brooks"], "venue": null, "citeRegEx": "Brooks.,? \\Q1975\\E", "shortCiteRegEx": "Brooks.", "year": 1975}, {"title": "Trade-off between accuracy and interpretability in fuzzy rule-based modelling", "author": ["J. Casillas", "O. Cordon", "F. Herrera", "L. Magdalena (Eds"], "venue": "Physica-Verlag,", "citeRegEx": "Casillas et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Casillas et al\\.", "year": 2002}, {"title": "A Light Touch for Heavily Constrained SGD", "author": ["A. Cotter", "M.R. Gupta", "J. Pfeifer"], "venue": "arXiv preprint,", "citeRegEx": "Cotter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2015}, {"title": "Deduplicating a places database", "author": ["N. Dalvi", "M. Olteanu", "M. Raghavan", "P. Bohannon"], "venue": "Proc. ACM WWW Conf.,", "citeRegEx": "Dalvi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2014}, {"title": "Monotone and partially monotone neural networks", "author": ["H. Daniels", "M. Velikova"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Daniels and Velikova.,? \\Q2010\\E", "shortCiteRegEx": "Daniels and Velikova.", "year": 2010}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Incorporating second-order functional knowledge for better option pricing", "author": ["C. Dugas", "Y. Bengio", "F. B\u00e9lisle", "C. Nadeau", "R. Garcia"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dugas et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dugas et al\\.", "year": 2000}, {"title": "Incorporating functional knowledge in neural networks", "author": ["C. Dugas", "Y. Bengio", "F. B\u00e9lisle", "C. Nadeau", "R. Garcia"], "venue": "Journal Machine Learning Research,", "citeRegEx": "Dugas et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dugas et al\\.", "year": 2009}, {"title": "Nearest neighbour classification with monotonicity constraints", "author": ["W. Duivesteijn", "A. Feelders"], "venue": "Proc. European Conf. Machine Learning,", "citeRegEx": "Duivesteijn and Feelders.,? \\Q2008\\E", "shortCiteRegEx": "Duivesteijn and Feelders.", "year": 2008}, {"title": "Monotone relabeling in ordinal classification", "author": ["A. Feelders"], "venue": "Proc. IEEE Conf. Data Mining, pages 803\u2013808,", "citeRegEx": "Feelders.,? \\Q2010\\E", "shortCiteRegEx": "Feelders.", "year": 2010}, {"title": "Do we need hundreds of classifiers to solve real world classification problems", "author": ["M. Fernandez-Delgado", "E. Cernadas", "S. Barro", "D. Amorim"], "venue": "Journal Machine Learning Research,", "citeRegEx": "Fernandez.Delgado et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fernandez.Delgado et al\\.", "year": 2014}, {"title": "Lattice regression", "author": ["E.K. Garcia", "M.R. Gupta"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Garcia and Gupta.,? \\Q2009\\E", "shortCiteRegEx": "Garcia and Gupta.", "year": 2009}, {"title": "Completely lazy learning", "author": ["E.K. Garcia", "S. Feldman", "M.R. Gupta", "S. Srivastava"], "venue": "IEEE Trans. Knowledge and Data Engineering,", "citeRegEx": "Garcia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2010}, {"title": "Optimized regression for efficient function evaluation", "author": ["E.K. Garcia", "R. Arora", "M.R. Gupta"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "Garcia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2012}, {"title": "A study of statistical techniques and performance measures for genetics-based machine learning: accuracy and interpretability", "author": ["S. Garcia", "A. Fernandez", "J. Luengo", "F. Herrera"], "venue": "Soft Computing,", "citeRegEx": "Garcia et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2009}, {"title": "The Estimation of Probabilities: An Essay on Modern Bayesian Methods", "author": ["I.J. Good"], "venue": null, "citeRegEx": "Good.,? \\Q1965\\E", "shortCiteRegEx": "Good.", "year": 1965}, {"title": "Sorting the slow way: an analysis of perversely awful randomized sorting algorithms", "author": ["H. Gruber", "M. Holzer", "O. Ruepp"], "venue": "In Fun with Algorithms,", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Training highly multiclass classifiers", "author": ["M. Gupta", "S. Bengio", "J. Weston"], "venue": "Journal Machine Learning Research,", "citeRegEx": "Gupta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "Nonparametric supervised learning by linear interpolation with maximum entropy", "author": ["M.R. Gupta", "R.M. Gray", "R.A. Olshen"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gupta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2006}, {"title": "Generalized Additive Models", "author": ["T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani.,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani.", "year": 1990}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Generalized monotonic regression using random change points", "author": ["C.C. Holmes", "N.A. Heard"], "venue": "Statistics in Medicine,", "citeRegEx": "Holmes and Heard.,? \\Q2003\\E", "shortCiteRegEx": "Holmes and Heard.", "year": 2003}, {"title": "Learning monotonic transformations for classification", "author": ["A. Howard", "T. Jebara"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Howard and Jebara.,? \\Q2007\\E", "shortCiteRegEx": "Howard and Jebara.", "year": 2007}, {"title": "Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning", "author": ["H. Ishibuchi", "Y. Nojima"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Ishibuchi and Nojima.,? \\Q2007\\E", "shortCiteRegEx": "Ishibuchi and Nojima.", "year": 2007}, {"title": "Accurately interpreting clickthrough data as implicit feedback", "author": ["T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "G. Gay"], "venue": "Proc. SIGIR,", "citeRegEx": "Joachims et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2005}, {"title": "Comparison of three-dimensional interpolation techniques by simulations", "author": ["H.R. Kang"], "venue": "SPIE Vol. 2414,", "citeRegEx": "Kang.,? \\Q1995\\E", "shortCiteRegEx": "Kang.", "year": 1995}, {"title": "Color Technology for Electronic Imaging Devices", "author": ["H.R. Kang"], "venue": "SPIE Press,", "citeRegEx": "Kang.,? \\Q1997\\E", "shortCiteRegEx": "Kang.", "year": 1997}, {"title": "A tetrahedral interpolation technique for color space conversion", "author": ["J. Kasson", "W. Plouffe", "S. Nin"], "venue": "SPIE Vol. 1909,", "citeRegEx": "Kasson et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kasson et al\\.", "year": 1993}, {"title": "Estimating monotonic functions and their bounds", "author": ["H. Kay", "L.H. Ungar"], "venue": "AIChE Journal,", "citeRegEx": "Kay and Ungar.,? \\Q2000\\E", "shortCiteRegEx": "Kay and Ungar.", "year": 2000}, {"title": "A note on hypercube partitions", "author": ["R.E. Knop"], "venue": "Journal of Combinatorial Theory, Ser. A,", "citeRegEx": "Knop.,? \\Q1973\\E", "shortCiteRegEx": "Knop.", "year": 1973}, {"title": "Rule learning with monotonicity constraints", "author": ["W. Kotlowski", "R. Slowinski"], "venue": "In Proceedings International Conference on Machine Learning,", "citeRegEx": "Kotlowski and Slowinski.,? \\Q2009\\E", "shortCiteRegEx": "Kotlowski and Slowinski.", "year": 2009}, {"title": "Incorporating prior knowledge in support vector regression", "author": ["F. Lauer", "G. Bloch"], "venue": "Machine Learning,", "citeRegEx": "Lauer and Bloch.,? \\Q2008\\E", "shortCiteRegEx": "Lauer and Bloch.", "year": 2008}, {"title": "Quadratically gated mixture of experts for incomplete data classification", "author": ["X. Liao", "H. Li", "L. Carin"], "venue": "Proc. ICML,", "citeRegEx": "Liao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2007}, {"title": "Learning to Rank for Information", "author": ["T.-Y. Liu"], "venue": null, "citeRegEx": "Liu.,? \\Q2011\\E", "shortCiteRegEx": "Liu.", "year": 2011}, {"title": "A linear fit gets the correct monotonicity directions", "author": ["Malik Magdon-Ismail", "J. Sill"], "venue": "Machine Learning,", "citeRegEx": "Magdon.Ismail and Sill.,? \\Q2008\\E", "shortCiteRegEx": "Magdon.Ismail and Sill.", "year": 2008}, {"title": "Efficient largescale distributed training of conditional maximum entropy models", "author": ["G. Mann", "R. McDonald", "M. Mohri", "N. Silberman", "D.D. Walker"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Dissection of the hypercube into simplexes", "author": ["D.G. Mead"], "venue": "Proc. Amer. Math. Soc.,", "citeRegEx": "Mead.,? \\Q1979\\E", "shortCiteRegEx": "Mead.", "year": 1979}, {"title": "Comparison of universal approximators incorporating partial monotonicity by structure", "author": ["A. Minin", "M. Velikova", "B. Lang", "H. Daniels"], "venue": "Neural Networks,", "citeRegEx": "Minin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Minin et al\\.", "year": 2010}, {"title": "Feasible nonparametric estimation of multiargument monotone functions", "author": ["H. Mukarjee", "S. Stern"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Mukarjee and Stern.,? \\Q1994\\E", "shortCiteRegEx": "Mukarjee and Stern.", "year": 1994}, {"title": "Bayesian isotonic regression and trend analysis", "author": ["B. Neelon", "D.B. Dunson"], "venue": "Biometrics, 60:398\u2013406,", "citeRegEx": "Neelon and Dunson.,? \\Q2004\\E", "shortCiteRegEx": "Neelon and Dunson.", "year": 2004}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Reliable integration of continuous constraints into extreme learning machines", "author": ["K. Neumann", "M. Rolf", "J.J. Steil"], "venue": "International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems,", "citeRegEx": "Neumann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Neumann et al\\.", "year": 2013}, {"title": "Inducing interpretable voting classifiers without trading accuracy for simplicity: Theoretical results, approximation algorithms, and experiments", "author": ["R. Nock"], "venue": "Journal Artificial Intelligence Research,", "citeRegEx": "Nock.,? \\Q2002\\E", "shortCiteRegEx": "Nock.", "year": 2002}, {"title": "Post-pruning in decision tree induction using multiple performance measures", "author": ["K.-M. Osei-Bryson"], "venue": "Computers and Operations Research,", "citeRegEx": "Osei.Bryson.,? \\Q2007\\E", "shortCiteRegEx": "Osei.Bryson.", "year": 2007}, {"title": "Classification trees for problems with monotonicity constraints", "author": ["R. Potharst", "A.J. Feelders"], "venue": "ACM SIGKDD Explorations,", "citeRegEx": "Potharst and Feelders.,? \\Q2002\\E", "shortCiteRegEx": "Potharst and Feelders.", "year": 2002}, {"title": "Pruning for monotone classification", "author": ["R. Potharst", "A.J. Feelders"], "venue": "trees. Springer Lecture Notes on Computer Science,", "citeRegEx": "Potharst and Feelders.,? \\Q2002\\E", "shortCiteRegEx": "Potharst and Feelders.", "year": 2002}, {"title": "Generalized constraint neural network regression model subject to linear priors", "author": ["Y.-J. Qu", "B.-G. Hu"], "venue": "IEEE Trans. on Neural Networks,", "citeRegEx": "Qu and Hu.,? \\Q2011\\E", "shortCiteRegEx": "Qu and Hu.", "year": 2011}, {"title": "Estimating smooth monotone functions", "author": ["J.O. Ramsay"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Ramsay.,? \\Q1998\\E", "shortCiteRegEx": "Ramsay.", "year": 1998}, {"title": "Learning interpretable SVMs for biological sequence classification", "author": ["G. R\u00e4tsch", "S. Sonnenburg", "C. Sch\u00e4fer"], "venue": "BMC Bioinformatics,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2006}, {"title": "Gaussian processes with monotonicity information", "author": ["J. Riihim\u00e4ki", "A. Vehtari"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Riihim\u00e4ki and Vehtari.,? \\Q2010\\E", "shortCiteRegEx": "Riihim\u00e4ki and Vehtari.", "year": 2010}, {"title": "A geometric approach to maximum-speed ndimensional continuous linear interpolation in rectangular grids", "author": ["R. Rovatti", "M. Borgatti", "R. Guerrieri"], "venue": "IEEE Trans. on Computers,", "citeRegEx": "Rovatti et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Rovatti et al\\.", "year": 1998}, {"title": "Some geometric probability problems involving the Eulerian numbers", "author": ["F. Schimdt", "R. Simon"], "venue": "Electronic Journal of Combinatorics,", "citeRegEx": "Schimdt and Simon.,? \\Q2007\\E", "shortCiteRegEx": "Schimdt and Simon.", "year": 2007}, {"title": "Digital Color Imaging Handbook", "author": ["G. Sharma", "R. Bala"], "venue": null, "citeRegEx": "Sharma and Bala.,? \\Q2002\\E", "shortCiteRegEx": "Sharma and Bala.", "year": 2002}, {"title": "A Bayesian approach to non-parametric monotone function estimation", "author": ["T.S. Shively", "T.W. Sager", "S.G. Walker"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Shively et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shively et al\\.", "year": 2009}, {"title": "A review on the interpretability-accuracy trade-off in evolutionary multi-objective fuzzy systems (EMOFS)", "author": ["P.K. Shukla", "S.P. Tripathi"], "venue": null, "citeRegEx": "Shukla and Tripathi.,? \\Q2012\\E", "shortCiteRegEx": "Shukla and Tripathi.", "year": 2012}, {"title": "Monotonicity hints", "author": ["J. Sill", "Y.S. Abu-Mostafa"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sill and Abu.Mostafa.,? \\Q1997\\E", "shortCiteRegEx": "Sill and Abu.Mostafa.", "year": 1997}, {"title": "Statistical and knowledge-based approaches to clinical decision support systems, with an application in gastroenterology", "author": ["D.J. Spiegelhalter", "R.P. Knill-Jones"], "venue": "Journal of the Royal Statistical Society A,", "citeRegEx": "Spiegelhalter and Knill.Jones.,? \\Q1984\\E", "shortCiteRegEx": "Spiegelhalter and Knill.Jones.", "year": 1984}, {"title": "Least squares isotonic regression in two dimensions", "author": ["J. Spouge", "H. Wan", "W.J. Wilbur"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Spouge et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Spouge et al\\.", "year": 2003}, {"title": "Transparent neural networks", "author": ["C. Strannegaard"], "venue": "Proc. Artificial General Intelligence,", "citeRegEx": "Strannegaard.,? \\Q2012\\E", "shortCiteRegEx": "Strannegaard.", "year": 2012}, {"title": "Study on the 3D interpolation models used in color conversion", "author": ["B. Sun", "S. Zhou"], "venue": "IACSIT Intl. Journal Engineering and Technology,", "citeRegEx": "Sun and Zhou.,? \\Q2012\\E", "shortCiteRegEx": "Sun and Zhou.", "year": 2012}, {"title": "Evaluating trauma patients: Addressing missing covariates with joint optimization", "author": ["A. van Esbroeck", "S. Singh", "I. Rubinfeld", "Z. Syed"], "venue": "Proc. AAAI,", "citeRegEx": "Esbroeck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Esbroeck et al\\.", "year": 2014}, {"title": "Inequality-constrained multivariate smoothing splines with application to the estimation of posterior probabilities", "author": ["M. Villalobos", "G. Wahba"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Villalobos and Wahba.,? \\Q1987\\E", "shortCiteRegEx": "Villalobos and Wahba.", "year": 1987}, {"title": "A neural network method of density estimation for univariate unimodal data", "author": ["S. Wang"], "venue": "Neural Computing & Applications,", "citeRegEx": "Wang.,? \\Q1994\\E", "shortCiteRegEx": "Wang.", "year": 1994}, {"title": "Trade-off between accuracy and interpretability: experience-oriented fuzzy modeling via reduced-set vectors", "author": ["L. Yu", "J. Xiao"], "venue": "Computers and Mathematics with Applications,", "citeRegEx": "Yu and Xiao.,? \\Q2012\\E", "shortCiteRegEx": "Yu and Xiao.", "year": 2012}], "referenceMentions": [{"referenceID": 61, "context": "(Sharma and Bala, 2002).", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "Estimating the parameters of an interpolated look-up table using structural risk minimization was proposed by Garcia and Gupta (2009) and called lattice regression.", "startOffset": 110, "endOffset": 134}, {"referenceID": 24, "context": "N\u00e4\u0131ve Bayes classifiers can be interpreted in terms of weights of evidence (Good, 1965; Spiegelhalter and Knill-Jones, 1984).", "startOffset": 75, "endOffset": 124}, {"referenceID": 65, "context": "N\u00e4\u0131ve Bayes classifiers can be interpreted in terms of weights of evidence (Good, 1965; Spiegelhalter and Knill-Jones, 1984).", "startOffset": 75, "endOffset": 124}, {"referenceID": 28, "context": "Linear approaches can be generalized to sum nonlinear components, as in generalized additive models (Hastie and Tibshirani, 1990) and some kernel methods, while still retaining some of their interpretable aspects.", "startOffset": 100, "endOffset": 129}, {"referenceID": 29, "context": "For example, Ishibuchi and Nojima (2007) minimize the number of fuzzy rules in a rule set, Osei-Bryson (2007) prunes a decision tree for interpretability, R\u00e4tsch et al.", "startOffset": 13, "endOffset": 41}, {"referenceID": 29, "context": "For example, Ishibuchi and Nojima (2007) minimize the number of fuzzy rules in a rule set, Osei-Bryson (2007) prunes a decision tree for interpretability, R\u00e4tsch et al.", "startOffset": 13, "endOffset": 110}, {"referenceID": 29, "context": "For example, Ishibuchi and Nojima (2007) minimize the number of fuzzy rules in a rule set, Osei-Bryson (2007) prunes a decision tree for interpretability, R\u00e4tsch et al. (2006) finds a sparse convex combination of kernels for a multi-kernel support vector machine, and Nock (2002) prefers smaller committees of ensemble classifiers.", "startOffset": 13, "endOffset": 176}, {"referenceID": 29, "context": "For example, Ishibuchi and Nojima (2007) minimize the number of fuzzy rules in a rule set, Osei-Bryson (2007) prunes a decision tree for interpretability, R\u00e4tsch et al. (2006) finds a sparse convex combination of kernels for a multi-kernel support vector machine, and Nock (2002) prefers smaller committees of ensemble classifiers.", "startOffset": 13, "endOffset": 280}, {"referenceID": 21, "context": "Similarly, Garcia et al. (2009) measure the interpretability of rule-based classifiers in terms of the number of rules and number of features used.", "startOffset": 11, "endOffset": 32}, {"referenceID": 29, "context": "Akaike information criterion (Hastie et al., 2001), sparsity regularizers like sparse linear regression models, and feature selection methods.", "startOffset": 29, "endOffset": 50}, {"referenceID": 67, "context": "Other approaches to simplicity may include simplified structure in graphical models or neural nets, such as the structured neural nets of (Strannegaard, 2012).", "startOffset": 138, "endOffset": 158}, {"referenceID": 9, "context": "While sparsity-based approaches to interpretability can provide regularization that reduces over-fitting and hence increases accuracy, it has also been noted that such strategies may create a trade-off between interpretability and accuracy (Casillas et al., 2002; Nock, 2002; Yu and Xiao, 2012; Shukla and Tripathi, 2012).", "startOffset": 240, "endOffset": 321}, {"referenceID": 51, "context": "While sparsity-based approaches to interpretability can provide regularization that reduces over-fitting and hence increases accuracy, it has also been noted that such strategies may create a trade-off between interpretability and accuracy (Casillas et al., 2002; Nock, 2002; Yu and Xiao, 2012; Shukla and Tripathi, 2012).", "startOffset": 240, "endOffset": 321}, {"referenceID": 72, "context": "While sparsity-based approaches to interpretability can provide regularization that reduces over-fitting and hence increases accuracy, it has also been noted that such strategies may create a trade-off between interpretability and accuracy (Casillas et al., 2002; Nock, 2002; Yu and Xiao, 2012; Shukla and Tripathi, 2012).", "startOffset": 240, "endOffset": 321}, {"referenceID": 63, "context": "While sparsity-based approaches to interpretability can provide regularization that reduces over-fitting and hence increases accuracy, it has also been noted that such strategies may create a trade-off between interpretability and accuracy (Casillas et al., 2002; Nock, 2002; Yu and Xiao, 2012; Shukla and Tripathi, 2012).", "startOffset": 240, "endOffset": 321}, {"referenceID": 22, "context": "In this paper we extend lattice regression, which is a spline method with fixed knots on a regular grid and a linear kernel (Garcia et al., 2012), to be monotonic.", "startOffset": 124, "endOffset": 145}, {"referenceID": 21, "context": "In this paper we extend lattice regression, which is a spline method with fixed knots on a regular grid and a linear kernel (Garcia et al., 2012), to be monotonic. There have been a number of proposals to learn monotonic one-dimensional splines. For example, building on Ramsay (1998), Shively et al.", "startOffset": 125, "endOffset": 285}, {"referenceID": 21, "context": "In this paper we extend lattice regression, which is a spline method with fixed knots on a regular grid and a linear kernel (Garcia et al., 2012), to be monotonic. There have been a number of proposals to learn monotonic one-dimensional splines. For example, building on Ramsay (1998), Shively et al. (2009) parameterize the set of all smooth and strictly monotonic one-dimensional functions using an integrated exponential form f(x) = a+ \u222b x 0 e b+u(t)dt, and showed better performance than the monotone functions estimators of Neelon and Dunson (2004) and Holmes and Heard (2003) for smooth functions.", "startOffset": 125, "endOffset": 308}, {"referenceID": 21, "context": "In this paper we extend lattice regression, which is a spline method with fixed knots on a regular grid and a linear kernel (Garcia et al., 2012), to be monotonic. There have been a number of proposals to learn monotonic one-dimensional splines. For example, building on Ramsay (1998), Shively et al. (2009) parameterize the set of all smooth and strictly monotonic one-dimensional functions using an integrated exponential form f(x) = a+ \u222b x 0 e b+u(t)dt, and showed better performance than the monotone functions estimators of Neelon and Dunson (2004) and Holmes and Heard (2003) for smooth functions.", "startOffset": 125, "endOffset": 554}, {"referenceID": 21, "context": "In this paper we extend lattice regression, which is a spline method with fixed knots on a regular grid and a linear kernel (Garcia et al., 2012), to be monotonic. There have been a number of proposals to learn monotonic one-dimensional splines. For example, building on Ramsay (1998), Shively et al. (2009) parameterize the set of all smooth and strictly monotonic one-dimensional functions using an integrated exponential form f(x) = a+ \u222b x 0 e b+u(t)dt, and showed better performance than the monotone functions estimators of Neelon and Dunson (2004) and Holmes and Heard (2003) for smooth functions.", "startOffset": 125, "endOffset": 582}, {"referenceID": 21, "context": "In this paper we extend lattice regression, which is a spline method with fixed knots on a regular grid and a linear kernel (Garcia et al., 2012), to be monotonic. There have been a number of proposals to learn monotonic one-dimensional splines. For example, building on Ramsay (1998), Shively et al. (2009) parameterize the set of all smooth and strictly monotonic one-dimensional functions using an integrated exponential form f(x) = a+ \u222b x 0 e b+u(t)dt, and showed better performance than the monotone functions estimators of Neelon and Dunson (2004) and Holmes and Heard (2003) for smooth functions. In other related spline work, Villalobos and Wahba (1987) considered smoothing splines with linear inequality constraints, but did not address monotonicity.", "startOffset": 125, "endOffset": 662}, {"referenceID": 4, "context": "Non-monotonic trees can be pruned to be monotonic using various strategies that iteratively reduce the non-monotonic branches (Ben-David, 1992; Potharst and Feelders, 2002b).", "startOffset": 126, "endOffset": 173}, {"referenceID": 5, "context": "Monontonicity can also be encouraged during tree construction by penalizing the splitting criterion to reduce the number of non-monotonic leaves a split would create (Ben-David, 1995).", "startOffset": 166, "endOffset": 183}, {"referenceID": 25, "context": "Potharst and Feelders (2002a) achieved completely flexible monotonic trees using a strategy akin to bogosort (Gruber et al., 2007): train many trees on different random subsets of the training samples, then select one that is monotonic.", "startOffset": 109, "endOffset": 130}, {"referenceID": 4, "context": "Non-monotonic trees can be pruned to be monotonic using various strategies that iteratively reduce the non-monotonic branches (Ben-David, 1992; Potharst and Feelders, 2002b). Monontonicity can also be encouraged during tree construction by penalizing the splitting criterion to reduce the number of non-monotonic leaves a split would create (Ben-David, 1995). Potharst and Feelders (2002a) achieved completely flexible monotonic trees using a strategy akin to bogosort (Gruber et al.", "startOffset": 127, "endOffset": 390}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 63, "endOffset": 86}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 63, "endOffset": 137}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 63, "endOffset": 203}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 63, "endOffset": 260}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 300, "endOffset": 319}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 300, "endOffset": 366}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 300, "endOffset": 428}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 300, "endOffset": 498}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al.", "startOffset": 300, "endOffset": 552}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al.", "startOffset": 300, "endOffset": 596}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al.", "startOffset": 300, "endOffset": 672}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al.", "startOffset": 300, "endOffset": 727}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al. (2000, 2009) neural net constrain function yes 4 3434 Shively et al. (2009) spline constrain function yes 1 100 Kotlowski and Slowinski (2009) rule-based re-label samples yes 11 1728 Daniels and Velikova (2010) neural net constrain function yes 6 174 Riihim\u00e4ki and Vehtari (2010) Gaussian process sample derivatives no 7 1222 Qu and Hu (2011) neural net derivatives / constrain yes 1 30 Neumann et al.", "startOffset": 300, "endOffset": 852}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al. (2000, 2009) neural net constrain function yes 4 3434 Shively et al. (2009) spline constrain function yes 1 100 Kotlowski and Slowinski (2009) rule-based re-label samples yes 11 1728 Daniels and Velikova (2010) neural net constrain function yes 6 174 Riihim\u00e4ki and Vehtari (2010) Gaussian process sample derivatives no 7 1222 Qu and Hu (2011) neural net derivatives / constrain yes 1 30 Neumann et al.", "startOffset": 300, "endOffset": 919}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al. (2000, 2009) neural net constrain function yes 4 3434 Shively et al. (2009) spline constrain function yes 1 100 Kotlowski and Slowinski (2009) rule-based re-label samples yes 11 1728 Daniels and Velikova (2010) neural net constrain function yes 6 174 Riihim\u00e4ki and Vehtari (2010) Gaussian process sample derivatives no 7 1222 Qu and Hu (2011) neural net derivatives / constrain yes 1 30 Neumann et al.", "startOffset": 300, "endOffset": 987}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al. (2000, 2009) neural net constrain function yes 4 3434 Shively et al. (2009) spline constrain function yes 1 100 Kotlowski and Slowinski (2009) rule-based re-label samples yes 11 1728 Daniels and Velikova (2010) neural net constrain function yes 6 174 Riihim\u00e4ki and Vehtari (2010) Gaussian process sample derivatives no 7 1222 Qu and Hu (2011) neural net derivatives / constrain yes 1 30 Neumann et al.", "startOffset": 300, "endOffset": 1056}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al. (2000, 2009) neural net constrain function yes 4 3434 Shively et al. (2009) spline constrain function yes 1 100 Kotlowski and Slowinski (2009) rule-based re-label samples yes 11 1728 Daniels and Velikova (2010) neural net constrain function yes 6 174 Riihim\u00e4ki and Vehtari (2010) Gaussian process sample derivatives no 7 1222 Qu and Hu (2011) neural net derivatives / constrain yes 1 30 Neumann et al.", "startOffset": 300, "endOffset": 1119}, {"referenceID": 0, "context": "Method Monotonicity Strategy Guaranteed Monotonic? Max D Max n Archer and Wang (1993) neural net constrain function yes 2 50 Wang (1994) neural net constrain function yes 1 150 Mukarjee and Stern (1994) kernel estimate post-process yes 2 2447 Ben-David (1995) tree penalize splits yes 8 125 Sill and Abu-Mostafa (1997) neural net penalize pairs no 6 550 Sill (1998) neural net constrain function yes 10 196 Kay and Ungar (2000) neural net constrain function yes 1 100 Potharst and Feelders (2002a) tree randomize yes 8 60 Potharst and Feelders (2002b) tree prune yes 11 1090 Spouge et al. (2003) isotonic regression constrain yes 2 100,000 Duivesteijn and Feelders (2008) k-NN re-label samples no 12 768 Lauer and Bloch (2008) svm sample derivatives no none none Dugas et al. (2000, 2009) neural net constrain function yes 4 3434 Shively et al. (2009) spline constrain function yes 1 100 Kotlowski and Slowinski (2009) rule-based re-label samples yes 11 1728 Daniels and Velikova (2010) neural net constrain function yes 6 174 Riihim\u00e4ki and Vehtari (2010) Gaussian process sample derivatives no 7 1222 Qu and Hu (2011) neural net derivatives / constrain yes 1 30 Neumann et al. (2013) neural net sample derivatives no 3 625", "startOffset": 300, "endOffset": 1185}, {"referenceID": 40, "context": "Lauer and Bloch (2008) encouraged support vector machines to be more monotonic by constraining the derivative of the function at the training samples.", "startOffset": 0, "endOffset": 23}, {"referenceID": 40, "context": "Lauer and Bloch (2008) encouraged support vector machines to be more monotonic by constraining the derivative of the function at the training samples. Riihim\u00e4ki and Vehtari (2010) used the same strategy to encourage more monotonic Gaussian processes.", "startOffset": 0, "endOffset": 180}, {"referenceID": 71, "context": "Other researchers explicitly proposed constraining the weights to be positive in a single hidden-layer neural network with the sigmoid or other monotonic nonlinear transformation (Wang, 1994; Kay and Ungar, 2000; Dugas et al., 2000, 2009; Minin et al., 2010).", "startOffset": 179, "endOffset": 258}, {"referenceID": 37, "context": "Other researchers explicitly proposed constraining the weights to be positive in a single hidden-layer neural network with the sigmoid or other monotonic nonlinear transformation (Wang, 1994; Kay and Ungar, 2000; Dugas et al., 2000, 2009; Minin et al., 2010).", "startOffset": 179, "endOffset": 258}, {"referenceID": 46, "context": "Other researchers explicitly proposed constraining the weights to be positive in a single hidden-layer neural network with the sigmoid or other monotonic nonlinear transformation (Wang, 1994; Kay and Ungar, 2000; Dugas et al., 2000, 2009; Minin et al., 2010).", "startOffset": 179, "endOffset": 258}, {"referenceID": 0, "context": "In perhaps the earliest work on monotonic neural networks, Archer and Wang (1993) adaptively down-weighted samples during training whose gradient updates would violate monotonicity, to produce a positive weighted neural net.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": "In perhaps the earliest work on monotonic neural networks, Archer and Wang (1993) adaptively down-weighted samples during training whose gradient updates would violate monotonicity, to produce a positive weighted neural net. Other researchers explicitly proposed constraining the weights to be positive in a single hidden-layer neural network with the sigmoid or other monotonic nonlinear transformation (Wang, 1994; Kay and Ungar, 2000; Dugas et al., 2000, 2009; Minin et al., 2010). Dugas et al. (2009) showed with simulations of four features and 400 training samples that both bias and variance were reduced by enforcing monotonicity.", "startOffset": 59, "endOffset": 505}, {"referenceID": 0, "context": "In perhaps the earliest work on monotonic neural networks, Archer and Wang (1993) adaptively down-weighted samples during training whose gradient updates would violate monotonicity, to produce a positive weighted neural net. Other researchers explicitly proposed constraining the weights to be positive in a single hidden-layer neural network with the sigmoid or other monotonic nonlinear transformation (Wang, 1994; Kay and Ungar, 2000; Dugas et al., 2000, 2009; Minin et al., 2010). Dugas et al. (2009) showed with simulations of four features and 400 training samples that both bias and variance were reduced by enforcing monotonicity. However, Daniels and Velikova (2010) showed this approach requires D hidden layers to arbitrarily approximate any D-dimensional monotonic function.", "startOffset": 59, "endOffset": 676}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples.", "startOffset": 0, "endOffset": 51}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint). Lauer and Bloch (2008), Riihim\u00e4ki and Vehtari (2010), and Neumann et al.", "startOffset": 0, "endOffset": 707}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint). Lauer and Bloch (2008), Riihim\u00e4ki and Vehtari (2010), and Neumann et al.", "startOffset": 0, "endOffset": 737}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint). Lauer and Bloch (2008), Riihim\u00e4ki and Vehtari (2010), and Neumann et al. (2013) encouraged extreme learning machines to be more monotonic by constraining the derivative of the function to be positive for a set of sampled points.", "startOffset": 0, "endOffset": 764}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint). Lauer and Bloch (2008), Riihim\u00e4ki and Vehtari (2010), and Neumann et al. (2013) encouraged extreme learning machines to be more monotonic by constraining the derivative of the function to be positive for a set of sampled points. Qu and Hu (2011) did a small-scale comparison of encouraging monotonicity by constraining input pairs to be monotonic, encouraging monotonic neural nets by constraining the function\u2019s derivatives at a subset of samples (analogous to Lauer and Bloch (2008)), and using a sigmoidal function with positive weights.", "startOffset": 0, "endOffset": 930}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint). Lauer and Bloch (2008), Riihim\u00e4ki and Vehtari (2010), and Neumann et al. (2013) encouraged extreme learning machines to be more monotonic by constraining the derivative of the function to be positive for a set of sampled points. Qu and Hu (2011) did a small-scale comparison of encouraging monotonicity by constraining input pairs to be monotonic, encouraging monotonic neural nets by constraining the function\u2019s derivatives at a subset of samples (analogous to Lauer and Bloch (2008)), and using a sigmoidal function with positive weights.", "startOffset": 0, "endOffset": 1169}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint). Lauer and Bloch (2008), Riihim\u00e4ki and Vehtari (2010), and Neumann et al. (2013) encouraged extreme learning machines to be more monotonic by constraining the derivative of the function to be positive for a set of sampled points. Qu and Hu (2011) did a small-scale comparison of encouraging monotonicity by constraining input pairs to be monotonic, encouraging monotonic neural nets by constraining the function\u2019s derivatives at a subset of samples (analogous to Lauer and Bloch (2008)), and using a sigmoidal function with positive weights. They concluded the positive-weight sigmoidal function is best. Sill (1998) proposed a guaranteed monotonic neural network with two hidden layers by requiring the first linear layer\u2019s weights to be positive, using hidden nodes that take the maximum of groups of first layer variables, and a second hidden layer that takes the minimum of the maxima.", "startOffset": 0, "endOffset": 1300}, {"referenceID": 0, "context": "Abu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint). Lauer and Bloch (2008), Riihim\u00e4ki and Vehtari (2010), and Neumann et al. (2013) encouraged extreme learning machines to be more monotonic by constraining the derivative of the function to be positive for a set of sampled points. Qu and Hu (2011) did a small-scale comparison of encouraging monotonicity by constraining input pairs to be monotonic, encouraging monotonic neural nets by constraining the function\u2019s derivatives at a subset of samples (analogous to Lauer and Bloch (2008)), and using a sigmoidal function with positive weights. They concluded the positive-weight sigmoidal function is best. Sill (1998) proposed a guaranteed monotonic neural network with two hidden layers by requiring the first linear layer\u2019s weights to be positive, using hidden nodes that take the maximum of groups of first layer variables, and a second hidden layer that takes the minimum of the maxima. The resulting surface is piecewise linear, and as such can represent any continuous differentiable function arbitrarily well. The resulting objective function is not strictly convex, but the authors propose training such monotonic networks using gradient descent where samples are associated with one active hyperplane at each iteration. Daniels and Velikova (2010) generalized this approach to handle the \u201cpartially monotonic\u201d case that the function is only monotonic with respect to some features.", "startOffset": 0, "endOffset": 1939}, {"referenceID": 66, "context": "But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003).", "startOffset": 205, "endOffset": 226}, {"referenceID": 18, "context": "Similar pre-processing of samples can be used to encourage any subsequently trained classifier to be more monotonic (Feelders, 2010).", "startOffset": 116, "endOffset": 132}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey.", "startOffset": 29, "endOffset": 50}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression.", "startOffset": 29, "endOffset": 505}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time. The isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples.", "startOffset": 29, "endOffset": 822}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time. The isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples.", "startOffset": 29, "endOffset": 860}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time. The isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples. Like isotonic regression, the output is a re-labeling of the original samples, the solution is at least O(n3) in the general case, and evaluation time is O(n). Ben-David et al. (1989); Ben-David (1992) constructed a monotonic rule-based classifier by sequentially adding training examples (each of which defines a rule) that do not violate monotonicity restrictions.", "startOffset": 29, "endOffset": 1122}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time. The isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples. Like isotonic regression, the output is a re-labeling of the original samples, the solution is at least O(n3) in the general case, and evaluation time is O(n). Ben-David et al. (1989); Ben-David (1992) constructed a monotonic rule-based classifier by sequentially adding training examples (each of which defines a rule) that do not violate monotonicity restrictions.", "startOffset": 29, "endOffset": 1140}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time. The isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples. Like isotonic regression, the output is a re-labeling of the original samples, the solution is at least O(n3) in the general case, and evaluation time is O(n). Ben-David et al. (1989); Ben-David (1992) constructed a monotonic rule-based classifier by sequentially adding training examples (each of which defines a rule) that do not violate monotonicity restrictions. Duivesteijn and Feelders (2008) proposed re-labeling samples before applying nearest neighbors based on a monotonicity violation graph with the training examples at the vertices.", "startOffset": 29, "endOffset": 1337}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time. The isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples. Like isotonic regression, the output is a re-labeling of the original samples, the solution is at least O(n3) in the general case, and evaluation time is O(n). Ben-David et al. (1989); Ben-David (1992) constructed a monotonic rule-based classifier by sequentially adding training examples (each of which defines a rule) that do not violate monotonicity restrictions. Duivesteijn and Feelders (2008) proposed re-labeling samples before applying nearest neighbors based on a monotonicity violation graph with the training examples at the vertices. Coupled with a proposed modified version of k-NN, they can enforce monotonic outputs. Similar pre-processing of samples can be used to encourage any subsequently trained classifier to be more monotonic (Feelders, 2010). Similarly, Kotlowski and Slowinski (2009) try to solve the isotonic regression problem to re-label the dataset to be monotonic, then fit a monotonic ensemble of rules to the re-labeled data, requiring zero training error.", "startOffset": 29, "endOffset": 1746}, {"referenceID": 3, "context": "This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples. Mukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time. The isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples. Like isotonic regression, the output is a re-labeling of the original samples, the solution is at least O(n3) in the general case, and evaluation time is O(n). Ben-David et al. (1989); Ben-David (1992) constructed a monotonic rule-based classifier by sequentially adding training examples (each of which defines a rule) that do not violate monotonicity restrictions. Duivesteijn and Feelders (2008) proposed re-labeling samples before applying nearest neighbors based on a monotonicity violation graph with the training examples at the vertices. Coupled with a proposed modified version of k-NN, they can enforce monotonic outputs. Similar pre-processing of samples can be used to encourage any subsequently trained classifier to be more monotonic (Feelders, 2010). Similarly, Kotlowski and Slowinski (2009) try to solve the isotonic regression problem to re-label the dataset to be monotonic, then fit a monotonic ensemble of rules to the re-labeled data, requiring zero training error. They showed overall better performance than the ordinal learning model of Ben-David et al. (1989) and isotonic separation (Chandrasekaran et al.", "startOffset": 29, "endOffset": 2024}, {"referenceID": 20, "context": "Before proposing monotonic lattice regression, we review lattice regression (Garcia and Gupta, 2009; Garcia et al., 2012).", "startOffset": 76, "endOffset": 121}, {"referenceID": 22, "context": "Before proposing monotonic lattice regression, we review lattice regression (Garcia and Gupta, 2009; Garcia et al., 2012).", "startOffset": 76, "endOffset": 121}, {"referenceID": 22, "context": "For image processing applications with only two to four features, much larger values of Md were needed (Garcia et al., 2012).", "startOffset": 103, "endOffset": 124}, {"referenceID": 22, "context": "Higher-order basis functions like the popular cubic spline will lead to smoother and potentially slightly more accurate functions (Garcia et al., 2012).", "startOffset": 130, "endOffset": 151}, {"referenceID": 27, "context": "The multilinear interpolation weights given in (2) are the maximum entropy solution to (5) (Gupta et al., 2006), and thus have good noise averaging and smoothness properties compared to other solutions.", "startOffset": 91, "endOffset": 111}, {"referenceID": 61, "context": "Historically, people created look-up tables by first fitting a function h(x) to the {xi, yi} using a regression algorithm such as a neural net or local linear regression, and then evaluating h(x) on a regular grid to produce the look-up table values (Sharma and Bala, 2002).", "startOffset": 250, "endOffset": 273}, {"referenceID": 20, "context": "Garcia and Gupta (2009) proposed directly optimizing the look-up table parameters \u03b8 to minimize the empirical error between the training labels and the interpolated look-up table:", "startOffset": 0, "endOffset": 24}, {"referenceID": 20, "context": "Prior work focused on squared error loss, and used graph regularizers R(\u03b8) of the form bTKb for some PSD matrix K, in which case (6) has a closed-form solution which can be computed with sparse matrix inversions (Garcia and Gupta, 2009; Garcia et al., 2010, 2012).", "startOffset": 212, "endOffset": 263}, {"referenceID": 42, "context": "The approach extends to the standard learning to rank from pairs problem (Liu, 2011), where the training data is pairs of samples xi and x \u2212 i and the goal is to learn a function such that f(xi ) \u2265 f(x \u2212 i ) for as many pairs as possible.", "startOffset": 73, "endOffset": 84}, {"referenceID": 61, "context": "Interpolating a look-up table has long been considered an efficient way to specify and evaluate a low-dimensional non-linear function (Sharma and Bala, 2002; Garcia et al., 2012).", "startOffset": 134, "endOffset": 178}, {"referenceID": 22, "context": "Interpolating a look-up table has long been considered an efficient way to specify and evaluate a low-dimensional non-linear function (Sharma and Bala, 2002; Garcia et al., 2012).", "startOffset": 134, "endOffset": 178}, {"referenceID": 2, "context": "Simplex interpolation is also known as the Lovasz extension in submodular optimization, where it is used to extend a function defined on the vertices of a unit hypercube to be defined on its interior (Bach, 2013).", "startOffset": 200, "endOffset": 212}, {"referenceID": 33, "context": "Many different linear interpolation strategies have been proposed to interpolate look-up tables using only a subset of the 2D vertices (for a review, see Kang (1997)).", "startOffset": 154, "endOffset": 166}, {"referenceID": 33, "context": "Many different linear interpolation strategies have been proposed to interpolate look-up tables using only a subset of the 2D vertices (for a review, see Kang (1997)). However, with most strategies it is too computationally expensive to determine exactly which of the vertices should be used to interpolate each x. The wonder of simplex interpolation is that it takes only O(D logD) operations to determine the D+1 vertices needed to interpolate any given x, and then only O(D) operations to interpolate the identified D + 1 vertices. A comparison of simplex and multilinear interpolation is given in Figure 3 for the same look-up table parameters. Simplex interpolation was proposed in the color management literature by Kasson et al. (1993), and independently later by Rovatti et al.", "startOffset": 154, "endOffset": 743}, {"referenceID": 33, "context": "Many different linear interpolation strategies have been proposed to interpolate look-up tables using only a subset of the 2D vertices (for a review, see Kang (1997)). However, with most strategies it is too computationally expensive to determine exactly which of the vertices should be used to interpolate each x. The wonder of simplex interpolation is that it takes only O(D logD) operations to determine the D+1 vertices needed to interpolate any given x, and then only O(D) operations to interpolate the identified D + 1 vertices. A comparison of simplex and multilinear interpolation is given in Figure 3 for the same look-up table parameters. Simplex interpolation was proposed in the color management literature by Kasson et al. (1993), and independently later by Rovatti et al. (1998). Simplex interpolation is also known as the Lovasz extension in submodular optimization, where it is used to extend a function defined on the vertices of a unit hypercube to be defined on its interior (Bach, 2013).", "startOffset": 154, "endOffset": 793}, {"referenceID": 60, "context": "This decomposition can also be described by the hyperplanes xk = xr for 1 \u2264 k \u2264 r \u2264 D (Schimdt and Simon, 2007).", "startOffset": 86, "endOffset": 111}, {"referenceID": 38, "context": "Knop (1973) discussed this decomposition as a special case of Eulerian partitioning of the hypercube, and Mead (1979) showed this is the smallest possible equivolume decomposition of the unit hypercube.", "startOffset": 0, "endOffset": 12}, {"referenceID": 38, "context": "Knop (1973) discussed this decomposition as a special case of Eulerian partitioning of the hypercube, and Mead (1979) showed this is the smallest possible equivolume decomposition of the unit hypercube.", "startOffset": 0, "endOffset": 118}, {"referenceID": 59, "context": "The general formula is detailed in Algorithm 2; for more mathematical details see Rovatti et al. (1998).", "startOffset": 82, "endOffset": 104}, {"referenceID": 68, "context": "For example, in a comparison by Sun and Zhou (2012) for the three-dimensional regression problem of color managing an LCD monitor, multilinear interpolation of a 9 \u00d7 9 \u00d7 9 look-up table (also called trilinear interpolation in the special case of three-dimensions) produced around 1% worse average error than simplex interpolation, but the maximum error with multilinear interpolation was only 60% of the", "startOffset": 32, "endOffset": 52}, {"referenceID": 34, "context": "Another study by Kang (1995) using simulations concluded that the interpolation errors of these methods was \u201cabout the same.", "startOffset": 17, "endOffset": 29}, {"referenceID": 20, "context": "Figure 6 illustrates the torsion regularizer and compares it to previously proposed lattice regularizers, the standard graph Laplacian (Garcia and Gupta, 2009) and graph Hessian (Garcia et al.", "startOffset": 135, "endOffset": 159}, {"referenceID": 22, "context": "Figure 6 illustrates the torsion regularizer and compares it to previously proposed lattice regularizers, the standard graph Laplacian (Garcia and Gupta, 2009) and graph Hessian (Garcia et al., 2012).", "startOffset": 178, "endOffset": 199}, {"referenceID": 31, "context": "Our approach is similar to the work of Howard and Jebara (2007), which jointly learns monotonic piecewise linear one-dimensional transformations and a linear function.", "startOffset": 39, "endOffset": 64}, {"referenceID": 68, "context": "Other researchers have also considered joint training of classifiers and imputations for missing data, for example van Esbroeck et al. (2014) and Liao et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 41, "context": "(2014) and Liao et al. (2007).", "startOffset": 11, "endOffset": 30}, {"referenceID": 49, "context": "The number of iterations needed for the SGD to converge depends on the squared Euclidean norms of the stochastic subgradients (Nemirovski et al., 2009), with larger norms resulting in slower convergence.", "startOffset": 126, "endOffset": 151}, {"referenceID": 13, "context": "We reduce the variance of the stochastic subgradient\u2019s loss term by mini-batching over multiple random samples (Dekel et al., 2012).", "startOffset": 111, "endOffset": 131}, {"referenceID": 7, "context": "A more complicated implementation of this strategy would use the alternating direction method of multipliers with a consensus constraint (Boyd et al., 2010), but that requires an additional regularization towards a local copy of the most recent consensus parameters.", "startOffset": 137, "endOffset": 156}, {"referenceID": 43, "context": "This parallelize-andaverage approach was investigated for large-scale training of linear models by Mann et al. (2009). Their results showed similar accuracies to distributed gradient descent, but 1000\u00d7 less network traffic and reduced wall-clock time for large datasets.", "startOffset": 99, "endOffset": 118}, {"referenceID": 10, "context": "An optimal approach we compared with for handling large-scale constraints is called LightTouch (Cotter et al., 2015).", "startOffset": 95, "endOffset": 116}, {"referenceID": 10, "context": "An optimal approach we compared with for handling large-scale constraints is called LightTouch (Cotter et al., 2015). At each iteration, LightTouch does not project onto any constraints, but rather moves the constraints into the objective, and applies a random subset of constraints each iteration as stochastic gradient updates to the parameters, where the distribution over the constraints is learned as the optimization proceeds to focus on constraints that are more likely to be active. This replaces the per-iteration projections with cheap gradient updates. Intermediate solutions may not satisfy all the constraints, but one full projection is performed at the very end to ensure final satisfaction of the constraints. Experimentally, we found LightTouch to generally lead to faster convergence (see Cotter et al. (2015) for its theoretical convergence rate), while producing similar experimental results to the above approximate projected SGD.", "startOffset": 96, "endOffset": 828}, {"referenceID": 14, "context": "One can generally improve the speed of SGD with adagrad (Duchi et al., 2011), even for nonconvex problems (Gupta et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 26, "context": ", 2011), even for nonconvex problems (Gupta et al., 2014).", "startOffset": 37, "endOffset": 57}, {"referenceID": 19, "context": "Random forests is an unconstrained method that consistently provides competitive results on benchmark datasets, compared to many other types of machine learning methods (Fernandez-Delgado et al., 2014)).", "startOffset": 169, "endOffset": 201}, {"referenceID": 11, "context": "This problem is also treated by Dalvi et al. (2014), where they focus on defining a good title similarity.", "startOffset": 32, "endOffset": 52}, {"referenceID": 33, "context": "Also, using bottom-clicked pairs removes the trust bias that users know they are being presented with a ranked list and prefer samples that are rankedhigher (Joachims et al., 2005).", "startOffset": 157, "endOffset": 180}, {"referenceID": 33, "context": "This choice is consistent with the findings of Joachims et al. (2005), whose eye-tracking experiments on webpage search results showed that users on average look at least at one result above the clicked result, and that these pairs of preferred/unpreferred samples correlated strongly with explicit relevance judgements.", "startOffset": 47, "endOffset": 70}, {"referenceID": 43, "context": "Magdon-Ismail and Sill (2008) point out that using the linear regression coefficients for this purpose can be misleading if features are correlated and not jointly Gaussian.", "startOffset": 0, "endOffset": 30}, {"referenceID": 8, "context": "If more than three (or more) vertices are used for a feature, then one can enforce Brooks\u2019 Law (Brooks, 1975), in which the function is constrained to first increase but then decrease as one input is increased, all other inputs held constant.", "startOffset": 95, "endOffset": 109}], "year": 2016, "abstractText": "Real-world machine learning applications may require functions to be fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of the learned function with respect to some of the inputs can be critical to user confidence. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to train monotonic functions by adding linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and random sampling of additive regularizer terms. Case studies on real-world problems with up to sixteen features and up to hundreds of millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy in practice while providing greater transparency to users.", "creator": "LaTeX with hyperref package"}}}