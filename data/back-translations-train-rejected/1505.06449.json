{"id": "1505.06449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2015", "title": "Efficient Elastic Net Regularization for Sparse Linear Models", "abstract": "We extend previous work on efficiently training linear models by applying stochastic updates to non-zero features only, lazily bringing weights current as needed. To date, only the closed form updates for the $\\ell_1$, $\\ell_{\\infty}$, and the rarely used $\\ell_2$ norm have been described. We extend this work by showing the proper closed form updates for the popular $\\ell^2_2$ and elastic net regularized models. We show a dynamic programming algorithm to calculate the proper elastic net update with only one constant-time subproblem computation per update. Our algorithm handles both fixed and decreasing learning rates and we derive the result for both stochastic gradient descent (SGD) and forward backward splitting (FoBoS). We empirically validate the algorithm, showing that on a bag-of-words dataset with $260,941$ features and $88$ nonzero features on average per example, our method trains a logistic regression classifier with elastic net regularization $612$ times faster than an otherwise identical implementation with dense updates.", "histories": [["v1", "Sun, 24 May 2015 15:42:58 GMT  (12kb)", "http://arxiv.org/abs/1505.06449v1", null], ["v2", "Tue, 26 May 2015 07:28:50 GMT  (12kb)", "http://arxiv.org/abs/1505.06449v2", null], ["v3", "Thu, 2 Jul 2015 20:44:57 GMT  (13kb)", "http://arxiv.org/abs/1505.06449v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zachary c lipton", "charles elkan"], "accepted": false, "id": "1505.06449"}, "pdf": {"name": "1505.06449.pdf", "metadata": {"source": "CRF", "title": "Efficient Elastic Net Regularization for Sparse Linear Models", "authors": ["Zachary C. Lipton", "Charles Elkan"], "emails": ["elkan}@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 150 5.06 449v 1 [cs.L G"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Background and Definitions", "text": "We look at a data matrix X-Rn-d, where each row corresponds to one of n examples, and each column indexed by j corresponds to one of d characteristics. We want a linear model, parameterized by a vector w-Rd, that minimizes a convex objective function F (w), which can be expressed as \u2211 ni = 1 Fi (w), in which F returns the loss in relation to the entire corpus X, and Fi the loss calculated using example xi.In many data sets, the vast majority of entries xij is null. The representation of text is one such case. We say that such data sets are sparse. If characteristics correspond to counting or binary values, as in dictionaries, we sometimes say that a zero value attribute xij is missing. We use p to refer to the average number of non-zero characteristics per example."}, {"heading": "2.1 Regularization", "text": "In order to avoid overadjustment, regulation limits the freedom of parameters of a model and penalizes its distance from previous assumptions. The most frequently used regulators punish large weights with an objective function of the formula F (w) = L (w) + R (w) (1) Many frequently used regulators R (w) have the form \u03bb | | w | |, where \u03bb determines the strength of the regulation and that the standard is often chosen by formulas 0, 1, 2, 22. Formula 1 regulators are popular due to their tendency to produce sparse models. In this paper, we focus on the elastic mesh, a linear combination of sizes 1 and 22 that has been shown to produce comparatively sparse models up to class 1, while often achieving superior accuracy [12]."}, {"heading": "2.2 Stochastic Gradient Descent", "text": "& # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & & & # 10; & & & # 10; & & # 10; & & & # 10; & & # 10; & # 10; & & # 10; & # 10; & & # 10; & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & & # 10; & & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & # 10; & # 10; & & # 10; & & # 10; & & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & & & # 10; & & & & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & & & # 10; & # 10; & # 10; & & & & # 10; & & # 10; & & & # 10; & & & # 10; & & # 10; & # 10; & & # 10; & & # 10; & & & & & # 10; & & &"}, {"heading": "2.3 Forward Backward Splitting", "text": "Proximal algorithms are an approach to optimization where each update consists of solving a convex optimization problem [8]. Forward Backlitting (FoBoS) [10] uses proximal algorithms to provide a principled approach to online optimization with uneven regularizers. We first move toward the negative gradient of the differentiable loss function. We then update the weight by solving a convex optimization problem that simultaneously penalizes the removal of the new parameters and minimizes the regularization date. Duchi and Singer established the convergence of the forward-backward method in [10]. First, a standard uneven stochastic gradient step is applied (t + 1 2) = w (t) \u2212 \u03b7 (t) \u2022 wLi (2). Then, a convex optimization that applies the regulatory penalty is solved."}, {"heading": "3 Lazy Updates", "text": "We expand the method for lazy updates, which is complicated in [2], [6] and [10] in the case of [2] and [4] complicated net updates. The essence of the algorithm for sparse lazy updates is as follows (algorithm 1). We maintain an array in which each element stores the index of the last iteration, at which each element was non-zero, and a counter that stores the index of the current iteration. In processing each example xi, we iterate through the unequal properties xij, comparing the current iteration index k with the current iteration index k. For each element with unequal weight distribution wj, we apply the k \u2212 j successive updates in constant time and bring these weights up to date. On the basis of the updated weights, we calculate the prediction y (k) with the current parameters w (k). We then calculate the grades and update the weights."}, {"heading": "4 Prior Work", "text": "Notable contributions include methods for adaptive attenuation of the learning rate for each parameter, such as AdaGrad [3] and AdaDelta [11], which use small batches to reduce the variance of the loud gradient ([7]), and methods for reducing variance, such as Stochastic Average Gradient (SAG) [9] and Stochastic Variance Reduced Gradient (SVRG) [5]. In 2008, Carpenter described an idea for performing lazy updates for stochastic gradient lineage [2]. In his method, they obtain a vector that stores the index of the last epoch in which each weight was last regulated, and then perform regular batch updates. However, they acknowledge, they lead to updates that do not produce the same result as applying an update after each time.Langford et al."}, {"heading": "5 Closed Form Lazy Updates for SGD", "text": "In this section, when processing examples from sparse data sets, we will derive stochastic gradient updates with constant time complexity, with which the lazy update algorithm can train linear models with time complexity O (p). Shortly, we will describe the more general case in which the learning rate varies. If the learning rate is constant, the algorithm can easily be modified to have a spatial complexity of O (1)."}, {"heading": "5.1 Lazy SGD Update \u21131 Regularization with Attenuated Learning Rate", "text": "The closed format update of the regulated models was derived by Singer and Duchi [10]. The constant time-lazy update can be applied with: w (k) j = sgn (w (ig) j) [| w (ig) j | -\u03bb1 (S (k \u2212 1) \u2212 S (n) j \u2212 1) +, where S (t) is a function that returns the subtotal as S (t) = 0 \u03b7 (ig). The base case for this recursion is S (0) = \u03b7 (0). For each iteration, we calculate S (t) in constant time taking into account its predecessor as S (t) = \u03b7 (t) + S (t \u2212 1). The base case for this recursion is S (0) = \u03b7 (0). We then calculate this value in an array for the subsequent constant time observation."}, {"heading": "5.2 Lazy SGD Update for \u211322 Regularization with Attenuated Learning Rate", "text": "For example (if a trait is xij = 0 and the learning rate varies, the stochastic gradient update rule applies to a \"22 regularized target isw (t + 1) j = w (t) j \u2212 \u03b7 (t) 2 (t) 2 \u00b7 w (t) j (5). In view of successive updates, the decreasing learning rate prevents a collection of lazy updates as terms in a geometric series, as we could if the learning rate were fixed. However, we can apply a dynamic programming strategy similar to the system we have shown for\" 1 \"with attenuated learning rate."}, {"heading": "5.3 Lazy SGD Update for Elastic Net Regularization with Attenuated Learning Rate", "text": "Finally, we derive the constant time lazy update for SGD with elastic net regulation. (...) We remember that a model regulated by elastic mesh n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n (n) n (n) n (n (n) n n (n) n (n) n (n) n (n) n (n (n) n n n (n) n n (n) n n (n) n (n) n n (n) n n n (n) n n n (n) n n n (n) n) n (n) n (n) n (n) n n (n) n n n n (n) n n (n) n) n (n n n n (n) n) n n (n) n n n n (n) n) n (n) n n (n n n n n) n (n n) n n (n n) n n n (n) n n (n) n (n) n n (n) n n (n) n n (n) n (n n n) n (n n n) n (n) n n) n (n) n (n (n) n n n n (n) n n) n (n) n n n (n (n) n (n) n (n) n n (n) n) n (n) n (n (n n) n (n n n n n) n) n (n) n (n (n) n) n) n (n) n (n) n (n) n (n (n) n) n n n (n) n n n (n (n) n) n"}, {"heading": "6 Lazy Updates for Forward Backward Splitting", "text": "Remember the optimisation problems in the regulation of track 1, 22 and elastic networks (section 2.3) and the lazy update in the regulation of equals (equation 4), so we are turning our attention to the FoBoS updates for track 22 and elastic networks."}, {"heading": "6.1 Lazy FoBoS Update for \u211322 Regularization", "text": "In order to apply the regularization update, we solve the problem of Equation 3 with \u03bb1 to 0. The solution of forw \u043a results in the simple updatew (t + 1) j = w (t) j / (1 + \u03b7 (t) \u03bb2) whenever xij = 0. Note that this differs from the standard stochastic gradient descending stage. We can store the values P \u2032 (t) = \u0443 t = 0 1 1 + \u03b7t\u03bb2. Therefore, constant time-lazy updating for FoBoS with B \u00b2 22 regularization in order to cause a weight current at time k out of time, which is the case P \u2032 (\u2212 1) jP \u2032 (k \u2212 1) P \u2032 (\u04411) P \u2032 (\u03bb1) (15), where P \u2032 (t) = (1 + \u03b7 (t) \u03bb2) 1 \u00b7 P \u2032 (t \u2212 1) with the base case P \u2032 (\u2212 1) = 1."}, {"heading": "6.2 Lazy FoboS Update for Elastic Net Regularization", "text": "Finally, we solve the convex optimization problem from Equation 3. This problem also solves and can be optimized separately for each wj. Determining the derivative program wt wj yields the following optimal solution: w (t + 1) j = sgn (t) j (t) j | \u2212 p (t) p + 1] j (w) j + 1] Theorem 2. The exact constant update for FoBoS with elastic net regulation and attenuated learning rate, in due course k (t) j isw (k) j (t) j (w) j \u2212 p) j) j (k \u2212 p (k \u2212 1) P \u2032 (k \u2212 1) p \u2032 (P \u2032 1) \u2212 t (k \u2212 t) \u00b7 p (k \u2212 t) \u00b7 t (n)."}, {"heading": "7 Experiments", "text": "However, in order to confirm the accuracy and speed of our dynamic programming solution, we tested the system using a verbose representation of abstractions from biomedical articles indexed in Medline. Our data set contained 1,000 000 examples, 260,941 features and 88.54 non-zero attributes per documentWe prototyped the system in Python. Scanty data sets are presented as dictionary lists in which keys correspond to feature indices and values. We implemented both standard and lazy FoBoS attributes for logistic regressions regulated by elastic mesh. We confirmed on a synthetic data set that standard FoBoS updates and rotten updates provide identical weights up to 4 significant numbers. To make a fair comparison, both systems take advantage of the economy in calculating predictions."}, {"heading": "8 Discussion", "text": "Many interesting datasets are high-dimensional, and many high-dimensional datasets are sparse. To be fast, algorithms should be scaled based on the number of non-zeros per example, not on nominal dimensionality. We have analytically and empirically demonstrated an algorithm for rapid learning on linear models regulating the elastic mesh and its correctness."}, {"heading": "Acknowledgments", "text": "This research was carried out with the generous support of the Department of Biomedical Informatics at the University of California, San Diego, which funded the lead author through a National Library of Medicine education fellowship. Galen Andrew began evaluating lazy updates for multi-label classification with Charles Elkan in the summer of 2014, and his notes provided a revealing starting point for this research. Sharad Vikram provided invaluable assistance in verifying the derivatives of closed-form updates."}], "references": [{"title": "Stochastic gradient descent tricks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression", "author": ["Bob Carpenter"], "venue": "Alias-i, Inc., Tech. Rep, pages", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["Mu Li", "Tong Zhang", "Yuqiang Chen", "Alexander J Smola"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Efficient learning using forward-backward splitting", "author": ["Yoram Singer", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "In this paper, we develop upon a method for lazily updating weights, first described by [2], [10], and [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "In this paper, we focus on elastic net, a linear combination of l1 and l2 regularization that has been shown to produce comparably sparse models to l1 while often achieving superior accuracy [12].", "startOffset": 191, "endOffset": 195}, {"referenceID": 1, "context": "An appropriately attenuated learning rate ensures both that the optimal parameters will eventually be reached and that the algorithm will yield a value within a distance \u01eb of the optimal value for any small value \u01eb [2] .", "startOffset": 215, "endOffset": 218}, {"referenceID": 0, "context": "t \u03b7t = \u221e and \u2211 t \u03b7 2 t < \u221e [1].", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "However, forward backward splitting (FoBoS) [10], offers a principled approach to this problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "Forward Backward Splitting (FoBoS) [10] uses proximal algorithms to provide a principled approach to online optimization with non-smooth regularizers.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "Duchi and Singer established the convergence of the Forward Backward method in [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "3 Lazy Updates We extend the method for lazy updates introduced in [2], [6], and [10] to the case of l2 and elastic net regularization.", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 9, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 215, "endOffset": 218}, {"referenceID": 7, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 294, "endOffset": 297}, {"referenceID": 4, "context": "Notable contributions include ways of adaptively attenuating the learning rate separately for each parameter such as AdaGrad [3] and AdaDelta [11], using small batches to reduce the variance of the noisy gradient ( [7]), and variance reduction methods such as Stochastic Average Gradient (SAG) [9], and Stochastic Variance Reduced Gradient (SVRG) [5].", "startOffset": 347, "endOffset": 350}, {"referenceID": 1, "context": "In 2008 Carpenter described an idea for performing lazy updates for stochastic gradient descent [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "concurrently developed a similar approach for lazily updating l1 regularized linear models [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "In 2009, Duchi and Singer describe a method they call Forward Backward Splitting (FoBoS) for online optimization of a regularized loss function [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "1 Lazy SGD Update l1 Regularization with Attenuated Learning Rate The closed form update for l1 regularized models was derived by Singer and Duchi [10].", "startOffset": 147, "endOffset": 151}], "year": 2017, "abstractText": "We extend previous work on efficiently training linear models by applying stochastic updates to non-zero features only, lazily bringing weights current as needed. To date, only the closed form updates for the l1, l\u221e, and the rarely used l2 norm have been described. We extend this work by showing the proper closed form updates for the popular l22 and elastic net regularized models. We show a dynamic programming algorithm to calculate the proper elastic net update with only one constant-time subproblem computation per update. Our algorithm handles both fixed and decreasing learning rates and we derive the result for both stochastic gradient descent (SGD) and forward backward splitting (FoBoS). We empirically validate the algorithm, showing that on a bag-of-words dataset with 260, 941 features and 88 nonzero features on average per example, our method trains a logistic regression classifier with elastic net regularization 612 times faster than an otherwise identical implementation with dense updates.", "creator": "LaTeX with hyperref package"}}}