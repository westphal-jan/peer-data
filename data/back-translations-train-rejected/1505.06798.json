{"id": "1505.06798", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2015", "title": "Accelerating Very Deep Convolutional Networks for Classification and Detection", "abstract": "This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs that have substantially impacted the computer vision community. Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while current methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., &gt;=10) layers are approximated. For the widely used very deep VGG-16 model, our method achieves a whole-model speedup of 4x with merely a 0.3% increase of top-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the latest Fast R-CNN detector.", "histories": [["v1", "Tue, 26 May 2015 03:30:59 GMT  (294kb,D)", "http://arxiv.org/abs/1505.06798v1", "Technical report. arXiv admin note: substantial text overlap witharXiv:1411.4229"], ["v2", "Wed, 18 Nov 2015 06:16:59 GMT  (308kb,D)", "http://arxiv.org/abs/1505.06798v2", "TPAMI, accepted. arXiv admin note: substantial text overlap witharXiv:1411.4229"]], "COMMENTS": "Technical report. arXiv admin note: substantial text overlap witharXiv:1411.4229", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["xiangyu zhang", "jianhua zou", "kaiming he", "jian sun"], "accepted": false, "id": "1505.06798"}, "pdf": {"name": "1505.06798.pdf", "metadata": {"source": "CRF", "title": "Accelerating Very Deep Convolutional Networks for Classification and Detection", "authors": ["Xiangyu Zhang", "Jianhua Zou", "Kaiming He", "Jian Sun"], "emails": ["kahe@microsoft.com", "jiansun@microsoft.com"], "sections": [{"heading": null, "text": "Index Terms - Convolutionary Neural Networks, Acceleration, Image Classification, Object RecognitionF"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves if they do not. (...) It is not so that they do it. (...) It is not so that they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...). \"(...) It is as if they do it. (...) It is as if they do it. (...).\" (...) It is as if they do it. \"(...).\" (...) It is so. \"(...).\" (...) It is. (...). \"(...).\" It is. (...). () It is. (...). \"It is. (...).\" It is. (...). \"It is. (...).\" It is. (...). \"It is. (...).\" It is. (. (...). \"It is. (...).\" It is. \"It is. (...). (. (...).\" It is. (). (). It is. (...). (. (). It is. It is. (). (. (...). It is. (. It is. It is. (). (. (...). It is. It is. (. (). It is. (. It is. (. (). It is. (). (). (. It is. It is. (). (. It is. It is. (). It is. (. It is. (. (). It is. (). It is. (). (. It is. (). (). It is. (. (). It is. (). It is. (. (). It is. (). It is. (). It is. It is. (. (). (. (). It is. It is. (. (). It is. (. (). It is. It is. (). (). ()."}, {"heading": "2 RELATED WORK", "text": "In fact, it is such that most people who are able to feel able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to move, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3 APPROACHES", "text": "We show that this decomposition has a closed solution (SVD) for linear neurons and a somewhat more complicated solution based on Generalized SVD (GSVD) for nonlinear neurons [27], [28], [29]. The simplicity of our solver allows an asymmetric reconstruction method to reduce accumulated errors in very deep models."}, {"heading": "3.1 Low-rank Approximation of Responses", "text": "Our assumption is that the filter response at one pixel of a layer is approximately on a low-level where we find an answer to a low-level layer. (b) An approximate layer with complex channels reduces to O (d). (b) An approximate layer with complexity reduces to O (d). (b) An approximate layer with complexity reduces to O (d). (b) An approximate layer with complexity reduces to O (d). (c) A layer with a filter size of k. (d). (c). (c). (c). (c). (c). (c). (c) The layer with a filter volume of k). (c). (c) The number of input channels in that layer is. To find an answer, this filter is applied to a k)."}, {"heading": "3.2 Nonlinear Case", "text": "Next, we will examine the case of the use of nonlinear units. We will use r () to name the nonlinear operator. In this essay, we will focus on the reconstructed linear unit (ReLU) [31]: r () = max (, 0). Drive by Eqn. (4), we will minimize the reconstruction error of nonlinear answers: min M, b) is the nonlinear answer calculated by the approximate filters. The above optimization problem is a challenge due to nonlinearity and low rank. To find a workable solution, we relax it as: min M, b, {zi}."}, {"heading": "3.3 Asymmetric Reconstruction for Multi-Layer", "text": "We propose an asymmetric reconstruction method to mitigate this problem. We apply our method sequentially to each layer, from the flatter layers to the deeper layers. Consider a layer whose input feature map is not precise due to the approximation of the previous layer / layer. We call the approximate input to the current layer x. For training data, we can still calculate its non-approximate reactions using y = Wx.Thus, we can optimize an \"asymmetric\" version of (5): min M, b, i, r (Wxi) \u2212 r (MWx, i + b) \u043222, (12) s.t. rank (M) \u2264 d \u2032. In the first term (Wx) = r (y) is the non-approximate output of this layer. In the second term, x \u0432i is the safe input to this layer, and in the second term (Wx) this layer (unrelatable) can (12) of this layer (x)."}, {"heading": "3.4 Rank Selection for Whole-Model Acceleration", "text": "In the above, the optimization is based on a target d \u2032 of each layer. d \u2032 is the only parameter that determines the complexity of an accelerated layer. However, given a desired acceleration ratio of the entire model, we must determine the correct rank d \u2032 for each layer. One can assume a uniform acceleration ratio for each layer. However, this is not an optimal solution, as the layers are not equally complex. 95969798100 \u2212 0.5 \u2212 0.4 \u2212 0.3 \u2212 0.10 PCA Accumulative Energy (%).Acc urac y (%) Conv2 Conv3 Conv4 Conv5 Conv7Figure 3: PCA Accumulative Energy and the accuracy rates (top-5).Here, the accuracy is evaluated using the linear solution (the non-linear solution).Each layer is independently evaluated, with other layers that are not approximate."}, {"heading": "3.5 Higher-Dimensional Decomposition", "text": "In our formulation, we focus on the reduction of channels (from d to d). There are algorithmic advantages of operating on the channel dimension. Firstly, this dimension can easily be controlled by the ranking constraint rank (M) \u2264 d. \"This constraint enables closed-form solutions, e.g. SVD or GSVD. Secondly, the optimized low-level projection M can be precisely broken down into low-dimensional filters (P and Q). These simple and closed-form solutions can produce good results with a very small subset of training images (3,000 out of a million). On the other hand, compared with decomposition methods that operate on multiple dimensions (spatial and channel) [16], our method must use a smaller d\" to approximate a given speeddup ratio that could limit the accuracy of our method. To avoid d \"being too small, we propose to combine our solution with Jaderberg et al.\""}, {"heading": "3.6 Fine-tuning", "text": "With any near complete model, we can \"refine\" this model end-to-end in the ImageNet training data. This process is similar to training a classification network with the approximate model as initialization. However, we find empirically that fine-tuning is very sensitive to the initialization (given by the approximate model) and the learning rate. If the initialization is bad and the learning rate is low, fine-tuning can easily get caught in a bad local optimum and makes little progress. If the learning rate is high, the fine-tuning process behaves very much like training the dismantled architecture \"from the ground up\" (as we will discuss later). A large learning rate can jump out of the initialized local optimum, and the initialization seems to be \"forgotten.\" Fortunately, our method has achieved very good accuracy even without fine-tuning, as we will show through experiments. With our approximate model as initialization, fine-tuning with a sufficiently small learning rate in the situation to further improve the net adjustment results."}, {"heading": "4 EXPERIMENTS", "text": "We evaluate our method comprehensively using two models: The first model is a 10-layer model of \"SPPnet (OverFeat-7)\" in [7], which we call \"SPP-10.\" This model (detailed in Table 1) has a similar architecture to the OverFeat model [6], but is deeper. It has 7 conv layers and 3 fc layers. The second model is the publicly available VGG-16 model [1] 1, the 131. www.robots.ox.ac.uk / \u0445 vgg / research / very deep / conv layers and has 3 fc layers. SPP-10 won 3rd place and VGG-16 2nd place in ILSVRC 2014 [18]. We evaluate the \"Top 5 Error\" based on single-view tests. The view is the central 224 x 224 region cut from the modified image, the shorter page of which is 256. The error rate in the single view of SP10-10% is increased by 12.51% in the Validation in the image values."}, {"heading": "4.1 Experiments with SPP-10", "text": "We first evaluate the impact of our individual steps on the SPP-10 model through a series of controlled experiments, unless we use the 3-D decomposition.Single Layer: Linear vs. Nonlinear In this subrange, we evaluate the approximate performance of a single approximate layer, the remaining layers are unchanged and not approximately.The performance is presented as an increase in error rates (which include only a single layer) over the speedup ratio of that layer. Fig. 4 shows that the nonlinear solution consistently works better than the linear solution. In Table 1, we2. http: / www.vllat.org / matvlat.net / pretrained the thrift (the part of zero activations according to LU) of each layer is zero activation."}, {"heading": "4.2 Experiments with VGG-16", "text": "The very deep VGG models [1] have significantly improved a wide range of visual recognition tasks, including object detection [9], [10], semantic segmentation [11], [12], [13], [40], captions [41], [43], video / action detection [44], image question response [45], texture detection [46], etc. Given the large impact of this model, we believe it is practical to accelerate this model. Acceleration of VGG-16 for ImageNet ClassificationFirst we discover that our entire model selection is particularly important for accelerating VGG16. In Table 6 we show the results without / with ranking selection. No 3d decomposition is used in this comparison. For a 4 \u00d7 speedup, ranking selection reduces the increased error from 6.38% to 3.84%."}, {"heading": "5 CONCLUSION", "text": "We have presented an acceleration method for very deep networks, which is evaluated on the basis of acceleration ratios in the overall model. Thanks to the nonlinear asymmetric reconstruction, our method can effectively reduce the accumulated errors of several layers. Competitive accelerations and accuracy are demonstrated in the complex ImageNet classification task and the PASCAL VOC object recognition task."}], "references": [{"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "arXiv:1504.08083, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, 1989.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "International Conference on Learning Representations (ICLR), 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Object detection networks on convolutional feature maps", "author": ["S. Ren", "K. He", "R. Girshick", "X. Zhang", "J. Sun"], "venue": "arXiv:1504.06066, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional feature masking for joint object and stuff segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving the speed of neural networks on CPUs", "author": ["V. Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference (BMVC), 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "arXiv:1409.0575, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient and accurate approximations of nonlinear convolutional networks", "author": ["X. Zhang", "J. Zou", "X. Ming", "K. He", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning separable filters", "author": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast convolutional nets with fbfft: A gpu performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv:1312.5851, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks at constrained time cost", "author": ["K. He", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory bounded deep convolutional networks", "author": ["M.D. Collins", "P. Kohli"], "venue": "arXiv:1412.1442, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized constrained redundancy analysis", "author": ["Y. Takane", "S. Jung"], "venue": "Behaviormetrika, pp. 179\u2013192, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularized linear and kernel redundancy analysis", "author": ["Y. Takane", "H. Hwang"], "venue": "Computational Statistics & Data Analysis, pp. 394\u2013405, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Matrix computations", "author": ["G.H. Golub", "C.F. van Van Loan"], "venue": "1996.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "International Conference on Machine Learning (ICML), 2010, pp. 807\u2013814.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "A new alternating minimization algorithm for total variation image reconstruction", "author": ["Y. Wang", "J. Yang", "W. Yin", "Y. Zhang"], "venue": "SIAM Journal on Imaging Sciences, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimized product quantization", "author": ["T. Ge", "K. He", "Q. Ke", "J. Sun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse projections for highdimensional binary codes", "author": ["Y. Xia", "K. He", "P. Kohli", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Modern heuristic techniques for combinatorial problems", "author": ["C.R. Reeves"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1993}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1502.01852, 2015.  14", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1852}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference (BMVC), 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "arXiv:1503.01640, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "International Conference on Machine Learning (ICML), 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "ICML 2015 Deep Learning Workshop, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Abstract\u2014This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs [1] that have substantially impacted the computer vision community.", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "For the widely used very deep VGG-16 model [1], our method achieves a whole-model speedup of 4\u00d7 with merely a 0.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Our 4\u00d7 accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the latest Fast R-CNN detector [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "The accuracy of convolutional neural networks (CNNs) [3], [4] has been continuously improving [5], [6], [7], [1], [8], but the computational cost of these networks also increases significantly.", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 143, "endOffset": 147}, {"referenceID": 3, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 194, "endOffset": 197}, {"referenceID": 4, "context": "For example, the very deep VGG models [1], which have witnessed great success in a wide range of recognition tasks [9], [2], [10], [11], [12], [13], are substantially slower than earlier models [4], [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.", "startOffset": 202, "endOffset": 205}, {"referenceID": 1, "context": "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.", "startOffset": 207, "endOffset": 210}, {"referenceID": 9, "context": "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.", "startOffset": 212, "endOffset": 216}, {"referenceID": 10, "context": "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.", "startOffset": 243, "endOffset": 247}, {"referenceID": 11, "context": "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.", "startOffset": 249, "endOffset": 253}, {"referenceID": 12, "context": "For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets may not afford slow models; some recognition tasks like object detection [7], [2], [10] and semantic segmentation [11], [12], [13] need to apply these models on higher-resolution images.", "startOffset": 255, "endOffset": 259}, {"referenceID": 13, "context": "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "There have been a series of studies on accelerating deep CNNs [14], [15], [16], [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "Experiments on complex datasets such as ImageNet [18] are also limited - e.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": ", the results in [15], [16], [17] are about accelerating a single layer of the shallower AlexNet [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "com the accelerated networks as generic feature extractors for other recognition tasks [2], [11] remain unclear.", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "com the accelerated networks as generic feature extractors for other recognition tasks [2], [11] remain unclear.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "Data (response) reconstruction solvers [16] based on stochastic gradient descent (SGD) and backpropagation work well for simpler tasks such as character classification [16], but are less effective for complex ImageNet models (as we will discussed in Sec.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "Data (response) reconstruction solvers [16] based on stochastic gradient descent (SGD) and backpropagation work well for simpler tasks such as character classification [16], but are less effective for complex ImageNet models (as we will discussed in Sec.", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "In experiments, we demonstrate the effects of the nonlinear solution, asymmetric reconstruction, and whole-model acceleration by controlled experiments of a 10-layer model on ImageNet classification [18].", "startOffset": 199, "endOffset": 203}, {"referenceID": 0, "context": "Furthermore, we apply our method on the publicly available VGG-16 model [1], and achieve a 4\u00d7 speedup with merely a 0.", "startOffset": 72, "endOffset": 75}, {"referenceID": 17, "context": "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].", "startOffset": 284, "endOffset": 287}, {"referenceID": 1, "context": "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].", "startOffset": 289, "endOffset": 292}, {"referenceID": 9, "context": "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].", "startOffset": 294, "endOffset": 298}, {"referenceID": 10, "context": "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].", "startOffset": 325, "endOffset": 329}, {"referenceID": 11, "context": "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].", "startOffset": 331, "endOffset": 335}, {"referenceID": 12, "context": "The impact of the ImageNet dataset [18] is not merely on the specific 1000-class classification task; deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and have showcased excellent accuracy for challenging tasks such as object detection [9], [2], [10] and semantic segmentation [11], [12], [13].", "startOffset": 337, "endOffset": 341}, {"referenceID": 1, "context": "We exploit our method to accelerate the very deep VGG-16 model for Fast R-CNN [2] object detection.", "startOffset": 78, "endOffset": 81}, {"referenceID": 18, "context": "1%) on the PASCAL VOC 2007 detection benchmark [19].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "A preliminary version of this manuscript has been presented in a conference [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "(2) We investigate transfer learning results of accelerated models through object detection [2], which is one of the most important applications of ImageNet pre-trained networks.", "startOffset": 92, "endOffset": 95}, {"referenceID": 13, "context": "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Methods [14], [15], [16], [17] for accelerating testtime computation of CNNs in general have two components: (i) a layer decomposition design that reduces time complexity, and (ii) an optimization scheme for the decomposition design.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": "[15] is one of the first to exploit low-rank decompositions of filters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This method presents experiments of accelerating a single layer of an OverFeat network [6], but no whole-model results are available.", "startOffset": 87, "endOffset": 90}, {"referenceID": 15, "context": "[16] present efficient decompositions by separating k \u00d7 k filters into k \u00d7 1 and 1 \u00d7 k filters, which was earlier developed for accelerating generic image filters [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[16] present efficient decompositions by separating k \u00d7 k filters into k \u00d7 1 and 1 \u00d7 k filters, which was earlier developed for accelerating generic image filters [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 16, "context": "[17] adopt \u201cCP-decomposition\u201d to decompose a layer into five layers of lower complexity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "But for ImageNet classification, only a single-layer acceleration of AlexNet is reported in [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "Despite some promising preliminary results have been obtained in the above works [15], [16], [17], the whole-model acceleration of very deep networks for ImageNet is still an open problem.", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "Despite some promising preliminary results have been obtained in the above works [15], [16], [17], the whole-model acceleration of very deep networks for ImageNet is still an open problem.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "Despite some promising preliminary results have been obtained in the above works [15], [16], [17], the whole-model acceleration of very deep networks for ImageNet is still an open problem.", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "FFT-based algorithms [22], [23] are applicable for both training and testing, and are particularly effective for large spatial kernels.", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "FFT-based algorithms [22], [23] are applicable for both training and testing, and are particularly effective for large spatial kernels.", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "On the other hand, it is also proposed to train \u201cthin\u201d and deep networks [24], [25] for good trade-off between speed and accuracy.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "On the other hand, it is also proposed to train \u201cthin\u201d and deep networks [24], [25] for good trade-off between speed and accuracy.", "startOffset": 79, "endOffset": 83}, {"referenceID": 25, "context": "Besides reducing running time, a related issue involving memory conservation [26] is also studied.", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "Our method exploits a low-rank assumption for decomposition, following the stream of [15], [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "Our method exploits a low-rank assumption for decomposition, following the stream of [15], [16].", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "We show that this decomposition has a closed-form solution (SVD) for linear neurons, and a slightly more complicated solution based on Generalized SVD (GSVD) [27], [28], [29] for nonlinear neurons.", "startOffset": 164, "endOffset": 168}, {"referenceID": 27, "context": "We show that this decomposition has a closed-form solution (SVD) for linear neurons, and a slightly more complicated solution based on Generalized SVD (GSVD) [27], [28], [29] for nonlinear neurons.", "startOffset": 170, "endOffset": 174}, {"referenceID": 28, "context": "A simple decomposition is the Singular Vector Decomposition (SVD) [30]: M = Ud\u2032Sd\u2032Vd\u2032, where Ud\u2032 and Vd\u2032 are d-by-d\u2032 column-orthogonal matrices and Sd\u2032 is a d\u2032-by-d\u2032 diagonal matrix.", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "This problem can be solved by SVD [30] or actually Principal Component Analysis (PCA): let Y be the d-by-n matrix concatenating n responses with the mean subtracted, compute the eigen-decomposition of the covariance matrix YY> = USU> where U is an orthogonal matrix and S is diagonal, and M = Ud\u2032Ud\u2032 where Ud\u2032 are the first d\u2032 eigenvectors.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "Although the low-rank assumptions about filter weights W have been adopted in recent work [15], [16], we further adopt the low-rank assumptions about the filter inputs x, which are local volumes and have correlations.", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "Although the low-rank assumptions about filter weights W have been adopted in recent work [15], [16], we further adopt the low-rank assumptions about the filter inputs x, which are local volumes and have correlations.", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "In this paper we focus on the Rectified Linear Unit (ReLU) [31]: r(\u00b7) = max(\u00b7, 0).", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "If \u03bb \u2192 \u221e, the solution to (6) will converge to the solution to (5) [32].", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "This optimization problem also has a closed-form solution by Generalized SVD (GSVD) [27], [28], [29].", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "This optimization problem also has a closed-form solution by Generalized SVD (GSVD) [27], [28], [29].", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "A problem in this form is known as Reduced Rank Regression [27], [28], [29].", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "A problem in this form is known as Reduced Rank Regression [27], [28], [29].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "This problem belongs to a broader category of procrustes problems [27] that have been adopted for various data reconstruction problems [33], [34], [35].", "startOffset": 135, "endOffset": 139}, {"referenceID": 32, "context": "This problem belongs to a broader category of procrustes problems [27] that have been adopted for various data reconstruction problems [33], [34], [35].", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "This problem belongs to a broader category of procrustes problems [27] that have been adopted for various data reconstruction problems [33], [34], [35].", "startOffset": 147, "endOffset": 151}, {"referenceID": 26, "context": "GSVD [27], [28], [29] is applied on M\u0302: M\u0302 = USV>, such that U is a d-by-d orthogonal matrix satisfying U>U = Id where Id is a d-by-d identity matrix, and V is a d-by-d matrix satisfying V>YY>V = Id (called generalized orthogonality).", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "GSVD [27], [28], [29] is applied on M\u0302: M\u0302 = USV>, such that U is a d-by-d orthogonal matrix satisfying U>U = Id where Id is a d-by-d identity matrix, and V is a d-by-d matrix satisfying V>YY>V = Id (called generalized orthogonality).", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "In theory, \u03bb should be gradually increased to infinity [32].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Table 1: The architecture of the SPP-10 model [7].", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "The final conv layer is followed by a spatial pyramid pooling layer [7] that have 4 levels ({6 \u00d7 6, 3 \u00d7 3, 2 \u00d7 2, 1 \u00d7 1}, totally 50 bins).", "startOffset": 68, "endOffset": 71}, {"referenceID": 34, "context": "The problem in (14) is a combinatorial problem [36].", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "On the other hand, compared with decomposition methods that operate on multiple dimensions (spatial and channel) [16], our method has to use a smaller d\u2032 to approach a given speedup ratio, which might limit the accuracy of our method.", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "Given their order as above, we first optimize the (k \u00d7 1, d\u2032\u2032) and (1 \u00d7 k, d) layers using \u201cfilter reconstruction\u201d [16] (we will discuss \u201cdata reconstruction\u201d later).", "startOffset": 115, "endOffset": 119}, {"referenceID": 6, "context": "The first model is a 10-layer model of \u201cSPPnet (OverFeat-7)\u201d in [7], which we denote as \u201cSPP-10\u201d.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "This model (detailed in Table 1) has a similar architecture to the OverFeat model [6] but is deeper.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "The second model is the publicly available VGG-16 model [1]1 that has 13", "startOffset": 56, "endOffset": 59}, {"referenceID": 17, "context": "SPP-10 won the 3-rd place and VGG-16 won the 2-nd place in ILSVRC 2014 [18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "09% in our testing (which is consistent with the number reported by [1]2).", "startOffset": 68, "endOffset": 71}, {"referenceID": 15, "context": "\u2019s method [16]", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "\u2019s method [16], which is a recent state-of-the-art solution to efficient evaluation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "Although our decomposition shares some high-level motivations as [16], we point out that our optimization strategy is different with [16] and is important for accuracy, especially for very deep models that previous acceleration methods rarely addressed.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Although our decomposition shares some high-level motivations as [16], we point out that our optimization strategy is different with [16] and is important for accuracy, especially for very deep models that previous acceleration methods rarely addressed.", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "\u2019s method [16] decomposes a k \u00d7 k spatial support into a cascade of k \u00d7 1 and 1 \u00d7 k spatial supports.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "\u2019s spatial decomposition method [16] for SPP-10.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "In the paper of [16], their method is only evaluated on a single layer of an OverFeat network [6] for ImageNet.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "In the paper of [16], their method is only evaluated on a single layer of an OverFeat network [6] for ImageNet.", "startOffset": 94, "endOffset": 97}, {"referenceID": 15, "context": "Our comparisons are based on our implementation of [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "We use the Scheme 2 decomposition in [16] and its \u201cfilter reconstruction\u201d version (as we explain below), which is used for ImageNet as in [16].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "We use the Scheme 2 decomposition in [16] and its \u201cfilter reconstruction\u201d version (as we explain below), which is used for ImageNet as in [16].", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "Our reproduction of the filter reconstruction in [16] gives a 2\u00d7 single-layer speedup on Conv2 of SPP-10 with 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "As a reference, in [16] it reports 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "5% increase of error on Conv2 under a 2\u00d7 single-layer speedup, evaluated on another OverFeat network [6] similar to SPP-10.", "startOffset": 101, "endOffset": 104}, {"referenceID": 15, "context": "\u2019s [16] \u201cdata reconstruction\u201d scheme, which was suggested to use SGD and backpropagation for optimization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In our reproduction, we find that data reconstruction works well for the character classification task as studied in [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "We observe that the learning rate needs to be carefully chosen for the SGD-based data reconstruction to converge (as also reported independently in [17] for another decomposition), and when the training starts to converge, the results are still sensitive to the initialization (for which we have tried Gaussian distributions of a wide range of variances).", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "\u2019s [16] only report \u201cfilter reconstruction\u201d results of a single layer on ImageNet.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "SPP-10 [7] 12.", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "[16] (our impl.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "AlexNet [4] 18.", "startOffset": 8, "endOffset": 11}, {"referenceID": 15, "context": "\u2019s [16] for whole-model speedup.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "For whole-model speedup of [16], we implement their method sequentially on Conv2-7 using the same speedup ratio.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "For completeness, we also evaluate our approximation method on the character classification model released by [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "7% in classification accuracy, which is better than the 1% drop for the same speedup reported by [16].", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "\u2019s method [16] on Conv1, because this layer has a small number of input channels (3), and the first k\u00d71 decomposed layer can only have a very small number of filters (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 35, "context": "The decomposed model is much deeper than the original model (each layer replaced by three layers), so we adopt the initialization method in [37] otherwise it is not easy to converge.", "startOffset": 140, "endOffset": 144}, {"referenceID": 36, "context": "We follow the common practice in [38], [7] of training ImageNet models.", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "We follow the common practice in [38], [7] of training ImageNet models.", "startOffset": 39, "endOffset": 42}, {"referenceID": 6, "context": "SPP-10 [7] 12.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "We also evaluate the AlexNet [4] which is similarly fast as our accelerated 4\u00d7 models.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Our AlexNet is the same as in [4] except that the GPU splitting is ignored.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "This is better than the one reported in [4]4.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "In [4] the 10-view error is top-5 18.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Table 5: The architecture of the VGG-16 model [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 176, "endOffset": 180}, {"referenceID": 37, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 182, "endOffset": 186}, {"referenceID": 38, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 188, "endOffset": 192}, {"referenceID": 39, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 211, "endOffset": 215}, {"referenceID": 40, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 217, "endOffset": 221}, {"referenceID": 41, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 223, "endOffset": 227}, {"referenceID": 42, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 254, "endOffset": 258}, {"referenceID": 43, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 285, "endOffset": 289}, {"referenceID": 44, "context": "The very deep VGG models [1] have substantially improved a wide range of visual recognition tasks, including object detection [9], [2], [10], semantic segmentation [11], [12], [13], [39], [40], image captioning [41], [42], [43], video/action recognition [44], image question answering [45], texture recognition [46], etc.", "startOffset": 311, "endOffset": 315}, {"referenceID": 15, "context": "[16] (our impl.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Table 7: Accelerating the VGG-16 model [1] using a speedup ratio of 3\u00d7, 4\u00d7, or 5\u00d7.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "VGG-16 [1] 10.", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "[16] (our impl.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Table 8: Absolute performance of accelerating the VGG-16 model [1].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Unlike SPP-10 (or other shallower models [4], [5]) that repeatedly applies 3\u00d73 filters on the same feature map size, the VGG-16 model applies them more evenly on five feature map sizes (224, 112, 56, 28, and 14).", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "Unlike SPP-10 (or other shallower models [4], [5]) that repeatedly applies 3\u00d73 filters on the same feature map size, the VGG-16 model applies them more evenly on five feature map sizes (224, 112, 56, 28, and 14).", "startOffset": 46, "endOffset": 49}, {"referenceID": 15, "context": "On the contrary, the previous method [16] suffers greatly from the increased depth because of the rapidly accumulated error of multiple approximated layers.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "\u2019s work [17] is one of few existing works that present results of accelerating the whole model of VGG-16.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "Accelerating VGG-16 for Object Detection Current state-of-the-art object detection methods [9], [2], [10] all exploit the VGG-16 model.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Accelerating VGG-16 for Object Detection Current state-of-the-art object detection methods [9], [2], [10] all exploit the VGG-16 model.", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Accelerating VGG-16 for Object Detection Current state-of-the-art object detection methods [9], [2], [10] all exploit the VGG-16 model.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Our method is based on the latest Fast R-CNN [2].", "startOffset": 45, "endOffset": 48}, {"referenceID": 18, "context": "We evaluate on the PASCAL VOC 2007 object detection benchmark [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "Note that unlike image classification where the conv layers dominate running time, for Fast R-CNN detection the conv layers consume about 70% actual running time [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "The detector is Fast R-CNN [2] using the pre-trained VGG-16 model.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "We believe this trade-off between accuracy and speed is of practical importance, because even with the recent advance of fast object detection [7], [9], the feature extraction running time is still considerable.", "startOffset": 143, "endOffset": 146}, {"referenceID": 8, "context": "We believe this trade-off between accuracy and speed is of practical importance, because even with the recent advance of fast object detection [7], [9], the feature extraction running time is still considerable.", "startOffset": 148, "endOffset": 151}], "year": 2015, "abstractText": "This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs [1] that have substantially impacted the computer vision community. Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while current methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g., \u226510) layers are approximated. For the widely used very deep VGG-16 model [1], our method achieves a whole-model speedup of 4\u00d7 with merely a 0.3% increase of top-5 error in ImageNet classification. Our 4\u00d7 accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the latest Fast R-CNN detector [2].", "creator": "LaTeX with hyperref package"}}}