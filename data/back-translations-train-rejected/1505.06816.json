{"id": "1505.06816", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2015", "title": "Representing Meaning with a Combination of Logical and Distributional Models", "abstract": "NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not adequately capture overall sentence structure. So it has been argued that the two are complementary. In this paper, we adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs). We focus on textual entailment (RTE), a task that can utilize the strengths of both representations. Our system is three components, 1) parsing and task representation, where input RTE problems are represented in probabilistic logic. This is quite different from representing them in standard first-order logic. 2) knowledge base construction in the form of weighted inference rules from different sources like WordNet, paraphrase collections, and lexical and phrasal distributional rules generated on the fly. We use a variant of Robinson resolution to determine the necessary inference rules. More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that counteract scaling differences between resources. 3) inference, where we show how to solve the inference problems efficiently. In this paper we focus on the SICK dataset, and we achieve a state-of-the-art result. Our system handles overall sentence structure and phenomena like negation in the logic, then uses our Robinson resolution variant to query distributional systems about words and short phrases. Therefor, we use our system to evaluate distributional lexical entailment approaches. We also publish the set of rules queried from the SICK dataset, which can be a good resource to evaluate them.", "histories": [["v1", "Tue, 26 May 2015 06:19:18 GMT  (126kb,D)", "http://arxiv.org/abs/1505.06816v1", null], ["v2", "Sun, 29 Nov 2015 03:51:26 GMT  (184kb,D)", "http://arxiv.org/abs/1505.06816v2", null], ["v3", "Tue, 23 Feb 2016 03:46:07 GMT  (184kb,D)", "http://arxiv.org/abs/1505.06816v3", null], ["v4", "Tue, 7 Jun 2016 13:30:01 GMT  (180kb,D)", "http://arxiv.org/abs/1505.06816v4", "Special issue of Computational Linguistics on Formal Distributional Semantics, 2016"], ["v5", "Wed, 8 Jun 2016 15:07:47 GMT  (180kb,D)", "http://arxiv.org/abs/1505.06816v5", "Special issue of Computational Linguistics on Formal Distributional Semantics, 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["i beltagy", "stephen roller", "pengxiang cheng", "katrin erk", "raymond j mooney"], "accepted": false, "id": "1505.06816"}, "pdf": {"name": "1505.06816.pdf", "metadata": {"source": "CRF", "title": "Representing Meaning with a Combination of Logical Form and Vectors", "authors": ["Islam Beltagy", "Stephen Roller", "Pengxiang Cheng", "Katrin Erk", "Raymond J. Mooney"], "emails": [], "sections": [{"heading": null, "text": "This article characterizes the structure of the sentence, but fails to grasp the graded aspect of the meaning. Distribution models provide graded similarity assessments for words and phrases, but they do not adequately grasp the overall structure of the sentence. Thus, it has been argued that the two are complementary. In this paper, we adopt a hybrid approach that combines logic and distributional logic."}, {"heading": "1. Introduction", "text": "It's not as if it's a purely formal constellation, but it's as if it's a purely formal constellation. It's as if it's a purely formal constellation. It's as if it's a purely formal constellation. It's as if it's a purely formal constellation. It's as if it's a purely formal constellation. It's as if it's a formal constellation. It's as if it's a formal constellation. It's as if it's a formal constellation. It's as if it's a formal constellation. It's as if it's a formal constellation."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Logic-based Semantics", "text": "Logic-based representations of meaning have a long tradition in semantics (Montague 1970; Dowty, Wall and Peters 1981; Kamp and Reyle 1993) and computer semantics (Blackburn and Bos 2005; van Eijck and Unger 2010), dealing with many complex semantic phenomena such as negation and quantifiers, identifying discourse referents along with the predicates that apply to them and the relationships that exist between them. However, standard logic and theory-testers are binary in nature, which prevents them from grasping the graded aspects of meaning in language: synonymy seems to come in degrees (Edmonds and Hirst 2000), as well as the difference between senses in polysemic words (Brown 2008). van Eijck and Lappin (2012) write: \"The case for abandoning the categorical view of competence and adopting a probabilistic model in semantics.\""}, {"heading": "2.2 Distributional Semantics", "text": "Distribution models (Turney and Pantel 2010) use statistics on large corpora contextual data to predict the semantic similarity of words and phrases (Landauer and Dumais 1997; Mitchell and Lapata 2010), based on the observation that semantically similar words occur in similar contexts so that words can be represented as vectors in high-dimensional spaces generated from the contexts in which they occur (Landauer and Dumais 1997; Lund and Burgess 1996). Therefore, distribution models are relatively easy to create than logical representations, automatically acquire knowledge from \"big data\" and capture the graduated nature of linguistic meanings, but they do not sufficiently capture the logical structure (Grefenstette 2013). Distribution models have also been expanded to include vector representations for larger phrases, e.g. by adding the vectors for the individual words (Landauer and Dumais 1997) or by a component Grefenstette of word extenders (Lapata 2011 and xata)."}, {"heading": "2.3 Integrating logic-based and distributional semantics", "text": "It has been repeatedly pointed out that logic-based and distributional approaches seem to complement each other in their strengths and weaknesses (Coecke, Sadrzadeh, and Clark 2011; Garrette, Erk, and Mooney 2011; Baroni, Bernardi, and Zamparelli 2014), suggesting that it may be useful to combine the two frameworks, and indeed there are several hybrid systems that do so today. Beltagy et al. (2013) transform distributional similarities into weighted distributional inference rules that are combined with logical propositions, and use probabilistic conclusions about both. This is the approach we are building on in this paper. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word sets, and perform standard inferences on the resulting logical forms. The main difference between the two approaches lies in the role of grace. Lewis and Steedman (2013), on the other hand, consider clustering on distributional data to represent weights and probabilities as a problem."}, {"heading": "2.4 Hybrid representations for a heterogeneous semantics", "text": "One possibility is to say that they are temporary systems that we must fall back on until we finally develop a unified framework that embraces the benefits of both logistical and distributional semantics. Another possibility is that hybrid systems are the right way to represent meaning - and it is this second option that we will argue for. In an article on \"formal semantics and Wittgenstein,\" Stokhof suggests that meaning is a heterogeneous phenomenon. He writes: \"This means that what we call\" meaning \"is both individual and social; internal and external theories; natural and socio-cultural theories; and so on.\" If we follow the close association between meaning and use that Wittgenstein's work is not, we can conclude that some aspects of meaning lie in the individual that are determined by the community (or communities)."}, {"heading": "2.5 Probabilistic Logic with Markov Logic Networks", "text": "In fact, most of them are able to survive by themselves if they do not play by the rules. (...) Most of them are able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...) Most of them are not able to survive by themselves. (...)"}, {"heading": "2.6 Recognizing Textual Entailment", "text": "The task we focus on in this paper is the recognition of Textual Entailment (RTE) (Dagan et al. 2013), the task of determining whether a text of natural language, the text T, contradicts or not (neutrally) with another, the hypothesis H. Here are examples from the SICK dataset (Marelli et al. 2014c): \u2022 Entailment T: A man and a woman walk through the forest together. \u2022 ContradictionT: A man jumps into an empty pool H: A man jumps into a full pool. \u2022 Neutral T: A young girl dances H: A young girl stands on one leg."}, {"heading": "3. System Overview", "text": "In fact, most of them are able to play by the rules they had in the past, and they are able to play by the rules they have established in the past."}, {"heading": "4. Parsing and Task Representation", "text": "Our system assigns sentences to logical formulas for conclusions. In this section, we discuss this process, and in particular how it differs from the semantic construction for testing first-order standard theorems based on the probabilistic inference setting we use. This section draws on Beltagy and Erk (2015)."}, {"heading": "4.1 Parsing using Boxer", "text": "Boxer is a rules-based semantic analysis system that translates a CCG parser into a logical form as in Equation 2. We refer to Boxer's output only as \"uninterpreted logical form,\" because the predicate symbols are just words and have no meaning on their own. Their semantics are derived from the KB knowledge base we have in Section 5. The default CCG parser Boxer uses is C & C. To reduce errors due to parsing, we want to use several parsers; however, we have found that the uppermost parses we get from C & C are usually not diverse enough and map to the same logical form. Therefore, in addition to the uppermost C & C parser, we use the uppermost parser of another current CCG parser, EasyCCG (Lewis and Steedman 2014), the uppermost parser of another newer CCG parser, EasyCCG (Lewis and Steedman 2014)."}, {"heading": "4.2 The RTE Task", "text": "We are given two propositions T and H, and we want to determine whether T calculates the probability P (H | T, KB, Wt, h), where H is the logical query of probability and Wt, h is the world configuration, which includes the number of constants in the domain and the previous probability of each soil atom. Wt, h is a function of T and H, and in sections 4.3 and 4.4 it is discussed how it is structured. The distinction between contradiction and neutral requires another conclusion, namely the calculation of the probability P (\u00ac H | T, KB, Wt, \u00ac H). If P (H | T, Wt, H) is high, while P (\u00ac H | T, KB, Wt \u00ac H, if this probability P (\u00ac H, KB, Wt \u00ac H) means that this decision significantly influences the SVM probability, the two SVM values are not indications of the SVM probability."}, {"heading": "4.3 The Domain Closure Assumption and its Consequences", "text": "We must deviate from the standardized logical form, which then becomes the nodes in the graphical model. Differentiated results in different models will be presented in different models. (a) Different constants refer to different objects in the domain, (b) the only objects in the domain are those that can be represented by the constant and the function symbols in F, and (c) for each function f that appears in F, the value of f that is applied to any tuple of arguments is known, and is a constant that appears in F. (c) If the constants of all constants are known, it can be used to generate the set of bottom words that then become the nodes in the graphical model."}, {"heading": "4.4 The Closed-World Assumption", "text": "The question that arises is to what extent it is about a way in which it is about the question, to what extent it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about which it is about which it is about which it is about a way in which it is about a way in which it is about which it is about which it is about a way in which it is about which it is about which it is about which it is about which it is about a way in which it"}, {"heading": "4.5 Coreference Resolution for Contradiction", "text": "In the SICK dataset, many of the contradictions cannot be grasped if T: a) he is not flying and H: a) he is flying, then he is not contradictory because the two sentences refer to different jets. Although the sentence is not the jet, the announcers make the assumption that the jet in T: a) he refers to the jet. We have to force a similar assumption to get many of the contradictions. Here are the logical formulas for the example above, the announcers make the assumption that the jet in T: a) he refers to the jet."}, {"heading": "5. Knowledge Base Construction", "text": "This section describes how to translate the weighted knowledge base KB for a given sentence pair T and H. First, we discuss the existing rule sets we add KB and how to translate them into logical rules. Then, we discuss how to integrate distribution information into KB. We use a variant of Robinson resolution to align T with H and find the difference between them in the form of an inference rule. Then, we use various distribution semantics to give weight to this inference rule. 4"}, {"heading": "5.1 Precompiled Rules", "text": "The first rules are collected by listing existing rule relationships. We collect rules from WordNet (Princeton University 2010) to the paraphrase collection PPDB (Ganitkevitch, Van Durme and Callison-Burch 2013). We use simple string matching rules that are relevant to a specific problem T and H. If the LHS rule is a subline of T (or H) and the RHS rule is a subline of H (or T), the rule that goes from H to T is negated, important rules in case T and H, for example T: There is no one driving a vehicle, H: There is no one driving a car. The rule we need goes from H to T.5.1.1 WordNet. WordNet (Princeton University 2010) is a lexical database of words that are grouped into groups synonyms."}, {"heading": "5.2 Robinson Resolution for Alignment and Rule Extraction", "text": "The distribution rules should be generated for a pair of text T and hypotheses in T or phrases. Previous versions of our system generated distribution rules that are tailored to each word or short phrase in T. (That is, we just need the necessary rules for T and H. We assume that T entailsH, and ask what the missing rule setKB is that is necessary to prove this linkage. We use a variant of Robinson resolution (Robinson 1983) to generate these KB. This gives us very specific rules that are tailored to a specific T and H. Below, we will use these rules as training data for a entailment rule."}, {"heading": "5.3 The Lexical and Phrasal Entailment Rule Classifier", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "6. Probabilistic Logical Inference", "text": "The last component is the probabilistic logical conclusion. In Section 4, we have shown how to present the task as probabilistic inference problems of the form P (Q | E, R, W), where Q is the query formula, E is the evidence set, R is a set of rules, and W is the world configuration. This section shows how to solve this problem. MLN inference is usually insoluble, and the use of MLN implementations \"out of the box\" does not work for our application. This section discusses an MLN implementation that supports complex Q queries. It also shows how to use CWA to reduce the problem size and thus make inference more efficient. Finally, this section discusses a simple scheme for learning global scale factors for weighted rules in KB from various sources. Apart from weight learning, this section largely refers to the previous paper by Beltagy and Mooney (2014)."}, {"heading": "6.1 Query Formula", "text": "In fact, it is as if most of us are able to set out in search of new ways to travel the world. (...) It is as if they are able to conquer the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are able to change the world. (...) It is as if they are the world of the world. (...) It is as if they are the world of the world. \"(...) It is as if they are the world of the world. (...) It is as if they are the world of the world."}, {"heading": "6.2 Inference Optimization using Closed-World Assumption", "text": "This section explains why our MLN sequence problems are mathematically difficult, then explains how the Closed World Assumption (CWA) can be used to reduce the size of the problem and speed up the conclusion. For more details, see Beltagy and Mooney (2014).In the sequence problems we deal with, formulas are typically long, especially the query formula. The number of soil clauses of a first order formula is exponential in the number of variables in the formula, it is O (cv), where c is the number of constants in the domain and v is the number of variables in the formula. For any moderately long formula, the number of resulting soil clauses is impracticable in order to process the available inference algorithms. Section 4.4 concludes that we should make the CWA as a probable logical sequence problem in formulating the RTE task, if all of them are likely to have low soil atoms."}, {"heading": "6.3 Weight Learning", "text": "The KB is a set of weighted rules. These weights come from different sources, in our case PPDB weights (Section 5.1.2) and the reliability of the rule classifier for consequential effects (Section 5.3). These weights must be mapped to MLN weights. We use weight learning to perform the mapping. Similar to Zirn et al. (2011), we learn for each rule source a single mapping parameter that functions as a scaling factor: MLNweight = Scaling Factor \u00d7 RuleWeight (9). We use a simple grid search to learn the scaling factors that optimize performance on the RTE training data. Assuming that all rule weights are in [0, 1] (this is the case for classification confidence values, and PPDB weights can be scaled), we also try the following mapping function: MLNweight = Scaling Factor \u00d7 (Rule Log Weights \u00d7 (Rule Weight Factor = 1) that RHS is guaranteed (Rule Weight Factor \u2212 10)."}, {"heading": "7. Evaluation", "text": "This section evaluates our system. First, we evaluate several lexical and phrasal distribution systems based on the rules we have gathered with modified Robinson resolution. Second, we use the best configuration that we find as a knowledge base in the first step, and evaluate our system on the RTE task using the SICK dataset. The SICK dataset described in Section 2 consists of 5,000 pairs for training purposes and 4,927 for testing purposes. Couples are commented on for RTE and STS (Semantic Textual Similarity) tasks. We use the RTE part of the dataset."}, {"heading": "7.1 Evaluating the Entailment Rule Classifier", "text": "This year it is more than ever before."}, {"heading": "7.2 RTE Task Evaluation", "text": "This year it is more than ever before."}, {"heading": "8. Future Work", "text": "It is not only a matter of time before there is such a process, but also of time before there is such a process. (...) It is a matter of time before there is such a process. (...) It is a matter of time before there is such a process. (...) It is a matter of time before there is such a process. (...) It is a matter of time before there is such a process. (...) It is a matter of time before there is such a process. (...) It is a matter of time before there is such a process. (...) It is a matter of time before there is such a process. \"(...) It is a matter of time before there is such a process.\" (...)"}, {"heading": "9. Conclusion", "text": "The ability to represent the semantics of natural languages effectively is important and has many important applications. We have introduced an approach that combines the expressivity and automated reasoning of logical representations with the ability to capture graded aspects of natural language captured by distributional semantics. We have evaluated this semantic representation in the RTE task, which requires a deep semantic understanding. Our system maps natural language sentences to logical formulas, uses them to build logical inference problems, builds a knowledge base of precompiled resources and distributed resources, and then performs conclusions using Markov logic. Experiments demonstrated state of the art in the recently introduced SICK-RTE task."}, {"heading": "Acknowledgments", "text": "This research was supported by the DARPA DEFT program under AFRL grant FA8750-13-2-0026 and NSF grant CAREER IIS 0845925. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of DARPA, DoD or the U.S. Government. Some experiments were conducted at the Mastodon Cluster, which was supported by NSF grant EIA-0303609. Some experiments were conducted at the Texas Advanced Computing Center (TACC) 6 at the University of Texas at Austin."}], "references": [{"title": "Integrating experiential and distributional data to learn semantic representations", "author": ["Andrews", "Vigliocco", "Vinson2009]Andrews", "Mark", "Gabriella Vigliocco", "David Vinson"], "venue": "Psychological Review,", "citeRegEx": "Andrews et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2009}, {"title": "Frege in space: A program for compositional distributional semantics", "author": ["Baroni", "Bernardi", "Zamparelli2014]Baroni", "Marco", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010]Baroni", "Marco", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "How we BLESSed distributional semantic evaluation", "author": ["Baroni", "Lenci2011]Baroni", "Marco", "Alessandro Lenci"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Baroni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2011}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010]Baroni", "Marco", "Roberto Zamparelli"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2010)", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Montague meets Markov: Deep semantics with probabilistic logical form", "author": ["Beltagy et al.2013]Beltagy", "Islam", "Cuong Chau", "Gemma Boleda", "Dan Garrette", "Katrin Erk", "Raymond Mooney"], "venue": "In Proceedings of the Second Joint Conference on Lexical and Computational Semantics", "citeRegEx": "al.2013.Beltagy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Beltagy et al\\.", "year": 2013}, {"title": "On the proper treatment of quantifiers in probabilistic logic semantics", "author": ["Beltagy", "Erk2015]Beltagy", "Islam", "Katrin Erk"], "venue": "In Proceedings of the International Conference on Computational Semantics", "citeRegEx": "Beltagy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2015}, {"title": "Efficient Markov logic inference for natural language semantics", "author": ["Beltagy", "Mooney2014]Beltagy", "Islam", "Raymond J. Mooney"], "venue": "In Proceedings of AAAI 2014 Workshop on Statistical Relational AI (StarAI-2014)", "citeRegEx": "Beltagy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Berant et al.2013]Berant", "Jonathan", "Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2013)", "citeRegEx": "al.2013.Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Berant et al\\.", "year": 2013}, {"title": "Natural language processing with Python. \"O\u2019Reilly Media, Inc.", "author": ["Bird", "Klein", "Loper2009]Bird", "Steven", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Bjerva et al.2014]Bjerva", "Johannes", "Johan Bos", "Rob van der Goot", "Malvina Nissim"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "al.2014.Bjerva et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014.Bjerva et al\\.", "year": 2014}, {"title": "Representation and Inference for Natural Language: A First Course in Computational Semantics", "author": ["Blackburn", "Bos2005]Blackburn", "Patrick", "Johan Bos"], "venue": "CSLI Publications,", "citeRegEx": "Blackburn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blackburn et al\\.", "year": 2005}, {"title": "Wide-coverage semantic analysis with Boxer", "author": ["Bos2008]Bos", "Johan"], "venue": "In Proceedings of Semantics in Text Processing", "citeRegEx": "Bos2008.Bos and Johan.,? \\Q2008\\E", "shortCiteRegEx": "Bos2008.Bos and Johan.", "year": 2008}, {"title": "Choosing sense distinctions for WSD: Psycholinguistic evidence", "author": ["Susan Windisch"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Windisch.,? \\Q2008\\E", "shortCiteRegEx": "Windisch.", "year": 2008}, {"title": "Distributional semantics in technicolor", "author": ["Bruni et al.2012]Bruni", "Elia", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Comptuational Linguistics (ACL", "citeRegEx": "al.2012.Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al.2012.Bruni et al\\.", "year": 2012}, {"title": "Multiple instance learning for sparse positive bags", "author": ["Bunescu", "Mooney2007]Bunescu", "Razvan", "Ray Mooney"], "venue": "In Proceedings of the 24th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm", "author": ["Chang", "Lin2001]Chang", "Chih-Chung", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Vector space models of lexical meaning", "author": ["Clark2012]Clark", "Stephen"], "venue": "In Handbook of Contemporary Semantics. Wiley-Blackwell,", "citeRegEx": "Clark2012.Clark and Stephen.,? \\Q2012\\E", "shortCiteRegEx": "Clark2012.Clark and Stephen.", "year": 2012}, {"title": "Parsing the WSJ using CCG and log-linear models", "author": ["Clark", "Curran2004]Clark", "Stephen", "James R. Curran"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Clark et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2004}, {"title": "Mathematical foundations for a compositional distributed model of meaning. Linguistic Analysis, 36(1-4):345\u2013384", "author": ["Coecke", "Sadrzadeh", "Clark2011]Coecke", "Bob", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": null, "citeRegEx": "Coecke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2011}, {"title": "A probabilistic rich type theory for semantic interpretation", "author": ["Cooper et al.2014]Cooper", "Robin", "Simon Dobnik", "Shalom Lappin", "Staffan Larsson"], "venue": "In Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS),", "citeRegEx": "al.2014.Cooper et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014.Cooper et al\\.", "year": 2014}, {"title": "An open-source grammar development environment and broad-coverage english grammar using HPSG", "author": ["Copestake", "Flickinger2000]Copestake", "Ann", "Dan Flickinger"], "venue": "In Proceedings of Language Resources and Evaluation Conference (LREC),", "citeRegEx": "Copestake et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Copestake et al\\.", "year": 2000}, {"title": "Recognizing textual entailment: Models and applications", "author": ["Dagan et al.2013]Dagan", "Ido", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "al.2013.Dagan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Dagan et al\\.", "year": 2013}, {"title": "Iterative join-graph propagation", "author": ["Dechter", "Kask", "Mateescu2002]Dechter", "Rina", "Kalev Kask", "Robert Mateescu"], "venue": "In Proceedings of 18th Conference on Uncertainty in Artificial Intelligence (UAI-2002)", "citeRegEx": "Dechter et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 2002}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Dietterich", "Lathrop", "Lozano-Perez1997]Dietterich", "Thomas G", "Richard H. Lathrop", "Tomas Lozano-Perez"], "venue": "Artificial Intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "A comparison of models of word meaning in context", "author": ["Dinu", "Thater", "Laue2012]Dinu", "Georgiana", "Stefan Thater", "S\u00f6ren Laue"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL", "citeRegEx": "Dinu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2012}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Domingos", "Lowd2009]Domingos", "Pedro", "Daniel Lowd"], "venue": null, "citeRegEx": "Domingos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Domingos et al\\.", "year": 2009}, {"title": "Introduction to Montague Semantics. D", "author": ["Dowty", "Wall", "Peters1981]Dowty", "David R", "Robert E. Wall", "Stanley Peters"], "venue": null, "citeRegEx": "Dowty et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Dowty et al\\.", "year": 1981}, {"title": "Reconciling fine-grained lexical knowledge and coarse-grained ontologies in the representation of near-synonyms", "author": ["Edmonds", "Hirst2000]Edmonds", "Philip", "Graeme Hirst"], "venue": "In Proceedings of the Workshop on Semantic Approximation,", "citeRegEx": "Edmonds et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Edmonds et al\\.", "year": 2000}, {"title": "A structured vector space model for word meaning in context", "author": ["Erk", "Pad\u00f32008]Erk", "Katrin", "Sebastian Pad\u00f3"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2008)", "citeRegEx": "Erk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2008}, {"title": "PPDB: The paraphrase database", "author": ["Ganitkevitch", "Van Durme", "Callison-Burch2013]Ganitkevitch", "Juri", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Integrating logical representations with probabilistic information using Markov logic", "author": ["Garrette", "Erk", "Mooney2011]Garrette", "Dan", "Katrin Erk", "Raymond Mooney"], "venue": "In Proceedings of International Conference on Computational Semantics", "citeRegEx": "Garrette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["Geffet", "Dagan2005]Geffet", "Maayan", "Ido Dagan"], "venue": "In Proceedings of the 43rd Annual meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Geffet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Geffet et al\\.", "year": 2005}, {"title": "Logical foundations of artificial intelligence", "author": ["Genesereth", "M.R. Nilsson1987]Genesereth", "N.J. Nilsson"], "venue": null, "citeRegEx": "Genesereth et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Genesereth et al\\.", "year": 1987}, {"title": "Introduction to Statistical Relational Learning", "author": ["Getoor", "L. Taskar2007]Getoor", "B. Taskar"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "SampleSearch: Importance sampling in presence of determinism", "author": ["Gogate", "Dechter2011]Gogate", "Vibhav", "Rina Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Gogate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2011}, {"title": "Probabilistic theorem proving", "author": ["Gogate", "Domingos2011]Gogate", "Vibhav", "Pedro Domingos"], "venue": "In Proceedings of 27th Conference on Uncertainty in Artificial Intelligence (UAI-2011)", "citeRegEx": "Gogate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2011}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Grefenstette2013]Grefenstette", "Edward"], "venue": "In Proceedings of Second Joint Conference on Lexical and Computational Semantics", "citeRegEx": "Grefenstette2013.Grefenstette and Edward.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette2013.Grefenstette and Edward.", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Grefenstette", "Sadrzadeh2011]Grefenstette", "Edward", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2011)", "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "The Alchemy system for statistical relational AI. http://www.cs.washington.edu/ai/alchemy", "author": ["Kok et al.2005]Kok", "Stanley", "Parag Singla", "Matthew Richardson", "Pedro Domingos"], "venue": null, "citeRegEx": "al.2005.Kok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al.2005.Kok et al\\.", "year": 2005}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2013)", "citeRegEx": "al.2013.Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Kwiatkowski et al\\.", "year": 2013}, {"title": "Illinois-lh: A denotational and distributional approach to semantics", "author": ["Lai", "Hockenmaier2014]Lai", "Alice", "Julia Hockenmaier"], "venue": "In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Lai et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2014}, {"title": "A solution to Plato\u2019s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997]Landauer", "Thomas", "Susan Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015]Levy", "Omer", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics a\u0302A\u0306S\u0327 Human Language Technologies (NAACL HLT 2015),", "citeRegEx": "al.2015.Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al.2015.Levy et al\\.", "year": 2015}, {"title": "Combined distributional and logical semantics. Transactions of the Association for Computational Linguistics (TACL-2013), 1:179\u2013192", "author": ["Lewis", "Steedman2013]Lewis", "Mike", "Mark Steedman"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2013}, {"title": "A* ccg parsing with a supertag-factored model", "author": ["Lewis", "Steedman2014]Lewis", "Mike", "Mark Steedman"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2014)", "citeRegEx": "Lewis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2014}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang", "Jordan", "Klein2011]Liang", "Percy", "Michael Jordan", "Dan Klein"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers, 28(2):203\u2013208", "author": ["Lund", "Burgess1996]Lund", "Kevin", "Curt Burgess"], "venue": null, "citeRegEx": "Lund et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lund et al\\.", "year": 1996}, {"title": "SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marelli et al.2014a]Marelli", "Marco", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": "In Proceedings of the International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "al.2014a.Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014a.Marelli et al\\.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marelli et al.2014b]Marelli", "Marco", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In Proceedings of the 9th Edition of the Language Resources and Evaluation Conference (LREC", "citeRegEx": "al.2014b.Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014b.Marelli et al\\.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marelli et al.2014c]Marelli", "Marco", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014),", "citeRegEx": "al.2014c.Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al.2014c.Marelli et al\\.", "year": 2014}, {"title": "Artificial intelligence \u2013 a personal view", "author": ["Marr1977]Marr", "David"], "venue": "Artificial Intelligence,", "citeRegEx": "Marr1977.Marr and David.,? \\Q1977\\E", "shortCiteRegEx": "Marr1977.Marr and David.", "year": 1977}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov et al.2013]Mikolov", "Tomas", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of International Conference on", "citeRegEx": "al.2013.Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al.2013.Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Yih", "Zweig2013]Mikolov", "Tomas", "Wentau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008]Mitchell", "Jeff", "Mirella Lapata"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010]Mitchell", "Jeff", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Paperno", "Pham", "Baroni2014]Paperno", "Denis", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Events in the semantics of English", "author": ["Parsons1990]Parsons", "Terry"], "venue": "MIT press,", "citeRegEx": "Parsons1990.Parsons and Terry.,? \\Q1990\\E", "shortCiteRegEx": "Parsons1990.Parsons and Terry.", "year": 1990}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["Poon", "Domingos2006]Poon", "Hoifung", "Pedro Domingos"], "venue": "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI-06),", "citeRegEx": "Poon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2006}, {"title": "A machine-oriented logic based on the resolution principle", "author": ["A. J"], "venue": null, "citeRegEx": "J.,? \\Q1983\\E", "shortCiteRegEx": "J.", "year": 1983}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Roller", "Erk", "Boleda2014]Roller", "Stephen", "Katrin Erk", "Gemma Boleda"], "venue": "In Proceedings of the Twenty Fifth International Conference on Computational Linguistics", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "The Frobenius anatomy of word meanings I: subject and object relative pronouns", "author": ["Sadrzadeh", "Clark", "Coecke2013]Sadrzadeh", "Mehrnoosh", "Stephen Clark", "Bob Coecke"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Sadrzadeh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sadrzadeh et al\\.", "year": 2013}, {"title": "Grounded models of semantic representation", "author": ["Silberer", "Lapata2012]Silberer", "Carina", "Mirella Lapata"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL", "citeRegEx": "Silberer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2012}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["Snow", "Jurafsky", "Ng2006]Snow", "Rion", "Daniel Jurafsky", "Andrew Y. Ng"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL", "citeRegEx": "Snow et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Contextualizing semantic representations using syntactically enriched vector models", "author": ["Thater", "F\u00fcrstenau", "Pinkal2010]Thater", "Stefan", "Hagen F\u00fcrstenau", "Manfred Pinkal"], "venue": "In Proceedings of Association for Computational Linguistics", "citeRegEx": "Thater et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thater et al\\.", "year": 2010}, {"title": "Formal Philosophy. Selected Papers of Richard Montague", "author": ["H. Richmond"], "venue": null, "citeRegEx": "Richmond,? \\Q1974\\E", "shortCiteRegEx": "Richmond", "year": 1974}, {"title": "Logical inference on dependency-based compositional semantics", "author": ["Tian", "Miyao", "Takuya2014]Tian", "Ran", "Yusuke Miyao", "Matsuzaki Takuya"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010]Turney", "Peter", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Probabilistic semantics for natural language. In Logic and interactive rationality (LIRA) yearbook", "author": ["van Eijck", "Lappin2012]van Eijck", "Jan", "Shalom Lappin"], "venue": null, "citeRegEx": "Eijck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Eijck et al\\.", "year": 2012}, {"title": "Computational Semantics with Functional Programming", "author": ["van Eijck", "Unger2010]van Eijck", "Jan", "Christina Unger"], "venue": null, "citeRegEx": "Eijck et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eijck et al\\.", "year": 2010}, {"title": "GiSS: Combining SampleSearch and Importance Sampling for inference in mixed probabilistic and deterministic graphical models", "author": ["Venugopal", "Gogate2013]Venugopal", "Deepak", "Vibhav Gogate"], "venue": "In Proceedings of Association for the Advancement of Artificial Intelligence(AAAI-13)", "citeRegEx": "Venugopal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Venugopal et al\\.", "year": 2013}, {"title": "Fine-grained sentiment analysis with structural features", "author": ["Zirn et al.2011]Zirn", "C\u00e4cilia", "Mathias Niepert", "Heiner Stuckenschmidt", "Michael Strube"], "venue": "In Proceedings of the The 5th International Joint Conference on Natural Language Processing (IJCNLP", "citeRegEx": "al.2011.Zirn et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Zirn et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "There are now two hybrid approaches that combine logic and distributional semantics (Beltagy et al. 2013; Lewis and Steedman 2013), which both use logic-based semantics as a basis and add in distributional information to help with inference tasks. What is the status of such hybrid approaches? One possibility is to say that what we really want is a uniform framework that encompasses the abilities of both logic-based and distributional semantics, but until we have that, we will have to use hybrid systems. Another possibility \u2013 and this is the one that we will argue for \u2013 is that hybrid models are actually the right way to represent meaning. We follow Stokhof (2013) in assuming that meaning is a heterogenous phenomenon that is about truth conditions and grounding and observed contexts (among other things).", "startOffset": 85, "endOffset": 672}, {"referenceID": 6, "context": "In this paper, we discuss the system proposed in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013) in more detail, including improvements that allow MLN inference to scale more effectively (Beltagy and Mooney 2014) and to adapt logical", "startOffset": 86, "endOffset": 108}, {"referenceID": 6, "context": "Beltagy et al. (2013) transform distributional similarity to weighted distributional inference rules that are combined with logic-based sentence representations, and use probabilistic inference over both.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Beltagy et al. (2013) transform distributional similarity to weighted distributional inference rules that are combined with logic-based sentence representations, and use probabilistic inference over both. This is the approach that we build on in this paper. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word senses, and perform standard first-order inference on the resulting logical forms.", "startOffset": 0, "endOffset": 284}, {"referenceID": 6, "context": "Beltagy et al. (2013) transform distributional similarity to weighted distributional inference rules that are combined with logic-based sentence representations, and use probabilistic inference over both. This is the approach that we build on in this paper. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word senses, and perform standard first-order inference on the resulting logical forms. The main difference between the two approaches lies in the role of gradience. Lewis and Steedman view weights and probabilities as a problem to be avoided. We believe that the uncertainty inherent in both language processing and world knowledge should be front and center in the inference that we do. Tian, Miyao, and Takuya (2014) represent sentences using Dependency-based Compositional Semantics (Liang,", "startOffset": 0, "endOffset": 771}, {"referenceID": 53, "context": "We use Word2Vec\u2019s skip-gram algorithm to create vectors (Mikolov et al. 2013).", "startOffset": 56, "endOffset": 77}], "year": 2017, "abstractText": "NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not adequately capture overall sentence structure. So it has been argued that the two are complementary. In this paper, we adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs). We focus on textual entailment (RTE), a task that can utilize the strengths of both representations. Our system is three components, 1) parsing and task representation, where input RTE problems are represented in probabilistic logic. This is quite different from representing them in standard first-order logic. 2) knowledge base construction in the form of weighted inference rules from different sources like WordNet, paraphrase collections, and lexical and phrasal distributional rules generated on the fly. We use a variant of Robinson resolution to determine the necessary inference rules. More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that counteract scaling differences between resources. 3) inference, where we show how to solve the inference problems efficiently. In this paper we focus on the SICK dataset, and we achieve a state-of-the-art result. Our system handles overall sentence structure and phenomena like negation in the logic, then uses our Robinson resolution variant to query distributional systems about words and short phrases. Therefor, we use our system to evaluate distributional lexical entailment approaches. We also publish the set of rules queried from the SICK dataset, which can be a good resource to evaluate them.", "creator": "LaTeX with hyperref package"}}}