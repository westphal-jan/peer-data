{"id": "1505.06907", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2015", "title": "Using Dimension Reduction to Improve the Classification of High-dimensional Data", "abstract": "In this work we show that the classification performance of high-dimensional structural MRI data with only a small set of training examples is improved by the usage of dimension reduction methods. We assessed two different dimension reduction variants: feature selection by ANOVA F-test and feature transformation by PCA. On the reduced datasets, we applied common learning algorithms using 5-fold cross-validation. Training, tuning of the hyperparameters, as well as the performance evaluation of the classifiers was conducted using two different performance measures: Accuracy, and Receiver Operating Characteristic curve (AUC). Our hypothesis is supported by experimental results.", "histories": [["v1", "Tue, 26 May 2015 11:33:04 GMT  (189kb,D)", "http://arxiv.org/abs/1505.06907v1", "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)"]], "COMMENTS": "Presented at OAGM Workshop, 2015 (arXiv:1505.01065)", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["andreas gr\\\"unauer", "markus vincze"], "accepted": false, "id": "1505.06907"}, "pdf": {"name": "1505.06907.pdf", "metadata": {"source": "CRF", "title": "Using Dimension Reduction to Improve the Classification of High-dimensional Data", "authors": ["Andreas Gr\u00fcnauer", "Markus Vincze"], "emails": ["vincze}@acin.tuwien.ac.at"], "sections": [{"heading": "1 Introduction", "text": "Dimensional reduction methods are not only powerful tools to avoid overfitting [10], but also able to make the formation of high-dimensional data a more mathematically feasible task. In this paper, we will investigate the influence of dimension reduction techniques on the performance of various known classification methods. Dimension reduction methods are categorized into two groups: feature selection and feature transformation. Feature selection methods [9, 6] aim to identify a subset of \"meaningful\" features from the original set of features. They can be divided into filters, wrappers and embedded methods. Filter methods compile a score for each feature, and then select only those features that have the best scores. Wrapper methods train a predictive model of features before selecting the subsets with the best scores."}, {"heading": "2 Dimension reduction", "text": "Filtered feature selection by ANOVA F-Test feature selection methods based on filtering determine the relevance of features by calculating a score (usually based on a statistical metric or a test). In view of a number of selected features, only the features with the highest score are subsequently passed on to the classification algorithm. In this study, we use the ANOVA F-Test Statistics [8] for the feature evaluation. The F-Test evaluates whether the expected values of a quantitative random variable x within a number of predefined groups differ from each other. In this study, the F-value is defined as F = MSB MSW, MSB reflects the \"intermediate group variability,\" expressed as MSB = 1 (x-x-x), where ni is the number of observations in the i-group, x-total value reflects the \"intermediate group variability,\" the variable W in the SW, the group i-denotes the mean value of the sample x in the component SW, the mean x in the component SW, the variable W in the Same group, the mean value of the W in the sample i-denotes the sample x in the component SW, and the mean x in the component W."}, {"heading": "3 Classifiers", "text": "(S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S) (S (S) (S) (S (S) (S) (S (S) (S (S) (S (S) (S (S) (S (S) (S (S (S) (S (S) (S (S) (S (S (S) (S (S) (S (S (S) (S (S) (S (S (S) (S (S) (S (S) (S (S) (S (S) (S (S (S) (S (S) (S (S) (S (S) (S (S) (S (S) (S (S) (S) (S (S (S (S) (S) (S (S) (S (S) (S (S) (S) (S (S) (S) (S (S) (S) (S (S) (S (S (S) (S) (S) (S) (S (S (S) (S) (S) (S) (S"}, {"heading": "4 Experiment settings", "text": "In this study, we used the machine learning library scikit-learn 1 version 0.14.1 for all the methods and scoring metrics proposed in this study. This open source Python library offers a wide variety of machine learning algorithms based on a unified interface that facilitates the comparison of different methods for a given task."}, {"heading": "4.1 Dataset", "text": "In our experiments, we used the data set for the binary classification task of MLC 2014 [13]. This data set consists of 250 T1-weighted structural brain MRI scans: 150 scans, including target class labels for training and another 100 samples with unknown class labels, reserved for submission of the challenge. For each scan, 184 morphological summary features are available. These features represent volumes of cortical and subcortical structures as well as average thickness measurements within cortical regions. Volume measurements were normalized with intracranial volume (ICV) to account for different head sizes. All features were extracted using the brain MRI software FreeSurfer [5]."}, {"heading": "4.2 Evaluation measures", "text": "As recommended by the MLC 2014 Challenge [13], we used two common performance measures: Accuracy and the range below the Receiver Operating Characteristic Curve (AUC), both of which compare the classifier's predictions with the basic truth stated in the training data.1http: / / scikit-learn.orgAccuracy is defined as: Accuracy = tp + tn tp + fp + tn + fn, where tp, tn, fp, fn indicates the number of true positives, true negatives, false positives and false negatives. Range below the ROC curve (AUC) The ROC curve represents the compromise between the true positive rate (TPR = tptp + fn), expressed as TPR + fn, and the false positive rate (FPR), defined as FPR = fpfp + tn."}, {"heading": "4.3 5-Fold cross-validation (CV)", "text": "We used a 5-fold CV by randomly dividing the training data set (D) of 150 samples into five mutually exclusive subsets (D1, D2, D3, D4, D5) of approximately the same size. Each classification model was trained and tested five times, each time (t-1, 2, 3, 4, 5) being trained on all but one fold (D-Dt) and tested on the remaining fold (Dt). Accuracy and AUC readings were averaged across the individual measurements of the five individual test folds."}, {"heading": "4.4 Experiment Methodology", "text": "Our experiments were conducted as follows: We applied each method of dimension reduction to the original training set with a different number of selected components: 3, 6, 12, 24, 48, 92, 184. We trained the classifiers on the 150 datasets with known target class names, using a 5x resume in two ways: firstly by optimizing the degree of accuracy and secondly by optimizing the AUC measure. For classifiers based on a set of specific hyperparameters, we performed a grid search to find the optimal configuration of the hyperparameters. Since an exhaustive search of all possible hyperparameters would be an impossible task, we limited our scope to a subset of hyperparameters for each classifier with a discrete set of tested values. Table 1 shows the selected hyperparameters and the corresponding set of values for each classifier."}, {"heading": "5 Results", "text": "Fig. 1 shows the performance of the classifiers based on ANOVA F > Test Feature Selection with Accuracy (Fig. 1a) and AUC (Fig. 1b) for hyperparameter tuning and performance evaluation. Both figures show that the classifiers with = 12 selected features already achieve the same or better performance than with the original s = 184 features. If the number of selected features is further increased, the performance of the RBF-SVM reaches its peak at s = 92, while the performance of the other classifiers does not decrease better or rather. This observation shows the importance of feature selection as more features do not necessarily lead to better performance (revision). Fig. 2 shows the classifier performance based on PCA-reduced data using Accuracy (Fig. 2a) and AUC (Fig. 2b) for hyperparameter tuning evaluation and performance evaluation of fixes."}, {"heading": "6 Discussion", "text": "The performance of the majority of the classifiers studied converges consistently for the same number of selected characteristics, regardless of the measure used to adjust hyperparameters, suggesting that finding the optimal number of selected characteristics is a robust method for improving classifiers \"performance in the face of high-dimensional data. Results confirm that the RBFSVM classifier outperforms other classifiers regardless of the number of reduced characteristics, but the results also show that linear classifiers such as GNB and Ridge are able to achieve the same or even better results in reduced dimensions when using the methods of the selected feature selection than the RBF-SVM classifier."}, {"heading": "7 Conclusion", "text": "The performance of classifiers is analyzed under different ratings for hyperparameter setting combined with different methods of dimension reduction. Both dimension reductions improved the performance of all classifiers compared to the original high-dimensional data. Results showed that the selection of features of the ANOVA F test yielded better results compared to PCA-based feature transformation."}, {"heading": "Acknowledgments", "text": "The research that led to these results was funded by the European Community under the Horizon 2020 programme (H2020-ICT-2014-1) under Funding Agreement No. 641474, FLOBOT. This work was supported by the Pattern Recognition and Image Processing Working Group at the Vienna University of Technology. I thank Yll Haximusa and Roxane Licandro for their thoughtful comments and corrections."}], "references": [{"title": "Shape quantization and recognition with randomized trees", "author": ["Y Amit", "D Geman"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher Bishop"], "venue": "Springer, 1st ed", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Pattern Classification and Scene Analysis: Pattern Classification. Wiley", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "An introduction to variable and feature selection", "author": ["I Guyon", "A Elisseeff"], "venue": "Journal of Machine Learning Research: Special Issue on Variable and Feature Selection, 3:1157\u20131182", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Principal Component Analysis", "author": ["IT Jolliffe"], "venue": "Springer-Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Statistical concepts: a second course", "author": ["Richard G Lomax", "Debbie L Hahs-Vaughn"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Feature selection for dimensionality reduction", "author": ["Dunja Mladeni\u0107"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Feature selection, L1 vs. L2 regularization, and rotational invariance", "author": ["Andrew Y. Ng"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Small sample size effects in statistical pattern recognition: recommendations for practitioners and open problems", "author": ["S.J. Raudys", "A.K. Jain"], "venue": "Pattern Recognition", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1990}, {"title": "Regularized Least-Squares Classification", "author": ["Ryan Rifkin", "Gene Yeo", "Tomaso Poggio"], "venue": "Nato Science Series Sub Series III Computer and Systems Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "MICCAI 2014 machine learning challenge.  https://www.nmr.mgh.harvard", "author": ["Mert R. Sabuncu", "Ender Konukoglu"], "venue": "edu/lab/laboratory-computational-imaging-biomarkers/ miccai-2014-machine-learning-challenge", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The curse of dimensionality in data mining and time series prediction", "author": ["Michel Verleysen", "Damien Fran\u00e7ois"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "A comparative study on feature selection in text categorization", "author": ["Yiming Yang", "Jan O. Pedersen"], "venue": "In Proceedings of ICML-97, 14th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "The Optimality of Naive Bayes", "author": ["Harry Zhang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}], "referenceMentions": [{"referenceID": 11, "context": "Unfortunately, the probability of overfitting of a learning algorithm increases with the number of features [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "Dimension reduction methods are not only powerful tools to avoid overfitting [10], but also capable of making the training of high-dimensional data a computationally more feasible task.", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "Feature selection methods [9, 6] aim to identify a subset of \u201cmeaning-ful\u201d features out of the original set of features.", "startOffset": 26, "endOffset": 32}, {"referenceID": 3, "context": "Feature selection methods [9, 6] aim to identify a subset of \u201cmeaning-ful\u201d features out of the original set of features.", "startOffset": 26, "endOffset": 32}, {"referenceID": 6, "context": "Among various feature selection methods, we limit our scope on filter methods, as they do not depend on a specific classification method and therefore are suitable for the comparison of different classifiers [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 12, "context": "and ANOVA F-test are among the most effective scores for filtered feature selection [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "As part of the 17th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), the MICCAI 2014 Machine Learning Challenge (MLC) aims for the objective comparison of the latest machine learning algorithms applied on Structural MRI data [13].", "startOffset": 273, "endOffset": 277}, {"referenceID": 5, "context": "In this study, we use the ANOVA F-Test statistic [8] for the feature scoring.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "Feature transformation by PCA PCA [7] reduces the dimension of the data by finding the first s orthogonal linear combinations of the original variables with the largest variance.", "startOffset": 34, "endOffset": 37}, {"referenceID": 13, "context": "Gaussian Naive Bayes (GNB) Bayes classifiers are based on the Bayes\u2019 theorem and depend on naive (strong) independence assumptions [16, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 2, "context": "Gaussian Naive Bayes (GNB) Bayes classifiers are based on the Bayes\u2019 theorem and depend on naive (strong) independence assumptions [16, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 9, "context": "Ridge The Ridge classifier is based on Ridge Regression, which extends the Ordinary Least Squares (OLS) method with an additional penalty term to limit the L2-norm of the weight vector [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "By generating a set of trees in randomly selected subspaces of the feature space [1], the different trees generalize their classification in complementary ways.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "1 Dataset In our experiments, we used the dataset for the binary classification task of the MLC 2014 [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "2 Evaluation measures As recommended by the MLC 2014 challenge [13], we used two common performance measures: Accuracy and the area under the Receiver Operating Characteristic curve (AUC).", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "This phenomenon is known as the \u201csmall sample size\u201d problem [11].", "startOffset": 60, "endOffset": 64}], "year": 2015, "abstractText": "In this work we show that the classification performance of high-dimensional structural MRI data with only a small set of training examples is improved by the usage of dimension reduction methods. We assessed two different dimension reduction variants: feature selection by ANOVA F-test and feature transformation by PCA. On the reduced datasets, we applied common learning algorithms using 5-fold crossvalidation. Training, tuning of the hyperparameters, as well as the performance evaluation of the classifiers was conducted using two different performance measures: Accuracy, and Receiver Operating Characteristic curve (AUC). Our hypothesis is supported by experimental results.", "creator": "LaTeX with hyperref package"}}}