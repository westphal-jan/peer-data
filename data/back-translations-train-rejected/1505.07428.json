{"id": "1505.07428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2015", "title": "Training a Convolutional Neural Network for Appearance-Invariant Place Recognition", "abstract": "Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.", "histories": [["v1", "Wed, 27 May 2015 18:21:54 GMT  (4919kb,D)", "http://arxiv.org/abs/1505.07428v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["ruben gomez-ojeda", "manuel lopez-antequera", "nicolai petkov", "javier gonzalez-jimenez"], "accepted": false, "id": "1505.07428"}, "pdf": {"name": "1505.07428.pdf", "metadata": {"source": "CRF", "title": "Training a Convolutional Neural Network for Appearance-Invariant Place Recognition", "authors": ["Ruben Gomez-Ojeda", "Manuel Lopez-Antequera", "Nicolai Petkov", "Javier Gonzalez-Jimenez"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "The process of identifying images belonging to the same place that is normally known as a place of recognition is still an open problem in the computer world. Placement recognition is a key factor in mobile robotics and in the autonomous driving of applications, such as visual reality, where the user receives information about important places, monuments or texts that can be included in the tasks of localization. [20] The difficulties caused by changes in the environment make it difficult to identify a place where one can think intuitively (see Figure 1). In this context, we focus on scenarios without great appearance."}, {"heading": "2. Related Work", "text": "In fact, it is such that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is"}, {"heading": "3. Methodology", "text": "To solve the task of determining whether an image belongs to a previously visited location, we propose to train a revolutionary neural network to embed images in a low-dimensional space where Euclidean distance is a location dissimilarity. Our solution is based on content-based image restoration work, but our network is trained to generate a feature vector that is invariable to dramatic changes in the scene, such as seasonal changes. To achieve this, we train the network using labeled data sets that represent the same locations under different lighting, view, or weather conditions. Below, we describe the architecture and training process of the proposed network, presenting the network with triplets of images consisting of a querying image xi, an image of the same location xj, and an image of another location xk."}, {"heading": "3.1. Architecture of the CNN", "text": "Faced with the difficulties of training a revolutionary neural network from the ground up using a relatively small specialized data set, we choose the approach of modifying a pre-trained network. In particular, we use the reference network CaffeNet [8], which reflects the architecture of Krizhevsky et al. [9], of which we retain only the first four revolutionary layers and replace the rest with a single fully connected layer, which is our descriptor output (see Figure 2). Since we discard all fully connected layers, we are not limited to the original input size of 227 x 227 pixels, and instead work with a smaller input of 160 x 120."}, {"heading": "3.2. Description of the Cost Function", "text": "In short, the network maps the input image aM \u00b7 N \u00b7 C to a descriptor vector of length D corresponding to the activation of the output layer of CNN, i.e.: h: RM \u00b7 N \u00b7 C 7 \u2212 \u2192 RDx 7 \u2212 \u2192 h (x) (1) where h (x) is the descriptor of the image x, whose euclidean distances to other descriptors must be representative of location differences. To achieve this behavior, the network parameters are obtained by minimizing the following objective mode of operation, the second term being a regulation of the network parameters, and the first term L being the sum of the cost functions of all the triplets that can be obtained by minimizing the following objective mode of operation. This can be expressed in such a way that L = can be resolved (xi, xj, xk), the second term being a regulation of the network parameters, and the first term L being the sum of the cost functions of all triplets."}, {"heading": "3.3. Training the CNN", "text": "In order to achieve the desired invariances in the representation generated by the network, triplets must be selected to provide relevant visual clues (see Figure 3 for an example).We train the network using a mix of triplets from multiple datasets, which are detailed in the following sections to improve invariance against lighting, weather and point of view change.The network is trained using the Caffe library [8], which has been modified to include the previously described cost function.As explained, the weights of the four convolutionary layers are fine-tuned by the CaffeNet Reference Network, an implementation of [9], while the last fully connected layer is new. We scale the learning rate of the trained layers by a factor of 1 / 1000 and set the global learning rate to 0.001. Margin \u03b2 is set to 1 and the regulation constant to 0.0005. We train 40,000 iterations, with a total of 1.2 million triplets in the following two parts."}, {"heading": "3.3.1 KITTI Dataset", "text": "The odometry benchmark from the KITTI dataset [7] consists of 11 training sequences with accurate trajectory ground truth and 10 non-trajectory test sequences for evaluation. Both the training and the test sequences are stereo images extracted from urban daylight environments. We select triplets to increase the robustness of the network against facial changes by selecting the similar pair in a variety of relative poses. We also verify that the different pairs do not belong to the same place by using the ground truth location (as there are loop closures in the sequences). Figure 3 shows a triplet extracted from the KITTI dataset."}, {"heading": "3.3.2 Alderley Dataset", "text": "We have also trained the network with the Alderley dataset [13], which contains major changes in lighting and weather conditions, consisting of two sequences of 8 km along the Brisbane suburb of Alderley, Australia. The first was taken on a clear morning, while the second was taken on a stormy night in poor visibility (see Figure 4). To achieve robustness to the above changes, we provide the network with challenging triplets during training, combining images from both sequences (we used the first 10k images from the day sequence and their matches from the night sequence for training, while we reserve the rest for experiments)."}, {"heading": "3.3.3 Nordland Dataset", "text": "The Nordland dataset [26], taken from Norwegian broadcaster NRK's television documentary \"Nordlandsbanen - Minutt for Minutt,\" consists of a 728 km train journey linking the cities of Trondheim and Bod\u00f8 in Norway. Recorded once a season, the sequence contains challenging changes in appearance, as shown in Figure 1. In addition, due to the large length of the dataset, it offers different weather conditions (the sequences are about 10 hours long). We produce triplets by providing two images of the same place at different seasons and one image of a different place at each season (we verify that the images actually originate from different locations by using the attached GPS ground truth)."}, {"heading": "4. Experimental Evaluation", "text": "To validate the proposed network, we conduct a series of experiments comparing the behavior of our system with two state-of-the-art space detection techniques: DBoW2 [14] and a feature vector extracted from an internal layer of a neural network trained for object classification as in [25].The implementations actually used are the official distribution of ORB-SLAM [14] and the CaffeNet [8] implementation of [9], which we simply refer to in this paper as CaffeNet. The resolution of the input images is 160 x 120 in our proposal, 227 x 227 in CaffeNet and the native resolution of each dataset in DBoW2. Below, we first describe the methodology used for the comparison, then present a series of experiments with datasets from multiple environments under different appearance changes. Finally, we also compare the computational costs of the algorithms and their feasibility for looping tasks, e.g. LAM."}, {"heading": "4.1. On Comparing Confusion Matrices", "text": "The key element of a placement detection system is the estimation of similarity between the compared images. To this end, we calculate a descriptor h (xi) for each input frame xi, and then we estimate the similarity with other images by comparing the euclidean distance from their descriptors. A widely used measurement in place captures any distance (or score) in a confusion matrix in which the rows and columns express the database and the query sequence, that is, M (i, j) = h (xi) \u2212 h (xj).In our case, a normalized confusion matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix images."}, {"heading": "4.2. KITTI Dataset", "text": "First, we compare the performance of the state-of-the-art algorithms with our proposal by processing the test sequences from the KITTI dataset [7], which has a resolution of 1241 x 376. Figure 5 shows the confusion matrices obtained with the KITTI-11 sequence by comparing images from the left and right cameras, where we can observe a good performance of all methods. We also note that both DBoW2 and CaffeNet have a thin diagonal and they do not show good matches outside the diagonal. In contrast, the confusion matrix obtained with our approach presents a thicker diagonal and also low-value zones, which correspond to parts of the sequence where the car either stops or circulates at low speed. It implies that our approach is more robust to changes in vision and is therefore a more versatile option for location recognition tasks where the camera does not have to be in exactly the same place."}, {"heading": "4.3. Ma\u0301laga Urban Dataset", "text": "We also evaluate the performance of the techniques with the Ma'laga Urban Dataset [2], which contains images of a stereo camera with a resolution of 1024 \u00d7 768, and data taken by five laser scanners during a 37 km long sequence in Ma'laga (Spain) with cloudy weather and direct sunlight in several parts of the sequence. As can be seen in Figure 8, the urban structure represented by this dataset differs significantly from that in the KITTI sequences, making it a challenging environment as none of the methods have been trained with this dataset. Figure 9 shows the performance of the three comparative methods when tested on the Ma'laga-10 (we use the left sequence as a database and the right as a query).While the CNN-based methods work well, with a small superiority of CaffeNet, DBoW2 exhibits poor behavior, with a high percentage of outliers (this proves that both are capable of being used in multiple sets of CNN)."}, {"heading": "4.4. Nordland Dataset", "text": "As mentioned above, the Nordland dataset [26] contains sequences with a resolution of 1920 x 1080 pixels from the same perspective over the four seasons, resulting in serious changes in the appearance of the environment. For these experiments, we used the last hour of the dataset that was not used for training, and removed the segments that comprise either tunnels or stations. Figure 10 shows the performance curves of the three approaches by comparing the most difficult sequence pair, summer and winter (other seasonal combinations produce similar results).We observe the better performance of our proposal against CaffeNet, which exhibits significantly less failure phenomena than our approach for all diagonal widths, both k = 5 and k = 10, which is logical since neither CaffeNet nor DBoW2 were trained to be robust against these changes in appearance."}, {"heading": "4.5. Alderley Dataset", "text": "We have also tested the robustness to challenging weather and lightning conditions of the threat methods by processing the last 5k images from the day sequence and their matches from the night sequence of the Alderley dataset (which has a resolution of 640 x 260). Figure 11 shows the superiority of our proposal over CaffeNet and DBoW2, with a better ratio of inliers to diagonal width in all cases. However, we note that all three approaches achieve a low ratio as it is a highly demanding dataset. Therefore, the use of sequentiality-based post-processing technology would be inevitable to obtain a system with reasonable performance in similar scenarios."}, {"heading": "4.6. Performance", "text": "Finally, we examine the computing power in several aspects shown in Table 1. Our tests run on an Intel Core i7-3770, while our GPU tests are also based on an NVidia GeForce GTX 790. First, we measure the time required to process a single image. In both CNN-based methods, the value includes loading the image and performing a forward pass to obtain the feature vector. In the case of DBoW2 [15], we measure the time required to compute the word histogram. Since the image resolution for DBoW2 is variable depending on the dataset, we have included the minimum and maximum average times from all sequences. Results suggest that DBoW2 is less demanding than both CNN-based methods and that ours is three times faster than the use of the reference network CaffeNet. Then, we measure the size of the descriptor, which is relevant since the calculation cost of calculating the net length of the loop matrix is significantly greater than the official length required for each of the data matrix."}, {"heading": "5. Conclusions", "text": "The network embeds images in a 128-dimensional space where samples from similar locations are separated by small Euclidean distances. The network has been trained using treble images from datasets with weather, lighting and vision changes to enable the network to learn invariances for these changes. The proposed network surpasses the most advanced location recognition methods in several sophisticated datasets and provides superior robustness for vision and weather changes. The small size of the resulting vector makes our system suitable for applications requiring long-term operation."}], "references": [{"title": "SURF: Speeded Up Robust Features BT - Computer VisionECCV", "author": ["H. Bay", "T. Tuytelaars", "L. Gool"], "venue": "Computer VisionECCV 2006,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "The M\u00e1laga urban dataset: High-rate stereo and LiDAR in a realistic urban scenario", "author": ["J.-L. Blanco-Claraco", "F.-A. Moreno-Due\u00f1as", "J. Gonz\u00e1lez-Jim\u00e9nez"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "BRIEF: Binary robust independent elementary features. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture", "author": ["M. Calonder", "V. Lepetit", "C. Strecha", "P. Fua"], "venue": "Notes in Bioinformatics),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Convolutional Neural Network-based Place Recognition", "author": ["Z. Chen", "O. Lam", "A. Jacobson", "M. Milford"], "venue": "arXiv preprint arXiv:1411.1509,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance", "author": ["M. Cummins", "P. Newman"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Bags of binary words for fast place recognition in image sequences. Robotics", "author": ["D. Galvez-Lopez", "J.D. Tardos"], "venue": "IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Outdoor place recognition in urban environments using straight lines", "author": ["J.H. Lee", "S. Lee", "G. Zhang", "J. Lim", "W.K. Chung", "I.H. Suh"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Vision-based place recognition: how low can you go", "author": ["M. Milford"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "SeqSLAM: Visual routebased navigation for sunny summer days and stormy winter nights", "author": ["M.J. Milford", "G.F. Wyeth"], "venue": "Proceedings - IEEE International Conference on Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "ORB- SLAM: a Versatile and Accurate Monocular SLAM System", "author": ["R. Mur-Artal", "J.M.M. Montiel", "J.D. Tardos"], "venue": "arXiv preprint arXiv:1502.00956,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Fast Relocalisation and Loop Closing in Keyframe-Based SLAM", "author": ["R. Mur-Artal", "J.D. Tard\u00f3s"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Superpixelbased appearance change prediction for long-term navigation across seasons", "author": ["P. Neubert", "N. Sunderhauf", "P. Protzel"], "venue": "European Conference on Mobile Robots, ECMR 2013 - Conference Proceedings,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Scalable recognition with a vocabulary tree", "author": ["D. Nist\u00e9r", "H. Stew\u00e9nius"], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "All-environment visual place recognition with smart", "author": ["E. Pepperell", "P.I. Corke", "M.J. Milford"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Appearancebased indoor localization: A comparison of patch descriptor performance", "author": ["J. Rivera-Rubio", "I. Alexiou", "A.A. Bharath"], "venue": "Pattern Recognition Letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "ORB: an efficient alternative to SIFT or SURF", "author": ["E. Rublee", "V. Rabaud", "K. Konolige", "G. Bradski"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "arXiv preprint arXiv:1409.0575,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Video Google: a text retrieval approach to object matching in videos", "author": ["J. Sivic", "a. Zisserman"], "venue": "Proceedings Ninth IEEE International Conference on Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "On the Performance of ConvNet Features for Place Recognition", "author": ["N. Sunderhauf", "F. Dayoub", "S. Sareh", "U. Ben", "M. Michael"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Are we there yet? challenging seqslam on a 3000 km journey across all four seasons", "author": ["N. S\u00fcnderhauf", "P. Neubert", "P. Protzel"], "venue": "Proc. of Workshop on Long-Term Autonomy, IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "MSLD: A robust descriptor for line matching", "author": ["Z. Wang", "F. Wu", "Z. Hu"], "venue": "Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Learning descriptors for object recognition and 3d pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "arXiv preprint arXiv:1502.05908,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Place recognition is a key part in mobile robotics and autonomous driving applications, such as vision-based simultaneous localization and mapping (SLAM) systems, where revisiting a location introduces important information which can be employed in the tasks of localization [20] and loop closure [15].", "startOffset": 275, "endOffset": 279}, {"referenceID": 14, "context": "Place recognition is a key part in mobile robotics and autonomous driving applications, such as vision-based simultaneous localization and mapping (SLAM) systems, where revisiting a location introduces important information which can be employed in the tasks of localization [20] and loop closure [15].", "startOffset": 297, "endOffset": 301}, {"referenceID": 25, "context": "Frames extracted from the Nordland dataset [26] that belong to the same place in winter, spring, summer and fall.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "In that context, most methods employ bags of visual words inspired by [24] and [17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "In that context, most methods employ bags of visual words inspired by [24] and [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 93, "endOffset": 96}, {"referenceID": 12, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 242, "endOffset": 246}, {"referenceID": 8, "context": "Convolutional neural networks (CNNs) are gaining importance in most classification tasks [9].", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "When used as generic feature generators, they often outperform the state-of-art algorithms even for tasks different to classification [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 3, "context": "However, their use in place recognition is limited to the exploitation of generic features extracted from the internal layers of pre-trained CNNs [4][25].", "startOffset": 146, "endOffset": 149}, {"referenceID": 24, "context": "However, their use in place recognition is limited to the exploitation of generic features extracted from the internal layers of pre-trained CNNs [4][25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 13, "context": "We demonstrate these claims with extensive experimentation in several challenging datasets, where we compare our proposal with two state-of-art algorithms: DBoW2 [14], and a generic network as in [18].", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "We demonstrate these claims with extensive experimentation in several challenging datasets, where we compare our proposal with two state-of-art algorithms: DBoW2 [14], and a generic network as in [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "One of the first SLAM techniques which introduced BoW in this context was FAB-MAP [5], where a probabilistic approach to place recognition based on the local appearance of each location was proposed.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "This was tackled in [6] with DBoW2, where for the first time they introduced the use bags of binary words obtained from BRIEF descriptors, reducing in more than an order of magnitude the time employed in the feature extraction process.", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "An improved version of this algorithm has been recently published in [14], where the authors build a urban dictionary based on ORB [21] which yields a better performance in popular datasets.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "An improved version of this algorithm has been recently published in [14], where the authors build a urban dictionary based on ORB [21] which yields a better performance in popular datasets.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "In [10], the authors deal with that by building a vocabulary tree that employs straight lines in combination with the MSLD descriptor [27], which increases the robustness against changes in weather conditions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [10], the authors deal with that by building a vocabulary tree that employs straight lines in combination with the MSLD descriptor [27], which increases the robustness against changes in weather conditions.", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "in [16], where they propose a place recognition algorithm capable of working across seasons.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "On the other hand, the algorithm is only tested with the Nordland dataset [26], which shows extreme seasonal changes, and hence it will not predict gradual changes in the environment.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "A different strategy works on local sequences instead of estimating the best single location, with the proposal of Milford and Wyeth as one of the most relevant contributions [13].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Recently, another group of techniques has irrupted with promising results, motivated by the outstanding performance achieved by CNNs as generic feature generators in several classification tasks [19].", "startOffset": 195, "endOffset": 199}, {"referenceID": 24, "context": "In this context, a recent work is [25], where the authors employ a pre-trained network named OverFeat [23], which was the", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "In this context, a recent work is [25], where the authors employ a pre-trained network named OverFeat [23], which was the", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "N is a local contrast normalization operation acting across channels as applied in [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 21, "context": "winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "We apply a training technique similar to [28], where the network presented with triplets of images, formed by a query image xi, an image from the same location, xj , and an image of a different location, xk.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "In particular, we resort to the reference CaffeNet network [8], which mirrors the architecture of Krizhevsky et al.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "[9], from which we only keep the first four convolutional layers, replacing the rest with a single fully connected layer which is our descriptor output (see Figure 2).", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "The cost function employed is similar to that in [28], and can be expressed as:", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "Training triplet extracted from the KITTI dataset [7], where large viewpoint invariances are present.", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "The network is trained using the Caffe library [8], modified to include the previously described cost function.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "As previously explained, the weights of the four convolutional layers are fine-tuned from the CaffeNet reference network, an implementation of [9], whereas the final fully connected layer is new.", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "The odometry benchmark from the KITTI dataset [7] is comprised of 11 training sequences with accurate ground truth of the trajectory, and 10 test sequences without ground truth for evaluation.", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "We have also trained the network with the Alderley dataset [13], which contains severe changes in illumination and weather conditions.", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "The Nordland dataset [26], extracted from the TV documentary \u201cNordlandsbanen - Minutt for Minutt\u201d produced by the Norwegian Broadcasting Corporation NRK consists of a 728 km long train journey connecting the cities of Trondheim and Bod\u00f8 in Norway.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "Frames extracted from the Alderley dataset [13], where drastic illumination changes are present.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "our system with two state-of-art techniques in place recognition: DBoW2 [14], and a feature vector extracted from an internal layer of a neural network trained for object classification as in [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "our system with two state-of-art techniques in place recognition: DBoW2 [14], and a feature vector extracted from an internal layer of a neural network trained for object classification as in [25].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "The actual implementations used are the official distribution of ORB-SLAM [14], and the CaffeNet [8] implementation of [9], which we simply name as CaffeNet in this work.", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "The actual implementations used are the official distribution of ORB-SLAM [14], and the CaffeNet [8] implementation of [9], which we simply name as CaffeNet in this work.", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "The actual implementations used are the official distribution of ORB-SLAM [14], and the CaffeNet [8] implementation of [9], which we simply name as CaffeNet in this work.", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "\u25e6 DBoW2 [14]: the proposed score is already normalized, but their approach associates high scores to similar images, thus we estimate the complementary matrix before the comparison.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "\u25e6 CaffeNet [25]: we extract the convolutional layers outputs conv4, which present the best results for the tested datasets, and compare them using Euclidean distance as they propose.", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "Place recognition methods for loop closure generally employ post-processing techniques to find good matches which actually represent the same location in the confusion matrix, usually by looking for sequences of similar frames [12] [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 17, "context": "Place recognition methods for loop closure generally employ post-processing techniques to find good matches which actually represent the same location in the confusion matrix, usually by looking for sequences of similar frames [12] [18].", "startOffset": 232, "endOffset": 236}, {"referenceID": 6, "context": "First, we compare the performance of the state-of-art algorithms with our proposal by processing the test sequences from the KITTI dataset [7], which has a resolution of 1241 \u00d7 376.", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "We also evaluate the performance of the techniques with the M\u00e1laga Urban Dataset [2], which contains frames obtained from a stereo camera, with a resolution of 1024\u00d7768,", "startOffset": 81, "endOffset": 84}, {"referenceID": 25, "context": "As mentioned above, the Nordland Dataset [26] includes sequences with 1920\u00d7 1080 resolution from the same perspective during the four seasons of the year, which leads to severe changes in the appearance of the environment.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "Frame extracted from the M\u00e1laga Urban Dataset [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "In the case of DBoW2 [15], we measure the time required to compute the bag-of-words histogram.", "startOffset": 21, "endOffset": 25}], "year": 2015, "abstractText": "Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.", "creator": "LaTeX with hyperref package"}}}