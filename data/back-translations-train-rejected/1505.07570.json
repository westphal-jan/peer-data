{"id": "1505.07570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2015", "title": "A Practical Guide to Randomized Matrix Computations with MATLAB Implementations", "abstract": "Matrix operations such as matrix inversion, eigenvalue decomposition, singular value decomposition are ubiquitous in real-world applications. Unfortunately, many of these matrix operations so time and memory expensive that they are prohibitive when the scale of data is large. In real-world applications, since the data themselves are noisy, machine-precision matrix operations are not necessary at all, and one can sacrifice a reasonable amount of accuracy for computational efficiency.", "histories": [["v1", "Thu, 28 May 2015 07:33:21 GMT  (393kb,D)", "https://arxiv.org/abs/1505.07570v1", null], ["v2", "Wed, 3 Jun 2015 14:33:48 GMT  (394kb,D)", "http://arxiv.org/abs/1505.07570v2", null], ["v3", "Sun, 7 Jun 2015 07:52:43 GMT  (399kb,D)", "http://arxiv.org/abs/1505.07570v3", null], ["v4", "Wed, 29 Jul 2015 09:26:48 GMT  (399kb,D)", "http://arxiv.org/abs/1505.07570v4", null], ["v5", "Mon, 17 Aug 2015 06:39:40 GMT  (399kb,D)", "http://arxiv.org/abs/1505.07570v5", null], ["v6", "Tue, 3 Nov 2015 03:26:49 GMT  (832kb,D)", "http://arxiv.org/abs/1505.07570v6", null]], "reviews": [], "SUBJECTS": "cs.MS cs.LG", "authors": ["shusen wang"], "accepted": false, "id": "1505.07570"}, "pdf": {"name": "1505.07570.pdf", "metadata": {"source": "CRF", "title": "A Practical Guide to Randomized Matrix Computations with MATLAB Implementations", "authors": ["Shusen Wang"], "emails": ["wssatzju@gmail.com"], "sections": [{"heading": null, "text": "A Practical Guide to Randomized Matrix Computations with MATLABImplementations1Shusen Wang wssatzju @ gmail.com. November 4, 20151Sample MATLAB code with demos is available at https: / / github.com / wangshusen / RandMatrixMatlab.ar Xiv: 150 5.07 570v 6 [cs.M S] 3 November 201 52ContentsAbstract 1"}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Elementary Matrix Algebra 5", "text": "2.1 Notation................................................ 52.2 Matrix decompositions.......................... 52.3 Matrix (pseudo) Reverse and orthogonal projector............................ 62.4 Time and storage costs......................."}, {"heading": "3 Matrix Sketching 9", "text": "3.1. Theoretical foundations.........................................................................................................................................................................................................................................."}, {"heading": "4 Regression 19", "text": "4.1. Standard Solutions..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Rank k Singular Value Decomposition 25", "text": "5.1. Standard Solutions.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 SPSD Matrix Sketching 31", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "A Several Facts of Matrix Algebra 47", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Notes and Further Reading 49", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "2.1 Notation", "text": "Let A = [aij] be a matrix, a = [ai] be a column vector and a be a scalar. The i-th row and the j-th column of A are denoted by ai: or a: j. If there is no ambiguity, either a column or row can be written as al. Let the n \u00b7 n be the identity matrix, i.e. the diagonal entries are one and the non-diagonal entries are zeros. The column space (the space spanned by the columns) of A is the set of all possible linear combinations of its column vectors. Let [n] be the set {1, 2, \u00b7 \u00b7 \u00b7, n}. Let nnz (A) be the number of unequal entries of A. The square vector '2 norm is defined by the number of its possible linear combinations."}, {"heading": "2.2 Matrix Decompositions", "text": "The QR decay of A is A = QA n,,"}, {"heading": "2.3 Matrix (Pseudo) Inverse and Orthogonal Projector", "text": "For a n \u00b7 n square matrix A, the matrix exists inversely if A is not singular (rank (A) = n). Let A \u2212 1 be the opposite of A. Then AA \u2212 1 = A \u2212 1A = In.Only square and full-scale matrices have the opposite. For general rectangular matrices or rank-deficient matrices, the pseudo-inverse of the matrix is used as a generalization of the matrix. [1] The book [1] provides a comprehensive study of pseudo-inverses. 6The Moore-Penrose inverse is the most common pseudo-inverse defined by A \u2020: = VA\u03a3 \u2212 1 A U T A. Let A be every m \u00b2 n and rank of a matrix. ThenAA \u2020 = UActuA V T AVA is the A matrix of A \u00b2, since it is a projection of A."}, {"heading": "2.4 Time and Memory Costs", "text": "The temporal complexity of matrix operations is listed below. \u2022 Multiply a m \u00b7 n matrix A by a n \u00b7 p matrix B: O (mnp) floatpoint operations (flops) in general and O (p \u00b7 nnz (A) if A is sparse. \u2022 k-SVD of a m \u00b7 n matrix: O (nmk) flops (provided that the spectral gap and logarithm of error tolerance are constant) \u2022 Matrix inversion or complete self-value degradation of a n \u00b7 n matrix: O (n3) flops \u2022 k self-value calculation of an n matrix: C (n2k) flop. Pass-efficient means that the comparison of a constant passes through the cost of a constant."}, {"heading": "3.1 Theoretical Properties", "text": "Property 3.1 (Subspace Embedding): For a fixed m \u00b7 n (m n) matrix A and all dimensional vectors y, the inequality is 1 \u03b3 \u00b1 y TAS \u0445 22 \u2264 yTA \u0445 22 \u2264 \u03b3holds a high probability. Here, S-Rn \u00b7 s (s n) is a specific sketching matrix. The subspace in which the property is embedded can be intuitively understood as follows: For all n dimensional vectors x in the row space of A (a rank m subspace within Rn), the length of the vector x does not change much after sketching."}, {"heading": "It is stronger because \u2016A\u2212CC\u2020A\u20162F \u2264 minrank(X)\u2264k \u2016A\u2212CX\u20162F .", "text": "Intuitively, the Low Rank Approximation property means that the columns of Ak are almost in the column space of C = AP. The Low Rank Approximation property allows us to solve k-SVD more efficiently (for k \u2264 s). Later, we will see that the calculation of the k-SVD of CC \u2020 A is cheaper than the k-SVD of A. The two properties can be verified by a few lines of MATLAB code."}, {"heading": "3.2 Random Projection", "text": "The section presents three techniques of matrix sketching: Gaussian projection, subsampled randomized Hadamard transform (SRHT) and counting sketch. Gaussian projection and SRHT can be combined with counting sketch."}, {"heading": "3.2.1 Gaussian Projection", "text": "The n \u00b7 s Gaussian random projection matrix S is a matrix formed by S = 1 \u221a s G, each entry of G i.i.d. being sampled from N (0, 1). Gaussian projection is also known as the Johnson-Lindenstrauss transformation due to the groundbreaking work [15]. Gaussian projection can be translated into four lines of MATLAB code.1 f unc t i on [C] = Gaussian ianPro jec t ion (A, s) 2 n = s i z e (A, 2); 3 S = randn (n, s) / s q r t (s); 4 C = A \u0445 S; Gaussian projection has the following properties: \u2022 Time Costs: O (mns) \u2022 Theoretical Guarantee1. If s = O (m / 2), the sub-space investment of property is highly probable."}, {"heading": "3.2.2 Subsampled Randomized Hadamard Transform (SRHT)", "text": "The subsampled Randomized Hadamard Transform (SRHT) matrix is defined by S = 1 \u221a sn DHnP, where \u2022 D \u0440Rn \u00b7 n is a diagonal matrix with diagonal entries sampled evenly from {+ 1, \u2212 1}; \u2022 Hn Rn \u00b7 n recursively by Hn = [Hn / 2 Hn / 2 \u2212 Hn / 2] and H2 = [+ 1 + 1 + 1 + 1 \u2212 1]; for all y Rn, the matrix vector product yTHn can be performed in O (n log n) time by the fast Walsh-Hadamard transformation algorithm in a divide-and-conquer manner; \u2022 P Rn \u00b7 s samples from the n column. SRHT can be implemented in nine lines."}, {"heading": "3.2.3 Count Sketch", "text": "It was applied to column 1, which is referred to as \"card reduction mode\" and \"streaming mode.\" Third, the columns with the same hash value add up to three steps. First, hash each column with a discrete value that is uniformly sampled from [s]. Second, flip the character of each column with the probability of 50%. Third, the columns with the same hash value add up."}, {"heading": "3.2.4 GaussianProjection + CountSketch", "text": "Then S fulfills the following properties. \u2022 Time complexity: The matrix product AS can be calculated in O (nz (A) (m / 2). \u2022 Theoretical properties: 141. If scs = O (m2 / 2) and s = O (m / 2), the Gaussian Projection + CountSketch matrix S fulfills the subspace embedding with = 1 + with a high probability. 2. If scs = O (k2 + k /) and s = O (k /), the Gaussian Projection + CountSketch matrix S fulfills the subspace embedding with = 1 + [2, Lemma]. If scs = O (k2 + k /) and s = O (k /)."}, {"heading": "3.3 Column Selection", "text": "This section presents three techniques for selecting columns: uniform sampling, leverage score sampling, and local selection of landmarks. Unlike random projections, column selection does not have to visit every entry of A, and column selection preserves the properties of column density and non-negativity of A."}, {"heading": "3.3.1 Uniform Sampling", "text": "The most important advantage is that uniform scanning forms a sketch without seeing the entire data matrix. When applied to core methods, uniform scanning avoids the calculation of each input of the core matrix. The performance of uniform scanning depends on the data. If the leverage values (defined in Section 3.3.2) are uniform or equivalent, the matrix coherence (namely the largest leverage effect) is small, uniform scanning has good performance. An analysis of uniform scanning can be found in [12; 13]."}, {"heading": "3.3.2 Leverage Score Sampling", "text": "Before studying leverage score sampling, let's first define the leverage scores. Let A be a m \u00b7 n matrix, with \u03c1 = rank (A) < n, and V \u30fb Rn \u00b7 \u03c1 be the correct singular vectors. (Sometimes, each selected column of A should be scaled by averages.) It can be in 8 lines of MATLAB code.151 f unc sampling ScoreSampling each column of A with probability proportional to its leverage score sampling (A, s) 2 n = s i z e (A, 2); 3 [B, V] = svd (A, \"econ\") e, 4 l e r a g s = sum (V, 2); 5 g = v / k = v; l = c (A) e (A) c; l = universal selective (A)."}, {"heading": "3.3.3 Local Landmark Selection", "text": "Zhang and Kwok [36] proposed to set k = s and run k-middle or k-centric cluster algorithms to cluster the columns from A to S, and s-centrists as a sketch of A. This heuristics works very well in practice, although it offers little theoretical warranty. There are several tricks to make local selection of k-centrists more efficient. \u2022 If n is large, one can consistently scan a subset of data, e.g. max. {0,2n, 20s} data points, and perform local selection of markers on these smaller data sets. \u2022 In supervised learning problems, each date ai is associated with a label yi."}, {"heading": "4.1 Standard Solutions", "text": "The Moore-Penrose inverse can be calculated by SVD, which costs O (nd2) time. LSR can also be solved by numerical algorithms such as the conjugate gradient (CG) algorithm, and machine precision can be achieved in a reasonable number of iterations. Let's (A): = \u03c31 (A) \u03c3d (A) the conditional number of A. The convergence of CG depends on the tenth iteration of CG. The pro iteration time of CG is O (nz (A) \u2212 2 (A) \u2212 1 (A) + 1) t, where x (t) is the model in the tenth iteration of CG."}, {"heading": "4.2 Inexact Solution", "text": "Any sketching matrix S-Rn \u00b7 s can be used to solve LSR approximately as long as it meets the pitch characteristic. We consider the following LSR problem: x-x = min x-x-x (STA) s-dx-STb-22, (4.2), which can be solved in O (sd2) time. If S is a Gaussian projection matrix, SRHT matrix, counter sketch or lever score sampling matrix, and s = poly (d /) for each error parameter."}, {"heading": "4.2.1 Implementation", "text": "If S is a counting matrix, the inaccurate LSR algorithm can be implemented in 5 lines of MATLAB code. Here, CountSketch is a MATLAB function described in Section 3.2.3. The total time is O (nnz (A) + poly (d /)) and the storage costs are O (poly (d /), which are lower than the cost of the exact LSR if d n.1 f unc t i on [x t i l d] = InexactLSR (A, b, s) 2 d = s i z e (A, 2); 3 sketch = (CountSketch ([A, b] ', s)'; 4 asceticetch = sketch (:, 1: d);% asceticetch = S'\u0445 A 5 bsketch = sketch (: \u2212, end);% bsketch = S '.b = sketch (1: d; sketch)."}, {"heading": "4.2.2 Theoretical Explanation", "text": "Based on the partial space embedding property, it can easily be shown that x-x is a good solution. Leave D = [A, b] and x-Rn \u00b7 (d + 1) and z = [x; \u2212 1] for all e.g. ThenAx \u2212 b = Dz and STAx \u2212 STb = STDz, 20and the partial space embedding property shows 1-Dz-22 \u2264 STDz-22 for all e.g. Thus1-22 for all e.g. ThenAx \u2212 b) and thus 1-ST (Ax \u2212 b), 22 and 1-ST (Ax \u2212 b), 22 \u2264 1-ST (Ax \u2212 b). The optimality of x-ST (Ax \u2212 b) indicates that x-ST (Ax \u2212 b), 22 \u2264 ST (Ax \u2212 b) and thus 1ually-ST (Ax \u2212 b) is an almost perfect solution for the partial space embedding property."}, {"heading": "4.3 Machine-Precision Solution", "text": "Randomized algorithms can also be used to find a machine-precise solution for LSR, and the time complexity is lower than standard solutions. The state-of-the-art algorithm [18] is based on a very similar idea described in this section."}, {"heading": "4.3.1 Basic Idea: Preconditioning", "text": "We have previously discussed that the time cost of the conjugate gradient (CG) = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A = A ="}, {"heading": "4.3.2 Algorithm Description", "text": "The algorithm is described in algorithm 4.1. We first make a sketch Y = STA, then Rs \u00b7 d and calculate the QR decomposition Y = QYRY from this QR decomposition. On the basis of this QR decomposition we can find the initial solution z (0) and the preconditioning matrix T = R \u2212 1Y. If we set s = O (d2), the initial solution is only consistently times worse than the optimal one in terms of the objective function value. Theory also ensures that the condition number \u0445 (AT) \u2264 2. With the good initialization and good condition number, the vanilla gradient decentering 1 or CG only needs O (log \u2212 1) steps to achieve 1 + solution. Note that rows 8 and 9 should be carefully implemented in the algorithm. Do not calculate the matrix product AT because it would take O (nnz (A) d) time!"}, {"heading": "4.4 Extension: CX-Type Regression", "text": "In view of any matrix A-Rm \u00b7 n, the CX decomposition considers the decomposition of A-CX? into A-CX?, where C-Rm \u00b7 c is a sketch of A and X-Rc \u00b7 n. If c m, this problem can be solved more efficiently by sketching. Specifically, we can draw a sketch of the matrix S-Rm \u00b2 s and calculate the approximate solutionX-Rm \u00b2 s-Rm \u00b2 s. If S is a sketch of the matrix S-Rm \u00b2 s and the approximate solutionX-Rm \u00b2 s = argmin X-STC-STC, we set s = O (c / + c2); if S samples the columns according to the series averages of C, we set s = O-Rc \u00b2 s (STC) \u2020 (STA). If S is a sketch of the matrix S-Rm \u00b2 s, we determine s = c (c+ 2)."}, {"heading": "4.5 Extension: CUR-Type Regression", "text": "A more complicated problem has also been considered in the literature [25; 29; 24]: X? = argmin X C m 2F (4.4), where c, r m, n. The solution is: X? = C \u2020 AR \u2020, which will cost O (mn \u00b7 min {c, r}) time. Wang et al. [31] suggested an algorithm to (4.4) about byX = argmin X STC (CXR \u2212 A) SR 2Fwo SC perm \u00b7 sc and SR Rn \u00b7 sr Sampling Score Score Score Score Score Score Score Score Score Score (4.4), sr = argmin X STC (CXR \u2212 A), Sampling SR Sampling 2Fr, sampling Score Score Score Score Score Score Score Score Score Score Score Score (4.4)."}, {"heading": "5.1 Standard Solutions", "text": "The standard solutions to k-SVD include the potency iteration algorithm and the Krylov subroom methods (Q = 6 sqm = 6 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 8 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 7 sqm = 8 sqm = 7 sqm = 7 sqm = 0 sqm = 0 sqm = 0 sqm = 0 sqm = 0 sqm = 0 sqm = 7 = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 sqm = 8 sqm = 8 sqm = 8 sqm = 8 sqm = 8 sqm = 8 sqm = 8 sqm = 8 sqm = 0 = 0 sqm = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 = 7 sqm = 7 sqm = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 = 7 sqm = 7 = 7 = 7 sqm = 7 sqm = 7 = 7 sqm = 7 sqm = 7 = 7 sqm = 7 sqm = 7 sqm = 0 sqm = 0 sqm = 0 sqm = 0 sqm = 0 sqm = 0 sqm = 0 sqm = 7 = 0 sqm = 0 sqm = 7 = 0 sqm ="}, {"heading": "5.2.1 Theoretical Explanation", "text": "If C = AS-Rm \u00b7 s is a good sketch of A, the column spacing of C should contain roughly the columns of Ak - this is the property with the lowest rank. If S-Rn \u00b7 s Gaussian projection matrix or counting sketch and s = O (k /), then the lowest rank is the approximation property rank (Z) \u2264 k \u00b2 CZ \u2212 A \u00b2 F \u2264 (1 +) \u2022 A \u2212 Ak \u00b2 2F (5,1) 26 Algorithm 5.2 prototype Randomized k-SVD algorithm. 1: Input: a m \u00b2 n matrix A and the target rank k. 2: Draw a n \u00b2 s sketching matrix S, where s = O \u00b2 (k); 3: C = AS; 4: QR Decomposition: [QC \u00b2 m \u00b2 s, RC] = qr (C \u00b2 m \u00b2 s); 5: SVk-D: [s \u00b2 K \u00b2, K \u00b2 K \u00b2 K, K \u00b2 K: K \u00b2 K: K: K \u00b2, K \u00b2 K: K: K \u00b2 s, K: K: K \u00b2 s, K: K: K: K \u00b2, K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: K: A: K: K: K: K: K: K: K: K: K: K: A: K: K: K: K: K = K = K = K = K = K = K = K = K = K = K = K = K: K = K = K: K: K = K = K = K: K = K = K = K = A: K = A: K = A: K = K = K = K = K = A: K = K = K = A: K = K = K = K = A: K = K = K"}, {"heading": "5.2.2 Algorithm Derivation", "text": "Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "5.2.3 Implementation", "text": "The algorithm is described in algorithm 5.2 and can be implemented in 5 lines of the MATLAB code. Here, s = O (k) is the size of the sketch. 1 f unc t i on [Uti lde, S t i l d e, Vt i lde] = ksvdPrototype (A, k, s) 2 C = CountSketch (A, s); 3 [Q, R] = qr (C, 0); 4 [Ubar, S t i l d e, Vt i lde] = svds (Q'aco A, k); 5 Ut i lde = Q \u00b2 Ubar; empirically, the use of \"svd (Q \u2032 A, \u2032 econ \u2032)\" followed by discarding the k + 1 to s components should be faster than the \"svds\" function in line 4.The algorithm has the following properties: 271. The algorithm runs through 2 A; 2. The function keeps the Svds (Svds) least time (Svds)."}, {"heading": "5.3.1 Theoretical Explanation", "text": "Now we draw a m \u00b7 p Gaussian projection + CountSketch matrix P and solve this problem: X = Argmin rank (X) \u2264 k \u00b7 PTQC p \u00b7 s X s \u00b7 n \u2212 PTA p \u00b7 n 2F. (5,3) To understand this trick, readers can look back at the extension of the LSR in Section 4.4. letter P = Pcs m pcs Psrht pcs = pcs = O (k / + k2) and p = O (k /). The subspace in which the property RSHT + CountSketch [6, theorem 46] is embedded implies that (1 +) \u2212 1 QCX \u2212 A 2F PT (QCX \u2212 K) and the property F 2F (QCX \u2212 K) follows the property (QCX \u2212 K)."}, {"heading": "5.3.2 Algorithm Derivation", "text": "The faster randomized k-SVD is described in algorithm 5.3 and is derived below. The algorithm solvesX = # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "5.3.3 Implementation", "text": "The faster randomized k-SVD is described in algorithm 5.3 and in 18 lines of MATLAB code.1 f unc t i on [Uti lde, S t i l d e, Vt i lde] = ksvdFaster (A, k, s, p1, p2) 2 n = s i z e (A, 2); 3 C = CountSketch (A, s); 4 A = [A, C]; 5 A = A '; 6 sketch = CountSketch (A, p1); 7 c l e a r A% A (m \u2212 by \u2212 n) w i l are not used 8 sketch = Gauss ianPro ject ion (sketch, p2); 9 sketch = sketch'; 10 L = sketch '(:, l)."}, {"heading": "6.1 Motivations", "text": "This section offers three motivational examples to show why we try to outline K by K \u2248 LLT or K \u2248 CZCT."}, {"heading": "6.1.1 Forming a Kernel Matrix", "text": "In the kernel approximation problems we get \u2022 a n \u00b7 d matrix X whose lines are data points x1, \u00b7 \u00b7, xn Rd, \u2022 a kernel function, e.g. the Gaussian RBF kernel function defined by the following MATLAB code: 1 f unc t i on [K] = rb f (X1, X2, sigma) 2 K = X1 \u0445 X2 '; 3 X1 line sq = sum (X1."}, {"heading": "6.1.2 Matrix Inversion", "text": "The exact solution costs O (n3) time and O (n2) memory. If we have a rank count K \u2248 LLT, then we can roughly get in O (nl2) time and O (nl) memory w. Here, we have to get the Sherman Morrison Woodbury matrix identity (A + BCD) -1 = A \u2212 1 \u2212 1B (C \u2212 1 + DA \u2212 1B) \u2212 1DA \u2212 1.We expand (LLT + \u03b1In) -1 by the above identity and get (LLT + \u03b1In) -1 \u2212 approximate \u2212 matrix version. \u2212 In approximate order is a better sequence. \u2212 1DA \u2212 1.We expand (LLT + \u03b1In) \u2212 1 by the above identity and get (LLT + \u03b1In) \u2212 1 \u2212 approximate \u2212 1 \u2212 approximate \u2212 solution is a sequence."}, {"heading": "6.1.3 Eigenvalue Decomposition", "text": "In the present small-scale decomposition K \u2248 LLT, we first decompose K byK \u2248 LLT = (UL\u0448LVTL) (UL\u0448LVTL) T = UL\u01092LUTL and then discard the k + 1 to l components in UL and \u0435L. L = UL\u0448LV T L is the SVD of L, which can be determined in O (nl2) time and O (nl) memory. In this way, the eigenvalue of k (k \u2264 rank (L) decomposition is approximately calculated."}, {"heading": "6.2 Prototype Algorithm", "text": "As usual, the simplest approach is to create a drawing (K = K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-"}, {"heading": "6.3 Faster SPSD Matrix Sketching", "text": "The readers have noticed that (6.1) this is the problem investigated in Section 4.5, where there is a decrease in the number of persons located in a given area; we can therefore select a column selection matrix P (PTKP) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "6.4 The Nystro\u0308m Method", "text": "It is easy to verify that QCZ-Q T C = CX-P (CTP) \u2020 \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s. W\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s. W\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s. W\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. W \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. W \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s."}, {"heading": "6.5 More Efficient Extensions", "text": "Recently, several SPSD matrix approximation methods have been proposed that are more scalable in certain applications than the Nystro \ufffd m method. In this section, some of these methods are briefly described."}, {"heading": "6.5.1 Memory Efficient Kernel Approximation (MEKA)", "text": "MEKA [24] uses the block structure of the kernel matrices and is more memory efficient than the Nystro \ufffd m method. MEKA first divides the data x1, \u00b7 \u00b7 \u00b7, xn into b groups (e.g. by inexact kmeans clustering), accordingly the kernel matrix has K b \u00b7 b blocks: K = K [1,1] \u00b7 \u00b7 K [1, b]..... K [b, 1] \u00b7 \u00b7 K [b, b] = K [1:]... K [b:]. MEKA then calculates approximately the uppermost left-standing singular vectors of K [1:], \u00b7 \u00b7, K [b:], denote U [1], \u00b7 \u00b7 \u00b7 \u00b7, U [b:] = K [b:]. For each (i, j) [b:] \u00d7 [b] \u00d7 [b] \u00d7 [MEKA] finds a very small-space matrix Z [i, j] = argmin Z."}, {"heading": "6.5.2 Structured Kernel Interpolation (SKI)", "text": "SKI [33] is a memory-efficient extension of the Nystro \ufffd m method. Let S be a column selection matrix, C = KS and W = STC = STKS. The Nystro \ufffd m method approaches K by CW \u2020 CT. SKI approaches each line of C by a convex combination of two rows of W and gets C \u2248 XW. Note that each line of X has only two unequal entries, making X extremely sparse. Thus, K is approximated by K \u2248 CW \u2020 C \u2248 (XW) W \u2020 (XW) T = XWXT. At the second approximation, much precision is lost, so SKI is much less accurate than the Nystro \ufffd m method. For the same reason as MEKA, there is no point in applying SKI to the acceleration of k-eigenvalue decomposition of K."}, {"heading": "6.6 Extension to Rectangular Matrices: CUR Matrix", "text": "Decomposition In this section, the problem of sketching any rectangular matrix A by decomposition of the CUR matrix is considered [16]. The decomposition of the CUR matrix is an extension of the previously discussed methods for sketching the SPSD matrix."}, {"heading": "6.6.1 Motivation", "text": "Suppose we get n training data x1, \u00b7 \u00b7, xn-Rd, m test data x \"1, \u00b7 \u00b7, x\" m-Rd and a core function. At its generalization (test) stage, core methods such as GPR and 38KPCA form a m \u00b7 n matrix K *, where (K) ij = q (x \"i, xj) and K * are applied to some vectors or matrices. Note that it takes O (mnd) time to form K * and O (mnp) to multiply K * by a n \u00b7 p matrix. If m is as large as n, the generalization stage of such core methods can be very expensive. Fortunately, the generalization stage of GPR or KPCA using the CUR matrix only takes time linearly in m + n."}, {"heading": "6.6.2 Prototype CUR Decomposition", "text": "Suppose we get an arbitrary m x n rectangular matrix A, which can be the K mentioned above. We sample the c columns of A to form C = ASC, Rm x c, sample the r-rows of A to form R = ASR, Rr x n, and calculate the cross-section matrix U? x Rc \u00b7 r by solving U? = argmin U \u00b2 A r R 2F = C \u2020 AR \u2020. (6,3) The approximation A \u00b2 CU? R is known as CUR decomposition [16]. This formulation bears a strong resemblance to the SPSD matrix sketching method in (6,1). The prototype of CUR decomposition is not particularly useful because (1) its time cost is O (mn \u00b7 min {r} and (2 visits) of each entry of this matrix."}, {"heading": "6.6.3 Faster CUR Decomposition", "text": "It is very useful when applied to kernel methods, as it avoids the total cost of kernel storage."}, {"heading": "6.7 Applications", "text": "This section presents implementations of kernel PCA, spectral clustering, and Gaussian process regression, all of which are accelerated by randomized algorithms."}, {"heading": "6.7.1 Kernel Principal Component Analysis (KPCA)", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x x, x, x, x x, x, x, x x, x, x x, x x, x, x x, x x, x x, x, x x, x x, x x, x x, x x, x x, x x, x x, x x, x x, x x, x x, x x x, x x, x x x, x x, x x x, x x, x x, x x x, x x, x x x, x x, x x, x x x, x x x, x x, x x, x, x, x x x x, x x, x, x, x x, x x, x x, x x, x x, x x, x x, x, x x x, x, x x, x x x, x, x x, x, x x, x, x x, x x, x x, x, x, x, x x, x x, x x, x x, x x x, x x, x x, x, x x, x x, x, x, x, x x x, x, x, x x, x, x x, x, x x x, x, x x, x x, x, x x, x, x, x, x x x, x x, x, x, x x, x x x, x x, x, x, x x"}, {"heading": "6.7.2 Spectral Clustering", "text": "Spectral clustering is one of the most popular cluster algorithms. Suppose we get the data points x1 = q = q = faster; n data points xn = Rd, \u2022 a kernel function \u0445 (\u00b7, \u00b7), \u2022 k: the number of class sessions. Spectral clustering performs the following operations: 1. Form to n \u00b7 n kernel matrix K, where Big Kij xi and xj are similar; 2. Form of degree matrix D with dii = x Kij and dij = 0 for all i 6 = j; 423. Calculate the normalized graph Laplacian G = D \u2212 1 / 2KD \u2212 1 / 2 Rn \u00b7 n; 4. Calculate the uppermost k eigenvectors of G, denote U Rn \u00b7 k, and normalize the rows of U; 5. Apply clustering to the rows of V to obtain the class labels.The first step costs O (n2d) and the fourth step costs (to achieve the linear ability to O)."}, {"heading": "6.7.3 Gaussian Process Regression (GPR)", "text": "The GPR is one of the most popular machine learning methods. (GPR is the basis of Bayesian optimization and has important applications such as the automatic adjustment of hyperparameters of deep neural networks.) Suppose we are able to use the training data x1, \u00b7, xn, Rd, (1), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (, (,), (, (,), (, (,), (, (,), (, (,), (,, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (, (,), (,), (, (,), (, (,), (, (,), (, (,), (, (,), (,), (,), (, (, (,), (,), (, (,), (, (,), (, (,),), (, (, (, (,), (,), (, (,), (, (,), (, (,), (, (,), (, (, (, (,), (, (,), (, (, (, (,), (,), (, (, (,), (, (,), (,), (, (, (, (,), (,), (, (, (, (, (,), (, (, (,), (, (, (,), (, (,"}, {"heading": "Appendix A", "text": "Several Facts of Matrix AlgebraThis chapter lists some facts that have been applied in this paper.Fact A.1. Matrices Q1, Rm, Rm, n, and p have orthonormous columns. Then matrix Q, Q1Q2 has orthonormous columns. Fact A.2. Let A be any m, n, and rank of a matrix. Then AA, B, UAU, AB have the projection of B onto the column space of A.Fact A.3. [34, Lemma 44] Matrices Q, Rm, s (m, s) have orthonormous columns."}, {"heading": "Appendix B", "text": "The more general p regression problems have also been studied in the literature [5; 7; 17; 8]. In particular, the \"1\" is of great interest because it has a strong robustness to noise. Currently, the strongest result is the \"p-line sampling by Lewis weights [8]. Distributed SVD. In the distributed model, each machine has a subset of A columns, and the system gives the best singular values and singular vectors. In this model, communication costs as well as time and storage costs should also be taken into account. The pioneering work [10] proposed to create a coreset to capture the properties of A, facilitating low compression and communication costs. Later, several algorithms with stronger error limits and lower communication and compression costs were proposed."}], "references": [{"title": "Generalized Inverses: Theory and Applications", "author": ["Adi Ben-Israel", "Thomas N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Communication-optimal distributed principal component analysis in the column-partition model", "author": ["Christos Boutsidis", "David P Woodruff"], "venue": "arXiv preprint arXiv:1504.06729,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Near-optimal columnbased matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "SIAM Journal on Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Finding frequent items in data streams", "author": ["Moses Charikar", "Kevin Chen", "Martin Farach-Colton"], "venue": "Theoretical Computer Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Subgradient and sampling algorithms for l1 regression", "author": ["Kenneth L Clarkson"], "venue": "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Annual ACM Symposium on theory of computing (STOC). ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The fast cauchy transform and faster robust linear regression", "author": ["Kenneth L Clarkson", "Petros Drineas", "Malik Magdon-Ismail", "Michael W Mahoney", "Xiangrui Meng", "David P Woodruff"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "`p row sampling by lewis weights", "author": ["Michael B Cohen", "Richard Peng"], "venue": "arXiv preprint arXiv:1412.0588,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Scalable kernel methods via doubly stochastic gradients", "author": ["Bo Dai", "Bo Xie", "Niao He", "Yingyu Liang", "Anant Raj", "Maria-Florina F Balcan", "Le Song"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["Dan Feldman", "Melanie Schmidt", "Christian Sohler"], "venue": "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["Charless Fowlkes", "Serge Belongie", "Fan Chung", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "The spectral norm error of the naive Nystr\u00f6m extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Revisiting the nystr\u00f6m method for improved large-scale machine learning", "author": ["Alex Gittens", "Michael W. Mahoney"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1984}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on theory of computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Lsrn: A parallel iterative solver for strongly over-or underdetermined systems", "author": ["Xiangrui Meng", "Michael A Saunders", "Michael W Mahoney"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Stronger approximate singular value decomposition via the block Lanczos and power methods", "author": ["Cameron Musco", "Christopher Musco"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "\u00dcber die praktische aufl\u00f6sung von integralgleichungen mit anwendungen auf randwertaufgaben", "author": ["Evert J. Nystr\u00f6m"], "venue": "Acta Mathematica,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1930}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Ninh Pham", "Rasmus Pagh"], "venue": "In the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Random features for large-scale kernel machines. In Advances in neural information processing systems (NIPS)", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Numerical methods for large eigenvalue problems. preparation", "author": ["Yousef Saad"], "venue": "Available from: http://www-users. cs. umn. edu/saad/books. html,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Memory efficient kernel approximation", "author": ["Si Si", "Cho-Jui Hsieh", "Inderjit Dhillon"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Four algorithms for the efficient computation of truncated pivoted QR approximations to a sparse matrix", "author": ["G.W. Stewart"], "venue": "Numerische Mathematik,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Tabulation-based 5-independent hashing with applications to linear probing and second moment estimation", "author": ["Mikkel Thorup", "Yin Zhang"], "venue": "SIAM J. Comput.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": "arXiv preprint arXiv:1501.01571,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Structured block basis factorization for scalable kernel matrix evaluation", "author": ["Ruoxi Wang", "Yingzhou Li", "Michael W Mahoney", "Eric Darve"], "venue": "arXiv preprint arXiv:1505.00398,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Improving CUR matrix decomposition and the Nystr\u00f6m approximation via adaptive sampling", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Spsd matrix approximation via column selection: Theories, algorithms, and extensions", "author": ["Shusen Wang", "Luo Luo", "Zhihua Zhang"], "venue": "CoRR, abs/1406.5675,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Towards more efficient symmetric matrix sketching and CUR matrix decomposition", "author": ["Shusen Wang", "Zhihua Zhang", "Tong Zhang"], "venue": "arXiv preprint arXiv:1503.08395,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["Christopher Williams", "Matthias Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Kernel interpolation for scalable structured gaussian processes (kiss-gp)", "author": ["Andrew Gordon Wilson", "Hannes Nickisch"], "venue": "arXiv preprint arXiv:1503.01057,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P Woodruff"], "venue": "arXiv preprint arXiv:1411.4357,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["Tianbao Yang", "Yu-Feng Li", "Mehrdad Mahdavi", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["Kai Zhang", "James T. Kwok"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Mahoney [16] and Woodruff [34] have written excellent but very technical reviews of the randomized algorithms.", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "Mahoney [16] and Woodruff [34] have written excellent but very technical reviews of the randomized algorithms.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "The papers written by Mahoney [16] and Woodruff [34] provide comprehensive and rigorous reviews of the randomized matrix computation algorithms.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "The papers written by Mahoney [16] and Woodruff [34] provide comprehensive and rigorous reviews of the randomized matrix computation algorithms.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "The book [1] offers a comprehensive study of the pseudo-inverses.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "The Gaussian projection is also well knows as the Johnson-Lindenstrauss transform due to the seminal work [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 2, "context": "When s = k + 1, the low-rank approximation property with \u03b7 = 1 + holds in expectation [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 35, "context": "Zhang and Kwok [36] proposed to set k = s and run k-means or k-centroids clustering algorithm to cluster the columns of A to s class, and use the s centroids as the sketch of A.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "The state-of-the-art algorithm [18] is based on very similar idea described in this section.", "startOffset": 31, "endOffset": 35}, {"referenceID": 33, "context": "Woodruff [34] proposed to use sketching to find RA approximately inO(nnz(A)+poly(d)) time.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "[31] proposed an algorithm to solve (4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "The experiments in [31] indicates that uniform sampling performs equally well as leverage score sampling.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "Here we introduce a simplified version of the block Lanczos method [19] which costs time O(mnkq), where q = log n is the number of iterations, and the inherent constant depends weakly on the spectral gap.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "Thus there are many numerical treatments to strengthen stability (see the numerically stable algorithms in [23]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "The algorithm is proposed by [14], and it is described in Algorithm 5.", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "To make matrix inversion possible, one can use the spectral shifting trick of [30]: fix a small constant \u03b1 > 0, form the low-rank approximation H \u2212 \u03b1In \u2248 CZC , and compute H\u22121g \u2248 (CZC + \u03b1In)g.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "This approach is first studied by [14].", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "[30] showed that if C contains s = O(k/ ) columns of K chosen by adaptive sampling, the error bound", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "When p = O( \u221a ns \u22121/2), the following error bounds holds with high probability [31] \u2016K\u2212QCZ\u0303QC\u2016F \u2264 (1 + ) min Z \u2016K\u2212QCZQC\u2016F .", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "See [13] for more discussions.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "1 Memory Efficient Kernel Approximation (MEKA) MEKA [24] exploits the block structure of kernel matrices and is more memory efficient than the Nystr\u00f6m method.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "K = \uf8ef\uf8f0 K[1,1] \u00b7 \u00b7 \u00b7 K[1,b] .", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "K = \uf8ef\uf8f0 K[1,1] \u00b7 \u00b7 \u00b7 K[1,b] .", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "Then MEKA approximately computes the top left singular vectors of K[1:], \u00b7 \u00b7 \u00b7 ,K[b:], denote U[1], \u00b7 \u00b7 \u00b7 ,U[b], respectively.", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "K \u2248 \uf8ef\uf8f0 U[1] 0 .", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "0 U[b] \uf8fa\uf8fb \uf8ef\uf8f0 Z[1,1] \u00b7 \u00b7 \u00b7 Z[1,b] .", "startOffset": 14, "endOffset": 19}, {"referenceID": 0, "context": "0 U[b] \uf8fa\uf8fb \uf8ef\uf8f0 Z[1,1] \u00b7 \u00b7 \u00b7 Z[1,b] .", "startOffset": 14, "endOffset": 19}, {"referenceID": 0, "context": "Z[b,1] \u00b7 \u00b7 \u00b7 Z[b,b] \uf8fa\uf8fb \uf8ef\uf8f0 U[1] 0 .", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Since Z and U[1], \u00b7 \u00b7 \u00b7 ,U[b] are small-scale matrices, MEKA is thus very memory efficient.", "startOffset": 13, "endOffset": 16}, {"referenceID": 23, "context": "the implementation provided by [24], can make MEKA numerically unstable, as was reported by [30; 28].", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "The readers had better to follow the stabler implementation in [28].", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "2 Structured Kernel Interpolation (SKI) SKI [33] is a memory efficient extension of the Nystr\u00f6m method.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "This section considers the problem of sketching any rectangular matrix A by the CUR matrix decomposition [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "The approximation A \u2248 CUR is well known as the CUR decomposition [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "\u2022 If PC is the leverage score sampling matrix corresponding to the columns of C and PR is the leverage score sampling matrix corresponding to the columns of R, then \u0168 is a very high quality approximation to U [31]: \u2016A\u2212C\u0168R\u2016F \u2264 (1 + ) min U \u2016A\u2212CUR\u2016F", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "[11] proposed to apply the Nystr\u00f6m method to make spectral clustering more scalable by avoiding forming the whole kernel matrix and speeding-up the k-eigenvalue decomposition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Currently the strongest result is the `p row sampling by Lewis weights [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "The seminal work [10] proposed to build a coreset to capture the properties of A, which facilitates low computation and communication costs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 1, "context": "Currently, the state of the art is [2].", "startOffset": 35, "endOffset": 38}, {"referenceID": 21, "context": "A parallel line of work is the random feature methods [22] which also form low-rank approximations to kernel matrices.", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "3 of [27] offers simple and elegant proof of a random feature method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "Since the sketching methods usually works better than the random feature methods (see the examples in [35]), the users are advised to apply the sketching methods introduced in Chapter 6.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "Besides the two kinds of low-rank approximation approaches, the stochastic optimization approach [9] also demonstrates very high scalability.", "startOffset": 97, "endOffset": 100}], "year": 2015, "abstractText": "1", "creator": "LaTeX with hyperref package"}}}