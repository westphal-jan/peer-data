{"id": "1505.07931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2015", "title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge", "abstract": "Learning vector representation for words is an important research field which may benefit many natural language processing tasks. Two limitations exist in nearly all available models, which are the bias caused by the context definition and the lack of knowledge utilization. They are difficult to tackle because these algorithms are essentially unsupervised learning approaches. Inspired by deep learning, the authors propose a supervised framework for learning vector representation of words to provide additional supervised fine tuning after unsupervised learning. The framework is knowledge rich approacher and compatible with any numerical vectors word representation. The authors perform both intrinsic evaluation like attributional and relational similarity prediction and extrinsic evaluations like the sentence completion and sentiment analysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show that the proposed fine tuning framework may significantly improve the quality of the vector representation of words.", "histories": [["v1", "Fri, 29 May 2015 06:11:00 GMT  (122kb)", "http://arxiv.org/abs/1505.07931v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xuefeng yang", "kezhi mao"], "accepted": false, "id": "1505.07931"}, "pdf": {"name": "1505.07931.pdf", "metadata": {"source": "CRF", "title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge", "authors": ["Xuefeng Yang"], "emails": ["yang0302@e.ntu.edu.sg", "ekzmao@ntu.edu.sg"], "sections": [{"heading": null, "text": "This year, we will be able to put ourselves at the top without being able to hide, \"he said."}, {"heading": "1 Related Work", "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive themselves. Most of them are not able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves."}, {"heading": "2 Supervised Fine Tuning for Word Representations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "To mitigate the bias problem mentioned above, this study proposes a supervised fine-tuning framework for learning word representation; the overview of the framework is shown in Figure 1. The proposed fine-tuning framework builds on the established word embedding and lexical semantic resources; there are two parts to the fine-tuning framework, including ranking data generation and supervised ranking fine-tuning; in the ranking data generation sentence, the ranking information for each word is extracted from each word, which is then integrated by a score-based fusion algorithm; humanly aggregated knowledge such as WordNet (Miller, 1995) can also be included in the phrase for data generation; and with the designated training data collected above, the supervised ranking learning algorithm can fine-tune the original word vectors to encode the complex knowledge from other word embeddings."}, {"heading": "2.2 Automatic Labeling of Training Data", "text": "As mentioned in the introduction, the training goal is to learn the semantic similarity ranking instead of measuring the similarity measure itself. The goal of labeling training data is to generate the ranking of semantically similar words for each training word."}, {"heading": "2.2.1 Why Ranking ?", "text": "Firstly, it is difficult to define the exact similarity values between two words. Secondly, it is not practical to manually label a huge amount of training data to support monitored word display learning. In previous studies, each method starts from zero without using the results of other embeddings. In contrast, our work builds on existing word embeddings so that the labelling of training data can be done automatically with existing word embeddings. Similarity measurement is influenced by many factors, such as the dimensionality of the word vectors, the learning algorithms used and the body size. An example from the GloVe embeddings (Pennington et al., 2014) could illustrate the phenomenon. The cosmic similarity value between the words \"fish\" and \"salmon\" in the glove version is 0.6596, and the value in the glove embeddings 50 dimension version 0.840. Although the similarities between the words themselves are the \"most similarity values\" of the two."}, {"heading": "2.2.2 Multiple Ranking Integration", "text": "Even state-of-the-art embedding may not always provide reliable ranking information. Table 2 shows some obvious errors in the related words of the words \"Hill,\" \"Run\" and \"Paper\" in the GloVe word embedding. The words in bold are the errors between the real related words and the number next to the word representing the ranking position. Obviously, there are errors even in the very common words such as \"paper\" and \"hill.\" To solve this problem, the authors suggest improving the ranking information label by multiple word embedding. The multiple word embedding used should be trained with different algorithms, context definition and corpus. The complementary effect of different word embedding will improve the robustness and reliability of ranking information."}, {"heading": "2.2.3 Algorithm", "text": "The entire procedure of labeled data generation and integration is described in Algorithm 11."}, {"heading": "2.2.4 Reduction of Unnecessary Data", "text": "The authors find out that it is not necessary to use complete ranking as training data for each word. Intuitively, a word has nothing to do with most of the the1The authors \"implementation is based on sparse matrix and nested mapping, because the size of the matrix is very large. Algorithm 1 Described data generation and integrationInput: VocabList V stores word index in a list 2-D Count Matrix D, which corresponds to the word in V 2-D Matrix C, stores the times of the selected 2-D Matrix S stores the times of the word pair appear List of standardized embeddings: embeddings Number of words to extract N data is a word data structure stores the output D = 0, C = 0 1 Calculate the score of the data for all wv in embeddings in V do (wN, sN) top of cosem (V = all)."}, {"heading": "2.3 Supervised Ranking Fine Tuning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3.1 Training Target", "text": "As mentioned in the introduction, the training goal in most neural language models is to maximize the context predictability of word embedding. Contrary to previous approaches, the proposed fine-tuning framework attempts to adjust the word vectors of a given word so that the precedence of word similarity matches the designated data. Adjusting the precedence of similarity can directly alleviate the pain caused by context distortion, so it is a supervised solution to the bias problem caused by the context definition. Furthermore, the ranking of semantic similarity is the most important feature of the desired word representation in most tasks. In this sense, it is an overarching solution for fine-tuning word embedding. To achieve the above goal, a ranking loss function is used as a cost function. Ranking loss function Jrank is presented in Equation (3)."}, {"heading": "2.3.2 Inverse error Weighted Mini-batch SGD Optimization", "text": "The iterative parameter updating rule is shown in Equation (5), where calculating the threshold of similarity distribution is very expensive, the minibatch SGD is used to speed up the learning process. In the mini-batch SGD, the learning process is accelerated. In the mini-batch SGD, the gradient of the loss function is approximated by a batch of data shown in Equation. (6).gbatch = 1N-batch-Batch-J (di)."}, {"heading": "2.4 Stopping Criterion and Overfitting", "text": "Overadjustment is a common problem in machine learning. In general, overadjustment is related to the number of parameters. If the size of the parameters is greater than the potential patterns in the observations, the model may suffer from the overadjustment problem. In this study, the overadjustment problem is more serious because the automatically generated data may contain some noise and contradictions. The widely used stop criterion is given in Equation (8). Jrank (i) \u2212 Jrank (i + 1) Jrank (i) < = (8) Where Jrank (i) is the overall ranking loss in the age that is identical to Jrank in Equation (3), Jrank (i) \u2212 Jrank is a very small value such as 0.01. If the relative improvement is smaller than the predefined value, the training should stop. However, the authors note that the criterion may not be generalized for the emplacements and tasks concerned, even if it < the number < <"}, {"heading": "3 Experiment", "text": "In order to evaluate the proposed supervised fine tuning framework, three groups of experiments will be designed. Intrinsic and extrinsic evaluation experiments will be conducted to test the effect of the fine tuning. 6 word embeddings will be further trained by the proposed supervised fine tuning framework and evaluated with 4 tasks based on the original word embeddings, which use many renowned word embeddings such as SENNA, Word2vec and GloVe.The effect of the inverse error-weighted mini-batch SGD optimization algorithm will be investigated in the second group experiments.These are two comparison groups, one using the widely used standard optimization algorithm and another the proposed inverse error-weighted variant. In comparison, 6 embeddings and 3 tasks will be used with 3 classic data sets, with the exception of the update rules, all other settings are the same for two comparison groups.The influence of the data size on the model will be tested in the last group experiments with best data sets of 8 and best data sets of 2."}, {"heading": "3.1 Evaluation Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Semantic Similarity Prediction", "text": "Measuring consistency between machine-predicted similarity values and human-annotated similarity values is the most common task for evaluating the quality of word embedding. Data sets are usually composed of crowd-sourced cognitive experiments.Many data sets are available, with authors selecting 5 to cover different perspectives and data sizes. Wordsim353 (Finkelstein et al., 2001) and RG65 (Rubenstein and Goodenough, 1965) are the two most commonly used sets of data for semantic similarity evaluation. MEN3000 (Bruni et al., 2014) and Mturk771 (Halawi et al., 2012) are two recently generated large datasets. The YP130 dataset (Yang and Powers, 2006) is used to evaluate semantic similarities between verbs. As in other work, the Spearman correlation is used to measure consistency between human annotation and prediction."}, {"heading": "3.1.2 Analogical Reasoning", "text": "Google's analog reasoning dataset (Mikolov et al., 2013a) was introduced to test the ability of models to perform relational similarity tasks, the question is in the following format: man: women = king:?, the answer should be the exact word \"queen.\" Microsoft's analog reasoning dataset (Mikolov et al., 2013c) is also used in this study. The main difference between these two datasets is the different types of relationships they focus on. As in other work, the next neighbor of the vector (women + king - man), which excludes the three words in the questions, is selected as the answer. If the candidate word is the same as the answer, the question is correctly solved. In this study, the general accuracy is used to evaluate word embedding. As the size of the learned word is not as large as the word used in the original study (Mikolov et al., 2013a), the word is not embedded in the experiment."}, {"heading": "3.1.3 Sentence Completion", "text": "The challenge of sentence completion (Zweig and Burges, 2012) is to stimulate research in the field of semantic modeling. Sentence completion questions consist in selecting words that are significant and coherent in the context of a complete sentence. In each sentence, a rare word is selected as the focus of the question, and four alternative candidates are selected from a list of words suggested as a disturbance by an N-gram language model. As the correct answer to the question, only the original word is taken into consideration. In this study, the author selects the candidate answer based on the average similarity between the candidates and all the words in the sentence. As the answer, the word with the highest average value is selected. This is the widely used approach when this data set is used to evaluate word embeddings."}, {"heading": "3.1.4 Sentiment Analysis", "text": "The customer ratings dataset (Hu and Liu, 2004) contains the ratings of 5 Amazon digital products, and the film ratings dataset (Pang and Lee, 2005) contains 5331 positive and 5331 negative edited sentences from the Rotten Tomatoes movie ratings website. In both datasets, the review record should be classified as positive or negative. In this study, the author uses the neural folding network described in (Kim, 2014) to perform the sentence level sentiment classification."}, {"heading": "3.2 Setting and Details", "text": "6 word embeddings are studied in our experiments, they are HLBL50 proposed in (Mnih and Hinton, 2009a), SENNA50 (Collobert et al., 2011b), RNNLM640 (Mikolov et al., 2011), GloVe300 (Pennington et al., 2014), DocAndDep2000 (Fyshe et al., 2013) and Word2vec (Mikolov et al., 2013b). All embeddings are available in Websites2, with the exception of Word2vec2Senna: http: / / ml.nec-labs.com / senna / RNLM: http: / / www.fit.vutbr.cz / imikolov / rnnlm / embeddings. The authors train the Word2vec model through the publicly available Word2vec toolkit using data from the 1 billion word language modeling benchmark."}, {"heading": "3.3 Result and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Intrinsic and Extrinsic Evaluation", "text": "The performance of the HLBL50 embedding is comparable to that of the SENNA50 and Skip50HLBL: http: / / metaoptimize.com / projects / wordreprs / Glove: http: / / nlp.stanford.edu / projects / glove / DocAndDep: http: / / www.cs.cmu.edu / afyshe / papers / conll2013 / embeddings, although the initial performance of HLBL50 is not satisfactory. The performance of the best word embedding Glove300 in this experiment is also significantly improved in all datasets. These notable improvements can show that the monitored fine-tuning framework transfers the complementary knowledge from the weak embedding and lexical semantic resources into the strong embedding."}, {"heading": "3.3.2 Comparison of Standard SGD and Inverse Error Weighted SGD", "text": "In the second experiment, the standard mini-batch SGD and the inverse error-weighted mini-batch SGD are compared. The learning curve of both approaches for all embedding to three tasks is shown in Figure 2, 3 and 4, respectively. It is shown that almost all solid lines represent Figure 2: Comparison of the standard SGD and inverse error-weighted SGD to the M3K similarity prediction data.. 0 1 2 3 4 5 epoch0.00.10.20.40.50.6sc o reGoogle Analogical Reasoning DatasetSENNA50 _ eu Skip50 _ eu HLBL50 _ eu GloVe300 _ eu phenomenon. RNLM640 _ eu Dep1000 _ eu SENNA50 _ et Skip50 _ et HLBL50 _ et GloVe300 _ et RNLM640 _ eture 3: Comparison of the standard GSD-weighted error does not mean that mini-batch SGD is weighted."}, {"heading": "3.3.3 Effect of Training Data Size", "text": "Table 6 shows the effect of different size of training data. Obviously, the performance of analog brainteasers improves slightly with the increase in training data size. However, the performance of similar prediction tasks shows two different paths: The small datasets like RG65 and YP130 prefer a small size of training data, and the performance of large datasets like M3K and Mturk771 is not affected by the size of training data. Generally, the increase in training data size for analog brainteasers is only slightly helpful, so it empirically confirms the hypothesis that using only top-related data is sufficient to support this supervised fine-tuning."}, {"heading": "4 Conclusion", "text": "The proposed approach of automatic label generation and the inverse error-weighted minibatch optimization algorithm of the SGD allow the provision of additional monitored fine-tuning phases for all unattended learning algorithms of word representation. Many experiments with 10 data sets and 6 well-trained embeddings have shown empirically that the framework is very effective to improve the quality of word representation."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Compositional Morphology for Word", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": null, "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Vector space models of lexical meaning. Handbook of Contemporary Semantics, Wiley-Blackwell, \u00e0 para\u0131\u0302tre", "author": ["Stephen Clark"], "venue": null, "citeRegEx": "Clark.,? \\Q2012\\E", "shortCiteRegEx": "Clark.", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Documents and dependencies: an exploration of vector space models for semantic composition", "author": ["Fyshe et al.2013] Alona Fyshe", "Partha Talukdar", "Brian Murphy", "Tom Mitchell"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Lan-", "citeRegEx": "Fyshe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2013}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Liu2004] Minqing Hu", "Bing Liu"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas et al.2011] Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Introduction to information retrieval, volume 1", "author": ["Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics:", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009a] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009b] Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In AISTATS05,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of ACL,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Qiu et al.2014] Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": "In Proceedings of COLING", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Verb similarity on the taxonomy", "author": ["M.W. Powers"], "venue": null, "citeRegEx": "Powers.,? \\Q2006\\E", "shortCiteRegEx": "Powers.", "year": 2006}, {"title": "A challenge set for advancing language modeling", "author": ["Burges."], "venue": "Proceedings of the NAACL-HLT 2012", "citeRegEx": "Burges.,? 2012", "shortCiteRegEx": "Burges.", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Various applications like information retrieval (Manning et al., 2008), sentiment analysis (Maas et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 12, "context": ", 2008), sentiment analysis (Maas et al., 2011) and semantic role labeling (Collobert et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "In DSMs, large sparse global cooccurrence matrix representing the context of words is first constructed , dimension reduction techniques such as SVD are then applied to find the low dimension representation of words (Clark, 2012; Turney et al., 2010).", "startOffset": 216, "endOffset": 250}, {"referenceID": 1, "context": "This method is firstly studied in (Bengio et al., 2003), and many variants have been proposed (Turian et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 28, "context": ", 2003), and many variants have been proposed (Turian et al., 2010; Collobert et al., 2011b; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 46, "endOffset": 163}, {"referenceID": 25, "context": ", 2003), and many variants have been proposed (Turian et al., 2010; Collobert et al., 2011b; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014).", "startOffset": 46, "endOffset": 163}, {"referenceID": 8, "context": "Distributional semantic models are based on the principle that words with similar semantic meanings have similar contextual information (Harris, 1954; Firth, 1957).", "startOffset": 136, "endOffset": 163}, {"referenceID": 1, "context": "For example, (Bengio et al., 2003) employs sequence of words to predict the next word, (Turian et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 28, "context": ", 2003) employs sequence of words to predict the next word, (Turian et al., 2010) employs context to predict the middle word, and (Mikolov et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 26, "context": ", 2014) propose a framework to add relational and categorical knowledge as regularization of the original training target, and (Qiu et al., 2014) utilize the morphological knowledge as both additional input representation and auxiliary supervision.", "startOffset": 127, "endOffset": 145}, {"referenceID": 24, "context": "(Passos et al., 2014) proposed a lexicon infused word representation learning algorithm for named entity recognition which is built upon the existing skip-gram model.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Bansal et al., 2014) employ an ensemble on different word representations which outperforms all individual word representation on the dependency parsing application, this suggests that the complementary information exists in different word representations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "Human summarized knowledge like WordNet (Miller, 1995) may also be included in the data generation phrase.", "startOffset": 40, "endOffset": 54}, {"referenceID": 25, "context": "An example from GloVe embedding (Pennington et al., 2014) may illustrate the phenomenon.", "startOffset": 32, "endOffset": 57}, {"referenceID": 7, "context": "Wordsim353 (Finkelstein et al., 2001) and RG65 (Rubenstein and Goodenough, 1965) are the two most used dataset for semantic similarity evaluation.", "startOffset": 11, "endOffset": 37}, {"referenceID": 3, "context": "MEN3000 (Bruni et al., 2014) and Mturk771 (Halawi et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 11, "context": "In this study, the author employ the convolution neural network (CNN) as described in (Kim, 2014) to do the sentence level sentiment classification.", "startOffset": 86, "endOffset": 97}, {"referenceID": 14, "context": ", 2011b), RNNLM640 (Mikolov et al., 2011), GloVe300 (Pennington et al.", "startOffset": 19, "endOffset": 41}, {"referenceID": 25, "context": ", 2011), GloVe300 (Pennington et al., 2014), DocAndDep2000 (Fyshe et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 9, "context": ", 2014), DocAndDep2000 (Fyshe et al., 2013), and Word2vec (Mikolov et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 11, "context": "The hyper-parameter setting for convolution neural network used in sentiment analysis follows the default setting utilized in (Kim, 2014)", "startOffset": 126, "endOffset": 137}], "year": 2015, "abstractText": "Learning vector representation for words is an important research field which may benefit many natural language processing tasks. Two limitations exist in nearly all available models, which are the bias caused by the context definition and the lack of knowledge utilization. They are difficult to tackle because these algorithms are essentially unsupervised learning approaches. Inspired by deep learning, the authors propose a supervised framework for learning vector representation of words to provide additional supervised fine tuning after unsupervised learning. The framework is knowledge rich approacher and compatible with any numerical vectors word representation. The authors perform both intrinsic evaluation like attributional and relational similarity prediction and extrinsic evaluations like the sentence completion and sentiment analysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show that the proposed fine tuning framework may significantly improve the quality of the vector representation of words. Learning a numerical vector to represent the semantic meaning of a word is a research topic of wide interests in computational linguistics. Various applications like information retrieval (Manning et al., 2008), sentiment analysis (Maas et al., 2011) and semantic role labeling (Collobert et al., 2011a) benefit from the vector representation of words. There are two classes of methods for vector representation learning of words, including distributional semantic models (DSMs) and neural language models (word embedding). In DSMs, large sparse global cooccurrence matrix representing the context of words is first constructed , dimension reduction techniques such as SVD are then applied to find the low dimension representation of words (Clark, 2012; Turney et al., 2010). Neural language models learn the vector representation of words using artificial neural networks. This method is firstly studied in (Bengio et al., 2003), and many variants have been proposed (Turian et al., 2010; Collobert et al., 2011b; Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014). In neural language methods, the key issue is the formulation of the training target function, minimization or maximization of which may produce meaningful vector representation of words. Ideally, the training target function should reflect the objective of word representation learning, that is the semantic similarity between words represented by distance measures between word vectors is consistent with human cognition. The training target in the above works is to maximise the context prediction ability which is not directly related to the word representations learning objective, therefore they are essentially unsupervised approaches working with a pseudo supervised trick. In both DSMs and neural language models, human defined context for each word is needed. The selection of context may have a strong influence on the word vectors obtained, however, there is no generally best context definition because emphasizing one perspective of context will result in the lack of other valuable information. The examples in Table 1 show that the context length have a impact on the learned word vectors. The three models are trained with the same settings except the size of context window. Table 1: Top 5 nearest neighbours for \u201csnake\u201d. window size 5 window size 7 window size 9 tarantula cobra cobra venomous tarantula lizard frog nonvenomous tarantula rattlesnake spider frog lizard python porcupine Many research works show that the marriage of unsupervised and supervised learning may achieve better performance. For example, in deep learning, unsupervised feature learning are employed to initialize the neural network to enhance the performance. This inspires the authors to introduce supervised fine tuning framework to the word representation learning, with the goal of addressing the bias problem resulted from context definition. However, introducing supervised learning into word representation learning is not a trivial issue. The most challenging problem is how to automatically generate labeled data. Manually labelling data is impractical because it is difficult to determine the exact numerical value for semantic similarity, and the size of required training data is very large. Although lexical semantic resources are available, the knowledge graph is not directly applicable for supervised learning. Another challenging problem is how to design a suitable learning algorithm so that the fine tuning will not disrupt too much of the contextbased learning results. The supervised fine tuning learning framework proposed in this paper provides solutions to the above mentioned problems. The core idea behind the proposed framework is to use the ranking of word similarities instead of the exact similarity measure values as the training target. First, an algorithm that automatically generates labeled data based on existing word embeddings and knowledge resources is proposed. Second, an inverse error weighted minibatch stochastic gradient descent optimization algorithm is designed, which can effectively absorb the complementary knowledge to fine tune the word embeddings without disrupting the original geometric similarity of word embeddings. The remainder of the paper is organized as follows. In Section 2, related work on word representation learning is briefly reviewed. The proposed supervised fine tuning framework is detailed in Section 3. Evaluation and model analysis are given in Section 4. Section 5 concludes the paper.", "creator": "LaTeX with hyperref package"}}}