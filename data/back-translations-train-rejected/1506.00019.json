{"id": "1506.00019", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2015", "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning", "abstract": "Countless learning tasks require awareness of time. Image captioning, speech synthesis, and video game playing all require that a model generate sequences of outputs. In other domains, such as time series prediction, video analysis, and music information retrieval, a model must learn from sequences of inputs. Significantly more interactive tasks, such as natural language translation, engaging in dialogue, and robotic control, often demand both.", "histories": [["v1", "Fri, 29 May 2015 20:16:51 GMT  (500kb,D)", "http://arxiv.org/abs/1506.00019v1", null], ["v2", "Mon, 29 Jun 2015 20:01:00 GMT  (583kb,D)", "http://arxiv.org/abs/1506.00019v2", null], ["v3", "Wed, 23 Sep 2015 04:59:24 GMT  (598kb,D)", "http://arxiv.org/abs/1506.00019v3", null], ["v4", "Sat, 17 Oct 2015 05:06:11 GMT  (598kb,D)", "http://arxiv.org/abs/1506.00019v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zachary c lipton", "john berkowitz", "charles elkan"], "accepted": false, "id": "1506.00019"}, "pdf": {"name": "1506.00019.pdf", "metadata": {"source": "CRF", "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning", "authors": ["Zachary C. Lipton"], "emails": ["zlipton@cs.ucsd.edu"], "sections": [{"heading": null, "text": "Recursive neural networks (RNNs) are a powerful family of connectionist models that capture time dynamics over cycles in graphs. Unlike feedback-forward neural networks, recursive networks can process examples one at a time, maintaining a state or memory that reflects an arbitrarily long context window. While these networks have long been difficult to train and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled comprehensive learning with recurring networks.In recent years, systems based on state-of-the-art long-term memory (LSTM) and bi-directional recursive neural network architectures (BRNN) have provided record-breaking performance on tasks as diverse as image captioning, language translation, and handwriting recognition. In this review of literature, we synthesize the research body that has produced these powerful models over the past three decades and reduced them to practice."}, {"heading": "1 Introduction", "text": "Recursive neural networks (RNNs) are a superset of feedback-forward neural networks supplemented by the ability to transmit information over time. Theyar Xiv: 150 6.00 019v 1 [cs.L G] 2 are a rich family of models that can perform almost any calculation. A well-known finding by Siegelman and Sontag from 1991 showed that a finite-sized recursive neural network with sigmoidal activation functions can simulate a universal Turing machine [54]. In practice, the ability to model time dependencies makes recursive neural networks particularly suitable for tasks where input and / or output consist of sequences of points that are not independent of each other."}, {"heading": "1.1 Comparison to Prior Work", "text": "Shorter works are based on familiarity with much of the background literature. Diagrams are often under-specified and do not indicate which edges include time steps and which do not. Worse, jargon abounds, while notation is often inconsistent between papers or overloaded within papers. Readers often find themselves in the unenviable position of having to synthesize contradictory information in many papers in order to understand only one. In many papers, for example, subscriptions index both nodes and time steps. In others, h stands for linking functions and a layer of hidden nodes at the same time. The variable t stands for time indexes and goals at the same time, sometimes in the same equation. Many great groundbreaking works have recently been published, but clear reviews of recurrent neural net literature are scarce. Among the most useful resources are Alex Graves's book on supervised sequencing with recurrent neural networks [24] covering a neural aspect specific to Felix's [24]."}, {"heading": "1.2 Why Recurrent Nets?", "text": "In this area, the fundamental reasons why recurrent neural networks play a serious role should be mentioned."}, {"heading": "2 Background", "text": "Here we present the formal notation and give a brief background on neural networks."}, {"heading": "2.1 Time", "text": "Let's be clear: RNNs are not limited to sequences that index time. They have been successfully applied to non-time sequence data, including genetic data [3]. However, the computation is temporal, and many important applications have an explicit or implicit temporal aspect. While we are referring to time in this essay, the methods described here apply to a larger family of tasks. In this essay, we are referring to data points x (t) that arrive and desired y (t) results that are generated in a discrete sequence of time steps indexed by t. We use superscripts with parentheses and no subscriptions to avoid confusion between time steps and neurons. Our sequences can be finite length or countable infinite. If they are finite, we call the maximum time index of the sequence T. Thus, a sequence of consecutive inputs can be recorded (1) (x), (2) (T), (and (1) (1) (T)."}, {"heading": "2.2 Neural Networks", "text": "Generally speaking, a neural network consists of a set of artificial neurons commonly referred to as nodes or units, and a set of directed edges between them that intuitively represent the synapses in a biological neural network. Associated with each neuron j is an activation function lj, sometimes referred to as the connecting function. We use the notation \"lj\" and not \"hj\" (unlike some other papers) to distinguish the activation function lj from the values of the hidden nodes in a network, which is usually noted as h in the RNN literature. Associated with each edge from node j \"to j\" is a weight class wyj. \"Following the convention adopted in several recurring net papers ([32], [21], [63]), we index neurons with j\" and j, and through wyj \"we denote the weight according to the directed class."}, {"heading": "2.3 Feedforward Neural Networks", "text": "With a neural model of the calculation, one must determine the order in which the calculation is to be continued. Should nodes be scanned and updated individually, or should the value of all nodes be calculated at once and then all updates applied at the same time? Feedforward neural networks (Figure 2) are a limited class of neural networks that address this problem by prohibiting cycles in the graph. Thus, all nodes can be arranged in layers. Results in each layer can be calculated based on results from the lower layers. Input x to a feedforward network is presented by specifying the values of the bottom layer. Each higher layer is then successively compressed until output is generated on the top layer y. These networks are often used for supervised learning tasks such as classification and regression. Learning is achieved by iteratively updating each of the weights to minimize a loss function."}, {"heading": "2.4 Training Neural Networks via Backpropagation", "text": "In practice, most networks are formed with stochastic learning processes (SGD) using mini-batches. Here, we will only discuss the base with batch size equal to 1. The stochastic gradient update equation is given by w & wFi, where the learning rate and objective function in relation to the parameters are calculated."}, {"heading": "3 Recurrent Neural Networks", "text": "Recursive neural networks are a strict superset of forward-facing neural networks, reinforced by the inclusion of recursive edges that extend over adjacent time steps and introduce an idea of time into the model.While RNs may not contain cycles between the conventional edges, recursive edges can form cycles, including self-connections. At this time, nodes receiving input along recursive edges receive input steps from the current example x (t) and also from hidden nodes h (t \u2212 1) in the previous state of the network.The output y (t) is calculated based on the hidden state h (t) at this time. Thus, input x (t \u2212 1) at this time t \u2212 1 can influence the output y (t) at this time by these recursive connections.We can show in two equations that all calculations necessary for the calculation can be performed at any time in a simple recursive neural network: Whyt (x) (x)."}, {"heading": "3.1 Past Approaches", "text": "In fact, it is the case that most people who are able to survive on their own are able to survive on their own by following the rules. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are not able to survive on their own. \"(...) Most of them are not able to survive on their own.\" (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. \"(...) Most of them are unable to survive on their own. (...) Most of them are unable to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own."}, {"heading": "3.2 Training Recurrent Networks", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3.3 Modern RNNs", "text": "The most successful RNN architectures for sequence learning date back to 1997. The first, Long Short-Term Memory, by Hochreiter and Schmidhuber, introduces the memory cell, a computing unit that replaces traditional artificial neurons in the hidden layer of a network, allowing networks to overcome some of the difficulties encountered in training in earlier recursive networks. The second, Bidirectional Recurrent Neural Networks, by Schuster and Paliwal, introduces the BRNN architecture, in which information from the future and past is used to determine output at any time t. Unlike previous systems, where only past inputs can affect output, these are successfully used, among other things, for sequence marking tasks in natural language processing. Fortunately, the two innovations are not mutually exclusive and have been successfully combined by Graves et al. for phoneme classification [26] and handwriting recognition [25]."}, {"heading": "3.3.1 Long Short-Term Memory (LSTM)", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "3.4 Bidirectional Recurrent Neural Networks (BRNNs)", "text": "Together with the LSTM, one of the most commonly used RNN constellations is the bi-directional recursive neural network (BRNN) (Figure 11), which is described for the first time in [53]. In this architecture, there are two layers of hidden nodes, both of which are associated with input and output; the two hidden layers differ in that the first has recurring connections from the past time steps, while in the second, the direction of the recurring connections is reversed, with activation proceeding backwards over time. In the face of a fixed length sequence, the BRNN can be learned with ordinary backpropagation. The following three equations describe a BRNN: h (t) f = \u03c3 (Whfhfh + bhf) h (t) b (t) b = \u03c3 (Whbxx + Whbhbhb + Whbhb) + whbhb) + whbhbhb (t) + whbhb (t) y (t) y (softmax) + Wyhbhb (t) + whb (t + whb) (h) (h) (fixed b) (h) (f)."}, {"heading": "4 Applications of LSTM and BRNN", "text": "In the previous sections, we have outlined the basic building blocks that make up almost all modern recursive neural networks. Now, we will look at several applications where recursive networks have been used successfully. We will quickly introduce the representations used for input and output, and the commonly used performance indicators. Then, we will examine the current results in machine translation, captioning, video captioning, and handwriting recognition."}, {"heading": "4.1 Representing Natural Language for Input and Output", "text": "In fact, it is so that most people who stand up for the rights of people, stand up for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, and for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people, for the rights of people of people, for the rights of people of themselves, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of themselves, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of themselves, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people, for the rights of themselves, for the rights of people who stand up for the rights of people, for the rights of people, for the rights of people, for the rights of themselves, for the rights of the rights of"}, {"heading": "4.1.1 Evaluation Methodology", "text": "A serious obstacle to training systems for issuing variable length sequences of words is the shortcomings of the available performance indicators. Developed in 2002, BLEU-Score, a common rating metric for machine translation, is related to modified universal strategies [48]. It is extended by taking the geometric mean of n-gram precisions for all values from n between 1 and an upper limit N. In practice, 4 is a typical value for N-Score, which is shown to maximize consistency with human raters. Since precision can be made high by offering absurdly short translations, the authors of BLEU introduce a Brexit penalty BP. Where q is the length of the applicant translation and r the length of the reference translations, the Brexit penalty is expressed asBP = {1 if q > re (1 \u2212 r / q), if q < = r.Then the BLEU score can be calculated with BLEU score aslog = BLN (exp = BLn)."}, {"heading": "4.2 Text Translation", "text": "In practice, this is a fundamental problem in machine learning, which resists solutions with shallow methods. Some tasks, such as the classification of documents, can be successfully performed with representations such as \"bag-of-words\" that ignore the sequence of words, but the sequence of words is indispensable in translation. The sentences \"Scientist killed by raging virus\" and \"Virus killed by raging scientist\" have identical terms such as \"Sequence to Sequence Learning,\" published in 2014, Sutskever et al. They present a translation model with two multi-layered LSTMs and show impressive achievements translated from English to French. The first LSTM is used to encode an input phrase from the source language and one for decoding the source passage in the target language."}, {"heading": "4.3 Image Captioning", "text": "Recently, recurrent neural networks have been successfully used for captions [35, 44, 67]. In this task, a training set consists of input images x and target captions y. In view of a large number of caption pairs, a model is trained to predict the appropriate caption. Vinyals et al. build on the success in language translation by treating the subtitles as a case of image-language translation. Instead of performing both encoding and decoding with LSTMs, they introduce the idea of encoding an image with a revolutionary neural network and then decoding it with an LSTM. This idea is at the core of several essays devoted to this topic. [35] This work clearly proceeds as it is quoted everywhere, but [44] they claim to be an independent innovation. With this architecture, they then reach the state of the art on Pascal, Flickr30k and COCO Datasets.Karpathy et al."}, {"heading": "4.4 Other Interesting Applications", "text": "In the work of Liwicki, Graves et al. ([42], [25]), the data is collected from a whiteboard using an eBeam interface, which captures the (x, y) coordinates of the pen in periodically scanned time steps. In the more recent work, they use a bi-directional LSTM model and surpass an HMM model with an accuracy of 81.5% at word level, compared with 70.1% for HMM. Over the past year, a number of papers have emerged that expand the success of recursive networks for translation and caption to new areas. Among the most interesting of these applications are the unattended video encoding [61], video captioning [66] and program execution [71]. In [66] Venugopalan et al. show a sequence to the sequence architecture similar to that is used in Sutskever a natural language, with most words being decoded across two layers."}, {"heading": "5 Conclusion", "text": "Over the last thirty-five years, recursive networks have shifted from impractical models, which are primarily of interest to cognitive modelling and neuroscience, to powerful and practical tools for large-scale, supervised learning, thanks to advances in model architectures, educational algorithms, and parallel computation. Recursive networks are particularly interesting because they are able to overcome the state of data through traditional machine learning processes. While the assumption of independence between individual examples is broken, the assumption of fixed input factors is incomprehensible, and yet the models lead to a competitive solution."}, {"heading": "Acknowledgements", "text": "My research is funded by generous support from the Department of Biomedical Informatics at UCSD, through a National Library of Medicine training grant. This review benefited from insightful commentary from Charles Elkan, John Berkowitz, Balakrishnan Narayanaswamy, Stefanos Poulis, Sharad Vikram."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "The principled design of large-scale recursive neural network architectures\u2013DAG-RNNs and the protein structure prediction problem", "author": ["Pierre Baldi", "Gianluca Pollastri"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization", "author": ["Satanjeev Banerjee", "Alon Lavie"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Evolving memory cell structures for sequence learning", "author": ["Justin Bayer", "Daan Wierstra", "Julian Togelius", "J\u00fcrgen Schmidhuber"], "venue": "In Artificial Neural Networks\u2013ICANN", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Evolving networks: Using the genetic algorithm with connectionist learning", "author": ["Richard K Belew", "John McInerney", "Nicol N Schraudolph"], "venue": "In In. Citeseer,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Advances in optimizing recurrent networks", "author": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Training a 3-node neural network is NP-complete", "author": ["Avrim L Blum", "Ronald L Rivest"], "venue": "In Machine learning: From theory to applications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression", "author": ["Bob Carpenter"], "venue": "Alias-i, Inc., Tech. Rep, pages", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A survey on the application of recurrent neural networks to statistical language modeling", "author": ["Wim De Mulder", "Steven Bethard", "Marie-Francine Moens"], "venue": "Computer Speech & Language,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}, {"title": "Long short-term memory in recurrent neural networks", "author": ["Felix Gers"], "venue": "Unpublished PhD dissertation, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Lausanne, Switzerland,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Recurrent nets that time and count", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber"], "venue": "In Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Alex Graves"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Neural network synthesis using cellular encoding and the genetic algorithm", "author": ["Frederic Gruau"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1994}, {"title": "Optimizing neural networks with genetic algorithms", "author": ["Steven A Harp", "Tariq Samad"], "venue": "In Proceedings of the 54th American Power Conference, Chicago,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["John J Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1982}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Handwritten digit recognition with a backpropagation network. In Advances in neural information processing systems", "author": ["B Boser Le Cun", "John S Denker", "D Henderson", "Richard E Howard", "W Hubbard", "Lawrence D Jackel"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1990}, {"title": "Efficient elastic net regularization for sparse linear models", "author": ["Zachary C Lipton", "Charles Elkan"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Optimal thresholding of classifiers to maximize F1 measure", "author": ["Zachary C Lipton", "Charles Elkan", "Balakrishnan Naryanaswamy"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks", "author": ["Marcus Liwicki", "Alex Graves", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. 9th Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Recurrent neural networks for noise reduction in robust ASR", "author": ["Andrew L Maas", "Quoc V Le", "Tyler M O\u2019Neil", "Oriol Vinyals", "Patrick Nguyen", "Andrew Y Ng"], "venue": "In INTERSPEECH. Citeseer,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan Yuille"], "venue": "arXiv preprint arXiv:1412.6632,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Gradient calculations for dynamic recurrent neural networks: A survey", "author": ["Barak A Pearlmutter"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1995}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Learning internal representations by error propagation", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Technical report, DTIC Document,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1985}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1997}, {"title": "Turing computability with neural nets", "author": ["Hava T Siegelmann", "Eduardo D Sontag"], "venue": "Applied Mathematics Letters,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1991}, {"title": "Efficient learning using forward-backward splitting", "author": ["Yoram Singer", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2014}, {"title": "Computing machinery and intelligence", "author": ["Alan M Turing"], "venue": "Mind, pages 433\u2013", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1950}, {"title": "Sequence to sequence\u2013video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeff Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "arXiv preprint arXiv:1505.00487,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1990}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J Williams", "David Zipser"], "venue": "Neural computation,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1989}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2012}, {"title": "On rectified linear units for speech processing", "author": ["Matthew D Zeiler", "M Ranzato", "Rajat Monga", "M Mao", "K Yang", "Quoc Viet Le", "Patrick Nguyen", "A Senior", "Vincent Vanhoucke", "Jeffrey Dean"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2013}], "referenceMentions": [{"referenceID": 47, "context": "A wellknown result by Siegelman and Sontag from 1991 demonstrated that a finite sized recurrent neural network with sigmoidal activation functions can simulate a universal Turing machine [54].", "startOffset": 187, "endOffset": 191}, {"referenceID": 21, "context": "Among the most useful resources are Alex Graves\u2019 2012 book on supervised sequence labelling with recurrent neural networks [24] and Felix Gers\u2019 doctoral thesis [19].", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "Among the most useful resources are Alex Graves\u2019 2012 book on supervised sequence labelling with recurrent neural networks [24] and Felix Gers\u2019 doctoral thesis [19].", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "More recently, [15] covers recurrent neural nets for language modeling.", "startOffset": 15, "endOffset": 19}, {"referenceID": 43, "context": "Other resources focus on a specific technical aspect such as [50], which surveys gradient calculations in recurrent neural networks.", "startOffset": 61, "endOffset": 65}, {"referenceID": 27, "context": "In the foundational papers, generally published in cognitive science and computational neuroscience journals ([33], [34], [18]), biologically plausible mechanisms are emphasized.", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "In the foundational papers, generally published in cognitive science and computational neuroscience journals ([33], [34], [18]), biologically plausible mechanisms are emphasized.", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "In the foundational papers, generally published in cognitive science and computational neuroscience journals ([33], [34], [18]), biologically plausible mechanisms are emphasized.", "startOffset": 122, "endOffset": 126}, {"referenceID": 46, "context": "In other papers ([53], [57], [35]), biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets.", "startOffset": 17, "endOffset": 21}, {"referenceID": 50, "context": "In other papers ([53], [57], [35]), biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "In other papers ([53], [57], [35]), biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets.", "startOffset": 29, "endOffset": 33}, {"referenceID": 36, "context": "This approach has been used with deep belief nets for speech modeling in [43].", "startOffset": 73, "endOffset": 77}, {"referenceID": 58, "context": "In Alan Turing\u2019s groundbreaking paper Computing Machinery and Intelligence, he proposes an \u201cImitation Game\u201d which judges a machine\u2019s intelligence by its ability to convincingly engage in dialogue [65].", "startOffset": 196, "endOffset": 200}, {"referenceID": 46, "context": "1 Bidirectional recurrent neural networks (BRNNs) [53] extend RNNs to model dependence on both past states and future states.", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "They have been used successfully on non-temporal sequence data, including genetic data [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 26, "context": "Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j.", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j.", "startOffset": 91, "endOffset": 95}, {"referenceID": 56, "context": "Following the convention adopted in several foundational recurrent net papers ([32], [19], [21], [63]), we index neurons with j and j\u2032, and by wjj\u2032 , we denote the weight corresponding to the directed edge from node j\u2032 to node j.", "startOffset": 97, "endOffset": 101}, {"referenceID": 56, "context": "Common choices for the activation function include the sigmoid \u03c3(z) = 1/(1 + e\u2212z) and the tanh function \u03c6(z) = (e \u2212 e\u2212z)/(ez + e\u2212z) which has become common in feedforward neural nets and was applied to recurrent nets in [63].", "startOffset": 220, "endOffset": 224}, {"referenceID": 36, "context": "These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].", "startOffset": 92, "endOffset": 96}, {"referenceID": 40, "context": "These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].", "startOffset": 98, "endOffset": 102}, {"referenceID": 65, "context": "These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "These units have been demonstrated to improve the performance of many deep neural networks ([43], [47], [73]) on tasks as varied as speech processing and object recognition, and have been used in recurrent neural networks by [7].", "startOffset": 225, "endOffset": 228}, {"referenceID": 45, "context": "Backpropagation, an algorithm introduced to neural networks in [52],", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Convolutional neural networks, developed by Yann LeCun, [38] are a variant of feedforward neural network that, since 2012, hold records in many computer vision tasks, such as object detection [36].", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "Convolutional neural networks, developed by Yann LeCun, [38] are a variant of feedforward neural network that, since 2012, hold records in many computer vision tasks, such as object detection [36].", "startOffset": 192, "endOffset": 196}, {"referenceID": 45, "context": "in 1985 [52].", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Some popular heuristics, such as AdaGrad [16], AdaDelta [72], and RMSprop [1], adaptively tune the learning rate for each feature.", "startOffset": 41, "endOffset": 45}, {"referenceID": 64, "context": "Some popular heuristics, such as AdaGrad [16], AdaDelta [72], and RMSprop [1], adaptively tune the learning rate for each feature.", "startOffset": 56, "endOffset": 60}, {"referenceID": 55, "context": "When the momentum parameter is well-tuned and the network is initialized well, momentum methods can train deep nets and recurrent nets competitively with more computationally expensive methods like the Hessian Free optimizer [62].", "startOffset": 225, "endOffset": 229}, {"referenceID": 30, "context": "Large scale feedforward neural networks trained via backpropagation have set many large-scale machine learning records, most notably on the computer vision task of object detection ([39], [36]).", "startOffset": 188, "endOffset": 192}, {"referenceID": 27, "context": "Early work, including Hopfield nets [33], learned via a Hebbian principle but did not produce networks useful for discriminative tasks.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "A number of papers from the 1990s ([6], [27]) championed the idea of learning neural networks with genetic algorithms with some even claiming that achieving success on real-world problems by applying many small changes to a network\u2019s weights was impossible.", "startOffset": 35, "endOffset": 38}, {"referenceID": 24, "context": "A number of papers from the 1990s ([6], [27]) championed the idea of learning neural networks with genetic algorithms with some even claiming that achieving success on real-world problems by applying many small changes to a network\u2019s weights was impossible.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Several recent papers explore genetic algorithms for neural networks, especially as means of learning the architecture of neural networks, a problem not addressed by backpropagation ([5], [28]).", "startOffset": 183, "endOffset": 186}, {"referenceID": 25, "context": "Several recent papers explore genetic algorithms for neural networks, especially as means of learning the architecture of neural networks, a problem not addressed by backpropagation ([5], [28]).", "startOffset": 188, "endOffset": 192}, {"referenceID": 19, "context": "However, a rectified linear units (ReLUs) introduce sparsity to hidden layers [22].", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).", "startOffset": 37, "endOffset": 41}, {"referenceID": 48, "context": "A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "A growing body of recent work ([12], [37], [55], [40]), thus read shows that given sparse inputs to a linear model with any standard regularizer, sparsity can be fully exploited even if the gradient is not sparse (owing to regularization).", "startOffset": 49, "endOffset": 53}, {"referenceID": 57, "context": "\u2019s model for sequence to sequence learning [64], compute the output at each time step and pass a representation of this information as the input at the following time step.", "startOffset": 43, "endOffset": 47}, {"referenceID": 61, "context": "This algorithm is called backpropagation through time (BPTT), and was introduced in 1990 [68].", "startOffset": 89, "endOffset": 93}, {"referenceID": 27, "context": "In 1982, Hopfield introduced a family of recurrent neural networks [33].", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "Jordan networks (Figure 5), introduced by Michael Jordan in 1986, present an early architecture for supervised learning on sequences ([34]).", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "Figure 6: An Elman network as described in Finding Structure in Time (1990) [18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 57, "context": "[64] translates sentences between natural languages.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Elman networks, introduced in [18], simplify the structure in the Jordan network.", "startOffset": 30, "endOffset": 34}, {"referenceID": 26, "context": "Some of Elman\u2019s ideas, including fixedweight edges and the importance of self-connected recurrent edges in hidden nodes survive in Hochreiter and Schmidhuber\u2019s subsequent work on Long ShortTerm Memory [32].", "startOffset": 201, "endOffset": 205}, {"referenceID": 7, "context": "But learning on recurrent networks can be especially hard due to the difficulty of learning long-range dependencies as described by Bengio et al in 1994 [9] and expanded upon in [31].", "startOffset": 153, "endOffset": 156}, {"referenceID": 42, "context": "In [49], Pascanu et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 62, "context": "Truncated backpropagation through time (TBPTT) is one solution to this problem for continuously running networks [70].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "It has been known since at least 1993 that optimizing even a 3-layer neural network is NPComplete [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "[14] shows that while many critical points exist on the error surfaces of large neural networks, the ratio of saddle points to true local minima increases exponentially with the size of the network Fast implementations and improved gradient following heuristics have rendered RNN training feasible.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "An illustration like this appears in [24]", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "backward propagation using GPUs, such as Theano ( [10]) and Torch ([13]), have made it easy to implement fast training algorithms.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "backward propagation using GPUs, such as Theano ( [10]) and Torch ([13]), have made it easy to implement fast training algorithms.", "startOffset": 67, "endOffset": 71}, {"referenceID": 38, "context": ", truncated Newton approach [45] and applied it to a network which learns to generate text one character at a time in [63].", "startOffset": 28, "endOffset": 32}, {"referenceID": 56, "context": ", truncated Newton approach [45] and applied it to a network which learns to generate text one character at a time in [63].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "In the paper that described the abundance of saddle points on the error surfaces of neural networks ([14]), the authors present a saddle-free version of Newton\u2019s method.", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "for phoneme classification [26] and handwriting recognition [25].", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "for phoneme classification [26] and handwriting recognition [25].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "Figure 9: LSTM memory cell as initially described in Hochreiter [32].", "startOffset": 64, "endOffset": 68}, {"referenceID": 57, "context": "Seeking comprehensibility, we break with this convention and use i, f , and o to refer to input, forget and output gates respectively as in [64].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "in [21].", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Forget gates, proposed in 2000 by Gers and Schmidhuber [21], add a gate similar to input and output gates to allow the network to flush information from the constant error carousel.", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "Also in 2000, Gers and Schmidhuber proposed peephole connections [20], which pass from the carousel directly to the input and output gates of that same node without first having to pass through the output gate.", "startOffset": 65, "endOffset": 69}, {"referenceID": 63, "context": "We use the tanh function \u03c6 for the input node g following the latest state of the art setup of Zaremba and Sutskever in [71].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "However, in the original LSTM paper [32], the activation function for g is the sigmoid \u03c3.", "startOffset": 36, "endOffset": 40}, {"referenceID": 46, "context": "Along with the LSTM, one of the most used RNN setups is the bidirectional recurrent neural network (BRNN) (Figure 11) first described in [53].", "startOffset": 137, "endOffset": 141}, {"referenceID": 46, "context": "Figure 11: Structure of a bidirectional recurrent neural network as described by Schuster and Paliwal in [53].", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "use such a network for generating captions for images [35].", "startOffset": 54, "endOffset": 58}, {"referenceID": 22, "context": "to achieve state of the art results on handwriting recognition and phoneme classification [25] [26].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "to achieve state of the art results on handwriting recognition and phoneme classification [25] [26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Such an encoding is discussed in [18] among others.", "startOffset": 33, "endOffset": 37}, {"referenceID": 44, "context": "Freely available code to produce word vectors from co-occurrence stats include Glove from Pennington Socher and Manning [51], and word2vec [23], which implements a word embedding algorithm from Mikolov et al.", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "Freely available code to produce word vectors from co-occurrence stats include Glove from Pennington Socher and Manning [51], and word2vec [23], which implements a word embedding algorithm from Mikolov et al.", "startOffset": 139, "endOffset": 143}, {"referenceID": 39, "context": "[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).", "startOffset": 8, "endOffset": 11}, {"referenceID": 52, "context": "in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).", "startOffset": 186, "endOffset": 190}, {"referenceID": 49, "context": "in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).", "startOffset": 192, "endOffset": 196}, {"referenceID": 53, "context": "in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).", "startOffset": 198, "endOffset": 202}, {"referenceID": 51, "context": "in 2003 [8], and more recently brought to wider attention in the deep learning community by Socher, Manning and Ng in a number of papers describing Recursive Autoencoder (RAE) networks ([59], [56], [60], [58]).", "startOffset": 204, "endOffset": 208}, {"referenceID": 15, "context": "In many experiments with recurrent neural networks ([18], [63], [71]), input is fed in one character at a time (and output generated one character at a time).", "startOffset": 52, "endOffset": 56}, {"referenceID": 56, "context": "In many experiments with recurrent neural networks ([18], [63], [71]), input is fed in one character at a time (and output generated one character at a time).", "startOffset": 58, "endOffset": 62}, {"referenceID": 63, "context": "In many experiments with recurrent neural networks ([18], [63], [71]), input is fed in one character at a time (and output generated one character at a time).", "startOffset": 64, "endOffset": 68}, {"referenceID": 41, "context": "Developed in 2002, BLEU score, a common evaluation metric for machine translation, is related to modified unigram precision [48].", "startOffset": 124, "endOffset": 128}, {"referenceID": 2, "context": "METEOR, introduced in 2005 by Banerjee and Lavie is an alternative metric intended to overcome these weaknesses of the BLEU score [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 34, "context": "Even in the straightforward case of binary classification, without time dependencies, commonly used performance metrics like F1 give rise to complicated thresholding strategies which may not accord with any reasonable intuition of what should constitute good performance [41].", "startOffset": 271, "endOffset": 275}, {"referenceID": 57, "context": "[64].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Their RNN model uses the word embeddings of Mikolov [2] and a lattice representation of the decoder output to facilitate search over the space of possible translations.", "startOffset": 52, "endOffset": 55}, {"referenceID": 29, "context": "Recently, recurrent neural networks have been used successfully for image captioning, [35, 44, 67].", "startOffset": 86, "endOffset": 98}, {"referenceID": 37, "context": "Recently, recurrent neural networks have been used successfully for image captioning, [35, 44, 67].", "startOffset": 86, "endOffset": 98}, {"referenceID": 60, "context": "Recently, recurrent neural networks have been used successfully for image captioning, [35, 44, 67].", "startOffset": 86, "endOffset": 98}, {"referenceID": 29, "context": "This work clearly precedes [35] as it is cited throughout, but [44] claim it as an independent innovation.", "startOffset": 27, "endOffset": 31}, {"referenceID": 37, "context": "This work clearly precedes [35] as it is cited throughout, but [44] claim it as an independent innovation.", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "similarly use a convolutional neural network to encode images together with a bidirectional neural network to decode translations, using word2vec embeddings as word representations [35].", "startOffset": 181, "endOffset": 185}, {"referenceID": 35, "context": "( [42], [25]), data is collected from a whiteboard using an eBeam interface.", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "( [42], [25]), data is collected from a whiteboard using an eBeam interface.", "startOffset": 8, "endOffset": 12}, {"referenceID": 54, "context": "Among the most interesting of these applications are unsupervised video encoding [61], video captioning [66] and program execution [71].", "startOffset": 81, "endOffset": 85}, {"referenceID": 59, "context": "Among the most interesting of these applications are unsupervised video encoding [61], video captioning [66] and program execution [71].", "startOffset": 104, "endOffset": 108}, {"referenceID": 63, "context": "Among the most interesting of these applications are unsupervised video encoding [61], video captioning [66] and program execution [71].", "startOffset": 131, "endOffset": 135}, {"referenceID": 59, "context": "In [66], Venugopalan et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 63, "context": "In Learning to Execute ([71]), Zaremba and Sutskever experiment with networks which read computer programs one character at a time and predict their output.", "startOffset": 24, "endOffset": 28}], "year": 2015, "abstractText": "Countless learning tasks require awareness of time. Image captioning, speech synthesis, and video game playing all require that a model generate sequences of outputs. In other domains, such as time series prediction, video analysis, and music information retrieval, a model must learn from sequences of inputs. Significantly more interactive tasks, such as natural language translation, engaging in dialogue, and robotic control, often", "creator": "LaTeX with hyperref package"}}}