{"id": "1506.00195", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Recurrent Neural Networks with External Memory for Language Understanding", "abstract": "Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.", "histories": [["v1", "Sun, 31 May 2015 05:10:03 GMT  (91kb,D)", "http://arxiv.org/abs/1506.00195v1", "submitted to Interspeech 2015"]], "COMMENTS": "submitted to Interspeech 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["baolin peng", "kaisheng yao"], "accepted": false, "id": "1506.00195"}, "pdf": {"name": "1506.00195.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Networks with External Memory for Language Understanding", "authors": ["Baolin Peng", "Kaisheng Yao"], "emails": ["blpeng@se.cuhk.edu.hk,", "kaisheny@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, relapsing neural networks (RNNs) have performed strongly, e.g. in language modeling [3], language comprehension [4], and machine translation [5]. The main task of a language understanding (LU) is to associate words with semantic meanings [7-9]. In the sentence \"Please book me a LU system that identifies\" Hong Kong, \"such as the absence of a trip, and\" Seattle \"as the place of arrival. Widespread approaches include conditional random fields (CRFS)."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Language understanding", "text": "A speech comprehension system predicts an output sequence with tags such as named-entity with an input sequence. Often, the output and input sequences have been aligned, and in these alignments, one input may correspond to a zero day or a single day. An example is shown in Table 1.For an input sequence of T-length xT1, a corresponding output day sequence yT1, and an alignment A, the posterior probability p (yT1 | A, xT1) of p (yT1 | xT1) is approximate T-length T-length = 1 p (yt | xt + kt \u2212 k), (1) where k is the size of a context window and t indexes the positions in the alignment.ar Xiv: 150 6.00 195v 1 [cs.C L] 31 May 201 5"}, {"heading": "2.2. Simple recurrent neural networks", "text": "An RNN consists of an input layer xt, a hidden layer ht, and an output layer yt. In the Elman architecture [16], the activity of the hidden layer ht depends on both the input text and the hidden layer ht. Due to repetition, the activity of the hidden layer ht depends on the observation sequence from the beginning. Therefore, the probability of the hidden layer ht is calculated as the following probabilities: asyt = g (ht), (3) ht = \u03c3 (xt, ht \u2212 1). (4) In the above equation, g (\u00b7) is a softmax function, while the output yt and hidden layer activity ht asyt = g (ht), (3) ht = \u03c3 (xt, ht \u2212 1) are calculated."}, {"heading": "2.3. Recurrent neural networks using gating functions", "text": "The current activity of the hidden layer of a simple RNN is related to its hidden activity ht \u2212 1 via the nonlinear function in Equation (4). Nonlinearity can cause errors of ht to be propagated backwards to explode or disappear, which prevents simple RNN learning patterns associated with long time dependence [14]. To address this problem, a neural network was proposed in [17] with the introduction of memory cells linearly depending on their past values. LSTM also introduces three gating functions, namely entrance gate, forget gate and exit gate. We follow a variant of LSTM in [18]. More recently, a gated recurrent neural network (GRNN) [6] has been proposed. Instead of the three gating functions in LSTM, the observation layers are activated in the past, it uses two gating functions, namely entrance gate, forget gate and exit gate."}, {"heading": "3. The RNN-EM architecture", "text": "Figure 1 illustrates the proposed model, which we call RNN-EM. As with the simple RNN, it consists of an input layer, a hidden layer, and an output layer. However, instead of transferring the activity of the past hidden layer directly to the hidden layer as with the simple RNN, an input to the hidden layer comes from the contents of an external memory. RNN-EM uses a weight vector to retrieve the contents from the external memory and use it next time. The element in the weight vector is proportional to the similarity of the current activity of the hidden layer with the contents in the external memory. Therefore, contents that are irrelevant to the current activity of the hidden layer have low weights. We describe RNN-EM in detail in the following sections. All equations to be described refer to their distorted terms, which we leave away for the simplicity of the descriptions."}, {"heading": "3.1. Model input and output", "text": "In the context of speech comprehension, xt is a projection of input words, also known as word embedding. The hidden layer reads both the input texts and a content vector from memory. ct is the activity of the hidden layer calculated as a sequence (Wihxt + Wcct) (9), where \u03c3 (\u00b7) is Tanh function. ct is the weight of the input vector. ct is the content of a read operation to be described in Equation (15). Wc is the weight of the content vector. The output from this model is fed as a sequence = g (Whoht) (10) into the output layer, where Who is the weight of the hidden layer activity and g (\u00b7) is the softmax function.Note that in the case of ct = ht \u2212 1, the above model is simply RNN."}, {"heading": "3.2. External memory read", "text": "The model creates a key vector to search for content in external memory. Although there are many ways to create the key vector, we choose a simple linear function that relates hidden layer activity to ht as follows: Wkht (11) whereWk \"Rm\" p is a linear transformation matrix. Our intuition is that the memory should be in the same space of hidden layer activity or affinity to the hidden layer activity. We use cosinal distance K (u, v) = u \u00b7 v \"u\" to compare this key vector with the content in external memory. The weight for the c-th slot Mt (:, c) in memory Mt is calculated as a sequential force."}, {"heading": "3.3. External memory update", "text": "RNN-EM creates a new content vector vt to be added to its memory; i.e., vt = Wvht (16), where Wv-Rm \u00b7 p. We use the above linear function, which is based on the same intuition in paragraph 3.2, that the new content and the activity of the hidden layer are in the same room or are related to each other. RNN-EM has a forgetgate as a follow-up slot = 1 \u2212 wt et (17), where et-Rn \u00b7 1 is a delete vector generated as et = \u03c3 (Whht). Note that the c-th element in the forgetgate is only zero if both the read weight and the delete vector et her c-th element are set to one. Therefore, the memory cannot be forgotten if it is not to be read. RNN-EM has an update gate. It simply uses the weight wt as a follow-up record = wwt \u2212 (18), therefore the memory is only to be updated."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Dataset", "text": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments with a well-studied language comprehension data set, the Air Travel Information System (ATIS) [22-24]. The training part of this data set consists of 4978 sentences and 56590 words. There are 893 sentences and 9198 words available for testing. The number of semantic terms is 127, including the usual zero label. In experiments, we only use lexiconic features."}, {"heading": "4.2. Comparison with the past results", "text": "The input xt in RNN-EM has a window size of 3, consisting of the current input word and its two adjacent words. We use the AdaDelta method for updating the gradients [25]. The maximum number of training siterations was 50. Hyperparameters for tuning were the hidden layer size p, the number of storage places n and the dimension for each storage space m. The most powerful RNN-EM had 100-dimensional hidden layer and 8 storage places with 40-dimensional storage space. Table 2 lists the performance in the F1 score of RNN-EM, together with the previous best results of alternative models in the literature. As there are no previous results of GRNN, we use our own implementation for this study. These results are optimal in their respective systems. The previous best result was achieved with LSTM. A change of 0.38% of the F1 score compared to the LSTM result is significant at the 90% confidence level. The results in the previous table show that the NN-EM result is significantly better than the previous one with the STN result."}, {"heading": "4.3. Analysis on convergence and averaged performances", "text": "The results in the previous sections were obtained with models of different sizes. In this section, the models of neural networks are further compared, since they have approximately the same number of parameters as those listed in Table 3. For all of these models, we use the method of gradient updating of AdaDelta [25]. Figure 2 records the entropy of their training sets in terms of iteration numbers. To better illustrate their convergences, we have converted the entropy values into their logarithms. Results show that RNN-EM converges to a lower training entropy than other models. RNN-EM also converges faster than the simple RNN and LSTM. We repeated ATIS experiments for 10 times with different random seeds for these neural network models. We evaluated their performance according to their convergences. Table 4 lists their average F1 values together with their maximum and minimum F1 values. A change of 0.90% is significantly higher than the STN level when compared to the STN level."}, {"heading": "4.4. Analysis on memory size", "text": "The size of the external MemoryMt is proportional to the number of memory slots n. We have set the size of the memory slots to 40 and the number of memory slots varies. Table 5 lists the values of the test set F1. RNN-EM performed best with n = 8. Note that RNN-EM performs better with n = 1 than the simple RNN with 94.09% F1 in Table 4. This can be explained, among other things, by the use of gate functions in equations. (17) and (18) in RNN-EM, which are missing in simple RNN. RNN-EM with n = 1 also performed similarly to the gated RNN with 94.70% F1 in Table 4. The storage capacity can be measured with the help of these gate functions. Table 5 shows that the training set entropy is initially increased with n, which shows that the storage capacity of the RNN-EM is improved."}, {"heading": "5. Related works", "text": "The RNN-EM is located on the same research line in [19, 29], which uses external memory to improve the storage capacity of neural networks. Perhaps the next work is the work on the Neural Turing Machine (NTM) in [19], which focuses on those tasks that require simple conclusions and has proven its effectiveness in copying, repeating and sorting tasks. NTM requires complex models due to these tasks. The proposed model is much simpler than NTM and can be considered an extension of the simple RNN."}, {"heading": "6. Conclusions and discussions", "text": "In this paper, we have proposed a novel architecture of neural networks, RNN-EM, that uses external memory to improve the storage capacity of simple relapsing neural networks. In a common speech comprehension task, RNN-EM achieves new state-of-the-art results and achieves significantly better performance than the previous best result using long-term neural memory. We have conducted experiments to analyze their convergence and storage capacity, and these experiments provide insights for future research directions, such as mechanisms of access to memory content and methods to increase storage capacity."}, {"heading": "7. Acknowledgement", "text": "The authors thank Shawn Tan and Kai Sheng Tai for useful discussions about the structure and implementation of NTM."}, {"heading": "8. References", "text": "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, \"Aneural probabilistic language model,\" Journal of Machine Learning Research, vol. 3, pp. 160-165, 2003. [2] R. Collobert and J. Weston, \"A unified architecture for natural language processing: deep neural networks with multitask learning,\" in: ICML, 2008, pp. 160-167. [3] T. Mikolov, M. Karafia \"t, L. Burget, J. Cernocky, and S. Khudanpur,\" Recurrent neural network based language model, \"in: INTERSPEECH, 2010, pp. 1045-1048. K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu,\" Recurrent neural networks for language understanding, \"in INTERSPEECH, 2013, pp."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML, 2008, pp. 160\u2013167.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH, 2010, pp. 1045\u20131048.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M. Hwang", "Y. Shi", "D. Yu"], "venue": "INTERSPEECH, 2013, pp. 2524\u20132528.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R.M. Schwartz", "J. Makhoul"], "venue": "ACL, 2014, pp. 1370\u20131380.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014, pp. 1724\u20131734.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "The cmu air travel information service: Understanding spontaneous speech", "author": ["W. Ward"], "venue": "Proceedings of the DARPA Speech and Natural Language Workshop, 1990, pp. 127\u2013129.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["C. Raymond", "G. Riccardi"], "venue": "INTERSPEECH, 2007, pp. 1605\u20131608.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Spoken language understanding: a survey", "author": ["R. de Mori"], "venue": "ASRU, 2007, pp. 365\u2013376.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "ICML, 2001, pp. 282\u2013289.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Chunking with support vector machines", "author": ["T. Kudo", "Y. Matsumoto"], "venue": "NAACL, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530\u2013539, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P.Y. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML, 2013, pp. 1310\u20131318.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Neural Networks: Tricks of the Trade - Second Edition, 2012, pp. 479\u2013535.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure in time", "author": ["J. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "ICASSP, 2013, pp. 6645\u20136649.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, vol. abs/1410.5401, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), Jun. 2010, oral Presentation.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Expanding the scope of the ATIS task: The ATIS-3 corpus", "author": ["D. Dahl", "M. Bates", "M. Brown", "W. Fisher", "K. Hunicke- Smith", "D. Pallett", "C. Pao", "A. Rudnicky", "E. Shriberg"], "venue": "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 43\u201348.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Combining statistical and knowledge-based spoken language understanding in conditional models", "author": ["Y.-Y. Wang", "A. Acero", "M. Mahajan", "J. Lee"], "venue": "COLING/ACL, 2006, pp. 882\u2013889.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "What\u2019s left to be understood in ATIS?", "author": ["G. Tur", "D. Hakkani-Tr", "L. Heck"], "venue": "IEEE Workshop on Spoken Language Technologies,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv:1212.5701, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "INTER- SPEECH, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "ASRU, 2013, pp. 78\u201383.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "IEEE SLT, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "submitted to ICLR, vol. abs/1410.3916, 2015. [Online]. Available: http://arxiv.org/abs/1410.3916", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Neural network based methods have recently demonstrated promising results on many natural language processing tasks [1, 2].", "startOffset": 116, "endOffset": 122}, {"referenceID": 1, "context": "Neural network based methods have recently demonstrated promising results on many natural language processing tasks [1, 2].", "startOffset": 116, "endOffset": 122}, {"referenceID": 2, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 184, "endOffset": 190}, {"referenceID": 5, "context": "Specifically, recurrent neural networks (RNNs) based methods have shown strong performances, for example, in language modeling [3], language understanding [4], and machine translation [5, 6] tasks.", "startOffset": 184, "endOffset": 190}, {"referenceID": 6, "context": "The main task of a language understanding (LU) system is to associate words with semantic meanings [7\u20139].", "startOffset": 99, "endOffset": 104}, {"referenceID": 7, "context": "The main task of a language understanding (LU) system is to associate words with semantic meanings [7\u20139].", "startOffset": 99, "endOffset": 104}, {"referenceID": 8, "context": "The main task of a language understanding (LU) system is to associate words with semantic meanings [7\u20139].", "startOffset": 99, "endOffset": 104}, {"referenceID": 7, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 68, "endOffset": 75}, {"referenceID": 9, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 68, "endOffset": 75}, {"referenceID": 10, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 131, "endOffset": 138}, {"referenceID": 11, "context": "The widely used approaches include conditional random fields (CRFs) [8, 10], support vector machine [11], and, more recently, RNNs [4, 12].", "startOffset": 131, "endOffset": 138}, {"referenceID": 12, "context": "However, RNNs are difficult to train, because of the gradient vanishing and exploding problem [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": ", using gradient clipping [14], and/or gradient vanishing, e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": ", using second-order optimization methods [15].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": ", Elman architecture [16].", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Specifically, the long shortterm memory (LSTM) [17,18] neural networks have three gates that control flows of error signals.", "startOffset": 47, "endOffset": 54}, {"referenceID": 17, "context": "Specifically, the long shortterm memory (LSTM) [17,18] neural networks have three gates that control flows of error signals.", "startOffset": 47, "endOffset": 54}, {"referenceID": 5, "context": "The recently proposed gated recurrent neural networks (GRNN) [6] may be considered as a simplified LSTM with fewer gates.", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "Inspired by the recent work in [19], we extend the simple RNN with Elman architecture to using an external memory.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "In Elman architecture [16], hidden layer activity ht is dependent on both the input xt and also recurrently on the past hidden layer activity ht\u22121.", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "This phenomenon prevents simple RNN from learning patterns that are spanned with long time dependence [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "To tackle this problem, long short-term memory (LSTM) neural network was proposed in [17] with an introduction of memory cells, linearly dependent on their past values.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "We follow a variant of LSTM in [18].", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "More recently, a gated recurrent neural network (GRNN) [6] was proposed.", "startOffset": 55, "endOffset": 58}, {"referenceID": 19, "context": "We implemented RNN-EM using Theano [20, 21].", "startOffset": 35, "endOffset": 43}, {"referenceID": 20, "context": "We implemented RNN-EM using Theano [20, 21].", "startOffset": 35, "endOffset": 43}, {"referenceID": 25, "context": "Method F1 score CRF [26] 92.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "94 simple RNN [4] 94.", "startOffset": 14, "endOffset": 17}, {"referenceID": 26, "context": "11 CNN [27] 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "35 LSTM [28] 94.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 22, "context": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 23, "context": "In order to compare the proposed model with alternative modeling techniques, we conducted experiments on a well studied language understanding dataset, Air Travel Information System (ATIS) [22\u201324].", "startOffset": 189, "endOffset": 196}, {"referenceID": 24, "context": "We use the AdaDelta method to update gradients [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "We use AdaDelta [25] gradient update method for all these models.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "The RNN-EM is along the same line of research in [19, 29] that uses external memory to improve memory capacity of neural networks.", "startOffset": 49, "endOffset": 57}, {"referenceID": 28, "context": "The RNN-EM is along the same line of research in [19, 29] that uses external memory to improve memory capacity of neural networks.", "startOffset": 49, "endOffset": 57}, {"referenceID": 18, "context": "Perhaps the closest work is the Neural Turing Machine (NTM) work in [19], which focuses on those tasks that require simple inference and has proved its effectiveness in copy, repeat and sorting tasks.", "startOffset": 68, "endOffset": 72}], "year": 2015, "abstractText": "Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.", "creator": "LaTeX with hyperref package"}}}