{"id": "1506.00278", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "abstract": "In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.", "histories": [["v1", "Sun, 31 May 2015 19:39:44 GMT  (5935kb,D)", "http://arxiv.org/abs/1506.00278v1", "10 pages; 8 figures; 4 tables"]], "COMMENTS": "10 pages; 8 figures; 4 tables", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["licheng yu", "eunbyung park", "alexander c berg", "tamara l berg"], "accepted": false, "id": "1506.00278"}, "pdf": {"name": "1506.00278.pdf", "metadata": {"source": "CRF", "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "authors": ["Licheng Yu", "Eunbyung Park", "Alexander C. Berg"], "emails": ["tlberg}@cs.unc.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2. Related work", "text": "In fact, it is the case that most people who are in a position to move into another world, to move into another world, to move into another world, in which they move into another world, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in fact, in which they, in which they, in which they, in which they, in which they, in fact, live, live, in which they, in which they, in which they, in fact, live, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they,"}, {"heading": "3. Designing and collecting Visual Madlibs", "text": "The aim of Visual Madlibs is to study targeted descriptions of image content that go beyond the description of the objects in the image, beyond general descriptions of the entire image. The experiments in this paper begin with a dataset of images in which the presence of some objects has already been labeled 1. Invitations for the Madlibs type are automatically generated based on the image content, in general, the acquisition of such labels could be designed as part of collecting Madlibs. Fashion to design more detailed descriptions of the objects, their interactions and the broader context of the scene generated in each image shown. Visual Madlibs: Image + Instruction + Blank will be designed as part of collecting Madlibs. Fashion to design the descriptions of the objects, to design their interactions."}, {"heading": "3.1. Data Collection", "text": "To collect the Visual Madlibs Dataset, we use a subset of 10,738 human-centered images from MS COCO, which make up about a quarter of the validation data, and instantiate fill-in-the-blank templates as described above. MS COCO images are commented on with a list of objects in the images, segmentations for the locations of these objects, and 5 general natural language descriptions of the image. To select the subset of images for collecting Madlibs, we start with the 19,338 images labeled with a person. We then look at the five descriptions for each and perform a dependency analysis [7], with a word referring to a person (woman, man, etc.), in the Fig. 3, Guys, men are labeled the head noun for a portion of the particles."}, {"heading": "4. Tasks: Multiple-choice question answering", "text": "The first task is to automatically generate multiple language descriptions of images that fill in the blank for one of the madlibs questions. This allows you to create targeted descriptions such as: a description that focuses specifically on the appearance of an object, or a description of the relationship between two objects. Our second task attempts to solve this problem by developing a new targeted multiple choice question that answers the task for images. Here, the input is again an image, an instruction, and a prompt, but instead of a free-form text answer, there is a fixed set of multiple choice answers to fill the blank. Possible multiple choice answers are sampled from the madlibs answers, one that was written as the correct answer for that particular image / prompt."}, {"heading": "5. Analyzing the Visual Madlibs Dataset", "text": "First, we perform quantitative analyses of the responses collected in the Visual Madlibs Dataset in paragraph 5.1. The main objective is to understand what additional information the targeted descriptions in the Visual Madlibs Dataset provide compared to general image descriptions. MS COCO Dataset [21] collects general image descriptions that follow a similar methodology to previous efforts to capture general image descriptions, e.g. [28, 34]. Therefore, we offer further analyses that compare the Visual Madlibs with the MS COCO descriptions collected in paragraph 5.2 for the same images."}, {"heading": "5.1. Quantifying Visual Madlibs responses", "text": "Three feet for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green, the green for the green. \""}, {"heading": "5.2. Visual Madlibs vs general descriptions", "text": "In fact, most people who live in the US are able to understand the world and what it is about. (...) Most people who live in the US are women, men, women and children. (...) Most people who live in the US are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are women. (...) Most of them are men. (...) Most of them are women. (...) Men. (...) Women. (...) Women. (...) Women.........."}, {"heading": "6. Experiments", "text": "In this section, we evaluate a number of methods based on the Visual Madlibs Dataset for targeted generation of multiple language and multiple choice questions to answer tasks introduced in Sec. 4. As methods, we evaluate simple jointembedding methods - canonical correlation analysis (CCA) and normalized CCA (nCCA) [14] - as well as a current, deep learning method for image description generation - CNN + LSTM [32]. We train these models on 80% of the images in the MadLibs collection and evaluate their performance on the remaining 20%. In our experiments, we extract image functions using the VGG Convolutional Neural Network (CNN). This model was developed based on the ILSVRC 2012 dataset to rec-ognize-ognize-ognize images related to the remaining 20%."}, {"heading": "6.1. Discussion of results", "text": "Table 2 shows the accuracy of each algorithm on the simple and hard versions of the targeted multiple-choice task. Figure 8 exemplifies right and wrong answers. There are several interesting observations we can make. First, the training of nCCA for all types of questions together, labeled as nCCA (all), is helpful for the simple variant of the task, but it is less useful for the \"fine-grained\" hard version of the task. Second, extracting visual features from the input field of the relevant person / object provides greater accuracy in predicting attributes, but not for other questions. Based on this finding, we try to answer the attribute question with automatic detection methods.The detectors are due to 4The lack of inputs for questions 7 and 12 are due to the fact that this priming fails for a fraction of the questions. ImageNet uses R-CNN [13] to cover 42 MS COCO categories. We observe a similar performance between the basic truth and asking people to answer the additional boxes in Table 3."}, {"heading": "7. Conclusions", "text": "We have introduced a new fill-in-the-blank strategy for targeted descriptions of natural language and use it to capture a Visual Madlibs dataset. Our analyses show that these descriptions are generally more detailed than generic overall picture descriptions. We also introduce a specific task to generate natural language descriptions and a multiplex question to answer tasks, followed by training and evaluation of common embedding and generation models."}, {"heading": "Acknowledgement", "text": "We thank the vision and the language community for feedback on this dataset, especially Julia Hockenmaier, Kate Saenko and Jason Corso. This research is supported by NSF Awards # 1417991, 1405822, 144234 and 1452851 and Microsoft Research."}], "references": [{"title": "Generating image descriptions using dependency relational patterns", "author": ["A. Aker", "R. Gaizauskas"], "venue": "ACL", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Understanding and predicting importance in images", "author": ["A.C. Berg", "T.L. Berg", "H.D. III", "J. Dodge", "A. Goyal", "X. Han", "A. Mensch", "M. Mitchell", "A. Sood", "K. Stratos", "K. Yamaguchi"], "venue": "CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1406.3676", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["A. Bordes", "J. Weston", "N. Usunier"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["M.-C. De Marneffe", "B. MacCartney", "C.D. Manning"], "venue": "Generating typed dependency parses from phrase structure parses. In Proceedings of LREC", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "From captions to visual concepts and back. arXiv preprint arXiv:1411.4952", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In ECCV", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Topic models for image annotation and text illustration", "author": ["Y. Feng", "M. Lapata"], "venue": "ACL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-view embedding space for modeling internet images", "author": ["Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik"], "venue": "tags, and their semantics. IJCV", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating natural-language video descriptions using text-mined knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R.J. Mooney", "K. Saenko", "S. Guadarrama"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "ACL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Phrase-based image captioning", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "CoRR, abs/1502.03671", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO: common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "CoRR, abs/1405.0312", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Don\u2019t just listen", "author": ["X. Lin", "D. Parikh"], "venue": "use your imagination: Leveraging visual common sense for non-visual tasks. arXiv preprint arXiv:1502.06108", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain-independent captioning of domainspecific images", "author": ["R. Mason"], "venue": "HLT-NAACL", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "EACL", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, pages 139\u2013147. Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly supervised memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "arXiv preprint arXiv:1503.08895", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "arXiv preprint arXiv:1412.4729", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daum\u00e9 III", "Y. Aloimonos"], "venue": "EMNLP", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 8, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 14, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 31, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 15, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 19, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 30, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 245, "endOffset": 252}, {"referenceID": 7, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 245, "endOffset": 252}, {"referenceID": 31, "context": "For both the generation and question-answering tasks, we study and evaluate a recent state of the art approach for image description generation [32], as well as a simple joint-embedding method learned on deep representations.", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 32, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 25, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 9, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 16, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 115, "endOffset": 122}, {"referenceID": 0, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 115, "endOffset": 122}, {"referenceID": 26, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 165, "endOffset": 177}, {"referenceID": 18, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 165, "endOffset": 177}, {"referenceID": 23, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 165, "endOffset": 177}, {"referenceID": 8, "context": "Some methods first detect words or phrases using Convolutional Neural Network (CNN) features, then generate and re-rank candidate sentences [9, 20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 19, "context": "Some methods first detect words or phrases using Convolutional Neural Network (CNN) features, then generate and re-rank candidate sentences [9, 20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 15, "context": "[16] learn a joint image-sentence embedding using visual CNNs and Long Short Term Memory (LSTM) networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Similarly, several other methods have made use of CNN features and LSTM or recurrent neural networks (RNN) for generation with a variety of different architectures [32, 15, 5].", "startOffset": 164, "endOffset": 175}, {"referenceID": 14, "context": "Similarly, several other methods have made use of CNN features and LSTM or recurrent neural networks (RNN) for generation with a variety of different architectures [32, 15, 5].", "startOffset": 164, "endOffset": 175}, {"referenceID": 4, "context": "Similarly, several other methods have made use of CNN features and LSTM or recurrent neural networks (RNN) for generation with a variety of different architectures [32, 15, 5].", "startOffset": 164, "endOffset": 175}, {"referenceID": 9, "context": "One of the first datasets collected for this problem was the UIUC Pascal Sentence data set [10] which contains 1,000 images with 5 sentences per image written by workers on Amazon Mechanical Turk.", "startOffset": 91, "endOffset": 95}, {"referenceID": 27, "context": "As the description problem gained popularity larger and richer datasets were collected, including the Flickr8K [28] and Flickr30K [34] datasets, containing 8,000 and 30,000 images respectively.", "startOffset": 111, "endOffset": 115}, {"referenceID": 33, "context": "As the description problem gained popularity larger and richer datasets were collected, including the Flickr8K [28] and Flickr30K [34] datasets, containing 8,000 and 30,000 images respectively.", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "In an alternative approach, the SBU Captioned photo dataset [27] contains 1 million images with existing captions collected from Flickr.", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "Most recently, Microsoft released the MS COCO [21] dataset.", "startOffset": 46, "endOffset": 50}, {"referenceID": 29, "context": "Recently, embedding and deep learning methods have shown great promise for question-answering [30, 3, 4].", "startOffset": 94, "endOffset": 104}, {"referenceID": 2, "context": "Recently, embedding and deep learning methods have shown great promise for question-answering [30, 3, 4].", "startOffset": 94, "endOffset": 104}, {"referenceID": 3, "context": "Recently, embedding and deep learning methods have shown great promise for question-answering [30, 3, 4].", "startOffset": 94, "endOffset": 104}, {"referenceID": 21, "context": "[22] take an interesting multi-modal approach to questionanswering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] combine computer vision and NLP in a Bayesian framework, but restrict their method to scene based questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] design a visual Turing test to test image understanding using a series of binary questions about image content.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Whether an object is mentioned in an image description can be viewed as an indication of the object\u2019s importance [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 20, "context": "To collect the Visual Madlibs Dataset we use a subset of 10,738 human-centric images from MS COCO, that make up about a quarter of the validation data [21], and instantiate fill-in-the-blank templates as described above.", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "We then look at the five descriptions for each and perform a dependency parse [7], only keeping those images where a word referring to a person (woman, man, etc.", "startOffset": 78, "endOffset": 81}, {"referenceID": 21, "context": "We first establish the easy task distractor answers by randomly choosing three descriptions (of the same question type) from other images [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "The MS COCO dataset [21] collects general image descriptions following a similar methodology to previous efforts for collecting general image descriptions, e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "[28, 34].", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "[28, 34].", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "Second, we use the phrase chunking [6] to analyze which phrasal structures are commonly used to fill in the blanks for different questions.", "startOffset": 35, "endOffset": 38}, {"referenceID": 24, "context": "A response is represented by the mean of the Word2Vec [25] vectors for each word in the response, following [22, 20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "A response is represented by the mean of the Word2Vec [25] vectors for each word in the response, following [22, 20].", "startOffset": 108, "endOffset": 116}, {"referenceID": 19, "context": "A response is represented by the mean of the Word2Vec [25] vectors for each word in the response, following [22, 20].", "startOffset": 108, "endOffset": 116}, {"referenceID": 19, "context": "serve that the Madlibs questions types, Table 1, cover much of the information in MS COCO descriptions [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 31, "context": "In addition to comparing how well the Madlibs or MS COCO descriptions can select the correct multiple-choice answer, we also use the descriptions automatically produced by a recent natural language generation system (CNN+LSTM [32], implementation from [15]) trained on MS COCO dataset.", "startOffset": 226, "endOffset": 230}, {"referenceID": 14, "context": "In addition to comparing how well the Madlibs or MS COCO descriptions can select the correct multiple-choice answer, we also use the descriptions automatically produced by a recent natural language generation system (CNN+LSTM [32], implementation from [15]) trained on MS COCO dataset.", "startOffset": 252, "endOffset": 256}, {"referenceID": 31, "context": "7 shows the accuracies resulting from using Madlibs, MSCOCO, or CNN+LSTM [32] to select the correct multiple-choice answer.", "startOffset": 73, "endOffset": 77}, {"referenceID": 31, "context": "Figure 7: The accuracy of Madlibs, MS COCO and CNN+LSTM [32](trained on MS COCO) used as references to answer the Madlibs hard multiple-choice questions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "69 on MS COCO [32]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "As methods, we evaluate simple jointembedding methods \u2013 canonical correlation analysis (CCA) and normalized CCA (nCCA) [14] \u2013 as well as a recent deep-learning based method for image description generation \u2013 CNN+LSTM [32].", "startOffset": 119, "endOffset": 123}, {"referenceID": 31, "context": "As methods, we evaluate simple jointembedding methods \u2013 canonical correlation analysis (CCA) and normalized CCA (nCCA) [14] \u2013 as well as a recent deep-learning based method for image description generation \u2013 CNN+LSTM [32].", "startOffset": 217, "endOffset": 221}, {"referenceID": 28, "context": "In our experiments we extract image features using the VGG Convolutional Neural Network (CNN) [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "[14] proposed a scalable approximation scheme of explicit kernel mapping followed by dimension reduction and linear CCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "This method, nCCA, provides high-quality retrieval results, improving over the original CCA performance significantly [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "Following the recent \u201cShow and Tell\u201d description generation technique [32] (using an implementation from [15]), we train a CNN+LSTM model for each question type on the Visual Madlibs training set.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Following the recent \u201cShow and Tell\u201d description generation technique [32] (using an implementation from [15]), we train a CNN+LSTM model for each question type on the Visual Madlibs training set.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "ImageNet using R-CNN [13], covering 42 MS COCO categories.", "startOffset": 21, "endOffset": 25}], "year": 2015, "abstractText": "In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.", "creator": "LaTeX with hyperref package"}}}