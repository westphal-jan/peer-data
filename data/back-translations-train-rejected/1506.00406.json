{"id": "1506.00406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural Networks Vector Representations", "abstract": "In this paper, we propose two new features for estimating phrase based machine translation parameters. We only use monolingual data with the assumption that the phrase table is given to us, but all of its scores have been removed.Our method is based on two recently introduced neural network vector representation models for words and sentences. It is the first use of these models in an end to end phrase based machine translation system. Scores obtained from our method can recover more than 80% of BLEU loss caused by removing phrase table probabilities. We also show that our features combined with the phrase table probabilities improve the BLEU score by 0.74 points", "histories": [["v1", "Mon, 1 Jun 2015 09:36:23 GMT  (367kb)", "http://arxiv.org/abs/1506.00406v1", null], ["v2", "Thu, 3 Sep 2015 16:44:32 GMT  (393kb)", "http://arxiv.org/abs/1506.00406v2", null], ["v3", "Tue, 24 May 2016 15:42:50 GMT  (223kb)", "http://arxiv.org/abs/1506.00406v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amir pouya aghasadeghi", "mohadeseh bastan"], "accepted": false, "id": "1506.00406"}, "pdf": {"name": "1506.00406.pdf", "metadata": {"source": "CRF", "title": "Rebuilding Phrase Table Scores from Monolingual Resources Using Neural Networks Vector Representations", "authors": ["Amir Pouya Aghasadeghi", "Mohadeseh Bastan", "Shahram Khadivi"], "emails": ["aghasadeghi@aut.ac.ir", "m.bastan@aut.ac.ir", "khadivi@aut.ac.ir"], "sections": [{"heading": null, "text": "Our method is based on two recently introduced models for displaying neural networks for words and sentences. It is the first use of these models in an end-to-end phrase-based machine translation system. Values obtained with our method can restore more than 80% of BLEU loss by removing phrase table probabilities. We also show that our features, in combination with the phrase table, improve the BLEU value by 0.74 points."}, {"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "2 Background", "text": "In the first part of this section, we will briefly discuss various parameters of phrase-based statistical machine translation. Later, we will show how our model can replace these parameters with a monolingual function. In the second part, we will present a short history of the use of neural networks in various tasks of the NLP and a more detailed description of the models we have used in this work."}, {"heading": "2.1 Parameters of phrase-based statistical", "text": "Due to the inefficiency of these vocabulary models, they have currently been replaced by a phrase-based translation model (Koehn, et al., 2003) and by hierarchical models (Chiang, 2007).In this section, we will briefly describe the parameters of the classic phrase-based machine translation system, which we will replace in the following sections with our vector-based translation model (Koehn, et al., 2003).The formation of a phrase-based statistical model of machine translation usually depends on the alignment between words extracted by vocabulary-level translation models (Brown, et al., 1993).These models have several important parameters as a result: phrase pairs are vocabulary pairs and parts of sentences based on phrases."}, {"heading": "2.2 Neural network approaches in various tasks of NLP and machine translation", "text": "This year, it will be able to take the lead, \"he said in an interview with the Taiwanese daily La."}, {"heading": "3 Rebuilding phrase table scores using vector representations similarity", "text": "Phrase tables usually have 4 characteristic values: probability of direct phrase translation \u03c6 (e | f) probability of direct lexical weighting lex (e | f) probability of reverse phrase translation \u03c6 (f | e) probability of reverse lexical weighting lex (f | e) In this section we will describe how each of these values has been replaced by monolingual attributes."}, {"heading": "3.1 Lexical weightings", "text": "As mentioned in Section 2.1, the main purpose of lexical weighting is to reduce the impact of phrase values overestimated by the MLE. Since lexical weighting is calculated with the probability of word translation, its calculation depends on bilingual data. Vectors trained with CBOW or Skipgram models capture large linear regularities between similar or related words. These regularities can be found in vectors trained in different languages by comparable corpora. Due to these regularities, when a correct linear transformation matrix is applied to source vectors, we can project them onto the target space. It has been shown that the next target vector for the projected source vector is the most likely translation of that word. In our model, we convert all source and target words existing in the lexicon table to vectors using the CW-BOD model."}, {"heading": "3.2 Phrase probabilities", "text": "Since phrases have different lengths, we had to use a different model to convert phrases to vectors. We found that PV-DM is suitable for this task. Our PV-DM network uses the same word for vector models used in the lexical weighting part. As PV-DM shares many properties from CBOW and Skipgram models, we can convert all unique phrases in the phrase table into vectors. The next step would be to find a method to project source and target space for the following sentences. As PV-DM shares many properties from CBOW and Skipgram models, the phrase vector space can probably be mapped using the same method described in Part 3.1. Figure (2) shows source and target space for the following sentences. For presentation purposes, we use a PCA to source and target space and reduce the space dimension to 2."}, {"heading": "4 Experiment Setup", "text": "For our experiments, we used Spanish as the source language and English as the target language. We trained our phrase model based on statistical machine translation models using the Moses system (Koehn et al., 2007) on the full version of the parallel Europarl V5 (Koehn, 2005). We limited the maximum phrase length to 6 phrases and removed the lexical reordering function from the Moses training. We used standard Moses settings for all other parameters. In this way, we were able to compare our model, which was trained with monolingual data against the parallel system. In addition, we trained our 4 gram language model with KenLM (Heafield, 2011) on the full English Wikipedia combined with Europarl English page. Since our test and development sets we used WMT07, each sentence contained 2000 sentences against the parallel system. For the CBOW training model, we used English and Spanish phrases can be found in at least two sets of this Wikipedia table."}, {"heading": "5 Experiment Results", "text": "We replace phrase tables with 4 independent monolingual characteristic values obtained from our vector-based models. Figure (3) shows our results. Experiment 1 is the base system trained by Moses default settings. Experiment 2 shows the BLEU value after all bilingual data has been removed. In Experiment 3 we replace the direct phrase translation probability with our model direct vector-based similarity. In Experiment 4 we supplement our direct monolingual lexical weighting to Experiment 3. We examine our reverse phrase probability effect in Experiment 5. In Experiment 6 we replace all bilingual phrase values with our monolingual alternative values. Experiment 7 shows all our features combined with baseline system values. A summary of our results can be found in the table (3). As we have shown, our method has reduced the loss of bilingual phrase values from the EU base system by more than 82%."}, {"heading": "6 Conclusion", "text": "These attributes can be achieved without using additional data or metadata such as timestamps, and they are also linguistically independent. By using these attributes, we successfully achieve more than 82% of the loss caused by removing the phrase table. Also, by combining these attributes with bilingual results, we improve BLEU result by 0.74%."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Ranking with Recursive Neural Networks and Its Application to MultiDocument Summarization", "author": ["Z. Cao", "F. Wei", "L. Dong", "S. Li", "M. Zhou"], "venue": null, "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "A connectionist approach to machine translation. Paper presented at the EUROSPEECH", "author": ["M.A. Casta\u00f1o", "F. Casacuberta"], "venue": null, "citeRegEx": "Casta\u00f1o and Casacuberta,? \\Q1997\\E", "shortCiteRegEx": "Casta\u00f1o and Casacuberta", "year": 1997}, {"title": "Machine translation using neural networks and finite-state models", "author": ["M.A. Castano", "F. Casacuberta", "E. Vidal"], "venue": "Theoretical and Methodological Issues in Machine Translation (TMI),", "citeRegEx": "Castano et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Castano et al\\.", "year": 1997}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "computational linguistics, 33(2), 201-228.", "citeRegEx": "Chiang,? 2007", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Fast and robust neural network joint models for", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Learning Bilingual Lexicons from Monolingual Corpora. Paper presented at the ACL", "author": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein"], "venue": null, "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "KenLM: Faster and smaller language model queries", "author": ["K. Heafield"], "venue": "Paper presented at the Proceedings of the Sixth Workshop on Statistical Machine Translation.", "citeRegEx": "Heafield,? 2011", "shortCiteRegEx": "Heafield", "year": 2011}, {"title": "Toward statistical machine translation without parallel corpora", "author": ["A. Klementiev", "A. Irvine", "C. Callison-Burch", "D. Yarowsky"], "venue": "Paper presented at the Proceedings of the 13th Conference of the European Chapter", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "Paper presented at the MT summit.", "citeRegEx": "Koehn,? 2005", "shortCiteRegEx": "Koehn", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation. Paper presented at the Proceedings of the 45th annual meeting", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N Bertoldi"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["P. Koehn", "K. Knight"], "venue": "Paper presented at the Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume", "citeRegEx": "Koehn and Knight,? \\Q2002\\E", "shortCiteRegEx": "Koehn and Knight", "year": 2002}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Paper presented at the Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Q.V. Le", "T. Mikolov"], "venue": null, "citeRegEx": "Le and Mikolov,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "Paper presented at the INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Polylingual topic models", "author": ["D. Mimno", "H.M. Wallach", "J. Naradowsky", "D.A. Smith", "A. McCallum"], "venue": "Paper presented at the Proceedings of the 2009 Conference on Empirical Methods in Natural Language", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "Paper presented at the Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1.", "citeRegEx": "Och,? 2003", "shortCiteRegEx": "Och", "year": 2003}, {"title": "The alignment template approach to statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och and Ney,? \\Q2004\\E", "shortCiteRegEx": "Och and Ney", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Identifying word translations in non-parallel texts", "author": ["R. Rapp"], "venue": "Paper presented at the Proceedings of the 33rd annual meeting on Association for Computational Linguistics.", "citeRegEx": "Rapp,? 1995", "shortCiteRegEx": "Rapp", "year": 1995}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Inducing translation lexicons via diverse similarity measures and bridge languages", "author": ["C. Schafer", "D. Yarowsky"], "venue": "Paper presented at the proceedings of the 6th conference on Natural language learning-", "citeRegEx": "Schafer and Yarowsky,? \\Q2002\\E", "shortCiteRegEx": "Schafer and Yarowsky", "year": 2002}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng", "A. Y"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A projection extension algorithm for statistical machine translation", "author": ["C. Tillmann"], "venue": "Paper presented at the Proceedings of the 2003 conference on Empirical methods in natural language processing.", "citeRegEx": "Tillmann,? 2003", "shortCiteRegEx": "Tillmann", "year": 2003}, {"title": "Effective phrase translation extraction from alignment models", "author": ["A. Venugopal", "S. Vogel", "A. Waibel"], "venue": "Paper presented at the Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-", "citeRegEx": "Venugopal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Venugopal et al\\.", "year": 2003}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["Zhang", "M.-L", "Zhou", "Z.-H"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "These corpora are used for learning and tuning parameter of statistical model(Brown et al., 1993).", "startOffset": 77, "endOffset": 97}, {"referenceID": 24, "context": "Contextual similarity (Rapp, 1995) , orthographic similarities (Haghighi et al.", "startOffset": 22, "endOffset": 34}, {"referenceID": 8, "context": "Contextual similarity (Rapp, 1995) , orthographic similarities (Haghighi et al., 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 13, "context": "Contextual similarity (Rapp, 1995) , orthographic similarities (Haghighi et al., 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al.", "startOffset": 63, "endOffset": 110}, {"referenceID": 26, "context": ", 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 20, "context": ", 2008; Koehn and Knight, 2002), temporal similarity (Schafer and Yarowsky, 2002) and topic models(Mimno et al., 2009) are some examples of those features.", "startOffset": 98, "endOffset": 118}, {"referenceID": 10, "context": "It has been demonstrated that (Klementiev et al., 2012) by using these features alone for estimating translation model parameters most of BLEU loss is recoverable.", "startOffset": 30, "endOffset": 55}, {"referenceID": 15, "context": "Another recent path of work in the field of natural language processing is learning continuous vector representations for words, sentences and documents using neural networks (Le and Mikolov, 2014; Mikolov, et al., 2013a; Mikolov et al., 2013c; Pennington et al., 2014).", "startOffset": 175, "endOffset": 269}, {"referenceID": 23, "context": "Another recent path of work in the field of natural language processing is learning continuous vector representations for words, sentences and documents using neural networks (Le and Mikolov, 2014; Mikolov, et al., 2013a; Mikolov et al., 2013c; Pennington et al., 2014).", "startOffset": 175, "endOffset": 269}, {"referenceID": 15, "context": ", 2013a) and paragraph vector with distributed memory, PVDM(Le and Mikolov, 2014) both trained on monolingual corpora, to rebuild phrase table scores.", "startOffset": 59, "endOffset": 81}, {"referenceID": 14, "context": "We used these new scores in an end to end phrase based statistical machine translation system (Koehn et al., 2003).", "startOffset": 94, "endOffset": 114}, {"referenceID": 5, "context": ", 2003) and hierarchical models (Chiang, 2007).", "startOffset": 32, "endOffset": 46}, {"referenceID": 22, "context": "\uf0b7 Phrase pairs: phrase pairs are pairs of words and part of sentences which are extracted using some heuristics from word alignment models (Och and Ney, 2004; Tillmann, 2003; Venugopal et al., 2003).", "startOffset": 139, "endOffset": 198}, {"referenceID": 28, "context": "\uf0b7 Phrase pairs: phrase pairs are pairs of words and part of sentences which are extracted using some heuristics from word alignment models (Och and Ney, 2004; Tillmann, 2003; Venugopal et al., 2003).", "startOffset": 139, "endOffset": 198}, {"referenceID": 29, "context": "\uf0b7 Phrase pairs: phrase pairs are pairs of words and part of sentences which are extracted using some heuristics from word alignment models (Och and Ney, 2004; Tillmann, 2003; Venugopal et al., 2003).", "startOffset": 139, "endOffset": 198}, {"referenceID": 27, "context": "By using neural networks in text classification (Zhang and Zhou, 2006), sentiment analysis(Socher et al., 2013), summarization (Cao et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 2, "context": ", 2013), summarization (Cao et al., 2015), statistical machine translation (Devlin et al.", "startOffset": 23, "endOffset": 41}, {"referenceID": 7, "context": ", 2015), statistical machine translation (Devlin et al., 2014; Mikolov et al., 2010) and multitask learning (Collobert and Weston, 2008) great results on baselines have been achieved.", "startOffset": 41, "endOffset": 84}, {"referenceID": 17, "context": ", 2015), statistical machine translation (Devlin et al., 2014; Mikolov et al., 2010) and multitask learning (Collobert and Weston, 2008) great results on baselines have been achieved.", "startOffset": 41, "endOffset": 84}, {"referenceID": 6, "context": ", 2010) and multitask learning (Collobert and Weston, 2008) great results on baselines have been achieved.", "startOffset": 31, "endOffset": 59}, {"referenceID": 3, "context": "In machine translation, neural networks were first used by (Casta\u00f1o and Casacuberta, 1997; Castano et al., 1997)They used a neural network for example-based machine translation.", "startOffset": 59, "endOffset": 112}, {"referenceID": 4, "context": "In machine translation, neural networks were first used by (Casta\u00f1o and Casacuberta, 1997; Castano et al., 1997)They used a neural network for example-based machine translation.", "startOffset": 59, "endOffset": 112}, {"referenceID": 25, "context": "Word representation by using a continuous vector was first done by (Rumelhart et al., 1988).", "startOffset": 67, "endOffset": 91}, {"referenceID": 0, "context": "Benjio also used a feedforward neural network to learn word representation whose work was followed by many others (Bengio et al., 2003).", "startOffset": 114, "endOffset": 135}, {"referenceID": 0, "context": "Benjio also used a feedforward neural network to learn word representation whose work was followed by many others (Bengio et al., 2003). In this way, words (which are known as one of the most important components of natural language processing) with different lengths were converted to vectors of real numbers with fixed lengths and used in many applications. Mikolov (2013a) proposed a new architecture for learning distributed representation of words.", "startOffset": 115, "endOffset": 376}, {"referenceID": 12, "context": "We trained our phrase based statistical machine translation model using Moses system (Koehn et al., 2007) on full version of the parallel Europarl V5 (Koehn, 2005).", "startOffset": 85, "endOffset": 105}, {"referenceID": 11, "context": ", 2007) on full version of the parallel Europarl V5 (Koehn, 2005).", "startOffset": 52, "endOffset": 65}, {"referenceID": 9, "context": "Also, we trained our 4-gram language model using KenLM (Heafield, 2011) on the full English Wikipedia combined with Europarl English side.", "startOffset": 55, "endOffset": 71}, {"referenceID": 21, "context": "For tuning model weights, in both Moses bases system and vector base system we used minimum error rate training (Och, 2003).", "startOffset": 112, "endOffset": 123}, {"referenceID": 9, "context": "Heafield, K. (2011). KenLM: Faster and smaller language model queries.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "Heafield, K. (2011). KenLM: Faster and smaller language model queries. Paper presented at the Proceedings of the Sixth Workshop on Statistical Machine Translation. Klementiev, A., Irvine, A., Callison-Burch, C., & Yarowsky, D. (2012). Toward statistical machine translation without parallel corpora.", "startOffset": 0, "endOffset": 234}, {"referenceID": 11, "context": "Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. Paper presented at the MT summit. Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., et al. (2007). Moses: Open source toolkit for statistical machine translation.", "startOffset": 0, "endOffset": 212}, {"referenceID": 11, "context": "Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. Paper presented at the MT summit. Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., et al. (2007). Moses: Open source toolkit for statistical machine translation. Paper presented at the Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Koehn, P., & Knight, K. (2002). Learning a translation lexicon from monolingual corpora.", "startOffset": 0, "endOffset": 431}, {"referenceID": 11, "context": "Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation.", "startOffset": 0, "endOffset": 42}, {"referenceID": 21, "context": "Och, F. J. (2003). Minimum error rate training in statistical machine translation.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation.", "startOffset": 0, "endOffset": 29}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation. Computational linguistics, 30(4), 417-449. Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation.", "startOffset": 0, "endOffset": 193}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation. Computational linguistics, 30(4), 417-449. Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12. Rapp, R. (1995). Identifying word translations in non-parallel texts.", "startOffset": 0, "endOffset": 344}, {"referenceID": 21, "context": "Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine translation. Computational linguistics, 30(4), 417-449. Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12. Rapp, R. (1995). Identifying word translations in non-parallel texts. Paper presented at the Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1988). Learning representations by back-", "startOffset": 0, "endOffset": 564}, {"referenceID": 21, "context": "Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., et al. (2013). Recursive deep models for semantic compositionality over a sentiment treebank.", "startOffset": 1, "endOffset": 91}, {"referenceID": 21, "context": "Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., et al. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. Paper presented at the Proceedings of the conference on empirical methods in natural language processing (EMNLP). Tillmann, C. (2003). A projection extension algorithm for statistical machine translation.", "startOffset": 1, "endOffset": 305}], "year": 2015, "abstractText": "In this paper, we propose two new features for estimating phrase based machine translation parameters. We only use monolingual data with the assumption that the phrase table is given to us, but all of its scores have been removed. Our method is based on two recently introduced neural network vector representation models for words and sentences. It is the first use of these models in an end to end phrase based machine translation system. Scores obtained from our method can recover more than 80% of BLEU loss caused by removing phrase table probabilities. We also show that our features combined with the phrase table probabilities improve the BLEU score by 0.74 points.", "creator": "Microsoft\u00ae Word 2013"}}}