{"id": "1506.00999", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases", "abstract": "This paper tackles the problem of endogenous link prediction for Knowledge Base completion. Knowledge Bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.", "histories": [["v1", "Tue, 2 Jun 2015 19:34:19 GMT  (398kb)", "http://arxiv.org/abs/1506.00999v1", "26 pages"]], "COMMENTS": "26 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["alberto garcia-duran", "antoine bordes", "nicolas usunier", "yves grandvalet"], "accepted": false, "id": "1506.00999"}, "pdf": {"name": "1506.00999.pdf", "metadata": {"source": "CRF", "title": "Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases", "authors": ["Alberto Gar\u0107\u0131a-Dur\u00e1n", "Nicolas Usunier", "Yves Grandvalet"], "emails": ["alberto.garcia-duran@utc.fr", "abordes@fb.com", "usunier@fb.com", "yves.grandvalet@utc.fr"], "sections": [{"heading": null, "text": "ar Xiv: 150 6.00 999v 1 [cs"}, {"heading": "1. Introduction", "text": "These databases can cover any kind of domain, from specific domains such as biological processes (e.g. in GeneOntology1), to very generic purposes. An example of knowledge engines is WolframAlpha3, an engine that provides answers to any question of natural language, how far it is from the sun, with human readable answers (1.492 \u00d7 109 km), which provides expert / common-level knowledge and skills to its users. An example of knowledge engines is WolframAlpha3, an engine that provides answers to any question of natural language, how far it is from the sun, with human readable answers (1.492 \u00d7 109 km). Part of this work was done while Nicolas Usunier was with Sorbonne universite."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "Although there are (pseudo) symbolic approaches to link predictions based on Markovlogic Networks, which both define a function, this is a threefold combination of observations (Kok & Domingos, 2007) or random walks (Lao, Mitchell, & Cohen, 2011), learning latent features representations of KBs components - the so-called embeddding methods - have recently proven more efficient for performing link predictions in KBs, e.g. (Bordes et al., 2013b; Wang, Zhang, Feng, & Chen, 2014b; Lin, Liu, Liu, & Zhu, 2015; Chang, Yih, & Meek, 2014; Wang, Zhang, Feng, & Chen, 2014) They have relationships, Salwen, Glass, & Gliozzo, 2014; Yang, Duan, Zhou, & Rim, 2014b) all of these activities are represented by entities and dimensions."}, {"heading": "2. Related work", "text": "In this section we discuss the state of the art in modeling large multirelational databases, with particular emphasis on the embedding of methods to complete the knowledge base. One of the simplest and most successful 2-way models is TransE (Bordes et al., 2013b). In this model, relationships are presented as translations into the embedding space: If (h, t) holds, then the embedding of the tail should be t close to the embedding of head h plus a vector that depends on the label. This is a natural approach for hierarchical and asymmetric relationships common in knowledge bases such as freebase. Several changes to TransE have recently been proposed, TransH et al., 2014b) and TransR (Lin et al., 2015). In TransH, the embedding of entities is projected onto a hyperlevel that depends on the prediction."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "The educational goal of RESCAL is the Frobenius standard between the original data sensors and their low-ranking reconstruction, whereas Tatec uses the ranking criterion of TransE. Another related 3-way model is SME (bilinear), and it also uses a ranking criterion as a training criterion. The latent factor model (LFM) and the neural tensor networks (NTN) (Socher, Chen, Manning, Manning, Ng, 2013) use combinations of a 3-way model with a more restricted 2-way model, and in this sense are closer to our algorithms."}, {"heading": "3. TATEC", "text": "We now describe our model and the motivations behind our parameterization."}, {"heading": "3.1 Scoring function", "text": "\"For the first time in my life, I have been able to do this for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the last two years. I have been able to do it for the first time. I have been able to do it for the second time. I have been able to do it for the first time. I have been able to do it for the first time."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "The combination of 2- and 3-way terms has already been used in (Jenatton et al., 2012; Socher et al., 2013), but apart from a different parameterization, Tatec differs from them in the additional freedom created by the use of different embedding in the two terms of interaction. In LFM (Jenatton et al., 2012), the relationship-dependent matrix of 3-way terms was constrained (lower rank on a limited basis of rank-one matrices), the relation vectors r\u0432 1 and r 2 were confined to the image of the matrix (D = 0 in their work). These global constraints severely limited the expressiveness of the 3-way model and act as a strict regulation that reduces the expressiveness of the 2-way model, which, as we explain in Section 3, differs between the 3-way reference ranges and the 3-way model."}, {"heading": "3.2 Term combination", "text": "The difference between our two strategies depends on whether or not we update (or refine) the parameters of s1 and s2 together in a second phase. Fine-tuning This first strategy, known as Tatec-ft, simply consists of adding both values according to Equation (1). (h, t) = 1 1 2 1 eh1 eh2 et2 All parameters of s1 and s2 (and thus of s) are then specified in a second training phase. \"This version could be used directly without pre-training s1 and s2."}, {"heading": "3.3 Interpretation and motivation of the model", "text": "This section discusses the motivations behind Tatec's parameterization, and in particular our choice of a 2-way model to supplement the 3-way concept. 7Garc \"a-Dura\" n, Bordes, Usunier & Grandvalet3.3.1 2-way interactions as a fiber biasesIt is common practice to add biases (also called offsets or intercepts) to the model in regression, classification or collaborative filtering. For example, a critical step of the most powerful Netflix pricing techniques has been to add user and item biases, i.e., a user rating according to Rui's (see e.g. (Koren, Bell, & Volinsky, 2009): Rui \"Pu\" Qi + \"+\" + Biases \"(2), with P\" RU \"c\" k \"biases, i.e. a user rating according to\" Rui \"biases,\" according to \"Bell,\" e.g., \"see\" insky, \"2009."}, {"heading": "3.3.2 The need for multiple embeddings", "text": "An essential feature of Tatec is the use of different embedding spaces for the two- and three-term, while existing approaches, which have both types of interaction, use the same embedding space (Jenatton et al., 2012; Socher et al., 2013). We motivate this choice in this section. It is important to note that biases in the matrix factoring model (2) or the bigram concept in the overall evaluation function (1) do not affect the meaningfulness of the model and, in particular, do not affect the main modeling assumptions that embedding should have a low rank. The prejudice of the user / object in (2) merely amounts to adding two rank-1 matrices \u03b11T and 1\u03b2T to the factoring model. Since the rank of the matrix is a hyperparameter, one can simply add 2 to this hyperparameter and maintain a marginally greater expression value in (2) than would previously obtained from the group of the original parameter (where the increase in the original 50 is a hyperparameter)."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "It's not just the way of the regulation, but also the way, like the way, like the way of the regulation in the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way"}, {"heading": "4. Training", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Ranking objective", "text": "The Tatec training is performed with stochastic gradient pedigree via a rank objective function designed to value positive triples (facts expressing true and verified information from the KB) more highly than negative triples (facts expressing false information).These negative triples can be provided by the KB but often are not, so we need a process to convert positive triples into corrupt ones in order to be able to perform our discriminatory training.A simple approach is to create negative examples by replacing an argument of a positive triple with a random element. This way is simple and efficient in practice, but can cause noise by generating false negatives.5. We can detect the equivalence by taking the eigenvalue decomposition of a symmetrical D: applying the modification of the basis to the embedding to apply only the diagonal part of D in term h1, h1, t1, and 1, and then applying the transformations to D and 1 and 1 thereafter."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "Let's optimize the set of positive tripels provided by the KB, we optimize the following ranking loss function: \"(h, t)\" (h, t \")\" (h, t \")\" (4) \"where [z] + = max (z, 0) and C (h, t) is the set of corrupt tripels.\" Depending on the application, this set can be defined in three different ways: \"1. C (h, t)\" (h,) \"(h,)\" (h,) \"(c)\" (\"c) (\" c) (\"c) (\" c) (\"c) (\" c) (\") (\" c) (\"(h,\" c \") (\") (\"c,\" c \"(\" c, \"c) (c) (c,\" (\"c) (c,\" (\"c) (c),\" (\"c) (\" (c), \"(c) (\" c) (\"(c),\" (\"c) (\" c, \"(\" c) (\"c) (\" (\"c),\" (\"c) (\" (\"c),\" (c) (\"(\" c), \"(c),\" (\"(c),\" (\"(c),\" (\"(c),\" (c), \"(\" (c), \"(\" (c), \"(c,\" (c), \"(\" (c), \"(\" (c), \"(c,\" (\"(c),\" (c, \"(c),\" (c), \"(\" (c), \"(c,\" (c, \"(c),\" (\"(c),\" (c, \"(c),\" (c, c), \"(c (c),\" (c), \"(c (c),\" (c), \"(c (c),\" (c, c, c, c (\"), c (\" (\"), c, c, c (\" (\"), c (\"), c (\"(\"), c, c (\"), c (\"), c (\"), c (\"),"}, {"heading": "4.2 Regularization", "text": "Previous models have applied two different regulatory strategies: either by restricting embedding in the system to have at most a 2-norm of value (1) or by adding a 2-norm penalty for weights (2). (2) Example: 1 Example: Embedding in the 2-norm of radius (1). (2) Example: Embedding in the 2-norm of radius (1). (2) Example: Embedding what we call soft regulation, a term of form [22 \u2212 2e] + for embedding the entity is added. (The soft scheme allows the 2-norm of embedding to grow further than a penalty. (2) In order to control the large capacity of the relationship in the model, we have two models of adjustment."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5. Experiments", "text": "In this section, various experiments are presented to illustrate how competitive Tatec is against several state-of-the-art models based on 4 literature benchmarks: UMLS, Kinships, FB15k and SVO. Statistics of these data sets are in Table 2. All versions of Tatec and its components Bigrams and Trigram are compared with the state-of-the-art models for each database."}, {"heading": "5.1 Experimental setting", "text": "This section describes the protocols used in our various experiments."}, {"heading": "5.1.1 Datasets and metrics", "text": "Our experimental settings and evaluation yardsticks correspond to the first set of negative examples in section 4.1, so we had 1,000 million euros available for comparing results. (Denham, 1973) is a KB that describes the relationship structure of the kinship system of the Australian tribe Alyawarra, and UMLS (McCray, 2003) is a KB of high-level biomedical concepts such as diseases or symptoms associated with verbs such as complications, affects or causes. For these sets of data, the entire set of possible triples, positive or negative, is observed. We used the range below the precision curve as a metric. The dataset was divided into 10 folds for cross-validation: 8 for validation and the last for testing. Since the number of available negative triples is much greater than the number of positive triples, the positive triples of each fold are replicated, the positive models of each fold will correspond to the negative triples, the number of which correspond to the negative triples."}, {"heading": "5.1.2 Implementation", "text": "To train our bigram and trigram models in advance, we validated the learning rate for stochastic gradient descent between {0.1, 0.01, 0.0001} and the margin between {0.1, 0.25, 0.5, 1}. The radius \u03c1e, which determines the value from which the L2 standard of entity embedding is penalized, was set to 1, but the radius \u03c1l of the trigram model is 6. The results for both FB15k and SVO with TransE and Tatec are given in (Garc'\u0131a-Dura \u0301 n et al., 2014), but the hyperparameters in this work were validated on a smaller validation set that led to suboptimal results.14"}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "Due to the different sizes of these CBs, the embedding dimension d was validated in different ranges. For SVO, it was selected under {25, 50}, under {50, 75, 100} for FB15k and under {10, 20, 40} for UMLS and Kinships. Applying soft regulation, the regularization parameter was validated under {0, 0.0001, 0.0001, 0.1, 1, 10, 100}. For fine-tuning Tatec, the learning rates were selected under the same values for the isolated learning of bigrams and trigram models, regardless of the values selected for pre-training, as well as the margin and for the penalties C1 and C2 when soft regulation is used. The configurations of the model, selected on the basis of their performance on the validation parameter, are performed in Appendix A. Training of the combination weights of Tc-atec and C2, using optimization criteria C1 and C2."}, {"heading": "5.1.3 Baselines", "text": "These variants are: \u2022 Tatec-ft-no-pretrain: Tatec-ft without pre-training s1 (h, l, t) and s2 (h, l, t). \u2022 Tatec-ft-shared: Tatec-ft but sharing the entities embeddings between s1 (h, l, t) and without pre-training.The experiments with these 3 versions of Tatec were conducted in the soft regulation setting, their hyperparameters were selected using the same grid as above. Previous models We retrained TransE ourselves with the same hyperparameter grid as for Tatec and used it as a running baseline for all datasets, using either soft or hard regulation."}, {"heading": "5.2 Results", "text": "We remember that the suffixes soft or hard refer to the regulatory scheme applied and the suffixes ft and lc to the combination strategy of Tatec.15Garc \ufffd \u0131a-Dura \ufffd n, Bordes, Usunier & Grandvalet."}, {"heading": "5.2.1 UMLS and Kinships", "text": "The results of these two knowledge bases are shown in Table 3. In UMLS, most models do well. Combining the bigram and trigram models is slightly better than the trigram alone, but not significant."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "The difference between TransE and bigrams on this dataset illustrates the potential effects of the diagonal matrix D, which does not restrict the embedding of both the head and tail units of a triple that has to be similar. In terms of kinships, there is a large gap between 2-way models such as TransE and 3-way models such as RESCAL. The cause of this deterioration stems from a peculiarity of the positive triple units of this KB: each unit appears 104 times - the number of units in this KB - as a head and it is connected to the 104 units - even itself - only once. In other words, the conditional probabilities P (head | tail) and P (tail | head) are completely uninformative, which has a very important consequence for the 2-way models, although they rely heavily on the interaction."}, {"heading": "5.2.2 FB15k", "text": "The simplicity of the 2-way models seems to be an advantage in FB15k: This is something that has already been observed in Quote (Yang, Yih, He, Gao, & Deng, 2014a).The combination of the Bigrams and Trigram models in Tatec results in an impressive improvement in performance, which means that for this KB the information encoded by these 2 models is complementary. Tatec surpasses all existing methods - except middle-order Transrams - with a large margin in Hits @ 10. Bigrams-soft results in an impressive improvement in performance, which means that the information encoded by these 2 models is complementary. Tatec surpasses all existing methods - except middle-order Transrams models - with a large margin in Hits @ 10. Bigrams-soft results in an impressive improvement in performance, which means that for this KB the information encoded by these 2 models is complementary."}, {"heading": "5.2.3 SVO", "text": "In terms of hits @ 5%, Tatec performs better than its components, but in terms of middle rank, the Bigrams model is significantly worse than Trigram and Tatec. LFM's performance lies between the Trigram and Bigrams models, which confirms that splitting the embedding into 2- and 3-way terms can actually prevent both types of interaction from being used optimally. As far as kinships are concerned, since the performance of Bigrams is much worse than that of Trigram, Tatec-lc is very competitive. It seems that if Bigrams and Trigram perform well for different types of relationships (as in FB15k), then the combination of Bigrams via fine-tuning (i.e. Tatec-ft) is very competitive when Bigrams and Trigram are good for different types of relationships (as in FBB15k), the training of Tatec models is worst."}, {"heading": "5.3 Illustrative experiments", "text": "This last experimental section provides some illustrations and insights into the capabilities of Tatec and TransE.18."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 TransE and symmetrical relationships", "text": "TransE has a peculiar behavior: it performs very well on FB15k, but performs pretty poorly on all other datasets. If we look more closely at FB15k, we noticed that this database consists of many pairs of symmetrical relationships, such as / film / film / subjects and / film / film subject / films, or / music / album / genre and / music / genre / albums. The simplicity of TransE's translation model works well if the model can use its symmetrical counterpart to predict the validity of an unknown triple when it was present in the training set. Specifically, 45,817 of 59,071 test triples of FB15k have a symmetrical triple in the training set. If we divide the test triples into two subsets, one of which contains the test triples containing Tatec test triples for which a symmetrical triple of B15k was used."}, {"heading": "5.3.2 Anecdotal examples", "text": "Some examples of Tatec predictions on FB15k are shown in Table 7. In the first row, we want to know the answer to the question, what is the location of the Polish national football team?; among the possible answers, we find not only locations, but also countries that make sense for a national team. In the question, what is the theme of the film \"Remember the Titans,\" the top 10 candidates can be potential film topics. The same applies to the answers to the question, to which religion Noam Chomsky belongs?, which can all be typed as religions. In these examples, both sides of the relationship are clearly typed: a certain type of being is expected in head or tail (country, religion, person, film, etc.) Tatec operators can then operate on specific regions of the embedding space. On the contrary, the relationship / web page / category is an example of non-typed relationships."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "This shows the effects of the interaction between heads and tails in the bigram model: it tends to push together the units connected in tripartite relationships, whatever the relationship is. In this case, this forms a Japanese cluster.Table 8 shows examples of predictions about the use of SVO. In the first example, although run is the target verb for the pair (minister, protest), other verbs such as offer or offer good matches are just as good. Similarly, non-target alliances form such as establish or join, and lead, participate or participate are good matches for the second and third example (emigrant, country) and protest. The fourth and fifth instances show an example of very heterogeneous performance for the same relationship (the target verb is in both cases), which can be explained easily from a semantic point of view: transport is a very well-fitting pair (coal), whereas a channel is a very heterogeneous one."}, {"heading": "6. Conclusion", "text": "This paper introduces Tatec, a tensor factorization method that satisfactorily combines 2- and 3-way interaction dates to achieve performance better than the best of the two components. Different data patterns are properly coded thanks to the use of different embedding spaces and a two-phase training (pre-training and fine-tuning / linear combination). Experiments on four benchmarks for different tasks and with different quality scales demonstrate the strength and versatility of this model, which could be considered a generalization of a variety of existing work."}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "about the two usual regulation programs used so far in these embedded models: both achieve similar performance, even if soft regulation seems a little more efficient, but with an additional hyperparameter."}, {"heading": "Acknowledgments", "text": "This work was carried out within the framework of the Labex MS2T (ANR-11-IDEX-0004-02) and financed by the French research agency EVEREST-12-JS02-005-01."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Appendix A. Optimal hyperparameters", "text": "The optimal configurations for UMLS are: - TransE-soft: d = 0.001 = 0.001 = 0.001 = = 0.5 = = = = = = 0.01, \u03b3 = = = 0.001, - Bigrams-hard: d1 = 40, - Bigrams-hard: d2 = 40, - Bigrams-hard: d1 = 40, d2 = 40, - Bigrams-hard: d1 = 40, d2 = 40, \u03bb1 = 0.001, \u03bb2 = 0.001, \u03b3-soft: 1 = 1, C1 = 0.001, - Bigrams-hard: d1 = 40, - Bigrams-hard: d1 = 0.01, \u03bb1 = 0.5, \u03bb1 = 0.5; - Trigram-hard: d2 = 0.001, \u03bb1 = 0.001, - hard-hard: 40, - Bigragram-hard-hard-hard: d2 = 0.001; - Bigrams-high-high-hard-hard-high-hard-hard-high-configurations: d1 = 0.001; - Figures-hard-0.1-hard 0.001-hard 0.001-hard = 0.5, - hard-conxi 0.001-0.00id = 0.5-001; - Bigrams-hard 0.00g = 0.5-hard 0.001-conxi = 0.5-001; - conxi-hard 0.00g = 0.5-001; - 0.00g = 0.00g = 0.001-hard-hard-hard: 0.001; - conxi-hard 0.001-hard = 0.001-hard = 0.5-conxi = 0.001; - 0.001-hard = 0.001-hard = 0.001-hard = 0.001; - configurations for UMLS are: - TransE-soft: d = 0,001 = 0.001, - Tatec-soft: d1 = 0.001, - Tatec-soft: d1 = 0.001, - Tatec-soft: d1 = 0.001, - Tatec-soft: d1 = 0.001, - Tatec-soft: d1 = 0.001, - Tatec-soft: d1 = 0.001, - Tatec-soft: d1 = 0.001, \u03b3-soft: d1"}, {"heading": "Embeddings Models for Link Prediction in KBs", "text": "In Proceedings of the 24th International Conference on Machine Learning, ICML '07, pp. 433-440.Koren, Y. Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for commender systems. Computer, 42 (8), 30-37. Lao, N., T., & Cohen, W. W. (2011). Random walk inference and learning in a large scale knowledge base. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 539. Association for Computational Linguistics.Lin, Y., Z., M., Liu, Y., Zhu, X. (2015). Learning entity and relation embeddings for knowledge graph completion."}], "references": [{"title": "A semantic matching energy function for learning with multi-relational data", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Gar\u0107\u0131a-Dur\u00e1n", "J. Weston", "O. Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Chang", "K.-W", "Yih", "W.-t", "B. Yang", "C. Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "The detection of patterns in Alyawarra nonverbal behavior", "author": ["W. Denham"], "venue": "Ph.D. thesis,", "citeRegEx": "Denham,? \\Q1973\\E", "shortCiteRegEx": "Denham", "year": 1973}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Effective blending of two and threeway interactions for modeling multi-relational data", "author": ["A. Gar\u0107\u0131a-Dur\u00e1n", "A. Bordes", "N. Usunier"], "venue": "In ECML PKDD", "citeRegEx": "Gar\u0107\u0131a.Dur\u00e1n et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gar\u0107\u0131a.Dur\u00e1n et al\\.", "year": 2014}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N. Le Roux", "A. Bordes", "G. Obozinski"], "venue": null, "citeRegEx": "Jenatton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J.B. Tenenbaum", "T.L. Griffiths", "T. Yamada", "N. Ueda"], "venue": "In Proc. of the 21st national conf. on Artif. Intel. (AAAI),", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Building a large-scale knowledge base for machine translation", "author": ["K. Knight", "S.K. Luk"], "venue": "In AAAI,", "citeRegEx": "Knight and Luk,? \\Q1994\\E", "shortCiteRegEx": "Knight and Luk", "year": 1994}, {"title": "Statistical predicate invention", "author": ["S. Kok", "P. Domingos"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Kok and Domingos,? \\Q2007\\E", "shortCiteRegEx": "Kok and Domingos", "year": 2007}, {"title": "Matrix factorization techniques for recommender", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "systems. Computer,", "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "In Proceedings of AAAI\u201915", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "An upper level ontology for the biomedical domain", "author": ["A.T. McCray"], "venue": "Comparative and Functional Genomics,", "citeRegEx": "McCray,? \\Q2003\\E", "shortCiteRegEx": "McCray", "year": 2003}, {"title": "WordNet: a Lexical Database for English", "author": ["G. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller,? \\Q1995\\E", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Structural semantic interconnections: a knowledge-based approach to word sense disambiguation", "author": ["R. Navigli", "P. Velardi"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Navigli and Velardi,? \\Q2005\\E", "shortCiteRegEx": "Navigli and Velardi", "year": 2005}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "Kriegel", "H.-P"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution", "author": ["S.P. Ponzetto", "M. Strube"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Ponzetto and Strube,? \\Q2006\\E", "shortCiteRegEx": "Ponzetto and Strube", "year": 2006}, {"title": "Collaborative filtering in a non-uniform world: Learning with the weighted trace norm", "author": ["R. Salakhutdinov", "N. Srebro"], "venue": null, "citeRegEx": "Salakhutdinov and Srebro,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov and Srebro", "year": 2010}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J. Tenenbaum"], "venue": "In Adv. in Neur. Inf. Proc. Syst", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Stochastic blockmodels for directed graphs", "author": ["Y.J. Wang", "G.Y. Wong"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wang and Wong,? \\Q1987\\E", "shortCiteRegEx": "Wang and Wong", "year": 1987}, {"title": "Knowledge graph and text jointly embedding", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Learning multi-relational semantics using neural-embedding models. CoRR, abs/1411.4072", "author": ["B. Yang", "Yih", "W.-t", "X. He", "J. Gao", "L. Deng"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Joint relational embeddings for knowledge-based question answering", "author": ["Yang", "M.-C", "N. Duan", "M. Zhou", "Rim", "H.-C"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Word semantic representations using bayesian probabilistic tensor factorization", "author": ["J. Zhang", "J. Salwen", "M. Glass", "A. Gliozzo"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": ", 2014b) and TransR (Lin et al., 2015).", "startOffset": 20, "endOffset": 38}, {"referenceID": 6, "context": "The Latent Factor Model (LFM) (Jenatton et al., 2012) and the Neural Tensor Networks (NTN) (Socher, Chen, Manning, & Ng, 2013) use combinations of a 3-way model with a more constrained 2-way model, and in that sense are closer to our algorithm Tatec.", "startOffset": 30, "endOffset": 53}, {"referenceID": 11, "context": "In that line of work, the Path Ranking Algorithm (PRA) (Lao et al., 2011) estimates the probability of an unobserved fact as a function of the different paths that go from the subject to the object in the multi-relational graph; learning consists in finding, for each relationship, a weight associated to a kind of path (represented as a sequence of relationships) linking two entities.", "startOffset": 55, "endOffset": 73}, {"referenceID": 16, "context": "The 3-way term corresponds exactly to the model used by the collective factorization method RESCAL (Nickel et al., 2011), and we chose it for its high expressiveness on", "startOffset": 99, "endOffset": 120}, {"referenceID": 6, "context": "The combination of 2- and 3way terms has already been used in (Jenatton et al., 2012; Socher et al., 2013), but, besides a different parameterization, Tatec contrasts with them by the additional freedom brought by using different embeddings in the two interaction terms.", "startOffset": 62, "endOffset": 106}, {"referenceID": 19, "context": "The combination of 2- and 3way terms has already been used in (Jenatton et al., 2012; Socher et al., 2013), but, besides a different parameterization, Tatec contrasts with them by the additional freedom brought by using different embeddings in the two interaction terms.", "startOffset": 62, "endOffset": 106}, {"referenceID": 6, "context": "In LFM (Jenatton et al., 2012), constraints were imposed on the relation-dependent matrix of the 3-way terms (low rank in a limited basis of rank-one matrices), the relation vectors r1 and r l 2 were constrained to be in the image of the matrix (D = 0 in their work).", "startOffset": 7, "endOffset": 30}, {"referenceID": 19, "context": "We are similar to NTN (Socher et al., 2013) in the respect that we do not share any parameter between relations.", "startOffset": 22, "endOffset": 43}, {"referenceID": 16, "context": "The 2-way + 3-way interaction model we propose can be seen as the 3-mode tensor version of this \u201cbiased\u201d version of matrix factorization: the trigram term (T) is the collective matrix factorization parameterization of the RESCAL algorithm (Nickel et al., 2011) and plays a role analogous to the term \u3008", "startOffset": 239, "endOffset": 260}, {"referenceID": 6, "context": "2 The need for multiple embeddings A key feature of Tatec is to use different embedding spaces for the 2-way and 3-way terms, while existing approaches that have both types of interactions use the same embedding space (Jenatton et al., 2012; Socher et al., 2013).", "startOffset": 218, "endOffset": 262}, {"referenceID": 19, "context": "2 The need for multiple embeddings A key feature of Tatec is to use different embedding spaces for the 2-way and 3-way terms, while existing approaches that have both types of interactions use the same embedding space (Jenatton et al., 2012; Socher et al., 2013).", "startOffset": 218, "endOffset": 262}, {"referenceID": 10, "context": "The critical feature of these biases in collaborative filtering is how they interfere with capacity control terms other than the rank, namely the 2-norm regularization: in (Koren et al., 2009) for instance, all terms of (2) are trained using a squared error as a measure of", "startOffset": 172, "endOffset": 192}, {"referenceID": 5, "context": "2 Regularization Previous work on embedding models have used two different regularization strategies: either by constraining the entity embeddings to have, at most, a 2-norm of value \u03c1e (Gar\u0107\u0131a-Dur\u00e1n et al., 2014) or by adding a 2-norm penalty on the weights (Wang et al.", "startOffset": 186, "endOffset": 213}, {"referenceID": 12, "context": ", 2014) or by adding a 2-norm penalty on the weights (Wang et al., 2014b; Lin et al., 2015) to the objective function (4).", "startOffset": 53, "endOffset": 91}, {"referenceID": 3, "context": "UMLS/Kinships Kinships (Denham, 1973) is a KB expressing the relational structure of the kinship system of the Australian tribe Alyawarra, and UMLS (McCray, 2003) is a KB of biomedical high-level concepts like diseases or symptoms connected by verbs like complicates, affects or causes.", "startOffset": 23, "endOffset": 37}, {"referenceID": 13, "context": "UMLS/Kinships Kinships (Denham, 1973) is a KB expressing the relational structure of the kinship system of the Australian tribe Alyawarra, and UMLS (McCray, 2003) is a KB of biomedical high-level concepts like diseases or symptoms connected by verbs like complicates, affects or causes.", "startOffset": 148, "endOffset": 162}, {"referenceID": 5, "context": "Following (Gar\u0107\u0131a-Dur\u00e1n et al., 2014), in order to reduce this noise in the measure, and thus granting a clearer view on ranking performance, we remove all the positive triples that can be found in either the training, validation or testing set, except the target one, from the ranking.", "startOffset": 10, "endOffset": 37}, {"referenceID": 6, "context": "It has been introduced in (Jenatton et al., 2012).", "startOffset": 26, "endOffset": 49}, {"referenceID": 5, "context": "Results on both FB15k and SVO with TransE and Tatec are provided in (Gar\u0107\u0131a-Dur\u00e1n et al., 2014), however in these works the hyperparameters were validated on a smaller validation set, that led to suboptimal results.", "startOffset": 68, "endOffset": 95}, {"referenceID": 12, "context": "On FB15k, recent variants of TransE, such as TransH, TransR and cTransR (Lin et al., 2015) have been chosen as main baselines.", "startOffset": 72, "endOffset": 90}, {"referenceID": 6, "context": "The results for these models have been extracted from (Jenatton et al., 2012), and we followed their experimental setting.", "startOffset": 54, "endOffset": 77}, {"referenceID": 14, "context": ", 2013b), the algorithm is shown to perform well on FB15k and on a dataset extracted from the KB WordNet (Miller, 1995): we suspect that the WordNet dataset also contains symmetrical counterparts of test triples in the training set (such as hyperonym vs hyponym, meronym vs holonym).", "startOffset": 105, "endOffset": 119}], "year": 2015, "abstractText": "This paper tackles the problem of endogenous link prediction for Knowledge Base completion. Knowledge Bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.", "creator": null}}}