{"id": "1506.01056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Performing Bayesian Risk Aggregation using Discrete Approximation Algorithms with Graph Factorization", "abstract": "Risk aggregation is a popular method used to estimate the sum of a collection of financial assets or events, where each asset or event is modelled as a random variable. Applications, in the financial services industry, include insurance, operational risk, stress testing, and sensitivity analysis, but the problem is widely encountered in many other application domains. This thesis has contributed two algorithms to perform Bayesian risk aggregation when model exhibit hybrid dependency and high dimensional inter-dependency. The first algorithm operates on a subset of the general problem, with an emphasis on convolution problems, in the presence of continuous and discrete variables (so called hybrid models) and the second algorithm offer a universal method for general purpose inference over much wider classes of Bayesian Network models.", "histories": [["v1", "Tue, 2 Jun 2015 20:53:26 GMT  (3166kb)", "http://arxiv.org/abs/1506.01056v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peng lin"], "accepted": false, "id": "1506.01056"}, "pdf": {"name": "1506.01056.pdf", "metadata": {"source": "CRF", "title": "Performing Bayesian Risk Aggregation using Discrete Approximation Algorithms with Graph Factorization", "authors": ["Peng Lin"], "emails": [], "sections": [{"heading": null, "text": "QUEEN MARY, UNIVERSITY OF LONDONPerforming Bayesian Risk Aggregation using DiscreteApproximation Algorithms with Graph FactorizationPeng LinDecember 2014 Filed in partial compliance with the requirements of the Doctorate of Philosophy Declaration of OriginalityI, Peng Lin, acknowledge that the research contained in this dissertation is my own work or, provided that it has been done in collaboration with or assisted by others, that this is duly acknowledged below and my contribution is acknowledged hereinafter. Pre-published material is also acknowledged hereinafter. I acknowledge that I have taken reasonable care to ensure that the work is original, and not to the best of my knowledge infringe any UK law, that third party copyright or other intellectual property rights are infringed or contain confidential material. I accept that the college has the right to use plagiarism recognition software to verify the electronic version of the dissertation, or that it has not been filed by the parent."}, {"heading": "Acknowledgement", "text": "This year, the time has come for it to be able to try to find a solution, to find a solution that is capable, that is able to find a solution that is capable, that is able to find a solution."}, {"heading": "1. Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Motivation", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "1.2. Research Hypotheses", "text": "The main objective of this thesis is to perform a stochastic risk aggregation using a Bayesian causal framework. Typically, such models could include independent random variables, hybrid random variables, and high-dimensional interdependent random variables. Therefore, this research is being conducted to answer the following four research hypotheses: First, can we perform a Bayesian stochastic risk aggregation with a discrete approach to accurately address the composition density if the composition variables are independent? Second, can we perform a Bayesian stochastic risk aggregation with a discrete approach to address the composition density if the composition variables are hybrid and dependent? Third, can we precisely disentangle the causal explanatory variables by using discrete approximation approaches for the Bayesian stochastic risk aggregation model? Fourth, can we use a Bayesian risk aggregation with precise risk aggregation?"}, {"heading": "1.3. Structure of the Thesis", "text": "Chapter 2 examines the background to Bayesian network aggregation with an emphasis on factorization, the central thrust of this thesis. It examines Bayesian theorems, Bayesian networks, and other supporting structures (such as Markov diagrams). Chapter 3 discusses the inference approaches for Bayesian networks. It reviews the current popular inference approaches, such as JT, VI, MCMC, and GBP. Chapter 4 introduces the BFE algorithm for N-fold convolution models, which are independent and hybrid dependent random variables. This chapter presents the factorization and variable elimination techniques that are central components of the BFE algorithm. Next, it demonstrates the de-convolution of the Bayesian risk aggregation model. This chapter focuses on the exploration of these hypotheses and variable elimination techniques that are central components of the BFE algorithm."}, {"heading": "1.4. Publications", "text": "Until the submission of this paper: 1. Lin, P., Neil, M., & Fenton, N. (2014): Risk Aggregation in the Presence of Discrete Causally Linked Random Variables. Annals of Actuarial Science, 8 (2), pp. 298-319 doi: 10.1017 / S17484995140000982. Lin, P., Neil, M., & Fenton, N. (2014): Inference for High-Dimensional Bajesian Network Models Using Dynamically Discredited Faith Propagation, submitted to Information Sciences."}, {"heading": "2. Belief Networks and Graph", "text": "Factorization This chapter provides an overview of the basic concepts of probability that are at the heart of this thesis. We will discuss Bayesian probability and (touch) causality, discuss Bayesian networks and other network structures in more detail, and attach particular importance to approaches to the factorization of graphs. Concepts presented in this chapter will be used to describe the algorithms presented in the following chapters of this thesis."}, {"heading": "2.1. Bayesian Probability", "text": "Bayesian probability (Cox, 1961) (Finetti, 1970) provides a path to reason in a coherent manner toward the uncertainty we face in the real world. It reflects personal knowledge, and any belief in uncertainty is assumed to be provisional based on previous experience (the previous one), then updated by new experiences and data (the probability) to provide a new personal disposition about uncertainty (the posterior one). Probabilities are quantitative measures of this uncertainty on a unit scale and subject to the axioms of probability theory (Devore, 2011). However, probability is not just about numbers, but also about the structure of reasoning (Pearl, 1988) that can be used for reason (i.e. cause-to-effect or vice versa). Indeed, Bayesian probability has gained popularity in recent decades because Bayesian's theorem supports the argument about causal propositions, how the variables relate to beliefs (probabilities)."}, {"heading": "2.2. Factorization and Bayesian Belief Networks", "text": "The complexity problem of calculating the common probability can be overcome by exploiting conditional independence (Dawid, 1979) (Pearl, 1988). According to the chain rule of probability, we can use the common probability 1 (,...), 2 (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X) (X), X (X), X (X) (X), X (X) (X) (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X), X (X (X), X (X), X (X), X (X (X), \"X (X), X (X (X), X (X), X (X (X), X (X), X (X (X),\" X (X (X), X (X), X (X (X), X (X (X), X (X), \"X (X (X), X (X (X), X (X), X (X (X),\" X (X (X), X (X), X (X (X), X (X (X), \"),\" X (X (X (X), \"),\" X (X (X (X), X (X (X (X), \"), X (X (X (X), X (X (X),\"), X (X (X (X (X), \"),\"), X (X (X (X (X (X), \"), X (X (X (X),\"), \"), X (X (X (X (X),\"),"}, {"heading": "2.3. Causation and BN Structuring", "text": "In fact, it is not as if people are able to save themselves. (...) It is not as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves. (...) It is as if people are able to save themselves."}, {"heading": "2.4. Decomposing Node Probability Tables", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2.5. Converting a sparse graph into a DCCD", "text": "A continuous, frugal graph model, G, can be converted to a DCCD, \"G,\" retaining the associated CPDs: 1. the arrangement of all original variables from parent to child, assigning higher tokens to the children than to their parents; 2. the addition of edges for each variable to all their higher-denominated descendants (each variable is then connected via a path to their descendants); 3. the assignment of each original variable to the new ancestors of the variable by blocking these unrelated ancestors (ancestors / parents) by placing zeros as weights on the new conditional deterministic expression relative to the variables in {ancestors / parents}. If a CPD is conditionally deterministic: 5 1 2 4 1 2 3 40X X X X X X X X X X X X X X X X X X X, the expression for 5X is defined to three parents, but it can also be defined on all four ancestors."}, {"heading": "3. Inference in Belief Networks", "text": "This chapter deals with the popular exact and approximation conclusions of BN algorithms. Section 3.1 deals with exact conclusions, 3.2 with approximation conclusions, and 3.3 and 3.4 focus in particular on developments related to the TRC and DDBP algorithms."}, {"heading": "3.1. Exact Inference", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "3.2. Approximate Inference", "text": "Most approaches to approximate inference in BNs include Markov Chain Monte Carlo (MCMC) based on samplers, variation scenarios (VI), Dynamic Discretization with Junction Tree (DDJT), (Generalized) Belief Propagation (G / BP) and their variants, based on continuous domain algorithms: Expectation, Propagation (EP), Non-Parametric Belief Propagation (PBP). This section briefly describes MCMC, VI, DDJT, BP, EBP, and PBP. GBP will be discussed further in Section 3.4.1. MCMC is a stochastic simulation method. There is a largenumber of specifically designed MCMC samplers (Gamerman & Lopes) applied to Bayesian inference."}, {"heading": "3.3. Dynamically Discretized Inference Algorithm", "text": "This section introduces dynamic discretization (Neil et al., 2007) alongside the junction-tree algorithm to draw approximate conclusions on hybrid models. Unlike conventional approximation conclusions, the dynamic discretization algorithm uses precise inference JT algorithms. The approximation error is caused by discretization, which is more accurate than static discretization methods."}, {"heading": "3.3.1 Dynamic Discretization", "text": "In fact, the fact is that most of them are able to move to another world, in which they are able, in which they are able to move, and in which they are able to change the world."}, {"heading": "3.3.2 Dynamically Discretized Junction Tree Algorithm", "text": "The discretization strategy is determined by the relative entropy error resulting from the inference, and then new factors can be generated for NPT tables to perform the next propagation. NPT generation is determined by discredited partitions and deterministic or statistical functions in which the child and the partitions of its parents are combined to form an NPT, either using a set of uniform mixtures, in the case of conditionally deterministic functions, or directly from a statistical distribution. Example of a conditional deterministic distribution, X Y Z, the NPT for (,) p Y Z can be determined by (Neil et al, 2007): (,], [,]) (min (,), max (,) u vu vu vu vu vv vv vv vv vp Y vy y y y y 3.Y factors."}, {"heading": "3.4. Generalized Belief Propagation", "text": "In all common BP algorithms (Kschischang et al., 2001), messages are sent from a node to an adjacent node in a factor diagram. Propagation is accurate if the factor diagram has no cycles, but for factor diagrams containing cycles, we can only perform approximate propagation. Although BP algorithms are well defined when factor diagrams have cycles, convergence is sometimes not achieved (Yedidia et al., 2005). (Yedidia et al., 2005) have generalized the BP algorithm and shown that BP convergence improves the Bethe approximation and achieves better accuracy than ordinary BP. These gains are achieved by constructing an energy diagram showing a region between a factor diagram and a region."}, {"heading": "3.4.1 Converting a BN to a Markov Network", "text": "GBP is defined on an undirected graph and this requires the conversion of a BN into a Markov network (MN). For a set of variables 1 {,...,} nX XX, an MN is an undirected graph G and is defined as a product of the factors () c c X, on subsets of variables c X: 111 (,...,) () Cn c ccp X X Z X, (3.6), where 1,..., c C are the maximum clusters of G and Z is a normalization constant. The conversion of BN to MN comprises two steps: 1. Convert the BN parameterization to MN parameterization: {} (, {}) i ii i i pa X i i i i i (pa), since the graph {ipa} {3 (Figure ipa}) represents a moral conversion."}, {"heading": "3.4.2 Factor Graphs", "text": "Both BNs and MNs can be represented by a unifying representation, which is referred to as a factor diagram (FG) (Kschischang et al., 2001).FGs explicitly express the factorization structure of the corresponding probability distribution. Standard BP performs the passing of factor graphs.An FG is a specific graphical model with applications in Bayesian inference, which allows an efficient calculation of boundary distributions by the sum product algorithm 4 (Koller & Friedman, 2009), (Kschischang et al., 2001).It is a two-part diagram that represents the factoring of a function.Given a factoring of the function 1 2 1 (,...) () mn j jg X X X X X X X X f S, where 1 2 {,...,} j nS X X {X X X, X {X, X}, {,} j nS X, the variable, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the"}, {"heading": "3.4.3 Belief Propagation on Factor Graph", "text": "For a given FG with the variables 1,..., NX\\ X is the common probability mass function 1 1 (,...,) () N a aaP X x x p f Z x x (3,7) Where x is set,..., {1 Nxx. In general, we are interested in the calculation to the marginal S pp SS xx\\) () (.In Figure 3.5, the message that is passed in an FG includes two steps: 1. Messages) (iia xm from factors to variables: What values include a sympathetic iX to on.2. Messages) (iai xm from variables to factors: What values iX likes are based on information from all but a.Referring to information theory, we define variable and factor nodes' belief (Yedidia et al., 2005): What value a sympathetic iX variable i."}, {"heading": "3.4.4 Region Free Energy and Region Graph", "text": "In all common BP algorithms, messages are sent from a node to an adjacent node. (D) Propagation is accurate when the graphical model has no cycles, but for models with cycles we can only perform in an approximate region. (D) Figure 3.7 is identical to Figure 3.5, but shows the cycles contained in the factor chart. The standard BP performance in this diagram is an approximation of an approximate region. (D) Figure 3.7 is identical to the construction of the free energy of a system. Rather than propagating messages among the nodes in a FG GBP operates on a region graph (RG), which is a graphical formalism for generating free energy. There are several ways to define the regions in a diagram to support message exchange. (D) et al has shown that a valid construction of the corresponding RG algorithms can be specified."}, {"heading": "3.4.5 GBP Message Passing", "text": "There are several ways of communicating messages in GBP: parents to children, child to parents and dual messaging. All messages transmitted are derived from faith equations. Each algorithm has its advantages and disadvantages, but here the dual messaging algorithm is used as the basis for the TRC algorithm, since TRC guarantees that a region graph is a DAG. Therefore, our discussion will focus on dual messaging. The dual messaging algorithm is particularly elegant when each region and its sub-regions form a tree, and all factors appear in the first level of the regional graph (Yedidia et al., 2005). In the dual messaging algorithm, the belief equation in a region is a product of local factors and messages arriving from all connected regions, whether they are parents or children, as in Equation 3.14. () () () Rseudogesc) () x R C R R R P P P P P b x x x\\ m."}, {"heading": "4. BFE Risk Aggregation", "text": "This chapter discusses Bayesian risk aggregation algorithms for hybrid models. Section 4.1 provides an overview of common risk aggregation methods. Section 4.2 illustrates n-fold convolution using BNs. The SFOE risk aggregation algorithm is described in Section 4.3, which shows how it builds on and extends the standard BN algorithms. Section 4.4 presents a version of BFE that performs deconvolution and Section 4.5 presents experimental results showing the performance of BFE. Section 4.6 closes the chapter."}, {"heading": "4.1. Risk Aggregation and BNs", "text": "The general formula for fixing-, investing-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-, system-"}, {"heading": "4.3. Bayesian Factorization and Elimination (BFE)", "text": "In order to solve the N fold problem using standard BN technology, it is not possible to effectively calculate the CDG and 2G from the conditional dependency structures defined in Figure 4.1. This is because, even with binary factorization, either the model size is prohibitively large (in the case of 1G) or the clique sizes of the connecting trees would be exponentially large (as in 2G). Therefore, the initial contribution of this thesis is to produce an iterative factored approach to the calculation, scaled to any size models, called Bayesian factorization and elimination (BFE), which creates confusion on the hybrid hybrid models needed to aggregate the presence (or absence) of causal dependencies. This algorithm uses a number of advances from the field of BNs already described in Chapters 2."}, {"heading": "4.3.1 Log Based Aggregation (LBA)", "text": "In Eq.4.3, each, 1,... iT i n is the sum of its parent variables 1iT and iS, the aggregation process simply contains repeated sums of the same variable iS. Since binary factorization continues intermediate variables jF, all two parents are aggregated, creating a hierarchy until the total aggregate T is calculated. An example of the presence and absence of a common cause vector is shown in Figure 4.3 (for simplicity's sake, we have assumed that the hierarchy has depth three and the other level contains intermediate variables called jF). The computational efficiency of this process is () O n. This aggregation approach is mathematically expensive, since all variables are explicitly entered and calculated into the BN. Log-based aggregation simply computes the previously calculated results and then reuses them recursively, so that in each subsequent step we can use the results from previous steps to calculate them as a total instead of generating them (without generating the BN)."}, {"heading": "4.3.2 Variable Elimination (VE)", "text": "So it is not that it is not as if it is not as if......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4.3.3 Compound Density Factorization (CDF)", "text": "iDe eeisrcnh-eaeaJnlhsrcnlhsrteeaeaD nvo rf\u00fc ide eeisrmtlrteaeaeVnlrlrgne\u00fce, nlrteew sasd hacu nvo eeisrmnn-eaJng0eaeaeaJng0o0-0) (.) D \"i\" nlrteew, ew os os os, rf\u00fc ide eeisn-eaJng0o0o0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0"}, {"heading": "4.3.4 The BFE Convolution Algorithm with Example", "text": "The SFOE folding algorithm is formalized as pseudo code: algorithm 2 SFOE folding algorithm Input: S: severity variable, N: frequency variable, C: vector of common causes (optional) Output: connection density T Main: 1. Calculate the probability density function of N, with sample space Z by: () () ({: ()}), 0 1N j jf x P N x P z Z N z x a j,,..., length (Z) 2. for 0j to (length of Z) do3. for 0i to jz do4. Calculate jz -fold convolution0jzz ii T S by BF and LBA algorithms5. Eliminate nodes (outside the query set) by VE algorithm"}, {"heading": "6. end for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7. While 2j  do", "text": "8. Application of the CDF algorithm for factorization (4.3) according to the probability density of N, calculation 1 1 2 1 () () yy y y zF P E True P F P E False P T 9. Removal of nodes iS, 2jF and jzT according to the VE algorithm"}, {"heading": "10. end while", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11. end for", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "12. return", "text": "1 () jP F {boundary distribution of T} algorithm 2 BFE folding algorithm Example 4.1 Let us consider a simple example model that aggregates events with boundary frequency ~ (0.5) N geometrically and with boundary gravity distribution ~ (1) S exponentially, as shown in Figure 4.8. Finally, we create and execute a BN parameter with the relevant values. Figure 4.9 shows the partial factoring steps for the first three terms of the connection density. By maintaining the factoring with the CDF algorithm, we obtain the connection density T with mean 1 and variance 3.1 in AgenaRisk, while the analytically derived mean and variance 1 and 3 are respectively. We have factored the conditional deterministic function () P T with the CDF algorithm, whereby the frequency N is factored by 0,..., jE E E is factored as generative boolean expressions. This factorization is reversible, since we can achieve the conditional function by means of CDF (CDP) by means of conditional ministic function."}, {"heading": "4.4. Deconvolution using the BFE Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 Deconvolution", "text": "Dre rf\u00fc ide f\u00fc die f \u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die f\u00fc die"}, {"heading": "4.4.2 Reconstructing the Frequency Variable", "text": "This year it is so far that it is a reactionerres, reactionerres, reactionerres, reactionerres, reactionerres, reactioneros, reactioneros, reactioneros pnpnpnpnpnpnPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzP.PzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzP.PzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPzPr, PzPzPzPzPzPzPzPzPzPr, PzPzPzPzPzPzPzPzPzPzPzPzP"}, {"heading": "4.4.3 The BFE Deconvolution Algorithm with Examples", "text": "The SFOE deconvolution algorithm for N-fold deconvolution is formalized as pseudo-code: algorithm 3 SFOE deconvolution algorithm Input: S: severity variable, N: frequency variable, C: vector of common causes and 0T tOutput: rear marginal limit of the query participants, i.e. 0 (|) P T tC, 0 (|) P N T Main: 1. Do convolution BFE algorithm to generate final query set 2. if N in query set3. Reconstruction N from jE"}, {"heading": "4. end if", "text": "5. Set evidence on T and draw conclusions 6. Return posterior marginal distributions for query setAlgorithm 3 SFOE deconvolution algorithmExample 4.3 deconvolutes frequency distribution: Consider a simplified example of deconvoluting N, take frequency distribution N is discretized as {0,1, 0,2, E E) with discrete states {0,1, 2, 3} and ~ (1) S exponential. Figure 4.12 (a) shows these incremental steps for example 4.3. In this example there are three parents (0 1, E E E E) to N.The incremental composition steps of jE have introduced two intermediatevariables 0N and 1N, and we expect the frequency N to be reconstructed at the end of the incremental step, which is variable 1N."}, {"heading": "4.5. Experiments", "text": "This year it is so far that it will be able to erenen.n the aforementioned lcihsrcsrteeSe"}, {"heading": "4.6. Summary", "text": "This chapter has reviewed historical, popular, risk aggregation methods and compared them with a new method called Bayesian Factorization and Elimination (BFE), which utilizes a number of advances in Bayesian Networks that cover methods of approaching statistical and conditional deterministic functions and factorizing multivariate distributions for efficient computation.The aim of the BFE was to perform aggregation for classes of problems that the existing methods cannot solve (namely hybrid situations with common causes), while targeting at least as well conventional aggregation problems. Experiments show that our goals have been attained. For more difficult hybrid problems, the experimental results show that BFE offers a more general solution than is possible or convenient with previous methods. For example, BFE does not surpass the Panjers and the FFT in hybrid cases."}, {"heading": "5. Inference for High Dimensional Models", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "5.1. Conditional Gaussian DCCD Model", "text": "In practice, a BN with continuous CPDs (i.e. conditional Gaussian, continuous conditional non-Gaussian models are only applicable to Gaussian models) is easy to convert into DCCD models, since the CPDs are deterministic probability functions that are easy to modify (when edges are added) using arithmetic operations. Normally, discrete and hybrid models will present some difficulties because their CPDs are not conditionally deterministic, although it is possible to use methods to ensure that they can be factored. Let us focus on the common peculiarity in which we have a complete diagram whose nodes correspond to all Gaussian distributions. It is well known that conditional Gaussian (CG) models can be used to factorize MGDs. Thus, an MGD model can always be factored by a DCCD model for a BN structure, S. A derivative Gaussian (CG) model can be used in 1992."}, {"heading": "5.2. The Triplet Region Construction Algorithm", "text": "This section develops the Triplet Region Construction (TRC) algorithm along with the associated subsidiary algorithms introduced to date. It automatically constructs an optimized regional graph using an n-dimensional, complete BFG as input. We have already reviewed the Generalized Belief Propagation (GBP) and discussed in Chapter 3 a special approach called the Cluster Variation Method (CVM), which generates an output object called a regional graph that can be used for conclusions. In Section 5.2.1, we outline the desirable properties of this regional graph that we need to preserve in our new TRC algorithm, identifying two levels of regions that contain primary triplets in the first region, and interaction triplets in the second region. These interaction triplets are then truncated to ensure that the equilibrium and Maximum Entropy Normal property are maintained. Section 5.2.2 provides complete definitions of all algorithmic steps."}, {"heading": "5.2.1 Identifying the TRC regions", "text": "In fact, most of them are able to move to another world in which they are able to integrate, and in which they are able to integrate."}, {"heading": "5.2.2 Constructing the TRC region graph", "text": "We aim to create an acyclic two-level diagram. A valid region diagram for a 5 complete BFG model that uses only CVM can be built with all primary triplets and all interaction triplets if the three regions in which the third level contains only marginal variables that include the width of the diagram include convergence. We found that the inference in a 5 complete BFG model that uses the construction of a three-level region can achieve convergence, but that higher dimensional models are often unable to receive messages that include the width of the diagram. We found that the inference in a 5 complete BFG model with three-level diagram that can achieve convergence, but that higher dimensional models often fail when in Section 5.4 Experiment 2 we use an 8 BFG model with some strong correlation factors that have failed to report CM with three levels."}, {"heading": "10. else return 0 1C X", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11. end if", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "12. end for", "text": "13. otherwise the only parent node results 0 1 0 {,} P P C"}, {"heading": "14. end if", "text": "15. If 0C is zero, add 0 1 {,} P P and 0C to a new interaction triplet and update 0U"}, {"heading": "16. end if", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "17. end if", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "18. end for", "text": "19. first level of InitRG by triplet sets 0F and 0U20. compose second level of InitRG by intersections of regions of the first level 21. Call algorithm 4 corresponding intersection pruning22. Return TRCRGAlgorithm 5 TRC algorithm"}, {"heading": "5.2.3 Proof that TRC region graph is MaxEnt-Normal and", "text": "The exact counting of the numbers is the proof that TRCRG for an example 5, b) (b) (b) (R) (R) (R) (R) (4) (R) (R) (R) (4) (R) (R) (R) (R) (R) (R) (R) {3) (R) {3) (R) {4) (R) {3) {4) (R) (R) (R) {3) {3) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (X) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (H) (N), where N) is the number of variables, iX is the sum of entropes from all variables in the region. () (aX) (aXare the sum of all variables in the region."}, {"heading": "5.2.4 TRC complexity", "text": "Here we compare the complexity of the TRC with the junction tree algorithm. The spatial complexity for conclusions in a DCCD model using JT is, of course, exponential and is () nO m, where m is the maximum number of discrete states for each variable and n is the number of variables. What matters now is that the spatial complexity of the TRC algorithm is polynomial in contrast. From our evidence (Appenditure C) we obtain the number of region edges transferred to TRCRG, RGE, updated duration message, the absolute value of counting number plus one across all intersections. These intersections are composed of all intersections with a count of -1 and those with a count of (3) n down to -2, so that we have the number of region edges according to Equation 5.6.2 (2) [2) 2) 4] (2) jR (11), 5) nx noC, which is the complexity of the messages."}, {"heading": "5.2.5 Relationship with the Join Graph approach", "text": "The proof of this can be found in the information theory, which is less intuitive than in the justification of linking algorithms to verify the effectiveness of the TRC region. This section converts the TRC graph into an accession process. 5.3We borrow the example of BN, G, there are two interaction trios renamed for consistency."}, {"heading": "5.3. DDBP algorithm", "text": "The TRC algorithm is sufficient to draw conclusions about complete BFG models that contain discrete variables. However, to draw conclusions about high-dimensional continuous DCCD models, we must first discredit the continuous variables. We use DD to perform dynamic discretization during faith propagation and call the combined method Dynamically Discretized Belief Propagation (DDBP). DDBP can be considered a replacement for JT in the DDJT algorithm with TRC.We formalize the DBP algorithm as a show in Algorithm 6.Algorithm 6 Dynamically Discretized Belief Propagation (DDBP). Input: original BN G (BF decomposable and Dimension 4d) Output: original BN G with marginals1. Reordering all nodes in G from ancestor to descendant with new label iX 2. Conversion G into a DCG by adding all precursors from 6. to 4. by DS (X)."}, {"heading": "18. end for", "text": "19. to the convergence of the rear limit for each region by stable entropy error stop rule and low entropy error stop rule 20. Extracting marginally for each node from the relevant region and copy after G 21. Returns the GAlgorithm 6 DDBP algorithm Both DD and TRC-based belief propagation have their stop presets. The TRC stop rule is a low discrepancy rule that indicates the discrepancy of the beliefs generated from the previous and current propagation round. For example, if the old beliefs are generated by the first M updates, then the current beliefs are generated by 2M updates, with M being the number of connections between the first and second levels in the regional graph. The convergence check determines the discrepancy between the old beliefs and the current beliefs (the convergence metric is determined by monitoring the individual probability between the current persuasion BP and the other 1,0BP messages) that the TRBP does not include a discrepancy between the old messages in our current ones."}, {"heading": "5.4. Experiments", "text": "We report on seven experiments to evaluate the TRC algorithm (Experiments 1-2) and the DDBP algorithm (Experiments 3-7). The parameterization of the used BN models is listed in Appendix B. We compare our results only for the purpose of illustrating and validating our algorithm with analytical results and MCMC approximations. Other related algorithms such as EP and PBP are not compared because unscheduled implementations of these algorithms (for the types of empirical problems investigated here) are not available and would require considerable analytical and programming effort. The DDBP algorithm was written in Java and used libraries that are available on the AgenaRisk product (AgenaRisk, 2014) using Java JDK 6. This also allowed a comparison with DDJT, as this algorithm is already available in AgenaRisk."}, {"heading": "5.4.1 Experiment 1: Inference for a 5 BFG model with all", "text": "The parameter settings for the NPTs in this experiment are listed in Appendix D. Table 5.1 shows that the TRC gave good approximations compared to the exact results generated under JT. We tested higher-dimensional complete BFG models with more discrete states, rather than using binary state variables only. We also compared TRC with the Bethe method, CVM (three levels), and the basic midfield approach (with standard message sequences in the packet) using the FastInf package (Jaimovich, et al, 2014), and the TRC result is better than the results of these competing approaches."}, {"heading": "5.4.2 Experiment 2: Inference for a 8 BFG with evidence", "text": "The NPT settings are listed in Appendix B. In Figure 5.15, the variables 3X, 27E and 71E have identical NPT settings, so these variables have a very narrow marginal distribution. We compare the TRC with the JT results in Table 5.2. The maximum KL distance in Table 5.2 is 3.55828e-08, which confirms the performance of the TRC. Also, the strictly correlated variables 2X, 27E and 71E are very approximate."}, {"heading": "5.4.3 Experiment 3: Inference for 20 dimensional CG-", "text": "DCCD model with DDBPWe test DDBP with the 20-dimensional conditional Gaussian model shown in Figure 5.16. Parameter settings for the original BN model are shown in Appendix D. The results for the mean and standard deviations statistics are given in Table 5.3: Compared to the exact result, the accuracy for the mean statistics for each variable is high, and a small discrepancy (less than 1%) results from the statistics for the standard deviation (SD).The results indicate a stable approximation without degradation, despite increased dimensionality. The regional diagram generated by the CVM with three planes does not match for this model."}, {"heading": "5.4.4 Experiment 4: Inference for 10 dimensional CG-", "text": "DCCD model with observations with DDBPThis experiment tests the accuracy of DDBP when the model contains an observation. Here, we have a 10-dimensional CG-DCCD model with an observation for variable10 10X. Parameter settings for the original ten-dimensional BN model are shown in Appendix B. We compare the result with that obtained with a single chain and updates of 1.0E6, as well as with the exact solution. Table 5.4 shows that the results are very similar for both DDBP and MCMC methods.11 This is the maximum iteration setting for BP, although it can converge before the boundary."}, {"heading": "5.4.5 Experiment 5: Pair correlation test for 15 dimensional", "text": "The parameter settings for the original BN model are shown in Appendix D. The DDBP model was executed with 30 DD iterations and a maximum of 240 GBP iterations. We use combination pairs of variables 1 6 11, X X X and 15X for this experiment (there are a total of 2 15 105C combinations). Figure 5.17 shows the 15 BFG model, which corresponds to the 15-dimensional CG-DCCD model. Results are shown in Table 5.5 next to the exact correlation model. We use this weak correlation model to test the accuracy of DDBP. Note that this correlation test for efficiency considerations is based only on a discredited statistical model created by DD to approximate the joint distribution."}, {"heading": "5.4.6 Experiment 6: Inference in a linear model using", "text": "DDBPThis experiment applies DDBP conclusions to a linear model that is a sparse model, not a DCCD. Therefore, the model is first converted from a sparse model to a DCCD by adding edges and assigning zero weights. This experiment shows how to use DDBP to perform conclusions on general (originally non-DCCD) models. We use a Bayesian linear regression model (notation is taken from (Bishop, 2006))) in Figure 5.18 (a), with the model specification: (,) y t x w, where 0 1 (,)... (T D Dy w x x x w x x x x x x x x x, The error term is a zero mean Gaussian random variable with precision. Randomness is defined as 0 () 1 x. The model in Figure 5.18 (b) is defined as: 20 ~ (0, 1000) w Normal, ression coefficient."}, {"heading": "5.4.7 Experiment 7: Aggregation of inter-dependent", "text": "In fact, it is the case that we will be able to solve the problem, \"he said.\" We have not yet understood why, \"he said.\" But we are not yet able to get a grip on the problem, \"he said.\" We are not yet able to get a grip on the problem. \""}, {"heading": "5.5. Summary", "text": "This chapter has an inference that DDBP develops by combining Dynamic Discretization and Belief Propagation algorithms to draw efficient conclusions for DCCD models. It overcomes the computational limitations of discrete approximation, such as DDJT, by ensuring approximation to the desired properties (i.e. maxent-normal) in two steps."}, {"heading": "6. Conclusions and Future Work", "text": "This thesis addresses a general Bayesian framework for risk aggregation by using a family of algorithms: BFE, TRC, and DDBP. Taken together, these algorithms positively fulfill the four hypotheses set out in Chapter 1 and together provide approximate general-purpose algorithms that deliver accurate outcomes. Many popular financial methods can be adapted to our approach and can now be expanded to include causal risk factors. In terms of the BFE algorithm, current and future research focuses on more complex situations involving both copulas and common cause variables. The challenge is to break these models down into low-dimensional distributions where complexity is minimized by factoring. A final area of interest includes optimizing outcomes so that we can choose a number of measures in the model that maximize returns for minimal risk: deconvolution looks promising."}], "references": [{"title": "AgenaRisk", "author": ["AgenaRisk."], "venue": "Retrieved from http://www.agenarisk.com/ Arbenz, P., & Canestraro, D. (2012). Estimating Copulas for Insurance from Scarce Observations, Expert Opinion and Prior Information: A Bayesian Approach. ASTIN Bulletin, 42(01), 271\u2013290. doi:10.2143/AST.42.1.2160743 Arbenz, P., Embrechts, P., & Puccetti, G. (2011). The AEP algorithm for the fast", "citeRegEx": "AgenaRisk.,? 2014", "shortCiteRegEx": "AgenaRisk.", "year": 2014}, {"title": "May). variational algorithms for approximate bayesian inference. University College London, The Gatsby Computational Neuroscience Unit", "author": ["M.J. Beal"], "venue": null, "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "Challenges and Advances in High Dimensional and High Complexity Monte Carlo Computation and Theory. Presented at the BIRS workshop, Canada", "author": ["D. Ceperley", "Y. Chen", "R. Craiu", "Meng", "X.-L", "A. Mira", "J. Rosentha"], "venue": null, "citeRegEx": "Ceperley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ceperley et al\\.", "year": 2012}, {"title": "Algebra of Probable Inference", "author": ["T. R"], "venue": "With Bayesian Networks. Journal of Risk and Insurance,", "citeRegEx": "R.,? \\Q2007\\E", "shortCiteRegEx": "R.", "year": 2007}, {"title": "Mini-buckets: A General Scheme for Bounded Inference", "author": ["R. Dechter", "I. Rish"], "venue": "J. ACM,", "citeRegEx": "Dechter and Rish,? \\Q2003\\E", "shortCiteRegEx": "Dechter and Rish", "year": 2003}, {"title": "Copula Bayesian Networks", "author": ["G. Elidan"], "venue": "Embrechts, P., & Frei, M. (2009). Panjer recursion versus FFT for compound distributions. Mathematical Methods of Operations Research, 69(3), 497\u2013508. doi:10.1007/s00186-008-0249-2 Fenton, N., & Neil, M. (2010). Comparing risks of alternative medical diagnosis", "citeRegEx": "Elidan,? 2010", "shortCiteRegEx": "Elidan", "year": 2010}, {"title": "Learning Bayesian Networks With Local Structure", "author": ["N. Friedman", "M. Goldszmidt"], "venue": null, "citeRegEx": "Friedman and Goldszmidt,? \\Q1996\\E", "shortCiteRegEx": "Friedman and Goldszmidt", "year": 1996}, {"title": "The calculation of aggregate loss distributions (pp. 22\u201361)", "author": ["W.K. Hastings"], "venue": "Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika,", "citeRegEx": "Hastings,? \\Q1970\\E", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Bayesian Approach to Inverse Problems", "author": ["J. Society. Idier"], "venue": "John Wiley & Sons. Ihler, A. T., & McAllester, D. A. (2009). Particle Belief Propagation. In AISTATS (pp. 256\u2013263). IMF. (2009). IMF Global Financial Stability Report -- Navigating the Financial", "citeRegEx": "Idier,? 2010", "shortCiteRegEx": "Idier", "year": 2010}, {"title": "Bayesian updating in causal probabilistic networks by local computations", "author": ["Ahead. Washington", "A. DC. Jaimovich", "O. Meshi", "I. Mcgraw", "G. Elidan", "N. Friedman"], "venue": "Computational Statistics Quaterly,", "citeRegEx": "Washington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Washington et al\\.", "year": 2014}, {"title": "Bayesian Networks and Decision Graphs", "author": ["F.V. 269\u2013282. Jensen", "T.D. Nielsen"], "venue": null, "citeRegEx": "Jensen and Nielsen,? \\Q2009\\E", "shortCiteRegEx": "Jensen and Nielsen", "year": 2009}, {"title": "Approximation algorithms for graphical models", "author": ["K. Irvine. Kask", "R. Dechter", "J. Larrosa", "A. Dechter"], "venue": "Information and Computer Science,", "citeRegEx": "Kask et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kask et al\\.", "year": 2001}, {"title": "A Theory of Cooperative Phenomena", "author": ["R. Kikuchi"], "venue": "Physical Review,", "citeRegEx": "Kikuchi,? \\Q1951\\E", "shortCiteRegEx": "Kikuchi", "year": 1951}, {"title": "Convergent Tree-Reweighted Message Passing for Energy Minimization", "author": ["V. Kolmogorov"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10), 1568\u20131583. doi:10.1109/TPAMI.2006.200 Kozlov, A. V., & Koller, D. (1997). Nonuniform Dynamic Discretization in Hybrid Networks. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial", "citeRegEx": "Kolmogorov,? 2006", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "Loeliger", "H.-A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Fast approximate inference in hybrid Bayesian networks using dynamic discretisation", "author": ["H. Langseth", "M. Neil", "D. Marquez"], "venue": "(SSRN Scholarly Paper No. ID 1278435)", "citeRegEx": "Langseth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Langseth et al\\.", "year": 2013}, {"title": "Propagation of Probabilities, Means and Variances in Mixed Graphical Association Models", "author": ["S.L. Lauritzen"], "venue": "Journal of the American Statistical Association, 87, 1098\u20131108. Lauritzen, S. L. (1996). Graphical Models. Oxford University Press. Lauritzen, S., & Spiegelhalter, D. (1988). Local Computations with Probabilities on", "citeRegEx": "Lauritzen,? 1992", "shortCiteRegEx": "Lauritzen", "year": 1992}, {"title": "Tree approximation for belief updating", "author": ["R. Mateescu", "R. Dechter", "K. Kask"], "venue": "Proceedings of the 18th National Conference on Artificial Intelligence", "citeRegEx": "Mateescu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mateescu et al\\.", "year": 2002}, {"title": "Stan, scalable software for Bayesian modeling", "author": ["H. Matthew", "B. Carpenter", "A. Gelman"], "venue": "In Proceedings of the NIPS Workshop on Probabilistic Programming", "citeRegEx": "Matthew et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matthew et al\\.", "year": 2012}, {"title": "Convergent message passing algorithms - a unifying view (pp. 393\u2013401)", "author": ["T. Meltzer", "A. Globerson", "Y. Weiss"], "venue": "Presented at the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Meltzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meltzer et al\\.", "year": 2009}, {"title": "Expectation Propagation for Approximate Bayesian Inference", "author": ["T.P. Minka"], "venue": "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (pp. 362\u2013369)", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Using Bayesian networks to model the operational risk to information technology infrastructure in financial institutions", "author": ["M. doi:10.1109/TKDE.2011.87 Neil", "N. Fenton"], "venue": "Journal of Financial Transformation,", "citeRegEx": "Neil and Fenton,? \\Q2008\\E", "shortCiteRegEx": "Neil and Fenton", "year": 2008}, {"title": "Modelling dependable systems using hybrid Bayesian networks", "author": ["M. Neil", "M. Tailor", "D. Marquez", "N. Fenton", "P. Hearty"], "venue": "Reliability Engineering & System Safety,", "citeRegEx": "Neil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Neil et al\\.", "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann. Pearl, J. (1993). [Bayesian Analysis in Expert Systems]: Comment: Graphical Models, Causality and Intervention. Statistical Science, 8(3), 266\u2013269. Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Modelling Operational Risk Losses with Graphical Models and Copula Functions", "author": ["D. Press. Politou", "P. Giudici"], "venue": "Methodology and Computing in Applied Probability,", "citeRegEx": "Politou and Giudici,? \\Q2009\\E", "shortCiteRegEx": "Politou and Giudici", "year": 2009}, {"title": "The R Project for Statistical Computing. Retrieved 23 December 2013, from http://www.r-project.org/ Rebonato, R", "author": [], "venue": null, "citeRegEx": "136,? \\Q2013\\E", "shortCiteRegEx": "136", "year": 2013}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": "Probability propagation. Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Russell and Norvig,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "Nonparametric belief propagation", "author": ["E.B. Sudderth", "A.T. Ihler", "W.T. Freeman", "A.S. Willsky"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Sudderth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2003}, {"title": "On the Choice of Regions for Generalized Belief Propagation", "author": ["M. Welling"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Welling,? \\Q2004\\E", "shortCiteRegEx": "Welling", "year": 2004}], "referenceMentions": [{"referenceID": 23, "context": "This algorithm exploits a number of advances from the field of Bayesian Networks (BNs) (Pearl, 1988) (F.", "startOffset": 87, "endOffset": 100}, {"referenceID": 3, "context": "Risk aggregation in the presence of discrete causally connected random variables. Annals of Actuarial Science, 8 (2), pp 298-319 doi:10.1017/S1748499514000098 2. Lin, P., Neil, M., & Fenton, N. (2014). Inference for high dimensional Bayesian network models using dynamically discretized belief propagation, submitted to Information Sciences.", "startOffset": 0, "endOffset": 201}, {"referenceID": 23, "context": "However, probability is not only about numbers, but also about the structure of reasoning (Pearl, 1988), which can be used to reason causally (i.", "startOffset": 90, "endOffset": 103}, {"referenceID": 23, "context": "The complexity problem of the computation of joint probability can be overcome by exploiting conditional independence (Dawid, 1979) (Pearl, 1988).", "startOffset": 132, "endOffset": 145}, {"referenceID": 2, "context": "When designing an MCMC algorithm there is always a balance to be found between exploiting information to adjust the parameters and searching for new regions of the sample space (Ceperley et al., 2012).", "startOffset": 177, "endOffset": 200}, {"referenceID": 1, "context": "The mainstream methods include \u2018variational\u2019 approaches (Beal, 2003) and the mean field algorithm [17], a simplified variational algorithm (Jordan, et al, 1998), which involves choosing a family of distributions over the latent (as opposed to observable) variables with their own variational parameters.", "startOffset": 56, "endOffset": 68}, {"referenceID": 23, "context": "BP: Belief Propagation (BP) (Pearl, 1988) (Kschischang, Frey, & Loeliger, 2001), also known as sum-product message passing, is a message passing", "startOffset": 28, "endOffset": 41}, {"referenceID": 28, "context": "Unlike most sampling schemes, BP does not suffer from high variance and is often much more efficient (Welling, 2004).", "startOffset": 101, "endOffset": 116}, {"referenceID": 12, "context": "A more general version is called Kikuchi (Kikuchi, 1951) approximation, where variables are grouped into clusters.", "startOffset": 41, "endOffset": 56}, {"referenceID": 14, "context": "In all standard BP algorithms (Kschischang et al., 2001), messages are sent from one node to a neighbour node in a factor graph.", "startOffset": 30, "endOffset": 56}, {"referenceID": 14, "context": "Both BNs and MNs can be represented by a unifying representation called a Factor Graph (FG) (Kschischang et al., 2001).", "startOffset": 92, "endOffset": 118}, {"referenceID": 14, "context": "An FG is a particular type of graphical model with applications in Bayesian inference that enables efficient computation of marginal distributions through the sum-product algorithm 4 (Koller & Friedman, 2009) , (Kschischang et al., 2001).", "startOffset": 211, "endOffset": 237}, {"referenceID": 28, "context": "(Welling, Minka, & Teh, 2005) (Welling, 2004) (Gelfand & Welling, 2012) discuss ways to structure region graphs based on graphic topology and offer guidance based on structural information criteria, a sequential approach where new regions are added bottom-up to the region graph, and tree-robustness.", "startOffset": 30, "endOffset": 45}, {"referenceID": 28, "context": "Welling (Welling, 2004) has mentioned the impact of interaction strength by adding extra candidate regions to the", "startOffset": 8, "endOffset": 23}, {"referenceID": 0, "context": "We have used AgenaRisk (AgenaRisk, 2014), a BN package and extended it to incorporate the new BFE algorithm and carry out the experiments described in Section 4.", "startOffset": 23, "endOffset": 40}, {"referenceID": 23, "context": "Our CDF algorithm uses DDJT as the approximate inference for mixture models, and is an implementation version of cut set conditioning (Pearl, 1988).", "startOffset": 134, "endOffset": 147}, {"referenceID": 8, "context": "However, it is first necessary that the density function for S possess an inverse, and should this not exist or if the convolution algebra admits zero divisors, this, unfortunately, results in an infinite number of solutions (Idier, 2010).", "startOffset": 225, "endOffset": 238}, {"referenceID": 7, "context": "MCMC, however, may be generally applicable, such as a Metropolis-Hasting sampler (Metropolis, et al, 1953) (Hastings, 1970) or Gelman\u2019s Stan toolbox (Matthew, Carpenter, & Gelman, 2012), but a general MCMC sampler may still perform poorly on a problem without bespoke design or parameter adjustment.", "startOffset": 107, "endOffset": 123}, {"referenceID": 5, "context": "copula Bayesian Network (CBN) (Elidan, 2010), where the joint multivariate distribution can be decomposed by copula functions, and in further can be factorized into conditional forms.", "startOffset": 30, "endOffset": 44}, {"referenceID": 0, "context": "All experiments are implemented onto Bayesian software AgenaRisk (AgenaRisk, 2014).", "startOffset": 65, "endOffset": 82}, {"referenceID": 28, "context": "To identify TRC regions, our algorithm is similar to Welling\u2019s (Welling, 2004), where we partition the full-BFG into triplets to model interactions.", "startOffset": 63, "endOffset": 78}, {"referenceID": 28, "context": ", 2005) (Welling, 2004) (Gelfand & Welling, 2012) we identify three properties necessary for guaranteeing the best approximation under GBP: Property 1: Acyclic \u2013 the region graph is acyclic and contains two levels.", "startOffset": 8, "endOffset": 23}, {"referenceID": 13, "context": "Many popular message passing algorithms, such as the tree reweighted max-product algorithm (Kolmogorov, 2006) and loopy BP (Murphy et al.", "startOffset": 91, "endOffset": 109}, {"referenceID": 28, "context": "Our TRC algorithm is top down and comparable, in terms of resulting region graph structure, to bottom-up region pursuing algorithms, such as (Welling, 2004).", "startOffset": 141, "endOffset": 156}, {"referenceID": 28, "context": "The cognate intersection pruning algorithm is actually generating a local equivalent structure to that which would be produced by adding outer regions 8 to existing region graph shown in (Welling, 2004), resulting in a valid region graph.", "startOffset": 187, "endOffset": 202}, {"referenceID": 0, "context": "The DDBP algorithm was written in Java and used libraries available on the AgenaRisk product (AgenaRisk, 2014) using Java JDK 6.", "startOffset": 93, "endOffset": 110}], "year": 2015, "abstractText": "Risk aggregation is a popular method used to estimate the sum of a collection of financial assets or events, where each asset or event is modelled as a random variable. Applications, in the financial services industry, include insurance, operational risk, stress testing, and sensitivity analysis, but the problem is widely encountered in many other application domains. This thesis has contributed two algorithms to perform Bayesian risk aggregation when model exhibit hybrid dependency and high dimensional inter-dependency. The first algorithm operates on a subset of the general problem, with an emphasis on convolution problems, in the presence of continuous and discrete variables (so called hybrid models) and the second algorithm offer a universal method for general purpose inference over much wider classes of Bayesian Network models. The first algorithm is called the Bayesian Factorization and Elimination (BFE) algorithm which performs convolution on the hybrid models required to aggregate risk in the presence of causal dependencies. This algorithm exploits a number of advances from the field of Bayesian Networks, covering methods to approximate statistical and conditionally deterministic functions to factorize multivariate distributions for efficient computation. This algorithm aims to support the representation of Bayesian \u201cviews\u201d in an explicit causal dependent structure, whilst providing the computational framework for evaluating convolution models. Such causal models would involve discrete explanatory (regime switching) variables, hybrid mixtures of dependent discrete and continuous variables, and high dimensional inter-dependent continuous variables. The second algorithm developed is called Dynamic Discretized Belief Propagation (DDBP). It combines a dynamic discretization approach, to approximate continuous variables, with a new Triplet Region Construction (TRC) algorithm to perform inference on high dimensional hybrid models. The TRC algorithm is an optimized region graph approach based on graph factorization and Generalized Belief Propagation (GBP), which reduces the model complexity from exponential to polynomial. Proofs and experiments show that the algorithm converges, meets the", "creator": "Microsoft\u00ae Word 2010"}}}