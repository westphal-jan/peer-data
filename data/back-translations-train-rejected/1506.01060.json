{"id": "1506.01060", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Global and Local Structure Preserving Sparse Subspace Learning: An Iterative Approach to Unsupervised Feature Selection", "abstract": "Subspace learning is becoming more and more popular thanks to its capabilities of good interpretation. However, existing approaches do not adapt both local structure and self reconstruction very well. We propose local-structure adaptive sparse subspace learning (ASSL) model for unsupervised feature selection. In this paper, we formulate the feature selection process as a subspace learning problem and incorporate a regularization term to preserve the local structure of the data. Furthermore, we develop a greedy algorithm to establish the basic model and an iterative strategy based on an accelerated block coordinate descent is used to solve the local-structure ASSL problem. We also provide the global convergence analysis of the proposed ASSL algorithm. Extensive experiments are conducted on real-world datasets to show the superiority of the proposed approach over several state-of-the-art unsupervised feature selection approaches.", "histories": [["v1", "Tue, 2 Jun 2015 21:02:16 GMT  (377kb,D)", "http://arxiv.org/abs/1506.01060v1", "25 page, 6 figures and 51 references"], ["v2", "Mon, 19 Oct 2015 18:13:24 GMT  (411kb,D)", "http://arxiv.org/abs/1506.01060v2", "32 page, 6 figures and 60 references"]], "COMMENTS": "25 page, 6 figures and 51 references", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nan zhou", "yangyang xu", "hong cheng", "jun fang", "witold pedrycz"], "accepted": false, "id": "1506.01060"}, "pdf": {"name": "1506.01060.pdf", "metadata": {"source": "CRF", "title": "Local-Structure Adaptive Sparse Subspace Learning: An Iterative Approach to Unsupervised Feature Selection", "authors": ["Nan Zhoua", "Yangyang Xuc", "Hong Chenga", "Jun Fanga", "Witold Pedryczb"], "emails": ["nzhouuestc@gmai.com", "yangyang.xu@uwaterloo.ca", "hcheng@uestc.edu.cn", "JunFang@uestc.edu.cn", "wpedrycz@ualberta.ca"], "sections": [{"heading": null, "text": "In this paper, we formulate the process of feature selection as a sub-space learning problem and insert a regularization term to maintain the local structure of the data. In addition, we develop a greedy algorithm to establish the basic model and an iterative strategy based on an accelerated origin of block coordinates is used to solve the local ASSL problem. In addition, we offer the global convergence analysis of the proposed ASSL algorithm. Extensive experiments are conducted on real data sets to demonstrate the superiority of the proposed approach over several state-of-the-art, unattended feature selection approaches. Keywords: Machine Learning, Feature Selection, Subspace Learning, Unsupervised Learning"}, {"heading": "1. Introduction", "text": "With advances in data processing, the dimensionality of the data also increases and can be extremely high in many areas such as computer vision, machine learning and image processing. High dimensionality of the data not only increases the time and memory space required to realize data analysis, but also introduces a lot of redundancy and noise that can reduce the accuracy of the resulting method. Therefore, dimensionality reduction is an important and often necessary step in pre-processing to facilitate clustering and classification. There are two widely used dimensionality reduction approaches, namely: subspace learning and feature selection aims to learn a projection that can include the original properties in a low-dimensional subspace by forming some forms of transformation of new features. Feature selection aims to select a subset of features according to specific criteria [3]. Both approaches can successfully address the problems we mentioned at the beginning of the e-mail [4]."}, {"heading": "2. Related Studies", "text": "This year, we have reached the point where we feel we are able to live in a country where most people are able to live in a country where they are able, where they are able to move, and where they are able to move."}, {"heading": "3. The Proposed Framework of Local Structure Adaptive Sparse Subspace Learning", "text": "The first model is combinatorial in nature and only allows variables with a value of 0-1. The modeling idea is intuitive, but it is not easy to find a good approximate solution to the problem. The second model loosens the first model and becomes its continuous counterpart. Various optimization methods can be used to determine its solution. More importantly, the relaxed model can in most cases provide better practical performance than the original one; refer to the numerical results reported in Section 5."}, {"heading": "3.1. A Generic Formulation", "text": "In view of n data samples {pi} ni = 1 located in d-dimensional space, the objective of the selection of the characteristics is to find a small set of characteristics that can capture the most useful information of the data and better serve for classification or clustering. A natural way to measure the information content is to see how close the original data samples are to the learned subspace covered by the selected characteristics. Mathematically, the distance of a vector x to a subspace X \u00b7 can be regulated, where PX denotes the projection on X and vice versa is the Euclidean 2 norm. Therefore, the selection problem of the characteristics can be represented as the following characteristic x to a subspace X \u00b7 XWH \u00b7 2F s.t., where PX > K denotes the projection on X and vice versa."}, {"heading": "3.2. Local Structure Preservation Methods", "text": "The local structure of the data often contains important information that can be used to distinguish the samples [5, 7]. A predictor that uses local structural information can be much more efficient than a predictor that only uses global information [29]. Therefore, it can be assumed that the learned low-dimensional subspace could preserve the local structure of the training data. We briefly consider two widely used methods for preserving local structures."}, {"heading": "3.2.1. Local Linear Embedding", "text": "The method of Local Linear Embedding (LLE) [35] first determines the setNm (p j) of m nearest neighbor for all j and then constructs the similarity matrix S as the (normalized) solution of the following problem in S n \u2211 i = 1% pi \u2212 n \u00b2 j = 1 S i jp j \u00b2 22, s.t. S i j = 0, \u0435j < Nm (pi), \u0442 i. (3) You can consider S i j as the coefficient of the j \u2212 th sample when approaching the i \u2212 th sample, and the coefficient is zero if the j \u2212 th sample is not the neighbor of the i \u2212 th sample. After getting S from (3), LLE normalizes it further, so that you can select p = 1 S i j = 1. Then it calculates the low-dimensional representation Y = W > X > RK \u00b2 n by solving the problem W n \u00b2 pi > j = 1 > rij = jsional > W (W = 4), where W = W can be expressed by W = 4 dimensions."}, {"heading": "3.2.2. Linear Preserve Projection", "text": "In the method of linear preventive projection [36] (LPP), the similarity matrix S is generated by S i j = exp, (6) where Nm (pi) is the set of m nearest neighbours of pi. The LPP method requires the low-dimensional representation to obtain the similarity of the original data and forms the linear transformation W by solving the following optimization problem W n [i]. (7) Let L = D \u2212 S be the laplactic matrix, where D is a diagonal matrix called a degree matrix, with diagonal elements Dii = \u2211 n j = 1 S i = 1 S i j. Then (7) can be expressed as min WTr (W > X > LW) (8) accordingly."}, {"heading": "3.3. Relaxed Formulation", "text": "The problem (2) is of a combinatorial nature, and we do not have many ways to solve it directly. (3) In the next section, we develop a greedy algorithm that selects the properties of K one by one. (4) Although the greedy method can sometimes produce a satisfactory solution, it is not reliable. (4) We are looking for an alternative way to find the features of Section 3.1, any feasible solution of W, and then apply a reliable optimization method to solve the relaxed problem. (4) As observed in our tests, the relaxed method can work comparatively well with and most of the time. (5) As noted at the end of Section 3.1, any feasible solution W is not negative and then has a reliable optimization method to solve the problem. (4) If K d (which is normally satisfied), W has many zeros."}, {"heading": "4. Solving the Proposed Sparse Subspace Learning", "text": "In this section, we present heuristic algorithms for approximate solution of (2) and (11). Later on, we assume that Loc (W) performs the function either as (5) or (8) and g (W) are given by (9). Due to the combinatorial structure of (2), we propose a greedy method for its solution. Problem (11) is smooth, and various optimization methods can be applied. Although its goal is not convex in relation to W and H, it is convex in relation to one of them, while the other is fixed. On the basis of this property, we choose the method of derivation of block coordinates to solve (11)."}, {"heading": "4.1. Its Greedy Strategy", "text": "The idea is that we select one of each of the remaining unselected characteristics, so that the objective value is diminished most. (1) We start designing the algorithm by making the following observations. (1) Let I1 and I2 select two index characteristics. (2) Let I1 and I2 select two index characteristics of characteristics. (12) From the above observations, submatrices of X are indexed with columns I1 and I2 respectively. (2) Thenmin H1 - X \u2212 XI1 H1 - XI2 H2 - XI2 H2 - XI2 H2 - XF. (12) If the current index of selected characteristics I, the data matching becomes better if we add more characteristics. Below, we describe how to select such additional characteristics."}, {"heading": "4.2. Its Application on Feature Selection", "text": "In this subsection, we present an alternative method of function selection based on (11). > Using the double convexity of the target, we will use the method of accelerated block coordinate updating (BCU) proposed in [41] to solve (11). As explained in [41], BCU is particularly suitable for solving bi-convex 1 optimization problems such as (11). Compared to traditional optimization methods such as gradient descent and the inner point method, BCU can have a much lower computational complexity and also maintain rapid convergence. In addition, it has a guaranteed global sequence convergence in the solution (11). Following the framework of the BCU, our algorithm is derived by alternately updating W and H at one time while the other is fixed to its most recent value. Specifically, letf (W) is a global sequence convergence solution (11)."}, {"heading": "4.2.1. Parameter settings", "text": "With direct calculation it is not difficult to see that \"W f (W, H) = X > (XWH \u2212 X) H > + \u00b5X > LXW. For all W, W \u2212 X \u2212 X) H > \u2212\" W \"(W, H) \u2212\" W \"(W, H) -\" F \"(XW, H) -\" X > (XW, H \u2212 X) H > + \"X\" (XW, H \u2212 X) - \"F\" (XW, H) - \"X\" (XW, H \u2212 X) H > - \"X\" (XW, H) - \"X\" (XW, H) - \"X\" (XW, H) - \"L\" -. \""}, {"heading": "4.2.2. Solution of W-subproblem", "text": "Note that (17a) can be equated asmin W + 0 1 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5"}, {"heading": "4.3. Convergence analysis", "text": "In this section we analyze the convergence of the ASSL algorithm. Let us consider the convergence (W) + (W) = [0] (W) (W) = [0] (W) = [0] (W) = [0] (W) = [0] (W) + [W] (W).Then the problem (11) is equal to tomin W, H F (W, H), and the optimality condition of first order is 0 [W, H).Here F denotes the subdifference of F (see [45] for example) and is equivalent to the sequence F (W, H).According to Proposition 2.1 of [46], 0 [W, H) a convergence of first order F (W, H) is equivalent to 0 (W, H).Second-order convergence is equivalent to W (W, H)."}, {"heading": "5. Experimental Studies", "text": "In this section, the proposed ASSL and GLPSL algorithms are tested on six benchmark datasets and compared with five state-of-the-art, unattended selection methods."}, {"heading": "5.1. Datasets", "text": "The six benchmark datasets we use come from different ranges and are listed in Table 2. Yale64, WarpPIE, Orl64, and Orlraws2 are face images, with each sample of the datasets representing a face image. Usps3 is a handwritten digit record that contains 9,298 handwritten digits. Isolet3 is a voice signal record that contains 30 alphabet speech signals twice. All records are normalized so that the vector corresponding to each feature has a unit \"2 standard."}, {"heading": "5.2. Experimental Settings", "text": "Our algorithms are compared to the local discriminatory characteristics of the data and the 2.1-economical standard selection method: 1. LS: Laplacian score (LS) method uses the Laplacian score method to assess the effectiveness of the characteristics, individually selecting the characteristics that maintain the local similarity of the samples specified by a similarity matrix. [7].2http: / / featured ureselection.asu.edu / datasp 3http: / www.cad.zju.edu.edu.cn / home / dengcai / data / data.html3. UDFS: Unsupervised discriminative feature selection (UDFS) method combines the local discriminatory characteristics of the data and the 2.1-economical standard selection."}, {"heading": "5.3. Experimental results", "text": "In this section we report on the results of all tested methods. In addition, we examine the sensitivity of the parameters contained in (11)."}, {"heading": "5.3.1. Performance comparison", "text": "In Tables 3 and 4, we present the ACC and NMI values generated by different methods. For each method, we vary the number of characteristics selected between {20, 30, 40,.., 100} and give the best result. It is clear from the tables that ASSL performs best out of all the methods compared except for Yale64, Orl64 and WarpPIE in Table 3 and Yale64, Orl64 in Table 4, for each of which ASSL is the second best. Furthermore, we see that the greedy method GLPSL performs fairly well in many cases, but in some cases can be very bad, such as Usps in both tables, and this confirms our reason to relax (2) and develop the ASSL method. Finally, we see that ASSL outperforms MFFS in all data sets, possibly due to the term used in ASSL to maintain the local structure."}, {"heading": "5.3.2. Compare the performance with all features", "text": "To illustrate the impact of feature selection on clustering, we compare the cluster results using all features and selected features of different methods. Figure 1 shows the ACC value and Figure 2 the NMI value in terms of the number of features selected. The baseline corresponds to the results using all features. The figures show that the proposed ASSL method provides the best results in most cases and can provide comparable and even better cluster results by selecting relatively many features (but far less than the total number of features) than by using all features. Therefore, feature selection eliminates the redundancy of data for clustering purposes. In addition, using fewer features can save the cluster time of the K-Method and thus feature selection can improve both cluster accuracy and efficiency."}, {"heading": "5.3.3. On sensitivity of parameters", "text": "In order to further demonstrate the good performance of the proposed ASSL method, we examine its sensitivity with respect to the parameters K, \u00b5 and \u03b2 in (11). Firstly, we fix \u00b5 = 1 and vary K and \u03b2. Figures 3 and 4 show the ACC and NMI values indicated by ASSK for different K and \u03b2. The figures show that ASSL performs well stable with the exception of isolet on different combinations of K and \u03b2, and that users can select the parameters within a large interval to achieve a satisfactory cluster performance. Secondly, we fix \u03b2 = 1 and vary K and \u00b5. Figures 5 and 6 show the ACC and NMI values indicated by ASSL for different K and \u00b5."}, {"heading": "6. Conclusions", "text": "We proposed a new, unattended feature selection model that enables sparse learning in subspace while preserving the local structure of the data. It was derived by relaxing an existing combinatorial model with 0-1 variables. For the first time, a greedy algorithm was developed to solve the combinatorial problem, and an accelerated method of block coordinate descent (BCD) was applied to solve the relaxed, continuous task. We established the global convergence of the BCD method. Extensive numerical tests on real data showed that the proposed method outperformed several state-of-the-art, unattended feature selection methods."}, {"heading": "Acknowledgements", "text": "Y. Xu is partially supported by AFOSR. W. Pedrycz is partially supported by NSERC and CRC."}, {"heading": "Appendix A. Proof of Theorem 2", "text": "For simplicity, we assume that there is no extrapolation (+ > K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (Wk, Hk) - F (Wk + 1, Hk) \u2212 K \u2212 K = K \u2212 K \u2212 K (Wk) - K \u2212 K \u2212 \u2212 K \u2212 \u2212 K (Wk) - K \u2212 K \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 K (Wk) - K \u2212 K \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 K \u2212 \u2212 K - K \u2212 K \u2212 K \u2212 K (Wk) - K \u2212 K (Wk) - K \u2212 K (Wk) - K \u2212 K \u2212 \u2212 \u2212 K \u2212 \u2212 \u2212 K \u2212 \u2212 \u2212 \u2212 \u2212 K \u2212 \u2212 \u2212 \u2212 - K \u2212 K \u2212 K) - K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K (Wk) - K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K (Wk) - K (K \u2212 K \u2212 K) - K (K \u2212 K) - K (Wk) - K (Wk) - K \u2212 K (Wk) - K (Wk) - K (Wk) - K (Wk) - K (Wk) - K (Wk) - K (Wk) - K - K (Wk) - K) - K (Wk) - K (Wk) - K (Wk) - K) - K (Wk) - K (Wk) - K (Wk) - K) - K (Wk) - K (Wk) - K) - K (Wk) - K (Wk) - K) - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K - K"}, {"heading": "Appendix B. Proof of Theorem 3", "text": "For the simplicity of the notation we leave Zk = (Wk, Hk) and Z = (W, H, H). Furthermore, we assume that it is the smallest single value of XW. \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z \u2212 Z (W \u2212 Z \u2212 Z) \u2212 Z \u2212 Z (XW, XW \u2212 Z \u2212 Z) \u2212 Z (XW, XW \u2212 Z \u2212 Z) XW, XW, XW \u2212 Z, XW \u2212 Z, XW \u2212 Z, XW \u2212 Z, XW \u2212 Z, XW \u2212 Z \u2212 Z, XW \u2212 Z \u2212 Z, XW \u2212 Z \u2212 Z, XW, XW, XW, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z,"}], "references": [{"title": "Feature extraction: foundations and applications", "author": ["I. Guyon"], "venue": "Vol. 207, Springer Science & Business Media", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "A robust elastic net approach for feature learning", "author": ["L. Wang", "H. Cheng", "Z. Liu", "C. Zhu"], "venue": "Journal of Visual Communication and Image Representation 25 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research 3 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Joint feature selection and subspace learning", "author": ["Q. Gu", "Z. Li", "J. Han"], "venue": "in: IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Vol. 22", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiley & Sons", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Neighborhood preserving embedding", "author": ["X. He", "D. Cai", "S. Yan", "H.-J. Zhang"], "venue": "in: Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, Vol. 2, IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition", "author": ["J. Lu", "K.N. Plataniotis", "A.N. Venetsanopoulos"], "venue": "Pattern Recognition Letters 26 (2) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Sparsity preserving projections with applications to face recognition", "author": ["L. Qiao", "S. Chen", "X. Tan"], "venue": "Pattern Recognition 43 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Thirteen ways to look at the correlation coefficient", "author": ["J. Lee Rodgers", "W.A. Nicewander"], "venue": "The American Statistician 42 (1) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1988}, {"title": "Feature selection based on mutual information criteria of max-dependency", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "maxrelevance, and min-redundancy, Pattern Analysis and Machine Intelligence, IEEE Transactions on 27 (8) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised feature selection using feature similarity", "author": ["P. Mitra", "C. Murthy", "S.K. Pal"], "venue": "IEEE transactions on pattern analysis and machine intelligence 24 (3) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "l2", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "1-norm regularized discriminative feature selection for unsupervised learning, in: IJCAI Proceedings-International Joint Conference on Artificial Intelligence, Vol. 22, Citeseer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "H", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou"], "venue": "Lu, Unsupervised feature selection using nonnegative spectral analysis., in: AAAI", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Global and local structure preservation for feature selection", "author": ["X. Liu", "L. Wang", "J. Zhang", "J. Yin", "H. Liu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on 25 (6) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "On similarity preserving feature selection", "author": ["Z. Zhao", "L. Wang", "H. Liu", "J. Ye"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 25 (3) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparsity induced similarity measure for label propagation", "author": ["H. Cheng", "Z. Liu", "J. Yang"], "venue": "in: Computer Vision, 2009 IEEE 12th International Conference on, IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Pattern-coupled sparse bayesian learning for recovery of block-sparse signals", "author": ["J. Fang", "Y. Shen", "H. Li", "P. Wang"], "venue": "IEEE Trans. Signal Processing 63 (2) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-task feature learning", "author": ["A. Evgeniou", "M. Pontil"], "venue": "Advances in neural information processing systems 19 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Visual classification with multitask joint sparse representation", "author": ["X.-T. Yuan", "X. Liu", "S. Yan"], "venue": "Image Processing, IEEE Transactions on 21 (10) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised feature selection by regularized self-representation", "author": ["P. Zhu", "W. Zuo", "L. Zhang", "Q. Hu", "S.C. Shiu"], "venue": "Pattern Recognition 48 (2) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "X", "author": ["Y. Yang", "H.T. Shen", "F. Nie", "R. Ji"], "venue": "Zhou, Nonnegative spectral clustering with discriminative regularization., in: AAAI", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust unsupervised feature selection", "author": ["M. Qian", "C. Zhai"], "venue": "in: Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, AAAI Press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear subspace learning-based dimensionality reduction", "author": ["X. Jiang"], "venue": "Signal Processing Magazine, IEEE 28 (2) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Local learning algorithms", "author": ["L. Bottou", "V. Vapnik"], "venue": "Neural computation 4 (6) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1992}, {"title": "Spectral regression for efficient regularized subspace learning", "author": ["D. Cai", "X. He", "J. Han"], "venue": "in: Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics 15 (2) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral bounds for sparse pca: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral regression: A unified approach for sparse subspace learning", "author": ["D. Cai", "X. He", "J. Han"], "venue": "in: Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on, IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Subspace learning for unsupervised feature selection via matrix factorization", "author": ["S. Wang", "W. Pedrycz", "Q. Zhu", "W. Zhu"], "venue": "Pattern Recognition 48 (1) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290 (5500) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2000}, {"title": "Locality preserving projections", "author": ["X. Niyogi"], "venue": "in: Neural information processing systems, Vol. 16, MIT", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68 (1) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Variable selection for the multicategory svm via adaptive sup-norm regularization", "author": ["H.H. Zhang", "Y. Liu", "Y. Wu", "J. Zhu"], "venue": "Electronic Journal of Statistics 2 ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "Information Theory, IEEE Transactions on 53 (12) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM Journal on imaging sciences 6 (3) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences 2 (1) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in optimization 1 (3) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "Variational analysis", "author": ["R.T. Rockafellar", "R.J.-B. Wets"], "venue": "Vol. 317, Springer Science & Business Media", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Proximal alternating minimization and projection methods for nonconvex problems: an approach based on the kurdyka-lojasiewicz inequality", "author": ["H. Attouch", "J. Bolte", "P. Redont", "A. Soubeyran"], "venue": "Mathematics of Operations Research 35 (2) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithm as 136: A k-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied statistics ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1979}, {"title": "Matching theory", "author": ["L. Lov\u00e1sz", "M. Plummer"], "venue": "Vol. 367, American Mathematical Soc.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Entropy and information theory", "author": ["R.M. Gray"], "venue": "Springer Science & Business Media", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "The lojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems", "author": ["J. Bolte", "A. Daniilidis", "A. Lewis"], "venue": "SIAM Journal on Optimization 17 (4) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Subspace learning aims to learn a projection which can map the original features into a lower-dimensional subspace by some transformation forming new features [1, 2].", "startOffset": 159, "endOffset": 165}, {"referenceID": 1, "context": "Subspace learning aims to learn a projection which can map the original features into a lower-dimensional subspace by some transformation forming new features [1, 2].", "startOffset": 159, "endOffset": 165}, {"referenceID": 2, "context": "Feature selection aims to select a subset of the features following a certain criterion [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "Both the two approaches can successfully address the problems we mentioned at the beginning of the discussion [4, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 4, "context": "Both the two approaches can successfully address the problems we mentioned at the beginning of the discussion [4, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 5, "context": "The most classic subspace learning method is Principal Component Analysis (PCA) [6] which considers the global structures of the data.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 102, "endOffset": 105}, {"referenceID": 8, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 142, "endOffset": 145}, {"referenceID": 9, "context": "Some popular ones include Locality Preserving Projection (LPP) [7], Neighborhood Preserving Embedding [8], Linear Discriminant Analysis (LDA) [9] and Sparsity Preserving Projections [10].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "Commonly used supervised feature selection methods include Fisher score [11], Pearson correlation coefficient [12], and mutual information [13].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "Commonly used supervised feature selection methods include Fisher score [11], Pearson correlation coefficient [12], and mutual information [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 83, "endOffset": 90}, {"referenceID": 6, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 108, "endOffset": 116}, {"referenceID": 14, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 108, "endOffset": 116}, {"referenceID": 15, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 139, "endOffset": 147}, {"referenceID": 16, "context": "The intrinsic structure information that is often used includes samples similarity [14, 7], local structure [15, 16], and global structure [17, 18].", "startOffset": 139, "endOffset": 147}, {"referenceID": 17, "context": "In recent years, sparsity regularized methods have been widely used in many fields such as computer vision [19, 20], image processing [21], signal recovery [22] and so on.", "startOffset": 107, "endOffset": 115}, {"referenceID": 18, "context": "In recent years, sparsity regularized methods have been widely used in many fields such as computer vision [19, 20], image processing [21], signal recovery [22] and so on.", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 134, "endOffset": 146}, {"referenceID": 22, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 134, "endOffset": 146}, {"referenceID": 23, "context": "Group sparsity which is used in multi-task learning [23] and joint representation [24] is also widely used in feature selection tasks [25, 26, 27].", "startOffset": 134, "endOffset": 146}, {"referenceID": 5, "context": "The well-known subspace learning method is PCA [6], which maximizes the global data structure information in the principal space and hence it is optimal for data reconstruction.", "startOffset": 47, "endOffset": 50}, {"referenceID": 24, "context": "Data reconstruction capturing the discriminative information plays a crucial role in pattern recognition [28].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "Local structure always contains important discriminative information [29].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Some popular ones include Linear Discriminant Analysis (LDA) [6], Locality Preserving Projection (LPP) [7] and Neighborhood Preserving Embedding (NPE) [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Some popular ones include Linear Discriminant Analysis (LDA) [6], Locality Preserving Projection (LPP) [7] and Neighborhood Preserving Embedding (NPE) [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Some popular ones include Linear Discriminant Analysis (LDA) [6], Locality Preserving Projection (LPP) [7] and Neighborhood Preserving Embedding (NPE) [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 26, "context": "[30] proposed a two-step Spectral Regression (SR) method to transform the eigen-decomposition problem into two steps regression problem which is very efficient and flexible to add the regularization term in the regression step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[31] proposed an elegant sparse PCA method (SPCA), which transforms the traditional PCA problem into regression problem and uses the \u201cElastic Net\u201d framework to solve `1 regularization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proposed a robust elastic net model (REN) which adds the M-estimator in the least square term of the SPCA model and improves the robustness of the SPCA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[32] proposed a spectral bounds framework for sparse subspace learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[33] proposed a unified sparse subspace learning method 3", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] combined the sparse subspace learning with feature selection and proposed the Multi-Cluster Feature Selection (MCFS) method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "in [4], `1 term is not quite suitable for feature selection, and they used the `2,1 regularization term in the regression step of the subspace learning process to control the weight matrix and improve the feature selection result.", "startOffset": 3, "endOffset": 6}, {"referenceID": 30, "context": "[34] proposed an unsupervised feature selection framework, which used the global reconstruction information in subspace learning and used the orthogonality to constrain the weight matrix for a specific feature selection task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In practical application, the local structure often contains essential discriminative information as demonstrated in [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "Instead of enforcing the feature weight matrix to be orthogonal, our model uses a regularization term to encourage row group sparsity, which is more reasonable [4] and in addition, it includes a local structure preserving term to adapt the local structure of the data.", "startOffset": 160, "endOffset": 163}, {"referenceID": 30, "context": "The recent work [34] mentions to use the 0-1 feature selection matrix, but it does not explicitly formulate an optimization model like the (1).", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "As shown in [29], a local structure of the data can often include important discriminative information to distinguish different samples.", "startOffset": 12, "endOffset": 16}, {"referenceID": 4, "context": "Local structure of the data often contain important information that can be used to distinguish the samples [5, 7].", "startOffset": 108, "endOffset": 114}, {"referenceID": 6, "context": "Local structure of the data often contain important information that can be used to distinguish the samples [5, 7].", "startOffset": 108, "endOffset": 114}, {"referenceID": 25, "context": "A predictor utilizing local structure information can be much more efficient than that only using global information [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "Local Linear Embedding The Local Linear Embedding (LLE) [35] method first finds the setNm(p j) of m nearest neighbors for all j and then constructs the similarity matrix S as the (normalized) solution of the following problem", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "Linear Preserve Projection In Linear Preserve Projection [36] (LPP) method, the similarity matrix S is generated by", "startOffset": 57, "endOffset": 61}, {"referenceID": 33, "context": "One choice of g(W) is group Lasso [37], i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 34, "context": "Some other alternatives of g can also be used for promoting row-sparsity, such as group infinity norm used in [38, 39] for multi-class support vector machine.", "startOffset": 110, "endOffset": 118}, {"referenceID": 30, "context": "Our model is similar to the Matrix Factorization Feature Selection (MFFS) model proposed in [34].", "startOffset": 92, "endOffset": 96}, {"referenceID": 35, "context": "As shown in [40], if Cor(xi, X) is large, then the columns of X can be better linearly represented by xi.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "Utilizing bi-convexity of the objective, we employ the accelerated block coordinate update (BCU) method proposed in [41] to solve (11).", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "As explained in [41], BCU especially fits to solving bi-convex1 optimization problems like (11).", "startOffset": 16, "endOffset": 20}, {"referenceID": 36, "context": "1More precisely, in [41], BCU is proposed to solve multi-convex optimization problems, which includes bi-convex problems as special cases.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "is an extrapolated point with weight \u03c9k \u2208 [0, 1], \u2200k.", "startOffset": 42, "endOffset": 48}, {"referenceID": 36, "context": "As suggested by [41], we set the extrapolation weight as", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": "[42]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Without nonnegativity constraint on xI, the minimizer of (23) is given by item 2 of Theorem 1 (for example, see [44]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 39, "context": "Here, \u2202F denotes the subdifferential of F (see [45] for example) and equals \u2207F if F is differentiable and a set otherwise.", "startOffset": 47, "endOffset": 51}, {"referenceID": 40, "context": "1 of [46], 0 \u2208 \u2202F(W,H) is equivalent to 0 \u2208 \u2202W F(W,H), and 0 = \u2207H F(W,H)", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "MCFS: Multi-cluster feature selection (MCFS) is two-step method, and it formulates the feature selection process as a spectral information regression problem with `1-norm regularization term [5].", "startOffset": 191, "endOffset": 194}, {"referenceID": 13, "context": "UDFS: Unsupervised discriminative feature selection (UDFS) method combines the data\u2019s local discriminative property and the `2,1-norm sparse constraint in one convex model to select the features which have the highest power of local discriminative property [15].", "startOffset": 257, "endOffset": 261}, {"referenceID": 21, "context": "RSR: Regularized self-representation (RSR) feature selection method uses the `2,1-norm to measure the fitting error and also `2,1-norm to promote sparsity [25].", "startOffset": 155, "endOffset": 159}, {"referenceID": 30, "context": "MFFS: Matrix factorization feature selection (MFFS) method [34] is similar to ours.", "startOffset": 59, "endOffset": 63}, {"referenceID": 41, "context": "After completing the feature selection process, we use the K-means algorithm [47] to cluster the samples using the selected features.", "startOffset": 77, "endOffset": 81}, {"referenceID": 42, "context": "We use the Kuhn-Munkres algorithm [48] to realize such a mapping.", "startOffset": 34, "endOffset": 38}, {"referenceID": 43, "context": "where I(P,Q) is the mutual information of P and Q, H(P) and H(Q) are the entropies of P and Q [49].", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "0 is more complicated but can be treated similarly with more care taken to handle details; see [41] for example.", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "1 of [41])", "startOffset": 5, "endOffset": 9}, {"referenceID": 44, "context": "[51]): in a neighborhood B(Z\u0304, \u03c1), there exists \u03c6(s) = cs1\u2212\u03b8 for some c > 0 and 0 \u2264 \u03b8 < 1 such that", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Subspace learning is becoming more and more popular thanks to its capabilities of good interpretation. However, existing approaches do not adapt both local structure and self reconstruction very well. We propose local-structure adaptive sparse subspace learning (ASSL) model for unsupervised feature selection. In this paper, we formulate the feature selection process as a subspace learning problem and incorporate a regularization term to preserve the local structure of the data. Furthermore, we develop a greedy algorithm to establish the basic model and an iterative strategy based on an accelerated block coordinate descent is used to solve the local-structure ASSL problem. We also provide the global convergence analysis of the proposed ASSL algorithm. Extensive experiments are conducted on real-world datasets to show the superiority of the proposed approach over several state-of-the-art unsupervised feature selection approaches.", "creator": "LaTeX with hyperref package"}}}