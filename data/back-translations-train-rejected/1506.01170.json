{"id": "1506.01170", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent Systems", "abstract": "The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.", "histories": [["v1", "Wed, 3 Jun 2015 09:12:58 GMT  (109kb,D)", "http://arxiv.org/abs/1506.01170v1", "Technical Report, The University of Edinburgh, 2013"]], "COMMENTS": "Technical Report, The University of Edinburgh, 2013", "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.MA", "authors": ["stefano v albrecht", "subramanian ramamoorthy"], "accepted": false, "id": "1506.01170"}, "pdf": {"name": "1506.01170.pdf", "metadata": {"source": "CRF", "title": "A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent Systems", "authors": ["Stefano V. Albrecht"], "emails": ["s.v.albrecht@sms.ed.ac.uk", "s.ramamoorthy@ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "(It is.) (It is.) (it is.) (it is.) (it is.) (it is.) (it is.) (it is.) (it is.) (it is.) (it is.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it.) (it. (it.) (it.) (it.) (it. (it.) (it.) (it. (it.) (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it.) (it. (it.) (it. (it.) (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it.) (it. (it. (it.) (it. (it.) (it.) (it. (it. (it.) (it. (it.) (it. (it.) (it. (it. (it. (it.) (it. (it.) (it. (it.) (it. (it. (it.) (it. (it. (it.) (it. (it.) (it. (it.) (it.) (it. (it.) (it. (it.) (it. (it. (it. (it.) is. (it. (it.). (it.). (it. (it.). (it. (it.). (it. (it. (it.)"}, {"heading": "2 Defining Ad Hoc Coordination", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Stochastic Bayesian Games", "text": "As discussed earlier, ad hoc coordination can be defined on the basis of the concept of private information in Bayesian games. However, in its original form [Harsanyi, 1967], Bayesian games are not descriptive enough to allow us to identify the types of problems we are interested in as they do not involve states or time. Therefore, we combine Bayesian games with the concept of stochastic games [Shapley, 1953] to obtain a more descriptive model that we call stochastic strategies Bayesian game: 1. A stochastic Bayesian game (SBG) consists of: \u2022 Discrete state space S with initial state space S and terminal states S-S1A related models are I-POMDP, in which agents have incomplete information regarding the state of the world and the behavior of other agents. [Gmytrasiewicz and Doshi, 2005] However, I-PODP are extremely complex and unworkable to solve most of their problems."}, {"heading": "2.2 Flexibility & Efficiency", "text": "Two important aspects of ad hoc coordination are either flexibility or efficiency. We now define each of these factors formally within the SBG model. (The definitions are based on the notion of paths and probabilities of paths: definition 4. A path \u03c1 in SBG is a sequence < s0\u03c1, \u03b80\u03c1, a0\u03c1, s1\u03c1, a1\u03c1,..., s t\u03c1 >, where the path between both is defined as Pr. (...) A path is terminated if it is not flexible. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.... (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...).). (...). (.). (...).). (.). (.).). (.).). (.). (.).). (.). (.).). (.). (.).).).). (. (.).). (.).). (.). (.). (.).).). (. (.).). (. (.).).).). (.). (.). (.). (.).). (.). (.). (.). (.).).).). (.). (.). (.). (.).). (.). (.). (.). (.).).). (.).).). (.).)."}, {"heading": "2.3 The Ad Hoc Coordination Problem", "text": "We are now in a position to formally define the ad hoc coordination problem. The core aspect is that there is no prior coordination between the ad hoc agent and the algorithm 1 evaluation procedure. Input: SBG, set of type distributions D, ad hoc agent \u03b1, player i (controlled by \u03b1) Output: Flexibility F (\u03b1 | \u03b1, D), Efficiency E (\u03b1, D) F \u2190 0 E \u2190 0 Repeat K-times: Randomly drawn type distributions. D Generates paths. Instance. Instance with. (\u03b1 controls. i) Terminates it doF. + 1 E + (.)."}, {"heading": "3 Harsanyi-Bellman Ad Hoc Coordination", "text": "The problem of incomplete information is solved in Bayesian games by assuming that the type of spaces and the way of distribution are common knowledge. (This allows a solution in the form of the Bayesian Nash balance [Harsanyi, 1968], here defined for SBGs: Definition 7. Let Ht be the story at the time t and define the type of player you play. (It is a strategy profile), which is difficult for all i-N players and players you play. (It is not the type of player you play.) The type of player you play. (It is not the type of player you play.)...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.1 Temporally Reweighted Posteriors", "text": "A potential problem with the posterior defined in (2) is that it assigns a zero probability for a type of event, and if Pr (\u03b8j | Ht) = 0 for a type of event that is not currently the true type of player j, then this can be problematic for the following reasons: If the game uses a dynamic or mixed type distribution, and if the type of player j changes rapidly to a certain type of event, and if we have a custom type of event that approximates the true type of player j in a subset of player j, then Pr (\u03b8j | H\u03c4) = 0 can change for all times by player j > t, even if the type of player j changes with respect to the event of significance. Furthermore, if we have a custom type that approximates the actual type of player j in a subset of player J."}, {"heading": "3.2 Conceptual Types", "text": "If the custom type space for player j does not include the true type space < j > several behaviors that are directly applicable, then j might assume a type that is unknown to HBA, causing its expected payouts to be inaccurate. In such cases, it would be useful if HBA were able to learn new types from experience, opening up the possibility for us to use methods of opposing modeling (e.g. case-based thinking [Wendler and Bach, 2004] or recursive modeling [Gmytrasiewicz and Durfee, 2000]) that can be incorporated into the respective types. In this work, we use a combination of case-based thinking and fictional game [Brown, 1951] which is referred to as conceptual types. Conceptual types are based on the observation that behavior may not be specified on a state-by-state basis, but rather on abstractions of state spaces. (An example are the \"information sets\" in extensive formulas) This means that some kind of conceptualization can give."}, {"heading": "4 Simulated Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setup", "text": "In fact, most of them are able to survive themselves and to survive. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of us are able to survive ourselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) (...) () () () () () () () () () ()) () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () (() () () (() () () () () () (() () () (() () () (() ((() () (() () () (() ((() (() () () (((() () (() (() () ((() () () () () () (() (() (() (((() () () (() () (((() () () ((() () ((((() () ((("}, {"heading": "4.2 Results", "text": "We tested the effectiveness of the TR posteriors by simulating the two situations described in Section 3.1. All tests were performed on an 8 \u00d7 8 grid of 2 players and 5 foods. In Figure 2a, we used the two situations described in Section 3.1. (1) In Figure 2b, we used the same type of players 2 after 10 to 20 time steps each. (2) In both cases, the effectiveness of the HBA was significantly higher when we used a TR posterior with general time weight (Gtw) compared to the two normal posteriors defined in (2) (Unl) and a normal posterior limited to the 9 most recent events (Lim), which was applied the same time frame."}, {"heading": "5 Human-Machine Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "We conducted a large-scale human machine experiment at the Royal Society Summer Science Exhibition 2012 (C = 3 C = 3 C = 3 C = 3 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 S = 1 D = 1 S = 1 D = 1 S = 1 D = 1 S / S = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1 D = 1"}, {"heading": "5.2 Results", "text": "Below, all significance statements are based on paired t-tests with 5% significance level. Numbers 3a and 3b show the results for PD and RPS, respectively. In both games, the average total payouts of HBA and C / JAL are statistically rather equivalent. Since the time has been set at 20 rounds, this means that they have achieved equal efficiency. This is indeed a positive result, considering that C / JAL are strong candidates in PD / RPS. Furthermore, as we discuss below, HBA behaves very differently from C / JAL, with beneficial side effects, it is the most desirable long-term outcome (C, C), as it is optimal both welfare and fairness, and since it is a non-myopic equilibrium [Brams, 1993], which means that no player has a long-term incentive to deviate."}, {"heading": "6 Summary & Open Questions", "text": "This work concerns the ad hoc coordination problem, where the goal is to design an autonomous agent (the ad hoc agent) that can achieve optimal flexibility and efficiency in a multi-agent system where the behavior of the other agents is not a priori known. We make three important contributions to the ad hoc coordination problem: 1. We propose a game theory model that captures the notion of private information in the form of types. 2. We provide formally precise definitions of flexibility, efficiency and the ad hoc coordination problem. 3. We offer a method that can be used to estimate the flexibility and efficiency of the ad hoc agent. 4. We derive a principled solution that uses a number of user-defined types in a planning process to find optimal measures in the sense of Bayesian Nash Equilibrium."}, {"heading": "Acknowledgements", "text": "This work was supported in part by grants from the UK Engineering and Physical Sciences Research Council (EP / H012338 / 1), the European Commission (TOMSY Grant 270436, FP7-ICT-2009.2.1 Call 6) and a Royal Academy of Engineering Ingenious."}], "references": [{"title": "Leading ad hoc agents in joint action settings with multiple teammates", "author": ["Agmon", "Stone", "N. 2012] Agmon", "P. Stone"], "venue": null, "citeRegEx": "Agmon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agmon et al\\.", "year": 2012}, {"title": "Comparative evaluation of MAL algorithms in a diverse set of ad hoc team problems", "author": ["Albrecht", "Ramamoorthy", "S. 2012] Albrecht", "S. Ramamoorthy"], "venue": "In 11th International Conference on Autonomous Agents and Multiagent Systems", "citeRegEx": "Albrecht et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Albrecht et al\\.", "year": 2012}, {"title": "Reaching pareto-optimality in prisoner\u2019s dilemma using conditional joint action learning", "author": ["Banerjee", "Sen", "D. 2007] Banerjee", "S. Sen"], "venue": "Autonomous Agents and Multiagent Systems,", "citeRegEx": "Banerjee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2007}, {"title": "Empirical evaluation of ad hoc teamwork in the pursuit domain", "author": ["Barrett et al", "S. 2011] Barrett", "P. Stone", "S. Kraus"], "venue": "In 10th International Conference on Autonomous Agents and Multiagent Systems", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Coordination and adaptation in impromptu teams", "author": ["Bowling", "McCracken", "M. 2005] Bowling", "P. McCracken"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Bowling et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bowling et al\\.", "year": 2005}, {"title": "Multiagent learning using a variable learning rate", "author": ["Bowling", "Veloso", "M. 2002] Bowling", "M. Veloso"], "venue": "Artificial Intelligence,", "citeRegEx": "Bowling et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bowling et al\\.", "year": 2002}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["Claus", "Boutilier", "C. 1998] Claus", "C. Boutilier"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Claus et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Claus et al\\.", "year": 1998}, {"title": "Learning to play", "author": ["Dekel et al", "E. 2004] Dekel", "D. Fudenberg", "D. Levine"], "venue": "Bayesian games. Games and Economic Behavior,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Dynamically formed human-robot teams performing coordinated tasks", "author": ["Dias et al", "M. 2006] Dias", "T. Harris", "B. Browning", "E. Jones", "B. Argall", "M. Veloso", "A. Stentz", "A. Rudnicky"], "venue": "In AAAI Spring Symposium \u201cTo Boldly Go Where No Human-Robot", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Perfect Bayesian equilibrium and sequential equilibrium", "author": ["Fudenberg", "Tirole", "D. 1991] Fudenberg", "J. Tirole"], "venue": "Journal of Economic Theory,", "citeRegEx": "Fudenberg et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Fudenberg et al\\.", "year": 1991}, {"title": "A framework for sequential planning in multiagent settings", "author": ["Gmytrasiewicz", "Doshi", "P. 2005] Gmytrasiewicz", "P. Doshi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gmytrasiewicz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gmytrasiewicz et al\\.", "year": 2005}, {"title": "Rational coordination in multi-agent environments", "author": ["Gmytrasiewicz", "Durfee", "P. 2000] Gmytrasiewicz", "E. Durfee"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Gmytrasiewicz et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gmytrasiewicz et al\\.", "year": 2000}, {"title": "Rational learning leads to Nash equilibrium", "author": ["Kalai", "Lehrer", "E. 1993] Kalai", "E. Lehrer"], "venue": null, "citeRegEx": "Kalai et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 1993}, {"title": "A sparse sampling algorithm for nearoptimal planning in large Markov decision processes", "author": ["Kearns et al", "M. 1999] Kearns", "Y. Mansour", "A. Ng"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Ad hoc autonomous agent teams: Collaboration without pre-coordination", "author": ["Stone et al", "P. 2010a] Stone", "G. Kaminka", "S. Kraus", "J. Rosenschein"], "venue": "In 24th AAAI Conference on Artificial Intelligence", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Leading a best-response teammate in an ad hoc team. In Agent-Mediated Electronic Commerce: Designing Trading Strategies and Mechanisms for Electronic Markets, pages", "author": ["Stone et al", "P. 2010b] Stone", "G. Kaminka", "J. Rosenschein"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "To teach or not to teach? Decision making", "author": ["Stone", "Kraus", "P. 2010] Stone", "S. Kraus"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Stone et al\\.", "year": 2010}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Barto", "R. 1998] Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Recognizing and predicting agent behavior with case based reasoning", "author": ["Wendler", "Bach", "J. 2004] Wendler", "J. Bach"], "venue": "RoboCup 2003: Robot Soccer World Cup VII,", "citeRegEx": "Wendler et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wendler et al\\.", "year": 2004}], "referenceMentions": [], "year": 2015, "abstractText": "The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner\u2019s Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.", "creator": "LaTeX with hyperref package"}}}