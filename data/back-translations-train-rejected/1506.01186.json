{"id": "1506.01186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Cyclical Learning Rates for Training Neural Networks", "abstract": "It is known that the learning rate is the most important hyper-parameter to tune for training deep convolutional neural networks (i.e., a \"guessing game\"). This report describes a new method for setting the learning rate, named cyclical learning rates, that eliminates the need to experimentally find the best values and schedule for the learning rates. Instead of setting the learning rate to fixed values, this method lets the learning rate cyclically vary within reasonable boundary values. This report shows that training with cyclical learning rates achieves near optimal classification accuracy without tuning and often in many fewer iterations. This report also describes a simple way to estimate \"reasonable bounds\" - by linearly increasing the learning rate in one training run of the network for only a few epochs. In addition, cyclical learning rates are demonstrated on training with the CIFAR-10 dataset and the AlexNet and GoogLeNet architectures on the ImageNet dataset. These methods are practical tools for everyone who trains convolutional neural networks.", "histories": [["v1", "Wed, 3 Jun 2015 09:54:31 GMT  (726kb,D)", "http://arxiv.org/abs/1506.01186v1", null], ["v2", "Fri, 5 Jun 2015 20:40:18 GMT  (726kb,D)", "http://arxiv.org/abs/1506.01186v2", "11 pages, 12 figures, 4 tables"], ["v3", "Wed, 26 Oct 2016 19:07:58 GMT  (2002kb,D)", "http://arxiv.org/abs/1506.01186v3", "This is an updated version of this paper, along with a change in title, and was submitted to WACV 2017, Submitted to WACV 2017"], ["v4", "Thu, 29 Dec 2016 15:20:01 GMT  (1189kb,D)", "http://arxiv.org/abs/1506.01186v4", "This is an updated version of this paper and was accepted to WACV 2017"], ["v5", "Thu, 23 Mar 2017 11:38:19 GMT  (2002kb,D)", "http://arxiv.org/abs/1506.01186v5", "Presented at WACV 2017; added instructions to implement CLR in Caffe"], ["v6", "Tue, 4 Apr 2017 11:34:46 GMT  (1210kb,D)", "http://arxiv.org/abs/1506.01186v6", "Presented at WACV 2017; seethis https URLfor instructions to implement CLR in Keras"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["leslie n smith"], "accepted": false, "id": "1506.01186"}, "pdf": {"name": "1506.01186.pdf", "metadata": {"source": "CRF", "title": "No More Pesky Learning Rate Guessing Games", "authors": ["Leslie N. Smith"], "emails": ["leslie.smith@nrl.navy.mil"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to change, in which they are able to change, in which they are able to change the world, in which they are able to change the world, in which they are able to change,"}, {"heading": "2 Related work", "text": "This year, it has come to the point where it will be able to take the lead at a time when it is not as far as it has been in the past, when it has never come so far."}, {"heading": "3 Optimal Learning Rates", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tuning learning rates", "text": "The difficulty of manually adjusting learning rates can be illustrated with the CIFAR-10 datasets using the Caffe command file [11], an open source implementation of Convolutionary Neural Networks. Caffe offers a CIFAR-10 tutorial along with the architecture and hyperparameter files. If you run the \"train full.sh\" command file, which uses the provided fixed learning rate of 0.001, you get a curve like the blue curve in Figure 1. The model converges quickly in the first 10,000 iterations, but convergence slows down considerably with the iterations of 10,000-60,000. However, you cannot stop training and not drop the learning rate in the 10,000 iteration because the network is still improving and some accuracy is lost."}, {"heading": "3.2 Cyclical Learning Rates", "text": "This observation leads to the idea that the learning rate varies within a range of values, rather than assuming a fixed or exponentially decreasing value (another common learning rate policy); that is, reasonable minimum and maximum limits are set, and the learning rate varies cyclically between these values. Experiments with similarly numerous functional forms, such as a triangular window (linear), a Which window (parabolic), and a Hannsfenster (sinusoidal) all produced equivalent results lead to adopting a triangular window (linear increase then decrease) as the simplest function incorporating this idea is illustrated in Figure 2. The rest of this paper refers to the triangular learning rate policy.An intuitive understanding of why CLR methods work comes from looking at the loss function."}, {"heading": "3.2.2 How can one estimate reasonable minimum and maximum boundary values?", "text": "This facilitates the need to find these limits experimentally. Do this by running your model for 4 \u2212 8 epochs (which are typically iterations of high convergence) while letting the learning rate vary from a relatively low to a relatively high value. If you have experience with your own data and models, you typically have an idea of the approximate range that will be appropriate (and if not, you can repeat this exercise twice; once for low values such as 0.0001 \u2212 0.01 and again for higher values such as 0.01 \u2212 0.1). Triangular learning rate policy provides a mechanism to do this (and if not, you can repeat this exercise twice; once for low values such as 0.0001 \u2212 0.01 and again for higher values such as 0.01 \u2212 0.1)."}, {"heading": "4 Results", "text": "The following sections apply CLR guidelines for training with the CIFAR 10 dataset and the AlexNet [13] and GoogleNet [18] architectures with the ImageNet dataset. These three cases cover a range of data and architecture sizes, and the subsections attempt to answer several questions, some of which are: 1. How are the CLR guidelines compared to using the original fixed or exp learning rate guidelines? 2. Are the above guidelines valid for the hyperparameters step size, max lr, base lr and gamma? 3. Is it better to use a triangle or exit range policy? All of the following experiments were conducted with Caffe using Cuda 7.0 and Nvidia's CuDNN. CIFAR 10 and AlexNet experiments ran on Titan GPUs with 6GB of memory, while the GoogLeNet architecture was used on K40 GB with 12GB."}, {"heading": "4.1 CIFAR-10", "text": "iii. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i. i."}, {"heading": "4.2 AlexNet", "text": "It is not as if it were a kind of learning process, but as a kind of learning processes, which move in the way and manner, as they move in the way and manner, as they do in the way and manner, as they do in the way and manner, as they do in the way and manner, as they do in the language, as they do in the language, as they do in the language, as they do in the language, as they do in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language of the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language of the language, as in the language, as in the language, as in the language, as in the language, as in the language, as in the language"}, {"heading": "4.3 GoogLeNet", "text": "The first steps in the right direction are the following: \"It is a question of the extent to which it is worthwhile to orient oneself in a different direction.\" The first steps in the right direction are the following: \"It is a question of the extent to which the future is at stake.\" The first steps in the right direction are the following: \"It is a question of the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the future, the"}, {"heading": "5 Summary", "text": "The results presented in this report demonstrate the benefits of the Cyclic Learning Rate (CLR) methodology. A short time span of only a few periods in which the learning rate increases linearly is sufficient to estimate the learning rates of the CLR strategies. Then, a policy in which the learning rate fluctuates cyclically between these limits is sufficient to achieve near-optimal classification results, often with much less iterations. This policy is easy to implement and dislikeable methods of learning rate do not require additional computation.This report presents several possible cyclic functions. All the functions tested revealed some improvements. The triangular and exp strategies have been described most deeply because they are both simple and flexible. Furthermore, the cyclical nature of these methods provides natural times to lower the learning rate values (after 3 or 4 cycles) and to stop the training. All of these factors reduce the guesswork in determining the learning rates and make these methods practical tools for forming neural networks."}, {"heading": "Acknowledgments", "text": "The author thanks David Aha, David Bonanno, Timothy Doster and Emily Hand for their suggestions and helpful comments on this work, which was supported by the US Naval Research Laboratory base program, Recursive Structure Learning."}], "references": [{"title": "Hot swapping for online adaptation of optimization hyperparameters", "author": ["K. Bache", "D. DeCoste", "P. Smyth"], "venue": "arXiv preprint arXiv:1412.6599,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural Networks: Tricks of the Trade, chapter Practical recommendations for gradient-based training of deep architectures, pages 437\u2013478", "author": ["Y. Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Rmsprop and equilibrated adaptive learning rates for nonconvex optimization", "author": ["Y.N. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.04390,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Adaptive stepsizes for recursive estimation with applications in approximate dynamic programming", "author": ["A.P. George", "W.B. Powell"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Adasecant: Robust adaptive secant method for stochastic gradient", "author": ["C. Gulcehre", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.7419,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "An empirical evaluation of deep learning on highway driving", "author": ["B. Huval", "T. Wang", "S. Tandon", "J. Kiske", "W. Song", "J. Pazhayampallil", "M. Andriluka", "R. Cheng-Yue", "F. Mujica", "A. Coates"], "venue": "arXiv preprint arXiv:1504.01716,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Adam: a method for stochastic optimization", "author": ["D. Kingma", "J. Lei-Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "ImageNet Large Scale Visual  Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "No more pesky learning rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1206.1106,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 17, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 15, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 6, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 130, "endOffset": 133}, {"referenceID": 18, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 7, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 177, "endOffset": 180}, {"referenceID": 16, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 202, "endOffset": 206}, {"referenceID": 20, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 9, "context": "Deep convolutional neural networks (CNN) are giving state-of-the-art results for image recognition [13, 18, 16], object detection [7], face recognition [19], speech recognition [8], machine translation [17], image caption generation [21], and driverless car technology [10].", "startOffset": 269, "endOffset": 273}, {"referenceID": 1, "context": "most important hyper-parameter to optimize and if \u201cthere is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyperparameter that is worth tuning\u201d [2].", "startOffset": 198, "endOffset": 201}, {"referenceID": 21, "context": "It is well known that too small a learning rate will make a training algorithm converge slowly while too large a learning rate will make the training algorithm diverge [22].", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "In addition to the CIFAR-10 dataset, cyclical learning rates are demonstrated on ImageNet with two well-known architectures: AlexNet [13] and GoogleNet [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "In addition to the CIFAR-10 dataset, cyclical learning rates are demonstrated on ImageNet with two well-known architectures: AlexNet [13] and GoogleNet [18].", "startOffset": 152, "endOffset": 156}, {"referenceID": 1, "context": "In particular, Yoshua Bengio [2] discusses reasonable ranges for learning rates and stresses the importance of tuning this hyper-parameter.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "A review of the early work on adaptive learning rates can be found in George and Powell [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "[5] proposed AdaGrad, which is one of the early adaptive methods that estimates the learning rates from the gradients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "RMSProp is discussed in the slides by Geoffrey Hinton mentioned above2 [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "[15] discuss an adaptive learning rate based on a diagonal estimation of the Hessian of the gradients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Zeiler [22] describes his AdaDelta method that improves on AdaGrad based on two ideas; to limit the sum of squared gradients over all time to a window, and to make the parameter update rule consistent with a units evaluation on the relationship between the update and the Hessian.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "Gulcehre and Bengio [9] propose an adaptive learning rate algorithm, AdaSecant, that utilizes the root mean square statistics and variance of the gradients.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "[3] propose an adaptive learning rate method based on estimating a diagonal equilibration preconditioner.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "The authors show that RMSProp [20] provides a biased estimate and go on to describe another estimator, ESGD, that is unbiased.", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "Kingma and Lei-Ba [12] introduce \u201cAdam\u201d that computes individual learning rates for different parameters from the gradients.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "[1] propose exploiting solutions to a multi-armed bandit problem for learning rate selection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The difficulty with manually tuning the learning rates can be illustrated with the CIFAR-10 datasetusing the Caffe [11], an open source implementation of convolutional neural networks.", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "[21] argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Also, one can use the rule of thumb that the optimum learning rate is usually within a factor of two of the largest one that converges [2] and set base lr to 1 3 or 1 4 of max lr.", "startOffset": 135, "endOffset": 138}, {"referenceID": 12, "context": "In the subsections below, CLR policies are used for training with the CIFAR-10 dataset and the AlexNet [13] and GoogleNet [18] architectures with the ImageNet dataset.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "In the subsections below, CLR policies are used for training with the CIFAR-10 dataset and the AlexNet [13] and GoogleNet [18] architectures with the ImageNet dataset.", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "ImageNet4 [14] is commonly used in many studies in the deep learning literature.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Fortunately, the Caffe website provides the architecture and hyper-parameter files for a modified AlexNet5 [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "[18] describe the architecture in detail but did not provide the architecture file.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "However, the GoogLeNet paper [18] does not state the values used for the learning rate and the hyper-parameter solver file is not available for a baseline.", "startOffset": 29, "endOffset": 33}], "year": 2015, "abstractText": "It is known that the learning rate is the most important hyper-parameter to tune for training deep convolutional neural networks (i.e., a \u201cguessing game\u201d). This report describes a new method for setting the learning rate, named cyclical learning rates, that eliminates the need to experimentally find the best values and schedule for the learning rates. Instead of setting the learning rate to fixed values, this method lets the learning rate cyclically vary within reasonable boundary values. This report shows that training with cyclical learning rates achieves near optimal classification accuracy without tuning and often in many fewer iterations. This report also describes a simple way to estimate \u201creasonable bounds\u201d by linearly increasing the learning rate in one training run of the network for only a few epochs. In addition, cyclical learning rates are demonstrated on training with the CIFAR-10 dataset and the AlexNet and GoogLeNet architectures on the ImageNet dataset. These methods are practical tools for everyone who trains convolutional neural networks.", "creator": "LaTeX with hyperref package"}}}