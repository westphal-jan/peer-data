{"id": "1506.01192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Personalizing Universal Recurrent Neural Network Language Model with User Characteristic Features by Social Network Crowdsouring", "abstract": "With the popularity of mobile devices, personalized speech recognizer becomes more realizable today and highly attractive. Each mobile device is primarily used by a single user, so it's possible to have a personalized recognizer well matching to the characteristics of individual user. Although acoustic model personalization has been investigated for decades, much less work have been reported on personalizing language model, probably because of the difficulties in collecting enough personalized corpora. Previous work used the corpora collected from social networks to solve the problem, but constructing a personalized model for each user is troublesome. In this paper, we propose a universal recurrent neural network language model with user characteristic features, so all users share the same model, except each with different user characteristic features. These user characteristic features can be obtained by crowdsouring over social networks, which include huge quantity of texts posted by users with known friend relationships, who may share some subject topics and wording patterns. The preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the model perplexity, but offered very good improvement in recognition accuracy in n-best rescoring tests. This approach also mitigated the data sparseness problem for personalized language models.", "histories": [["v1", "Wed, 3 Jun 2015 10:14:21 GMT  (137kb,D)", "http://arxiv.org/abs/1506.01192v1", null], ["v2", "Tue, 23 Aug 2016 03:40:47 GMT  (168kb,D)", "http://arxiv.org/abs/1506.01192v2", "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA"], ["v3", "Tue, 22 Nov 2016 10:12:12 GMT  (169kb,D)", "http://arxiv.org/abs/1506.01192v3", "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["bo-hsiang tseng", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1506.01192"}, "pdf": {"name": "1506.01192.pdf", "metadata": {"source": "CRF", "title": "Personalizing A Universal Recurrent Neural Network Language Model with User Characteristic Features by Crowdsouring over Social Networks", "authors": ["Bo-Hsiang Tseng", "Hung-Yi Lee", "Lin-Shan Lee"], "emails": ["r02942037@ntu.edu.tw,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "Although the personalization of acoustic models has been researched for decades, the personalization of speech models has been much less reported, probably due to the difficulty of collecting enough personalized corpora. Previous work used corpora collected from social networks to solve the problem, but creating a personalized model for each user is tedious. In this essay, we propose a universal, recurring neural network language model with user characteristics, so that all users share the same model, with the exception of each with different user characteristics. These user characteristics can be achieved through crowdsourcing through social networks, which include huge amounts of texts posted by users with known friendship relationships who can share some subject areas and formulation patterns. Preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the problem of perplexity of the model, but also a very good improvement of personalized network models to best possible adaptation."}, {"heading": "1. Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country"}, {"heading": "2. Scenario of LM personalization", "text": "Crowdsourcing [21, 22] has different definitions and has been widely used in various areas of activity. For example, a crowdsourcing approach has been proposed to collect queries for retrieving information taking into account time information. [23] The MIT movie browser [24, 25] relied on Amazon Mechanical Turk to build a crowdsourcing-monitored language system. In this work, a cloud-based application was implemented that helps users access their social network via language, and was considered a crowdsourcing platform for collecting personal data. If the user logs into his Facebook account, he may choose to give that application the power to collect his acoustic and linguistic data to personalize the voice access service. The user should enjoy the benefits of better recognition accuracy that the personalized detector brings with it due to the scrawled data. The scenario of the proposed approach is presented in paragraph 1. For a user, the red figure will be displayed in the left-hand of the Social Figure 1."}, {"heading": "3. Proposed Approach", "text": "As shown in the right part of Figure 1, the universal RNNLM consists of three layers: the input layer, the hidden layer, and the output layer [16], except that the input layer is the concatenation of the word vector w (t), which represents the t-th word in a sentence using a 1-of-N encoding and an additional user characteristic f. User characteristic f is associated with both the hidden layer s (t) and the output layers y (t) 1. This feature f helps the model to take the specific user into account. Network weights to be learned are the matrices W, F, S, G, and O to the right of Figure 1. Contributions from a large group of users serve as training data for the universal RNNLM."}, {"heading": "3.1. Extraction of User Characteristic Feature", "text": "In order to make the user characteristic f reflect the semantics of the frequently mentioned topics of a user, it is naturally used to represent the user characteristic f as the topic distribution of the social texts that the user has posted on the social network.However, in our preliminary experiments, we have found that the use of such topic distributions as user characteristics has led to poor results because a user in the real world has a large variety of topics and very often changes topics. To address this problem, we make the user characteristic not only user dependent, but also sentence dependent. We first formed a theme model from a large corpus, and then the topic model is used to derive the topic distribution of all the sentences in the dataset of the social network.During the training phase of RNNLM, a sentence of a particular user is given (e.g. sentence i of user A to the left of picture 1), the N-sentences within the personal corpus of this user characteristic (the red circle of the user I) are used as a set of this user characteristic."}, {"heading": "3.2. Effect of user characteristic feature on RNNLM", "text": "User A mentioned a lot about problems related to \"coffee\" in the Facebook data, while user B never did, resulting in very different user characteristics for the two users. In view of the phrase \"a bottle of milk can yield 3 cups of latte,\" which was more likely produced by user A, the perplexity evaluated by the conventional RNNLM and the personalized RNNLM with different user characteristics in Table 1. The conventional RNLM is in row (a). We see that the personalized RNLM with user characteristic fA of user A drastically reduced the PPL (152 vs 355) as in row (b), while with user B's user characteristic significantly increased the perplexity (604 vs 355) in row (c).1This structure runs parallel to the context-dependent RNLM variant, with user B's fin the context being replaced by feature 18."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setups", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. Corpus & LMs", "text": "A total of 42 users signed up and authorized this project to collect their messages and basic information for research. These 42 users were our target users and were divided into 3 groups for cross-validation, i.e. to train the universal LM on the data of two groups and to test it for the rest. In addition, the experiments were provided with the observable public data (personal and friend data) of these 42 target users, which contained an average of 10.6 words (Chinese, English or mixed) per sentence. A total of 12,000 sentences for the 42 target users were taken as a test set and a total of 2.4 million sentences. The number of sentences for each user among the 93,000 moving words ranged from 1 to 8,566 with an average of 25.7, consisting of 10.6 words (Chinese, English or mixed) per sentence. A total of 12,000 sentences for the 42 target users were taken as a test set, and among them 948 sentences for the target users produced as solos."}, {"heading": "4.1.2. N-best rescoring", "text": "We used grids made with the HTK toolkit [31] to generate 1,000 leaderboards for rescoring. LM, which was used to create the n-best lists, was a trigram model adapted to the personal corpora and friends corpora with Kneser-Ney smoothing (KN3). Mandarin triphonic models used for first-pass decoding were trained on the ASTMIC body [ref], while English triphon models were trained on the Sinica Taiwan English Corpus [ref], both with hundreds of speakers. Both models were adapted by unattended MLLR."}, {"heading": "4.2. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1. Extraction of User Characteristic Feature", "text": "As mentioned in Section 3.1, only the N sentences that are most similar to the sentence under consideration are used. Fig. 2 shows the helplessness with different N (from the user corpus plus the corpus of friends) and different number of topics for LDA. In Fig. 2, there was almost no difference between N = 1 and N = 2. However, as N grew beyond 2, the helplessness increased, which implied a very wide variety of topics even for the same user and his friends. In the following experiments, we therefore opted for N = 1."}, {"heading": "4.2.2. Perplexity and rescoring result", "text": "The results of the perplexity and recognition accuracy are listed in Table 2. part (a) is for Kneser-Ney 3grams, row (a-1) without fitting, while rows (a-2) and (a-3) are adapted by personal corpus (marked P) or personal plus friend corpus (marked P + F). It is clear that the fitting Kneser-Ney2Only is a tenth of the personal corpus used in this preliminary experiment. 3gram LM with personal and friend corpus are helpful (lines (a-2), (a-3) vs (a-1). Part (b) is the RNLM baseline without fitting. Here, the RNLM baseline resulted in a worse perplexity compared with Kneser-Ney 3grams (part b) vs NPERPERPLEXE (2) vs PERPERPLEXE) 3, but it improved detection accuracy (parts)."}, {"heading": "4.2.3. Sizes of personal corpora", "text": "As mentioned above, the previous approach of adjusting the background model to a personalized model may suffer from over-adjustment with limited adjustment data and therefore lead to poor performance in the new data of the respective user, as confirmed in Fig. 3. The horizontal axis of Fig. 3 is the percentage of the originally used personal corpora, and 1.00 means the use of the original personal corpora or case series (c-1) and (d-1) in Table 2. We see in Fig. 3 that the less data available, the suggested approach (RNLM / UCF, P) had grown much slower and with a much more stable accuracy, whereas the proposed approach (RNNLM / adapt, P) with the relatively small hidden layer size of 50 in the preliminary experiments may have reached a perplexity higher than n-gram-based LM [15]."}, {"heading": "5. Conclusions", "text": "In this paper, we proposed a new framework for the personalization of a universal RNNLM, in which data is searched through social networks.The proposed approach is based on the user characteristic extracted from the user corpus and the corpus of friends, which is not only user-specific but also sentence-dependent. This universal RNNLM can predict different word distributions for different users due to the same content. Experiments showed that very good improvements were made in both perplexity and accuracy, and the proposed approach is much more robust in terms of data poverty than the previous work."}, {"heading": "6. References", "text": "[1] G. Zweig and C. Shuang yu, \"Personalizing model m for voice-search,\" in Proc. on InterSpeech, 2011. [2] M. Speretta and S. Gauch, \"Personalized search based on user search histories,\" in Proc. on Web Intelligence, 2005. [3] Y. H. Cho, J. K. Kim, and S. H. Kim, \"A personalized commender system on web usage mining and decision tree induction,\" Expert Systems with Applications, 2002. [4] Y. Koren, R. Bell, and C. Volinsky, \"Matrix factorization techniques for commender systems,\" Computer, 2009. [5] F. Walter, S. Battiston, and F. Schweitzer, \"A model of a trustbased recommendation system on a social network,\" Autonomous Agents and Multi-Agent Systems, 2008. [6] M.-H. Park, J.-H. Hong, and S.-B. Cho. \""}], "references": [{"title": "Personalizing model m for voicesearch", "author": ["G. Zweig", "C. Shuang yu"], "venue": "Proc. on InterSpeech, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Personalized search based on user search histories", "author": ["M. Speretta", "S. Gauch"], "venue": "Proc. on Web Intelligence, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A personalized recommender system based on web usage mining and decision tree induction", "author": ["Y.H. Cho", "J.K. Kim", "S.H. Kim"], "venue": "Expert Systems with Applications, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A model of a trustbased recommendation system on a social network", "author": ["F. Walter", "S. Battiston", "F. Schweitzer"], "venue": "Autonomous Agents and Multi-Agent Systems, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Location-based recommendation system using bayesian users preference model in mobile devices", "author": ["M.-H. Park", "J.-H. Hong", "S.-B. Cho"], "venue": "Ubiquitous Intelligence and Computing, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models", "author": ["C.J. Leggetter", "P.C. Woodland"], "venue": "Computer Speech and Language, 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Speaker adaptation for continuous density hmms: A review", "author": ["P.C. Woodland"], "venue": "Proc. on ITRW on Adaptation Methods for Speech Recognition, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains", "author": ["P.C. Woodland"], "venue": "IEEE Transactions on Speech and Audio Processing, 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton", "Li Deng", "Dong Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE , vol.29, no.6, pp.82,97, Nov. 2012", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical language model adaptation: review and perspectives", "author": ["PJ.R. Bellegarda"], "venue": "Speech Communication, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust topic inference for latent semantic language model adaptation", "author": ["A. Heidel", "L.-S. Lee"], "venue": "Proc. on ASRU, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Style and topic language model adaptation using hmm-lda", "author": ["H. Bo-June", "J. Glass"], "venue": "Proc. on EMNLP, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Personalized language modeling by crowd sourcing with social network data for voice access of cloud applications", "author": ["T.-H. Wen", "H.-Y. Lee", "T.-Y. Chen", "L.-S. Lee"], "venue": "Proc. on IEEE SLT workshop, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent Neural Network Based Language Model Personalization by Social Network Crowdsourcing", "author": ["T.-H. Wen", "A. Heidel", "H.-Y. Lee", "Yu Tsao", "L.-S. Lee"], "venue": "Proc. on InterSpeech, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafit", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": "Proc. on InterSpeech, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "Proc. on ICASSP, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "Proc. on IEEE SLT workshop, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proc. on ASRU, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "I-vector-based speaker adaptation of deep neural networks for French broadcast audio transcription", "author": ["V. Gupta", "P. Kenny", "P. Ouellet", "T. Stafylakis"], "venue": "Proc. on ICASSP, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Crowdsourcing systems on the world-wide web", "author": ["A. Doan", "R. Ramakrishnan", "A.Y. Halevy"], "venue": "Communications of the ACM, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Crowdsourcing and language studies: the new generation of linguistic data", "author": ["Munro", "Robert"], "venue": "Proc. on NAACL, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "A language modeling approach for temporal information needs", "author": ["K. Berberich", "S. Bedathur", "O. Alonso", "G. Weikum"], "venue": "Advances in Information Retrieval, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A conversational movie search system based on conditional random field", "author": ["J. Liu", "S. Cyphers", "P. Pasupat", "I. McGraw", "J. Glass"], "venue": "Proc. on InterSpeech, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Automating crowd-supervised learning for spoken language systems", "author": ["I. McGraw", "S. Cyphers", "P. Pasupat", "J. Liu", "J. Glass"], "venue": "Proc. on InterSpeech, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res., 2003.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "MALLET: A Machine Learning for Language Toolkit", "author": ["McCallum", "Andrew Kachites"], "venue": "http://mallet.cs.umass.edu. 2002.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Modified kneser-ney smoothing of n-gram models modified kneser-ney smoothing of n-gram models", "author": ["F. James"], "venue": "Tech. Rep., 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Srilm - an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "Proc. on Spoken Language Processing, 2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Rnnlm - recurrent neural network language modeling toolkit", "author": ["T. Mikolov", "S. Kombrink", "A. Deoras", "L. Burget", "J. Cernocky"], "venue": "Proc. on ASRU, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].", "startOffset": 47, "endOffset": 53}, {"referenceID": 1, "context": "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].", "startOffset": 47, "endOffset": 53}, {"referenceID": 2, "context": "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].", "startOffset": 94, "endOffset": 106}, {"referenceID": 3, "context": "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].", "startOffset": 94, "endOffset": 106}, {"referenceID": 4, "context": "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].", "startOffset": 94, "endOffset": 106}, {"referenceID": 5, "context": "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].", "startOffset": 94, "endOffset": 106}, {"referenceID": 6, "context": "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].", "startOffset": 26, "endOffset": 35}, {"referenceID": 7, "context": "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].", "startOffset": 26, "endOffset": 35}, {"referenceID": 8, "context": "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].", "startOffset": 26, "endOffset": 35}, {"referenceID": 9, "context": "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].", "startOffset": 239, "endOffset": 243}, {"referenceID": 10, "context": "Although LM adaptation [11, 12, 13] primarily focus on the problem of crossdomain or cross-genre linguistic mismatch, while the cross individual linguistic mismatch is often ignored.", "startOffset": 23, "endOffset": 35}, {"referenceID": 11, "context": "Although LM adaptation [11, 12, 13] primarily focus on the problem of crossdomain or cross-genre linguistic mismatch, while the cross individual linguistic mismatch is often ignored.", "startOffset": 23, "endOffset": 35}, {"referenceID": 12, "context": "Although LM adaptation [11, 12, 13] primarily focus on the problem of crossdomain or cross-genre linguistic mismatch, while the cross individual linguistic mismatch is often ignored.", "startOffset": 23, "endOffset": 35}, {"referenceID": 13, "context": "Personalization of LM was proposed and investigated based on both N-gram-based LM [14] and recurrent neural network(RNNLM) [15] in the very limited previous works.", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "Personalization of LM was proposed and investigated based on both N-gram-based LM [14] and recurrent neural network(RNNLM) [15] in the very limited previous works.", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "In the conventional RNNLM [16, 17, 18], the 1-of-N encoding of wach word is taken as the input of the RNN, and then given the history word sequence, RNN outputs the estimated probability distribution for the next word.", "startOffset": 26, "endOffset": 38}, {"referenceID": 16, "context": "In the conventional RNNLM [16, 17, 18], the 1-of-N encoding of wach word is taken as the input of the RNN, and then given the history word sequence, RNN outputs the estimated probability distribution for the next word.", "startOffset": 26, "endOffset": 38}, {"referenceID": 17, "context": "In the conventional RNNLM [16, 17, 18], the 1-of-N encoding of wach word is taken as the input of the RNN, and then given the history word sequence, RNN outputs the estimated probability distribution for the next word.", "startOffset": 26, "endOffset": 38}, {"referenceID": 18, "context": "The concept of modifying input features for personalization is similar to the i-vectors used in deep neural network (DNN) -based acoustic models [19, 20], in which the i-vector of each speaker is used to extend the acoustic features like MFCC.", "startOffset": 145, "endOffset": 153}, {"referenceID": 19, "context": "The concept of modifying input features for personalization is similar to the i-vectors used in deep neural network (DNN) -based acoustic models [19, 20], in which the i-vector of each speaker is used to extend the acoustic features like MFCC.", "startOffset": 145, "endOffset": 153}, {"referenceID": 20, "context": "Crowdsourcing [21, 22] has varying definitions and was widely applied in various tasks.", "startOffset": 14, "endOffset": 22}, {"referenceID": 21, "context": "Crowdsourcing [21, 22] has varying definitions and was widely applied in various tasks.", "startOffset": 14, "endOffset": 22}, {"referenceID": 22, "context": "For example, a crowdsourcing approach was proposed to collect queries for information retrieval considering temporal information [23].", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "The MIT movie browser [24, 25] relied on Amazon Mechanical Turk to build a crowd-supervised spoken language system.", "startOffset": 22, "endOffset": 30}, {"referenceID": 24, "context": "The MIT movie browser [24, 25] relied on Amazon Mechanical Turk to build a crowd-supervised spoken language system.", "startOffset": 22, "endOffset": 30}, {"referenceID": 10, "context": "Considering the fact that all these collected personal corpora are small, they were used for LM adaptation in the previous work [11], in which the personal corpus and friend corpus of a user are treated as the adaptation corpus to adapt a background LM trained with a large background corpus not sufficiently related to the target task.", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "As shown in the right part of Figure 1, the universal RNNLM comprises three layers: the input layer, the hidden layer, and the output layer are used [16], except that the input layer is the concatenation of the word vector w(t) representing the t-th word in a sentence using an 1-of-N encoding and an additional user characteristic feature f.", "startOffset": 149, "endOffset": 153}, {"referenceID": 17, "context": "1This structure is parallel to the context dependent RNNLM variant [18], except that the context feature in the input layer is replaced by the user characteristic feature f.", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "The topic model we used is trained by Latent Dirichlet Allocation (LDA) [26] with Mallet toolkit [27], taking each sentence as a document.", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "The topic model we used is trained by Latent Dirichlet Allocation (LDA) [26] with Mallet toolkit [27], taking each sentence as a document.", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "The modified Kneser-Ney algorithm [28] was used for the N-gram LM smoothing.", "startOffset": 34, "endOffset": 38}, {"referenceID": 28, "context": "The SRILM [29] toolkit was used for the Ngram LM training and adaptation, while RNNLM toolkit [30] is used for RNNLM here.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "The SRILM [29] toolkit was used for the Ngram LM training and adaptation, while RNNLM toolkit [30] is used for RNNLM here.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Part (c) is for RNNLM personalization with adaptation in previous work [15].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "3With the relatively small hidden layer size of 50 in the preliminary experiments, it is possible that RNNLM obtained perplexity higher than n-gram-based LM [15].", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "With the popularity of mobile devices, personalized speech recognizer becomes more realizable today and highly attractive. Each mobile device is primarily used by a single user, so it\u2019s possible to have a personalized recognizer well matching to the characteristics of individual user. Although acoustic model personalization has been investigated for decades, much less work have been reported on personalizing language model, probably because of the difficulties in collecting enough personalized corpora. Previous work used the corpora collected from social networks to solve the problem, but constructing a personalized model for each user is troublesome. In this paper, we propose a universal recurrent neural network language model with user characteristic features, so all users share the same model, except each with different user characteristic features. These user characteristic features can be obtained by crowdsouring over social networks, which include huge quantity of texts posted by users with known friend relationships, who may share some subject topics and wording patterns. The preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the model perplexity, but offered very good improvement in recognition accuracy in n-best rescoring tests. This approach also mitigated the data sparseness problem for personalized language models.", "creator": "LaTeX with hyperref package"}}}